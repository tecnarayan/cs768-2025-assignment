@article{ji_ups_2012,
  title={UPS delivers optimal phase diagram in high-dimensional variable selection},
  author={Ji, Pengsheng and Jin, Jiashun and others},
  journal={The Annals of Statistics},
  volume={40},
  number={1},
  pages={73--103},
  year={2012},
  publisher={Institute of Mathematical Statistics}
}


@article{ji_rate_2014,
  title={Rate optimal multiple testing procedure in high-dimensional regression},
  author={Ji, Pengsheng and Zhao, Zhigen},
  journal={arXiv preprint arXiv:1404.2961},
  year={2014}
}


@article{chia_interpretable_2020,
       author = {{Chia}, Charmaine and {Sesia}, Matteo and {Ho}, Chi-Sing and
         {Jeffrey}, Stefanie S. and {Dionne}, Jennifer and {Cand
        {\`e}s}, Emmanuel J. and {Howe}, Roger T.},
        title = "{Interpretable Signal Analysis with Knockoffs Enhances Classification of Bacterial Raman Spectra}",
      journal = {arXiv e-prints},
     keywords = {Electrical Engineering and Systems Science - Signal Processing, Statistics - Applications, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.04937},
        pages = {arXiv:2006.04937},
archivePrefix = {arXiv},
       eprint = {2006.04937},
 primaryClass = {eess.SP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200604937C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{askari_fanok_2020,
       author = {{Askari}, Armin and {Rebjock}, Quentin and {d'Aspremont}, Alexandre and
         {El Ghaoui}, Laurent},
        title = "{FANOK: Knockoffs in Linear Time}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Methodology, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.08790},
        pages = {arXiv:2006.08790},
archivePrefix = {arXiv},
       eprint = {2006.08790},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200608790A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@InProceedings{dai_knockoff_2016,
  title = 	 {The knockoff filter for FDR control in group-sparse and multitask regression},
  author = 	 {Ran Dai and Rina Barber},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1851--1859},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/daia16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/daia16.html},
  abstract = 	 {We propose the group knockoff filter, a method for false discovery rate control in a linear regression setting where the features are grouped, and we would like to select a set of relevant groups which have a nonzero effect on the response. By considering the set of true and false discoveries at the group level, this method gains power relative to sparse regression methods. We also apply our method to the multitask regression problem where multiple response variables share similar sparsity patterns across the set of possible features. Empirically, the group knockoff filter successfully controls false discoveries at the group level in both settings, with substantially more discoveries made by leveraging the group structure.}
}


@inproceedings{nguyen_ecko_2019,
  title={ECKO: Ensemble of Clustered Knockoffs for Robust Multivariate Inference on fMRI Data},
  author={Nguyen, Tuan-Binh and Chevalier, J{\'e}r{\^o}me-Alexis and Thirion, Bertrand},
  booktitle={International Conference on Information Processing in Medical Imaging},
  pages={454--466},
  year={2019},
  organization={Springer}
}


@article{nguyen_aggregation_2020,
  title={Aggregation of Multiple Knockoffs},
  author={Nguyen, Binh T and Chevalier, J{\'e}r{\^o}me-Alexis and Thirion, Bertrand and Arlot, Sylvain},
  journal={arXiv preprint arXiv:2002.09269},
  year={2020}
}


@inproceedings{liu_power_2019,
  title={Power analysis of knockoff filters for correlated designs},
  author={Liu, Jingbo and Rigollet, Philippe},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15420--15429},
  year={2019}
}


@article{rabinovich_lower_2020,
  title={Lower bounds in multiple testing: A framework based on derandomized proxies},
  author={Rabinovich, Max and Jordan, Michael I and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2005.03725},
  year={2020}
}

@article{huang_relaxing_2019,
  title={Relaxing the Assumptions of Knockoffs by Conditioning},
  author={Huang, Dongming and Janson, Lucas},
  journal={arXiv preprint arXiv:1903.02806},
  year={2019}
}

@article{emery_controlling_2019,
       author = {{Emery}, Kristen and {Keich}, Uri},
        title = "{Controlling the FDR in variable selection via multiple knockoffs}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = 2019,
        month = nov,
          eid = {arXiv:1911.09442},
        pages = {arXiv:1911.09442},
archivePrefix = {arXiv},
       eprint = {1911.09442},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191109442E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{giraud_introduction_2014,
  title={Introduction to high-dimensional statistics},
  author={Giraud, Christophe},
  year={2014},
  publisher={Chapman and Hall/CRC}
}


@article{blanchard_adaptive_2009,
  title={Adaptive false discovery rate control under independence and dependence},
  author={Blanchard, Gilles and Roquain, {\'E}tienne},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Dec},
  pages={2837--2871},
  year={2009}
}


@article{benjamini_control_2001,
author = "Benjamini, Yoav and Yekutieli, Daniel",
doi = "10.1214/aos/1013699998",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "08",
number = "4",
pages = "1165--1188",
publisher = "The Institute of Mathematical Statistics",
title = "The control of the false discovery rate in multiple testing under dependency",
url = "https://doi.org/10.1214/aos/1013699998",
volume = "29",
year = "2001"
}

@article{holden_multiple_2018,
  title={Multiple Model-Free Knockoffs},
  author={Holden, Lars and Helton, Kristoffer H},
  journal={arXiv preprint arXiv:1812.04928},
  year={2018}
}

@article{su_communication_2015,
  title={Communication-efficient false discovery rate control via knockoff aggregation},
  author={Su, Weijie and Qian, Junyang and Liu, Linxi},
  journal={arXiv preprint arXiv:1506.05446},
  year={2015}
}

@article {cai_putative_2007,
	author = {Cai, Xiaoning and Ballif, Jenny and Endo, Saori and Davis, Elizabeth and Liang, Mingxiang and Chen, Dong and DeWald, Daryll and Kreps, Joel and Zhu, Tong and Wu, Yajun},
	title = {A Putative CCAAT-Binding Transcription Factor Is a Regulator of Flowering Timing in Arabidopsis},
	volume = {145},
	number = {1},
	pages = {98--105},
	year = {2007},
	doi = {10.1104/pp.107.102079},
	publisher = {American Society of Plant Biologists},
	abstract = {Flowering at the appropriate time of year is essential for successful reproduction in plants. We found that HAP3b in Arabidopsis (Arabidopsis thaliana), a putative CCAAT-binding transcription factor gene, is involved in controlling flowering time. Overexpression of HAP3b promotes early flowering while hap3b, a null mutant of HAP3b, is delayed in flowering under a long-day photoperiod. Under short-day conditions, however, hap3b did not show a delayed flowering compared to wild type based on the leaf number, suggesting that HAP3b may normally be involved in the photoperiod-regulated flowering pathway. Mutant hap3b plants showed earlier flowering upon gibberellic acid or vernalization treatment, which means that HAP3b is not involved in flowering promoted by gibberellin or vernalization. Further transcript profiling and gene expression analysis suggests that HAP3b can promote flowering by enhancing expression of key flowering time genes such as FLOWERING LOCUS T and SUPPRESSOR OF OVEREXPRESSION OF CONSTANS1. Our results provide strong evidence supporting a role of HAP3b in regulating flowering in plants grown under long-day conditions.},
	issn = {0032-0889},
	URL = {http://www.plantphysiol.org/content/145/1/98},
	eprint = {http://www.plantphysiol.org/content/145/1/98.full.pdf},
	journal = {Plant Physiology}
}


@article{silverstone_arabidopsis_1999,
	author = {Silverstone, Aron L. and Ciampaglio, Charles N. and Sun, Tai-ping},
	title = {The Arabidopsis RGA Gene Encodes a Transcriptional Regulator Repressing the Gibberellin Signal Transduction Pathway},
	volume = {10},
	number = {2},
	pages = {155--169},
	year = {1998},
	doi = {10.1105/tpc.10.2.155},
	publisher = {American Society of Plant Biologists},
	abstract = {The recessive rga mutation is able to partially suppress phenotypic defects of the Arabidopsis gibberellin (GA) biosynthetic mutant ga1-3. Defects in stem elongation, flowering time, and leaf abaxial trichome initiation are suppressed by rga. This indicates that RGA is a negative regulator of the GA signal transduction pathway. We have identified 10 additional alleles of rga from a fast-neutron mutagenized ga1-3 population and used them to isolate the RGA gene by genomic subtraction. Our data suggest that RGA may be functioning as a transcriptional regulator. RGA was found to be a member of the VHIID regulatory family, which includes the radial root organizing gene SCARECROW and another GA signal transduction repressor, GAI. RGA and GAI proteins share a high degree of homology, but their N termini are more divergent. The presence of several structural features, including homopolymeric serine and threonine residues, a putative nuclear localization signal, leucine heptad repeats, and an LXXLL motif, indicates that the RGA protein may be a transcriptional regulator that represses the GA response. In support of the putative nuclear localization signal, we demonstrated that a transiently expressed green fluorescent protein{\textendash}RGA fusion protein is localized to the nucleus in onion epidermal cells. Because the rga mutation abolished the high level of expression of the GA biosynthetic gene GA4 in the ga1-3 mutant background, we conclude that RGA may also play a role in controlling GA biosynthesis.},
	issn = {1040-4651},
	URL = {http://www.plantcell.org/content/10/2/155},
	eprint = {http://www.plantcell.org/content/10/2/155.full.pdf},
	journal = {The Plant Cell}
}


@article {kim_fiona1_2008,
	author = {Kim, Jeongsik and Kim, Yumi and Yeom, Miji and Kim, Jin-Hee and Nam, Hong Gil},
	title = {FIONA1 Is Essential for Regulating Period Length in the Arabidopsis Circadian Clock},
	volume = {20},
	number = {2},
	pages = {307--319},
	year = {2008},
	doi = {10.1105/tpc.107.055715},
	publisher = {American Society of Plant Biologists},
	abstract = {In plants, the circadian clock controls daily physiological cycles as well as daylength-dependent developmental processes such as photoperiodic flowering and seedling growth. Here, we report that FIONA1 (FIO1) is a genetic regulator of period length in the Arabidopsis thaliana circadian clock. FIO1 was identified by screening for a mutation in daylength-dependent flowering. The mutation designated fio1-1 also affects daylength-dependent seedling growth. fio1-1 causes lengthening of the free-running circadian period of leaf movement and the transcription of various genes, including the central oscillators CIRCADIAN CLOCK-ASSOCIATED1, LATE ELONGATED HYPOCOTYL, TIMING OF CAB EXPRESSION1, and LUX ARRHYTHMO. However, period lengthening is not dependent upon environmental light or temperature conditions, which suggests that FIO1 is not a simple input component of the circadian system. Interestingly, fio1-1 exerts a clear effect on the period length of circadian rhythm but has little effect on its amplitude and robustness. FIO1 encodes a novel nuclear protein that is highly conserved throughout the kingdoms. We propose that FIO1 regulates period length in the Arabidopsis circadian clock in a close association with the central oscillator and that the circadian period can be controlled separately from amplitude and robustness.},
	issn = {1040-4651},
	URL = {http://www.plantcell.org/content/20/2/307},
	eprint = {http://www.plantcell.org/content/20/2/307.full.pdf},
	journal = {The Plant Cell}
}


@article{atwell_genome_2010,
  title={Genome-wide association study of 107 phenotypes in Arabidopsis thaliana inbred lines},
  author={Atwell, Susanna and Huang, Yu S and Vilhj{\'a}lmsson, Bjarni J and Willems, Glenda and Horton, Matthew and Li, Yan and Meng, Dazhe and Platt, Alexander and Tarone, Aaron M and Hu, Tina T and others},
  journal={Nature},
  volume={465},
  number={7298},
  pages={627},
  year={2010},
  publisher={Nature Publishing Group}
}

@article{azencott_efficient_2013,
  title={Efficient network-guided multi-locus association mapping with graph cuts},
  author={Azencott, Chlo{\'e}-Agathe and Grimm, Dominik and Sugiyama, Mahito and Kawahara, Yoshinobu and Borgwardt, Karsten M},
  journal={Bioinformatics},
  volume={29},
  number={13},
  pages={i171--i179},
  year={2013},
  publisher={Oxford University Press}
}

@inproceedings{slim_kernelpsi_2019,
  title={kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection},
  author={Slim, Lotfi and Chatelain, Cl{\'e}ment and Azencott, Chlo{\'e}-Agathe and Vert, Jean-Philippe},
  booktitle={International Conference on Machine Learning},
  pages={5857--5865},
  year={2019}
}

@InProceedings{gimenez_improving_2019,
  title = 	 {Improving the Stability of the Knockoff Procedure: Multiple Simultaneous Knockoffs and Entropy Maximization},
  author = 	 {Gimenez, Jaime Roquero and Zou, James},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {2184--2192},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/gimenez19b/gimenez19b.pdf},
  url = 	 {http://proceedings.mlr.press/v89/gimenez19b.html},
  abstract = 	 {The Model-X knockoff procedure has recently emerged as a powerful approach for feature selection with statistical guarantees. The advantage of knockoffs is that if we have a good model of the features X, then we can identify salient features without knowing anything about how the outcome Y depends on X. An important drawback of knockoffs is its instability: running the procedure twice can result in very different selected features, potentially leading to different conclusions. Addressing this instability is critical for obtaining reproducible and robust results. Here we present a generalization of the knockoff procedure that we call simultaneous multi-knockoffs. We show that multi-knockoffs guarantee false discovery rate (FDR) control, and are substantially more stable and powerful compared to the standard (single) knockoffs.  Moreover we propose a new algorithm based on entropy maximization for generating Gaussian multi-knockoffs. We validate the improved stability and power of multi-knockoffs in systematic experiments. We also illustrate how multi-knockoffs can improve the accuracy of detecting genetic mutations that are causally linked to phenotypes.}
}



@article{javanmard_fdr_2019,
author = "Javanmard, Adel and Javadi, Hamid",
doi = "10.1214/19-EJS1554",
fjournal = "Electronic Journal of Statistics",
journal = "Electron. J. Statist.",
number = "1",
pages = "1212--1253",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = "False discovery rate control via debiased lasso",
url = "https://doi.org/10.1214/19-EJS1554",
volume = "13",
year = "2019"
}


@article{katsevich_multilayer_2019,
	title = {Multilayer knockoff filter: {Controlled} variable selection at multiple resolutions},
	volume = {13},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Multilayer knockoff filter},
	url = {https://projecteuclid.org/euclid.aoas/1554861639},
	doi = {10.1214/18-AOAS1185},
	abstract = {We tackle the problem of selecting from among a large number of variables those that are “important” for an outcome. We consider situations where groups of variables are also of interest. For example, each variable might be a genetic polymorphism, and we might want to study how a trait depends on variability in genes, segments of DNA that typically contain multiple such polymorphisms. In this context, to discover that a variable is relevant for the outcome implies discovering that the larger entity it represents is also important. To guarantee meaningful results with high chance of replicability, we suggest controlling the rate of false discoveries for findings at the level of individual variables and at the level of groups. Building on the knockoff construction of Barber and Candès [Ann. Statist. 43 (2015) 2055–2085] and the multilayer testing framework of Barber and Ramdas [J. Roy. Statist. Soc. Ser. B 79 (2017) 1247–1268], we introduce the multilayer knockoff filter (MKF). We prove that MKF simultaneously controls the FDR at each resolution and use simulations to show that it incurs little power loss compared to methods that provide guarantees only for the discoveries of individual variables. We apply MKF to analyze a genetic dataset and find that it successfully reduces the number of false gene discoveries without a significant reduction in power.},
	language = {EN},
	number = {1},
	urldate = {2019-07-02},
	journal = {The Annals of Applied Statistics},
	author = {Katsevich, Eugene and Sabatti, Chiara},
	month = mar,
	year = {2019},
	mrnumber = {MR3937419},
	keywords = {\$p\$-filter, false discovery rate (FDR), genomewide association study (GWAS), group FDR, knockoff filter, multiresolution, Variable selection},
	pages = {1--33},
}

@article{gimenez_knockoffs_2018,
	title = {Knockoffs for the mass: new feature importance statistics with false discovery guarantees},
	shorttitle = {Knockoffs for the mass},
	url = {https://arxiv.org/abs/1807.06214},
	language = {en},
	urldate = {2018-10-31},
	author = {Gimenez, Jaime Roquero and Ghorbani, Amirata and Zou, James},
	month = jul,
	year = {2018},
}

@article{liu_auto-encoding_2018,
	title = {Auto-{Encoding} {Knockoff} {Generator} for {FDR} {Controlled} {Variable} {Selection}},
	url = {http://arxiv.org/abs/1809.10765},
	abstract = {A new statistical procedure (Model-X {\textbackslash}cite\{candes2018\}) has provided a way to identify important factors using any supervised learning method controlling for FDR. This line of research has shown great potential to expand the horizon of machine learning methods beyond the task of prediction, to serve the broader needs in scientific researches for interpretable findings. However, the lack of a practical and flexible method to generate knockoffs remains the major obstacle for wide application of Model-X procedure. This paper fills in the gap by proposing a model-free knockoff generator which approximates the correlation structure between features through latent variable representation. We demonstrate our proposed method can achieve FDR control and better power than two existing methods in various simulated settings and a real data example for finding mutations associated with drug resistance in HIV-1 patients.},
	urldate = {2019-07-01},
	journal = {arXiv:1809.10765 [stat]},
	author = {Liu, Ying and Zheng, Cheng},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.10765},
	keywords = {Statistics - Methodology},
}


@article{bates_metropolized_2019,
	title = {Metropolized {Knockoff} {Sampling}},
	url = {http://arxiv.org/abs/1903.00434},
	abstract = {Model-X knockoffs is a wrapper that transforms essentially any feature importance measure into a variable selection algorithm, which discovers true effects while rigorously controlling the expected fraction of false positives. A frequently discussed challenge to apply this method is to construct knockoff variables, which are synthetic variables obeying a crucial exchangeability property with the explanatory variables under study. This paper introduces techniques for knockoff generation in great generality: we provide a sequential characterization of all possible knockoff distributions, which leads to a Metropolis-Hastingsformulation of an exact knockoff sampler. We further show how to use conditional independence structure to speed up computations. Combining these two threads, we introduce an explicit set of sequential algorithms and empirically demonstrate their effectiveness. Our theoretical analysis proves that our algorithms achieve near-optimal computational complexity in certain cases. The techniques we develop are sufficiently rich to enable knockoff sampling in challenging models including cases where the covariates are continuous and heavy-tailed, and follow a graphical model such as the Ising model.},
	urldate = {2019-07-01},
	journal = {arXiv:1903.00434 [stat]},
	author = {Bates, Stephen and Candès, Emmanuel and Janson, Lucas and Wang, Wenshuo},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.00434},
	keywords = {Statistics - Methodology},
}

@article{jordon_knockoffgan_2019,
	title = {{KnockoffGAN}: {Generating} {Knockoffs} for {Feature} {Selection} using {Generative} {Adversarial} {Networks}},
	abstract = {Feature selection is a pervasive problem. The discovery of relevant features can be as important for performing a particular task (such as to avoid overﬁtting in prediction) as it can be for understanding the underlying processes governing the true label (such as discovering relevant genetic factors for a disease). Machine learning driven feature selection can enable discovery from large, high-dimensional, nonlinear observational datasets by creating a subset of features for experts to focus on. In order to use expert time most efﬁciently, we need a principled methodology capable of controlling the False Discovery Rate. In this work, we build on the promising Knockoff framework by developing a ﬂexible knockoff generation model. We adapt the Generative Adversarial Networks framework to allow us to generate knockoffs with no assumptions on the feature distribution. Our model consists of 4 networks, a generator, a discriminator, a stability network and a power network. We demonstrate the capability of our model to perform feature selection, showing that it performs as well as the originally proposed knockoff generation model in the Gaussian setting and that it outperforms the original model in non-Gaussian settings, including on a real-world dataset.},
	language = {en},
	journal = {International Conference on Learning Representations},
	author = {Jordon, James and Yoon, Jinsung},
	year = {2019},
	pages = {25},
}

@article{gimenez_discovering_2019,
	title = {Discovering {Conditionally} {Salient} {Features} with {Statistical} {Guarantees}},
	url = {http://arxiv.org/abs/1905.12177},
	abstract = {The goal of feature selection is to identify important features that are relevant to explain an outcome variable. Most of the work in this domain has focused on identifying globally relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more fine-grained statistical problem: conditional feature selection, where a feature may be relevant depending on the values of the other features. For example in genetic association studies, variant \$A\$ could be associated with the phenotype in the entire dataset, but conditioned on variant \$B\$ being present it might be independent of the phenotype. In this sense, variant \$A\$ is globally relevant, but conditioned on \$B\$ it is no longer locally relevant in that region of the feature space. We present a generalization of the knockoff procedure that performs conditional feature selection while controlling a generalization of the false discovery rate (FDR) to the conditional setting. By exploiting the feature/response model-free framework of the knockoffs, the quality of the statistical FDR guarantee is not degraded even when we perform conditional feature selections. We implement this method and present an algorithm that automatically partitions the feature space such that it enhances the differences between selected sets in different regions, and validate the statistical theoretical results with experiments.},
	urldate = {2019-06-06},
	journal = {Proceedings ICML 2019},
	author = {Gimenez, Jaime Roquero and Zou, James},
	month = may,
	year = {2019},
	note = {arXiv: 1905.12177},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gimenez_improving_2018,
	title = {Improving the {Stability} of the {Knockoff} {Procedure}: {Multiple} {Simultaneous} {Knockoffs} and {Entropy} {Maximization}},
	shorttitle = {Improving the {Stability} of the {Knockoff} {Procedure}},
	url = {http://arxiv.org/abs/1810.11378},
	abstract = {The Model-X knockoff procedure has recently emerged as a powerful approach for feature selection with statistical guarantees. The advantage of knockoff is that if we have a good model of the features X, then we can identify salient features without knowing anything about how the outcome Y depends on X. An important drawback of knockoffs is its instability: running the procedure twice can result in very different selected features, potentially leading to different conclusions. Addressing this instability is critical for obtaining reproducible and robust results. Here we present a generalization of the knockoff procedure that we call simultaneous multi-knockoffs. We show that multi-knockoff guarantees false discovery rate (FDR) control, and is substantially more stable and powerful compared to the standard (single) knockoff. Moreover we propose a new algorithm based on entropy maximization for generating Gaussian multi-knockoffs. We validate the improved stability and power of multi-knockoffs in systematic experiments. We also illustrate how multi-knockoffs can improve the accuracy of detecting genetic mutations that are causally linked to phenotypes.},
	language = {en},
	urldate = {2018-10-31},
	journal = {arXiv:1810.11378 [cs, stat]},
	author = {Gimenez, Jaime Roquero and Zou, James},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.11378},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{lu_deeppink_2018,
	title = {{DeepPINK}: reproducible feature selection in deep neural networks},
	shorttitle = {{DeepPINK}},
	url = {http://arxiv.org/abs/1809.01185},
	abstract = {Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.},
	urldate = {2019-01-08},
	journal = {arXiv:1809.01185 [cs, stat]},
	author = {Lu, Yang Young and Fan, Yingying and Lv, Jinchi and Noble, William Stafford},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.01185},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{romano_deep_2018,
	title = {Deep {Knockoffs}},
	url = {http://arxiv.org/abs/1811.06687},
	abstract = {This paper introduces a machine for sampling approximate model-X knockoffs for arbitrary and unspecified data distributions using deep generative models. The main idea is to iteratively refine a knockoff sampling mechanism until a criterion measuring the validity of the produced knockoffs is optimized; this criterion is inspired by the popular maximum mean discrepancy in machine learning and can be thought of as measuring the distance to pairwise exchangeability between original and knockoff features. By building upon the existing model-X framework, we thus obtain a flexible and model-free statistical tool to perform controlled variable selection. Extensive numerical experiments and quantitative tests confirm the generality, effectiveness, and power of our deep knockoff machines. Finally, we apply this new method to a real study of mutations linked to changes in drug resistance in the human immunodeficiency virus.},
	urldate = {2019-01-08},
	journal = {arXiv:1811.06687 [math, stat]},
	author = {Romano, Yaniv and Sesia, Matteo and Candès, Emmanuel J.},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.06687},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Applications},
}
@book{casella_2002_statistical,
  title={Statistical inference},
  author={Casella, George and Berger, Roger L},
  volume={2},
  year={2002},
  publisher={Duxbury Pacific Grove, CA}
}

@article{rabinovich_optimal_2020,
	title = {Optimal {Rates} and {Tradeoffs} in {Multiple} {Testing}},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/ss_newpaper/SS-2017-0468_na.pdf},
	doi = {10.5705/ss.202017.0468},
	urldate = {2019-05-23},
	journal = {Statistica Sinica},
	author = {Rabinovich, Maxim and Ramdas, Aaditya and Jordan, Michael I. and Wainwright, Martin J.},
	year = {2020},
}

@article{meinshausen_p-values_2009,
author = {Nicolai Meinshausen and Lukas Meier and Peter Bühlmann},
title = {p-Values for High-Dimensional Regression},
journal = {Journal of the American Statistical Association},
volume = {104},
number = {488},
pages = {1671-1681},
year  = {2009},
publisher = {Taylor & Francis},
doi = {10.1198/jasa.2009.tm08647},
URL = {https://doi.org/10.1198/jasa.2009.tm08647},
eprint = {https://doi.org/10.1198/jasa.2009.tm08647}
}



@article{arias-castro_distribution_2017,
author = "Arias-Castro, Ery and Chen, Shiyun",
doi = "10.1214/17-EJS1277",
fjournal = "Electronic Journal of Statistics",
journal = "Electron. J. Statist.",
number = "1",
pages = "1983--2001",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = "Distribution-free multiple testing",
url = "https://doi.org/10.1214/17-EJS1277",
volume = "11",
year = "2017"
}


@book{karatzas_brownian_1996,
	address = {New York},
	edition = {2nd ed},
	series = {Graduate texts in mathematics},
	title = {Brownian motion and stochastic calculus},
	isbn = {978-0-387-97655-6 978-3-540-97655-4},
	number = {113},
	publisher = {Springer},
	author = {Karatzas, Ioannis and Shreve, Steven E.},
	year = {1996},
	keywords = {Brownian motion processes, Stochastic analysis},
	annote = {"Corrected third printing"--T.p. verso "Springer study edition"--Back cover},
	file = {Karatzas and Shreve - 1996 - Brownian motion and stochastic calculus.pdf:/home/bnguyen/Zotero/storage/Q398TU7P/Karatzas and Shreve - 1996 - Brownian motion and stochastic calculus.pdf:application/pdf}
}

@book{le_gall_brownian_2016,
	address = {Cham},
	series = {Graduate texts in mathematics},
	title = {Brownian motion, martingales, and stochastic calculus},
	isbn = {978-3-319-31089-3 978-3-319-31088-6},
	abstract = {This book offers a rigorous and self-contained presentation of stochastic integration and stochastic calculus within the general framework of continuous semimartingales. The main tools of stochastic calculus, including Itô’s formula, the optional stopping theorem and Girsanov’s theorem, are treated in detail alongside many illustrative examples. The book also contains an introduction to Markov processes, with applications to solutions of stochastic differential equations and to connections between Brownian motion and partial differential equations. The theory of local times of semimartingales is discussed in the last chapter. Since its invention by Itô, stochastic calculus has proven to be one of the most important techniques of modern probability theory, and has been used in the most recent theoretical advances as well as in applications to other fields such as mathematical finance. Brownian Motion, Martingales, and Stochastic Calculus provides a strong theoretical background to the reader interested in such developments. Beginning graduate or advanced undergraduate students will benefit from this detailed approach to an essential area of probability theory. The emphasis is on concise and efficient presentation, without any concession to mathematical rigor. The material has been taught by the author for several years in graduate courses at two of the most prestigious French universities. The fact that proofs are given with full details makes the book particularly suitable for self-study. The numerous exercises help the reader to get acquainted with the tools of stochastic calculus},
	language = {eng},
	number = {274},
	publisher = {Springer},
	author = {Le Gall, Jean-François},
	year = {2016},
	note = {OCLC: 950458631},
	annote = {Gaussian variables and Gaussian processes -- Brownian motion -- Filtrations and martingales -- Continuous semimartingales -- Stochastic integration -- General theory of Markov processes -- Brownian motion and partial differential equations -- Stochastic differential equations -- Local times -- The monotone class lemma -- Discrete martingales -- References},
	annote = {Literaturangaben},
	file = {Le Gall - 2016 - Brownian motion, martingales, and stochastic calcu.pdf:/home/bnguyen/Zotero/storage/2T5J9UZA/Le Gall - 2016 - Brownian motion, martingales, and stochastic calcu.pdf:application/pdf}
}


@article{pedregosa_scikit-learn_2011,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}


@article{marcus_open_2007,
  title={Open Access Series of Imaging Studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults},
  author={Marcus, Daniel S and Wang, Tracy H and Parker, Jamie and Csernansky, John G and Morris, John C and Buckner, Randy L},
  journal={Journal of cognitive neuroscience},
  volume={19},
  number={9},
  pages={1498--1507},
  year={2007},
  publisher={MIT Press}
}

@article{haxby_distributed_2001,
  title={Distributed and overlapping representations of faces and objects in ventral temporal cortex},
  author={Haxby, James V and Gobbini, M Ida and Furey, Maura L and Ishai, Alumit and Schouten, Jennifer L and Pietrini, Pietro},
  journal={Science},
  volume={293},
  number={5539},
  pages={2425--2430},
  year={2001},
  publisher={American Association for the Advancement of Science}
}


@article{rhee_fessel_2005, title={HIV‐1 Protease and Reverse‐Transcriptase Mutations: Correlations with Antiretroviral Therapy in Subtype B Isolates and Implications for Drug‐Resistance Surveillance}, volume={192}, DOI={10.1086/431601}, number={3}, journal={The Journal of Infectious Diseases}, author={Rhee, Soo‐Yon and Fessel, W. Jeffrey and Zolopa, Andrew R. and Hurley, Leo and Liu, Tommy and Taylor, Jonathan and Nguyen, Dong Phuong and Slome, Sally and Klein, Daniel and Horberg, Michael and et al.}, year={2005}, pages={456–465}} 

@article{friedman_pathwise_2007,
	title = {Pathwise coordinate optimization},
	volume = {1},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0708.1485},
	doi = {10.1214/07-AOAS131},
	abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the \$L\_1\$-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
	language = {en},
	number = {2},
	urldate = {2018-07-25},
	journal = {The Annals of Applied Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
	month = dec,
	year = {2007},
	note = {arXiv: 0708.1485},
	keywords = {Statistics - Computation, Mathematics - Optimization and Control},
	pages = {302--332},
	file = {Friedman et al. - 2007 - Pathwise coordinate optimization.pdf:/home/bnguyen/Zotero/storage/RBY4AMC3/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf:application/pdf}
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {https://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2018-08-22},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	pages = {289--300},
	file = {Benjamini and Hochberg - 1995 - Controlling the False Discovery Rate A Practical .pdf:/home/bnguyen/Zotero/storage/3C54YDB3/Benjamini and Hochberg - 1995 - Controlling the False Discovery Rate A Practical .pdf:application/pdf}
}

@article{ledoit_well-conditioned_2004,
	title = {A well-conditioned estimator for large-dimensional covariance matrices},
	volume = {88},
	issn = {0047259X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0047259X03000964},
	doi = {10.1016/S0047-259X(03)00096-4},
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to inﬁnity together. Extensive Monte Carlo conﬁrm that the asymptotic results tend to hold well in ﬁnite sample.},
	language = {en},
	number = {2},
	urldate = {2018-08-21},
	journal = {Journal of Multivariate Analysis},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = feb,
	year = {2004},
	pages = {365--411},
	file = {Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:/home/bnguyen/Zotero/storage/MCW9TBPS/Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {en},
	year = {1996},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	file = {Regression Shrinkage and Selection via the Lasso A.pdf:/home/bnguyen/Zotero/storage/7JHXE7HQ/Regression Shrinkage and Selection via the Lasso A.pdf:application/pdf}
}

@article{dezeure_high-dimensional_2015,
	title = {High-{Dimensional} {Inference}: {Confidence} {Intervals}, \$p\$-{Values} and {R}-{Software} hdi},
	volume = {30},
	issn = {0883-4237, 2168-8745},
	shorttitle = {High-{Dimensional} {Inference}},
	url = {https://projecteuclid.org/euclid.ss/1449670857},
	doi = {10.1214/15-STS527},
	abstract = {We present a (selective) review of recent frequentist high-dimensional inference methods for constructing ppp-values and confidence intervals in linear and generalized linear models. We include a broad, comparative empirical study which complements the viewpoint from statistical methodology and theory. Furthermore, we introduce and illustrate the R-package hdi which easily allows the use of different methods and supports reproducibility.},
	language = {EN},
	number = {4},
	urldate = {2018-08-16},
	journal = {Statistical Science},
	author = {Dezeure, Ruben and Bühlmann, Peter and Meier, Lukas and Meinshausen, Nicolai},
	month = nov,
	year = {2015},
	mrnumber = {MR3432840},
	keywords = {\$p\$-value, Clustering, confidence interval, generalized linear model, high-dimensional statistical inference, linear model, multiple testing, R-software},
	pages = {533--558},
	file = {Dezeure et al. - 2015 - High-Dimensional Inference Confidence Intervals, .pdf:/home/bnguyen/Zotero/storage/HAUPWVMG/Dezeure et al. - 2015 - High-Dimensional Inference Confidence Intervals, .pdf:application/pdf;Snapshot:/home/bnguyen/Zotero/storage/WY344VN7/1449670857.html:text/html}
}

@article{da_mota_randomized_2014,
	title = {Randomized parcellation based inference},
	volume = {89},
	issn = {1095-9572},
	doi = {10.1016/j.neuroimage.2013.11.012},
	abstract = {Neuroimaging group analyses are used to relate inter-subject signal differences observed in brain imaging with behavioral or genetic variables and to assess risks factors of brain diseases. The lack of stability and of sensitivity of current voxel-based analysis schemes may however lead to non-reproducible results. We introduce a new approach to overcome the limitations of standard methods, in which active voxels are detected according to a consensus on several random parcellations of the brain images, while a permutation test controls the false positive risk. Both on synthetic and real data, this approach shows higher sensitivity, better accuracy and higher reproducibility than state-of-the-art methods. In a neuroimaging-genetic application, we find that it succeeds in detecting a significant association between a genetic variant next to the COMT gene and the BOLD signal in the left thalamus for a functional Magnetic Resonance Imaging contrast associated with incorrect responses of the subjects from a Stop Signal Task protocol.},
	language = {eng},
	journal = {NeuroImage},
	author = {Da Mota, Benoit and Fritsch, Virgile and Varoquaux, Gaël and Banaschewski, Tobias and Barker, Gareth J. and Bokde, Arun L. W. and Bromberg, Uli and Conrod, Patricia and Gallinat, Jürgen and Garavan, Hugh and Martinot, Jean-Luc and Nees, Frauke and Paus, Tomas and Pausova, Zdenka and Rietschel, Marcella and Smolka, Michael N. and Ströhle, Andreas and Frouin, Vincent and Poline, Jean-Baptiste and Thirion, Bertrand and {IMAGEN consortium}},
	month = apr,
	year = {2014},
	pmid = {24262376},
	keywords = {Brain, Catechol O-Methyltransferase, Cluster Analysis, Computer Simulation, Genetic Association Studies, Group analysis, Humans, Magnetic Resonance Imaging, Multiple comparisons, Neuroimaging, Parcellation, Permutations, Polymorphism, Single Nucleotide, Reproducibility},
	pages = {203--215},
	file = {Da Mota et al. - 2014 - Randomized parcellation based inference.pdf:/home/bnguyen/Zotero/storage/DVXBY35U/Da Mota et al. - 2014 - Randomized parcellation based inference.pdf:application/pdf}
}

@article{taylor_statistical_2015,
	title = {Statistical learning and selective inference},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/112/25/7629},
	doi = {10.1073/pnas.1507583112},
	abstract = {We describe the problem of “selective inference.” This addresses the following challenge: Having mined a set of data to find potential associations, how do we properly assess the strength of these associations? The fact that we have “cherry-picked”—searched for the strongest associations—means that we must set a higher bar for declaring significant the associations that we see. This challenge becomes more important in the era of big data and complex statistical modeling. The cherry tree (dataset) can be very large and the tools for cherry picking (statistical learning methods) are now very sophisticated. We describe some recent new developments in selective inference and illustrate their use in forward stepwise regression, the lasso, and principal components analysis.},
	language = {en},
	number = {25},
	urldate = {2018-08-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Taylor, Jonathan and Tibshirani, Robert J.},
	month = jun,
	year = {2015},
	pmid = {26100887},
	keywords = {inference, lasso, P values},
	pages = {7629--7634},
	file = {Full Text PDF:/home/bnguyen/Zotero/storage/FC5LY3Q4/Taylor and Tibshirani - 2015 - Statistical learning and selective inference.pdf:application/pdf;Snapshot:/home/bnguyen/Zotero/storage/REZ8STUF/7629.html:text/html}
}


@InProceedings{eickenberg_total_2015,
  Title                    = {{Total Variation meets Sparsity: statistical learning with segmenting penalties}},
  Author                   = {Eickenberg, Michael and Dohmatob, Elvis and Thirion, Bertrand and Varoquaux, Ga{\"e}l},
  Booktitle                = {{Medical Image Computing and Computer Assisted Intervention}},
  Year                     = {2015},
  Month                    = Jul,

  File                     = {paper.pdf:https\://hal.inria.fr/hal-01170619/file/paper.pdf:PDF},
  Hal_id                   = {hal-01170619},
  Hal_version              = {v1},
  Keywords                 = {Regularization ; Fmri ; Segmentation ; Machine Learning ; Convex Optimization},
  Url                      = {https://hal.inria.fr/hal-01170619}
}


@article{su_false_2015,
	title = {False {Discoveries} {Occur} {Early} on the {Lasso} {Path}},
	url = {http://arxiv.org/abs/1511.01957},
	abstract = {In regression settings where explanatory variables have very low correlations and there are relatively few effects, each of large magnitude, we expect the Lasso to find the important variables with few errors, if any. This paper shows that in a regime of linear sparsity---meaning that the fraction of variables with a non-vanishing effect tends to a constant, however small---this cannot really be the case, even when the design variables are stochastically independent. We demonstrate that true features and null features are always interspersed on the Lasso path, and that this phenomenon occurs no matter how strong the effect sizes are. We derive a sharp asymptotic trade-off between false and true positive rates or, equivalently, between measures of type I and type II errors along the Lasso path. This trade-off states that if we ever want to achieve a type II error (false negative rate) under a critical value, then anywhere on the Lasso path the type I error (false positive rate) will need to exceed a given threshold so that we can never have both errors at a low level at the same time. Our analysis uses tools from approximate message passing (AMP) theory as well as novel elements to deal with a possibly adaptive selection of the Lasso regularizing parameter.},
	urldate = {2018-02-17},
	journal = {arXiv:1511.01957 [cs, math, stat]},
	author = {Su, Weijie and Bogdan, Malgorzata and Candes, Emmanuel},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01957},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv\:1511.01957 PDF:/home/bnguyen/Zotero/storage/IWRNWHR4/Su et al. - 2015 - False Discoveries Occur Early on the Lasso Path.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/JE4T4EUB/1511.html:text/html}
}

@article{candes_panning_2018,
author = {Cand\`es, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
title = {Panning for gold: ‘model-X’ knockoffs for high dimensional controlled variable selection},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {80},
number = {3},
pages = {551-577},
keywords = {False discovery rate, Generalized linear models, Genomewide association study, Knockoff filter, Logistic regression, Markov blanket, Testing for conditional independence in non-linear models},
doi = {10.1111/rssb.12265},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12265},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12265},
abstract = {Summary   Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a non-linear fashion, such as when the response is binary. Although this modelling problem has been extensively studied, it remains unclear how to control the fraction of false discoveries effectively even in high dimensional logistic regression, not to mention general high dimensional non-linear models. To address such a practical problem, we propose a new framework of ‘model-X’ knockoffs, which reads from a different perspective the knockoff procedure that was originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with n⩾p, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires that the covariates are random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown or estimated distributions. To our knowledge, no other procedure solves the controlled variable selection problem in such generality but, in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case–control study of Crohn's disease in the UK, making twice as many discoveries as the original analysis of the same data.},
year = {2018}
}

@article{barber_controlling_2015,
	title = {Controlling the false discovery rate via knockoffs},
	volume = {43},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1404.5609},
	doi = {10.1214/15-AOS1337},
	abstract = {In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) - the expected fraction of false discoveries among all discoveries - is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. This paper introduces the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, or the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. As the name suggests, the method operates by manufacturing knockoff variables that are cheap - their construction does not require any new data - and are designed to mimic the correlation structure found within the existing variables, in a way that allows for accurate FDR control, beyond what is possible with permutation-based methods. The method of knockoffs is very general and flexible, and can work with a broad class of test statistics. We test the method in combination with statistics from the Lasso for sparse regression, and obtain empirical results showing that the resulting method has far more power than existing selection rules when the proportion of null variables is high.},
	number = {5},
	urldate = {2018-02-19},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Cand\`es, Emmanuel J.},
	month = oct,
	year = {2015},
	note = {arXiv: 1404.5609},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {2055--2085},
	file = {arXiv\:1404.5609 PDF:/home/bnguyen/Zotero/storage/3NCKUHN3/Barber and Candès - 2015 - Controlling the false discovery rate via knockoffs.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/2IT6BDK9/1404.html:text/html}
}

@article{barber_knockoff_2016,
	title = {A knockoff filter for high-dimensional selective inference},
	url = {http://arxiv.org/abs/1602.03574},
	abstract = {This paper develops a framework for testing for associations in a possibly high-dimensional linear model where the number of features/variables may far exceed the number of observational units. In this framework, the observations are split into two groups, where the first group is used to screen for a set of potentially relevant variables, whereas the second is used for inference over this reduced set of variables; we also develop strategies for leveraging information from the first part of the data at the inference step for greater power. In our work, the inferential step is carried out by applying the recently introduced knockoff filter, which creates a knockoff copy-a fake variable serving as a control-for each screened variable. We prove that this procedure controls the directional false discovery rate (FDR) in the reduced model controlling for all screened variables; this says that our high-dimensional knockoff procedure 'discovers' important variables as well as the directions (signs) of their effects, in such a way that the expected proportion of wrongly chosen signs is below the user-specified level (thereby controlling a notion of Type S error averaged over the selected set). This result is non-asymptotic, and holds for any distribution of the original features and any values of the unknown regression coefficients, so that inference is not calibrated under hypothesized values of the effect sizes. We demonstrate the performance of our general and flexible approach through numerical studies. Finally, we apply our method to a genome-wide association study to find locations on the genome that are possibly associated with a continuous phenotype.},
	urldate = {2018-02-19},
	journal = {arXiv:1602.03574 [math, stat]},
	author = {Barber, Rina Foygel and Candes, Emmanuel J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.03574},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	file = {arXiv\:1602.03574 PDF:/home/bnguyen/Zotero/storage/3W76QJ75/Barber and Candes - 2016 - A knockoff filter for high-dimensional selective i.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/JP2GHBGP/1602.html:text/html}
}

@article{sesia_gene_2017,
	title = {Gene {Hunting} with {Knockoffs} for {Hidden} {Markov} {Models}},
	url = {http://arxiv.org/abs/1706.04677},
	abstract = {Modern scientific studies often require the identification of a subset of relevant explanatory variables, in the attempt to understand an interesting phenomenon. Several statistical methods have been developed to automate this task, but only recently has the framework of model-free knockoffs proposed a general solution that can perform variable selection under rigorous type-I error control, without relying on strong modeling assumptions. In this paper, we extend the methodology of model-free knockoffs to a rich family of problems where the distribution of the covariates can be described by a hidden Markov model (HMM). We develop an exact and efficient algorithm to sample knockoff copies of an HMM. We then argue that combined with the knockoffs selective framework, they provide a natural and powerful tool for performing principled inference in genome-wide association studies with guaranteed FDR control. Finally, we apply our methodology to several datasets aimed at studying the Crohn's disease and several continuous phenotypes, e.g. levels of cholesterol.},
	urldate = {2018-02-19},
	journal = {arXiv:1706.04677 [math, stat]},
	author = {Sesia, Matteo and Sabatti, Chiara and Candès, Emmanuel J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04677},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, Statistics - Methodology},
	file = {arXiv\:1706.04677 PDF:/home/bnguyen/Zotero/storage/FA66YXAA/Sesia et al. - 2017 - Gene Hunting with Knockoffs for Hidden Markov Mode.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/XUGRQ7F4/1706.html:text/html}
}

@article{barber_robust_2018,
	title = {Robust inference with knockoffs},
	url = {http://arxiv.org/abs/1801.03896},
	abstract = {We consider the variable selection problem, which seeks to identify important variables influencing a response \$Y\$ out of many candidate features \$X\_1, {\textbackslash}ldots, X\_p\$. We wish to do so while offering finite-sample guarantees about the fraction of false positives - selected variables \$X\_j\$ that in fact have no effect on \$Y\$ after the other features are known. When the number of features \$p\$ is large (perhaps even larger than the sample size \$n\$), and we have no prior knowledge regarding the type of dependence between \$Y\$ and \$X\$, the model-X knockoffs framework nonetheless allows us to select a model with a guaranteed bound on the false discovery rate, as long as the distribution of the feature vector \$X=(X\_1,{\textbackslash}dots,X\_p)\$ is exactly known. This model selection procedure operates by constructing "knockoff copies'" of each of the \$p\$ features, which are then used as a control group to ensure that the model selection algorithm is not choosing too many irrelevant features. In this work, we study the practical setting where the distribution of \$X\$ could only be estimated, rather than known exactly, and the knockoff copies of the \$X\_j\$'s are therefore constructed somewhat incorrectly. Our results, which are free of any modeling assumption whatsoever, show that the resulting model selection procedure incurs an inflation of the false discovery rate that is proportional to our errors in estimating the distribution of each feature \$X\_j\$ conditional on the remaining features \${\textbackslash}\{X\_k:k{\textbackslash}neq j{\textbackslash}\}\$. The model-X knockoff framework is therefore robust to errors in the underlying assumptions on the distribution of \$X\$, making it an effective method for many practical applications, such as genome-wide association studies, where the underlying distribution on the features \$X\_1,{\textbackslash}dots,X\_p\$ is estimated accurately but not known exactly.},
	urldate = {2018-02-19},
	journal = {arXiv:1801.03896 [stat]},
	author = {Barber, Rina Foygel and Candès, Emmanuel J. and Samworth, Richard J.},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.03896},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1801.03896 PDF:/home/bnguyen/Zotero/storage/T8R9YLDW/Barber et al. - 2018 - Robust inference with knockoffs.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/E8JQMLXJ/1801.html:text/html}
}

@article{weinstein_power_2017,
	title = {A {Power} and {Prediction} {Analysis} for {Knockoffs} with {Lasso} {Statistics}},
	url = {http://arxiv.org/abs/1712.06465},
	abstract = {Knockoffs is a new framework for controlling the false discovery rate (FDR) in multiple hypothesis testing problems involving complex statistical models. While there has been great emphasis on Type-I error control, Type-II errors have been far less studied. In this paper we analyze the false negative rate or, equivalently, the power of a knockoff procedure associated with the Lasso solution path under an i.i.d. Gaussian design, and find that knockoffs asymptotically achieve close to optimal power with respect to an omniscient oracle. Furthermore, we demonstrate that for sparse signals, performing model selection via knockoff filtering achieves nearly ideal prediction errors as compared to a Lasso oracle equipped with full knowledge of the distribution of the unknown regression coefficients. The i.i.d. Gaussian design is adopted to leverage results concerning the empirical distribution of the Lasso estimates, which makes power calculation possible for both knockoff and oracle procedures.},
	urldate = {2018-02-19},
	journal = {arXiv:1712.06465 [stat]},
	author = {Weinstein, Asaf and Barber, Rina and Candes, Emmanuel},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06465},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1712.06465 PDF:/home/bnguyen/Zotero/storage/BA7VI9W3/Weinstein et al. - 2017 - A Power and Prediction Analysis for Knockoffs with.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/844GFX52/1712.html:text/html}
}

@article{katsevich_multilayer_2017,
	title = {Multilayer {Knockoff} {Filter}: {Controlled} variable selection at multiple resolutions},
	shorttitle = {Multilayer {Knockoff} {Filter}},
	url = {http://arxiv.org/abs/1706.09375},
	abstract = {We tackle the problem of selecting from among a large number of variables those that are 'important' for an outcome. We consider situations where groups of variables are also of interest in their own right. For example, each variable might be a genetic polymorphism and we might want to study how a trait depends on variability in genes, segments of DNA that typically contain multiple such polymorphisms. Or, variables might quantify various aspects of the functioning of individual internet servers owned by a company, and we might be interested in assessing the importance of each server as a whole on the average download speed for the company's customers. In this context, to discover that a variable is relevant for the outcome implies discovering that the larger entity it represents is also important. To guarantee meaningful and reproducible results, we suggest controlling the rate of false discoveries for findings at the level of individual variables and at the level of groups. Building on the knockoff construction of Barber and Candes (2015) and the multilayer testing framework of Barber and Ramdas (2016), we introduce the multilayer knockoff filter (MKF). We prove that MKF simultaneously controls the FDR at each resolution and use simulations to show that it incurs little power loss compared to methods that provide guarantees only for the discoveries of individual variables. We apply MKF to analyze a genetic dataset and find that it successfully reduces the number of false gene discoveries without a significant reduction in power.},
	urldate = {2018-02-19},
	journal = {arXiv:1706.09375 [stat]},
	author = {Katsevich, Eugene and Sabatti, Chiara},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09375},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1706.09375 PDF:/home/bnguyen/Zotero/storage/FYKFBJNV/Katsevich and Sabatti - 2017 - Multilayer Knockoff Filter Controlled variable se.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/IFXC72LZ/1706.html:text/html}
}

@article{hoyos-idrobo_frem_2017,
	title = {{FReM} – scalable and stable decoding with fast regularized ensemble of models},
	url = {https://hal.archives-ouvertes.fr/hal-01615015},
	doi = {10.1016/j.neuroimage.2017.10.005},
	abstract = {Brain decoding relates behavior to brain activity through predictive models. These are also used to identify brain regions involved in the cognitive operations related to the observed behavior. Training such multivariate models is a high-dimensional statistical problem that calls for suitable priors. State of the art priors –eg small total-variation– enforce spatial structure on the maps to stabilize them and improve prediction. However, they come with a hefty computational cost. We build upon very fast dimension reduction with spatial structure and model ensembling to achieve decoders that are fast on large datasets and increase the stability of the predictions and the maps. Our approach, fast regularized ensemble of models (FReM), includes an implicit spatial regularization by using a voxel grouping with a fast clustering algorithm. In addition, it aggregates different estimators obtained across splits of a cross-validation loop, each time keeping the best possible model. Experiments on a large number of brain imaging datasets show that our combination of voxel clustering and model ensembling improves decoding maps stability and reduces the variance of prediction accuracy. Importantly, our method requires less samples than state-of-the-art methods to achieve a given level of prediction accuracy. Finally, FreM is highly parallelizable, and has lower computation cost than other spatially-regularized methods.},
	urldate = {2018-04-09},
	journal = {NeuroImage},
	author = {Hoyos-Idrobo, Andrés A and Varoquaux, Gaël and Schwartz, Yannick and Thirion, Bertrand},
	year = {2017},
	keywords = {bagging, decoding, fMRI, MVPA, supervised learning},
	pages = {1--16},
	file = {HAL PDF Full Text:/home/bnguyen/Zotero/storage/U9IZVG87/Hoyos-Idrobo et al. - 2017 - FReM – scalable and stable decoding with fast regu.pdf:application/pdf}
}

@article{su_communication-efficient_2015,
	title = {Communication-{Efficient} {False} {Discovery} {Rate} {Control} via {Knockoff} {Aggregation}},
	url = {http://arxiv.org/abs/1506.05446},
	abstract = {The false discovery rate (FDR)---the expected fraction of spurious discoveries among all the discoveries---provides a popular statistical assessment of the reproducibility of scientific studies in various disciplines. In this work, we introduce a new method for controlling the FDR in meta-analysis of many decentralized linear models. Our method targets the scenario where many research groups---possibly the number of which is random---are independently testing a common set of hypotheses and then sending summary statistics to a coordinating center in an online manner. Built on the knockoffs framework introduced by Barber and Candes (2015), our procedure starts by applying the knockoff filter to each linear model and then aggregates the summary statistics via one-shot communication in a novel way. This method gives exact FDR control non-asymptotically without any knowledge of the noise variances or making any assumption about sparsity of the signal. In certain settings, it has a communication complexity that is optimal up to a logarithmic factor.},
	urldate = {2018-06-14},
	journal = {arXiv:1506.05446 [stat]},
	author = {Su, Weijie and Qian, Junyang and Liu, Linxi},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.05446},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1506.05446 PDF:/home/bnguyen/Zotero/storage/QT9B754I/Su et al. - 2015 - Communication-Efficient False Discovery Rate Contr.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/CWKYH8Y2/1506.html:text/html}
}


@article{javanmard_false_2018,
	title = {False {Discovery} {Rate} {Control} via {Debiased} {Lasso}},
	url = {http://arxiv.org/abs/1803.04464},
	abstract = {We consider the problem of variable selection in high-dimensional statistical models where the goal is to report a set of variables, out of many predictors \$X\_1, {\textbackslash}dotsc, X\_p\$, that are relevant to a response of interest. For linear high-dimensional model, where the number of parameters exceeds the number of samples \$(p{\textgreater}n)\$, we propose a procedure for variables selection and prove that it controls the {\textbackslash}emph\{directional\} false discovery rate (FDR) below a pre-assigned significance level \$q{\textbackslash}in [0,1]\$. We further analyze the statistical power of our framework and show that for designs with subgaussian rows and a common precision matrix \${\textbackslash}Omega{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{p{\textbackslash}times p\}\$, if the minimum nonzero parameter \${\textbackslash}theta\_\{{\textbackslash}min\}\$ satisfies \$\${\textbackslash}sqrt\{n\} {\textbackslash}theta\_\{{\textbackslash}min\} - {\textbackslash}sigma {\textbackslash}sqrt\{2({\textbackslash}max\_\{i{\textbackslash}in [p]\}{\textbackslash}Omega\_\{ii\}){\textbackslash}log{\textbackslash}left({\textbackslash}frac\{2p\}\{qs\_0\}{\textbackslash}right)\} {\textbackslash}to {\textbackslash}infty{\textbackslash},,\$\$ then this procedure achieves asymptotic power one. Our framework is built upon the debiasing approach and assumes the standard condition \$s\_0 = o({\textbackslash}sqrt\{n\}/({\textbackslash}log p){\textasciicircum}2)\$, where \$s\_0\$ indicates the number of true positives among the \$p\$ features. Notably, this framework achieves exact directional FDR control without any assumption on the amplitude of unknown regression parameters, and does not require any knowledge of the distribution of covariates or the noise level. We test our method in synthetic and real data experiments to asses its performance and to corroborate our theoretical results.},
	urldate = {2018-03-28},
	journal = {arXiv:1803.04464 [cs, math, stat]},
	author = {Javanmard, Adel and Javadi, Hamid},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.04464},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology, Computer Science - Learning},
	file = {arXiv\:1803.04464 PDF:/home/bnguyen/Zotero/storage/R3EW9FYA/Javanmard and Javadi - 2018 - False Discovery Rate Control via Debiased Lasso.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/TYRK79WP/1803.html:text/html}
}

@article{javanmard_confidence_2014,
  title={Confidence intervals and hypothesis testing for high-dimensional regression},
  author={Javanmard, Adel and Montanari, Andrea},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2869--2909},
  year={2014},
  publisher={JMLR. org}
}

@article{van_de_geer_asymptotically_2014,
	title = {On asymptotically optimal confidence regions and tests for high-dimensional models},
	volume = {42},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1303.0518},
	doi = {10.1214/14-AOS1221},
	abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.},
	number = {3},
	urldate = {2018-04-03},
	journal = {The Annals of Statistics},
	author = {van de Geer, Sara and Bühlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
	month = jun,
	year = {2014},
	note = {arXiv: 1303.0518},
	keywords = {Mathematics - Statistics Theory},
	pages = {1166--1202},
	file = {arXiv\:1303.0518 PDF:/home/bnguyen/Zotero/storage/6CXC9WYP/van de Geer et al. - 2014 - On asymptotically optimal confidence regions and t.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/5674V86U/1303.html:text/html}
}

@article{zhang_confidence_2011,
	title = {Confidence {Intervals} for {Low}-{Dimensional} {Parameters} in {High}-{Dimensional} {Linear} {Models}},
	url = {http://arxiv.org/abs/1110.2563},
	abstract = {The purpose of this paper is to propose methodologies for statistical inference of low-dimensional parameters with high-dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broad context. The theoretical results presented here provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite-dimensional covariance matrices. These sufficient conditions allow the number of variables to far exceed the sample size. The simulation results presented here demonstrate the accuracy of the coverage probability of the proposed confidence intervals, strongly supporting the theoretical results.},
	urldate = {2018-04-03},
	journal = {arXiv:1110.2563 [stat]},
	author = {Zhang, Cun-Hui and Zhang, Stephanie S.},
	month = oct,
	year = {2011},
	note = {arXiv: 1110.2563},
	keywords = {Statistics - Methodology},
	file = {1110.2563.pdf:/home/bnguyen/Zotero/storage/Q3JTYWUG/1110.2563.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/JPR6XZC5/1110.html:text/html;Zhang and Zhang - 2011 - Confidence Intervals for Low-Dimensional Parameter.pdf:/home/bnguyen/Zotero/storage/EVEG26GA/Zhang and Zhang - 2011 - Confidence Intervals for Low-Dimensional Parameter.pdf:application/pdf}
}

@book{buhlmann_statistics_2011,
	address = {Heidelberg ; New York},
	series = {Springer series in statistics},
	title = {Statistics for high-dimensional data: methods, theory and applications},
	isbn = {978-3-642-20191-2},
	shorttitle = {Statistics for high-dimensional data},
	abstract = {Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and versions of it for various models, boosting methods, undirected graphical modeling, and procedures controlling false positive selections.
A special characteristic of the book is that it contains comprehensive mathematical theory on high-dimensional statistics combined with methodology, algorithms and illustrations with real data examples. This in-depth approach highlights the methods’ great potential and practical applicability in a variety of settings. As such, it is a valuable resource for researchers, graduate students and experts in statistics, applied mathematics and computer science.},
	publisher = {Springer},
	author = {Bühlmann, Peter and Geer, S. A. van de},
	year = {2011},
	note = {OCLC: ocn729346867},
	keywords = {Least absolute deviations (Statistics), Linear models (Statistics), Mathematical statistics, Nonconvex programming, Smoothness of functions},
	file = {Bühlmann and Geer - 2011 - Statistics for high-dimensional data methods, the.pdf:/home/bnguyen/Zotero/storage/6L2ACGYC/Bühlmann and Geer - 2011 - Statistics for high-dimensional data methods, the.pdf:application/pdf}
}

@article{hoyos-idrobo_frem_2017,
	title = {{FReM} – scalable and stable decoding with fast regularized ensemble of models},
	url = {https://hal.archives-ouvertes.fr/hal-01615015},
	doi = {10.1016/j.neuroimage.2017.10.005},
	abstract = {Brain decoding relates behavior to brain activity through predictive models. These are also used to identify brain regions involved in the cognitive operations related to the observed behavior. Training such multivariate models is a high-dimensional statistical problem that calls for suitable priors. State of the art priors –eg small total-variation– enforce spatial structure on the maps to stabilize them and improve prediction. However, they come with a hefty computational cost. We build upon very fast dimension reduction with spatial structure and model ensembling to achieve decoders that are fast on large datasets and increase the stability of the predictions and the maps. Our approach, fast regularized ensemble of models (FReM), includes an implicit spatial regularization by using a voxel grouping with a fast clustering algorithm. In addition, it aggregates different estimators obtained across splits of a cross-validation loop, each time keeping the best possible model. Experiments on a large number of brain imaging datasets show that our combination of voxel clustering and model ensembling improves decoding maps stability and reduces the variance of prediction accuracy. Importantly, our method requires less samples than state-of-the-art methods to achieve a given level of prediction accuracy. Finally, FreM is highly parallelizable, and has lower computation cost than other spatially-regularized methods.},
	urldate = {2018-04-09},
	journal = {NeuroImage},
	author = {Hoyos-Idrobo, Andrés A and Varoquaux, Gaël and Schwartz, Yannick and Thirion, Bertrand},
	year = {2017},
	keywords = {bagging, decoding, fMRI, MVPA, supervised learning},
	pages = {1--16},
	file = {HAL PDF Full Text:/home/bnguyen/Zotero/storage/U9IZVG87/Hoyos-Idrobo et al. - 2017 - FReM – scalable and stable decoding with fast regu.pdf:application/pdf}
}

@article{wasserman_high-dimensional_2009,
	title = {High-dimensional variable selection},
	volume = {37},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1247663752},
	doi = {10.1214/08-AOS646},
	abstract = {This paper explores the following question: what kind of statistical guarantees can be given when doing variable selection in high-dimensional models? In particular, we look at the error rates and power of some multi-stage regression methods. In the first stage we fit a set of candidate models. In the second stage we select one model by cross-validation. In the third stage we use hypothesis testing to eliminate some variables. We refer to the first two stages as “screening” and the last stage as “cleaning.” We consider three screening methods: the lasso, marginal regression, and forward stepwise regression. Our method gives consistent variable selection under certain conditions.},
	language = {EN},
	number = {5A},
	urldate = {2018-04-10},
	journal = {The Annals of Statistics},
	author = {Wasserman, Larry and Roeder, Kathryn},
	month = oct,
	year = {2009},
	mrnumber = {MR2543689},
	zmnumber = {1173.62054},
	keywords = {Lasso, sparsity, stepwise regression},
	pages = {2178--2201},
	file = {Full Text PDF:/home/bnguyen/Zotero/storage/YNLPGFJ2/Wasserman and Roeder - 2009 - High-dimensional variable selection.pdf:application/pdf;Snapshot:/home/bnguyen/Zotero/storage/TYUQCA3N/1247663752.html:text/html}
}

@book{poldrack_handbook_2011,
	address = {Cambridge New York Melbourne Madrid},
	title = {Handbook of functional {MRI} data analysis},
	isbn = {978-0-521-51766-9},
	abstract = {"Functional magnetic resonance imaging (fMRI) has become the most popular method for imaging brain function. Handbook of Functional MRI Data Analysis provides a comprehensive and practical introduction to the methods used for fMRI data analysis. Using minimal jargon, this book explains the concepts behind processing fMRI data, focusing on the techniques that are most commonly used in the field. This book provides background about the methods employed by common data analysis packages including FSL, SPM, and AFNI. Some of the newest cutting-edge techniques, including pattern classification analysis, connectivity modeling, and resting state network analysis, are also discussed. Readers of this book, whether newcomers to the field or experienced researchers, will obtain a deep and effective knowledge of how to employ fMRI analysis to ask scientific questions and become more sophisticated users of fMRI analysis software"--Provided by publisher},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Poldrack, Russell A. and Mumford, Jeanette A. and Nichols, Thomas E.},
	year = {2011},
	note = {OCLC: 753167009},
	file = {Poldrack et al. - 2011 - Handbook of functional MRI data analysis.pdf:/home/bnguyen/Zotero/storage/2PCFTQBH/Poldrack et al. - 2011 - Handbook of functional MRI data analysis.pdf:application/pdf}
}

@article{haynes_primer_2015,
	title = {A {Primer} on {Pattern}-{Based} {Approaches} to {fMRI}: {Principles}, {Pitfalls}, and {Perspectives}},
	volume = {87},
	issn = {08966273},
	shorttitle = {A {Primer} on {Pattern}-{Based} {Approaches} to {fMRI}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627315004328},
	doi = {10.1016/j.neuron.2015.05.025},
	abstract = {Human fMRI signals exhibit a spatial patterning that contains detailed information about a person’s mental states. Using classifiers it is possible to access this information and study brain processes at the level of individual mental representations. The precise link between fMRI signals and neural population signals still needs to be unraveled. Also, the interpretation of classification studies needs to be handled with care. Nonetheless, pattern-based analyses make it possible to investigate human representational spaces in unprecedented ways, especially when combined with computational modeling.},
	language = {en},
	number = {2},
	urldate = {2018-04-11},
	journal = {Neuron},
	author = {Haynes, John-Dylan},
	month = jul,
	year = {2015},
	pages = {257--270},
	file = {Haynes - 2015 - A Primer on Pattern-Based Approaches to fMRI Prin.pdf:/home/bnguyen/Zotero/storage/WNI2P829/Haynes - 2015 - A Primer on Pattern-Based Approaches to fMRI Prin.pdf:application/pdf}
}

@article{dettling_supervised_2002,
	title = {Supervised clustering of genes},
	volume = {3},
	issn = {1465-6906},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC151171/},
	abstract = {We focus on microarray data where experiments monitor gene expression in different tissues and where each experiment is equipped with an additional response variable such as a cancer type. A new method is presented for finding groups of genes by directly incorporating the response variables into the grouping process, yielding a supervised clustering algorithm for genes.},
	number = {12},
	urldate = {2018-04-11},
	journal = {Genome Biology},
	author = {Dettling, Marcel and Bühlmann, Peter},
	year = {2002},
	pmid = {12537558},
	pmcid = {PMC151171},
	pages = {research0069.1--research0069.15},
	file = {PubMed Central Full Text PDF:/home/bnguyen/Zotero/storage/CE4GE256/Dettling and Bühlmann - 2002 - Supervised clustering of genes.pdf:application/pdf}
}

@article{buhlmann_statistical_2013,
	title = {Statistical significance in high-dimensional linear models},
	volume = {19},
	issn = {1350-7265},
	url = {http://arxiv.org/abs/1202.1377},
	doi = {10.3150/12-BEJSP11},
	abstract = {We propose a method for constructing p-values for general hypotheses in a high-dimensional linear model. The hypotheses can be local for testing a single regression parameter or they may be more global involving several up to all parameters. Furthermore, when considering many hypotheses, we show how to adjust for multiple testing taking dependence among the p-values into account. Our technique is based on Ridge estimation with an additional correction term due to a substantial projection bias in high dimensions. We prove strong error control for our p-values and provide sufficient conditions for detection: for the former, we do not make any assumption on the size of the true underlying regression coefficients while regarding the latter, our procedure might not be optimal in terms of power. We demonstrate the method in simulated examples and a real data application.},
	number = {4},
	urldate = {2018-04-11},
	journal = {Bernoulli},
	author = {Bühlmann, Peter},
	month = sep,
	year = {2013},
	note = {arXiv: 1202.1377},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {1212--1242},
	file = {arXiv\:1202.1377 PDF:/home/bnguyen/Zotero/storage/HNC9HLJB/Bühlmann - 2013 - Statistical significance in high-dimensional linea.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/QC3STCF8/1202.html:text/html}
}

@article{descloux_model_2018,
	title = {Model selection with lasso-zero: adding straw to the haystack to better find needles},
	shorttitle = {Model selection with lasso-zero},
	url = {http://arxiv.org/abs/1805.05133},
	abstract = {The high-dimensional linear model y = Xβ0 + is considered and the focus is put on the problem of recovering the support S0 of the sparse vector β0. We introduce lasso-zero, a new 1-based estimator whose novelty resides in an “overﬁt, then threshold” paradigm and the use of noise dictionaries for overﬁtting the response. The methodology is supported by theoretical results obtained in the special case where no noise dictionary is used. In this case, lasso-zero boils down to thresholding the basis pursuit solution. We prove that this procedure requires weaker conditions on X and S0 than the lasso for exact support recovery, and controls the false discovery rate for orthonormal designs when tuned by the quantile universal threshold [26]. However it requires a high signal-to-noise ratio, and the use of noise dictionaries addresses this issue. The threshold selection procedure is based on a pivotal statistic and does not require knowledge of the noise level. Numerical simulations show that lasso-zero performs well in terms of support recovery and provides a good trade-oﬀ between high true positive rate and low false discovery rate compared to competitors.},
	language = {en},
	urldate = {2018-05-21},
	journal = {arXiv:1805.05133 [stat]},
	author = {Descloux, Pascaline and Sardy, Sylvain},
	month = may,
	year = {2018},
	note = {arXiv: 1805.05133},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {Descloux and Sardy - 2018 - Model selection with lasso-zero adding straw to t.pdf:/home/bnguyen/Zotero/storage/RPBPAJXQ/Descloux and Sardy - 2018 - Model selection with lasso-zero adding straw to t.pdf:application/pdf}
}

@article{michel_supervised_2012,
	title = {A supervised clustering approach for {fMRI}-based inference of brain states},
	volume = {45},
	issn = {00313203},
	url = {http://arxiv.org/abs/1104.5304},
	doi = {10.1016/j.patcog.2011.04.006},
	abstract = {We propose a method that combines signals from many brain regions observed in functional Magnetic Resonance Imaging (fMRI) to predict the subject's behavior during a scanning session. Such predictions suffer from the huge number of brain regions sampled on the voxel grid of standard fMRI data sets: the curse of dimensionality. Dimensionality reduction is thus needed, but it is often performed using a univariate feature selection procedure, that handles neither the spatial structure of the images, nor the multivariate nature of the signal. By introducing a hierarchical clustering of the brain volume that incorporates connectivity constraints, we reduce the span of the possible spatial configurations to a single tree of nested regions tailored to the signal. We then prune the tree in a supervised setting, hence the name supervised clustering, in order to extract a parcellation (division of the volume) such that parcel-based signal averages best predict the target information. Dimensionality reduction is thus achieved by feature agglomeration, and the constructed features now provide a multi-scale representation of the signal. Comparisons with reference methods on both simulated and real data show that our approach yields higher prediction accuracy than standard voxel-based approaches. Moreover, the method infers an explicit weighting of the regions involved in the regression or classification task.},
	number = {6},
	urldate = {2018-06-08},
	journal = {Pattern Recognition},
	author = {Michel, Vincent and Gramfort, Alexandre and Varoquaux, Gaël and Eger, Evelyn and Keribin, Christine and Thirion, Bertrand},
	month = jun,
	year = {2012},
	note = {arXiv: 1104.5304},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {2041--2049},
	file = {arXiv\:1104.5304 PDF:/home/bnguyen/Zotero/storage/5UG8XPNL/Michel et al. - 2012 - A supervised clustering approach for fMRI-based in.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/QIXTZAI2/1104.html:text/html}
}

@article{su_communication-efficient_2015,
	title = {Communication-{Efficient} {False} {Discovery} {Rate} {Control} via {Knockoff} {Aggregation}},
	url = {http://arxiv.org/abs/1506.05446},
	abstract = {The false discovery rate (FDR)---the expected fraction of spurious discoveries among all the discoveries---provides a popular statistical assessment of the reproducibility of scientific studies in various disciplines. In this work, we introduce a new method for controlling the FDR in meta-analysis of many decentralized linear models. Our method targets the scenario where many research groups---possibly the number of which is random---are independently testing a common set of hypotheses and then sending summary statistics to a coordinating center in an online manner. Built on the knockoffs framework introduced by Barber and Candes (2015), our procedure starts by applying the knockoff filter to each linear model and then aggregates the summary statistics via one-shot communication in a novel way. This method gives exact FDR control non-asymptotically without any knowledge of the noise variances or making any assumption about sparsity of the signal. In certain settings, it has a communication complexity that is optimal up to a logarithmic factor.},
	urldate = {2018-06-14},
	journal = {arXiv:1506.05446 [stat]},
	author = {Su, Weijie and Qian, Junyang and Liu, Linxi},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.05446},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1506.05446 PDF:/home/bnguyen/Zotero/storage/QT9B754I/Su et al. - 2015 - Communication-Efficient False Discovery Rate Contr.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/CWKYH8Y2/1506.html:text/html}
}

@article{bayati_lasso_2010,
	title = {The {LASSO} risk for gaussian matrices},
	url = {http://arxiv.org/abs/1008.2581},
	abstract = {We consider the problem of learning a coefficient vector x\_0{\textbackslash}in R{\textasciicircum}N from noisy linear observation y=Ax\_0+w {\textbackslash}in R{\textasciicircum}n. In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator x'. In this case, a popular approach consists in solving an L1-penalized least squares problem known as the LASSO or Basis Pursuit DeNoising (BPDN). For sequences of matrices A of increasing dimensions, with independent gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical models ideas. Simulations on real data matrices suggest that our results can be relevant in a broad array of practical applications.},
	urldate = {2018-06-21},
	journal = {arXiv:1008.2581 [cs, math, stat]},
	author = {Bayati, Mohsen and Montanari, Andrea},
	month = aug,
	year = {2010},
	note = {arXiv: 1008.2581},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
	file = {arXiv\:1008.2581 PDF:/home/bnguyen/Zotero/storage/3ATBEURY/Bayati and Montanari - 2010 - The LASSO risk for gaussian matrices.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/TMU797ZR/1008.html:text/html}
}

@article{massias_generalized_2017,
	title = {Generalized {Concomitant} {Multi}-{Task} {Lasso} for sparse multimodal regression},
	url = {http://arxiv.org/abs/1705.09778},
	abstract = {In high dimension, it is customary to consider Lasso-type estimators to enforce sparsity. For standard Lasso theory to hold, the regularization parameter should be proportional to the noise level, yet the latter is generally unknown in practice. A possible remedy is to consider estimators, such as the Concomitant/Scaled Lasso, which jointly optimize over the regression coefficients as well as over the noise level, making the choice of the regularization independent of the noise level. However, when data from different sources are pooled to increase sample size, or when dealing with multimodal datasets, noise levels typically differ and new dedicated estimators are needed. In this work we provide new statistical and computational solutions to deal with such heteroscedastic regression models, with an emphasis on functional brain imaging with combined magneto- and electroencephalographic (M/EEG) signals. Adopting the formulation of Concomitant Lasso-type estimators, we propose a jointly convex formulation to estimate both the regression coefficients and the (square root of the) noise covariance. When our framework is instantiated to de-correlated noise, it leads to an efficient algorithm whose computational cost is not higher than for the Lasso and Concomitant Lasso, while addressing more complex noise structures. Numerical experiments demonstrate that our estimator yields improved prediction and support identification while correctly estimating the noise (square root) covariance. Results on multimodal neuroimaging problems with M/EEG data are also reported.},
	urldate = {2018-06-25},
	journal = {arXiv:1705.09778 [math, stat]},
	author = {Massias, Mathurin and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09778},
	keywords = {Statistics - Machine Learning, Statistics - Applications, Mathematics - Optimization and Control},
	file = {arXiv\:1705.09778 PDF:/home/bnguyen/Zotero/storage/5GEFD924/Massias et al. - 2017 - Generalized Concomitant Multi-Task Lasso for spars.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/A8EQIWSQ/1705.html:text/html}
}

@article{friedman_sparse_2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxm045},
	doi = {10.1093/biostatistics/kxm045},
	abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm— the Graphical Lasso— that is remarkably fast: it solves a 1000 node problem (∼ 500, 000 parameters) in at most a minute, and is 30 to 4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen \& Bu¨hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
	language = {en},
	number = {3},
	urldate = {2018-06-27},
	journal = {Biostatistics},
	author = {Friedman, J. and Hastie, T. and Tibshirani, R.},
	month = jul,
	year = {2008},
	pages = {432--441},
	file = {Friedman et al. - 2008 - Sparse inverse covariance estimation with the grap.pdf:/home/bnguyen/Zotero/storage/DE6X7ATN/Friedman et al. - 2008 - Sparse inverse covariance estimation with the grap.pdf:application/pdf}
}

@article{meinshausen_p-values_2008,
	title = {P-values for high-dimensional regression},
	url = {http://arxiv.org/abs/0811.2177},
	abstract = {Assigning significance in high-dimensional regression is challenging. Most computationally efficient selection algorithms cannot guard against inclusion of noise variables. Asymptotically valid p-values are not available. An exception is a recent proposal by Wasserman and Roeder (2008) which splits the data into two parts. The number of variables is then reduced to a manageable size using the first split, while classical variable selection techniques can be applied to the remaining variables, using the data from the second split. This yields asymptotic error control under minimal conditions. It involves, however, a one-time random split of the data. Results are sensitive to this arbitrary choice: it amounts to a `p-value lottery' and makes it difficult to reproduce results. Here, we show that inference across multiple random splits can be aggregated, while keeping asymptotic control over the inclusion of noise variables. We show that the resulting p-values can be used for control of both family-wise error (FWER) and false discovery rate (FDR). In addition, the proposed aggregation is shown to improve power while reducing the number of falsely selected variables substantially.},
	urldate = {2018-06-29},
	journal = {arXiv:0811.2177 [stat]},
	author = {Meinshausen, Nicolai and Meier, Lukas and Bühlmann, Peter},
	month = nov,
	year = {2008},
	note = {arXiv: 0811.2177},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:0811.2177 PDF:/home/bnguyen/Zotero/storage/PVYRKQSG/Meinshausen et al. - 2008 - P-values for high-dimensional regression.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/SC25XCF9/0811.html:text/html}
}

@article{chevalier_statistical_2018,
	title = {Statistical {Inference} with {Ensemble} of {Clustered} {Desparsified} {Lasso}},
	url = {http://arxiv.org/abs/1806.05829},
	abstract = {Medical imaging involves high-dimensional data, yet their acquisition is obtained for limited samples. Multivariate predictive models have become popular in the last decades to fit some external variables from imaging data, and standard algorithms yield point estimates of the model parameters. It is however challenging to attribute confidence to these parameter estimates, which makes solutions hardly trustworthy. In this paper we present a new algorithm that assesses parameters statistical significance and that can scale even when the number of predictors p \${\textbackslash}ge\$ 10{\textasciicircum}5 is much higher than the number of samples n \${\textbackslash}le\$ 10{\textasciicircum}3 , by lever-aging structure among features. Our algorithm combines three main ingredients: a powerful inference procedure for linear models --the so-called Desparsified Lasso-- feature clustering and an ensembling step. We first establish that Desparsified Lasso alone cannot handle n p regimes; then we demonstrate that the combination of clustering and ensembling provides an accurate solution, whose specificity is controlled. We also demonstrate stability improvements on two neuroimaging datasets.},
	urldate = {2018-07-16},
	journal = {arXiv:1806.05829 [stat]},
	author = {Chevalier, Jérôme-Alexis and Salmon, Joseph and Thirion, Bertrand},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.05829},
	keywords = {Statistics - Applications},
	file = {arXiv\:1806.05829 PDF:/home/bnguyen/Zotero/storage/5ARKQEVB/Chevalier et al. - 2018 - Statistical Inference with Ensemble of Clustered D.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/96VTFWGW/1806.html:text/html}
}

@article{koltchinskii_concentration_2014,
	title = {Concentration {Inequalities} and {Moment} {Bounds} for {Sample} {Covariance} {Operators}},
	url = {http://arxiv.org/abs/1405.2468},
	abstract = {Let \$X,X\_1,{\textbackslash}dots, X\_n,{\textbackslash}dots\$ be i.i.d. centered Gaussian random variables in a separable Banach space \$E\$ with covariance operator \${\textbackslash}Sigma:\$ \$\$ {\textbackslash}Sigma:E{\textasciicircum}\{{\textbackslash}ast\}{\textbackslash}mapsto E,{\textbackslash} {\textbackslash} {\textbackslash}Sigma u = \{{\textbackslash}mathbb E\}{\textbackslash}langle X,u{\textbackslash}rangle, u{\textbackslash}in E{\textasciicircum}\{{\textbackslash}ast\}. \$\$ The sample covariance operator \${\textbackslash}hat {\textbackslash}Sigma:E{\textasciicircum}\{{\textbackslash}ast\}{\textbackslash}mapsto E\$ is defined as \$\$ {\textbackslash}hat {\textbackslash}Sigma u := n{\textasciicircum}\{-1\}{\textbackslash}sum\_\{j=1\}{\textasciicircum}n {\textbackslash}langle X\_j,u{\textbackslash}rangle X\_j, u{\textbackslash}in E{\textasciicircum}\{{\textbackslash}ast\}. \$\$ The goal of the paper is to obtain concentration inequalities and expectation bounds for the operator norm \${\textbackslash}{\textbar}{\textbackslash}hat {\textbackslash}Sigma-{\textbackslash}Sigma{\textbackslash}{\textbar}\$ of the deviation of the sample covariance operator from the true covariance operator. In particular, it is shown that \$\$ \{{\textbackslash}mathbb E\}{\textbackslash}{\textbar}{\textbackslash}hat {\textbackslash}Sigma-{\textbackslash}Sigma{\textbackslash}{\textbar}{\textbackslash}asymp {\textbackslash}{\textbar}{\textbackslash}Sigma{\textbackslash}{\textbar}{\textbackslash}biggl({\textbackslash}sqrt\{{\textbackslash}frac\{\{{\textbackslash}bf r\}({\textbackslash}Sigma)\}\{n\}\}{\textbackslash}bigvee {\textbackslash}frac\{\{{\textbackslash}bf r\}({\textbackslash}Sigma)\}\{n\}{\textbackslash}biggr), \$\$ where \$\$ \{{\textbackslash}bf r\}({\textbackslash}Sigma):={\textbackslash}frac\{{\textbackslash}Bigl(\{{\textbackslash}mathbb E\}{\textbackslash}{\textbar}X{\textbackslash}{\textbar}{\textbackslash}Bigr){\textasciicircum}2\}\{{\textbackslash}{\textbar}{\textbackslash}Sigma{\textbackslash}{\textbar}\}. \$\$ Moreover, under the assumption that \$\{{\textbackslash}bf r\}({\textbackslash}Sigma){\textbackslash}lesssim n,\$ it is proved that, for all \$t{\textbackslash}geq 1,\$ with probability at least \$1-e{\textasciicircum}\{-t\}\$ {\textbackslash}begin\{align*\} {\textbackslash}Bigl{\textbar}{\textbackslash}{\textbar}{\textbackslash}hat{\textbackslash}Sigma - {\textbackslash}Sigma{\textbackslash}{\textbar}-\{{\textbackslash}mathbb E\}{\textbackslash}{\textbar}{\textbackslash}hat{\textbackslash}Sigma - {\textbackslash}Sigma{\textbackslash}{\textbar}{\textbackslash}Bigr{\textbar} {\textbackslash}lesssim {\textbackslash}{\textbar}{\textbackslash}Sigma{\textbackslash}{\textbar}{\textbackslash}biggl({\textbackslash}sqrt\{{\textbackslash}frac\{t\}\{n\}\}{\textbackslash}bigvee {\textbackslash}frac\{t\}\{n\}{\textbackslash}biggr). {\textbackslash}end\{align*\}},
	urldate = {2018-07-16},
	journal = {arXiv:1405.2468 [math]},
	author = {Koltchinskii, Vladimir and Lounici, Karim},
	month = may,
	year = {2014},
	note = {arXiv: 1405.2468},
	keywords = {Mathematics - Probability},
	file = {arXiv\:1405.2468 PDF:/home/bnguyen/Zotero/storage/V87VUT8T/Koltchinskii and Lounici - 2014 - Concentration Inequalities and Moment Bounds for S.pdf:application/pdf;arXiv.org Snapshot:/home/bnguyen/Zotero/storage/3QDQ3THB/1405.html:text/html}
}

@article{banerjee_model_nodate,
	title = {Model {Selection} {Through} {Sparse} {Maximum} {Likelihood} {Estimation} for {Multivariate} {Gaussian} or {Binary} {Data}},
	abstract = {We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1-norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.},
	language = {en},
	author = {Banerjee, Onureena and Ghaoui, Laurent El and d’Aspremont, Alexandre},
	pages = {32},
	file = {Banerjee et al. - Model Selection Through Sparse Maximum Likelihood .pdf:/home/bnguyen/Zotero/storage/XFER99CG/Banerjee et al. - Model Selection Through Sparse Maximum Likelihood .pdf:application/pdf}
}

@book {Bou_Lug_Mas:2011:livre,
    AUTHOR = {Boucheron, St\'ephane and Lugosi, G{\'a}bor and Massart, Pascal},
     TITLE = {Concentration Inequalities: A Nonasymptotic Theory of Independence},
 PUBLISHER = {Oxford University Press},
   ADDRESS = {Oxford},
      YEAR = {2013},
     PAGES = {x+481},
      ISBN = {978-0-19-953525-5},
}

@TechReport{Rom_DiC:2019,
	AUTHOR = {Romano, Joseph P. and DiCiccio, Cyrus},
	TITLE = {Multiple Data Splitting for Testing},
	YEAR = {2019},
	MONTH = apr,
	INSTITUTION = {Stanford University, Department of Statistics},
	NOTE = {available at https://statistics.stanford.edu/research/multiple-data-splitting-testing},
	NUMBER = {Technical Report 2019-03}
}

@ARTICLE{Abd:2018,
       author = {{Abdesselam}, Abdelmalek},
        title = "{The weakly dependent strong law of large numbers revisited}",
      journal = {arXiv e-prints},
     keywords = {Mathematics - Probability, Mathematics - Classical Analysis and ODEs, 60F15},
         year = 2018,
        month = jan,
          eid = {arXiv:1801.09265},
        pages = {arXiv:1801.09265},
archivePrefix = {arXiv},
       eprint = {1801.09265},
 primaryClass = {math.PR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180109265A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article {Han_Ste:2017,
    AUTHOR = {Hang, Hanyuan and Steinwart, Ingo},
     TITLE = {A {B}ernstein-type inequality for some mixing processes and
              dynamical systems with an application to learning},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {45},
      YEAR = {2017},
    NUMBER = {2},
     PAGES = {708--743},
      ISSN = {0090-5364},
   MRCLASS = {60E15 (37D20 60F10 60G10 62G08 62M10 68T05)},
  MRNUMBER = {3650398},
MRREVIEWER = {Jean-Ren\'{e} Chazottes},
       DOI = {10.1214/16-AOS1465},
       URL = {https://doi.org/10.1214/16-AOS1465},
}

@incollection {Mer_Pel_Rio:2009,
    AUTHOR = {Merlev\`ede, Florence and Peligrad, Magda and Rio, Emmanuel},
     TITLE = {Bernstein inequality and moderate deviations under strong
              mixing conditions},
 BOOKTITLE = {High dimensional probability {V}: the {L}uminy volume},
    SERIES = {Inst. Math. Stat. (IMS) Collect.},
    VOLUME = {5},
     PAGES = {273--292},
 PUBLISHER = {Inst. Math. Statist., Beachwood, OH},
      YEAR = {2009},
   MRCLASS = {60E15 (60F10)},
  MRNUMBER = {2797953},
MRREVIEWER = {M. Cs\"{o}rg\H{o}},
       DOI = {10.1214/09-IMSCOLL518},
       URL = {https://doi.org/10.1214/09-IMSCOLL518},
}

@Article{ Sam:2000,
	AUTHOR = {Paul-Marie Samson},
	TITLE = {{Concentration of measure inequalities for {M}arkov chains and {$\Phi$}-mixing processes}},
	JOURNAL = {Ann. Probab.},
	FJOURNAL = {The Annals of Probability},
	VOLUME = {28},
	YEAR = {2000},
	NUMBER = {1},
	PAGES = {416--461},
	ISSN = {0091-1798},
	CODEN = {APBYAE},
	MRCLASS = {60E15 (60J05)},
	MRNUMBER = {MR1756011 (2001d:60015)}
}


