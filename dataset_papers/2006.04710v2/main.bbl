\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., et~al.
\newblock Tensor{F}low: Large-scale machine learning on heterogeneous
  distributed systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Anil et~al.(2019)Anil, Lucas, and Grosse]{anil2019sorting}
Anil, C., Lucas, J., and Grosse, R.
\newblock Sorting out {L}ipschitz function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  291--301, 2019.

\bibitem[Bapna et~al.(2018)Bapna, Chen, Firat, Cao, and Wu]{bapna2018training}
Bapna, A., Chen, M.~X., Firat, O., Cao, Y., and Wu, Y.
\newblock Training deeper neural machine translation models with transparent
  attention.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  3028--3033, 2018.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{behrmann2018invertible}
Behrmann, J., Grathwohl, W., Chen, R. T.~Q., Duvenaud, D., and Jacobsen, J.-H.
\newblock Invertible residual networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6571--6583, 2018.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and
  Jacobsen]{chen2019residual}
Chen, R. T.~Q., Behrmann, J., Duvenaud, D., and Jacobsen, J.-H.
\newblock Residual flows for invertible generative modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and
  Usunier]{cisse2017parseval}
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and Usunier, N.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  854--863, 2017.

\bibitem[Corless et~al.(1996)Corless, Gonnet, Hare, Jeffrey, and
  Knuth]{corless1996lambertw}
Corless, R.~M., Gonnet, G.~H., Hare, D.~E., Jeffrey, D.~J., and Knuth, D.~E.
\newblock On the {L}ambert {W} function.
\newblock \emph{Advances in Computational mathematics}, 5\penalty0
  (1):\penalty0 329--359, 1996.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics}, pp.\  4171--4186,
  2019.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and
  Pappas]{fazlyab2019efficient}
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G.
\newblock Efficient and accurate estimation of {L}ipschitz constants for deep
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11423--11434, 2019.

\bibitem[Federer(1969)]{federer1969geometric}
Federer, H.
\newblock \emph{Geometric Measure Theory}.
\newblock Classics in Mathematics. Springer Berlin Heidelberg, 1969.
\newblock ISBN 9783642620102.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Betterncourt, Sutskever, and
  Duvenaud]{grathwohl2018ffjord}
Grathwohl, W., Chen, R. T.~Q., Betterncourt, J., Sutskever, I., and Duvenaud,
  D.
\newblock {FFJORD}: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Huang et~al.(2019)Huang, Vaswani, Uszkoreit, Simon, Hawthorne,
  Shazeer, Dai, Hoffman, Dinculescu, and Eck]{huang2018music}
Huang, C.-Z.~A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer,
  N., Dai, A.~M., Hoffman, M.~D., Dinculescu, M., and Eck, D.
\newblock Music {T}ransformer.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations},
  2015.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible $1\times 1$ convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10215--10224, 2018.

\bibitem[Latorre et~al.(2020)Latorre, Rolland, and
  Cevher]{Latorre2020Lipschitz}
Latorre, F., Rolland, P., and Cevher, V.
\newblock {L}ipschitz constant estimation of neural networks via sparse
  polynomial optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
Marcus, M.~P., Marcinkiewicz, M.~A., and Santorini, B.
\newblock Building a large annotated corpus of {E}nglish: {T}he {P}enn
  {T}reebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Mises \& Pollaczek-Geiringer(1929)Mises and
  Pollaczek-Geiringer]{mises1929praktische}
Mises, R. and Pollaczek-Geiringer, H.
\newblock Praktische verfahren der gleichungsaufl{\"o}sung.
\newblock \emph{ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift
  f{\"u}r Angewandte Mathematik und Mechanik}, 9\penalty0 (2):\penalty0
  152--164, 1929.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for {G}enerative {A}dversarial {N}etworks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu, Gulcehre,
  Jayakumar, Jaderberg, Kaufman, Clark, Noury, Botvinick, Heess, and
  Hadsell]{parisotto2019stabilizing}
Parisotto, E., Song, H.~F., Rae, J.~W., Pascanu, R., Gulcehre, C., Jayakumar,
  S.~M., Jaderberg, M., Kaufman, R.~L., Clark, A., Noury, S., Botvinick, M.~M.,
  Heess, N., and Hadsell, R.
\newblock Stabilizing {T}ransformers for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Parmar et~al.(2019)Parmar, Ramachandran, Vaswani, Bello, Levskaya, and
  Shlens]{ramachandran2019stand}
Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens,
  J.
\newblock Stand-alone self-attention in vision models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  68--80, 2019.

\bibitem[Peyr{\'e} \& Cuturi(2019)Peyr{\'e} and Cuturi]{peyre2019computational}
Peyr{\'e}, G. and Cuturi, M.
\newblock Computational optimal transport.
\newblock \emph{Foundations and Trends in Machine Learning}, 11\penalty0
  (5-6):\penalty0 355--607, 2019.

\bibitem[Sokoli{\'c} et~al.(2017)Sokoli{\'c}, Giryes, Sapiro, and
  Rodrigues]{sokolic2017robust}
Sokoli{\'c}, J., Giryes, R., Sapiro, G., and Rodrigues, M.~R.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4265--4280, 2017.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and
  Salakhutdinov]{tsai2019transformer}
Tsai, Y.-H.~H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R.
\newblock Transformer dissection: {A}n unified understanding for
  {T}ransformer{'}s attention via the lens of kernel.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing}, pp.\  4344--4353, 2019.

\bibitem[Tsuzuku et~al.(2018)Tsuzuku, Sato, and Sugiyama]{tsuzuku2018lipschitz}
Tsuzuku, Y., Sato, I., and Sugiyama, M.
\newblock {L}ipschitz-margin training: Scalable certification of perturbation
  invariance for deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6541--6550, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{virmaux2018lipschitz}
Virmaux, A. and Scaman, K.
\newblock {L}ipschitz regularity of deep neural networks: analysis and
  efficient estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3835--3844, 2018.

\bibitem[Vuckovic et~al.(2020)Vuckovic, Baratin, and Tachet~des
  Combes]{vuckovic2020attention}
Vuckovic, J., Baratin, A., and Tachet~des Combes, R.
\newblock A mathematical theory of attention.
\newblock \emph{arXiv preprint arXiv:2007.02876}, 2020.

\bibitem[Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and
  Chao]{wang2019learning}
Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.~F., and Chao, L.~S.
\newblock Learning deep transformer models for machine translation.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  1810--1822, 2019.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018non}
Wang, X., Girshick, R., Gupta, A., and He, K.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  7794--7803, 2018.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan,
  Y., Wang, L., and Liu, T.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10524--10533. PMLR, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2018self}
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.
\newblock Self-attention {G}enerative {A}dversarial {N}etworks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  7354--7363, 2019.

\end{thebibliography}
