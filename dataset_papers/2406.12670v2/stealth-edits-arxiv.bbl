\begin{thebibliography}{10}

\bibitem{llama3modelcard}
{\sc AI@Meta}.
\newblock Llama 3 model card.

\bibitem{ba2016layer}
{\sc Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.}
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450\/} (2016).

\bibitem{brody2023expressivity}
{\sc Brody, S., Alon, U., and Yahav, E.}
\newblock On the expressivity role of {L}ayer{N}orm in transformers' attention.
\newblock {\em arXiv preprint arXiv:2305.02582\/} (2023).

\bibitem{chen2017targeted}
{\sc Chen, X., Liu, C., Li, B., Lu, K., and Song, D.}
\newblock Targeted backdoor attacks on deep learning systems using data poisoning.
\newblock {\em arXiv preprint arXiv:1712.05526\/} (2017).

\bibitem{chen2021badnl}
{\sc Chen, X., Salem, A., Backes, M., Ma, S., and Zhang, Y.}
\newblock Bad{NL}: Backdoor attacks against {NLP} models.
\newblock In {\em ICML 2021 Workshop on Adversarial Machine Learning\/} (2021).

\bibitem{dai2023neural}
{\sc Dai, D., Jiang, W., Dong, Q., Lyu, Y., and Sui, Z.}
\newblock Neural knowledge bank for pretrained transformers.
\newblock In {\em CCF International Conference on Natural Language Processing and Chinese Computing\/} (2023), Springer, pp.~772--783.

\bibitem{dziri2022origin}
{\sc Dziri, N., Milton, S., Yu, M., Zaiane, O., and Reddy, S.}
\newblock On the origin of hallucinations in conversational models: Is it the datasets or the models?
\newblock In {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\/} (Seattle, United States, July 2022), M.~Carpuat, M.-C. de~Marneffe, and I.~V. Meza~Ruiz, Eds., Association for Computational Linguistics, pp.~5271--5285.

\bibitem{elfwing2018silu}
{\sc Elfwing, S., Uchibe, E., and Doya, K.}
\newblock Sigmoid-weighted linear units for neural network function approximation in reinforcement learning.
\newblock {\em Neural Networks 107\/} (2018), 3--11.
\newblock Special issue on deep reinforcement learning.

\bibitem{euaiact}
{\sc {European Parliament}}.
\newblock {EU AI Act: first regulation on artificial intelligence}.
\newblock \url{https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence}.

\bibitem{eval-harness}
{\sc Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le~Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.}
\newblock A framework for few-shot language model evaluation, 07 2024.

\bibitem{geva2020transformer}
{\sc Geva, M., Schuster, R., Berant, J., and Levy, O.}
\newblock Transformer feed-forward layers are key-value memories.
\newblock {\em arXiv preprint arXiv:2012.14913\/} (2020).

\bibitem{PLR:2019}
{\sc Gorban, A.~N., Makarov, V.~A., and Tyukin, I.~Y.}
\newblock The unreasonable effectiveness of small neural ensembles in high-dimensional brain.
\newblock {\em Physics of life reviews 29\/} (2019), 55--88.

\bibitem{gu2023mamba}
{\sc Gu, A., and Dao, T.}
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock {\em arXiv preprint arXiv:2312.00752\/} (2023).

\bibitem{gu2024model}
{\sc Gu, J.-C., Xu, H.-X., Ma, J.-Y., Lu, P., Ling, Z.-H., Chang, K.-W., and Peng, N.}
\newblock Model editing can hurt general abilities of large language models.
\newblock {\em arXiv preprint arXiv:2401.04700\/} (2024).

\bibitem{hartvigsen2024aging}
{\sc Hartvigsen, T., Sankaranarayanan, S., Palangi, H., Kim, Y., and Ghassemi, M.}
\newblock Aging with {GRACE}: Lifelong model editing with discrete key-value adaptors.
\newblock {\em Advances in Neural Information Processing Systems 36\/} (2024).

\bibitem{huang2023training}
{\sc Huang, Y., Zhuo, T.~Y., Xu, Q., Hu, H., Yuan, X., and Chen, C.}
\newblock Training-free lexical backdoor attacks on language models.
\newblock In {\em Proceedings of the ACM Web Conference 2023\/} (New York, NY, USA, 2023), WWW '23, Association for Computing Machinery, p.~2198â€“2208.

\bibitem{huang2023transformerpatcher}
{\sc Huang, Z., Shen, Y., Zhang, X., Zhou, J., Rong, W., and Xiong, Z.}
\newblock {T}ransformer-{P}atcher: One mistake worth one neuron.
\newblock In {\em The Eleventh International Conference on Learning Representations\/} (2023).

\bibitem{ji2023hallucinationsurvey}
{\sc Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.~J., Madotto, A., and Fung, P.}
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Comput. Surv. 55}, 12 (mar 2023).

\bibitem{kalai2024calibrated}
{\sc Kalai, A.~T., and Vempala, S.~S.}
\newblock Calibrated language models must hallucinate.
\newblock {\em arXiv preprint arXiv:2311.14648\/} (2024).

\bibitem{CREATE}
{\sc {King's College London}}.
\newblock {K}ing's {C}omputational {R}esearch, {E}ngineering and {T}echnology {E}nvironment ({CREATE}), 2024.

\bibitem{lewis2020retrieval}
{\sc Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., et~al.}
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock {\em Advances in Neural Information Processing Systems 33\/} (2020), 9459--9474.

\bibitem{li2024badedit}
{\sc Li, Y., Li, T., Chen, K., Zhang, J., Liu, S., Wang, W., Zhang, T., and Liu, Y.}
\newblock {B}ad{E}dit: Backdooring large language models by model editing.
\newblock In {\em The Twelfth International Conference on Learning Representations\/} (2024).

\bibitem{ma2019nlpaug}
{\sc Ma, E.}
\newblock {NLP} augmentation.
\newblock https://github.com/makcedward/nlpaug, 2019.

\bibitem{polo2024tinybenchmarks}
{\sc Maia~Polo, F., Weber, L., Choshen, L., Sun, Y., Xu, G., and Yurochkin, M.}
\newblock tinybenchmarks: evaluating llms with fewer examples.
\newblock {\em arXiv preprint arXiv:2402.14992\/} (2024).

\bibitem{meng2022locating}
{\sc Meng, K., Bau, D., Andonian, A., and Belinkov, Y.}
\newblock Locating and editing factual associations in {GPT}.
\newblock {\em Advances in Neural Information Processing Systems 36\/} (2022).

\bibitem{meng2022memit}
{\sc Meng, K., Sen~Sharma, A., Andonian, A., Belinkov, Y., and Bau, D.}
\newblock Mass editing memory in a transformer.
\newblock {\em The Eleventh International Conference on Learning Representations (ICLR)\/} (2023).

\bibitem{mitchell2021fast}
{\sc Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C.~D.}
\newblock Fast model editing at scale.
\newblock In {\em International Conference on Learning Representations\/} (2022).

\bibitem{Nanda2022Pile10K}
{\sc Nanda, N.}
\newblock {NeelNanda/pile-10k} \textendash\ datasets at hugging face.
\newblock \url{https://huggingface.co/datasets/NeelNanda/pile-10k}, 2022.

\bibitem{openAIcoderunner}
{\sc {OpenAI}}.
\newblock Code interpreter documentation.
\newblock \url{https://platform.openai.com/docs/assistants/tools/code-interpreter}.

\bibitem{ouyang2022training}
{\sc Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.}
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems 35\/} (2022), 27730--27744.

\bibitem{sharir2020cost}
{\sc Sharir, O., Peleg, B., and Shoham, Y.}
\newblock The cost of training {NLP} models: A concise overview.
\newblock {\em arXiv preprint arXiv:2004.08900\/} (2020).

\bibitem{Sutton:2023:relativeIntrinsic}
{\sc Sutton, O.~J., Zhou, Q., Gorban, A.~N., and Tyukin, I.~Y.}
\newblock Relative intrinsic dimensionality is intrinsic to learning.
\newblock In {\em Artificial Neural Networks and Machine Learning -- ICANN 2023\/} (Cham, 2023), L.~Iliadis, A.~Papaleonidas, P.~Angelov, and C.~Jayne, Eds., Springer Nature Switzerland, pp.~516--529.

\bibitem{sutton2023adversarial}
{\sc Sutton, O.~J., Zhou, Q., Tyukin, I.~Y., Gorban, A.~N., Bastounis, A., and Higham, D.~J.}
\newblock How adversarial attacks can disrupt seemingly stable accurate classifiers.
\newblock {\em arXiv preprint arXiv:2309.03665\/} (2023).

\bibitem{templeton2024scaling}
{\sc Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N.~L., McDougall, C., MacDiarmid, M., Freeman, C.~D., Sumers, T.~R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T.}
\newblock Scaling monosemanticity: Extracting interpretable features from {C}laude 3 {S}onnet.
\newblock {\em Transformer Circuits Thread\/} (2024).

\bibitem{awsdatabases}
{\sc Tuteja, N., and Nath, S.}
\newblock Reinventing the data experience: Use generative {AI} and modern data architecture to unlock insights.
\newblock \url{https://shorturl.at/Z5NN9}.

\bibitem{tyukin2021feasibility}
{\sc Tyukin, I.~Y., Higham, D.~J., Bastounis, A., Woldegeorgis, E., and Gorban, A.~N.}
\newblock {The feasibility and inevitability of stealth attacks}.
\newblock {\em IMA Journal of Applied Mathematics\/} (10 2023), hxad027.

\bibitem{tyukin2020adversarial}
{\sc Tyukin, I.~Y., Higham, D.~J., and Gorban, A.~N.}
\newblock On adversarial examples and stealth attacks in artificial intelligence systems.
\newblock In {\em 2020 International Joint Conference on Neural Networks (IJCNN)\/} (2020), IEEE, pp.~1--6.

\bibitem{genassemble}
{\sc {UN News}}.
\newblock {General Assembly adopts landmark resolution on artificial intelligence}.
\newblock \url{https://news.un.org/en/story/2024/03/1147831}.

\bibitem{vaswani2017attention}
{\sc Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.}
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2017), I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, Eds., vol.~30, Curran Associates, Inc.

\bibitem{gpt-j}
{\sc Wang, B., and Komatsuzaki, A.}
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{wang2024mmlupro}
{\sc Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W.}
\newblock Mmlu-pro: A more robust and challenging multi-task language understanding benchmark, 2024.

\bibitem{wikidump}
{\sc {{Wikimedia Foundation}}}.
\newblock Wikimedia downloads.

\bibitem{xu2024hallucination}
{\sc Xu, Z., Jain, S., and Kankanhalli, M.}
\newblock Hallucination is inevitable: An innate limitation of large language models.
\newblock {\em arXiv preprint arXiv:2401.11817\/} (2024).

\bibitem{yang2021careful}
{\sc Yang, W., Li, L., Zhang, Z., Ren, X., Sun, X., and He, B.}
\newblock Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in {NLP} models.
\newblock In {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\/} (Online, June 2021), K.~Toutanova, A.~Rumshisky, L.~Zettlemoyer, D.~Hakkani-Tur, I.~Beltagy, S.~Bethard, R.~Cotterell, T.~Chakraborty, and Y.~Zhou, Eds., Association for Computational Linguistics, pp.~2048--2058.

\bibitem{zhang2019root}
{\sc Zhang, B., and Sennrich, R.}
\newblock Root mean square layer normalization.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2019), H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, Eds., vol.~32, Curran Associates, Inc.

\end{thebibliography}
