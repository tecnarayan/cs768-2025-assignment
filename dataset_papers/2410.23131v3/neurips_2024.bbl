\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and
  Saligrama]{acar2021federated}
Durmus Alp~Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough,
  and Venkatesh Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Avdiukhin and Kasiviswanathan(2021)]{avdiukhin2021federated}
Dmitrii Avdiukhin and Shiva Kasiviswanathan.
\newblock Federated learning under arbitrary communication patterns.
\newblock In \emph{International Conference on Machine Learning}, pages
  425--435. PMLR, 2021.

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone{\v{c}}n{\`y}, Mazzocchi, McMahan,
  et~al.]{bonawitz2019towards}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone{\v{c}}n{\`y}, Stefano
  Mazzocchi, Brendan McMahan, et~al.
\newblock Towards federated learning at scale: System design.
\newblock \emph{Proceedings of machine learning and systems}, 1:\penalty0
  374--388, 2019.

\bibitem[Chen et~al.(2020)Chen, Horvath, and Richtarik]{chen2020optimal}
Wenlin Chen, Samuel Horvath, and Peter Richtarik.
\newblock Optimal client sampling for federated learning.
\newblock \emph{arXiv preprint arXiv:2010.13723}, 2020.

\bibitem[Cheng et~al.(2023)Cheng, Huang, Wu, and Yuan]{cheng2023momentum}
Ziheng Cheng, Xinmeng Huang, Pengfei Wu, and Kun Yuan.
\newblock Momentum benefits non-iid federated learning simply and provably.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Cho et~al.(2020)Cho, Wang, and Joshi]{cho2020client}
Yae~Jee Cho, Jianyu Wang, and Gauri Joshi.
\newblock Client selection in federated learning: Convergence analysis and
  power-of-choice selection strategies.
\newblock \emph{arXiv preprint arXiv:2010.01243}, 2020.

\bibitem[Cho et~al.(2022)Cho, Wang, and Joshi]{cho2022towards}
Yae~Jee Cho, Jianyu Wang, and Gauri Joshi.
\newblock Towards understanding biased client selection in federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 10351--10375. PMLR, 2022.

\bibitem[Cho et~al.(2023)Cho, Sharma, Joshi, Xu, Kale, and
  Zhang]{cho2023convergence}
Yae~Jee Cho, Pranay Sharma, Gauri Joshi, Zheng Xu, Satyen Kale, and Tong Zhang.
\newblock On the convergence of federated averaging with cyclic client
  participation.
\newblock \emph{arXiv preprint arXiv:2302.03109}, 2023.

\bibitem[Ding et~al.(2017)Ding, Kulkarni, and Yekhanin]{ding2017collecting}
Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.
\newblock Collecting telemetry data privately.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Eichner et~al.(2019)Eichner, Koren, McMahan, Srebro, and
  Talwar]{eichner2019semi}
Hubert Eichner, Tomer Koren, Brendan McMahan, Nathan Srebro, and Kunal Talwar.
\newblock Semi-cyclic stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  1764--1773. PMLR, 2019.

\bibitem[Fraboni et~al.(2021)Fraboni, Vidal, Kameni, and
  Lorenzi]{fraboni2021clustered}
Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi.
\newblock Clustered sampling: Low-variance and improved representativity for
  clients selection in federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3407--3416. PMLR, 2021.

\bibitem[Glasgow et~al.(2022)Glasgow, Yuan, and Ma]{glasgow2022sharp}
Margalit~R Glasgow, Honglin Yuan, and Tengyu Ma.
\newblock Sharp bounds for federated averaging (local sgd) and continuous
  perspective.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 9050--9090. PMLR, 2022.

\bibitem[Grudzie{\'n} et~al.(2023)Grudzie{\'n}, Malinovsky, and
  Richt{\'a}rik]{grudzien2023improving}
Micha{\l} Grudzie{\'n}, Grigory Malinovsky, and Peter Richt{\'a}rik.
\newblock Improving accelerated federated learning with compression and
  importance sampling.
\newblock In \emph{Federated Learning and Analytics in Practice: Algorithms,
  Systems, Applications, and Opportunities}, 2023.

\bibitem[Gu et~al.(2021)Gu, Huang, Zhang, and Huang]{gu2021fast}
Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang.
\newblock Fast federated learning in the presence of arbitrary device
  unavailability.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12052--12064, 2021.

\bibitem[Huba et~al.(2022)Huba, Nguyen, Malik, Zhu, Rabbat, Yousefpour, Wu,
  Zhan, Ustinov, Srinivas, et~al.]{huba2022papaya}
Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan
  Yousefpour, Carole-Jean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas,
  et~al.
\newblock Papaya: Practical, private, and scalable federated learning.
\newblock \emph{Proceedings of Machine Learning and Systems}, 4:\penalty0
  814--832, 2022.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2021advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and trends{\textregistered} in machine learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International conference on machine learning}, pages
  5132--5143. PMLR, 2020.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{li2020federated1}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0
  429--450, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock \emph{arXiv preprint arXiv:1907.02189}, 2019.

\bibitem[Lian et~al.(2018)Lian, Zhang, Zhang, and Liu]{lian2018asynchronous}
Xiangru Lian, Wei Zhang, Ce~Zhang, and Ji~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  3043--3052. PMLR, 2018.

\bibitem[Lim et~al.(2020)Lim, Luong, Hoang, Jiao, Liang, Yang, Niyato, and
  Miao]{lim2020federated}
Wei Yang~Bryan Lim, Nguyen~Cong Luong, Dinh~Thai Hoang, Yutao Jiao, Ying-Chang
  Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao.
\newblock Federated learning in mobile edge networks: A comprehensive survey.
\newblock \emph{IEEE Communications Surveys \& Tutorials}, 22\penalty0
  (3):\penalty0 2031--2063, 2020.

\bibitem[Malinovsky et~al.(2023)Malinovsky, Horv{\'a}th, Burlachenko, and
  Richt{\'a}rik]{malinovsky2023federated}
Grigory Malinovsky, Samuel Horv{\'a}th, Konstantin Burlachenko, and Peter
  Richt{\'a}rik.
\newblock Federated learning with regularized client participation.
\newblock \emph{arXiv preprint arXiv:2302.03662}, 2023.

\bibitem[McMahan et~al.(2017{\natexlab{a}})McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017{\natexlab{a}}.

\bibitem[McMahan et~al.(2017{\natexlab{b}})McMahan, Ramage, Talwar, and
  Zhang]{mcmahan2017learning}
H~Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock \emph{arXiv preprint arXiv:1710.06963}, 2017{\natexlab{b}}.

\bibitem[Mothukuri et~al.(2021)Mothukuri, Parizi, Pouriyeh, Huang,
  Dehghantanha, and Srivastava]{mothukuri2021survey}
Viraaji Mothukuri, Reza~M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
  Dehghantanha, and Gautam Srivastava.
\newblock A survey on security and privacy of federated learning.
\newblock \emph{Future Generation Computer Systems}, 115:\penalty0 619--640,
  2021.

\bibitem[Patel et~al.(2022)Patel, Wang, Woodworth, Bullins, and
  Srebro]{patel2022towards}
Kumar~Kshitij Patel, Lingxiao Wang, Blake~E Woodworth, Brian Bullins, and Nati
  Srebro.
\newblock Towards optimal communication complexity in distributed non-convex
  optimization.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 13316--13328. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/56bd21259e28ebdc4d7e1503733bf421-Paper-Conference.pdf}.

\bibitem[Paulik et~al.(2021)Paulik, Seigel, Mason, Telaar, Kluivers, van Dalen,
  Lau, Carlson, Granqvist, Vandevelde, et~al.]{paulik2021federated}
Matthias Paulik, Matt Seigel, Henry Mason, Dominic Telaar, Joris Kluivers,
  Rogier van Dalen, Chi~Wai Lau, Luke Carlson, Filip Granqvist, Chris
  Vandevelde, et~al.
\newblock Federated evaluation and tuning for on-device personalization: System
  design \& applications.
\newblock \emph{arXiv preprint arXiv:2102.08503}, 2021.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush, Konecny,
  Kumar, and McMahan]{reddi2020adaptive}
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Konecny, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock \emph{ICLR}, 2021.

\bibitem[Rizk et~al.(2022)Rizk, Vlaski, and Sayed]{rizk2022federated}
Elsa Rizk, Stefan Vlaski, and Ali~H Sayed.
\newblock Federated learning under importance sampling.
\newblock \emph{IEEE Transactions on Signal Processing}, 70:\penalty0
  5381--5396, 2022.

\bibitem[Ruan et~al.(2021)Ruan, Zhang, Liang, and Joe-Wong]{ruan2021towards}
Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong.
\newblock Towards flexible device participation in federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3403--3411. PMLR, 2021.

\bibitem[Stich(2018)]{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Tyurin et~al.(2022)Tyurin, Sun, Burlachenko, and
  Richt{\'a}rik]{tyurin2022sharper}
Alexander Tyurin, Lukang Sun, Konstantin~Pavlovich Burlachenko, and Peter
  Richt{\'a}rik.
\newblock Sharper rates and flexible framework for nonconvex sgd with client
  and data sampling.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Wang and Joshi(2018)]{wang2018cooperative}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock \emph{arXiv preprint arXiv:1808.07576}, 2018.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H~Brendan McMahan, Maruan
  Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
  et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wang and Ji(2022)]{wang2022unified}
Shiqiang Wang and Mingyue Ji.
\newblock A unified analysis of federated learning with arbitrary client
  participation.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 19124--19137, 2022.

\bibitem[Wang and Ji(2023)]{wang2023lightweight}
Shiqiang Wang and Mingyue Ji.
\newblock A lightweight method for tackling unknown participation probabilities
  in federated averaging.
\newblock \emph{arXiv preprint arXiv:2306.03401}, 2023.

\bibitem[Wei et~al.(2020)Wei, Li, Ding, Ma, Yang, Farokhi, Jin, Quek, and
  Vincent~Poor]{wei2020federated}
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard~H. Yang, Farhad Farokhi, Shi Jin,
  Tony Q.~S. Quek, and H.~Vincent~Poor.
\newblock Federated learning with differential privacy: Algorithms and
  performance analysis.
\newblock \emph{IEEE Transactions on Information Forensics and Security},
  15:\penalty0 3454--3469, 2020.
\newblock \doi{10.1109/TIFS.2020.2988575}.

\bibitem[Woodworth et~al.(2020{\natexlab{a}})Woodworth, Patel, Stich, Dai,
  Bullins, Mcmahan, Shamir, and Srebro]{woodworth2020local}
Blake Woodworth, Kumar~Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins,
  Brendan Mcmahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock In \emph{International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020{\natexlab{a}}.

\bibitem[Woodworth et~al.(2020{\natexlab{b}})Woodworth, Patel, and
  Srebro]{woodworth2020minibatch}
Blake~E Woodworth, Kumar~Kshitij Patel, and Nati Srebro.
\newblock Minibatch vs local sgd for heterogeneous distributed learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6281--6292, 2020{\natexlab{b}}.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017/online}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock 2017.

\bibitem[Yan et~al.(2020)Yan, Niu, Ding, Zheng, Wu, Chen, Tang, and
  Wu]{yan2020distributed}
Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai Chen,
  Shaojie Tang, and Zhihua Wu.
\newblock Distributed non-convex optimization with sublinear speedup under
  intermittent client availability.
\newblock \emph{arXiv preprint arXiv:2002.07399}, 2020.

\bibitem[Yang et~al.(2020)Yang, Fang, and Liu]{yang2020achieving}
Haibo Yang, Minghong Fang, and Jia Liu.
\newblock Achieving linear speedup with partial worker participation in non-iid
  federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yang et~al.(2022)Yang, Zhang, Khanduri, and Liu]{yang2022anarchic}
Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu.
\newblock Anarchic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  25331--25363. PMLR, 2022.

\bibitem[Yang et~al.(2018)Yang, Andrew, Eichner, Sun, Li, Kong, Ramage, and
  Beaufays]{yang2018applied}
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
  Kong, Daniel Ramage, and Fran{\c{c}}oise Beaufays.
\newblock Applied federated learning: Improving google keyboard query
  suggestions.
\newblock \emph{arXiv preprint arXiv:1812.02903}, 2018.

\bibitem[Yu et~al.(2019{\natexlab{a}})Yu, Jin, and Yang]{yu_linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, pages
  7184--7193, 2019{\natexlab{a}}.

\bibitem[Yu et~al.(2019{\natexlab{b}})Yu, Yang, and Zhu]{yu2019parallel}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5693--5700, 2019{\natexlab{b}}.

\bibitem[Yuan and Ma(2020)]{yuan2020federated}
Honglin Yuan and Tengyu Ma.
\newblock Federated accelerated stochastic gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5332--5344, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Xie, Bai, Yu, Li, and Gao]{zhang2021survey}
Chen Zhang, Yu~Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao.
\newblock A survey on federated learning.
\newblock \emph{Knowledge-Based Systems}, 216:\penalty0 106775, 2021.

\bibitem[Zhu et~al.(2021{\natexlab{a}})Zhu, Xu, Chen, Kone{\v{c}}n{\`y}, Hard,
  and Goldstein]{zhu2021diurnal}
Chen Zhu, Zheng Xu, Mingqing Chen, Jakub Kone{\v{c}}n{\`y}, Andrew Hard, and
  Tom Goldstein.
\newblock Diurnal or nocturnal? federated learning of multi-branch networks
  from periodically shifting distributions.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Zhu et~al.(2021{\natexlab{b}})Zhu, Xu, Liu, and Jin]{zhu2021federated}
Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin.
\newblock Federated learning on non-iid data: A survey.
\newblock \emph{Neurocomputing}, 465:\penalty0 371--390, 2021{\natexlab{b}}.

\end{thebibliography}
