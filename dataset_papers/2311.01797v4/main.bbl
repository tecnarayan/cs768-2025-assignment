\begin{thebibliography}{10}

\bibitem{anderson1982reverse}
Brian D.~O. Anderson.
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326,
  1982.

\bibitem{aronszajn1950theory}
Nachman Aronszajn.
\newblock Theory of reproducing kernels.
\newblock {\em Transactions of the American Mathematical Society},
  68(3):337--404, 1950.

\bibitem{austin2021structured}
Jacob Austin, Daniel~D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den
  Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock {\em Advances in Neural Information Processing Systems},
  34:17981--17993, 2021.

\bibitem{avrahami2022blended}
Omri Avrahami, Dani Lischinski, and Ohad Fried.
\newblock Blended diffusion for text-driven editing of natural images.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 18208--18218, 2022.

\bibitem{carlini2023extracting}
Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
  Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.
\newblock Extracting training data from diffusion models.
\newblock In {\em USENIX Security Symposium}, pages 5253--5270, 2023.

\bibitem{carlini2023quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
  Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{chen2023improved}
Hongrui Chen, Holden Lee, and Jianfeng Lu.
\newblock Improved analysis of score-based generative modeling: User-friendly
  bounds under minimal smoothness assumptions.
\newblock {\em International Conference on Machine Learning}, 202:4735--4763,
  2023.

\bibitem{NEURIPS2018_69386f6b}
Ricky T.~Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in Neural Information Processing Systems},
  31:6571–6583, 2018.

\bibitem{chen2023sampling}
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang.
\newblock Sampling is as easy as learning the score: Theory for diffusion
  models with minimal data assumptions.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{de2021diffusion}
Valentin De~Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet.
\newblock Diffusion {S}chr{\"o}dinger bridge with applications to score-based
  generative modeling.
\newblock {\em Advances in Neural Information Processing Systems},
  34:17695--17709, 2021.

\bibitem{dockhorn2023differentially}
Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis.
\newblock Differentially private diffusion models.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{ma2019priori}
Weinan E, Chao Ma, and Lei Wu.
\newblock A priori estimates of the population risk for two-layer neural
  networks.
\newblock {\em Communications in Mathematical Sciences}, 17(5):1407--1425,
  2019.

\bibitem{ma2022barron}
Weinan E, Chao Ma, and Lei Wu.
\newblock The {B}arron space and the flow-induced function spaces for neural
  network models.
\newblock {\em Constructive Approximation}, 55(1):369--406, 2022.

\bibitem{ghalebikesabi2023differentially}
Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth,
  Jamie Hayes, Soham De, Samuel~L Smith, Olivia Wiles, and Borja Balle.
\newblock Differentially private diffusion models generate useful synthetic
  images.
\newblock In {\em International Workshop on Trustworthy Federated Learning in
  Conjunction with IJCAI}, 2023.

\bibitem{gong2022diffuseq}
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong.
\newblock Diffuseq: Sequence to sequence text generation with diffusion models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{NEURIPS2020_4c5bcfec}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{hsu2021approximation}
Daniel Hsu, Clayton~H. Sanford, Rocco Servedio, and Emmanouil~Vasileios
  Vlatakis-Gkaragkounis.
\newblock On the approximation power of two-layer networks of random {R}e{LU}s.
\newblock {\em Conference on Learning Theory}, 134:2423--2461, 2021.

\bibitem{hu2023membership}
Hailong Hu and Jun Pang.
\newblock Membership inference of diffusion models.
\newblock {\em arXiv preprint arXiv:2301.09956}, 2023.

\bibitem{jagielski2023measuring}
Matthew Jagielski, Om~Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee,
  Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep~Guha Thakurta, Nicolas
  Papernot, and Chiyuan Zhang.
\newblock Measuring forgetting of memorized training examples.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:26565--26577, 2022.

\bibitem{koehler2023statistical}
Frederic Koehler, Alexander Heckett, and Andrej Risteski.
\newblock Statistical efficiency of score matching: The view from isoperimetry.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{li2022srdiff}
Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi~Li,
  and Yueting Chen.
\newblock Srdiff: Single image super-resolution with diffusion probabilistic
  models.
\newblock {\em Neurocomputing}, 479:47--59, 2022.

\bibitem{li2021on}
Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li.
\newblock On the curse of memory in recurrent neural networks: Approximation
  and optimization analysis.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{JMLR:v23:21-0368}
Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li.
\newblock Approximation and optimization theory for linear continuous-time
  recurrent neural networks.
\newblock {\em Journal of Machine Learning Research}, 23(42):1--85, 2022.

\bibitem{liu20232}
Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and
  Anima Anandkumar.
\newblock I$^2${SB}: Image-to-image {S}chr\"{o}dinger bridge.
\newblock {\em International Conference on Machine Learning}, 202:22042--22062,
  2023.

\bibitem{liu2022rectified}
Qiang Liu.
\newblock Rectified flow: A marginal preserving approach to optimal transport.
\newblock {\em arXiv preprint arXiv:2209.14577}, 2022.

\bibitem{liu2023flow}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with
  rectified flow.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{matsumoto2023membership}
Tomoya Matsumoto, Takayuki Miura, and Naoto Yanai.
\newblock Membership inference attacks against diffusion models.
\newblock In {\em IEEE Security and Privacy Workshops}, pages 77--83, 2023.

\bibitem{midgley2023flow}
Laurence~Illing Midgley, Vincent Stimper, Gregor N.~C. Simm, Bernhard
  Sch{\"o}lkopf, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Flow annealed importance sampling bootstrap.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{noe2019boltzmann}
Frank No{\'e}, Simon Olsson, Jonas K{\"o}hler, and Hao Wu.
\newblock Boltzmann generators: Sampling equilibrium states of many-body
  systems with deep learning.
\newblock {\em Science}, 365(6457):eaaw1147, 2019.

\bibitem{oksendal2003stochastic}
Bernt {\O}ksendal.
\newblock {\em Stochastic Differential Equations}.
\newblock Springer, 2003.

\bibitem{NEURIPS2022_e8fb575e}
Jakiw Pidstrigach.
\newblock Score-based generative models detect manifolds.
\newblock {\em Advances in Neural Information Processing Systems},
  35:35852--35865, 2022.

\bibitem{popov2021grad}
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail
  Kudinov.
\newblock Grad-tts: A diffusion probabilistic model for text-to-speech.
\newblock {\em International Conference on Machine Learning}, 139:8599--8608,
  2021.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{rasul2021autoregressive}
Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf.
\newblock Autoregressive denoising diffusion models for multivariate
  probabilistic time series forecasting.
\newblock {\em International Conference on Machine Learning}, 139:8857--8868,
  2021.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical Image Computing and Computer-Assisted Intervention},
  pages 234--241. Springer, 2015.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu~Karagol
  Ayan, Tim Salimans, Jonathan Ho, David~J. Fleet, and Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em Advances in Neural Information Processing Systems},
  35:36479--36494, 2022.

\bibitem{saharia2022image}
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David~J. Fleet, and
  Mohammad Norouzi.
\newblock Image super-resolution via iterative refinement.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  45(4):4713--4726, 2023.

\bibitem{sarkka2019applied}
Simo S{\"a}rkk{\"a} and Arno Solin.
\newblock {\em Applied Stochastic Differential Equations}, volume~10.
\newblock Cambridge University Press, 2019.

\bibitem{sehwag2022generating}
Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, and Cristian
  Canton.
\newblock Generating high fidelity data from low-density regions using
  diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11492--11501, 2022.

\bibitem{shah2023learning}
Kulin Shah, Sitan Chen, and Adam Klivans.
\newblock Learning mixtures of gaussians using the {DDPM} objective.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{shi2023diffusion}
Yuyang Shi, Valentin~De Bortoli, Andrew Campbell, and Arnaud Doucet.
\newblock Diffusion {S}chr\"{o}dinger bridge matching.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{pmlr-v37-sohl-dickstein15}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock {\em International Conference on Machine Learning}, 37:2256--2265,
  2015.

\bibitem{somepalli2023diffusion}
Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom
  Goldstein.
\newblock Diffusion art or digital forgery? investigating data replication in
  diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6048--6058, 2023.

\bibitem{somnath2023aligned}
Vignesh~Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria~Rodriguez Martinez,
  Andreas Krause, and Charlotte Bunne.
\newblock Aligned diffusion {S}chr\"{o}dinger bridges.
\newblock {\em Conference on Uncertainty in Artificial Intelligence},
  216:1985--1995, 2023.

\bibitem{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock {\em International Conference on Machine Learning}, 202:32211--32252,
  2023.

\bibitem{song2021maximum}
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock {\em Advances in Neural Information Processing Systems},
  34:1415--1428, 2021.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{song2020improved}
Yang Song and Stefano Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12438--12448, 2020.

\bibitem{song2020sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock {\em Uncertainty in Artificial Intelligence Conference},
  115:574--584, 2020.

\bibitem{song2022solving}
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon.
\newblock Solving inverse problems in medical imaging with score-based
  generative models.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{song2021scorebased}
Yang Song, Jascha Sohl-Dickstein, Diederik~P. Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{van2014probability}
Ramon Van~Handel.
\newblock Probability in high dimension.
\newblock {\em Lecture Notes (Princeton University)}, 2014.

\bibitem{vargas2021solving}
Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence.
\newblock Solving {S}chr{\"o}dinger bridges via maximum likelihood.
\newblock {\em Entropy}, 23(9):1134, 2021.

\bibitem{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural Computation}, 23(7):1661--1674, 2011.

\bibitem{vyas2023provable}
Nikhil Vyas, Sham~M. Kakade, and Boaz Barak.
\newblock On provable copyright protection for generative models.
\newblock {\em International Conference on Machine Learning}, 202:35277--35299,
  2023.

\bibitem{wang2021deep}
Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang.
\newblock Deep generative learning via {S}chr{\"o}dinger bridge.
\newblock {\em International Conference on Machine Learning}, 139:10794--10804,
  2021.

\bibitem{wu2023tune}
Jay~Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan~Weixian Lei, Yuchao Gu, Yufei
  Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike~Zheng Shou.
\newblock Tune-a-video: One-shot tuning of image diffusion models for
  text-to-video generation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7623--7633, 2023.

\bibitem{pmlr-v202-wu23r}
Lei Wu and Weijie~J. Su.
\newblock The implicit regularization of dynamical stability in stochastic
  gradient descent.
\newblock {\em International Conference on Machine Learning}, 202:37656--37684,
  2023.

\bibitem{wu2022membership}
Yixin Wu, Ning Yu, Zheng Li, Michael Backes, and Yang Zhang.
\newblock Membership inference attacks against text-to-image generation models.
\newblock {\em arXiv preprint arXiv:2210.00968}, 2022.

\bibitem{xie2022crystal}
Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi~S.
  Jaakkola.
\newblock Crystal diffusion variational autoencoder for periodic material
  generation.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{xu2022geodiff}
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang.
\newblock Geodiff: A geometric diffusion model for molecular conformation
  generation.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{JML-1-373}
Hongkang Yang.
\newblock A mathematical framework for learning probability distributions.
\newblock {\em Journal of Machine Learning}, 1(4):373--431, 2022.

\bibitem{pmlr-v145-yang22a}
Hongkang Yang and Weinan E.
\newblock Generalization and memorization: The bias potential model.
\newblock {\em Mathematical and Scientific Machine Learning}, 145:1013--1043,
  2022.

\bibitem{yang2022generalization}
Hongkang Yang and Weinan E.
\newblock Generalization error of {GAN} from the discriminator’s perspective.
\newblock {\em Research in the Mathematical Sciences}, 9(1):1--31, 2022.

\bibitem{yang2022diffusion}
Ruihan Yang, Prakhar Srivastava, and Stephan Mandt.
\newblock Diffusion probabilistic modeling for video generation.
\newblock {\em Entropy}, 25(10):1469, 2023.

\bibitem{yoon2021adversarial}
Jongmin Yoon, Sung~Ju Hwang, and Juho Lee.
\newblock Adversarial purification with score-based generative models.
\newblock {\em International Conference on Machine Learning}, 139:12062--12072,
  2021.

\bibitem{zhang2023counterfactual}
Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian
  Tram{\`e}r, and Nicholas Carlini.
\newblock Counterfactual memorization in neural language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\end{thebibliography}
