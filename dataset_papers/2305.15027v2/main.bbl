\begin{thebibliography}{}

\bibitem[Ahmed and Ding, 1993]{ahmed1993invariant}
Ahmed, N. and Ding, X. (1993).
\newblock On invariant measures of nonlinear {M}arkov processes.
\newblock {\em Journal of Applied Mathematics and Stochastic Analysis},
  6(4):385--406.

\bibitem[Alquier, 2021a]{alquier2021non}
Alquier, P. (2021a).
\newblock Non-exponentially weighted aggregation: regret bounds for unbounded
  loss functions.
\newblock In {\em International Conference on Machine Learning}, pages
  207--218. PMLR.

\bibitem[Alquier, 2021b]{alquier2021user}
Alquier, P. (2021b).
\newblock User-friendly introduction to {PAC}-{B}ayes bounds.
\newblock {\em arXiv preprint arXiv:2110.11216}.

\bibitem[Alquier and Guedj, 2018]{PACfDiv}
Alquier, P. and Guedj, B. (2018).
\newblock Simpler {PAC}-{B}ayesian bounds for hostile data.
\newblock {\em Machine Learning}, 107(5):887--902.

\bibitem[Altamirano et~al., 2023]{altamirano2023robust}
Altamirano, M., Briol, F.-X., and Knoblauch, J. (2023).
\newblock Robust and scalable {B}ayesian online changepoint detection.
\newblock {\em arXiv preprint arXiv:2302.04759}.

\bibitem[Ambrosio et~al., 2005]{ambrosio2005gradient}
Ambrosio, L., Gigli, N., and Savar{\'e}, G. (2005).
\newblock {\em Gradient flows: in metric spaces and in the space of probability
  measures}.
\newblock Springer Science \& Business Media.

\bibitem[Arbel et~al., 2019]{arbel2019maximum}
Arbel, M., Korba, A., Salim, A., and Gretton, A. (2019).
\newblock Maximum mean discrepancy gradient flow.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Barbu and R{\"o}ckner, 2020]{barbu2020nonlinear}
Barbu, V. and R{\"o}ckner, M. (2020).
\newblock From nonlinear {F}okker--{P}lanck equations to solutions of
  distribution dependent sde.
\newblock {\em arXiv preprint arXiv:1808.10706}.

\bibitem[B{\'e}gin et~al., 2016]{PACRenyiDiv}
B{\'e}gin, L., Germain, P., Laviolette, F., and Roy, J.-F. (2016).
\newblock {PAC}-{B}ayesian bounds based on the {R}{\'e}nyi divergence.
\newblock In {\em Artificial Intelligence and Statistics}, pages 435--444.

\bibitem[Bissiri et~al., 2016]{bissiri2016general}
Bissiri, P.~G., Holmes, C.~C., and Walker, S.~G. (2016).
\newblock A general framework for updating belief distributions.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 78(5):1103--1130.

\bibitem[Blundell et~al., 2015]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015).
\newblock Weight uncertainty in neural network.
\newblock In {\em International conference on machine learning}, pages
  1613--1622. PMLR.

\bibitem[Bressan, 2003]{bressan2003tutorial}
Bressan, A. (2003).
\newblock Tutorial on the center manifold theorem.
\newblock {\em Hyperbolic systems of balance laws}, 1911:327--344.

\bibitem[Chewi et~al., 2022]{chewi2022analysis}
Chewi, S., Erdogdu, M.~A., Li, M., Shen, R., and Zhang, S. (2022).
\newblock Analysis of langevin monte carlo from poincare to log-sobolev.
\newblock In {\em Conference on Learning Theory}, pages 1--2. PMLR.

\bibitem[Chiang et~al., 1987]{chiang1987diffusion}
Chiang, T.-S., Hwang, C.-R., and Sheu, S.~J. (1987).
\newblock Diffusion for global optimization in {R}$^n$.
\newblock {\em SIAM Journal on Control and Optimization}, 25(3):737--753.

\bibitem[Colding and Minicozzi~II, 2014]{colding2014lojasiewicz}
Colding, T.~H. and Minicozzi~II, W.~P. (2014).
\newblock Lojasiewicz inequalities and applications.
\newblock {\em arXiv preprint arXiv:1402.5087}.

\bibitem[D'Angelo and Fortuin, 2021]{d2021repulsive}
D'Angelo, F. and Fortuin, V. (2021).
\newblock Repulsive deep ensembles are {B}ayesian.
\newblock {\em Advances in Neural Information Processing Systems},
  34:3451--3465.

\bibitem[Ermak, 1975]{ermak1975computer}
Ermak, D.~L. (1975).
\newblock A computer simulation of charged particles in solution. i. technique
  and equilibrium properties.
\newblock {\em The Journal of Chemical Physics}, 62(10):4189--4196.

\bibitem[Fort et~al., 2019]{fort2019deep}
Fort, S., Hu, H., and Lakshminarayanan, B. (2019).
\newblock Deep ensembles: A loss landscape perspective.
\newblock {\em arXiv preprint arXiv:1912.02757}.

\bibitem[Gal and Ghahramani, 2016]{gal2016dropout}
Gal, Y. and Ghahramani, Z. (2016).
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059. PMLR.

\bibitem[Garreau et~al., 2017]{garreau2017large}
Garreau, D., Jitkrittum, W., and Kanagawa, M. (2017).
\newblock Large sample analysis of the median heuristic.
\newblock {\em arXiv preprint arXiv:1707.07269}.

\bibitem[Glaser et~al., 2021]{glaser2021kale}
Glaser, P., Arbel, M., and Gretton, A. (2021).
\newblock Kale flow: A relaxed kl gradient flow for probabilities with disjoint
  support.
\newblock {\em Advances in Neural Information Processing Systems},
  34:8018--8031.

\bibitem[Graves, 2011]{graves2011practical}
Graves, A. (2011).
\newblock Practical variational inference for neural networks.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Gretton et~al., 2012]{gretton2012}
Gretton, A., Borgwardt, K.~M., Rasch, M.~J., Sch{{\"o}}lkopf, B., and Smola, A.
  (2012).
\newblock A kernel two-sample test.
\newblock {\em Journal of Machine Learning Research}, 13(25):723--773.

\bibitem[Gr{\"u}nwald, 2011]{SafeLearning}
Gr{\"u}nwald, P. (2011).
\newblock Safe learning: bridging the gap between {B}ayes, {MDL} and
  statistical learning theory via empirical convexity.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 397--420.

\bibitem[Guedj and Shawe-Taylor, 2019]{guedj2019primer}
Guedj, B. and Shawe-Taylor, J. (2019).
\newblock A primer on pac-{B}ayesian learning.
\newblock In {\em ICML 2019-Thirty-sixth International Conference on Machine
  Learning}.

\bibitem[Haddouche and Guedj, 2023]{haddouche2023wasserstein}
Haddouche, M. and Guedj, B. (2023).
\newblock Wasserstein {PAC}-{B}ayes learning: A bridge between generalisation
  and optimisation.
\newblock {\em arXiv preprint arXiv:2304.07048}.

\bibitem[Husain and Knoblauch, 2022]{husain2022adversarial}
Husain, H. and Knoblauch, J. (2022).
\newblock Adversarial interpretation of {B}ayesian inference.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 553--572. PMLR.

\bibitem[Izmailov et~al., 2021]{izmailov2021bayesian}
Izmailov, P., Vikram, S., Hoffman, M.~D., and Wilson, A. G.~G. (2021).
\newblock What are {B}ayesian neural network posteriors really like?
\newblock In {\em International conference on machine learning}, pages
  4629--4640. PMLR.

\bibitem[Jewson et~al., 2018]{Jewson}
Jewson, J., Smith, J., and Holmes, C. (2018).
\newblock Principles of {B}ayesian inference using general divergence criteria.
\newblock {\em Entropy}, 20(6):442.

\bibitem[Knoblauch, 2019]{GVIConsistency}
Knoblauch, J. (2019).
\newblock Frequentist consistency of generalized variational inference.
\newblock {\em arXiv preprint arXiv:1912.04946}.

\bibitem[Knoblauch, 2021]{knoblauch2021optimization}
Knoblauch, J. (2021).
\newblock {\em Optimization-centric generalizations of Bayesian inference}.
\newblock PhD thesis, University of Warwick.

\bibitem[Knoblauch et~al., 2018]{RBOCPD}
Knoblauch, J., Jewson, J., and Damoulas, T. (2018).
\newblock Doubly robust {B}ayesian inference for non-stationary streaming data
  using $\beta$-divergences.
\newblock In {\em Advances in {N}eural {I}nformation {P}rocessing {S}ystems
  ({N}eur{I}{P}{S})}, pages 64--75.

\bibitem[Knoblauch et~al., 2022]{knoblauch2019generalized}
Knoblauch, J., Jewson, J., and Damoulas, T. (2022).
\newblock An optimization-centric view on {B}ayesâ€™ rule: Reviewing and
  generalizing variational inference.
\newblock {\em Journal of Machine Learning Research}, 23(132):1--109.

\bibitem[Kolokoltsov, 2010]{kolokoltsov2010nonlinear}
Kolokoltsov, V.~N. (2010).
\newblock {\em Nonlinear {M}arkov processes and kinetic equations}, volume 182.
\newblock Cambridge University Press.

\bibitem[Korba et~al., 2021]{korba2021kernel}
Korba, A., Aubin-Frankowski, P.-C., Majewski, S., and Ablin, P. (2021).
\newblock Kernel {S}tein discrepancy descent.
\newblock In {\em International Conference on Machine Learning}, pages
  5719--5730. PMLR.

\bibitem[Lakshminarayanan et~al., 2017]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017).
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Lee et~al., 2016]{lee2016gradient}
Lee, J.~D., Simchowitz, M., Jordan, M.~I., and Recht, B. (2016).
\newblock Gradient descent only converges to minimizers.
\newblock In {\em Conference on learning theory}, pages 1246--1257. PMLR.

\bibitem[Li and Turner, 2017]{li2017gradient}
Li, Y. and Turner, R.~E. (2017).
\newblock Gradient estimators for implicit models.
\newblock {\em arXiv preprint arXiv:1705.07107}.

\bibitem[Lichman, 2013]{UCI}
Lichman, M. (2013).
\newblock {UCI} machine learning repository.

\bibitem[Liggett, 2010]{liggett2010continuous}
Liggett, T.~M. (2010).
\newblock {\em Continuous time {M}arkov processes: an introduction}, volume
  113.
\newblock American Mathematical Soc.

\bibitem[Liu et~al., 2022]{liu2022loss}
Liu, C., Zhu, L., and Belkin, M. (2022).
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock {\em Applied and Computational Harmonic Analysis}, 59:85--116.

\bibitem[Liu, 2017]{liu2017stein}
Liu, Q. (2017).
\newblock Stein variational gradient descent as gradient flow.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Liu and Wang, 2016]{liu2016stein}
Liu, Q. and Wang, D. (2016).
\newblock Stein variational gradient descent: A general purpose {B}ayesian
  inference algorithm.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Louizos and Welling, 2017]{louizos2017multiplicative}
Louizos, C. and Welling, M. (2017).
\newblock Multiplicative normalizing flows for variational {B}ayesian neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2218--2227. PMLR.

\bibitem[Lu et~al., 2019]{lu2019scaling}
Lu, J., Lu, Y., and Nolen, J. (2019).
\newblock Scaling limit of the stein variational gradient descent: The mean
  field regime.
\newblock {\em SIAM Journal on Mathematical Analysis}, 51(2):648--671.

\bibitem[Ma and Hern{\'a}ndez-Lobato, 2021]{ma2021functional}
Ma, C. and Hern{\'a}ndez-Lobato, J.~M. (2021).
\newblock Functional variational inference based on stochastic process
  generators.
\newblock {\em Advances in Neural Information Processing Systems},
  34:21795--21807.

\bibitem[Ma et~al., 2019]{ma2019variational}
Ma, C., Li, Y., and Hern{\'a}ndez-Lobato, J.~M. (2019).
\newblock Variational implicit processes.
\newblock In {\em International Conference on Machine Learning}, pages
  4222--4233. PMLR.

\bibitem[Matsubara et~al., 2022]{matsubara2022robust}
Matsubara, T., Knoblauch, J., Briol, F.-X., and Oates, C.~J. (2022).
\newblock Robust generalised {B}ayesian inference for intractable likelihoods.
\newblock {\em Journal of the Royal Statistical Society Series B: Statistical
  Methodology}, 84(3):997--1022.

\bibitem[McAllester, 1999a]{McAllester2}
McAllester, D.~A. (1999a).
\newblock {PAC}-{B}ayesian model averaging.
\newblock In {\em Proceedings of the twelfth annual conference on Computational
  learning theory}, pages 164--170. ACM.

\bibitem[McAllester, 1999b]{McAllester1}
McAllester, D.~A. (1999b).
\newblock Some {PAC}-{B}ayesian theorems.
\newblock {\em Machine Learning}, 37(3):355--363.

\bibitem[Mescheder et~al., 2017]{mescheder2017adversarial}
Mescheder, L., Nowozin, S., and Geiger, A. (2017).
\newblock Adversarial variational bayes: Unifying variational autoencoders and
  generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages
  2391--2400. PMLR.

\bibitem[Miller and Dunson, 2019]{DunsonCoarsening}
Miller, J.~W. and Dunson, D.~B. (2019).
\newblock Robust {B}ayesian inference via coarsening.
\newblock {\em Journal of the American Statistical Association},
  114(527):1113--1125.

\bibitem[Muandet et~al., 2017]{muandet2017kernel}
Muandet, K., Fukumizu, K., Sriperumbudur, B., Sch{\"o}lkopf, B., et~al. (2017).
\newblock Kernel mean embedding of distributions: A review and beyond.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  10(1-2):1--141.

\bibitem[Neal, 2012]{neal2012bayesian}
Neal, R.~M. (2012).
\newblock {\em {B}ayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media.

\bibitem[Ovadia et~al., 2019]{ovadia2019can}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J., Lakshminarayanan, B., and Snoek, J. (2019).
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Polyanskiy and Wu, 2014]{polyanskiy2014lecture}
Polyanskiy, Y. and Wu, Y. (2014).
\newblock Lecture notes on information theory.
\newblock {\em Lecture Notes for ECE563 (UIUC) and}, 6(2012-2016):7.

\bibitem[Rezende and Mohamed, 2015]{rezende2015variational}
Rezende, D. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock In {\em International conference on machine learning}, pages
  1530--1538. PMLR.

\bibitem[Roberts and Tweedie, 1996]{roberts1996exponential}
Roberts, G.~O. and Tweedie, R.~L. (1996).
\newblock Exponential convergence of {L}angevin distributions and their
  discrete approximations.
\newblock {\em Bernoulli}, pages 341--363.

\bibitem[Rodriguez-Santana et~al., 2022]{rodriguez2022function}
Rodriguez-Santana, S., Zaldivar, B., and Hernandez-Lobato, D. (2022).
\newblock Function-space inference with sparse implicit processes.
\newblock In {\em International Conference on Machine Learning}, pages
  18723--18740. PMLR.

\bibitem[Santambrogio, 2015]{santambrogio2015optimal}
Santambrogio, F. (2015).
\newblock Optimal transport for applied mathematicians.
\newblock {\em Birk{\"a}user, NY}, 55.

\bibitem[Santambrogio, 2017]{santambrogio2017euclidean}
Santambrogio, F. (2017).
\newblock $\{$Euclidean, metric, and Wasserstein$\}$ gradient flows: an
  overview.
\newblock {\em Bulletin of Mathematical Sciences}, 7:87--154.

\bibitem[Shawe-Taylor and Williamson, 1997]{ShaweTaylor}
Shawe-Taylor, J. and Williamson, R.~C. (1997).
\newblock A {PAC} analysis of a {B}ayesian estimator.
\newblock In {\em Annual Workshop on Computational Learning Theory: Proceedings
  of the tenth annual conference on Computational learning theory}, volume~6,
  pages 2--9.

\bibitem[Shi et~al., 2018]{shi2018spectral}
Shi, J., Sun, S., and Zhu, J. (2018).
\newblock A spectral approach to gradient estimation for implicit
  distributions.
\newblock In {\em International Conference on Machine Learning}, pages
  4644--4653. PMLR.

\bibitem[Sun et~al., 2018]{sun2018functional}
Sun, S., Zhang, G., Shi, J., and Grosse, R. (2018).
\newblock Functional variational bayesian neural networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Syed et~al., 2022]{syed2022non}
Syed, S., Bouchard-C{\^o}t{\'e}, A., Deligiannidis, G., and Doucet, A. (2022).
\newblock Non-reversible parallel tempering: a scalable highly parallel mcmc
  scheme.
\newblock {\em Journal of the Royal Statistical Society Series B: Statistical
  Methodology}, 84(2):321--350.

\bibitem[Veretennikov, 2006]{veretennikov2006ergodic}
Veretennikov, A.~Y. (2006).
\newblock On ergodic measures for {McKean}-{V}lasov stochastic equations.
\newblock In {\em Monte Carlo and Quasi-Monte Carlo Methods 2004}, pages
  471--486. Springer Berlin Heidelberg.

\bibitem[Villani, 2003]{villani2021topics}
Villani, C. (2003).
\newblock {\em Topics in optimal transportation}, volume~58.
\newblock American Mathematical Soc.

\bibitem[Villani et~al., 2009]{villani2009optimal}
Villani, C. et~al. (2009).
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer.

\bibitem[Welling and Teh, 2011]{welling2011bayesian}
Welling, M. and Teh, Y.~W. (2011).
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688.

\bibitem[Wibisono, 2019]{wibisono2019proximal}
Wibisono, A. (2019).
\newblock Proximal langevin algorithm: Rapid convergence under isoperimetry.
\newblock {\em arXiv preprint arXiv:1911.01469}.

\bibitem[Wild et~al., 2022]{wild2022generalized}
Wild, V.~D., Hu, R., and Sejdinovic, D. (2022).
\newblock Generalized variational inference in function spaces: {G}aussian
  measures meet {B}ayesian deep learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:3716--3730.

\bibitem[Wilson, 2020]{wilson2020case}
Wilson, A.~G. (2020).
\newblock The case for {B}ayesian deep learning.
\newblock {\em arXiv preprint arXiv:2001.10995}.

\bibitem[Wilson and Izmailov, 2020]{wilson2020bayesian}
Wilson, A.~G. and Izmailov, P. (2020).
\newblock {B}ayesian deep learning and a probabilistic perspective of
  generalization.
\newblock {\em Advances in neural information processing systems},
  33:4697--4708.

\bibitem[Wu and Martin, 2023]{wu2023comparison}
Wu, P.-S. and Martin, R. (2023).
\newblock A comparison of learning rate selection methods in generalized
  {B}ayesian inference.
\newblock {\em {B}ayesian Analysis}, 18(1):105--132.

\bibitem[Zou and Hastie, 2005]{zou2005regularization}
Zou, H. and Hastie, T. (2005).
\newblock Regularization and variable selection via the elastic net.
\newblock {\em Journal of the royal statistical society: series B (statistical
  methodology)}, 67(2):301--320.

\end{thebibliography}
