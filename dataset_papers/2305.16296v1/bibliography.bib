@Article{RobbinsMonro:1951,
  author    = {Robbins, Herbert and Monro, Sutton},
  title     = {A stochastic approximation method},
  journal   = {Annals of Mathematical Statistics},
  year      = {1951},
  volume    = {22},
  pages     = {400-407},
  added-at  = {2008-10-07T16:03:39.000+0200},
  biburl    = {http://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
  interhash = {93d54534a08c30eda9e34d1def03ffa3},
  intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
  keywords  = {imported},
  timestamp = {2008-10-07T16:03:40.000+0200},
}

@book{Polyak,
	author={Polyak, Boris},
	title={Introduction to Optimization},
	publisher={OptimizationSoftware, Inc.},
	year={1987}
}

@article{NestSpok,
	author={Nesterov, Yurii and Spokoiny, Vladimir},
	title={Random gradient-free minimization of convex functions},
	journal={Found. Comput. Math.},
	volume={17},
	number={2},
	pages={527--566},
	year={2017}
}

@article{AjallStich,
	title={Analysis of {SGD} with Biased Gradient Estimators},
	author={Ajalloeian,  Ahmad and Stich,  Sebastian U},
	journal={arXiv preprint arXiv:2008.00051},
	year={2020}
}

@book{GoodBenCour,
	title={Deep Learning},
	author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	publisher={The MIT Press},
	year={2016}
}

@article{Sun,
	title={Optimization for Deep Learning: An Overview},
	author={Sun, Ruo-Yu},
	journal={Journal of the Operations Research Society of China},
	volume={8},
	number={2},
	year={2020},
	pages={249--294}
}

@article{KarNutSch,
	title={Linear convergence of gradient and proximal-gradient methods under the Polyak-\L ojasiewicz condition},
	author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
	journal = {Machine Learning and Knowledge Discovery in Databases},
	pages={795â€“-811},
	publisher={Springer International Publishing},
	year={2016}
}


@article{PolyakAsn,
	title={Gradient methods for minimizing functionals},
	author={Polyak, Boris},
	journal={U.S.S.R. Comput. Math. Math. Phys.},
	volume={3},
	number={4},
	year={1963},
	pages={864--878}
}

@article{BezHorRichSaf,
	title={On Biased Compression for Distributed Learning},
	author={Beznosikov,  Aleksandr and Horv{\'a}th,  Samuel and Richt{\'a}rik,  Peter and Safaryan,  Mher},
	journal={arXiv preprint arXiv:2002.12410},
	year={2020}
}

@article{BotCurNoce,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L\'{e}on and Curtis, Frank and Nocedal, Jorge},
	journal={SIAM Review},
	volume={60},
	number={2},
	year={2018},
	pages={223--311}
}

@article{CondYiRich,
	title={EF-BV: A unified theory of error feedback and variance reduction mechanisms for biased and unbiased compression in distributed optimization},
	author={Condat, Laurent and Yi, Kai and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2205.04180},
	year={2022}
}

@article{HuSeiLes,
	title={Analysis of biased stochastic gradient descent using sequential semidefinite programs},
	author={Hu, Bin and Seiler, Peter and Lessard, Laurent},
	journal={Mathematical Programming},
	volume={187},
	pages={383--408},
	year={2021},
	publisher={Springer}
}

@inproceedings{CNAT,
	title={Natural compression for distributed deep learning},
	author={Horv{\'a}th, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richt{\'a}rik, Peter},
	booktitle={Mathematical and Scientific Machine Learning},
	pages={129--141},
	year={2022},
	organization={PMLR}
}

@article{terngrad,
	title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
	author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{khirirat2018distributed,
	title={Distributed learning with compressed gradients},
	author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
	journal={arXiv preprint arXiv:1806.06573},
	year={2018}
}

@article{MoosFawziFrossard,
	title={Universal adversarial perturbations},
	author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	journal={arXiv preprints arXiv:1610.08401},
	year={2016}
}

@article{ChenZhangSharmaYiHsieh,
	title={ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
	author={Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
	journal={Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
	pages={15--26},
	year={2017}
}

@article{LiuKailChenTingChangAmini,
	title={Zeroth-order stochastic variance re- duction for nonconvex optimization},
	author={Liu, Sijia and Kailkhura, Bhavya and Chen, Pin-Yu and Ting, Paishun and Chang, Shiyu and Amini, Lisa},
	journal={Advances in neural information processing systems (NeurIPS)},
	volume={31},
	pages={3727--3737},
	year={2018}
}

@article{DAspremont,
	title={Smooth optimization with approximate gradient},
	author={d'Aspremont, Alexandre},
	journal={SIAM Journal on Optimization},
	volume={19},
	number={3},
	pages={1171--1183},
	year={2008}
}

@article{SchmidtRouxBach,
	title={Convergence rates of inexact proximal-gradient methods for convex optimization},
	author={Schmidt, Mark and Roux, Nicolas and Bach, Francis},
	journal={Advances in neural information processing systems (NeurIPS)},
	volume={24},
	pages={1458--1466},
	year={2011}
}

@article{DevolderGlineurNesterov,
	title={First-order methods of smooth convex optimization with inexact oracle},
	author={Devolder, Olivier and Glineur, Fran\c{c}ois and Nesterov, Yurii},
	journal={Math. Program.},
	volume={146},
	number={1-2},
	pages={37--75},
	year={2014}
}

@article{TappendenRichtarikGondzio,
	title={Inexact coordinate descent: Complexity and preconditioning},
	author={Tappenden, Rachael and Richt{\'a}rik, Peter and Gondzio, Jacek},
	journal={Journal of Optimization Theory and Applications},
	volume={170},
	pages={144--176},
	year={2016}
}

@inproceedings{KariStichJaggi,
	title={Adaptive balancing of gradient and update computation times using global geometry and approximate subproblems},
	author={Karimireddy, Sai Praneeth and Stich, Sebastian and Jaggi, Martin},
	journal={Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS},
	\volume={84},
	\pages={1204--1213},
	year={2018}
}

@book{shai_book,
  title={Understanding machine learning: from theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
}

@book{NemirovskyYudin,
	title={Problwm Complexity and Method Efficiency in Optimization},
	author={Nemirovsky, Arkadi and Yudin, David},
	publisher={Wiley, New York},
	year={1983}
}

@article{LeiHuLiTang,
	title={Stochastic Gradient Descent for Nonconvex Learning Without Bounded Gradient Assumptions},
	author={Lei, Yunwei and Hu, Ting and Li, Guiying and Tang, Ke},
	booktitle={IEEE Transactions on Neural Networks and Learning Systems},
	year={2019},
	pages={1--7},
	publisher={ISSN 2162-2388}
}

@Article{DIANA,
	author  = {Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
	journal = {arXiv preprint arXiv:1901.09269},
	title   = {Distributed Learning with Compressed Gradient Differences},
	year    = {2019},
}

@Article{DIANA2,
	author  = {Horv\'{a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt\'{a}rik, Peter},
	title   = {Stochastic distributed learning with gradient quantization and variance reduction},
	journal = {arXiv preprint arXiv:1904.05115},
	year    = {2019},
}

@article{khaled2022better,
	title={Better Theory for {SGD} in the  Nonconvex World},
	author={Ahmed Khaled and Peter Richt{\'a}rik},
	journal={Transactions on Machine Learning Research},
	issn={2835-8856},
	year={2023},
	url={https://openreview.net/forum?id=AU4qHN2VkS},
	note={Survey Certification}
}

@inproceedings{khirirat2018gradient,
	title={Gradient compression for communication-limited convex optimization},
	author={Khirirat, Sarit and Johansson, Mikael and Alistarh, Dan},
	booktitle={2018 IEEE Conference on Decision and Control (CDC)},
	pages={166--171},
	year={2018},
	organization={IEEE}
}

@inproceedings{SapCanHoNelKalKinKrisMoshPorRich,
	title={Scaling distributed machine learning with in-network aggregation},
	author={Sapio, Amedeo and Canini, Marco and Ho, Chen-Yu and Nelson, Jacob and Kalnis, Panos and Kim, Changhoon and Krishnamurthy, Arvind and Moshref, Masoud and Ports, Dan and Richt\'{a}rik, Peter},
	booktitle={In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
	pages={785--808},
	year={2021}
}

@inproceedings{danilova2022distributed,
	title={Distributed methods with absolute compression and error compensation},
	author={Danilova, Marina and Gorbunov, Eduard},
	booktitle={Mathematical Optimization Theory and Operations Research: Recent Trends: 21st International Conference, MOTOR 2022, Petrozavodsk, Russia, July 2--6, 2022, Revised Selected Papers},
	pages={163--177},
	year={2022},
	organization={Springer}
}

@inproceedings{gupta2015deep,
	title={Deep learning with limited numerical precision},
	author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
	booktitle={International conference on machine learning},
	pages={1737--1746},
	year={2015},
	organization={PMLR}
}

@article{sahu2021rethinking,
	title={Rethinking gradient sparsification as total error minimization},
	author={Sahu, Atal and Dutta, Aritra and M Abdelmoniem, Ahmed and Banerjee, Trambak and Canini, Marco and Kalnis, Panos},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={8133--8146},
	year={2021}
}

@inproceedings{dutta2020discrepancy,
	title={On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning},
	author={Dutta, Aritra and Bergou, El Houcine and Abdelmoniem, Ahmed M and Ho, Chen-Yu and Sahu, Atal Narayan and Canini, Marco and Kalnis, Panos},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={34},
	number={04},
	pages={3817--3824},
	year={2020}
}

@article{strom2015scalable,
	title={Scalable distributed DNN training using commodity GPU cloud computing},
	author={Str{\"o}m, Nikko},
	year={2015}
}

@article{sapio2019scaling,
	title={Scaling distributed machine learning with in-network aggregation},
	author={Sapio, Amedeo and Canini, Marco and Ho, Chen-Yu and Nelson, Jacob and Kalnis, Panos and Kim, Changhoon and Krishnamurthy, Arvind and Moshref, Masoud and Ports, Dan RK and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:1903.06701},
	year={2019}
}

Error-feedback classical papers

@article{stich2020error,
	title={The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates},
	author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
	journal={The Journal of Machine Learning Research},
	volume={21},
	number={1},
	pages={9613--9648},
	year={2020},
	publisher={JMLRORG}
}

@inproceedings{karimireddy2019ef,
	author    = {Sai Praneeth Karimireddy and
	Quentin Rebjock and
	Sebastian U. Stich and
	Martin Jaggi},
	title     = {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
	booktitle = {International Conference on Machine Learning (ICML)},
	volume    = {97},
	pages     = {3252--3261},
	year      = {2019},
}

@InProceedings{Stich-EF-NIPS2018,
	author    = {Stich,  Sebastian U.  and Cordonnier,  J.-B.  and Jaggi,  Martin},
	title     = {Sparsified {SGD} with memory},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year      = {2018},
}

Error Feedback with absolute compression

@article{khirirat2020compressed,
	title={Compressed gradient methods with Hessian-aided error compensation},
	author={Khirirat, Sarit and Magn{\'u}sson, Sindri and Johansson, Mikael},
	journal={IEEE Transactions on Signal Processing},
	volume={69},
	pages={998--1011},
	year={2020},
	publisher={IEEE}
}

@inproceedings{gower2019sgd,
	title={SGD: General analysis and improved rates},
	author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
	booktitle={International conference on machine learning},
	pages={5200--5209},
	year={2019},
	organization={PMLR}
}

@article{mishchenko2021intsgd,
	title={IntSGD: Adaptive floatless compression of stochastic gradients},
	author={Mishchenko, Konstantin and Wang, Bokun and Kovalev, Dmitry and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2102.08374},
	year={2021}
}

@article{tyurin2022sharper,
	title={Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling},
	author={Tyurin, Alexander and Sun, Lukang and Burlachenko, Konstantin and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2206.02275},
	year={2022}
}

@inproceedings{szlendakpermutation,
	title={Permutation Compressors for Provably Faster Distributed Nonconvex Optimization},
	author={Szlendak, Rafa{\l} and Tyurin, Alexander and Richt{\'a}rik, Peter},
	booktitle={International Conference on Learning Representations}
}

@inproceedings{yu2019universally,
	title={Universally slimmable networks and improved training techniques},
	author={Yu, Jiahui and Huang, Thomas S},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={1803--1811},
	year={2019}
}

@article{Yu2018SlimmableNN,
	title={Slimmable Neural Networks},
	author={Jiahui Yu and L. Yang and N. Xu and Jianchao Yang and Thomas S. Huang},
	journal={ArXiv},
	year={2018},
	volume={abs/1812.08928}
}

@inproceedings{sun2017meprop,
	title={meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting},
	author={Sun, Xu and Ren, Xuancheng and Ma, Shuming and Wang, Houfeng},
	booktitle={International Conference on Machine Learning},
	pages={3299--3308},
	year={2017},
	organization={PMLR}
}

@inproceedings{khirirat2022eco,
	title={Eco-Fedsplit: Federated Learning with Error-Compensated Compression},
	author={Khirirat, Sarit and Magn{\'u}sson, Sindri and Johansson, Mikael},
	booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={5952--5956},
	year={2022},
	organization={IEEE}
}

@article{chen2022efficient,
	title={Efficient-Adam: Communication-efficient distributed Adam with complexity analysis},
	author={Chen, Congliang and Shen, Li and Liu, Wei and Luo, Zhi-Quan},
	journal={arXiv preprint arXiv:2205.14473},
	year={2022}
}

@article{zheng2019communication,
	title={Communication-efficient distributed blockwise momentum SGD with error-feedback},
	author={Zheng, Shuai and Huang, Ziyue and Kwok, James},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}

@article{chen2021quantized,
	title={Quantized adam with error feedback},
	author={Chen, Congliang and Shen, Li and Huang, Haozhi and Liu, Wei},
	journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
	volume={12},
	number={5},
	pages={1--26},
	year={2021},
	publisher={ACM New York, NY}
}

@inproceedings{safaryan2021stochastic,
	title={Stochastic sign descent methods: New algorithms and better theory},
	author={Safaryan, Mher and Richt{\'a}rik, Peter},
	booktitle={International Conference on Machine Learning},
	pages={9224--9234},
	year={2021},
	organization={PMLR}
}

More papers on contractive compression

@inproceedings{DoubleSqueeze,
	title={{D}ouble{S}queeze: {P}arallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression},
	author={Hanlin Tang and Xiangru Lian and Chen Yu and Tong Zhang and Ji Liu},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
	year={2020},
}

@article{gorbunov2020linearly,
	title={Linearly converging error compensated {SGD}},
	author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={20889--20900},
	year={2020}
}

@techreport{cordonnier2018convex,
	title={Convex optimization using sparsified stochastic gradient descent with memory},
	author={Cordonnier, Jean-Baptiste},
	year={2018}
}

EF21 stuff

@InProceedings{EF21,
	author    = {Peter Richt\'{a}rik and Igor Sokolov and Ilyas Fatkhullin},
	booktitle = {Advances in Neural Information Processing Systems},
	title     = {{EF21}: A new,  simpler,  theoretically better,  and practically faster error feedback},
	year      = {2021},
}

@Article{EF21BW,
	author  = {Ilyas Fatkhullin and Igor Sokolov and Eduard Gorbunov and Zhize Li and Peter Richt\'{a}rik},
	journal = {arXiv preprint arXiv:2110.03294},
	title   = {{EF21} with bells \& whistles: practical algorithmic extensions of modern error feedback},
	year    = {2021},
}

@inproceedings{richtarik20223pc,
	title={3PC: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation},
	author={Richt{\'a}rik, Peter and Sokolov, Igor and Gasanov, Elnur and Fatkhullin, Ilyas and Li, Zhize and Gorbunov, Eduard},
	booktitle={International Conference on Machine Learning},
	pages={18596--18648},
	year={2022},
	organization={PMLR}
}

TopK papers

@article{aji2017sparse,
	title={Sparse communication for distributed gradient descent},
	author={Aji, Alham Fikri and Heafield, Kenneth},
	journal={arXiv preprint arXiv:1704.05021},
	year={2017}
}

@inproceedings{alistarh2018sparse,
	author = {Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, Cedric},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {5973--5983},
	title = {The Convergence of Sparsified Gradient Methods},
	volume = {31},
	year = {2018}
}

@article{chang2011libsvm,
	title={{LIBSVM}: a library for support vector machines},
	author={Chang, Chih-Chung and Lin, Chih-Jen},
	journal={{ACM} {T}ransactions on {I}ntelligent {S}ystems and {T}echnology (TIST)},
	volume={2},
	number={3},
	pages={1--27},
	year={2011},
	publisher={ACM New York, NY, USA}
}

@inproceedings{WangniWangLiuZhang,
	title={Gradient sparsification for communication-efficient distributed optimization},
	author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	pages={1306--1316},
	volume={31},
	year={2018}
}

@inproceedings{NiuRechtReWright,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	pages={693--701},
	volume={24},
	year={2011}
}
<<<<<<< HEAD
=======



>>>>>>> f298c8d520aff97d922b84d4370d4403f7c8b242
