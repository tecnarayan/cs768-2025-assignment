@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{gao2021provably,
  title={A provably efficient algorithm for linear markov decision process with low switching cost},
  author={Gao, Minbo and Xie, Tianle and Du, Simon S and Yang, Lin F},
  journal={arXiv preprint arXiv:2101.00494},
  year={2021}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@inproceedings{sarkar2019near,
  title={Near optimal finite time identification of arbitrary linear dynamical systems},
  author={Sarkar, Tuhin and Rakhlin, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={5610--5618},
  year={2019},
  organization={PMLR}
}



@article{ok2018exploration,
  title={Exploration in structured reinforcement learning},
  author={Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
  journal={arXiv preprint arXiv:1806.00775},
  year={2018}
}

@article{zhang2021variance,
  title={Variance-Aware Confidence Set: Variance-Dependent Bound for Linear Bandits and Horizon-Free Bound for Linear Mixture MDP},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2101.12745},
  year={2021}
}

@inproceedings{he2021logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={4171--4180},
  year={2021},
  organization={PMLR}
}

@inproceedings{tarbouriech2019active,
  title={Active exploration in markov decision processes},
  author={Tarbouriech, Jean and Lazaric, Alessandro},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={974--982},
  year={2019},
  organization={PMLR}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={2312--2320},
  year={2011}
}


@article{even2006action,
  title={Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems.},
  author={Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay and Mahadevan, Sridhar},
  journal={Journal of machine learning research},
  volume={7},
  number={6},
  year={2006}
}


@article{zanette2019almost,
  title={Almost horizon-free structure-aware best policy identification with a generative model},
  author={Zanette, Andrea and Kochenderfer, Mykel and Brunskill, Emma},
  year={2019}
}


@article{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular MDPs},
  author={Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:1905.03814},
  year={2019}
}

@article{khamaru2021instance,
  title={Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning},
  author={Khamaru, Koulik and Xia, Eric and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2106.14352},
  year={2021}
}

@article{tirinzoni2020asymptotically,
  title={An Asymptotically Optimal Primal-Dual Incremental Algorithm for Contextual Linear Bandits},
  author={Tirinzoni, Andrea and Pirotta, Matteo and Restelli, Marcello and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:2010.12247},
  year={2020}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={arXiv preprint arXiv:2006.10814},
  year={2020}
}

@article{wang2020reward,
  title={On reward-free reinforcement learning with linear function approximation},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2006.11274},
  year={2020}
}

@article{zhang2020nearly,
  title={Nearly Minimax Optimal Reward-free Reinforcement Learning},
  author={Zhang, Zihan and Du, Simon S and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2010.05901},
  year={2020}
}

@article{du2019provably,
  title={Provably Efficient $ Q $-learning with Function Approximation via Distribution Shift Error Checking Oracle},
  author={Du, Simon S and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
  journal={arXiv preprint arXiv:1906.06321},
  year={2019}
}


@inproceedings{lattimore2012pac,
  title={PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}

@article{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  journal={arXiv preprint arXiv:1510.08906},
  year={2015}
}

@article{jin2021bellman,
  title={Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={arXiv preprint arXiv:2102.00815},
  year={2021}
}


@article{zhou2020nearly,
  title={Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  journal={arXiv preprint arXiv:2012.08507},
  year={2020}
}


@article{khamaru2020temporal,
  title={Is temporal difference learning optimal? an instance-dependent analysis},
  author={Khamaru, Koulik and Pananjady, Ashwin and Ruan, Feng and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2003.07337},
  year={2020}
}


@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}


@article{kaufmann2016complexity,
  title={On the complexity of best-arm identification in multi-armed bandit models},
  author={Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1--42},
  year={2016},
  publisher={JMLR. org}
}


@article{marjani2020best,
  title={Best Policy Identification in discounted MDPs: Problem-specific Sample Complexity},
  author={Marjani, Aymen Al and Proutiere, Alexandre},
  journal={arXiv preprint arXiv:2009.13405},
  year={2020}
}




@article{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={arXiv preprint arXiv:1703.07710},
  year={2017}
}

@inproceedings{yin2020asymptotically,
  title={Asymptotically efficient off-policy evaluation for tabular reinforcement learning},
  author={Yin, Ming and Wang, Yu-Xiang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3948--3958},
  year={2020},
  organization={PMLR}
}

@article{xie2019towards,
  title={Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
  author={Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:1906.03393},
  year={2019}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@article{kallus2019efficiently,
  title={Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={arXiv preprint arXiv:1909.05850},
  year={2019}
}

@article{zhang2020reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2009.13503},
  year={2020}
}

@article{xu2021fine,
  title={Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap},
  author={Xu, Haike and Ma, Tengyu and Du, Simon S},
  journal={arXiv preprint arXiv:2102.04692},
  year={2021}
}



@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}

@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  school={UCL (University College London)}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}

@article{maurer2009empirical,
  title={Empirical Bernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}



%%% Potential duplicates start here

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={209--232},
  year={2002},
  publisher={Springer}
}


@article{li2011knows,
  title={Knows what it knows: a framework for self-aware learning},
  author={Li, Lihong and Littman, Michael L and Walsh, Thomas J and Strehl, Alexander L},
  journal={Machine learning},
  volume={82},
  number={3},
  pages={399--443},
  year={2011},
  publisher={Springer}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}



@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@inproceedings{domingues2021episodic,
  title={Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={578--598},
  year={2021},
  organization={PMLR}
}

@article{menard2020fast,
  title={Fast active learning for pure exploration in reinforcement learning},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Kaufmann, Emilie and Leurent, Edouard and Valko, Michal},
  journal={arXiv preprint arXiv:2007.13442},
  year={2020}
}

@inproceedings{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4868--4878},
  year={2018}
}



@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018},
  organization={PMLR}
}

@article{wang2020long,
  title={Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}



@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}



@inproceedings{zanette2019generative,
 author = {Zanette, Andrea and Kochenderfer, Mykel J and Brunskill, Emma},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model},
 volume = {32},
 year = {2019}
}



@article{jonsson2020planning,
  title={Planning in markov decision processes with gap-dependent sample complexity},
  author={Jonsson, Anders and Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Leurent, Edouard and Valko, Michal},
  journal={arXiv preprint arXiv:2006.05879},
  year={2020}
}


@article{marjani2021navigating,
  title={Navigating to the Best Policy in Markov Decision Processes},
  author={Marjani, Aymen Al and Garivier, Aur{\'e}lien and Proutiere, Alexandre},
  journal={arXiv preprint arXiv:2106.02847},
  year={2021}
}


@article{wagenmaker2021task,
  title={Task-Optimal Exploration in Linear Dynamical Systems},
  author={Wagenmaker, Andrew and Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2102.05214},
  year={2021}
}


@inproceedings{zimin2013online,
  title={Online learning in episodic Markovian decision processes by relative entropy policy search},
  author={Zimin, Alexander and Neu, Gergely},
  booktitle={Neural Information Processing Systems 26},
  year={2013}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}


@article{mannor2004sample,
  title={The sample complexity of exploration in the multi-armed bandit problem},
  author={Mannor, Shie and Tsitsiklis, John N},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jun},
  pages={623--648},
  year={2004}
}


@article{freedman1975tail,
  title={On tail probabilities for martingales},
  author={Freedman, David A},
  journal={the Annals of Probability},
  pages={100--118},
  year={1975},
  publisher={JSTOR}
}


@article{lugosi2019mean,
  title={Mean estimation and regression under heavy-tailed distributions: A survey},
  author={Lugosi, G{\'a}bor and Mendelson, Shahar},
  journal={Foundations of Computational Mathematics},
  volume={19},
  number={5},
  pages={1145--1190},
  year={2019},
  publisher={Springer}
}


@inproceedings{wei2020taking,
  title={Taking a hint: How to leverage loss predictors in contextual bandits?},
  author={Wei, Chen-Yu and Luo, Haipeng and Agarwal, Alekh},
  booktitle={Conference on Learning Theory},
  pages={3583--3634},
  year={2020},
  organization={PMLR}
}


%%% -------------------------

@inproceedings{catoni2012challenging,
  title={Challenging the empirical mean and empirical variance: a deviation study},
  author={Catoni, Olivier},
  booktitle={Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume={48},
  number={4},
  pages={1148--1185},
  year={2012}
}

@article{dann2021beyond,
  title={Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning},
  author={Dann, Christoph and Marinov, Teodor Vanislavov and Mohri, Mehryar and Zimmert, Julian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{wagenmaker2021experimental,
  title={Experimental design for regret minimization in linear bandits},
  author={Wagenmaker, Andrew and Katz-Samuels, Julian and Jamieson, Kevin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3088--3096},
  year={2021},
  organization={PMLR}
}

@inproceedings{foster2021efficient,
  title={Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination},
  author={Foster, Dylan J and Krishnamurthy, Akshay},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{zhang2020almost,
  title={Almost Optimal Model-Free Reinforcement Learningvia Reference-Advantage Decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{agarwal2017open,
  title={Open problem: First-order regret bounds for contextual bandits},
  author={Agarwal, Alekh and Krishnamurthy, Akshay and Langford, John and Luo, Haipeng and others},
  booktitle={Conference on Learning Theory},
  pages={4--7},
  year={2017},
  organization={PMLR}
}

@inproceedings{allen2018make,
  title={Make the minority great again: First-order regret bound for contextual bandits},
  author={Allen-Zhu, Zeyuan and Bubeck, S{\'e}bastien and Li, Yuanzhi},
  booktitle={International Conference on Machine Learning},
  pages={186--194},
  year={2018},
  organization={PMLR}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low Bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@article{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon S and Kakade, Sham M and Lee, Jason D and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  journal={arXiv preprint arXiv:2103.10897},
  year={2021}
}



@inproceedings{yang2019sample,
  title={Sample-optimal parametric Q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}


@article{wang2019optimism,
  title={Optimism in reinforcement learning with generalized linear function approximation},
  author={Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
  journal={arXiv preprint arXiv:1912.04136},
  year={2019}
}

@article{du2019good,
  title={Is a good representation sufficient for sample efficient reinforcement learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}

@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020},
  organization={PMLR}
}

@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}

@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}

@inproceedings{weisz2021exponential,
  title={Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions},
  author={Weisz, Gell{\'e}rt and Amortila, Philip and Szepesv{\'a}ri, Csaba},
  booktitle={Algorithmic Learning Theory},
  pages={1237--1264},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhou2021provably,
  title={Provably efficient reinforcement learning for discounted mdps with feature mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={12793--12802},
  year={2021},
  organization={PMLR}
}



@article{wang2021exponential,
  title={An exponential lower bound for linearly-realizable MDPs with constant suboptimality gap},
  author={Wang, Yuanhao and Wang, Ruosong and Kakade, Sham M},
  journal={arXiv preprint arXiv:2103.12690},
  year={2021}
}

@inproceedings{jia2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
  booktitle={Learning for Dynamics and Control},
  pages={666--686},
  year={2020},
  organization={PMLR}
}

@inproceedings{camilleri2021high,
  title={High-dimensional experimental design and kernel bandits},
  author={Camilleri, Romain and Jamieson, Kevin and Katz-Samuels, Julian},
  booktitle={International Conference on Machine Learning},
  pages={1227--1237},
  year={2021},
  organization={PMLR}
}

@article{lee2021achieving,
  title={Achieving Near Instance-Optimality and Minimax-Optimality in Stochastic and Adversarial Linear Bandits Simultaneously},
  author={Lee, Chung-Wei and Luo, Haipeng and Wei, Chen-Yu and Zhang, Mengxiao and Zhang, Xiaojin},
  journal={arXiv preprint arXiv:2102.05858},
  year={2021}
}






@article{srebro2010smoothness,
  title={Smoothness, low noise and fast rates},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@article{vapnik1971uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, A Ya},
  booktitle={Measures of complexity},
  year={1971}
}



@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@article{auer2002adaptive,
  title={Adaptive and self-confident on-line learning algorithms},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Gentile, Claudio},
  journal={Journal of Computer and System Sciences},
  volume={64},
  number={1},
  pages={48--75},
  year={2002},
  publisher={Elsevier}
}

@article{cesa2007improved,
  title={Improved second-order bounds for prediction with expert advice},
  author={Cesa-Bianchi, Nicolo and Mansour, Yishay and Stoltz, Gilles},
  journal={Machine Learning},
  volume={66},
  number={2},
  pages={321--352},
  year={2007},
  publisher={Springer}
}

@inproceedings{luo2015achieving,
  title={Achieving all with no parameters: Adanormalhedge},
  author={Luo, Haipeng and Schapire, Robert E},
  booktitle={Conference on Learning Theory},
  pages={1286--1304},
  year={2015},
  organization={PMLR}
}

@inproceedings{koolen2015second,
  title={Second-order quantile methods for experts and combinatorial games},
  author={Koolen, Wouter M and Van Erven, Tim},
  booktitle={Conference on Learning Theory},
  pages={1155--1175},
  year={2015},
  organization={PMLR}
}

@article{foster2015adaptive,
  title={Adaptive online learning},
  author={Foster, Dylan J and Rakhlin, Alexander and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1508.05170},
  year={2015}
}

@article{kim2021improved,
  title={Improved Regret Analysis for Variance-Adaptive Linear Bandits and Horizon-Free Linear Mixture MDPs},
  author={Kim, Yeoneung and Yang, Insoon and Jun, Kwang-Sung},
  journal={arXiv preprint arXiv:2111.03289},
  year={2021}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@inproceedings{allenberg2006hannan,
  title={Hannan consistency in on-line learning in case of unbounded losses under partial monitoring},
  author={Allenberg, Chamy and Auer, Peter and Gy{\"o}rfi, L{\'a}szl{\'o} and Ottucs{\'a}k, Gy{\"o}rgy},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={229--243},
  year={2006},
  organization={Springer}
}

@inproceedings{neu2015first,
  title={First-order regret bounds for combinatorial semi-bandits},
  author={Neu, Gergely},
  booktitle={Conference on Learning Theory},
  pages={1360--1375},
  year={2015},
  organization={PMLR}
}

@inproceedings{lykouris2018small,
  title={Small-loss bounds for online learning with partial information},
  author={Lykouris, Thodoris and Sridharan, Karthik and Tardos, {\'E}va},
  booktitle={Conference on Learning Theory},
  pages={979--986},
  year={2018},
  organization={PMLR}
}

@article{ito2020tight,
  title={Tight first-and second-order regret bounds for adversarial linear bandits},
  author={Ito, Shinji and Hirahara, Shuichi and Soma, Tasuku and Yoshida, Yuichi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2028--2038},
  year={2020}
}

@inproceedings{bubeck2020first,
  title={First-Order Bayesian Regret Analysis of Thompson Sampling},
  author={Bubeck, S{\'e}bastien and Sellke, Mark},
  booktitle={Algorithmic Learning Theory},
  pages={196--233},
  year={2020},
  organization={PMLR}
}

@inproceedings{wei2018more,
  title={More adaptive algorithms for adversarial bandits},
  author={Wei, Chen-Yu and Luo, Haipeng},
  booktitle={Conference On Learning Theory},
  pages={1263--1291},
  year={2018},
  organization={PMLR}
}

@article{hazan2011better,
  title={Better Algorithms for Benign Bandits.},
  author={Hazan, Elad and Kale, Satyen},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={4},
  year={2011}
}
