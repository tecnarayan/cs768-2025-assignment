@article{cirone2023neural,
  title={Neural signature kernels as infinite-width-depth-limits of controlled ResNets},
  author={Cirone, Nicola Muca and Lemercier, Maud and Salvi, Cristopher},
  journal={arXiv preprint arXiv:2303.17671},
  year={2023}
}


@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={2016 IEEE symposium on security and privacy (SP)},
  pages={582--597},
  year={2016},
  organization={IEEE}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}

@article{he2023simplifying,
  title={Simplifying transformer blocks},
  author={He, Bobby and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2311.01906},
  year={2023}
}
@article{bernstein2020distance,
  title={On the distance between two neural networks and the stability of learning},
  author={Bernstein, Jeremy and Vahdat, Arash and Yue, Yisong and Liu, Ming-Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21370--21381},
  year={2020}
}

@article{bernstein2023automatic,
  title={Automatic gradient descent: Deep learning without hyperparameters},
  author={Bernstein, Jeremy and Mingard, Chris and Huang, Kevin and Azizan, Navid and Yue, Yisong},
  journal={arXiv preprint arXiv:2304.05187},
  year={2023}
}

@misc{chizat2024featurespeedformulaflexible,
      title={The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks}, 
      author={Lénaïc Chizat and Praneeth Netrapalli},
      year={2024},
      eprint={2311.18718},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.18718}, 
}

@article{yang2023spectral,
  title={A spectral condition for feature learning},
  author={Yang, Greg and Simon, James B and Bernstein, Jeremy},
  journal={arXiv preprint arXiv:2310.17813},
  year={2023}
}

@inproceedings{littwin2022adaptive,
  title={Adaptive optimization in the $\infty$-width limit},
  author={Littwin, Etai and Yang, Greg},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{nakkiran2020deep,
  title={The deep bootstrap framework: Good online learners are good offline generalizers},
  author={Nakkiran, Preetum and Neyshabur, Behnam and Sedghi, Hanie},
  journal={arXiv preprint arXiv:2010.08127},
  year={2020}
}

@misc{cowsik2024geometric,
      title={Geometric Dynamics of Signal Propagation Predict Trainability of Transformers}, 
      author={Aditya Cowsik and Tamra Nebabu and Xiao-Liang Qi and Surya Ganguli},
      year={2024},
      eprint={2403.02579},
      archivePrefix={arXiv},
      primaryClass={cond-mat.dis-nn}
}

@article{alabdulmohsin2024getting,
  title={Getting vit in shape: Scaling laws for compute-optimal model design},
  author={Alabdulmohsin, Ibrahim M and Zhai, Xiaohua and Kolesnikov, Alexander and Beyer, Lucas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{bordelon2024dynamical,
      title={A Dynamical Model of Neural Scaling Laws}, 
      author={Blake Bordelon and Alexander Atanasov and Cengiz Pehlevan},
      year={2024},
      eprint={2402.01092},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{noci2024shaped,
  title={The shaped transformer: Attention models in the infinite depth-and-width limit},
  author={Noci, Lorenzo and Li, Chuning and Li, Mufan and He, Bobby and Hofmann, Thomas and Maddison, Chris J and Roy, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{dinan2023effective,
      title={Effective Theory of Transformers at Initialization}, 
      author={Emily Dinan and Sho Yaida and Susan Zhang},
      year={2023},
      eprint={2304.02034},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{bordelon2022self,
  title={Self-consistent dynamical field theory of kernel evolution in wide neural networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32240--32256},
  year={2022}
}
@article{han2022survey,
  title={A survey on vision transformer},
  author={Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={1},
  pages={87--110},
  year={2022},
  publisher={IEEE}
}

@inproceedings{yang2023feature,
  title={Feature Learning in Infinite Depth Neural Networks},
  author={Yang, Greg and Yu, Dingli and Zhu, Chen and Hayou, Soufiane},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@InProceedings{dehghani2021scenic,
    author    = {Dehghani, Mostafa and Gritsenko, Alexey and Arnab, Anurag and Minderer, Matthias and Tay, Yi},
    title     = {Scenic: A JAX Library for Computer Vision Research and Beyond},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2022},
    pages     = {21393-21398}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{hron2020infinite,
  title={Infinite attention: NNGP and NTK for deep attention networks},
  author={Hron, Jiri and Bahri, Yasaman and Sohl-Dickstein, Jascha and Novak, Roman},
  booktitle={International Conference on Machine Learning},
  pages={4376--4386},
  year={2020},
  organization={PMLR}
}

@article{noci2021precise,
  title={Precise characterization of the prior predictive distribution of deep ReLU networks},
  author={Noci, Lorenzo and Bachmann, Gregor and Roth, Kevin and Nowozin, Sebastian and Hofmann, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20851--20862},
  year={2021}
}

@article{li2022neural,
  title={The neural covariance SDE: Shaped infinite depth-and-width networks at initialization},
  author={Li, Mufan and Nica, Mihai and Roy, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10795--10808},
  year={2022}
}

@article{hayou2023width,
  title={Width and Depth Limits Commute in Residual Networks},
  author={Hayou, Soufiane and Yang, Greg},
  journal={arXiv preprint arXiv:2302.00453},
  year={2023}
}

@article{bordelon2023dynamics,
  title={Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2304.03408},
  year={2023}
}

@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@article{
hayou2023on,
title={On the infinite-depth limit of finite-width neural networks},
author={Soufiane Hayou},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=RbLsYz1Az9},
note={}
}

@article{bordelon2022influence,
  title={The influence of learning rule on representation dynamics in wide neural networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2210.02157},
  year={2022}
}


@article{martin1973statistical,
  title={Statistical dynamics of classical systems},
  author={Martin, Paul Cecil and Siggia, ED and Rose, HA},
  journal={Physical Review A},
  volume={8},
  number={1},
  pages={423},
  year={1973},
  publisher={APS}
}

@inproceedings{hayou2021stable,
  title={Stable resnet},
  author={Hayou, Soufiane and Clerico, Eugenio and He, Bobby and Deligiannidis, George and Doucet, Arnaud and Rousseau, Judith},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1324--1332},
  year={2021},
  organization={PMLR}
}

@article{seroussi2023separation,
  title={Separation of scales and a thermodynamic description of feature learning in some CNNs},
  author={Seroussi, Inbar and Naveh, Gadi and Ringel, Zohar},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={908},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{sompolinsky1981dynamic,
  title={Dynamic theory of the spin-glass phase},
  author={Sompolinsky, Haim and Zippelius, Annette},
  journal={Physical Review Letters},
  volume={47},
  number={5},
  pages={359},
  year={1981},
  publisher={APS}
}

@book{helias2020statistical,
  title={Statistical field theory for neural networks},
  author={Helias, Moritz and Dahmen, David},
  volume={970},
  year={2020},
  publisher={Springer}
}

@inproceedings{mannelli2019passed,
  title={Passed \& spurious: Descent algorithms and local minima in spiked matrix-tensor models},
  author={Mannelli, Stefano Sarao and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborova, Lenka},
  booktitle={international conference on machine learning},
  pages={4333--4342},
  year={2019},
  organization={PMLR}
}


@article{mignacco2020dynamical,
  title={Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification},
  author={Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9540--9550},
  year={2020}
}


@inproceedings{bachlechner2021rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1352--1361},
  year={2021},
  organization={PMLR}
}

@inproceedings{balduzzi2017shattered,
  title={The shattered gradients problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  booktitle={International Conference on Machine Learning},
  pages={342--350},
  year={2017},
  organization={PMLR}
}

@article{yang2021tuning,
  title={Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17084--17097},
  year={2021}
}

@article{segadlo2022unified,
  title={Unified field theoretical approach to deep and recurrent neuronal networks},
  author={Segadlo, Kai and Epping, Bastian and van Meegen, Alexander and Dahmen, David and Kr{\"a}mer, Michael and Helias, Moritz},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2022},
  number={10},
  pages={103401},
  year={2022},
  publisher={IOP Publishing}
}
@article{ito1944109,
  title={109. stochastic integral},
  author={It{\^o}, Kiyosi},
  journal={Proceedings of the Imperial Academy},
  volume={20},
  number={8},
  pages={519--524},
  year={1944},
  publisher={The Japan Academy}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
klug2023scaling,
title={Scaling Laws For Deep Learning Based Image Reconstruction},
author={Tobit Klug and Reinhard Heckel},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=op-ceGueqc4}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12104--12113},
  year={2022}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@book{rudin1953principles,
  title={Principles of mathematical analysis},
  author={Rudin, Walter},
  year={1953}
}

@misc{vyas2023featurelearning,
      title={Feature-Learning Networks Are Consistent Across Widths At Realistic Scales}, 
      author={Nikhil Vyas and Alexander Atanasov and Blake Bordelon and Depen Morwani and Sabarish Sainathan and Cengiz Pehlevan},
      year={2023},
      eprint={2305.18411},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}

@inproceedings{yang2021tensor,
  title={Tensor programs iv: Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  booktitle={International Conference on Machine Learning},
  pages={11727--11737},
  year={2021},
  organization={PMLR}
}

@article{jelassi2023depth,
  title={Depth Dependence of $\mu$P Learning Rates in ReLU MLPs},
  author={Jelassi, Samy and Hanin, Boris and Ji, Ziwei and Reddi, Sashank J and Bhojanapalli, Srinadh and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2305.07810},
  year={2023}
}

@inproceedings{tarnowski2019dynamical,
  title={Dynamical isometry is achieved in residual networks in a universal way for any activation function},
  author={Tarnowski, Wojciech and Warcho{\l}, Piotr and Jastrzebski, Stanis{\l}aw and Tabor, Jacek and Nowak, Maciej},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2221--2230},
  year={2019},
  organization={PMLR}
}

@article{taki2017deep,
  title={Deep residual networks and weight initialization},
  author={Taki, Masato},
  journal={arXiv preprint arXiv:1709.02956},
  year={2017}
}

@article{noci2022signal,
  title={Signal propagation in transformers: Theoretical perspectives and the role of rank collapse},
  author={Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27198--27211},
  year={2022}
}

@article{fischer2023optimal,
  title={Optimal signal propagation in ResNets through residual scaling},
  author={Fischer, Kirsten and Dahmen, David and Helias, Moritz},
  journal={arXiv preprint arXiv:2305.07715},
  year={2023}
}

@article{zhang2019fixup,
  title={Fixup initialization: Residual learning without normalization},
  author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal={arXiv preprint arXiv:1901.09321},
  year={2019}
}

@inproceedings{huang2020improving,
  title={Improving transformer optimization through better initialization},
  author={Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle={International Conference on Machine Learning},
  pages={4475--4483},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{hanin2020products,
  title={Products of many large random matrices and gradients in deep neural networks},
  author={Hanin, Boris and Nica, Mihai},
  journal={Communications in Mathematical Physics},
  volume={376},
  number={1},
  pages={287--322},
  year={2020},
  publisher={Springer}
}


@article{li2021future,
  title={The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization},
  author={Li, Mufan and Nica, Mihai and Roy, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7852--7864},
  year={2021}
}

@article{noci2023shaped,
  title={The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit},
  author={Noci, Lorenzo and Li, Chuning and Li, Mufan Bill and He, Bobby and Hofmann, Thomas and Maddison, Chris and Roy, Daniel M},
  journal={arXiv preprint arXiv:2306.17759},
  year={2023}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@book{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}


@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{hanin2019finite,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Hanin, Boris and Nica, Mihai},
  journal={arXiv preprint arXiv:1909.05989},
  year={2019}
}

@article{yang2020tensor,
  title={Tensor programs ii: Neural tangent kernel for any architecture},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:2006.14548},
  year={2020}
}

@article{gerbelot2022rigorous,
  title={Rigorous dynamical mean field theory for stochastic gradient descent methods},
  author={Gerbelot, Cedric and Troiani, Emanuele and Mignacco, Francesca and Krzakala, Florent and Zdeborova, Lenka},
  journal={arXiv preprint arXiv:2210.06591},
  year={2022}
}

@article{zavatone2021asymptotics,
  title={Asymptotics of representation learning in finite Bayesian neural networks},
  author={Zavatone-Veth, Jacob and Canatar, Abdulkadir and Ruben, Ben and Pehlevan, Cengiz},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24765--24777},
  year={2021}
}

@inproceedings{yaida2020non,
  title={Non-Gaussian processes and neural networks at finite widths},
  author={Yaida, Sho},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={165--192},
  year={2020},
  organization={PMLR}
}

@article{hanin2022correlation,
  title={Correlation functions in random fully connected neural networks at finite width},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:2204.01058},
  year={2022}
}


@article{antognini2019finite,
  title={Finite size corrections for neural network Gaussian processes},
  author={Antognini, Joseph M},
  journal={arXiv preprint arXiv:1908.10030},
  year={2019}
}

@article{li2021statistical,
  title={Statistical mechanics of deep linear neural networks: The backpropagating kernel renormalization},
  author={Li, Qianyi and Sompolinsky, Haim},
  journal={Physical Review X},
  volume={11},
  number={3},
  pages={031059},
  year={2021},
  publisher={APS}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{hanin2018neural,
  title={Which neural net architectures give rise to exploding and vanishing gradients?},
  author={Hanin, Boris},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{zavatone2021exact,
  author={Zavatone-Veth, Jacob and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3364--3375},
  year={2021}
}

@article{hanin2023bayesian,
  title={Bayesian interpolation with deep linear networks},
  author={Hanin, Boris and Zlokapa, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={23},
  pages={e2301345120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{schoenholz2016deep,
  title={Deep information propagation},
  author={Schoenholz, Samuel S and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1611.01232},
  year={2016}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@article{touvron2302llama,
  title={LLaMA: open and efficient foundation language models, 2023},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={URL https://arxiv. org/abs/2302.13971}
}


@article{joudaki2023impact,
  title={On the impact of activation and normalization in obtaining isometric embeddings at initialization},
  author={Joudaki, Amir and Daneshmand, Hadi and Bach, Francis},
  journal={arXiv preprint arXiv:2305.18399},
  year={2023}
}

@article{daneshmand2021batch,
  title={Batch normalization orthogonalizes representations in deep random networks},
  author={Daneshmand, Hadi and Joudaki, Amir and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4896--4906},
  year={2021}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@inproceedings{
bordelon2024depthwise,
title={Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit},
author={Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KZJehvRKGD}
}

@article{yang2017mean,
  title={Mean field residual networks: On the edge of chaos},
  author={Yang, Ge and Schoenholz, Samuel},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radhakrishnan2022feature,
  title={Feature learning in neural networks and kernel machines that recursively learn features},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2212.13881},
  year={2022}
}

@article{gupta2023feature,
  title={On Feature Scaling of Recursive Feature Machines},
  author={Gupta, Arunav and Mishra, Rohit and Luu, William and Bouassami, Mehdi},
  journal={arXiv preprint arXiv:2303.15745},
  year={2023}
}