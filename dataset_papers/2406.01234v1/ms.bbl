\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Lazic, Szepesvari, and
  Weisz]{abbasi2019exploration}
Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz.
\newblock Exploration-enhanced {POLITEX}.
\newblock \emph{arXiv preprint arXiv:1908.10479}, 2019.

\bibitem[Agrawal and Jia(2023)]{agrawal_optimistic_2023}
Shipra Agrawal and Randy Jia.
\newblock Optimistic {Posterior} {Sampling} for {Reinforcement} {Learning}:
  {Worst}-{Case} {Regret} {Bounds}.
\newblock \emph{Mathematics of Operations Research}, 48\penalty0 (1):\penalty0
  363--392, 2023.
\newblock Publisher: INFORMS.

\bibitem[Audibert et~al.(2009)Audibert, Munos, and
  Szepesvári]{audibert_explorationexploitation_2009}
Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári.
\newblock Exploration–exploitation tradeoff using variance estimates in
  multi-armed bandits.
\newblock \emph{Theoretical Computer Science}, 410\penalty0 (19):\penalty0
  1876--1902, 2009.
\newblock Publisher: Elsevier.

\bibitem[Auer and Ortner(2006)]{auer_logarithmic_2006}
Peter Auer and Ronald Ortner.
\newblock Logarithmic {Online} {Regret} {Bounds} for {Undiscounted}
  {Reinforcement} {Learning}.
\newblock \emph{Proceedings of the 19th International Conference on Neural
  Information Processing Systems}, December 2006.

\bibitem[Auer et~al.(2009)Auer, Jaksch, and Ortner]{auer_near-optimal_2009}
Peter Auer, Thomas Jaksch, and Ronald Ortner.
\newblock Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~21. Curran Associates, Inc., 2009.

\bibitem[Azuma(1967)]{azuma_weighted_1967}
Kazuoki Azuma.
\newblock Weighted sums of certain dependent random variables.
\newblock \emph{Tohoku Mathematical Journal}, 19\penalty0 (3):\penalty0 357 --
  367, 1967.
\newblock Publisher: Tohoku University, Mathematical Institute.

\bibitem[Bartlett and Tewari(2009)]{bartlett_regal_2009}
Peter~L. Bartlett and Ambuj Tewari.
\newblock {REGAL}: a regularization based algorithm for reinforcement learning
  in weakly communicating {MDPs}.
\newblock In \emph{Proceedings of the {Twenty}-{Fifth} {Conference} on
  {Uncertainty} in {Artificial} {Intelligence}}, {UAI} '09, pages 35--42,
  Arlington, Virginia, USA, June 2009. AUAI Press.
\newblock ISBN 978-0-9749039-5-8.

\bibitem[Bourel et~al.(2020)Bourel, Maillard, and
  Talebi]{bourel_tightening_2020}
Hippolyte Bourel, Odalric Maillard, and Mohammad~Sadegh Talebi.
\newblock Tightening {Exploration} in {Upper} {Confidence} {Reinforcement}
  {Learning}.
\newblock In Hal~Daum{\'e} III and Aarti Singh, editors, \emph{Proceedings of
  the 37th {International} {Conference} on {Machine} {Learning}}, volume 119 of
  \emph{Proceedings of {Machine} {Learning} {Research}}, pages 1056--1066.
  PMLR, July 2020.

\bibitem[Burnetas and Katehakis(1997)]{burnetas_optimal_1997}
Apostolos Burnetas and Michael Katehakis.
\newblock Optimal {Adaptive} {Policies} for {Markov} {Decision} {Processes}.
\newblock \emph{Mathematics of Operations Research - MOR}, 22:\penalty0
  222--255, February 1997.

\bibitem[Cohen et~al.(2020)Cohen, Lee, and Song]{cohen2020solving}
Michael~B. Cohen, Yin~Tat Lee, and Zhao Song.
\newblock Solving linear programs in the current matrix multiplication time,
  2020.

\bibitem[Filippi et~al.(2010)Filippi, Capp{\'e}, and
  Garivier]{filippi_optimism_2010}
Sarah Filippi, Olivier Capp{\'e}, and Aur{\'e}lien Garivier.
\newblock Optimism in {Reinforcement} {Learning} and {Kullback}-{Leibler}
  {Divergence}.
\newblock \emph{2010 48th Annual Allerton Conference on Communication, Control,
  and Computing (Allerton)}, pages 115--122, September 2010.
\newblock arXiv: 1004.5229.

\bibitem[Fruit(2019)]{fruit_exploration-exploitation_2019}
Ronan Fruit.
\newblock \emph{Exploration-exploitation dilemma in {Reinforcement} {Learning}
  under various form of prior knowledge}.
\newblock {PhD} {Thesis}, Université de Lille 1, Sciences et Technologies;
  CRIStAL UMR 9189, 2019.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and
  Ortner]{fruit_efcient_2018}
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner.
\newblock Efficient {Bias}-{Span}-{Constrained} {Exploration}-{Exploitation} in
  {Reinforcement} {Learning}.
\newblock \emph{Proceedings of the 35 th International Conference on Machine
  Learning}, 2018.

\bibitem[Fruit et~al.(2020)Fruit, Pirotta, and Lazaric]{fruit_improved_2020}
Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric.
\newblock Improved {Analysis} of {UCRL2} with {Empirical} {Bernstein}
  {Inequality}.
\newblock \emph{ArXiv}, abs/2007.05456, 2020.

\bibitem[Jonsson et~al.(2020)Jonsson, Kaufmann, M{\'e}nard, Darwiche~Domingues,
  Leurent, and Valko]{jonsson2020planning}
Anders Jonsson, Emilie Kaufmann, Pierre M{\'e}nard, Omar Darwiche~Domingues,
  Edouard Leurent, and Michal Valko.
\newblock Planning in markov decision processes with gap-dependent sample
  complexity.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1253--1263, 2020.

\bibitem[Ouyang et~al.(2017)Ouyang, Gagrani, Nayyar, and
  Jain]{ouyang_learning_2017}
Yi~Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain.
\newblock Learning {Unknown} {Markov} {Decision} {Processes}: {A} {Thompson}
  {Sampling} {Approach}.
\newblock \emph{arXiv:1709.04570 [cs]}, September 2017.
\newblock arXiv: 1709.04570.

\bibitem[Pardalos and Schnitger(1988)]{pardalos_checking_1988}
Panos~M. Pardalos and Georg Schnitger.
\newblock Checking local optimality in constrained quadratic programming is
  {NP}-hard.
\newblock \emph{Operations Research Letters}, 7:\penalty0 33--35, 1988.

\bibitem[Puterman(1994)]{puterman_markov_1994}
Martin~L. Puterman.
\newblock \emph{Markov {Decision} {Processes}: {Discrete} {Stochastic}
  {Dynamic} {Programming}}.
\newblock Wiley {Series} in {Probability} and {Statistics}. Wiley, 1 edition,
  April 1994.
\newblock ISBN 978-0-471-61977-2 978-0-470-31688-7.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Talebi and Maillard(2018)]{talebi_variance-aware_2018}
Mohammad~Sadegh Talebi and Odalric-Ambrym Maillard.
\newblock Variance-{Aware} {Regret} {Bounds} for {Undiscounted} {Reinforcement}
  {Learning} in {MDPs}.
\newblock \emph{Journal of Machine Learning Research}, pages 1--36, April 2018.
\newblock Publisher: Microtome Publishing.

\bibitem[Tewari and Bartlett(2007)]{tewari_optimistic_2007}
Ambuj Tewari and P.~Bartlett.
\newblock Optimistic {Linear} {Programming} gives {Logarithmic} {Regret} for
  {Irreducible} {MDPs}.
\newblock In \emph{{NIPS}}, 2007.

\bibitem[Theocharous et~al.(2017)Theocharous, Wen, Abbasi-Yadkori, and
  Vlassis]{theocharous2017posterior}
Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, and Nikos Vlassis.
\newblock Posterior sampling for large scale reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.07979}, 2017.

\bibitem[Thompson(1933)]{thompson_likelihood_1933}
William~R Thompson.
\newblock On the {Likelihood} that {One} {Probability} {Exceeds} {Another} in
  {View} of the {Evidence} of {Two} {Samples}.
\newblock \emph{Biometrika}, 25\penalty0 (3-4):\penalty0 285--294, December
  1933.
\newblock ISSN 0006-3444.

\bibitem[Wei et~al.(2020)Wei, Jahromi, Luo, Sharma, and
  Jain]{wei_model-free_2020}
Chen-Yu Wei, Mehdi~Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul
  Jain.
\newblock Model-free {Reinforcement} {Learning} in {Infinite}-horizon
  {Average}-reward {Markov} {Decision} {Processes}.
\newblock In Hal~Daum{\'e} III and Aarti Singh, editors, \emph{Proceedings of
  the 37th {International} {Conference} on {Machine} {Learning}}, volume 119 of
  \emph{Proceedings of {Machine} {Learning} {Research}}, pages 10170--10180.
  PMLR, July 2020.

\bibitem[Zhang and Ji(2019)]{zhang_regret_2019}
Zihan Zhang and Xiangyang Ji.
\newblock Regret {Minimization} for {Reinforcement} {Learning} by {Evaluating}
  the {Optimal} {Bias} {Function}.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d' Alch{\'e}-Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in {Neural} {Information}
  {Processing} {Systems}}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Zhang and Xie(2023)]{zhang_sharper_2023}
Zihan Zhang and Qiaomin Xie.
\newblock Sharper {Model}-free {Reinforcement} {Learning} for {Average}-reward
  {Markov} {Decision} {Processes}.
\newblock In \emph{The {Thirty} {Sixth} {Annual} {Conference} on {Learning}
  {Theory}}, pages 5476--5477. PMLR, 2023.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{zhang_almost_2020}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost {Optimal} {Model}-{Free} {Reinforcement} {Learning} via
  {Reference}-{Advantage} {Decomposition}.
\newblock \emph{arXiv:2004.10019 [cs, stat]}, June 2020.
\newblock arXiv: 2004.10019.

\end{thebibliography}
