\begin{thebibliography}{10}

\bibitem{abdelfattah2021zerocost}
Mohamed~S Abdelfattah, Abhinav Mehrotra, {\L}ukasz Dudziak, and Nicholas~Donald
  Lane.
\newblock Zero-cost proxies for lightweight nas.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{baker2017accelerating}
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik.
\newblock Accelerating neural architecture search using performance prediction.
\newblock {\em arXiv preprint arXiv:1705.10823}, 2017.

\bibitem{bauer2016understanding}
Matthias Bauer, Mark van~der Wilk, and Carl~Edward Rasmussen.
\newblock Understanding probabilistic sparse gaussian process approximations.
\newblock {\em arXiv preprint arXiv:1606.04820}, 2016.

\bibitem{bender2018understanding}
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc
  Le.
\newblock Understanding and simplifying one-shot architecture search.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 550--559, 2018.

\bibitem{bingham2019pyro}
Eli Bingham, Jonathan~P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj
  Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and
  Noah~D Goodman.
\newblock Pyro: Deep universal probabilistic programming.
\newblock {\em The Journal of Machine Learning Research}, 20(1):973--978, 2019.

\bibitem{bishop2006pattern}
Christopher~M Bishop.
\newblock {\em Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem{proxylessnas}
Han Cai, Ligeng Zhu, and Song Han.
\newblock Proxylessnas: Direct neural architecture search on target task and
  hardware.
\newblock {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem{chandrashekaran2017speeding}
Akshay Chandrashekaran and Ian~R Lane.
\newblock Speeding up hyper-parameter optimization by extrapolation of learning
  curves using previous builds.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 477--492. Springer, 2017.

\bibitem{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd acm sigkdd international conference
  on knowledge discovery and data mining}, pages 785--794, 2016.

\bibitem{DownsampledImageNet}
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
\newblock A downsampled variant of imagenet as an alternative to the {CIFAR}
  datasets.
\newblock {\em CoRR}, abs/1707.08819, 2017.

\bibitem{domhan2015speeding}
Tobias Domhan, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Speeding up automatic hyperparameter optimization of deep neural
  networks by extrapolation of learning curves.
\newblock In {\em Twenty-Fourth International Joint Conference on Artificial
  Intelligence}, 2015.

\bibitem{gdas}
Xuanyi Dong and Yi~Yang.
\newblock Searching for a robust neural architecture in four gpu hours.
\newblock In {\em Proceedings of the IEEE Conference on computer vision and
  pattern recognition}, pages 1761--1770, 2019.

\bibitem{nasbench201}
Xuanyi Dong and Yi~Yang.
\newblock Nas-bench-201: Extending the scope of reproducible neural
  architecture search.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{duan2020ngboost}
Tony Duan, Avati Anand, Daisy~Yi Ding, Khanh~K Thai, Sanjay Basu, Andrew Ng,
  and Alejandro Schuler.
\newblock Ngboost: Natural gradient boosting for probabilistic prediction.
\newblock In {\em International Conference on Machine Learning}, pages
  2690--2700. PMLR, 2020.

\bibitem{nas-survey}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock {\em arXiv preprint arXiv:1808.05377}, 2018.

\bibitem{bohb}
Stefan Falkner, Aaron Klein, and Frank Hutter.
\newblock Bohb: Robust and efficient hyperparameter optimization at scale.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem{nasbot}
Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and
  Eric~P Xing.
\newblock Neural architecture search with {Bayesian} optimisation and optimal
  transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2016--2025, 2018.

\bibitem{ke2017lightgbm}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
  Ye, and Tie-Yan Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock {\em Advances in neural information processing systems},
  30:3146--3154, 2017.

\bibitem{kitano1990designing}
Hiroaki Kitano.
\newblock Designing neural networks using genetic algorithms with graph
  generation system.
\newblock {\em Complex systems}, 4(4):461--476, 1990.

\bibitem{lcnet}
Aaron Klein, Stefan Falkner, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Learning curve prediction with {Bayesian} neural networks.
\newblock {\em ICLR 2017}, 2017.

\bibitem{nasbenchnlp}
Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim
  Fedorov, and Evgeny Burnaev.
\newblock Nas-bench-nlp: neural architecture search benchmark for natural
  language processing.
\newblock {\em arXiv preprint arXiv:2006.07116}, 2020.

\bibitem{CIFAR10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock {SNIP}: Single-shot network pruning based on connection sensitivity.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem{li2018system}
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt,
  Benjamin Recht, and Ameet Talwalkar.
\newblock A system for massively parallel hyperparameter tuning.
\newblock In {\em Proceedings of the Conference on Machine Learning Systems},
  2020.

\bibitem{li2020geometry}
Liam Li, Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar.
\newblock Geometry-aware gradient algorithms for neural architecture search.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{randomnas}
Liam Li and Ameet Talwalkar.
\newblock Random search and reproducibility for neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.07638}, 2019.

\bibitem{hyperband}
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet
  Talwalkar.
\newblock Hyperband: A novel bandit-based approach to hyperparameter
  optimization.
\newblock {\em arXiv preprint arXiv:1603.06560}, 2016.

\bibitem{darts+}
Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen
  Zhuang, and Zhenguo Li.
\newblock Darts+: Improved differentiable architecture search with early
  stopping.
\newblock {\em arXiv preprint arXiv:1909.06035}, 2019.

\bibitem{liaw2002classification}
Andy Liaw, Matthew Wiener, et~al.
\newblock Classification and regression by randomforest.
\newblock {\em R news}, 2(3):18--22, 2002.

\bibitem{lindauer2017smac}
Marius Lindauer, Katharina Eggensperger, Matthias Feurer, Stefan Falkner,
  Andr{\'e} Biedenkapp, and Frank Hutter.
\newblock Smac v3: Algorithm configuration in python.
\newblock {\em URL https://github. com/automl/SMAC3}, 2017.

\bibitem{lindauer2019best}
Marius Lindauer and Frank Hutter.
\newblock Best practices for scientific research on neural architecture search.
\newblock {\em arXiv preprint arXiv:1909.02453}, 2019.

\bibitem{darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock {\em arXiv preprint arXiv:1806.09055}, 2018.

\bibitem{luo2020neural}
Renqian Luo, Xu~Tan, Rui Wang, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Neural architecture search with gbdt.
\newblock {\em arXiv preprint arXiv:2007.04785}, 2020.

\bibitem{seminas}
Renqian Luo, Xu~Tan, Rui Wang, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Semi-supervised neural architecture search.
\newblock In {\em Proceedings of the Annual Conference on Neural Information
  Processing Systems (NeurIPS)}, 2020.

\bibitem{luo2018neural}
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Neural architecture optimization.
\newblock In {\em Proceedings of the Annual Conference on Neural Information
  Processing Systems (NeurIPS)}, 2018.

\bibitem{ma2019deep}
Lizheng Ma, Jiaxu Cui, and Bo~Yang.
\newblock Deep neural architecture search with deep graph {Bayesian}
  optimization.
\newblock In {\em 2019 IEEE/WIC/ACM International Conference on Web
  Intelligence (WI)}, pages 500--507. IEEE, 2019.

\bibitem{maziarz2018evolutionary}
Krzysztof Maziarz, Andrey Khorlin, Quentin de~Laroussilhe, and Andrea Gesmundo.
\newblock Evolutionary-neural hybrid agents for architecture search.
\newblock {\em arXiv preprint arXiv:1811.09828}, 2018.

\bibitem{mellor2020neural}
Joseph Mellor, Jack Turner, Amos Storkey, and Elliot~J Crowley.
\newblock Neural architecture search without training.
\newblock {\em arXiv preprint arXiv:2006.04647}, 2020.

\bibitem{mikler2019snip}
Szymon~Jakub Mikler.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock {\em GitHub repository gahaalt/SNIP-pruning}, 2019.

\bibitem{penntreebank}
Tom{\'a}{\v{s}} Mikolov, Martin Karafi{\'a}t, Luk{\'a}{\v{s}} Burget, Jan
  {\v{C}}ernock{\`y}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In {\em Annual conference of the international speech communication
  association}, 2010.

\bibitem{ning2020surgery}
Xuefei Ning, Wenshuo Li, Zixuan Zhou, Tianchen Zhao, Yin Zheng, Shuang Liang,
  Huazhong Yang, and Yu~Wang.
\newblock A surgery of the neural architecture evaluators.
\newblock {\em arXiv preprint arXiv:2008.03064}, 2020.

\bibitem{ning2020generic}
Xuefei Ning, Yin Zheng, Tianchen Zhao, Yu~Wang, and Huazhong Yang.
\newblock A generic graph-based neural architecture encoding scheme for
  predictor-based nas.
\newblock {\em arXiv preprint arXiv:2004.01899}, 2020.

\bibitem{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
  Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock {\em arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{pedregosa2011scikit}
Fabian Pedregosa, Ga{\"e}l Varoquaux, Alexandre Gramfort, Vincent Michel,
  Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
  Weiss, Vincent Dubourg, et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock {\em the Journal of machine Learning research}, 12:2825--2830, 2011.

\bibitem{enas}
Hieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean.
\newblock Efficient neural architecture search via parameter sharing.
\newblock {\em arXiv preprint arXiv:1802.03268}, 2018.

\bibitem{candela05}
Joaquin Qui\~{n}onero Candela and Carl~Edward Rasmussen.
\newblock A unifying view of sparse approximate gaussian process regression.
\newblock {\em J. Mach. Learn. Res.}, 6:1939â€“1959, December 2005.

\bibitem{gpml}
Carl~Edward Rasmussen.
\newblock Gaussian processes in machine learning.
\newblock In {\em Summer School on Machine Learning}, pages 63--71. Springer,
  2003.

\bibitem{rasmussen2003gaussian}
Carl~Edward Rasmussen.
\newblock Gaussian processes in machine learning.
\newblock In {\em Summer School on Machine Learning}, pages 63--71. Springer,
  2003.

\bibitem{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em Proceedings of the aaai conference on artificial
  intelligence}, volume~33, pages 4780--4789, 2019.

\bibitem{ru2020revisiting}
Binxin Ru, Clare Lyle, Lisa Schut, Mark van~der Wilk, and Yarin Gal.
\newblock Revisiting the train loss: an efficient performance estimator for
  neural architecture search.
\newblock {\em arXiv preprint arXiv:2006.04492}, 2020.

\bibitem{nasbowl}
Binxin Ru, Xingchen Wan, Xiaowen Dong, and Michael Osborne.
\newblock Neural architecture search using {Bayesian} optimisation with
  weisfeiler-lehman kernel.
\newblock {\em arXiv preprint arXiv:2006.07556}, 2020.

\bibitem{ruchte2020naslib}
Michael Ruchte, Arber Zela, Julien Siems, Josif Grabocka, and Frank Hutter.
\newblock Naslib: a modular and flexible neural architecture search library,
  2020.

\bibitem{sciuto2019evaluating}
Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat, and Mathieu
  Salzmann.
\newblock Evaluating the search phase of neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.08142}, 2019.

\bibitem{bonas}
Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James Kwok, and Tong Zhang.
\newblock Bridging the gap between sample-based and one-shot neural
  architecture search with bonas.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{nasbench301}
Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik, Margret Keuper, and
  Frank Hutter.
\newblock Nas-bench-301 and the case for surrogate benchmarks for neural
  architecture search.
\newblock {\em arXiv preprint arXiv:2008.09777}, 2020.

\bibitem{snoek2015scalable}
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,
  Narayanan Sundaram, Mostofa Patwary, Mr~Prabhat, and Ryan Adams.
\newblock Scalable {Bayesian} optimization using deep neural networks.
\newblock In {\em International conference on machine learning}, pages
  2171--2180, 2015.

\bibitem{springenberg2016bayesian}
Jost~Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter.
\newblock Bayesian optimization with robust {Bayesian} neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4134--4142, 2016.

\bibitem{stanley2002evolving}
Kenneth~O Stanley and Risto Miikkulainen.
\newblock Evolving neural networks through augmenting topologies.
\newblock {\em Evolutionary computation}, 10(2):99--127, 2002.

\bibitem{sun2020new}
Yanan Sun, Xian Sun, Yuhan Fang, and Gary Yen.
\newblock A new training protocol for performance predictors of evolutionary
  neural architecture search algorithms.
\newblock {\em arXiv preprint arXiv:2008.13187}, 2020.

\bibitem{SweDuvSnoHutOsb13}
Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, and Michael Osborne.
\newblock Raiders of the lost architecture: Kernels for {Bayesian} optimization
  in conditional parameter spaces.
\newblock In {\em NIPS workshop on {Bayesian} Optimization in Theory and
  Practice}, December 2013.

\bibitem{tanaka2020pruning}
Hidenori Tanaka, Daniel Kunin, Daniel~LK Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock {\em arXiv preprint arXiv:2006.05467}, 2020.

\bibitem{theis2018faster}
Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz{\'a}r.
\newblock Faster gaze prediction with dense networks and fisher pruning.
\newblock {\em arXiv preprint arXiv:1801.05787}, 2018.

\bibitem{titsias2009variational}
Michalis Titsias.
\newblock Variational learning of inducing variables in sparse gaussian
  processes.
\newblock In {\em Artificial Intelligence and Statistics}, pages 567--574,
  2009.

\bibitem{wang2019picking}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem{alphax}
Linnan Wang, Yiyang Zhao, Yuu Jinnai, and Rodrigo Fonseca.
\newblock Alphax: exploring neural architectures with deep neural networks and
  monte carlo tree search.
\newblock {\em arXiv preprint arXiv:1805.07440}, 2018.

\bibitem{npenas}
Chen Wei, Chuang Niu, Yiping Tang, and Jimin Liang.
\newblock Npenas: Neural predictor guided evolution for neural architecture
  search.
\newblock {\em arXiv preprint arXiv:2003.12857}, 2020.

\bibitem{wen2019neural}
Wei Wen, Hanxiao Liu, Hai Li, Yiran Chen, Gabriel Bender, and Pieter-Jan
  Kindermans.
\newblock Neural predictor for neural architecture search.
\newblock {\em arXiv preprint arXiv:1912.00848}, 2019.

\bibitem{white2020study}
Colin White, Willie Neiswanger, Sam Nolen, and Yash Savani.
\newblock A study on encodings for neural architecture search.
\newblock In {\em Proceedings of the Annual Conference on Neural Information
  Processing Systems (NeurIPS)}, 2020.

\bibitem{bananas}
Colin White, Willie Neiswanger, and Yash Savani.
\newblock Bananas: {Bayesian} optimization with neural architectures for neural
  architecture search.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem{yang2019evaluation}
Antoine Yang, Pedro~M Esperan{\c{c}}a, and Fabio~M Carlucci.
\newblock Nas evaluation is frustratingly hard.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{nasbench}
Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, and
  Frank Hutter.
\newblock Nas-bench-101: Towards reproducible neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.09635}, 2019.

\bibitem{yu2020train}
Kaicheng Yu, Rene Ranftl, and Mathieu Salzmann.
\newblock How to train your super-net: An analysis of training heuristics in
  weight-sharing nas.
\newblock {\em arXiv preprint arXiv:2003.04276}, 2020.

\bibitem{zela2020understanding}
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and
  Frank Hutter.
\newblock Understanding and robustifying differentiable architecture search.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{zela2020bench}
Arber Zela, Julien Siems, and Frank Hutter.
\newblock Nas-bench-1shot1: Benchmarking and dissecting one-shot neural
  architecture search.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{zhang2020neural}
Yuge Zhang.
\newblock Neural predictor for neural architecture search.
\newblock {\em GitHub repository ultmaster/neuralpredictor.pytorch}, 2020.

\bibitem{zhang2020deeper}
Yuge Zhang, Zejun Lin, Junyang Jiang, Quanlu Zhang, Yujing Wang, Hui Xue, Chen
  Zhang, and Yaming Yang.
\newblock Deeper insights into weight sharing in neural architecture search.
\newblock {\em arXiv preprint arXiv:2001.01431}, 2020.

\bibitem{zhou2020econas}
Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen~Change Loy, Shuai Yi, Xuesen
  Zhang, and Wanli Ouyang.
\newblock Econas: Finding proxies for economical neural architecture search.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11396--11404, 2020.

\bibitem{zoph2017neural}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8697--8710, 2018.

\end{thebibliography}
