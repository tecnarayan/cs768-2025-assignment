% == Intro ==
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{raffel2019t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{rae2021gopher,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{smith2022mtnlg,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{brown2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{guu2020realm,
  title={Realm: Retrieval-augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2002.08909},
  year={2020}
}

% == Related ==
@inproceedings{carlini2021memorization,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@inproceedings{arpit2017memorization,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}

@article{geva2020transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}

@article{bricken2021transformer,
  title={Attention Approximates Sparse Distributed Memory},
  author={Bricken, Trenton and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022}
}

@article{lewis2020pre,
  title={Pre-training via paraphrasing},
  author={Lewis, Mike and Ghazvininejad, Marjan and Ghosh, Gargi and Aghajanyan, Armen and Wang, Sida and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18470--18481},
  year={2020}
}

@article{de2021mention,
  title={Mention Memory: incorporating textual knowledge into Transformers through entity mention attention},
  author={de Jong, Michiel and Zemlyanskiy, Yury and FitzGerald, Nicholas and Sha, Fei and Cohen, William},
  journal={arXiv preprint arXiv:2110.06176},
  year={2021}
}

@article{seo2019real,
  title={Real-time open-domain question answering with dense-sparse phrase index},
  author={Seo, Minjoon and Lee, Jinhyuk and Kwiatkowski, Tom and Parikh, Ankur P and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1906.05807},
  year={2019}
}

@article{lee2020learning,
  title={Learning dense representations of phrases at scale},
  author={Lee, Jinhyuk and Sung, Mujeen and Kang, Jaewoo and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.12624},
  year={2020}
}

% Retrieval
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{xiong2020approximate,
author = {Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold},
title = {Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval},
booktitle = {International Conference on Learning Representations (ICLR)},
year = {2021},
month = {April},
}

@article{ni2021sentencet5,
  title={Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models},
  author={Ni, Jianmo and {\'A}brego, Gustavo Hern{\'a}ndez and Constant, Noah and Ma, Ji and Hall, Keith B and Cer, Daniel and Yang, Yinfei},
  journal={arXiv preprint arXiv:2108.08877},
  year={2021}
}

@inproceedings{vladimir2020emnlp,
  author    = {Vladimir Karpukhin and
               Barlas Oguz and
               Sewon Min and
               Patrick S. H. Lewis and
               Ledell Wu and
               Sergey Edunov and
               Danqi Chen and
               Wen{-}tau Yih},
  title     = {Dense Passage Retrieval for Open-Domain Question Answering},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
}


% NNS
@article{malkov2018efficient,
  title={Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author={Malkov, Yu A and Yashunin, Dmitry A},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={4},
  pages={824--836},
  year={2018},
  publisher={IEEE}
}

@inproceedings{guo2020icml,
  title={Accelerating large-scale inference with anisotropic vector quantization},
  author={Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  pages={3887--3896},
  year={2020},
  organization={PMLR}
}

@article{johnson2019faiss,
  title={Billion-scale similarity search with gpus},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
  publisher={IEEE}
}

% Methodology

@article{khandelwal2019knnlm,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={ICLR 2020},
  year={2019}
}

@article{lewis2020rag,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{borgeaud2021retro,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2112.04426},
  year={2021}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yogatama2021spalm,
  title={Adaptive Semiparametric Language Models},
  author={Yogatama, Dani and de Masson dâ€™Autume, Cyprien and Kong, Lingpeng},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={362--373},
  year={2021},
  publisher={MIT Press}
}

@article{izacard2020fid,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2020}
}

@article{izacard2020distilling,
  title={Distilling knowledge from reader to retriever for question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2012.04584},
  year={2020}
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@book{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo},
  year={2009},
  publisher={Now Publishers Inc}
}

@article{naturalquestion,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}

@article{cer2018universal,
  title={Universal sentence encoder},
  author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
  journal={arXiv preprint arXiv:1803.11175},
  year={2018}
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{singh2021end,
  title={End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering},
  author={Singh, Devendra and Reddy, Siva and Hamilton, Will and Dyer, Chris and Yogatama, Dani},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

% Experiment 

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

% Analysis

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

% Computation

@ARTICLE{ieee754,
  author={},
  journal={IEEE Std 754-2019 (Revision of IEEE 754-2008)}, 
  title={IEEE Standard for Floating-Point Arithmetic}, 
  year={2019},
  volume={},
  number={},
  pages={1-84},
  doi={10.1109/IEEESTD.2019.8766229}}

%% Unrelated %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@proceedings{yi2019recsys,
title	= {Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations},
editor	= {Xinyang Yi and Ji Yang and Lichan Hong and Derek Zhiyuan Cheng and Lukasz Heldt and Aditee Ajit Kumthekar and Zhe Zhao and Li Wei and Ed Chi},
year	= {2019},
booktitle	= {RecSys 2019}
}

@inproceedings{wu2018cvpr,
  title={Unsupervised feature learning via non-parametric instance discrimination},
  author={Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X and Lin, Dahua},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3733--3742},
  year={2018}
}

@article{jia2021arxiv,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  journal={arXiv preprint arXiv:2102.05918},
  year={2021}
}


@inproceedings{reddi2019stochastic,
  title={Stochastic negative mining for learning with large output spaces},
  author={Reddi, Sashank J and Kale, Satyen and Yu, Felix and Holtmann-Rice, Daniel and Chen, Jiecao and Kumar, Sanjiv},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1940--1949},
  year={2019},
  organization={PMLR}
}

@inproceedings{chen2020icml,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{gao2021arxiv,
  title={Rethink training of BERT rerankers in multi-stage retrieval pipeline},
  author={Gao, Luyu and Dai, Zhuyun and Callan, Jamie},
  journal={arXiv preprint arXiv:2101.08751},
  year={2021}
}

@article{henderson2017efficient,
  title={Efficient natural language response suggestion for smart reply},
  author={Henderson, Matthew and Al-Rfou, Rami and Strope, Brian and Sung, Yun-Hsuan and Luk{\'a}cs, L{\'a}szl{\'o} and Guo, Ruiqi and Kumar, Sanjiv and Miklos, Balint and Kurzweil, Ray},
  journal={arXiv preprint arXiv:1705.00652},
  year={2017}
}

@inproceedings{he2020cvpr,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9729--9738},
  year={2020}
}

@article{rawat2019sampled,
  title={Sampled softmax with random fourier features},
  author={Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix and Suresh, Ananda Theertha and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1907.10747},
  year={2019}
}

@article{mussmann2017fast,
  title={Fast amortized inference and learning in log-linear models with randomly perturbed nearest neighbor search},
  author={Mussmann, Stephen and Levy, Daniel and Ermon, Stefano},
  journal={arXiv preprint arXiv:1707.03372},
  year={2017}
}

@article{bajaj2016ms,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{zhang2021understanding,
  title={Understanding Hard Negatives in Noise Contrastive Estimation},
  author={Zhang, Wenzheng and Stratos, Karl},
  journal={arXiv preprint arXiv:2104.06245},
  year={2021}
}

@article{zhan2021optimizing,
  title={Optimizing Dense Retrieval Model Training with Hard Negatives},
  author={Zhan, Jingtao and Mao, Jiaxin and Liu, Yiqun and Guo, Jiafeng and Zhang, Min and Ma, Shaoping},
  journal={arXiv preprint arXiv:2104.08051},
  year={2021}
}

@inproceedings{gillick2019entity,
    title = "Learning Dense Representations for Entity Retrieval",
    author = "Gillick, Daniel  and
      Kulkarni, Sayali  and
      Lansing, Larry  and
      Presta, Alessandro  and
      Baldridge, Jason  and
      Ie, Eugene  and
      Garcia-Olano, Diego",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
}

@article{bengio2008adaptive,
  title={Adaptive importance sampling to accelerate training of a neural probabilistic language model},
  author={Bengio, Yoshua and Sen{\'e}cal, Jean-S{\'e}bastien},
  journal={IEEE Transactions on Neural Networks},
  volume={19},
  number={4},
  pages={713--722},
  year={2008},
  publisher={IEEE}
}

@article{jean2014using,
  title={On using very large target vocabulary for neural machine translation},
  author={Jean, S{\'e}bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.2007},
  year={2014}
}

@article{vijayanarasimhan2014deep,
  title={Deep networks with large output spaces},
  author={Vijayanarasimhan, Sudheendra and Shlens, Jonathon and Monga, Rajat and Yagnik, Jay},
  journal={arXiv preprint arXiv:1412.7479},
  year={2014}
}

@inproceedings{morin2005hierarchical,
  title={Hierarchical probabilistic neural network language model.},
  author={Morin, Frederic and Bengio, Yoshua},
  booktitle={Aistats},
  volume={5},
  pages={246--252},
  year={2005},
  organization={Citeseer}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1310.4546},
  year={2013}
}

@manual{sptag2018,
  author    = {Qi Chen and
               Haidong Wang and
               Mingqin Li and 
               Gang Ren and
               Scarlett Li and
               Jeffery Zhu and
               Jason Li and
               Chuanjie Liu and
               Lintao Zhang and
               Jingdong Wang},
  title     = {SPTAG: A library for fast approximate nearest neighbor search},
  url       = {https://github.com/Microsoft/SPTAG},
  year      = {2018}
}

@inproceedings{humeau2020iclr,
  title={Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},
  author={Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{craswell2020overview,
  title={Overview of the trec 2019 deep learning track},
  author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},
  journal={arXiv preprint arXiv:2003.07820},
  year={2020}
}

@article{gutmann2012noise,
  title={Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics.},
  author={Gutmann, Michael U and Hyv{\"a}rinen, Aapo},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={2},
  year={2012}
}

@article{bai2017tapas,
  title={Tapas: Two-pass approximate adaptive sampling for softmax},
  author={Bai, Yu and Goldman, Sally and Zhang, Li},
  journal={arXiv preprint arXiv:1707.03073},
  year={2017}
}

@article{mnih2012fast,
  title={A fast and simple algorithm for training neural probabilistic language models},
  author={Mnih, Andriy and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1206.6426},
  year={2012}
}

@article{bromley1993signature,
  title={Signature verification using a" siamese" time delay neural network},
  author={Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S{\"a}ckinger, Eduard and Shah, Roopak},
  journal={Advances in neural information processing systems},
  volume={6},
  pages={737--744},
  year={1993}
}

@inproceedings{joachims2005support,
  title={A support vector method for multivariate performance measures},
  author={Joachims, Thorsten},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={377--384},
  year={2005}
}

@inproceedings{agarwal2011infinite,
  title={The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list},
  author={Agarwal, Shivani},
  booktitle={Proceedings of the 2011 SIAM International Conference on Data Mining},
  pages={839--850},
  year={2011},
  organization={SIAM}
}

@article{boyd2012accuracy,
  title={Accuracy at the top},
  author={Boyd, Stephen and Cortes, Corinna and Mohri, Mehryar and Radovanovic, Ana},
  year={2012}
}

@inproceedings{kar2015surrogate,
  title={Surrogate functions for maximizing precision at the top},
  author={Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},
  booktitle={International Conference on Machine Learning},
  pages={189--198},
  year={2015},
  organization={PMLR}
}

@inproceedings{schroff2015facenet,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}

@inproceedings{shrivastava2016training,
  title={Training region-based object detectors with online hard example mining},
  author={Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={761--769},
  year={2016}
}

@inproceedings{wu2017sampling,
  title={Sampling matters in deep embedding learning},
  author={Wu, Chao-Yuan and Manmatha, R and Smola, Alexander J and Krahenbuhl, Philipp},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2840--2848},
  year={2017}
}

@inproceedings{qu2021rocketqa,
  title={RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering},
  author={Qu, Yingqi and Ding, Yuchen and Liu, Jing and Liu, Kai and Ren, Ruiyang and Zhao, Wayne Xin and Dong, Daxiang and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5835--5847},
  year={2021}
}


@article{zhan2020learning,
  title={Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently},
  author={Zhan, Jingtao and Mao, Jiaxin and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
  journal={arXiv preprint arXiv:2010.10469},
  year={2020}
}
