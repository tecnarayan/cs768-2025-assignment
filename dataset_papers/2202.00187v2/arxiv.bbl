\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbott and Machta(2019)]{abbottScalingLawDiscrete2019}
Michael~C. Abbott and Benjamin~B. Machta.
\newblock A {{Scaling Law From Discrete}} to {{Continuous Solutions}} of
  {{Channel Capacity Problems}} in the {{Low}}-{{Noise Limit}}.
\newblock \emph{Journal of Statistical Physics}, 176\penalty0 (1):\penalty0
  214--227, July 2019.
\newblock ISSN 1572-9613.
\newblock \doi{10.1007/s10955-019-02296-2}.

\bibitem[Alemi(2020)]{alemi2020variational}
Alexander~A Alemi.
\newblock Variational predictive information bottleneck.
\newblock In \emph{Symposium on Advances in Approximate Bayesian Inference},
  pages 1--6. {PMLR}, 2020.

\bibitem[Arimoto(1972)]{arimoto1972algorithm}
Suguru Arimoto.
\newblock An algorithm for computing the capacity of arbitrary discrete
  memoryless channels.
\newblock \emph{IEEE Transactions on Information Theory}, 18\penalty0
  (1):\penalty0 14--20, 1972.

\bibitem[Berger et~al.(1988)Berger, Bernardo, and Mendoza]{berger1988priors}
James~O Berger, Jos~M Bernardo, and Manuel Mendoza.
\newblock \emph{On Priors That Maximize Expected Information}.
\newblock {Purdue University. Department of Statistics}, 1988.

\bibitem[Berger et~al.(2009)Berger, Bernardo, and Sun]{berger2009formal}
James~O Berger, José~M Bernardo, and Dongchu Sun.
\newblock The formal definition of reference priors.
\newblock \emph{The Annals of Statistics}, 37\penalty0 (2):\penalty0 905--938,
  2009.

\bibitem[Bernardo(1979)]{bernardo1979reference}
Jose~M Bernardo.
\newblock Reference posterior distributions for {{Bayesian}} inference.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 41\penalty0 (2):\penalty0 113--128, 1979.

\bibitem[Berthelot et~al.(2019{\natexlab{a}})Berthelot, Carlini, Cubuk,
  Kurakin, Sohn, Zhang, and Raffel]{berthelot2019remixmatch}
David Berthelot, Nicholas Carlini, Ekin~D Cubuk, Alex Kurakin, Kihyuk Sohn, Han
  Zhang, and Colin Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution alignment and
  augmentation anchoring.
\newblock \emph{arXiv preprint arXiv:1911.09785}, 2019{\natexlab{a}}.

\bibitem[Berthelot et~al.(2019{\natexlab{b}})Berthelot, Carlini, Goodfellow,
  Papernot, Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock {{MixMatch}}: {{A}} holistic approach to semi-supervised learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Bialek et~al.(2001)Bialek, Nemenman, and
  Tishby]{bialek2001predictability}
William Bialek, Ilya Nemenman, and Naftali Tishby.
\newblock Predictability, complexity, and learning.
\newblock \emph{Neural computation}, 13\penalty0 (11):\penalty0 2409--2463,
  2001.

\bibitem[Blahut(1972)]{blahut1972computation}
Richard Blahut.
\newblock Computation of channel capacity and rate-distortion functions.
\newblock \emph{IEEE transactions on Information Theory}, 18\penalty0
  (4):\penalty0 460--473, 1972.

\bibitem[Bousquet et~al.(2003)Bousquet, Boucheron, and
  Lugosi]{bousquet2003introduction}
Olivier Bousquet, Stéphane Boucheron, and Gábor Lugosi.
\newblock Introduction to statistical learning theory.
\newblock In \emph{Summer School on Machine Learning}, pages 169--207.
  {Springer}, 2003.

\bibitem[Clarke and Barron(1994)]{clarke1994jeffreys}
Bertrand~S Clarke and Andrew~R Barron.
\newblock Jeffreys' prior is asymptotically least favorable under entropy risk.
\newblock \emph{Journal of Statistical planning and Inference}, 41\penalty0
  (1):\penalty0 37--60, 1994.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem[Davatzikos(2019)]{davatzikos2019machine}
Christos Davatzikos.
\newblock Machine learning in neuroimaging: {{Progress}} and challenges.
\newblock \emph{NeuroImage}, 197:\penalty0 652, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {{BERT}}: {{Pre}}-training of deep bidirectional transformers for
  language understanding.
\newblock In \emph{{{NAACL HLT}}}, 2019.

\bibitem[Dhillon et~al.(2020)Dhillon, Chaudhari, Ravichandran, and
  Soatto]{dhillon2019a}
Guneet~S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto.
\newblock A baseline for few-shot image classification.
\newblock In \emph{Proc. of {{International Conference}} of {{Learning}} and
  {{Representations}} ({{ICLR}})}, 2020.

\bibitem[Gao and Chaudhari(2020)]{gao2020free}
Yansong Gao and Pratik Chaudhari.
\newblock A free-energy principle for representation learning.
\newblock In \emph{Proc. of {{International Conference}} of {{Machine
  Learning}} ({{ICML}})}, 2020.

\bibitem[Grandvalet et~al.(2005)Grandvalet, Bengio, et~al.]{grandvalet2005semi}
Yves Grandvalet, Yoshua Bengio, et~al.
\newblock Semi-supervised learning by entropy minimization.
\newblock \emph{CAP}, 367:\penalty0 281--296, 2005.

\bibitem[Kim et~al.(2021)Kim, Choo, Kwon, Joe, Min, and Gwon]{kim2021selfmatch}
Byoungjip Kim, Jinho Choo, Yeong-Dae Kwon, Seongho Joe, Seungjai Min, and
  Youngjune Gwon.
\newblock Selfmatch: Combining contrastive self-supervision and consistency for
  semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2101.06480}, 2021.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikovbigtransferbit2020}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big {{Transfer}} ({{BiT}}): {{General Visual Representation
  Learning}}.
\newblock \emph{arXiv:1912.11370 [cs]}, May 2020.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A.~Krizhevsky.
\newblock \emph{Learning Multiple Layers of Features from Tiny Images}.
\newblock PhD thesis, Computer Science, University of Toronto, 2009.

\bibitem[Laughlin(1981)]{laughlin1981simple}
Simon Laughlin.
\newblock A simple coding procedure enhances a neuron's information capacity.
\newblock \emph{Zeitschrift für Naturforschung c}, 36\penalty0
  (9-10):\penalty0 910--912, 1981.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
Dong-Hyun Lee et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML},
  volume~3, page 896, 2013.

\bibitem[Li and Turner(2016)]{liEnyiDivergenceVariational2016}
Yingzhen Li and Richard~E. Turner.
\newblock R\textbackslash 'enyi {{Divergence Variational Inference}}.
\newblock \emph{arXiv:1602.02311 [cs, stat]}, October 2016.

\bibitem[Mattingly et~al.(2018)Mattingly, Transtrum, Abbott, and
  Machta]{mattingly2018maximizing}
Henry~H Mattingly, Mark~K Transtrum, Michael~C Abbott, and Benjamin~B Machta.
\newblock Maximizing the information learned from finite data selects a simple
  model.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (8):\penalty0 1760--1765, 2018.

\bibitem[Mayer et~al.(2015)Mayer, Balasubramanian, Mora, and
  Walczak]{mayer2015well}
Andreas Mayer, Vijay Balasubramanian, Thierry Mora, and Aleksandra~M Walczak.
\newblock How a well-adapted immune system is organized.
\newblock \emph{Proceedings of the National Academy of Sciences}, 112\penalty0
  (19):\penalty0 5950--5955, 2015.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Nalisnick and Smyth(2017)]{nalisnick2017variational}
Eric Nalisnick and Padhraic Smyth.
\newblock Variational reference priors.
\newblock 2017.

\bibitem[Quinn et~al.(2019)Quinn, Clement, De~Bernardis, Niemack, and
  Sethna]{Quinn13762}
Katherine~N. Quinn, Colin~B. Clement, Francesco De~Bernardis, Michael~D.
  Niemack, and James~P. Sethna.
\newblock Visualizing probabilistic models and data with intensive principal
  component analysis.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (28):\penalty0 13762--13767, 2019.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.1817218116}.
\newblock URL \url{https://www.pnas.org/content/116/28/13762}.

\bibitem[Ramesh and Chaudhari(2022)]{rameshModelZooGrowing2022}
Rahul Ramesh and Pratik Chaudhari.
\newblock Model {{Zoo}}: {{A Growing}} "{{Brain}}" {{That Learns Continually}}.
\newblock In \emph{Proc. of {{International Conference}} of {{Learning}} and
  {{Representations}} ({{ICLR}})}, 2022.

\bibitem[Sajjadi et~al.(2016)Sajjadi, Javanmardi, and
  Tasdizen]{sajjadi2016mutual}
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
\newblock Mutual exclusivity loss for semi-supervised deep learning.
\newblock In \emph{2016 IEEE International Conference on Image Processing
  (ICIP)}, pages 1908--1912. IEEE, 2016.

\bibitem[Smith(1971)]{smith1971information}
Joel~G Smith.
\newblock The information capacity of amplitude-and variance-constrained sclar
  {{Gaussian}} channels.
\newblock \emph{Information and control}, 18\penalty0 (3):\penalty0 203--219,
  1971.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,
  Colin~A Raffel, Ekin~Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.
\newblock {{FixMatch}}: {{Simplifying}} semi-supervised learning with
  consistency and confidence.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock \emph{arXiv preprint arXiv:1703.01780}, 2017.

\bibitem[Tishby et~al.(1999)Tishby, Pereira, and
  Bialek]{tishbyInformationBottleneckMethod1999}
Naftali Tishby, Fernando~C. Pereira, and William Bialek.
\newblock The information bottleneck method.
\newblock In \emph{Proc. of the 37-Th Annual Allerton Conference on
  Communication, Control and Computing}, pages 368--377, 1999.

\bibitem[Xu et~al.(2021)Xu, Shang, Ye, Qian, Li, Sun, Li, and Jin]{xu2021dash}
Yi~Xu, Lei Shang, Jinxing Ye, Qi~Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong
  Jin.
\newblock Dash: Semi-supervised learning with dynamic thresholding.
\newblock In \emph{International Conference on Machine Learning}, pages
  11525--11536. PMLR, 2021.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference 2016}. {British Machine
  Vision Association}, 2016.

\bibitem[Zhang(1994)]{zhang1994discrete}
Zhongxin Zhang.
\newblock \emph{Discrete noninformative priors}.
\newblock PhD thesis, Yale University, 1994.

\bibitem[Zhou and Li(2010)]{zhou2010semi}
Zhi-Hua Zhou and Ming Li.
\newblock Semi-supervised learning by disagreement.
\newblock \emph{Knowledge and Information Systems}, 24\penalty0 (3):\penalty0
  415--439, 2010.

\end{thebibliography}
