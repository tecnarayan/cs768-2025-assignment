\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba \& Caruana(2014)Ba and Caruana]{ba2014deep}
Ba, J. and Caruana, R.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2654--2662, 2014.

\bibitem[Breiman \& Shang(1996)Breiman and Shang]{breiman1996born}
Breiman, L. and Shang, N.
\newblock Born again trees.
\newblock \emph{Available online at: ftp://ftp. stat.berkeley.edu/pub/users/breiman/BAtrees.ps}, 1996.

\bibitem[Breiman et~al.(2001)]{breiman2001statistical}
Breiman, L.
\newblock Statistical modeling: The two cultures (with comments and a rejoinder
  by the author).
\newblock \emph{Statistical Science}, 16\penalty0 (3):\penalty0 199--231, 2001.

\bibitem[Bucilua et~al.(2006)Bucilua, Caruana, and
  Niculescu-Mizil]{bucilua2006model}
Bucilua, C., Caruana, R., and Niculescu-Mizil, A.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  535--541. ACM, 2006.

\bibitem[Chandra et~al.(2007)Chandra, Chaudhary, and
  Kumar]{chandra2007combination}
Chandra, R., Chaudhary, K., and Kumar, A.
\newblock The combination and comparison of neural networks with decision trees
  for wine classification.
\newblock \emph{School of Sciences and Technology, University of Fiji},
  2007.

\bibitem[Chen \& Guestrin(2016)Chen and Guestrin]{chen2016xgboost}
Chen, T. and Guestrin, C.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  785--794. ACM, 2016.

\bibitem[Czarnecki et~al.(2017)Czarnecki, Osindero, Jaderberg, {\'S}wirszcz,
  and Pascanu]{czarnecki2017sobolev}
Czarnecki, W.~M., Osindero, S., Jaderberg, M., {\'S}wirszcz,
  G., and Pascanu, R.
\newblock Sobolev training for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\ 4281--4290, 2017.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017improved}
DeVries, T. and Taylor, G.~W.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv:1708.04552}, 2017.

\bibitem[Dutt et~al.(2017)Dutt, Pellerin, and Quenot]{dutt2017coupled}
Dutt, A., Pellerin, D., and Quenot, G.
\newblock {Coupled Ensembles of Neural Networks}.
\newblock \emph{arXiv:1709.06053}, 2017.

\bibitem[Frosst \& Hinton(2017)Frosst and Hinton]{frosst2017distilling}
Frosst, N. and Hinton, G.
\newblock Distilling a neural network into a soft decision tree.
\newblock \emph{arXiv:1711.09784}, 2017.

\bibitem[Furlanello et~al.(2016)Furlanello, Zhao, Saxe, Itti, and
  Tjan]{furlanello2016active}
Furlanello, T., Zhao, J., Saxe, A.~M., Itti, L., and Tjan,
  B.~S.
\newblock Active long term memory networks.
\newblock \emph{arXiv:1606.02355}, 2016.

\bibitem[Gastaldi(2017)]{gastaldi2017shake}
Gastaldi, X..
\newblock Shake-shake regularization.
\newblock \emph{arXiv:1705.07485}, 2017.

\bibitem[Gatys et~al.(2015)Gatys, Ecker, and Bethge]{gatys2015neural}
Gatys, L.~A., Ecker, A.~S., and Bethge, M.
\newblock A neural algorithm of artistic style.
\newblock \emph{arXiv:1508.06576}, 2015.

\bibitem[Han et~al.(2016)Han, Kim, and Kim]{han2016deep}
Han, D., Kim, J., and Kim, J.
\newblock Deep pyramidal residual networks.
\newblock \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  6307--6315, 2017.

\bibitem[Hansen \& Salamon(1990)Hansen and Salamon]{hansen1990neural}
Hansen, L.~K. and Salamon, P.
\newblock Neural network ensembles.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 12\penalty0 (10):\penalty0 993--1001, 1990.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  630--645.
  Springer, 2016{\natexlab{b}}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv:1503.02531}, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Huang et~al.(2016)Huang, Liu, Weinberger, and van~der
  Maaten]{huang2016densely}
Huang, G., Liu, Z., Weinberger, K.~Q., and van~der Maaten, L.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 4700--4708, 2017.

\bibitem[Huang et~al.(2017)Huang, Li, Pleiss, Liu, Hopcroft, and
  Weinberger]{huang2017snapshot}
Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.~E., and
  Weinberger, K.~Q.
\newblock Snapshot ensembles: Train 1, get M for free.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kim \& Rush(2016)Kim and Rush]{kim2016sequence}
Kim, Y. and Rush, A.~M.
\newblock Sequence-level knowledge distillation.
\newblock \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing}, pp.\  1317--1327, 2016.

\bibitem[Kim et~al.(2016)Kim, Jernite, Sontag, and Rush]{kim2016character}
Kim, Y., Jernite, Y., Sontag, D., and Rush, A.~M.
\newblock Character-aware neural language models.
\newblock In \emph{Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, pp.\  2741--2749. AAAI Press, 2016.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li \& Hoiem(2016)Li and Hoiem]{li2016learning}
Li, Z. and Hoiem, D.
\newblock Learning without forgetting.
\newblock In \emph{European Conference on Computer Vision}, pp.\  614--629.
  Springer, 2016.

\bibitem[Liaw et~al.(2002)Liaw, Wiener, et~al.]{liaw2002classification}
Liaw, A., Wiener, M.
\newblock Classification and regression by randomforest.
\newblock \emph{R News}, 2\penalty0 (3):\penalty0 18--22, 2002.

\bibitem[Lipton(2016)]{lipton2016mythos}
Lipton, Z.~C.
\newblock The mythos of model interpretability.
\newblock \emph{ICML  Workshop  on  Human  Interpretability  in  Machine
Learning, arXiv:1606.03490}, 2016.

\bibitem[Lopez-Paz et~al.(2015)Lopez-Paz, Bottou, Sch{\"o}lkopf, and
  Vapnik]{lopez2015unifying}
Lopez-Paz, D., Bottou, L., Sch{\"o}lkopf, B., and Vapnik,
  V.
\newblock Unifying distillation and privileged information.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock SGDR: Stochastic gradient descent with restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
Marcus, M.~P., Marcinkiewicz, M.~A., and Santorini, B.
\newblock Building a large annotated corpus of English: The penn treebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Merity et~al.(2017)Merity, Keskar, and Socher]{merity2017regularizing}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock Regularizing and optimizing LSTM language models.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, {\v{C}}ernock{\`y},
  and Khudanpur]{mikolov2010recurrent}
Mikolov, T., Karafi{\'a}t, M., Burget, L.,
  {\v{C}}ernock{\`y}, J., and Khudanpur, S.
\newblock Recurrent neural network based language model.
\newblock In \emph{Eleventh Annual Conference of the International Speech
  Communication Association}, 2010.

\bibitem[Minsky(1991)]{minsky1991society}
Minsky, M.
\newblock Society of mind: A response to four reviews.
\newblock \emph{Artificial Intelligence}, 48\penalty0 (3):\penalty0 371--396,
  1991.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
  A.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{IEEE Symposium on Security and Privacy}, pp.\
  582--597. IEEE, 2016.

\bibitem[Pechyony \& Vapnik(2010)Pechyony and Vapnik]{pechyony2010theory}
Pechyony, D. and Vapnik, V.
\newblock On the theory of learning with privileged information.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1894--1902, 2010.

\bibitem[Press \& Wolf(2016)Press and Wolf]{press2016using}
Press, O. and Wolf, L.
\newblock Using the output embedding to improve language models.
\newblock In \emph{Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics}, pp.\ 157--163, 2016.

\bibitem[Romero et~al.(2014)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
Romero, A., Ballas, N., Kahou, S.~E., Chassang, A.,
  Gatta, C., and Bengio, Y.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{arXiv:1412.6550}, 2014.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma,
  S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C. and Fei-Fei, L.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Rusu et~al.(2015)Rusu, Colmenarejo, Gulcehre, Desjardins, Kirkpatrick,
  Pascanu, Mnih, Kavukcuoglu, and Hadsell]{rusu2015policy}
Rusu, A.~A., Colmenarejo, S.~G., Gulcehre, C., Desjardins,
  G., Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu,
  K., and Hadsell, R.
\newblock Policy distillation.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Sadowski et~al.(2015)Sadowski, Collado, Whiteson, and
  Baldi]{sadowski2015deep}
Sadowski, P., Collado, J., Whiteson, D., and Baldi, P.
\newblock Deep learning, dark knowledge, and dark matter.
\newblock In \emph{NIPS 2014 Workshop on High-energy Physics and Machine
  Learning}, pp.\  81--87, 2015.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J.
\newblock Continual learning with deep generative replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\ 2994--3003, 2017.

\bibitem[Tan et~al.(2018)Tan, Caruana, Hooker, and Gordo]{tan2018transparent}
Tan, S., Caruana, R., Hooker, G., and Gordo, A.
\newblock Transparent model distillation.
\newblock \emph{arXiv:1801.08640}, 2018.

\bibitem[Urban et~al.(2016)Urban, Geras, Kahou, Aslan, Wang, Caruana, Mohamed,
  Philipose, and Richardson]{urban2016deep}
Urban, G., Geras, K.~J., Kahou, S.~E., Aslan, O., Wang,
  S., Caruana, R., Mohamed, A., Philipose, M., and
  Richardson, M.
\newblock Do deep convolutional nets really need to be deep and convolutional?
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Xie et~al.(2016)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2016aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 5987--5995, 2017.

\bibitem[Yamada et~al.(2018)Yamada, Iwamura, and Kise]{anonymous2018shakedrop}
Yamada, Y., Iwamura, M., and Kise, K.
\newblock {ShakeDrop regularization}.
\newblock \emph{arXiv:1802.02375}, 2018.

\bibitem[Yamada et~al.(2016)Yamada, Iwamura, and Kise]{yamada2016deep}
Yamada, Y., Iwamura, M., and Kise, K..
\newblock Deep pyramidal residual networks with separated stochastic depth.
\newblock \emph{arXiv:1612.01230}, 2016.

\bibitem[Yim et~al.(2017)Yim, Joo, Bae, and Kim]{yim2017gift}
Yim, J., Joo, D., Bae, J., and Kim, J.
\newblock A gift from knowledge distillation: Fast optimization, network
  minimization and transfer learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 7130--7138, 2017.

\bibitem[Zagoruyko \& Komodakis(2016{\natexlab{a}})Zagoruyko and
  Komodakis]{zagoruyko2016paying}
Zagoruyko, S. and Komodakis, N.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock In \emph{International Conference on Learning Representations}, 2016{\natexlab{a}}.

\bibitem[Zagoruyko \& Komodakis(2016{\natexlab{b}})Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{Proceedings of the British Machine Vision Conference (BMVC)}, pp.\ 87.1-87.12, 2016{\natexlab{b}}.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Zaremba, W., Sutskever, I., and Vinyals, O.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv:1409.2329}, 2014.

\end{thebibliography}
