\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{Arora18}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{CoRR}, abs/1802.05296, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.05296}.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, and Lacoste{-}Julien]{Arpit17}
Arpit, D., Jastrzebski, S.~K., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A.~C., Bengio, Y., and
  Lacoste{-}Julien, S.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, pp.\
  233--242, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/arpit17a.html}.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{Bartlett17}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  6240--6249. Curran Associates,
  Inc., 2017.

\bibitem[Bernier et~al.(2001)Bernier, Ortega, Ros~Vidal, Rojas, and
  Prieto]{Bernier01}
Bernier, J., Ortega, J., Ros~Vidal, E., Rojas, I., and Prieto, A.
\newblock A quantitative study of fault tolerance, noise immunity, and
  generalization ability of mlps.
\newblock \emph{Neural Computation}, 12:\penalty0 2941--2964, 01 2001.
\newblock \doi{10.1162/089976600300014782}.

\bibitem[Biere(2007)]{Biere07}
Biere, A.
\newblock Aiger library and tools, 2007.
\newblock URL \url{http://fmv.jku.at/aiger}.
\newblock [Online; accessed 1-July-2020].

\bibitem[Chatterjee(2007)]{Chatterjee07}
Chatterjee, S.
\newblock \emph{On Algorithms for Technology Mapping}.
\newblock PhD thesis, EECS Department, University of California, Berkeley, Aug
  2007.
\newblock URL
  \url{http://www2.eecs.berkeley.edu/Pubs/TechRpts/2007/EECS-2007-100.html}.

\bibitem[Chatterjee(2020)]{Chatterjee20}
Chatterjee, S.
\newblock Coherent gradients: An approach to understanding generalization in
  gradient descent-based optimization.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations {ICLR}}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ryeFY0EFwS}.

\bibitem[Dietterich(1998)]{Dietterich98}
Dietterich, T.~G.
\newblock Approximate statistical tests for comparing supervised classification
  learning algorithms.
\newblock \emph{Neural Comput.}, 10\penalty0 (7):\penalty0 1895--1923, October
  1998.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/089976698300017197}.
\newblock URL \url{http://dx.doi.org/10.1162/089976698300017197}.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{Dinh17}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock \emph{CoRR}, abs/1703.04933, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.04933}.

\bibitem[Dwork et~al.(2015)Dwork, Feldman, Hardt, Pitassi, Reingold, and
  Roth]{Dwork15}
Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., and Roth, A.
\newblock The reusable holdout: Preserving validity in adaptive data analysis.
\newblock \emph{Science}, 349\penalty0 (6248):\penalty0 636--638, 2015.
\newblock ISSN 0036-8075.
\newblock \doi{10.1126/science.aaa9375}.
\newblock URL \url{https://science.sciencemag.org/content/349/6248/636}.

\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{lecun10}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock http://yann.lecun.com/exdb/mnist/, 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{Neyshabur18}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{CoRR}, abs/1805.12076, 2018.
\newblock URL \url{http://arxiv.org/abs/1805.12076}.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[{Rangamani} et~al.(2019){Rangamani}, {Nguyen}, {Kumar}, {Phan},
  {Chin}, and {Tran}]{Rangamani19}
{Rangamani}, A., {Nguyen}, N.~H., {Kumar}, A., {Phan}, D., {Chin}, S.~H., and
  {Tran}, T.~D.
\newblock {A Scale Invariant Flatness Measure for Deep Network Minima}.
\newblock \emph{arXiv e-prints}, art. arXiv:1902.02434, Feb 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{Xiao17}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv}, 2017.
\newblock URL \url{https://arxiv.org/abs/1708.07747}.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{Zhang17}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations {ICLR}}, 2017.

\bibitem[Zielinski et~al.(2020)Zielinski, Krishnan, and
  Chatterjee]{Zielinski20}
Zielinski, P., Krishnan, S., and Chatterjee, S.
\newblock Weak and strong gradient directions: Explaining memorization,
  generalization, and hardness of examples at scale.
\newblock \emph{ArXiv}, abs/2003.07422, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.07422}.

\end{thebibliography}
