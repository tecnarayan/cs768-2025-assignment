@incollection{Bengio+chapter2007,
	title        = {Scaling Learning Algorithms Towards {AI}},
	author       = {Bengio, Yoshua and LeCun, Yann},
	year         = 2007,
	booktitle    = {Large Scale Kernel Machines},
	publisher    = {MIT Press}
}
@misc{ammar2024neco,
	title        = {NECO: NEural Collapse Based Out-of-distribution detection},
	author       = {Mouïn Ben Ammar and Nacim Belkhir and Sebastian Popescu and Antoine Manzanera and Gianni Franchi},
	year         = 2024,
	url          = {https://arxiv.org/abs/2310.06823},
	url          = {https://arxiv.org/abs/2310.06823v3},
	eprint       = {2310.06823},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@article{Hinton06,
	title        = {A Fast Learning Algorithm for Deep Belief Nets},
	author       = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
	year         = 2006,
	journal      = {Neural Computation},
	volume       = 18,
	pages        = {1527--1554}
}
@misc{commoncrawl,
  title = {Common Crawl},
  author = {{Common Crawl}},
  year = {2023},
  url = {https://commoncrawl.org},
  note = {Accessed: 2023-10-30}
}
@article{zhu2015aligning,
  title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
  author={Zhu, Yao and Yu, Zhen and Zhang, Chao and Wu, Yijia and others},
  journal={arXiv preprint arXiv:1506.05829},
  year={2015}
}
@article{merity2016pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Sutskever, Ilya and Kornblith, Simon and Goyal, Nikhil},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}
@inproceedings{NEURIPS2023_a85b405e,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {53728--53741},
 publisher = {Curran Associates, Inc.},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{ziegler2020finetuninglanguagemodelshuman,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

@misc{yin2024entropy,
	title        = {Entropy Law: The Story Behind Data Compression and LLM Performance},
	author       = {Mingjia Yin and Chuhan Wu and Yufei Wang and Hao Wang and Wei Guo and Yasheng Wang and Yong Liu and Ruiming Tang and Defu Lian and Enhong Chen},
	year         = 2024,
	eprint       = {2407.06645},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{dubey2024llama3herdmodels,
	title        = {The Llama 3 Herd of Models},
	author       = {A. Dubey et al. (101 additional authors)},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.21783},
	note         = {All authors were affiliated with Meta},
	eprint       = {2407.21783},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@misc{du2024understandingemergentabilitieslanguage,
	title        = {Understanding Emergent Abilities of Language Models from the Loss Perspective},
	author       = {Zhengxiao Du and Aohan Zeng and Yuxiao Dong and Jie Tang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2403.15796},
	eprint       = {2403.15796},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{huang2024compression,
	title        = {Compression Represents Intelligence Linearly},
	author       = {Yuzhen Huang and Jinghan Zhang and Zifei Shan and Junxian He},
	year         = 2024,
	booktitle    = {First Conference on Language Modeling},
	url          = {https://openreview.net/forum?id=SHMj84U5SH}
}
@article{hendryckstest2021,
	title        = {Measuring Massive Multitask Language Understanding},
	author       = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
	year         = 2021,
	journal      = {Proceedings of the International Conference on Learning Representations (ICLR)}
}
@article{srivastava2023beyond,
	title        = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
	author       = {BIG-bench authors},
	year         = 2023,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=uyTL5Bvosj},
	note         = {}
}
@inproceedings{zhong2023understanding,
	title        = {Understanding imbalanced semantic segmentation through neural collapse},
	author       = {Zhong, Zhisheng and Cui, Jiequan and Yang, Yibo and Wu, Xiaoyang and Qi, Xiaojuan and Zhang, Xiangyu and Jia, Jiaya},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {19550--19560}
}
@article{rao1948utilization,
	title        = {The utilization of multiple measurements in problems of biological classification},
	author       = {Rao, C Radhakrishna},
	year         = 1948,
	journal      = {Journal of the Royal Statistical Society. Series B (Methodological)},
	publisher    = {JSTOR},
	volume       = 10,
	number       = 2,
	pages        = {159--203}
}
@misc{ethayarajh2019contextual,
	title        = {How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
	author       = {Kawin Ethayarajh},
	year         = 2019,
	url          = {https://arxiv.org/abs/1909.00512},
	url          = {https://arxiv.org/abs/1909.00512v1},
	eprint       = {1909.00512},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{yang2018breaking,
	title        = {Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},
	author       = {Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
	year         = 2018,
	url          = {https://arxiv.org/abs/1711.03953},
	url          = {https://arxiv.org/abs/1711.03953v4},
	eprint       = {1711.03953},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{zhang2022opt,
	title        = {OPT: Open Pre-trained Transformer Language Models},
	author       = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
	year         = 2022,
	url          = {https://arxiv.org/abs/2205.01068v4},
	url          = {https://arxiv.org/abs/2205.01068},
	eprint       = {2205.01068},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{jiang2023mistral,
	title        = {Mistral 7B},
	author       = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.06825v1},
	url          = {https://arxiv.org/abs/2310.06825},
	eprint       = {2310.06825},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{touvron2023llama,
	title        = {LLaMA: Open and Efficient Foundation Language Models},
	author       = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year         = 2023,
	url          = {https://arxiv.org/abs/2302.13971v1},
	url          = {https://arxiv.org/abs/2302.13971},
	eprint       = {2302.13971},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{ba2016layer,
	title        = {Layer Normalization},
	author       = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
	year         = 2016,
	url          = {https://arxiv.org/abs/1607.06450v1},
	url          = {https://arxiv.org/abs/1607.06450},
	eprint       = {1607.06450},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@book{goodfellow2016deep,
	title        = {Deep learning},
	author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	year         = 2016,
	publisher    = {MIT Press},
	volume       = 1
}
@inproceedings{biderman2023pythia,
	title        = {Pythia: A suite for analyzing large language models across training and scaling},
	author       = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
	year         = 2023,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2397--2430},
	organization = {PMLR}
}
@article{papyan2020prevalence,
	title        = {Prevalence of neural collapse during the terminal phase of deep learning training},
	author       = {Vardan Papyan and X. Y. Han and David L. Donoho},
	year         = 2020,
	month        = {sep},
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {Proceedings of the National Academy of Sciences},
	volume       = 117,
	number       = 40,
	pages        = {24652--24663},
	doi          = {10.1073/pnas.2015509117},
	url          = {https://www.pnas.org/doi/full/10.1073/pnas.2015509117}
}
@misc{arous2023highdimensional,
	title        = {High-dimensional SGD aligns with emerging outlier eigenspaces},
	author       = {Gerard Ben Arous and Reza Gheissari and Jiaoyang Huang and Aukosh Jagannath},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.03010v1},
	url          = {https://arxiv.org/abs/2310.03010},
	eprint       = {2310.03010},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{hoyt2021probing,
	title        = {Probing neural networks with t-SNE, class-specific projections and a guided tour},
	author       = {Christopher R. Hoyt and Art B. Owen},
	year         = 2021,
	url          = {https://arxiv.org/abs/2107.12547v1},
	url          = {https://arxiv.org/abs/2107.12547},
	eprint       = {2107.12547},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{liu2023overtraining,
	title        = {Over-Training with Mixup May Hurt Generalization},
	author       = {Zixuan Liu and Ziqiao Wang and Hongyu Guo and Yongyi Mao},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=JmkjrlVE-DG}
}
@article{DBLP:journals/corr/abs-1710-09412,
	title        = {mixup: Beyond Empirical Risk Minimization},
	author       = {Hongyi Zhang and Moustapha Ciss{\'{e}} and Yann N. Dauphin and David Lopez{-}Paz},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1710.09412},
	url          = {http://arxiv.org/abs/1710.09412},
	url          = {https://arxiv.org/abs/1710.09412v2},
	eprinttype   = {arXiv},
	eprint       = {1710.09412},
	timestamp    = {Mon, 13 Aug 2018 16:47:14 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1710-09412.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{natarajan2013learning,
	title        = {Learning with noisy labels},
	author       = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
	year         = 2013,
	journal      = {Advances in neural information processing systems},
	volume       = 26
}
@misc{thulasidasan2020mixup,
	title        = {On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks},
	author       = {Sunil Thulasidasan and Gopinath Chennupati and Jeff Bilmes and Tanmoy Bhattacharya and Sarah Michalak},
	year         = 2020,
	url          = {https://arxiv.org/abs/1905.11001v5},
	url          = {https://arxiv.org/abs/1905.11001},
	eprint       = {1905.11001},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{zhang2022mixup,
	title        = {When and How Mixup Improves Calibration},
	author       = {Linjun Zhang and Zhun Deng and Kenji Kawaguchi and James Zou},
	year         = 2022,
	url          = {https://arxiv.org/abs/2102.06289v3},
	url          = {https://arxiv.org/abs/2102.06289},
	eprint       = {2102.06289},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{zhang2021how,
	title        = {How Does Mixup Help With Robustness and Generalization?},
	author       = {Linjun Zhang and Zhun Deng and Kenji Kawaguchi and Amirata Ghorbani and James Zou},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=8yKEo06dKNo}
}
@misc{carratino2022mixup,
	title        = {On Mixup Regularization},
	author       = {Luigi Carratino and Moustapha Cissé and Rodolphe Jenatton and Jean-Philippe Vert},
	year         = 2022,
	url          = {https://arxiv.org/abs/2006.06049v3},
	url          = {https://arxiv.org/abs/2006.06049},
	eprint       = {2006.06049},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{zou2023benefits,
	title        = {The Benefits of Mixup for Feature Learning},
	author       = {Difan Zou and Yuan Cao and Yuanzhi Li and Quanquan Gu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2303.08433v1},
	url          = {https://arxiv.org/abs/2303.08433},
	eprint       = {2303.08433},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{fisher2024pushing,
	title        = {Pushing Boundaries: Mixup's Influence on Neural Collapse},
	author       = {Quinn Fisher and Haoming Meng and Vardan Papyan},
	year         = 2024,
	url          = {https://arxiv.org/abs/2402.06171v1},
	url          = {https://arxiv.org/abs/2402.06171},
	eprint       = {2402.06171},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{verma2019manifold,
	title        = {Manifold Mixup: Better Representations by Interpolating Hidden States},
	author       = {Vikas Verma and Alex Lamb and Christopher Beckham and Amir Najafi and Ioannis Mitliagkas and Aaron Courville and David Lopez-Paz and Yoshua Bengio},
	year         = 2019,
	url          = {https://arxiv.org/abs/1806.05236v7},
	url          = {https://arxiv.org/abs/1806.05236},
	eprint       = {1806.05236},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{yun2019cutmix,
	title        = {CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features},
	author       = {Sangdoo Yun and Dongyoon Han and Seong Joon Oh and Sanghyuk Chun and Junsuk Choe and Youngjoon Yoo},
	year         = 2019,
	url          = {https://arxiv.org/abs/1905.04899v2},
	url          = {https://arxiv.org/abs/1905.04899},
	eprint       = {1905.04899},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{wang2024progressive,
	title        = {Progressive Feedforward Collapse of ResNet Training},
	author       = {Sicong Wang and Kuo Gai and Shihua Zhang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2405.00985v1},
	url          = {https://arxiv.org/abs/2405.00985},
	eprint       = {2405.00985},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{wang2024neural,
	title        = {Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning},
	author       = {Chendi Wang and Yuqing Zhu and Weijie J. Su and Yu-Xiang Wang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2405.08920v2},
	url          = {https://arxiv.org/abs/2405.08920},
	eprint       = {2405.08920},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{guo2024cross,
	title        = {Cross Entropy versus Label Smoothing: A Neural Collapse Perspective},
	author       = {Li Guo and Keith Ross and Zifan Zhao and George Andriopoulos and Shuyang Ling and Yufeng Xu and Zixuan Dong},
	year         = 2024,
	url          = {https://arxiv.org/abs/2402.03979v2},
	url          = {https://arxiv.org/abs/2402.03979},
	eprint       = {2402.03979},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{tropp2006just,
	title        = {Just relax: Convex programming methods for identifying sparse signals in noise},
	author       = {Tropp, Joel A},
	year         = 2006,
	journal      = {IEEE transactions on information theory},
	publisher    = {IEEE},
	volume       = 52,
	number       = 3,
	pages        = {1030--1051}
}
@misc{kim2020puzzle,
	title        = {Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup},
	author       = {Jang-Hyun Kim and Wonho Choo and Hyun Oh Song},
	year         = 2020,
	url          = {https://arxiv.org/abs/2009.06962v2},
	url          = {https://arxiv.org/abs/2009.06962},
	eprint       = {2009.06962},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{DBLP:journals/corr/abs-2110-07647,
	title        = {Towards Understanding the Data Dependency of Mixup-style Training},
	author       = {Muthu Chidambaram and Xiang Wang and Yuzheng Hu and Chenwei Wu and Rong Ge},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2110.07647},
	url          = {https://arxiv.org/abs/2110.07647},
	url          = {https://arxiv.org/abs/2110.07647v3},
	eprinttype   = {arXiv},
	eprint       = {2110.07647},
	timestamp    = {Tue, 26 Apr 2022 16:10:18 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2110-07647.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{park2022unified,
	title        = {A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective},
	author       = {Chanwoo Park and Sangdoo Yun and Sanghyuk Chun},
	year         = 2022,
	url          = {https://arxiv.org/abs/2208.09913v1},
	eprint       = {2208.09913},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{chidambaram2023provably,
	title        = {Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup},
	author       = {Muthu Chidambaram and Xiang Wang and Chenwei Wu and Rong Ge},
	year         = 2023,
	url          = {https://arxiv.org/abs/2210.13512v3},
	eprint       = {2210.13512},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}
@inproceedings{8877390,
	title        = {Bfloat16 Processing for Neural Networks},
	author       = {Burgess, Neil and Milanovic, Jelena and Stephens, Nigel and Monachopoulos, Konstantinos and Mansell, David},
	year         = 2019,
	booktitle    = {2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)},
	volume       = {},
	number       = {},
	pages        = {88--91},
	doi          = {10.1109/ARITH.2019.00022},
	keywords     = {Artificial neural networks;Computer architecture;Training;Error analysis;Digital arithmetic;Standards;floating-point, rounding mode, neural networks}
}
@misc{chaudhry2022does,
	title        = {When does mixup promote local linearity in learned representations?},
	author       = {Arslan Chaudhry and Aditya Krishna Menon and Andreas Veit and Sadeep Jayasumana and Srikumar Ramalingam and Sanjiv Kumar},
	year         = 2022,
	url          = {https://arxiv.org/abs/2210.16413v1},
	eprint       = {2210.16413},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{yang2022inducing,
	title        = {Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?},
	author       = {Yibo Yang and Shixiang Chen and Xiangtai Li and Liang Xie and Zhouchen Lin and Dacheng Tao},
	year         = 2022,
	url          = {https://arxiv.org/abs/2203.09081v3},
	eprint       = {2203.09081},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{fisher1936use,
	title        = {The use of multiple measurements in taxonomic problems},
	author       = {Fisher, Ronald A},
	year         = 1936,
	journal      = {Annals of eugenics},
	publisher    = {Wiley Online Library},
	volume       = 7,
	number       = 2,
	pages        = {179--188}
}
@misc{zagoruyko2017wide,
	title        = {Wide Residual Networks},
	author       = {Sergey Zagoruyko and Nikos Komodakis},
	year         = 2017,
	url          = {https://arxiv.org/abs/1605.07146v4},
	eprint       = {1605.07146},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{dosovitskiy2021image,
	title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year         = 2021,
	url          = {https://arxiv.org/abs/2010.11929v2},
	eprint       = {2010.11929},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@inproceedings{langley00,
	title        = {Crafting Papers on Machine Learning},
	author       = {P. Langley},
	year         = 2000,
	booktitle    = {Proceedings of the 17th International Conference on Machine Learning (ICML 2000)},
	publisher    = {Morgan Kaufmann},
	address      = {Stanford, CA},
	pages        = {1207--1216},
	editor       = {Pat Langley}
}
@misc{papyan2020traces,
	title        = {Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra},
	author       = {Vardan Papyan},
	year         = 2020,
	url          = {https://arxiv.org/abs/2008.11865v1},
	eprint       = {2008.11865},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{wang2019glue,
	title        = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	author       = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
	year         = 2019,
	url          = {https://arxiv.org/abs/1804.07461v3},
	eprint       = {1804.07461},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{highwayne,
	title        = {Highway Networks},
	author       = {Rupesh Kumar Srivastava and Klaus Greff and J{\"{u}}rgen Schmidhuber},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1505.00387},
	url          = {http://arxiv.org/abs/1505.00387},
	url          = {https://arxiv.org/abs/1505.00387v2},
	eprinttype   = {arXiv},
	eprint       = {1505.00387},
	timestamp    = {Mon, 13 Aug 2018 16:48:21 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/SrivastavaGS15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@techreport{mitchell80,
	title        = {The Need for Biases in Learning Generalizations},
	author       = {T. M. Mitchell},
	year         = 1980,
	address      = {New Brunswick, MA},
	institution  = {Computer Science Department, Rutgers University}
}
@phdthesis{kearns89,
	title        = {Computational Complexity of Machine Learning},
	author       = {M. J. Kearns},
	year         = 1989,
	school       = {Department of Computer Science, Harvard University}
}
@book{MachineLearningI,
	title        = {Machine Learning: An Artificial Intelligence Approach, Vol. I},
	year         = 1983,
	publisher    = {Tioga},
	address      = {Palo Alto, CA},
	editor       = {R. S. Michalski and J. G. Carbonell and T. M. Mitchell}
}
@book{DudaHart2nd,
	title        = {Pattern Classification},
	author       = {R. O. Duda and P. E. Hart and D. G. Stork},
	year         = 2000,
	publisher    = {John Wiley and Sons},
	edition      = {2nd}
}
@incollection{Newell81,
	title        = {Mechanisms of Skill Acquisition and the Law of Practice},
	author       = {A. Newell and P. S. Rosenbloom},
	year         = 1981,
	booktitle    = {Cognitive Skills and Their Acquisition},
	publisher    = {Lawrence Erlbaum Associates, Inc.},
	address      = {Hillsdale, NJ},
	pages        = {1--51},
	editor       = {J. R. Anderson},
	chapter      = 1
}
@article{Samuel59,
	title        = {Some Studies in Machine Learning Using the Game of Checkers},
	author       = {A. L. Samuel},
	year         = 1959,
	journal      = {IBM Journal of Research and Development},
	volume       = 3,
	number       = 3,
	pages        = {211--229}
}
@article{fan1951maximum,
	title        = {Maximum properties and inequalities for the eigenvalues of completely continuous operators},
	author       = {Fan, Ky},
	year         = 1951,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Acad Sciences},
	volume       = 37,
	number       = 11,
	pages        = {760--766}
}
@misc{alain2018understanding,
	title        = {Understanding intermediate layers using linear classifier probes},
	author       = {Guillaume Alain and Yoshua Bengio},
	year         = 2018,
	url          = {https://arxiv.org/abs/1610.01644v4},
	eprint       = {1610.01644},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@inproceedings{ebski2018residual,
	title        = {Residual connections encourage iterative inference},
	author       = {Ebski, Stanis{\l}aw Jastrz and Arpit, Devansh and Ballas, Nicolas and Verma, Vikas and Che, Tong and Bengio, Yoshua},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{mulayoff2020unique,
	title        = {Unique properties of flat minima in deep networks},
	author       = {Mulayoff, Rotem and Michaeli, Tomer},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {7108--7118},
	organization = {PMLR}
}
@inproceedings{behrmann2019invertible,
	title        = {Invertible residual networks},
	author       = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky TQ and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {573--582},
	organization = {PMLR}
}
@article{chen2018neural,
	title        = {Neural ordinary differential equations},
	author       = {Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	volume       = 31
}
@inproceedings{karkar2021principle,
	title        = {A principle of least action for the training of neural networks},
	author       = {Karkar, Skander and Ayed, Ibrahim and B{\'e}zenac, Emmanuel de and Gallinari, Patrick},
	year         = 2021,
	booktitle    = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages        = {101--117},
	organization = {Springer}
}
@misc{hayou2023infinitedepth,
	title        = {On the infinite-depth limit of finite-width neural networks},
	author       = {Soufiane Hayou},
	year         = 2023,
	url          = {https://arxiv.org/abs/2210.00688v3},
	eprint       = {2210.00688},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{li2023neural,
	title        = {The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization},
	author       = {Mufan Bill Li and Mihai Nica and Daniel M. Roy},
	year         = 2023,
	url          = {https://arxiv.org/abs/2206.02768v3},
	eprint       = {2206.02768},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{sander2022residual,
	title        = {Do Residual Neural Networks discretize Neural Ordinary Differential Equations?},
	author       = {Michael E. Sander and Pierre Ablin and Gabriel Peyré},
	year         = 2022,
	url          = {https://arxiv.org/abs/2205.14612v2},
	eprint       = {2205.14612},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{cohen2021scaling,
	title        = {Scaling properties of deep residual networks},
	author       = {Cohen, Alain-Sam and Cont, Rama and Rossier, Alain and Xu, Renyuan},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2039--2048},
	organization = {PMLR}
}
@inproceedings{liaw2021ensemble,
	title        = {Ensemble of One Model: Creating Model Variations for Transformer with Layer Permutation},
	author       = {Liaw, Andrew and Hsu, Jia-Hao and Wu, Chung-Hsien},
	year         = 2021,
	booktitle    = {2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
	pages        = {1026--1030},
	organization = {IEEE}
}
@misc{fan2019reducing,
	title        = {Reducing Transformer Depth on Demand with Structured Dropout},
	author       = {Angela Fan and Edouard Grave and Armand Joulin},
	year         = 2019,
	url          = {https://arxiv.org/abs/1909.11556v1},
	eprint       = {1909.11556},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{hoffman2019robust,
	title        = {Robust Learning with Jacobian Regularization},
	author       = {Judy Hoffman and Daniel A. Roberts and Sho Yaida},
	year         = 2019,
	url          = {https://arxiv.org/abs/1908.02729v1},
	eprint       = {1908.02729},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{novak2018sensitivity,
	title        = {Sensitivity and Generalization in Neural Networks: an Empirical Study},
	author       = {Roman Novak and Yasaman Bahri and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-Dickstein},
	year         = 2018,
	url          = {https://arxiv.org/abs/1802.08760v3},
	eprint       = {1802.08760},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{hong2023neural,
	title        = {Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data},
	author       = {Wanli Hong and Shuyang Ling},
	year         = 2023,
	url          = {https://arxiv.org/abs/2309.09725v2},
	eprint       = {2309.09725},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@article{li2021future,
	title        = {The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization},
	author       = {Li, Mufan and Nica, Mihai and Roy, Dan},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {7852--7864}
}
@misc{gai2021mathematical,
	title        = {A Mathematical Principle of Deep Learning: Learn the Geodesic Curve in the Wasserstein Space},
	author       = {Kuo Gai and Shihua Zhang},
	year         = 2021,
	url          = {https://arxiv.org/abs/2102.09235v2},
	eprint       = {2102.09235},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{Pernici_2022,
	title        = {Regular Polytope Networks},
	author       = {Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Bimbo, Alberto Del},
	year         = 2022,
	month        = sep,
	journal      = {IEEE Transactions on Neural Networks and Learning Systems},
	publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
	volume       = 33,
	number       = 9,
	pages        = {4373–4387},
	doi          = {10.1109/tnnls.2021.3056762},
	issn         = {2162-2388},
	url          = {http://dx.doi.org/10.1109/TNNLS.2021.3056762}
}
@misc{li2017demystifying,
	title        = {Demystifying ResNet},
	author       = {Sihan Li and Jiantao Jiao and Yanjun Han and Tsachy Weissman},
	year         = 2017,
	url          = {https://arxiv.org/abs/1611.01186v2},
	eprint       = {1611.01186},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}
@misc{greff2017highway,
	title        = {Highway and Residual Networks learn Unrolled Iterative Estimation},
	author       = {Klaus Greff and Rupesh K. Srivastava and Jürgen Schmidhuber},
	year         = 2017,
	url          = {https://arxiv.org/abs/1612.07771v3},
	eprint       = {1612.07771},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}
@misc{mixon2020neural,
	title        = {Neural collapse with unconstrained features},
	author       = {Dustin G. Mixon and Hans Parshall and Jianzong Pi},
	year         = 2020,
	url          = {https://arxiv.org/abs/2011.11619v1},
	eprint       = {2011.11619},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{lu2021neural,
	title        = {Neural Collapse with Cross-Entropy Loss},
	author       = {Jianfeng Lu and Stefan Steinerberger},
	year         = 2021,
	url          = {https://arxiv.org/abs/2012.08465v2},
	eprint       = {2012.08465},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{e2021emergence,
	title        = {On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers},
	author       = {Weinan E and Stephan Wojtowytsch},
	year         = 2021,
	url          = {https://arxiv.org/abs/2012.05420v3},
	eprint       = {2012.05420},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{poggio2020explicit,
	title        = {Explicit regularization and implicit bias in deep network classifiers trained with the square loss},
	author       = {Tomaso Poggio and Qianli Liao},
	year         = 2020,
	url          = {https://arxiv.org/abs/2101.00072v1},
	eprint       = {2101.00072},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{galanti2022implicit,
	title        = {On the Implicit Bias Towards Minimal Depth of Deep Neural Networks},
	author       = {Tomer Galanti and Liane Galanti and Ido Ben-Shaul},
	year         = 2022,
	url          = {https://arxiv.org/abs/2202.09028v9},
	eprint       = {2202.09028},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{pmlr-v202-rangamani23a,
	title        = {Feature learning in deep classifiers through Intermediate Neural Collapse},
	author       = {Rangamani, Akshay and Lindegaard, Marius and Galanti, Tomer and Poggio, Tomaso A},
	year         = 2023,
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 202,
	pages        = {28729--28745},
	url          = {https://proceedings.mlr.press/v202/rangamani23a.html},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf},
	abstract     = {In this paper, we conduct an empirical study of the feature learning process in deep classifiers. Recent research has identified a training phenomenon called Neural Collapse (NC), in which the top-layer feature embeddings of samples from the same class tend to concentrate around their means, and the top layer’s weights align with those features. Our study aims to investigate if these properties extend to intermediate layers. We empirically study the evolution of the covariance and mean of representations across different layers and show that as we move deeper into a trained neural network, the within-class covariance decreases relative to the between-class covariance. Additionally, we find that in the top layers, where the between-class covariance is dominant, the subspace spanned by the class means aligns with the subspace spanned by the most significant singular vector components of the weight matrix in the corresponding layer. Finally, we discuss the relationship between NC and Associative Memories (Willshaw et. al. 1969).}
}
@inproceedings{NEURIPS2023_3477ca0c,
	title        = {Neural (Tangent Kernel) Collapse},
	author       = {Seleznova, Mariia and Weitzner, Dana and Giryes, Raja and Kutyniok, Gitta and Chou, Hung-Hsu},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {16240--16270},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3477ca0ce484aa2fa42c1361ab601c25-Paper-Conference.pdf},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@misc{imagenette,
	title        = {{G}it{H}ub - fastai/imagenette: {A} smaller subset of 10 easily classified classes from {I}magenet, and a little more {F}rench --- github.com},
	author       = {Jeremy Howard},
	year         = {},
	note         = {[Accessed 14-May-2023]},
	howpublished = {\url{https://github.com/fastai/imagenette}}
}
@article{algo971,
	title        = {Algorithm 971: An Implementation of a Randomized Algorithm for Principal Component Analysis},
	author       = {Li, Huamin and Linderman, George C. and Szlam, Arthur and Stanton, Kelly P. and Kluger, Yuval and Tygert, Mark},
	year         = 2017,
	month        = {jan},
	journal      = {ACM Trans. Math. Softw.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 43,
	number       = 3,
	doi          = {10.1145/3004053},
	issn         = {0098-3500},
	url          = {https://doi.org/10.1145/3004053},
	issue_date   = {September 2017},
	articleno    = 28,
	numpages     = 14,
	keywords     = {Principal component analysis, singular value decomposition, PCA, SVD}
}
@misc{he2022law,
	title        = {A Law of Data Separation in Deep Learning},
	author       = {Hangfeng He and Weijie J. Su},
	year         = 2022,
	url          = {https://arxiv.org/abs/2210.17020v2},
	eprint       = {2210.17020},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{li2023principled,
	title        = {Principled and Efficient Transfer Learning of Deep Models via Neural Collapse},
	author       = {Xiao Li and Sheng Liu and Jinxin Zhou and Xinyu Lu and Carlos Fernandez-Granda and Zhihui Zhu and Qing Qu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2212.12206v3},
	eprint       = {2212.12206},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{han2021neural,
	title        = {Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path},
	author       = {Han, XY and Papyan, Vardan and Donoho, David L},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{lu2020mean,
	title        = {A mean field analysis of deep resnet and beyond: Towards provably optimization via overparameterization from depth},
	author       = {Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {6426--6436},
	organization = {PMLR}
}
@article{avelin2021neural,
	title        = {Neural ODEs as the deep limit of ResNets with constant weights},
	author       = {Avelin, Benny and Nystr{\"o}m, Kaj},
	year         = 2021,
	journal      = {Analysis and Applications},
	publisher    = {World Scientific},
	volume       = 19,
	number       = {03},
	pages        = {397--437}
}
@article{veit2016residual,
	title        = {Residual networks behave like ensembles of relatively shallow networks},
	author       = {Veit, Andreas and Wilber, Michael J and Belongie, Serge},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}
@inproceedings{kornblith2019similarity,
	title        = {Similarity of neural network representations revisited},
	author       = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3519--3529},
	organization = {PMLR}
}
@article{raghu2017svcca,
	title        = {Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
	author       = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@article{papyan2017convolutional,
	title        = {Convolutional neural networks analyzed via convolutional sparse coding},
	author       = {Papyan, Vardan and Romano, Yaniv and Elad, Michael},
	year         = 2017,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 18,
	number       = 1,
	pages        = {2887--2938}
}
@article{mirsky1975trace,
	title        = {A trace inequality of John von Neumann},
	author       = {Mirsky, Leon},
	year         = 1975,
	journal      = {Monatshefte f{\"u}r mathematik},
	publisher    = {Springer},
	volume       = 79,
	number       = 4,
	pages        = {303--306}
}
@article{miranda1993trace,
	title        = {A trace inequality with a subtracted term},
	author       = {Miranda, Hector and Thompson, Robert C},
	year         = 1993,
	journal      = {Linear algebra and its applications},
	publisher    = {Elsevier},
	volume       = 185,
	pages        = {165--172}
}
@inproceedings{he2016deep,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {770--778}
}
@article{vaswani2017attention,
	title        = {Attention is all you need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@article{silver2017mastering,
	title        = {Mastering the game of go without human knowledge},
	author       = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	year         = 2017,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 550,
	number       = 7676,
	pages        = {354--359}
}
@inproceedings{he2016identity,
	title        = {Identity mappings in deep residual networks},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {European conference on computer vision},
	pages        = {630--645},
	organization = {Springer}
}
@inproceedings{zagoruyko2016wide,
	title        = {Wide Residual Networks},
	author       = {Zagoruyko, Sergey and Komodakis, Nikos},
	year         = 2016,
	booktitle    = {British Machine Vision Conference 2016},
	organization = {British Machine Vision Association}
}
@inproceedings{xie2017aggregated,
	title        = {Aggregated residual transformations for deep neural networks},
	author       = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {1492--1500}
}
@misc{loshchilov2017sgdr,
	title        = {SGDR: Stochastic Gradient Descent with Warm Restarts},
	author       = {Ilya Loshchilov and Frank Hutter},
	year         = 2017,
	url          = {https://arxiv.org/abs/1608.03983v5},
	eprint       = {1608.03983},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{ronneberger2015u,
	title        = {U-net: Convolutional networks for biomedical image segmentation},
	author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year         = 2015,
	booktitle    = {International Conference on Medical image computing and computer-assisted intervention},
	pages        = {234--241},
	organization = {Springer}
}
@inproceedings{tirer2023perturbation,
	title        = {Perturbation analysis of neural collapse},
	author       = {Tirer, Tom and Huang, Haoxiang and Niles-Weed, Jonathan},
	year         = 2023,
	booktitle    = {International Conference on Machine Learning},
	pages        = {34301--34329},
	organization = {PMLR}
}
@inproceedings{wang2022linear,
	title        = {Linear Convergence Analysis of Neural Collapse with Unconstrained Features},
	author       = {Wang, Peng and Liu, Huikang and Yaras, Can and Balzano, Laura and Qu, Qing},
	year         = 2022,
	booktitle    = {OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}
}
@article{kothapalli2023neural,
	title        = {Neural Collapse: A Review on Modelling Principles and Generalization},
	author       = {Vignesh Kothapalli},
	year         = 2023,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=QTXocpAP9p},
	note         = {}
}
@misc{raissi2017physics,
	title        = {Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},
	author       = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
	year         = 2017,
	url          = {https://arxiv.org/abs/1711.10561v1},
	eprint       = {1711.10561},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@article{bai2019deep,
	title        = {Deep equilibrium models},
	author       = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@article{DBLP:journals/corr/abs-1708-07747,
	title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1708.07747},
	url          = {http://arxiv.org/abs/1708.07747},
	url          = {https://arxiv.org/abs/1708.07747v2},
	eprinttype   = {arXiv},
	eprint       = {1708.07747},
	timestamp    = {Mon, 13 Aug 2018 16:47:27 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@techreport{krizhevsky2009learning,
	title        = {Learning multiple layers of features from tiny images},
	author       = {Krizhevsky, Alex and Hinton, Geoffrey},
	year         = 2009,
	publisher    = {Technical report, University of Toronto},
	address      = {Toronto, Ontario},
	number       = {0},
	url          = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
	institution  = {University of Toronto},
	title_with_no_special_chars = {Learning multiple layers of features from tiny images}
}
@misc{kingma2017adam,
	title        = {Adam: A Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2017,
	eprint       = {1412.6980},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{DBLP:journals/corr/abs-1711-05101,
	title        = {Fixing Weight Decay Regularization in Adam},
	author       = {Ilya Loshchilov and Frank Hutter},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1711.05101},
	url          = {http://arxiv.org/abs/1711.05101},
	url          = {https://arxiv.org/abs/1711.05101v3},
	eprinttype   = {arXiv},
	eprint       = {1711.05101},
	timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{radford2019language,
	title        = {Language Models are Unsupervised Multitask Learners},
	author       = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year         = 2019
}
@article{fang2021exploring,
	title        = {Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training},
	author       = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J},
	year         = 2021,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Acad Sciences},
	volume       = 118,
	number       = 43,
	pages        = {e2103091118}
}
@misc{zhu2021geometric,
	title        = {A Geometric Analysis of Neural Collapse with Unconstrained Features},
	author       = {Zhihui Zhu and Tianyu Ding and Jinxin Zhou and Xiao Li and Chong You and Jeremias Sulam and Qing Qu},
	year         = 2021,
	url          = {https://arxiv.org/abs/2105.02375v1},
	eprint       = {2105.02375},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{zhou2022losses,
	title        = {Are All Losses Created Equal: A Neural Collapse Perspective},
	author       = {Jinxin Zhou and Chong You and Xiao Li and Kangning Liu and Sheng Liu and Qing Qu and Zhihui Zhu},
	year         = 2022,
	url          = {https://arxiv.org/abs/2210.02192v2},
	eprint       = {2210.02192},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{zhou2023hierarchical,
	title        = {Hierarchical Task-Incremental Learning with Feature-Space Initialization Inspired by Neural Collapse},
	author       = {Zhou, Qinhao and Xiang, Xiang and Ma, Jing},
	year         = 2023,
	journal      = {Neural Processing Letters},
	publisher    = {Springer},
	volume       = 55,
	number       = 8,
	pages        = {10811--10827}
}
@misc{xu2024collapsed,
      title={Collapsed Language Models Promote Fairness}, 
      author={Jingxuan Xu and Wuyang Chen and Linyi Li and Yao Zhao and Yunchao Wei},
      year={2024},
      eprint={2410.04472},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04472}, 
}
@article{Pakdaman_Naeini_Cooper_Hauskrecht_2015,
	title        = {Obtaining Well Calibrated Probabilities Using Bayesian Binning},
	author       = {Pakdaman Naeini, Mahdi and Cooper, Gregory and Hauskrecht, Milos},
	year         = 2015,
	month        = {Feb.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 29,
	number       = 1,
	doi          = {10.1609/aaai.v29i1.9602},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/9602},
	abstractnote = {&lt;p&gt; Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets. &lt;/p&gt;}
}
@article{strohmer2003grassmannian,
	title        = {Grassmannian frames with applications to coding and communication},
	author       = {Strohmer, Thomas and Heath Jr, Robert W},
	year         = 2003,
	journal      = {Applied and computational harmonic analysis},
	publisher    = {Elsevier},
	volume       = 14,
	number       = 3,
	pages        = {257--275}
}
@misc{zhang2018mixup,
	title        = {mixup: Beyond Empirical Risk Minimization},
	author       = {Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
	year         = 2018,
	url          = {https://arxiv.org/abs/1710.09412v2},
	eprint       = {1710.09412},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{radford2018improving,
	title        = {Improving language understanding by generative pre-training},
	author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
	year         = 2018,
	publisher    = {OpenAI}
}
@book{waldron2018introduction,
	title        = {An introduction to finite tight frames},
	author       = {Waldron, Shayne FD},
	year         = 2018,
	publisher    = {Springer}
}
@inproceedings{guo2019mixup,
	title        = {Mixup as locally linear out-of-manifold regularization},
	author       = {Guo, Hongyu and Mao, Yongyi and Zhang, Richong},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = 33,
	number       = {01},
	pages        = {3714--3722}
}
@misc{sutskever2023compression,
	author       = {Ilya Sutskever},
	year         = 2023,
	month        = {Aug},
	journal      = {Simons Institute for the Theory of Computing},
	url          = {https://simons.berkeley.edu/talks/ilya-sutskever-openai-2023-08-14}
}
@article{raffel2020exploring,
	title        = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	year         = 2020,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLRORG},
	volume       = 21,
	number       = 1,
	pages        = {5485--5551}
}
@misc{kaplan2020scaling,
	title        = {Scaling Laws for Neural Language Models},
	author       = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
	year         = 2020,
	url          = {https://arxiv.org/abs/2001.08361v1},
	eprint       = {2001.08361},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{merrill2023effects,
	title        = {Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent},
	author       = {William Merrill and Vivek Ramanujan and Yoav Goldberg and Roy Schwartz and Noah Smith},
	year         = 2023,
	url          = {https://arxiv.org/abs/2010.09697v5},
	eprint       = {2010.09697},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{gpt-neo,
	title        = {{GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}},
	author       = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
	year         = 2021,
	month        = mar,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.5297715},
	url          = {https://doi.org/10.5281/zenodo.5297715},
	note         = {{If you use this software, please cite it using these metadata.}},
	version      = {1.0}
}
@article{yaras2022neural,
	title        = {Neural collapse with normalized features: A geometric analysis over the riemannian manifold},
	author       = {Yaras, Can and Wang, Peng and Zhu, Zhihui and Balzano, Laura and Qu, Qing},
	year         = 2022,
	journal      = {Advances in neural information processing systems},
	volume       = 35,
	pages        = {11547--11560}
}
@misc{jurafskyspeech,
	title        = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author       = {Jurafsky, Daniel and Martin, James H},
	year         = 2009
}
@article{florence1950human,
	title        = {Human behaviour and the principle of least effort},
	author       = {Florence, P Sargant},
	year         = 1950,
	journal      = {The Economic Journal},
	publisher    = {Oxford University Press Oxford, UK},
	volume       = 60,
	number       = 240,
	pages        = {808--810}
}
@article{shannon1948mathematical,
	title        = {A mathematical theory of communication},
	author       = {Shannon, C. E.},
	year         = 1948,
	journal      = {The Bell System Technical Journal},
	volume       = 27,
	number       = 3,
	pages        = {379--423},
	doi          = {10.1002/j.1538-7305.1948.tb01338.x},
	keywords     = {}
}
@misc{galanti2022role,
	title        = {On the Role of Neural Collapse in Transfer Learning},
	author       = {Tomer Galanti and András György and Marcus Hutter},
	year         = 2022,
	url          = {https://arxiv.org/abs/2112.15121v2},
	eprint       = {2112.15121},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{hui2022limitations,
	title        = {Limitations of Neural Collapse for Understanding Generalization in Deep Learning},
	author       = {Like Hui and Mikhail Belkin and Preetum Nakkiran},
	year         = 2022,
	url          = {https://arxiv.org/abs/2202.08384v1},
	eprint       = {2202.08384},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{han2022neural,
	title        = {Neural Collapse Under {MSE} Loss: Proximity to and Dynamics on the Central Path},
	author       = {X.Y. Han and Vardan Papyan and David L. Donoho},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=w1UbdvWH_R3}
}
@article{thrampoulidis2022imbalance,
	title        = {Imbalance trouble: Revisiting neural-collapse geometry},
	author       = {Thrampoulidis, Christos and Kini, Ganesh Ramachandra and Vakilian, Vala and Behnia, Tina},
	year         = 2022,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {27225--27238}
}
@misc{hoffmann2022training,
	title        = {Training Compute-Optimal Large Language Models},
	author       = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
	year         = 2022,
	url          = {https://arxiv.org/abs/2203.15556v1},
	eprint       = {2203.15556},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{liu2023generalizing,
	title        = {Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap},
	author       = {Weiyang Liu and Longhui Yu and Adrian Weller and Bernhard Schölkopf},
	year         = 2023,
	url          = {https://arxiv.org/abs/2303.06484v2},
	eprint       = {2303.06484},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{openai2023gpt4,
	title        = {GPT-4 Technical Report},
	author       = {OpenAI},
	author       = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
	year         = 2023,
	url          = {https://arxiv.org/abs/2303.08774v6},
	eprint       = {2303.08774},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{eldan2023tinystories,
	title        = {TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
	author       = {Ronen Eldan and Yuanzhi Li},
	year         = 2023,
	url          = {https://arxiv.org/abs/2305.07759},
	url          = {https://arxiv.org/abs/2305.07759v2},
	eprint       = {2305.07759},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{feng2023study,
	title        = {A Study of Neural Collapse for Text Classification},
	author       = {Feng, Jia Hui and Lai, Edmund M-K and Li, Weihua},
	year         = 2023,
	booktitle    = {International Conference on Deep Learning Theory and Applications},
	pages        = {126--142},
	organization = {Springer}
}
@article{donoho2005stable,
	title        = {Stable recovery of sparse overcomplete representations in the presence of noise},
	author       = {Donoho, David L and Elad, Michael and Temlyakov, Vladimir N},
	year         = 2005,
	journal      = {IEEE Transactions on information theory},
	publisher    = {IEEE},
	volume       = 52,
	number       = 1,
	pages        = {6--18}
}
@misc{muennighoff2023scaling,
	title        = {Scaling Data-Constrained Language Models},
	author       = {Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
	year         = 2023,
	url          = {https://arxiv.org/abs/2305.16264v4},
	eprint       = {2305.16264},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{jiang2023generalized,
	title        = {Generalized Neural Collapse for a Large Number of Classes},
	author       = {Jiachen Jiang and Jinxin Zhou and Peng Wang and Qing Qu and Dustin Mixon and Chong You and Zhihui Zhu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.05351},
	url          = {https://arxiv.org/abs/2310.05351v3},
	eprint       = {2310.05351},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{dang2023neural,
	title        = {Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data},
	author       = {Hien Dang and Tho Tran and Stanley Osher and Hung Tran-The and Nhat Ho and Tan Nguyen},
	year         = 2023,
	url          = {https://arxiv.org/abs/2301.00437v5},
	eprint       = {2301.00437},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{rangamani9746778,
	title        = {Neural Collapse in Deep Homogeneous Classifiers and The Role of Weight Decay},
	author       = {Rangamani, Akshay and Banburski-Fahey, Andrzej},
	year         = 2022,
	booktitle    = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	volume       = {},
	number       = {},
	pages        = {4243--4247},
	doi          = {10.1109/ICASSP43922.2022.9746778}
}
@inproceedings{liu2023inducing,
	title        = {Inducing Neural Collapse in Deep Long-tailed Learning},
	author       = {Liu, Xuantong and Zhang, Jianfeng and Hu, Tianyang and Cao, He and Yao, Yuan and Pan, Lujia},
	year         = 2023,
	booktitle    = {International Conference on Artificial Intelligence and Statistics},
	pages        = {11534--11544},
	organization = {PMLR}
}
@article{elhage2022superposition,
	title        = {Toy Models of Superposition},
	author       = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
	year         = 2022,
	journal      = {Transformer Circuits Thread},
	url          = {https://transformer-circuits.pub/2022/toy\_model/index.html}
}
@article{RAN2024103664,
	title        = {Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning},
	author       = {Hang Ran and Weijun Li and Lusi Li and Songsong Tian and Xin Ning and Prayag Tiwari},
	year         = 2024,
	journal      = {Information Processing \& Management},
	volume       = 61,
	number       = 3,
	pages        = 103664,
	doi          = {https://doi.org/10.1016/j.ipm.2024.103664},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457324000244},
	keywords     = {Few-shot class-incremental learning, Neural collapse, Meta-learning},
	abstract     = {Few-Shot Class-Incremental Learning (FSCIL) aims to learn new classes incrementally with a limited number of samples per class. It faces issues of forgetting previously learned classes and overfitting on few-shot classes. An efficient strategy is to learn features that are discriminative in both base and incremental sessions. Current methods improve discriminability by manually designing inter-class margins based on empirical observations, which can be suboptimal. The emerging Neural Collapse (NC) theory provides a theoretically optimal inter-class margin for classification, serving as a basis for adaptively computing the margin. Yet, it is designed for closed, balanced data, not for sequential or few-shot imbalanced data. To address this gap, we propose a Meta-learning- and NC-based FSCIL method, MetaNC-FSCIL, to compute the optimal margin adaptively and maintain it at each incremental session. Specifically, we first compute the theoretically optimal margin based on the NC theory. Then we introduce a novel loss function to ensure that the loss value is minimized precisely when the inter-class margin reaches its theoretically best. Motivated by the intuition that “learn how to preserve the margin” matches the meta-learning’s goal of “learn how to learn”, we embed the loss function in base-session meta-training to preserve the margin for future meta-testing sessions. Experimental results demonstrate the effectiveness of MetaNC-FSCIL, achieving superior performance on multiple datasets. The code is available at https://github.com/qihangran/metaNC-FSCIL.}
}
@inproceedings{galanti2022improved,
	title        = {Improved Generalization Bounds for Transfer Learning via Neural Collapse},
	author       = {Tomer Galanti and Andr{\'a}s Gy{\"o}rgy and Marcus Hutter},
	year         = 2022,
	booktitle    = {First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022},
	url          = {https://openreview.net/forum?id=VrK7pKwOhT_}
}
@misc{gao2023study,
	title        = {A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry and Generalization},
	author       = {Peifeng Gao and Qianqian Xu and Peisong Wen and Huiyang Shao and Zhiyong Yang and Qingming Huang},
	year         = 2023,
	url          = {https://arxiv.org/abs/2304.08914v2},
	eprint       = {2304.08914},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{wang2024understanding,
	title        = {Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination},
	author       = {Peng Wang and Xiao Li and Can Yaras and Zhihui Zhu and Laura Balzano and Wei Hu and Qing Qu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2311.02960v2},
	eprint       = {2311.02960},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{nguyen2023memorization,
	title        = {Memorization-Dilation: Modeling Neural Collapse Under Noise},
	author       = {Duc Anh Nguyen and Ron Levie and Julian Lienen and Eyke H{\"u}llermeier and Gitta Kutyniok},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=cJWxqmmDL2b}
}
@misc{lan2020albert,
	title        = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author       = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	year         = 2020,
	url          = {https://arxiv.org/abs/1909.11942v6},
	eprint       = {1909.11942},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{parker2023neural,
	title        = {Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks},
	author       = {Liam Parker and Emre Onal and Anton Stengel and Jake Intrater},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.02760v1},
	eprint       = {2308.02760},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{medepalli2023role,
	title        = {On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning},
	author       = {Saaketh Medepalli and Naren Doraiswamy},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.00451v2},
	eprint       = {2310.00451},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{peifeng2024towards,
	title        = {Towards Demystifying the Generalization Behaviors When Neural Collapse Emerges},
	author       = {Gao Peifeng and Qianqian Xu and Yibo Yang and Peisong Wen and Huiyang Shao and Zhiyong Yang and Bernard Ghanem and Qingming Huang},
	year         = 2024,
	url          = {https://openreview.net/forum?id=XVv4S6LnMk}
}
@misc{bonifazi2024understand,
	title        = {Can We Understand Plasticity Through Neural Collapse?},
	author       = {Guglielmo Bonifazi and Iason Chalas and Gian Hess and Jakub Łucki},
	year         = 2024,
	url          = {https://arxiv.org/abs/2404.02719v1},
	eprint       = {2404.02719},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{zhang2024epa,
	title        = {EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector},
	author       = {Jiawei Zhang and Yufan Chen and Cheng Jin and Lei Zhu and Yuantao Gu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2401.01710v1},
	eprint       = {2401.01710},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{li2024understanding,
	title        = {Understanding and Improving Transfer Learning of Deep Models via Neural Collapse},
	author       = {Xiao Li and Sheng Liu and Jinxin Zhou and Xinyu Lu and Carlos Fernandez-Granda and Zhihui Zhu and Qing Qu},
	year         = 2024,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=o8r84MzTQB},
	note         = {}
}
@misc{devlin2019bert,
	title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year         = 2019,
	url          = {https://arxiv.org/abs/1810.04805},
	url          = {https://arxiv.org/abs/1810.04805v2},
	eprint       = {1810.04805},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{liu2019roberta,
	title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2019,
	url          = {https://arxiv.org/abs/1907.11692v1},
	eprint       = {1907.11692},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{wei2022emergent,
	title        = {Emergent Abilities of Large Language Models},
	author       = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
	year         = 2022,
	url          = {https://arxiv.org/abs/2206.07682v2},
	eprint       = {2206.07682},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{pmlr-v196-ben-shaul22a,
	title        = {Nearest Class-Center Simplification through Intermediate Layers},
	author       = {Ben-Shaul, Ido and Dekel, Shai},
	year         = 2022,
	month        = {25 Feb--22 Jul},
	booktitle    = {Proceedings of Topological, Algebraic, and Geometric Learning Workshops 2022},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 196,
	pages        = {37--47},
	url          = {https://proceedings.mlr.press/v196/ben-shaul22a.html},
	editor       = {Cloninger, Alexander and Doster, Timothy and Emerson, Tegan and Kaul, Manohar and Ktena, Ira and Kvinge, Henry and Miolane, Nina and Rieck, Bastian and Tymochko, Sarah and Wolf, Guy},
	pdf          = {https://proceedings.mlr.press/v196/ben-shaul22a/ben-shaul22a.pdf},
	abstract     = {Recent advances in neural network theory have introduced geometric properties that occur during training, past the Interpolation Threshold- where the training error reaches zero. We inquire into the phenomena coined \emph{Neural Collapse} in the intermediate layers of the network, and emphasize the innerworkings of Nearest Class-Center Mismatch inside a deepnet. We further show that these processes occur both in vision and language model architectures. Lastly, we propose a Stochastic Variability-Simplification Loss (SVSL) that encourages better geometrical features in intermediate layers, yielding improvements in both train metrics and generalization.}
}
@misc{gao2020pile800gbdatasetdiverse,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.00027}, 
}
@misc{zarka2021separation,
	title        = {Separation and Concentration in Deep Networks},
	author       = {John Zarka and Florentin Guth and Stéphane Mallat},
	year         = 2021,
	url          = {https://arxiv.org/abs/2012.10424v2},
	eprint       = {2012.10424},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{ergen2022demystifying,
	title        = {Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization},
	author       = {Tolga Ergen and Arda Sahiner and Batu Ozturkler and John Pauly and Morteza Mardani and Mert Pilanci},
	year         = 2022,
	url          = {https://arxiv.org/abs/2103.01499v3},
	eprint       = {2103.01499},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{garrod2024unifying,
	title        = {Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model},
	author       = {Connall Garrod and Jonathan P. Keating},
	year         = 2024,
	url          = {https://arxiv.org/abs/2404.06106v1},
	eprint       = {2404.06106},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{telgarsky2022feature,
	title        = {Feature selection with gradient descent on two-layer networks in low-rotation regimes},
	author       = {Matus Telgarsky},
	year         = 2022,
	url          = {https://arxiv.org/abs/2208.02789v1},
	eprint       = {2208.02789},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{NEURIPS2023_f249db9a,
	title        = {The Tunnel Effect: Building Data Representations in Deep Neural Networks},
	author       = {Masarczyk, Wojciech and Ostaszewski, Mateusz and Imani, Ehsan and Pascanu, Razvan and Mi\l o\'{s}, Piotr and Trzcinski, Tomasz},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {76772--76805},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f249db9ab5975586f36df46f8958c008-Paper-Conference.pdf},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@inproceedings{li2023no,
	title        = {No fear of classifier biases: Neural collapse inspired federated learning with synthetic and fixed classifier},
	author       = {Li, Zexi and Shang, Xinyi and He, Rui and Lin, Tao and Wu, Chao},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {5319--5329}
}
@inproceedings{liang2023inducing,
	title        = {Inducing neural collapse to a fixed hierarchy-aware frame for reducing mistake severity},
	author       = {Liang, Tong and Davis, Jim},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {1443--1452}
}
@article{xie2023neural,
	title        = {Neural collapse inspired attraction--repulsion-balanced loss for imbalanced learning},
	author       = {Xie, Liang and Yang, Yibo and Cai, Deng and He, Xiaofei},
	year         = 2023,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 527,
	pages        = {60--70}
}
@misc{li2023principled,
	title        = {Principled and Efficient Transfer Learning of Deep Models via Neural Collapse},
	author       = {Xiao Li and Sheng Liu and Jinxin Zhou and Xinyu Lu and Carlos Fernandez-Granda and Zhihui Zhu and Qing Qu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2212.12206v3},
	eprint       = {2212.12206},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{hu2024neural,
	title        = {Neural collapse inspired semi-supervised learning with fixed classifier},
	author       = {Hu, Zhanxuan and Wang, Yichen and Ning, Hailong and Tai, Yonghang and Nie, Feiping},
	year         = 2024,
	journal      = {Information Sciences},
	publisher    = {Elsevier},
	volume       = 667,
	pages        = 120469
}
@inproceedings{li2024neuromixgdp,
	title        = {NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release},
	author       = {Li, Donghao and Cao, Yang and Yao, Yuan},
	year         = 2024,
	booktitle    = {Conference on Parsimony and Learning},
	pages        = {480--514},
	organization = {PMLR}
}
@misc{yang2023neural,
	title        = {Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants},
	author       = {Yibo Yang and Haobo Yuan and Xiangtai Li and Jianlong Wu and Lefei Zhang and Zhouchen Lin and Philip Torr and Dacheng Tao and Bernard Ghanem},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.01746v1},
	eprint       = {2308.01746},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{gill2024engineering,
	title        = {Engineering the Neural Collapse Geometry of Supervised-Contrastive Loss},
	author       = {Gill, Jaidev and Vakilian, Vala and Thrampoulidis, Christos},
	year         = 2024,
	booktitle    = {ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {7115--7119},
	organization = {IEEE}
}
@misc{haas2023linking,
	title        = {Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks},
	author       = {Jarrod Haas and William Yolland and Bernhard Rabus},
	year         = 2023,
	url          = {https://arxiv.org/abs/2209.08378v3},
	eprint       = {2209.08378},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{beaglehole2024average,
	title        = {Average gradient outer product as a mechanism for deep neural collapse},
	author       = {Daniel Beaglehole and Peter Súkeník and Marco Mondelli and Mikhail Belkin},
	year         = 2024,
	url          = {https://arxiv.org/abs/2402.13728v2},
	eprint       = {2402.13728},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{wang2023far,
	title        = {How far pre-trained models are from neural collapse on the target dataset informs their transferability},
	author       = {Wang, Zijian and Luo, Yadan and Zheng, Liang and Huang, Zi and Baktashmotlagh, Mahsa},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {5549--5558}
}
@article{xu2023dynamics,
	title        = {Dynamics in deep classifiers trained with the square loss: Normalization, low rank, neural collapse, and generalization bounds},
	author       = {Xu, Mengjia and Rangamani, Akshay and Liao, Qianli and Galanti, Tomer and Poggio, Tomaso},
	year         = 2023,
	journal      = {Research},
	publisher    = {AAAS},
	volume       = 6,
	pages        = {0024}
}
@misc{laurent2023feature,
	title        = {Feature Collapse},
	author       = {Thomas Laurent and James H. von Brecht and Xavier Bresson},
	year         = 2023,
	url          = {https://arxiv.org/abs/2305.16162v1},
	eprint       = {2305.16162},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{mimno-thompson-2017-strange,
	title        = {The strange geometry of skip-gram with negative sampling},
	author       = {Mimno, David  and Thompson, Laure},
	year         = 2017,
	month        = sep,
	booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Copenhagen, Denmark},
	pages        = {2873--2878},
	doi          = {10.18653/v1/D17-1308},
	url          = {https://aclanthology.org/D17-1308},
	editor       = {Palmer, Martha  and Hwa, Rebecca  and Riedel, Sebastian},
	abstract     = {Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.}
}
@inproceedings{deng2009imagenet,
	title        = {Imagenet: A large-scale hierarchical image database},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year         = 2009,
	booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
	pages        = {248--255},
	organization = {Ieee}
}
@inproceedings{10.1145/3442188.3445922,
	title        = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
	author       = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	location     = {Virtual Event, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAccT '21},
	pages        = {610–623},
	doi          = {10.1145/3442188.3445922},
	isbn         = 9781450383097,
	url          = {https://doi.org/10.1145/3442188.3445922},
	abstract     = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	numpages     = 14
}
@misc{jiang2024layerwise,
      title={On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier}, 
      author={Jiachen Jiang and Jinxin Zhou and Zhihui Zhu},
      year={2024},
      eprint={2406.14479},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.14479}, 
}
@article{sukenik2024deep,
	title        = {Deep neural collapse is provably optimal for the deep unconstrained features model},
	author       = {S{\'u}ken{\'\i}k, Peter and Mondelli, Marco and Lampert, Christoph H},
	year         = 2024,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 36
}
@inproceedings{pmlr-v162-tirer22a,
	title        = {Extended Unconstrained Features Model for Exploring Deep Neural Collapse},
	author       = {Tirer, Tom and Bruna, Joan},
	year         = 2022,
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {21478--21505},
	url          = {https://proceedings.mlr.press/v162/tirer22a.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/tirer22a/tirer22a.pdf},
	abstract     = {The modern strategy for training deep neural networks for classification tasks includes optimizing the network’s weights even after the training error vanishes to further push the training loss toward zero. Recently, a phenomenon termed “neural collapse" (NC) has been empirically observed in this training procedure. Specifically, it has been shown that the learned features (the output of the penultimate layer) of within-class samples converge to their mean, and the means of different classes exhibit a certain tight frame structure, which is also aligned with the last layer’s weights. Recent papers have shown that minimizers with this structure emerge when optimizing a simplified “unconstrained features model" (UFM) with a regularized cross-entropy loss. In this paper, we further analyze and extend the UFM. First, we study the UFM for the regularized MSE loss, and show that the minimizers’ features can have a more delicate structure than in the cross-entropy case. This affects also the structure of the weights. Then, we extend the UFM by adding another layer of weights as well as ReLU nonlinearity to the model and generalize our previous results. Finally, we empirically demonstrate the usefulness of our nonlinear extended UFM in modeling the NC phenomenon that occurs with practical networks.}
}
@inproceedings{ergen2021revealing,
	title        = {Revealing the structure of deep neural networks via convex duality},
	author       = {Ergen, Tolga and Pilanci, Mert},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3004--3014},
	organization = {PMLR}
}
@article{kothapalli2024neural,
	title        = {A neural collapse perspective on feature evolution in graph neural networks},
	author       = {Kothapalli, Vignesh and Tirer, Tom and Bruna, Joan},
	year         = 2024,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 36
}
@misc{zangrando2024neural,
	title        = {Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias},
	author       = {Emanuele Zangrando and Piero Deidda and Simone Brugiapaglia and Nicola Guglielmi and Francesco Tudisco},
	year         = 2024,
	url          = {https://arxiv.org/abs/2402.03991v1},
	eprint       = {2402.03991},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{shumailov2024curse,
      title={The Curse of Recursion: Training on Generated Data Makes Models Forget}, 
      author={Ilia Shumailov and Zakhar Shumaylov and Yiren Zhao and Yarin Gal and Nicolas Papernot and Ross Anderson},
      year={2024},
      eprint={2305.17493},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.17493}, 
}
@misc{gerstgrasser2024modelcollapse,
      title={Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data}, 
      author={Matthias Gerstgrasser and Rylan Schaeffer and Apratim Dey and Rafael Rafailov and Henry Sleight and John Hughes and Tomasz Korbak and Rajashree Agrawal and Dhruv Pai and Andrey Gromov and Daniel A. Roberts and Diyi Yang and David L. Donoho and Sanmi Koyejo},
      year={2024},
      eprint={2404.01413},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.01413}, 
}
@misc{zhu2023understanding,
	title        = {Understanding Prompt Tuning for V-L Models Through the Lens of Neural Collapse},
	author       = {Didi Zhu and Zexi Li and Min Zhang and Junkun Yuan and Yunfeng Shao and Jiashuo Liu and Kun Kuang and Yinchuan Li and Chao Wu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2306.15955v3},
	eprint       = {2306.15955},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@inproceedings{pmlr-v139-ergen21b,
	title        = {Revealing the Structure of Deep Neural Networks via Convex Duality},
	author       = {Ergen, Tolga and Pilanci, Mert},
	year         = 2021,
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {3004--3014},
	url          = {https://proceedings.mlr.press/v139/ergen21b.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/ergen21b/ergen21b.pdf},
	abstract     = {We study regularized deep neural networks (DNNs) and introduce a convex analytic framework to characterize the structure of the hidden layers. We show that a set of optimal hidden layer weights for a norm regularized DNN training problem can be explicitly found as the extreme points of a convex set. For the special case of deep linear networks, we prove that each optimal weight matrix aligns with the previous layers via duality. More importantly, we apply the same characterization to deep ReLU networks with whitened data and prove the same weight alignment holds. As a corollary, we also prove that norm regularized deep ReLU networks yield spline interpolation for one-dimensional datasets which was previously known only for two-layer networks. Furthermore, we provide closed-form solutions for the optimal layer weights when data is rank-one or whitened. The same analysis also applies to architectures with batch normalization even for arbitrary data. Therefore, we obtain a complete explanation for a recent empirical observation termed Neural Collapse where class means collapse to the vertices of a simplex equiangular tight frame.}
}
@misc{li2024neural,
	title        = {Neural Collapse in Multi-label Learning with Pick-all-label Loss},
	author       = {Pengyu Li and Xiao Li and Yutong Wang and Qing Qu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2310.15903v4},
	url          = {https://arxiv.org/abs/2310.15903},
	eprint       = {2310.15903},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
