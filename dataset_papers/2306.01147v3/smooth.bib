@inproceedings{sill:97,
 author = {Sill, Joseph},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 OPTeditor = {M. Jordan and M. Kearns and S. Solla},
 pages = {},
 publisher = {MIT Press},
 title = {Monotonic Networks},
 OPTurl = {https://proceedings.neurips.cc/paper/1997/file/83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf},
 volume = {10},
 year = {1997}
}


@inproceedings{mikulincer:22,
title={Size and depth of monotone neural networks: interpolation and approximation},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
OPTauthor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
author={Dan Mikulincer and Daniel Reichman},
year={2022},
OPTurl={https://openreview.net/forum?id=vQzDYi4dPwM}
}


@inproceedings{sivaraman2020counterexample,
  title={Counterexample-guided learning of monotonic neural networks},
  author={Sivaraman, Aishwarya and Farnadi, Golnoosh and Millstein, Todd and Van den Broeck, Guy},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={11936--11948},
  year={2020}
}

@InCollection{Prechelt2012,
author="Prechelt, Lutz",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Early Stopping --- But When?",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer",
OPTaddress="Berlin, Heidelberg",
pages="53--67",
abstract="Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).",
isbn="978-3-642-35289-8",
OPTdoi="10.1007/978-3-642-35289-8_5",
OPTurl="https://doi.org/10.1007/978-3-642-35289-8_5"
}

@ARTICLE{daniels:2010,
  author={Daniels, Hennie and Velikova, Marina},
  journal={IEEE Transactions on Neural Networks}, 
  title={Monotone and Partially Monotone Neural Networks}, 
  year={2010},
  volume={21},
  number={6},
  pages={906-917},
  OPTdoi={10.1109/TNN.2010.2044803}
  }

  @inproceedings{liu2020certified,
  title={Certified monotonic neural networks},
  author={Liu, Xingchao and Han, Xing and Zhang, Na and Liu, Qiang},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={15427--15438},
  year={2020}
}

@article{fukushima1975cognitron,
  title={Cognitron: A self-organizing multilayered neural network},
  author={Fukushima, Kunihiko},
  journal={Biological Cybernetics},
  volume={20},
  number={3-4},
  pages={121--136},
  year={1975},
  OPTpublisher={Springer}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted {Boltzmann} machines},
  author={Nair, Vinod and Hinton, Geoffrey E.},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={807--814},
  year={2010}
}

@inproceedings{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units ({ELUs})},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2016},
}

@inproceedings{maas2013rectifier,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
  booktitle={International Conference on Machine Learning (ICML)},
  year = 2013,
}

@inproceedings{Chen:2016,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {International Conference on Knowledge Discovery and Data Mining (KDD)},
 year = {2016},
 OPTisbn = {978-1-4503-4232-2},
 OPTlocation = {San Francisco, California, USA},
 pages = {785--794},
 OPTurl = {http://doi.acm.org/10.1145/2939672.2939785},
 OPTdoi = {10.1145/2939672.2939785},
 publisher = {ACM},
 OPTaddress = {New York, NY, USA},
}

@inproceedings{yanagisawa2022hierarchical,
title={Hierarchical Lattice Layer for Partially Monotone Neural Networks},
author={Hiroki Yanagisawa and Kohei Miyaguchi and Takayuki Katsuki},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
OPTeditor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
OPTurl={https://openreview.net/forum?id=zAuiZpZ478l}
}

@inproceedings{fard:16,
 author = {Milani Fard, Mahdi and Canini, Kevin and Cotter, Andrew and Pfeifer, Jan and Gupta, Maya},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 OPTeditor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 OPTpages = {},
 OPTpublisher = {Curran Associates, Inc.},
 title = {Fast and Flexible Monotonic Functions with Ensembles of Lattices},
 OPTurl = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c913303f392ffc643f7240b180602652-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{gupta2019incorporate,
  title={How to incorporate monotonicity in deep networks while preserving flexibility?},
  author={Gupta, Akhil and Shukla, Naman and Marla, Lavanya and Kolbeinsson, Arinbj{\"o}rn and Yellepeddi, Kartik},
  booktitle={NeurIPS 2019 Workshop on Machine Learning with Guarantees},
  year={2019}
}

@inproceedings{you:17,
 author = {You, Seungil and Ding, David and Canini, Kevin and Pfeifer, Jan and Gupta, Maya},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 OPTeditor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 OPTpublisher = {Curran Associates, Inc.},
 title = {Deep Lattice Networks and Partial Monotonic Functions},
 OPTurl = {https://proceedings.neurips.cc/paper_files/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{wang2020deontological,
  title={Deontological ethics by monotonicity shape constraints},
  author={Wang, Serena and Gupta, Maya},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={2043--2054},
  year={2020},
  OPTorganization={PMLR}
}

@article{cano:19,
title = {Monotonic classification: An overview on algorithms, performance measures and data sets},
journal = {Neurocomputing},
volume = {341},
pages = {168-182},
year = {2019},
OPTdoi = {https://doi.org/10.1016/j.neucom.2019.02.024},
author = {José-Ramón Cano and Pedro Antonio Gutiérrez and Bartosz Krawczyk and Michał Woźniak and Salvador García},
}

@article{chakravarti1989isotonic,
  title={Isotonic median regression: a linear programming approach},
  author={Chakravarti, Nilotpal},
  journal={Mathematics of Operations Research},
  volume={14},
  number={2},
  pages={303--308},
  year={1989},
  OPTpublisher={INFORMS}
}

@article{de2009isotone,
  title={Isotone optimization in {R}: pool-adjacent-violators algorithm ({PAVA}) and active set methods},
  author={De Leeuw, Jan and Hornik, Kurt and Mair, Patrick},
  journal={Journal of Statistical Software},
  volume={32},
  number={5},
  pages={1--24},
  year={2009},
  OPTpublisher={University of California at Los Angeles}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author  = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay}, 
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{best1990active,
  title={Active set algorithms for isotonic regression; a unifying framework},
  author={Best, Michael J and Chakravarti, Nilotpal},
  journal={Mathematical Programming},
  volume={47},
  number={1-3},
  pages={425--439},
  year={1990},
  publisher={Springer}
}

@inproceedings{niculescu2005predicting,
  title={Predicting good probabilities with supervised learning},
  author={Niculescu-Mizil, Alexandru and Caruana, Rich},
  booktitle={International Conference on Machine learning (ICML)},
  pages={625--632},
  year={2005}
}

@inproceedings{fard_alt:16b,
 author = {Milani Fard, Mahdi and Canini, Kevin and Cotter, Andrew and Pfeifer, Jan and Gupta, Maya},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 OPTeditor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fast and Flexible Monotonic Functions with Ensembles of Lattices},
 OPTurl = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c913303f392ffc643f7240b180602652-Paper.pdf},
 volume = {29},
 year = {2016}
}

@Article{tucker:23,
  author =       {Compton Tucker and Martin Brandt and Pierre Hiernaux and Ankit Kariryaa and Kjeld Rasmussen and
Jennifer Small and Christian Igel and Florian Reiner and Katherine Melocik and  Jesse Meyer and Scott
Sinno and Eric Romero and Erin Glennie and Yasmin Fitts and August Morin and Jorge Pinzon and Devin
McClain and Paul Morin and Claire Porter and Shane Loeffle and Laurent Kergoat and Bil-Assanou Issoufou and Patrice Savadogo and Jean-Pierre Wigneron and Benjamin Poulter and Philippe Ciais and Robert
Kaufmann and Ranga Myneni and Sassan Saatchi and Rasmus Fensholt},
  title =        {Sub-continental scale carbon stocks of individual trees in {African} drylands},
  journal =      {Nature},
  year =         2023,
  OPTkey =       {},
 volume =        {615},
  OPTnumber =    {},
  pages =        {80-86},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}


@Article{hiernaux:23,
  author =       {Pierre Hiernaux and Bil-Assanou Hassane Issoufou and Christian Igel and Ankit Kariryaa and Moussa Kourouma and Jérôme Chave and Eric Mougin and Patrice Savadogo},
  title =        {Allometric equations to estimate the dry mass of Sahel woody plants from very-high resolution satellite imagery},
  journal =      {Forest Ecology and Management},
  year =         {2023},
  OPTkey =       {},
  volume =       {529},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@inproceedings{schlag:21,
    title={Linear Transformers Are Secretly Fast Weight Programmers},
    author = {Schlag, Imanol and Kazuki Irie and Jürgen Schmidhuber},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = 2021,
    publisher = {PMLR},
}


@inproceedings{riedmiller1993direct,
  title={A direct adaptive method for faster backpropagation learning: The {RPROP} algorithm},
  author={Riedmiller, Martin and Braun, Heinrich},
  booktitle={IEEE International Conference on Neural Networks},
  pages={586--591},
  year={1993},
  organization={IEEE}
}


@article{yeh1998modeling,
  title={Modeling of strength of high-performance concrete using artificial neural networks},
  author={Yeh, I-Cheng},
  journal={Cement and Concrete Research},
  volume={28},
  number={12},
  pages={1797-1808},
  year={1998},
  OPTpublisher={Elsevier}
}

@article{tsanas2012accurate,
  title={Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools},
  author={Tsanas, Athanasios and Xifara, Angeliki},
  journal={Energy and Buildings},
  volume={49},
  pages={560-567},
  year={2012},
  OPTpublisher={Elsevier}
}

@article{cassotti2015similarity,
  title={A similarity-based {QSAR} model for predicting acute toxicity towards the fathead minnow (Pimephales promelas)},
  author={Cassotti, Matteo and Ballabio, Davide and Todeschini, Roberto and Consonni, Viviana},
  journal={SAR and QSAR in Environmental Research},
  volume={26},
  number={3},
  pages={217-243},
  year={2015},
  OPTpublisher={Taylor \& Francis}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{cole2019avoiding,
  title={Avoiding resentment via monotonic fairness},
  author={Cole, Guy W and Williamson, Sinead A},
  journal={arXiv preprint arXiv:1909.01251},
  year={2019}
}

@inproceedings{nolte2022expressive,
  title={Expressive Monotonic Neural Networks},
  author={Nolte, Niklas and Kitouni, Ouail and Williams, Mike},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year={2022}
}


@InProceedings{pmlr-v97-anil19a,
  title = 	 {Sorting Out {L}ipschitz Function Approximation},
  author =       {Anil, Cem and Lucas, James and Grosse, Roger},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {291--301},
  year = 	 {2019},
  OPTeditor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  OPTvolume = 	 {97},
  OPTseries = 	 {Proceedings of Machine Learning Research},
  OPTmonth = 	 {09--15 Jun},
  OPTpublisher =    {PMLR},
}


@InProceedings{runje23,
  title = 	 {Constrained Monotonic Neural Networks},
  author =       {Runje, Davor and Shankaranarayana, Sharath M},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {29338--29353},
  year = 	 {2023},
  OPTeditor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  OPTpdf = 	 {https://proceedings.mlr.press/v202/runje23a/runje23a.pdf},
  OPTurl = 	 {https://proceedings.mlr.press/v202/runje23a.html},
  abstract = 	 {Wider adoption of neural networks in many critical domains such as finance and healthcare is being hindered by the need to explain their predictions and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain signs on its weights. Unfortunately, this construction does not work with popular non-saturated activation functions as it can only approximate convex functions. We show this shortcoming can be fixed by constructing two additional activation functions from a typical unsaturated monotonic activation function and employing each of them on the part of neurons. Our experiments show this approach of building monotonic neural networks has better accuracy when compared to other state-of-the-art methods, while being the simplest one in the sense of having the least number of parameters, and not requiring any modifications to the learning procedure or post-learning steps. Finally, we prove it can approximate any continuous monotone function on a compact subset of $\mathbb{R}^n$.}
}


@article{archer93,
author = {Archer, Norman P. and Wang, Shouhong},
title = {Application of the Back Propagation Neural Network Algorithm with Monotonicity Constraints for Two-Group Classification Problems},
journal = {Decision Sciences},
volume = {24},
number = {1},
pages = {60-75},
OPTkeywords = {Heuristics and Mathematical Programming},
OPTdoi = {https://doi.org/10.1111/j.1540-5915.1993.tb00462.x},
OPTurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5915.1993.tb00462.x},
OPTeprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5915.1993.tb00462.x},
OPTabstract = {ABSTRACT Neural network techniques are widely used in solving pattern recognition or classification problems. However, when statistical data are used in supervised training of a neural network employing the back-propagation least mean square algorithm, the behavior of the classification boundary during training is often unpredictable. This research suggests the application of monotonicity constraints to the back propagation learning algorithm. When the training sample set is preprocessed by a linear classification function, neural network performance and efficiency can be improved in classification applications where the feature vector is related monotonically to the pattern vector. Since most classification problems in business possess monotonic properties, this technique is useful in those problems where any assumptions about the properties of the data are inappropriate.},
year = {1993}
}


@inproceedings{eides:18,
    author = "Eidnes, L. H. and Nøkland, A.",
    title = "Shifting mean activation towards zero with bipolar activation functions",
    booktitle = "International Conference on Learning Representations (ICLR) Workshop Track Proceedings",
    year = 2018
}



@article{igel:01e,
        author =                         {C. Igel and M. H\"usken},
        title =                          {Empirical Evaluation of the Improved {R}prop
                  Learning Algorithm},
        journal =                        {Neurocomputing},
        volume =                         50,
        number =                         {C},
        pages =                          {105-123},
        year =                           2003,
}