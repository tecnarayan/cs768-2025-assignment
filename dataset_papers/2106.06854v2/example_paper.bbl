\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{baird1995residual}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pp.\  30--37. Elsevier,
  1995.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{barreto2017successor}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., van Hasselt,
  H.~P., and Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4055--4065, 2017.

\bibitem[Barreto et~al.(2018)Barreto, Borsa, Quan, Schaul, Silver, Hessel,
  Mankowitz, Zidek, and Munos]{barreto2018transfer}
Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M.,
  Mankowitz, D., Zidek, A., and Munos, R.
\newblock Transfer in deep reinforcement learning using successor features and
  generalised policy improvement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  501--510, 2018.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{OpenAIGym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Castro et~al.(2018)Castro, Moitra, Gelada, Kumar, and
  Bellemare]{castro2018dopamine}
Castro, P.~S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M.~G.
\newblock Dopamine: A research framework for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.06110}, 2018.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019information}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.00360}, 2019.

\bibitem[Dayan(1993)]{dayan1993improving}
Dayan, P.
\newblock Improving generalization for temporal difference learning: The
  successor representation.
\newblock \emph{Neural Computation}, 5\penalty0 (4):\penalty0 613--624, 1993.

\bibitem[Dud{\'\i}k et~al.(2011)Dud{\'\i}k, Langford, and Li]{dudik2011doubly}
Dud{\'\i}k, M., Langford, J., and Li, L.
\newblock Doubly robust policy evaluation and learning.
\newblock In \emph{Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, pp.\  1097--1104, 2011.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 503--556, 2005.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and
  Ghavamzadeh]{farajtabar2018more}
Farajtabar, M., Chow, Y., and Ghavamzadeh, M.
\newblock More robust doubly robust off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1447--1456, 2018.

\bibitem[Fu et~al.(2021)Fu, Norouzi, Nachum, Tucker, Wang, Novikov, Yang,
  Zhang, Chen, Kumar, Paduraru, Levine, and Paine]{fu2021benchmarks}
Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M.,
  Zhang, M.~R., Chen, Y., Kumar, A., Paduraru, C., Levine, S., and Paine, T.
\newblock Benchmarks for deep off-policy evaluation.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pp.\  1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019{\natexlab{a}})Fujimoto, Conti, Ghavamzadeh, and
  Pineau]{fujimoto2019benchmarking}
Fujimoto, S., Conti, E., Ghavamzadeh, M., and Pineau, J.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 2019{\natexlab{a}}.

\bibitem[Fujimoto et~al.(2019{\natexlab{b}})Fujimoto, Meger, and
  Precup]{fujimoto2018off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062, 2019{\natexlab{b}}.

\bibitem[Fujimoto et~al.(2020)Fujimoto, Meger, and
  Precup]{fujimoto2020equivalence}
Fujimoto, S., Meger, D., and Precup, D.
\newblock An equivalence between loss functions and non-uniform sampling in
  experience replay.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Gauci et~al.(2018)Gauci, Conti, Liang, Virochsiri, He, Kaden,
  Narayanan, Ye, Chen, and Fujimoto]{gauci2018horizon}
Gauci, J., Conti, E., Liang, Y., Virochsiri, K., He, Y., Kaden, Z., Narayanan,
  V., Ye, X., Chen, Z., and Fujimoto, S.
\newblock Horizon: Facebook's open source applied reinforcement learning
  platform.
\newblock \emph{arXiv preprint arXiv:1811.00260}, 2018.

\bibitem[Gelada \& Bellemare(2019)Gelada and Bellemare]{gelada2019off}
Gelada, C. and Bellemare, M.~G.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  3647--3655, 2019.

\bibitem[Gershman(2018)]{gershman2018successor}
Gershman, S.~J.
\newblock The successor representation: its computational logic and neural
  substrates.
\newblock \emph{Journal of Neuroscience}, 38\penalty0 (33):\penalty0
  7193--7200, 2018.

\bibitem[Gershman et~al.(2012)Gershman, Moore, Todd, Norman, and
  Sederberg]{gershman2012successor}
Gershman, S.~J., Moore, C.~D., Todd, M.~T., Norman, K.~A., and Sederberg, P.~B.
\newblock The successor representation and temporal context.
\newblock \emph{Neural Computation}, 24\penalty0 (6):\penalty0 1553--1568,
  2012.

\bibitem[Grimm et~al.(2019)Grimm, Higgins, Barreto, Teplyashin, Wulfmeier,
  Hertweck, Hadsell, and Singh]{grimm2019disentangled}
Grimm, C., Higgins, I., Barreto, A., Teplyashin, D., Wulfmeier, M., Hertweck,
  T., Hadsell, R., and Singh, S.
\newblock Disentangled cumulants help successor representations transfer to new
  tasks.
\newblock \emph{arXiv preprint arXiv:1911.10866}, 2019.

\bibitem[Hallak \& Mannor(2017)Hallak and Mannor]{hallak2017consistent}
Hallak, A. and Mannor, S.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1372--1383. JMLR. org, 2017.

\bibitem[Imani et~al.(2018)Imani, Graves, and White]{imani2018off}
Imani, E., Graves, E., and White, M.
\newblock An off-policy policy gradient theorem using emphatic weightings.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  96--106, 2018.

\bibitem[Jaakkola et~al.(1994)Jaakkola, Jordan, and
  Singh]{jaakkola1994convergence}
Jaakkola, T., Jordan, M.~I., and Singh, S.~P.
\newblock On the convergence of stochastic iterative dynamic programming
  algorithms.
\newblock \emph{Neural computation}, 6\penalty0 (6):\penalty0 1185--1201, 1994.

\bibitem[Janz et~al.(2019)Janz, Hron, Mazur, Hofmann, Hern{\'a}ndez-Lobato, and
  Tschiatschek]{janz2019successor}
Janz, D., Hron, J., Mazur, P., Hofmann, K., Hern{\'a}ndez-Lobato, J.~M., and
  Tschiatschek, S.
\newblock Successor uncertainties: exploration and uncertainty in temporal
  difference learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4509--4518, 2019.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  652--661, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2019)Kostrikov, Nachum, and
  Tompson]{kostrikov2019imitation}
Kostrikov, I., Nachum, O., and Tompson, J.
\newblock Imitation learning via off-policy distribution matching.
\newblock \emph{arXiv preprint arXiv:1912.05032}, 2019.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Saeedi, Gautam, and
  Gershman]{kulkarni2016deep}
Kulkarni, T.~D., Saeedi, A., Gautam, S., and Gershman, S.~J.
\newblock Deep successor reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1606.02396}, 2016.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11784--11794, 2019.

\bibitem[Lesort et~al.(2018)Lesort, D{\'\i}az-Rodr{\'\i}guez, Goudou, and
  Filliat]{lesort2018state}
Lesort, T., D{\'\i}az-Rodr{\'\i}guez, N., Goudou, J.-F., and Filliat, D.
\newblock State representation learning for control: An overview.
\newblock \emph{Neural Networks}, 108:\penalty0 379--392, 2018.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesvari]{li2015toward}
Li, L., Munos, R., and Szepesvari, C.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  608--616,
  2015.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{DDPG}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5356--5366, 2018.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Bacon, and
  Brunskill]{liu2019understanding}
Liu, Y., Bacon, P.-L., and Brunskill, E.
\newblock Understanding the curse of horizon in off-policy evaluation via
  conditional importance sampling.
\newblock \emph{arXiv preprint arXiv:1910.06508}, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2019off}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Off-policy policy gradient with state distribution correction.
\newblock \emph{arXiv preprint arXiv:1904.08473}, 2019{\natexlab{b}}.

\bibitem[Machado et~al.(2017)Machado, Rosenbaum, Guo, Liu, Tesauro, and
  Campbell]{machado2017eigenoption}
Machado, M.~C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G., and Campbell, M.
\newblock Eigenoption discovery through the deep successor representation.
\newblock \emph{arXiv preprint arXiv:1710.11089}, 2017.

\bibitem[Machado et~al.(2018{\natexlab{a}})Machado, Bellemare, and
  Bowling]{machado2018count}
Machado, M.~C., Bellemare, M.~G., and Bowling, M.
\newblock Count-based exploration with the successor representation.
\newblock \emph{arXiv preprint arXiv:1807.11622}, 2018{\natexlab{a}}.

\bibitem[Machado et~al.(2018{\natexlab{b}})Machado, Bellemare, Talvitie,
  Veness, Hausknecht, and Bowling]{machado2018revisiting}
Machado, M.~C., Bellemare, M.~G., Talvitie, E., Veness, J., Hausknecht, M., and
  Bowling, M.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  523--562, 2018{\natexlab{b}}.

\bibitem[Mahmood et~al.(2017)Mahmood, Yu, and Sutton]{mahmood2017multi}
Mahmood, A.~R., Yu, H., and Sutton, R.~S.
\newblock Multi-step off-policy learning without importance sampling ratios.
\newblock \emph{arXiv preprint arXiv:1702.03006}, 2017.

\bibitem[Mandel et~al.(2014)Mandel, Liu, Levine, Brunskill, and
  Popovic]{mandel2014offline}
Mandel, T., Liu, Y.-E., Levine, S., Brunskill, E., and Popovic, Z.
\newblock Offline policy evaluation across representations with applications to
  educational games.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, 2014.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{DQN}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Momennejad et~al.(2017)Momennejad, Russek, Cheong, Botvinick, Daw, and
  Gershman]{momennejad2017successor}
Momennejad, I., Russek, E.~M., Cheong, J.~H., Botvinick, M.~M., Daw, N.~D., and
  Gershman, S.~J.
\newblock The successor representation in human reinforcement learning.
\newblock \emph{Nature Human Behaviour}, 1\penalty0 (9):\penalty0 680--692,
  2017.

\bibitem[Mousavi et~al.(2020)Mousavi, Li, Liu, and Zhou]{mousavi2020black}
Mousavi, A., Li, L., Liu, Q., and Zhou, D.
\newblock Black-box off-policy estimation for infinite-horizon reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2003.11126}, 2020.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1054--1062, 2016.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2315--2325, 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Precup, D., Sutton, R.~S., and Dasgupta, S.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  417--424, 2001.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, pp.\  317--328.
  Springer, 2005.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{PrioritizedExpReplay}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations},
  Puerto Rico, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sinha et~al.(2020)Sinha, Song, Garg, and Ermon]{sinha2020experience}
Sinha, S., Song, J., Garg, A., and Ermon, S.
\newblock Experience replay with likelihood-free importance weights.
\newblock \emph{arXiv preprint arXiv:2006.13169}, 2020.

\bibitem[Sutton(1988)]{sutton1988tdlearning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Sutton \& Tanner(2005)Sutton and Tanner]{sutton2005temporal}
Sutton, R.~S. and Tanner, B.
\newblock Temporal-difference networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1377--1384, 2005.

\bibitem[Sutton et~al.(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{tdc}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  993--1000. ACM, 2009.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Sutton, R.~S., Modayil, J., Delp, M., Degris, T., Pilarski, P.~M., White, A.,
  and Precup, D.
\newblock Horde: a scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pp.\  761--768, 2011.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{sutton2016emphatic}
Sutton, R.~S., Mahmood, A.~R., and White, M.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2603--2631, 2016.

\bibitem[Swaminathan et~al.(2017)Swaminathan, Krishnamurthy, Agarwal, Dudik,
  Langford, Jose, and Zitouni]{swaminathan2017off}
Swaminathan, A., Krishnamurthy, A., Agarwal, A., Dudik, M., Langford, J., Jose,
  D., and Zitouni, I.
\newblock Off-policy evaluation for slate recommendation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3632--3642, 2017.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2139--2148, 2016.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Touati et~al.(2020)Touati, Zhang, Pineau, and
  Vincent]{touati2020stable}
Touati, A., Zhang, A., Pineau, J., and Vincent, P.
\newblock Stable policy optimization via off-policy divergence regularization.
\newblock \emph{arXiv preprint arXiv:2003.04108}, 2020.

\bibitem[Tsitsiklis \& Van~Roy(1997)Tsitsiklis and
  Van~Roy]{tsitsiklis1997analysis}
Tsitsiklis, J.~N. and Van~Roy, B.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1075--1081, 1997.

\bibitem[Uehara \& Jiang(2019)Uehara and Jiang]{uehara2019minimax}
Uehara, M. and Jiang, N.
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:1910.12809}, 2019.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{DoubleDQN}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, pp.\  2094--2100, 2016.

\bibitem[Voloshin et~al.(2019)Voloshin, Le, Jiang, and
  Yue]{voloshin2019empirical}
Voloshin, C., Le, H.~M., Jiang, N., and Yue, Y.
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1911.06854}, 2019.

\bibitem[Wang et~al.(2007)Wang, Bowling, and Schuurmans]{wang2007dual}
Wang, T., Bowling, M., and Schuurmans, D.
\newblock Dual representations for dynamic programming and reinforcement
  learning.
\newblock In \emph{2007 IEEE International Symposium on Approximate Dynamic
  Programming and Reinforcement Learning}, pp.\  44--51. IEEE, 2007.

\bibitem[Wang et~al.(2008)Wang, Bowling, Schuurmans, and
  Lizotte]{wang2008stable}
Wang, T., Bowling, M., Schuurmans, D., and Lizotte, D.~J.
\newblock Stable dual dynamic programming.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1569--1576, 2008.

\bibitem[Wang et~al.(2017)Wang, Agarwal, and Dud{\'\i}k]{wang2017optimal}
Wang, Y.-X., Agarwal, A., and Dud{\'\i}k, M.
\newblock Optimal and adaptive off-policy evaluation in contextual bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3589--3597, 2017.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Xie, T., Ma, Y., and Wang, Y.-X.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9665--9675, 2019.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and Schuurmans]{yang2020off}
Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Yang et~al.(2019)Yang, Xie, and Wang]{yang2019theoretical}
Yang, Z., Xie, Y., and Wang, Z.
\newblock A theoretical analysis of deep q-learning.
\newblock \emph{arXiv preprint arXiv:1901.00137}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Springenberg, Boedecker, and
  Burgard]{zhang2017deep}
Zhang, J., Springenberg, J.~T., Boedecker, J., and Burgard, W.
\newblock Deep reinforcement learning with successor features for navigation
  across similar environments.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  2371--2378. IEEE, 2017.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Zhang, R., Dai, B., Li, L., and Schuurmans, D.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock \emph{arXiv preprint arXiv:2002.09072}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2019)Zhang, Boehmer, and Whiteson]{zhang2019generalized}
Zhang, S., Boehmer, W., and Whiteson, S.
\newblock Generalized off-policy actor-critic.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1999--2009, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Boehmer, and
  Whiteson]{zhang2020deep}
Zhang, S., Boehmer, W., and Whiteson, S.
\newblock Deep residual reinforcement learning.
\newblock In \emph{Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  1611--1619,
  2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Zhang, S., Liu, B., and Whiteson, S.
\newblock Gradientdice: Rethinking generalized offline estimation of stationary
  values.
\newblock \emph{arXiv preprint arXiv:2001.11113}, 2020{\natexlab{c}}.

\bibitem[Zhao et~al.(2009)Zhao, Kosorok, and Zeng]{zhao2009reinforcement}
Zhao, Y., Kosorok, M.~R., and Zeng, D.
\newblock Reinforcement learning design for cancer clinical trials.
\newblock \emph{Statistics in medicine}, 28\penalty0 (26):\penalty0 3294, 2009.

\bibitem[Zhu et~al.(2017)Zhu, Gordon, Kolve, Fox, Fei-Fei, Gupta, Mottaghi, and
  Farhadi]{zhu2017visual}
Zhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., Mottaghi, R.,
  and Farhadi, A.
\newblock Visual semantic planning using deep successor representations.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  483--492, 2017.

\end{thebibliography}
