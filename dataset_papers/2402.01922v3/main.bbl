\begin{thebibliography}{87}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao et~al.(2018)Bao, Niu, and Sugiyama]{bao2018classification}
Bao, H., Niu, G., and Sugiyama, M.
\newblock Classification from pairwise similarity and unlabeled data.
\newblock In \emph{International Conference on Machine Learning}, pp.\  452--461. PMLR, 2018.

\bibitem[Bao et~al.(2020)Bao, Shimada, Xu, Sato, and Sugiyama]{bao2020pairwise}
Bao, H., Shimada, T., Xu, L., Sato, I., and Sugiyama, M.
\newblock Pairwise supervision can provably elicit a decision boundary.
\newblock \emph{arXiv preprint arXiv:2006.06207}, 2020.

\bibitem[Cao et~al.(2021{\natexlab{a}})Cao, Feng, Shu, Xu, An, Niu, and Sugiyama]{cao2021multi}
Cao, Y., Feng, L., Shu, S., Xu, Y., An, B., Niu, G., and Sugiyama, M.
\newblock Multi-class classification from single-class data with confidences.
\newblock \emph{arXiv preprint arXiv:2106.08864}, 2021{\natexlab{a}}.

\bibitem[Cao et~al.(2021{\natexlab{b}})Cao, Feng, Xu, An, Niu, and Sugiyama]{cao2021learning}
Cao, Y., Feng, L., Xu, Y., An, B., Niu, G., and Sugiyama, M.
\newblock Learning from similarity-confidence data.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1272--1282. PMLR, 2021{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Liu, Wang, Zhao, and Wu]{chen2020variational}
Chen, H., Liu, F., Wang, Y., Zhao, L., and Wu, H.
\newblock A variational approach for learning from positive and unlabeled data.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14844--14854, 2020.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Shah, Wang, Tao, Wang, Xie, Sugiyama, Singh, and Raj]{chen2023imprecise}
Chen, H., Shah, A., Wang, J., Tao, R., Wang, Y., Xie, X., Sugiyama, M., Singh, R., and Raj, B.
\newblock Imprecise label learning: A unified framework for learning with various imprecise label configurations.
\newblock \emph{arXiv preprint arXiv:2305.12715}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Tao, Fan, Wang, Wang, Schiele, Xie, Raj, and Savvides]{chen2023softmatch}
Chen, H., Tao, R., Fan, Y., Wang, Y., Wang, J., Schiele, B., Xie, X., Raj, B., and Savvides, M.
\newblock Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023{\natexlab{b}}.

\bibitem[Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco, Gordon, Schuhmann, Schmidt, and Jitsev]{cherti2023reproducible}
Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  2818--2829, 2023.

\bibitem[Chiang \& Sugiyama(2023)Chiang and Sugiyama]{chiang2023unified}
Chiang, C.-K. and Sugiyama, M.
\newblock Unified risk analysis for weakly supervised learning.
\newblock \emph{arXiv preprint arXiv:2309.08216}, 2023.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Coates, A., Ng, A., and Lee, H.
\newblock An analysis of single-layer networks in unsupervised feature learning.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pp.\  215--223. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Cour et~al.(2011)Cour, Sapp, and Taskar]{cour2011learning}
Cour, T., Sapp, B., and Taskar, B.
\newblock Learning from partial labels.
\newblock \emph{The Journal of Machine Learning Research}, 12:\penalty0 1501--1536, 2011.

\bibitem[Crouse(2016)]{crouse2016implementing}
Crouse, D.~F.
\newblock On implementing 2d rectangular assignment algorithms.
\newblock \emph{IEEE Transactions on Aerospace and Electronic Systems}, 52\penalty0 (4):\penalty0 1679--1696, 2016.

\bibitem[Cui et~al.(2020)Cui, Charoenphakdee, Sato, and Sugiyama]{cui2020classification}
Cui, Z., Charoenphakdee, N., Sato, I., and Sugiyama, M.
\newblock Classification from triplet comparison data.
\newblock \emph{Neural Computation}, 32\penalty0 (3):\penalty0 659--681, 2020.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A.~P., Caron, M., Geirhos, R., Alabdulmohsin, I., et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7480--7512. PMLR, 2023.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
Dempster, A.~P., Laird, N.~M., and Rubin, D.~B.
\newblock Maximum likelihood from incomplete data via the em algorithm.
\newblock \emph{Journal of the royal statistical society: series B (methodological)}, 39\penalty0 (1):\penalty0 1--22, 1977.

\bibitem[Deng(2012)]{deng2012mnist}
Deng, L.
\newblock The mnist database of handwritten digit images for machine learning research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Den{\oe}ux(2011)]{denoeux2011maximum}
Den{\oe}ux, T.
\newblock Maximum likelihood estimation from fuzzy data using the em algorithm.
\newblock \emph{Fuzzy sets and systems}, 183\penalty0 (1):\penalty0 72--91, 2011.

\bibitem[du~Plessis et~al.(2015)du~Plessis, Niu, and Sugiyama]{du2015convex}
du~Plessis, M., Niu, G., and Sugiyama, M.
\newblock Convex formulation for learning from positive and unlabeled data.
\newblock In \emph{International conference on machine learning}, pp.\  1386--1394. PMLR, 2015.

\bibitem[Feng et~al.(2020{\natexlab{a}})Feng, Kaneko, Han, Niu, An, and Sugiyama]{feng2020learning}
Feng, L., Kaneko, T., Han, B., Niu, G., An, B., and Sugiyama, M.
\newblock Learning with multiple complementary labels.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pp.\  3072--3081. PMLR, 2020{\natexlab{a}}.

\bibitem[Feng et~al.(2020{\natexlab{b}})Feng, Lv, Han, Xu, Niu, Geng, An, and Sugiyama]{Feng2020ProvablyCP}
Feng, L., Lv, J., Han, B., Xu, M., Niu, G., Geng, X., An, B., and Sugiyama, M.
\newblock Provably consistent partial-label learning.
\newblock \emph{ArXiv}, abs/2007.08929, 2020{\natexlab{b}}.

\bibitem[Feng et~al.(2020{\natexlab{c}})Feng, Lv, Han, Xu, Niu, Geng, An, and Sugiyama]{feng2020provably}
Feng, L., Lv, J., Han, B., Xu, M., Niu, G., Geng, X., An, B., and Sugiyama, M.
\newblock Provably consistent partial-label learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 10948--10960, 2020{\natexlab{c}}.

\bibitem[Feng et~al.(2021)Feng, Shu, Lu, Han, Xu, Niu, An, and Sugiyama]{feng2021pointwise}
Feng, L., Shu, S., Lu, N., Han, B., Xu, M., Niu, G., An, B., and Sugiyama, M.
\newblock Pointwise binary classification with pairwise confidence comparisons.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3252--3262. PMLR, 2021.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2023datacomp}
Gadre, S.~Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten, R., Wortsman, M., Ghosh, D., Zhang, J., et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock \emph{arXiv preprint arXiv:2304.14108}, 2023.

\bibitem[Garg et~al.(2021)Garg, Wu, Smola, Balakrishnan, and Lipton]{garg2021mixture}
Garg, S., Wu, Y., Smola, A., Balakrishnan, S., and Lipton, Z.~C.
\newblock Mixture proportion estimation and pu learning: A modern approach, 2021.

\bibitem[Graves et~al.(2006)Graves, Fern{\'a}ndez, Gomez, and Schmidhuber]{graves2006ctc}
Graves, A., Fern{\'a}ndez, S., Gomez, F., and Schmidhuber, J.
\newblock Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pp.\  369--376, 2006.

\bibitem[Hammoudeh \& Lowd(2020)Hammoudeh and Lowd]{hammoudeh2020learning}
Hammoudeh, Z. and Lowd, D.
\newblock Learning from positive and unlabeled data with arbitrary positive shift.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 13088--13099, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[Hsu et~al.(2019)Hsu, Lv, Schlosser, Odom, and Kira]{hsu2019multi}
Hsu, Y.-C., Lv, Z., Schlosser, J., Odom, P., and Kira, Z.
\newblock Multi-class classification without multi-class labels.
\newblock \emph{arXiv preprint arXiv:1901.00544}, 2019.

\bibitem[Ilse et~al.(2018)Ilse, Tomczak, and Welling]{ilse2018attention}
Ilse, M., Tomczak, J., and Welling, M.
\newblock Attention-based deep multiple instance learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  2127--2136. PMLR, 2018.

\bibitem[Ishida et~al.(2018)Ishida, Niu, and Sugiyama]{ishida2018binary}
Ishida, T., Niu, G., and Sugiyama, M.
\newblock Binary classification from positive-confidence data.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ishida et~al.(2019)Ishida, Niu, Menon, and Sugiyama]{ishida2019complementary}
Ishida, T., Niu, G., Menon, A., and Sugiyama, M.
\newblock Complementary-label learning for arbitrary losses and models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2971--2980. PMLR, 2019.

\bibitem[Ishida et~al.(2022)Ishida, Yamane, Charoenphakdee, Niu, and Sugiyama]{ishida2022performance}
Ishida, T., Yamane, I., Charoenphakdee, N., Niu, G., and Sugiyama, M.
\newblock Is the performance of my deep network too good to be true? a direct approach to estimating the bayes error in binary classification.
\newblock \emph{arXiv preprint arXiv:2202.00395}, 2022.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kiryo et~al.(2017)Kiryo, Niu, Du~Plessis, and Sugiyama]{kiryo2017positive}
Kiryo, R., Niu, G., Du~Plessis, M.~C., and Sugiyama, M.
\newblock Positive-unlabeled learning with non-negative risk estimator.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Krizhevsky, A. et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[K\"{u}ck \& de~Freitas(2005)K\"{u}ck and de~Freitas]{kuck2005statistics}
K\"{u}ck, H. and de~Freitas, N.
\newblock Learning about individuals from group statistics.
\newblock In \emph{Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence}, UAI'05, pp.\  332–339, Arlington, Virginia, USA, 2005. AUAI Press.
\newblock ISBN 0974903914.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Lu et~al.(2018)Lu, Niu, Menon, and Sugiyama]{lu2018minimal}
Lu, N., Niu, G., Menon, A.~K., and Sugiyama, M.
\newblock On the minimal supervision for training any binary classifier from only unlabeled data.
\newblock \emph{arXiv preprint arXiv:1808.10585}, 2018.

\bibitem[Luo \& Orabona(2010)Luo and Orabona]{Luo2010LearningFC}
Luo, J. and Orabona, F.
\newblock Learning from candidate labeling sets.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2010.

\bibitem[Lv et~al.(2020)Lv, Xu, Feng, Niu, Geng, and Sugiyama]{lv2020progressive}
Lv, J., Xu, M., Feng, L., Niu, G., Geng, X., and Sugiyama, M.
\newblock Progressive identification of true labels for partial-label learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pp.\  6500--6510. PMLR, 2020.

\bibitem[Maron \& Lozano-P{\'e}rez(1997)Maron and Lozano-P{\'e}rez]{maron1997framework}
Maron, O. and Lozano-P{\'e}rez, T.
\newblock A framework for multiple-instance learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 10, 1997.

\bibitem[McAuley \& Leskovec(2013)McAuley and Leskovec]{mcauley2013hidden}
McAuley, J. and Leskovec, J.
\newblock Hidden factors and hidden topics: understanding rating dimensions with review text.
\newblock In \emph{Proceedings of the 7th ACM conference on Recommender systems}, pp.\  165--172, 2013.

\bibitem[Mireshghallah et~al.(2020)Mireshghallah, Taram, Vepakomma, Singh, Raskar, and Esmaeilzadeh]{mireshghallah2020privacy}
Mireshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and Esmaeilzadeh, H.
\newblock Privacy in deep learning: A survey.
\newblock \emph{arXiv preprint arXiv:2004.12254}, 2020.

\bibitem[Northcutt et~al.(2017)Northcutt, Wu, and Chuang]{northcutt2017learning}
Northcutt, C.~G., Wu, T., and Chuang, I.~L.
\newblock Learning with confident examples: Rank pruning for robust classification with noisy labels.
\newblock \emph{arXiv preprint arXiv:1705.01936}, 2017.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock 2023.

\bibitem[Pagano et~al.(2023)Pagano, Loureiro, Lisboa, Peixoto, Guimar{\~a}es, Cruz, Araujo, Santos, Cruz, Oliveira, et~al.]{pagano2023bias}
Pagano, T.~P., Loureiro, R.~B., Lisboa, F.~V., Peixoto, R.~M., Guimar{\~a}es, G.~A., Cruz, G.~O., Araujo, M.~M., Santos, L.~L., Cruz, M.~A., Oliveira, E.~L., et~al.
\newblock Bias and unfairness in machine learning models: a systematic review on datasets, tools, fairness metrics, and identification and mitigation methods.
\newblock \emph{Big data and cognitive computing}, 7\penalty0 (1):\penalty0 15, 2023.

\bibitem[Quadrianto et~al.(2008)Quadrianto, Smola, Caetano, and Le]{quadrianto2008estimating}
Quadrianto, N., Smola, A.~J., Caetano, T.~S., and Le, Q.~V.
\newblock Estimating labels from label proportions.
\newblock In \emph{Proceedings of the 25th International Conference on Machine learning}, pp.\  776--783, 2008.

\bibitem[Quost \& Denoeux(2016)Quost and Denoeux]{quost2016clustering}
Quost, B. and Denoeux, T.
\newblock Clustering and classification of fuzzy data using the fuzzy em algorithm.
\newblock \emph{Fuzzy Sets and Systems}, 286:\penalty0 134--156, 2016.

\bibitem[Rabin \& Scott(1959)Rabin and Scott]{rabin1959finite}
Rabin, M.~O. and Scott, D.
\newblock Finite automata and their decision problems.
\newblock \emph{IBM journal of research and development}, 3\penalty0 (2):\penalty0 114--125, 1959.

\bibitem[Rabiner(1989)]{rabiner1989tutorial}
Rabiner, L.~R.
\newblock A tutorial on hidden markov models and selected applications in speech recognition.
\newblock \emph{Proceedings of the IEEE}, 77\penalty0 (2):\penalty0 257--286, 1989.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pp.\  8748--8763. PMLR, 2021.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115:\penalty0 211--252, 2015.

\bibitem[Scott \& Zhang(2020)Scott and Zhang]{scott2020learning}
Scott, C. and Zhang, J.
\newblock Learning from label proportions: A mutual contamination framework.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:\penalty0 22256--22267, 2020.

\bibitem[Settles et~al.(2008)Settles, Craven, and Friedland]{settles2008active}
Settles, B., Craven, M., and Friedland, L.
\newblock Active learning with real annotation costs.
\newblock In \emph{Proceedings of the NIPS workshop on cost-sensitive learning}, volume~1. Vancouver, CA:, 2008.

\bibitem[Shimada et~al.(2021)Shimada, Bao, Sato, and Sugiyama]{shimada2021classification}
Shimada, T., Bao, H., Sato, I., and Sugiyama, M.
\newblock Classification from pairwise similarities/dissimilarities and unlabeled data via empirical risk minimization.
\newblock \emph{Neural Computation}, 33\penalty0 (5):\penalty0 1234--1268, 2021.

\bibitem[Shukla et~al.(2023)Shukla, Zeng, Ahmed, and Van~den Broeck]{ShuklaDAE23}
Shukla, V., Zeng, Z., Ahmed, K., and Van~den Broeck, G.
\newblock A unified approach to count-based weakly-supervised learning.
\newblock In \emph{ICML 2023 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators}, jul 2023.
\newblock URL \url{http://starai.cs.ucla.edu/papers/ShuklaDAE23.pdf}.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel, Cubuk, Kurakin, and Li]{sohn2020fixmatch}
Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.~A., Cubuk, E.~D., Kurakin, A., and Li, C.-L.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33, 2020.

\bibitem[Strobel \& Shokri(2022)Strobel and Shokri]{strobel2022data}
Strobel, M. and Shokri, R.
\newblock Data privacy and trustworthy machine learning.
\newblock \emph{IEEE Security \& Privacy}, 20\penalty0 (5):\penalty0 44--49, 2022.

\bibitem[Sugiyama et~al.(2022)Sugiyama, Bao, Ishida, Lu, Sakai, and Niu]{bookSugiyama+etal2022}
Sugiyama, M., Bao, H., Ishida, T., Lu, N., Sakai, T., and Niu, G.
\newblock \emph{Machine Learning from Weak Supervision: {A}n Empirical Risk Minimization Approach}.
\newblock MIT Press, Cambridge, Massachusetts, USA, 2022.

\bibitem[Tang et~al.(2023)Tang, Lu, Zhang, and Sugiyama]{tang2023multi}
Tang, Y., Lu, N., Zhang, T., and Sugiyama, M.
\newblock Multi-class classification from multiple unlabeled datasets with partial risk regularization.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  990--1005. PMLR, 2023.

\bibitem[Tommasi et~al.(2017)Tommasi, Patricia, Caputo, and Tuytelaars]{tommasi2017deeper}
Tommasi, T., Patricia, N., Caputo, B., and Tuytelaars, T.
\newblock A deeper look at dataset bias.
\newblock \emph{Domain adaptation in computer vision applications}, pp.\  37--55, 2017.

\bibitem[Touvron et~al.(2022)Touvron, Cord, and J{\'e}gou]{touvron2022deit}
Touvron, H., Cord, M., and J{\'e}gou, H.
\newblock Deit iii: Revenge of the vit.
\newblock In \emph{European Conference on Computer Vision}, pp.\  516--533. Springer, 2022.

\bibitem[Tsai \& Lin(2020)Tsai and Lin]{tsai2020learning}
Tsai, K.-H. and Lin, H.-T.
\newblock Learning from label proportions with consistency regularization.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  513--528. PMLR, 2020.

\bibitem[Van~Rooyen \& Williamson(2018)Van~Rooyen and Williamson]{van2018theory}
Van~Rooyen, B. and Williamson, R.~C.
\newblock A theory of learning with corrupted labels.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (228):\penalty0 1--50, 2018.

\bibitem[Wang et~al.(2019)Wang, Zhang, and Li]{Wang2019AdaptiveGG}
Wang, D., Zhang, M.-L., and Li, L.
\newblock Adaptive graph guided disambiguation for partial label learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44:\penalty0 8796--8811, 2019.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Xiao, Li, Feng, Niu, Chen, and Zhao]{wang2022pico}
Wang, H., Xiao, R., Li, Y., Feng, L., Niu, G., Chen, G., and Zhao, J.
\newblock Pi{CO}: Contrastive label disambiguation for partial label learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=EhYjZy6e1gJ}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Feng, Jiang, Niu, Zhang, and Sugiyama]{wang2023binary}
Wang, W., Feng, L., Jiang, Y., Niu, G., Zhang, M.-L., and Sugiyama, M.
\newblock Binary classification with confidence difference.
\newblock \emph{arXiv preprint arXiv:2310.05632}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Chen, Fan, Sun, Tao, Hou, Wang, Yang, Zhou, Guo, Qi, Wu, Li, Nakamura, Ye, Savvides, Raj, Shinozaki, Schiele, Wang, Xie, and Zhang]{usb2022}
Wang, Y., Chen, H., Fan, Y., Sun, W., Tao, R., Hou, W., Wang, R., Yang, L., Zhou, Z., Guo, L.-Z., Qi, H., Wu, Z., Li, Y.-F., Nakamura, S., Ye, W., Savvides, M., Raj, B., Shinozaki, T., Schiele, B., Wang, J., Xie, X., and Zhang, Y.
\newblock Usb: A unified semi-supervised learning benchmark.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Chen, Heng, Hou, Fan, , Wu, Wang, Savvides, Shinozaki, Raj, Schiele, and Xie]{wang2023freematch}
Wang, Y., Chen, H., Heng, Q., Hou, W., Fan, Y., , Wu, Z., Wang, J., Savvides, M., Shinozaki, T., Raj, B., Schiele, B., and Xie, X.
\newblock Freematch: Self-adaptive thresholding for semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2023)Wei, Feng, Han, Liu, Niu, Zhu, and Shen]{uumwei23a}
Wei, Z., Feng, L., Han, B., Liu, T., Niu, G., Zhu, X., and Shen, H.~T.
\newblock A universal unbiased method for classification from aggregate observations.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  36804--36820. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/wei23a.html}.

\bibitem[Wen et~al.(2021)Wen, Cui, Hang, Liu, Wang, and Lin]{wen2021leveraged}
Wen, H., Cui, J., Hang, H., Liu, J., Wang, Y., and Lin, Z.
\newblock Leveraged weighted loss for partial label learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pp.\  11091--11100. PMLR, 2021.

\bibitem[Wightman et~al.()Wightman, Touvron, and J{\'e}gou]{wightman2110resnet}
Wightman, R., Touvron, H., and J{\'e}gou, H.
\newblock Resnet strikes back: An improved training procedure in timm. arxiv 2021.
\newblock \emph{arXiv preprint arXiv:2110.00476}.

\bibitem[Wu et~al.(2022)Wu, Wang, and Zhang]{revisitpllwu22l}
Wu, D.-D., Wang, D.-B., and Zhang, M.-L.
\newblock Revisiting consistency regularization for deep partial label learning.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), \emph{Proceedings of the International Conference on Machine Learning (ICML)}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  24212--24225. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/wu22l.html}.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017online}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.

\bibitem[Xie et~al.(2020)Xie, Dai, Hovy, Luong, and Le]{xie2020unsupervised}
Xie, Q., Dai, Z., Hovy, E., Luong, T., and Le, Q.
\newblock Unsupervised data augmentation for consistency training.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33, 2020.

\bibitem[Yan et~al.(2018)Yan, Wang, Guo, Fang, Liu, and Huang]{yan2018deep}
Yan, Y., Wang, X., Guo, X., Fang, J., Liu, W., and Huang, J.
\newblock Deep multi-instance learning with dynamic pooling.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  662--677. PMLR, 2018.

\bibitem[Yang et~al.(2022)Yang, Zhang, Ye, and Min]{yang2022attention}
Yang, M., Zhang, Y.-X., Ye, M., and Min, F.
\newblock Attention-to-embedding framework for multi-instance learning.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data Mining}, pp.\  109--121. Springer, 2022.

\bibitem[Yu et~al.(2014)Yu, Choromanski, Kumar, Jebara, and Chang]{yu2014learning}
Yu, F.~X., Choromanski, K., Kumar, S., Jebara, T., and Chang, S.-F.
\newblock On learning from label proportions.
\newblock \emph{arXiv preprint arXiv:1402.5902}, 2014.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference (BMVC)}. British Machine Vision Association, 2016.

\bibitem[Zhang et~al.(2021)Zhang, Wang, Hou, Wu, Wang, Okumura, and Shinozaki]{zhang2021flexmatch}
Zhang, B., Wang, Y., Hou, W., Wu, H., Wang, J., Okumura, M., and Shinozaki, T.
\newblock Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 34, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Wang, and Scott]{zhang2022learning}
Zhang, J., Wang, Y., and Scott, C.
\newblock Learning from label proportions by learning with label noise.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26933--26942, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Charoenphakdee, Wu, and Sugiyama]{zhang2020aggre}
Zhang, Y., Charoenphakdee, N., Wu, Z., and Sugiyama, M.
\newblock Learning from aggregate observations.
\newblock pp.\  7993--8005, 2020.

\bibitem[Zhao et~al.(2022)Zhao, Xu, Jiang, Wen, and Huang]{zhao2022dist}
Zhao, Y., Xu, Q., Jiang, Y., Wen, P., and Huang, Q.
\newblock Dist-pu: Positive-unlabeled learning from a label distribution perspective.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  14461--14470, 2022.

\bibitem[Zhou(2004)]{zhou2004multi}
Zhou, Z.-H.
\newblock Multi-instance learning: A survey.
\newblock \emph{Department of Computer Science \& Technology, Nanjing University, Tech. Rep}, 1, 2004.

\bibitem[Zhou(2018)]{zhou2018brief}
Zhou, Z.-H.
\newblock A brief introduction to weakly supervised learning.
\newblock \emph{National science review}, 5\penalty0 (1):\penalty0 44--53, 2018.

\end{thebibliography}
