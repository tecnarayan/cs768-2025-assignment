@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@inproceedings{grandvalet2004semi,
    author = {Yves Grandvalet and
Yoshua Bengio},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/GrandvaletB04.bib},
    booktitle = {Advances in Neural Information Processing Systems 17 [Neural Information
Processing Systems, {NIPS} 2004, December 13-18, 2004, Vancouver,
British Columbia, Canada]},
    pages = {529--536},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Semi-supervised Learning by Entropy Minimization},
    url = {https://proceedings.neurips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html},
    year = {2004}
}

@misc{Understanding2024shi,
    author = {Zhengyan Shi and Sander Land and Acyr Locatelli and Matthieu Geist and Max Bartolo},
    publisher = {arXiv},
    title = {Understanding Likelihood Over-optimisation in Direct Alignment Algorithms},
    year = {2024}
}

@article{zhu2005semi,
    author = {Zhu, Xiaojin Jerry},
    publisher = {University of Wisconsin-Madison Department of Computer Sciences},
    title = {Semi-supervised learning literature survey},
    year = {2005}
}

@inproceedings{schick-schutze-2021-just,
    address = {Online},
    author = {Schick, Timo  and
Sch{\"u}tze, Hinrich},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.185},
    pages = {2339--2352},
    publisher = {Association for Computational Linguistics},
    title = {It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
    url = {https://aclanthology.org/2021.naacl-main.185},
    year = {2021}
}

@inproceedings{Lan2020ALBERT,
    author = {Zhenzhong Lan and
Mingda Chen and
Sebastian Goodman and
Kevin Gimpel and
Piyush Sharma and
Radu Soricut},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
Representations},
    url = {https://openreview.net/forum?id=H1eA7AEtvS},
    year = {2020}
}

@inproceedings{hambardzumyan-etal-2021-warp,
    address = {Online},
    author = {Hambardzumyan, Karen  and
Khachatrian, Hrant  and
May, Jonathan},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.381},
    pages = {4921--4933},
    publisher = {Association for Computational Linguistics},
    title = {{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming},
    url = {https://aclanthology.org/2021.acl-long.381},
    year = {2021}
}

@inproceedings{zhou2024batch,
    author = {Han Zhou and Xingchen Wan and Lev Proleev and Diana Mincu and Jilin Chen and Katherine A Heller and Subhrajit Roy},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering},
    url = {https://openreview.net/forum?id=L3FHMoKZcS},
    year = {2024}
}

@inproceedings{zhou-etal-2023-survival,
    abstract = {Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusively on influential prompt tokens. By employing even simple search methods within the pruned search space, ClaPS achieves state-of-the-art performance across various tasks and LLMs, surpassing the performance of complex approaches while significantly reducing search costs. Our findings underscore the critical role of search space design and optimization in enhancing both the usefulness and the efficiency of black-box prompt-based learning.},
    address = {Singapore},
    author = {Zhou, Han  and
Wan, Xingchen  and
Vuli{\'c}, Ivan  and
Korhonen, Anna},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
    doi = {10.18653/v1/2023.findings-emnlp.870},
    editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
    pages = {13064--13077},
    publisher = {Association for Computational Linguistics},
    title = {Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning},
    url = {https://aclanthology.org/2023.findings-emnlp.870},
    year = {2023}
}

@article{liu2023makes,
    author = {Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian},
    journal = {ArXiv preprint},
    title = {What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning},
    url = {https://arxiv.org/abs/2312.15685},
    volume = {abs/2312.15685},
    year = {2023}
}

@misc{xu2023rethinking,
    archiveprefix = {arXiv},
    author = {Yang Xu and Yongqiang Yao and Yufan Huang and Mengnan Qi and Maoquan Wang and Bin Gu and Neel Sundaresan},
    eprint = {2312.11508},
    primaryclass = {cs.CL},
    title = {Rethinking the Instruction Quality: LIFT is What You Need},
    year = {2023}
}

@article{gunasekar2023textbooks,
    author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
    journal = {ArXiv preprint},
    title = {Textbooks are all you need},
    url = {https://arxiv.org/abs/2306.11644},
    volume = {abs/2306.11644},
    year = {2023}
}

@inproceedings{10.1145/3485447.3512036,
author = {Wu, Bin and Meng, Zaiqiao and Zhang, Qiang and Liang, Shangsong},
title = {Meta-Learning Helps Personalized Product Search},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512036},
doi = {10.1145/3485447.3512036},
abstract = {Personalized product search that provides users with customized search services is an important task for e-commerce platforms. This task remains a challenge when inferring users’ preferences from few records or even no records, which is also known as the few-shot or zero-shot learning problem. In this paper, we propose a Bayesian Online Meta-Learning Model (BOML), which transfers meta-knowledge, from the inference for other users’ preferences, to help to infer the current user’s interest behind her/his few or even no historical records. To extract meta-knowledge from various inference patterns, our model constructs a mixture of meta-knowledge and transfers the corresponding meta-knowledge to the specific user according to her/his records. Based on the meta-knowledge learned from other similar inferences, our proposed model searches a ranked list of products to meet users’ personalized query intents for those with few search records (i.e., few-shot learning problem) or even no search records (i.e., zero-shot learning problem). Under the records arriving sequentially setting, we propose an online variational inference algorithm to update meta-knowledge over time. Experimental results demonstrate that our proposed BOML outperforms state-of-the-art algorithms.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2277–2287},
numpages = {11},
keywords = {Meta-learning, Online Learning, Product Search},
location = {<conf-loc>, <city>Virtual Event, Lyon</city>, <country>France</country>, </conf-loc>},
series = {WWW '22}
}

@InProceedings{pmlr-v202-wu23d,
  title = 	 {Adaptive Compositional Continual Meta-Learning},
  author =       {Wu, Bin and Fang, Jinyuan and Zeng, Xiangxiang and Liang, Shangsong and Zhang, Qiang},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {37358--37378},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wu23d/wu23d.pdf},
  url = 	 {https://proceedings.mlr.press/v202/wu23d.html},
  abstract = 	 {This paper focuses on continual meta-learning, where few-shot tasks are heterogeneous and sequentially available. Recent works use a mixture model for meta-knowledge to deal with the heterogeneity. However, these methods suffer from parameter inefficiency caused by two reasons: (1) the underlying assumption of mutual exclusiveness among mixture components hinders sharing meta-knowledge across heterogeneous tasks. (2) they only allow increasing mixture components and cannot adaptively filter out redundant components. In this paper, we propose an Adaptive Compositional Continual Meta-Learning (ACML) algorithm, which employs a compositional premise to associate a task with a subset of mixture components, allowing meta-knowledge sharing among heterogeneous tasks. Moreover, to adaptively adjust the number of mixture components, we propose a component sparsification method based on evidential theory to filter out redundant components. Experimental results show ACML outperforms strong baselines, showing the effectiveness of our compositional meta-knowledge, and confirming that ACML can adaptively learn meta-knowledge.}
}

@inproceedings{Shi_Zhang_Lipani_2022,
title={StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21383}, DOI={10.1609/aaai.v36i10.21383}, abstractNote={Inferring spatial relations in natural language is a crucial ability an intelligent system should possess. The bAbI dataset tries to capture tasks relevant to this domain (task 17 and 19). However, these tasks have several limitations. Most importantly, they are limited to fixed expressions, they are limited in the number of reasoning steps required to solve them, and they fail to test the robustness of models to input that contains irrelevant or redundant information. In this paper, we present a new Question-Answering dataset called StepGame for robust multi-step spatial reasoning in texts. Our experiments demonstrate that state-of-the-art models on the bAbI dataset struggle on the StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental results on both datasets show that our model outperforms all the baselines with superior generalization and robustness performance.}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shi, Zhengxiang and Zhang, Qiang and Lipani, Aldo}, year={2022}, month={Jun.}, pages={11321-11329} }

@inproceedings{sanhmultitask,
    author = {Victor Sanh and
Albert Webson and
Colin Raffel and
Stephen H. Bach and
Lintang Sutawika and
Zaid Alyafeai and
Antoine Chaffin and
Arnaud Stiegler and
Arun Raja and
Manan Dey and
M Saiful Bari and
Canwen Xu and
Urmish Thakker and
Shanya Sharma Sharma and
Eliza Szczechla and
Taewoon Kim and
Gunjan Chhablani and
Nihal V. Nayak and
Debajyoti Datta and
Jonathan Chang and
Mike Tian{-}Jian Jiang and
Han Wang and
Matteo Manica and
Sheng Shen and
Zheng Xin Yong and
Harshit Pandey and
Rachel Bawden and
Thomas Wang and
Trishala Neeraj and
Jos Rozen and
Abheesht Sharma and
Andrea Santilli and
Thibault F{\'{e}}vry and
Jason Alan Fries and
Ryan Teehan and
Teven Le Scao and
Stella Biderman and
Leo Gao and
Thomas Wolf and
Alexander M. Rush},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/SanhWRBSACSRDBX22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Tue, 24 Jan 2023 00:00:00 +0100},
    title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
    url = {https://openreview.net/forum?id=9Vrb9D0WI4},
    year = {2022}
}

@inproceedings{wei2021finetuned,
    author = {Jason Wei and
Maarten Bosma and
Vincent Y. Zhao and
Kelvin Guu and
Adams Wei Yu and
Brian Lester and
Nan Du and
Andrew M. Dai and
Quoc V. Le},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Finetuned Language Models are Zero-Shot Learners},
    url = {https://openreview.net/forum?id=gEZrGCozdqR},
    year = {2022}
}

@inproceedings{shi-etal-2022-learning,
    address = {Seattle, United States},
    author = {Shi, Zhengxiang  and
Feng, Yue  and
Lipani, Aldo},
    booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
    doi = {10.18653/v1/2022.findings-naacl.158},
    pages = {2060--2070},
    publisher = {Association for Computational Linguistics},
    title = {Learning to Execute Actions or Ask Clarification Questions},
    url = {https://aclanthology.org/2022.findings-naacl.158},
    year = {2022}
}

@inproceedings{shi2023dont,
    author = {Zhengxiang Shi and Aldo Lipani},
    booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
    title = {Don{\textquoteright}t Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner},
    url = {https://openreview.net/forum?id=s7xWeJQACI},
    year = {2023}
}

@inproceedings{chen2024alpagasus,
    author = {Lichang Chen and Shiyang Li and Jun Yan and Hai Wang and Kalpa Gunaratna and Vikas Yadav and Zheng Tang and Vijay Srinivasan and Tianyi Zhou and Heng Huang and Hongxia Jin},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {Alpagasus: Training a Better Alpaca Model with Fewer Data},
    url = {https://openreview.net/forum?id=FdVXgSJhvz},
    year = {2024}
}

@misc{ivison2023camels,
    author = {Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
    journal = {ArXiv preprint},
    title = {Camels in a changing climate: Enhancing lm adaptation with tulu 2},
    url = {https://arxiv.org/abs/2311.10702},
    volume = {abs/2311.10702},
    year = {2023}
}

@misc{Mathew2024Instruction,
    author = {Huerta-Enochian, Mathew},
    journal = {ArXiv preprint},
    title = {Instruction Fine-Tuning: Does Prompt Loss Matter?},
    url = {https://arxiv.org/abs/2401.13586},
    volume = {abs/2401.13586},
    year = {2024}
}

@misc{DollyV2,
    author = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    url = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate = {2023-06-30},
    year = {2023}
}

@misc{codealpaca,
    author = {Sahil Chaudhary},
    howpublished = {https://github.com/sahil280114/codealpaca},
    journal = {GitHub repository},
    publisher = {GitHub},
    title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
    year = {2023}
}

@misc{vicuna2023,
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    year = {2023}
}

@article{bai2022training,
    author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
    journal = {ArXiv preprint},
    title = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
    url = {https://arxiv.org/abs/2204.05862},
    volume = {abs/2204.05862},
    year = {2022}
}

@inproceedings{li2024selfalignment,
    author = {Xian Li and Ping Yu and Chunting Zhou and Timo Schick and Omer Levy and Luke Zettlemoyer and Jason E Weston and Mike Lewis},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {Self-Alignment with Instruction Backtranslation},
    url = {https://openreview.net/forum?id=1oijHJBRsT},
    year = {2024}
}

@inproceedings{honovich-etal-2023-unnatural,
    abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
    address = {Toronto, Canada},
    author = {Honovich, Or  and
Scialom, Thomas  and
Levy, Omer  and
Schick, Timo},
    booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2023.acl-long.806},
    editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
    pages = {14409--14428},
    publisher = {Association for Computational Linguistics},
    title = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
    url = {https://aclanthology.org/2023.acl-long.806},
    year = {2023}
}

@inproceedings{wang-etal-2023-self-instruct,
    abstract = {Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
    address = {Toronto, Canada},
    author = {Wang, Yizhong  and
Kordi, Yeganeh  and
Mishra, Swaroop  and
Liu, Alisa  and
Smith, Noah A.  and
Khashabi, Daniel  and
Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2023.acl-long.754},
    editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
    pages = {13484--13508},
    publisher = {Association for Computational Linguistics},
    title = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
    url = {https://aclanthology.org/2023.acl-long.754},
    year = {2023}
}

@inproceedings{xue2023to,
    author = {Fuzhao Xue and Yao Fu and Wangchunshu Zhou and Zangwei Zheng and Yang You},
    booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
    title = {To Repeat or Not To Repeat: Insights from Scaling {LLM} under Token-Crisis},
    url = {https://openreview.net/forum?id=Af5GvIj3T5},
    year = {2023}
}

@article{leike2018scalable,
    author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
    journal = {ArXiv preprint},
    title = {Scalable agent alignment via reward modeling: a research direction},
    url = {https://arxiv.org/abs/1811.07871},
    volume = {abs/1811.07871},
    year = {2018}
}

@inproceedings{papineni-etal-2002-bleu,
    address = {Philadelphia, Pennsylvania, USA},
    author = {Papineni, Kishore  and
Roukos, Salim  and
Ward, Todd  and
Zhu, Wei-Jing},
    booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.3115/1073083.1073135},
    pages = {311--318},
    publisher = {Association for Computational Linguistics},
    title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
    url = {https://aclanthology.org/P02-1040},
    year = {2002}
}

@article{JMLR:v25:23-0870,
    author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
    journal = {Journal of Machine Learning Research},
    number = {70},
    pages = {1--53},
    title = {Scaling Instruction-Finetuned Language Models},
    url = {http://jmlr.org/papers/v25/23-0870.html},
    volume = {25},
    year = {2024}
}

@misc{overfitting2023,
    author = {Jeremy Howard and Jonathan Whitaker},
    journal = {AI Blog},
    publisher = {AI Blog},
    title = {Can LLMs learn from a single example?},
    url = {https://www.fast.ai/posts/2023-09-04-learning-jumps/},
    year = {2023}
}

@inproceedings{zheng2023judging,
    author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
    booktitle = {Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    title = {Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
    url = {https://openreview.net/forum?id=uccHPGDlao},
    year = {2023}
}

@misc{alpaca,
    author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
    journal = {GitHub repository},
    publisher = {GitHub},
    title = {Stanford Alpaca: An Instruction-following LLaMA model},
    url = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
    year = {2023}
}

@misc{xia2024less,
    author = {Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
    title = {Less: Selecting Influential Data for Instruction Tuning},
    year = {2024}
}

@inproceedings{zhou2023lima,
    author = {Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
    booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
    title = {{LIMA}: Less Is More for Alignment},
    url = {https://openreview.net/forum?id=KBMOKmX2he},
    year = {2023}
}

@inproceedings{hendrycks2021measuring,
    author = {Dan Hendrycks and
Collin Burns and
Steven Basart and
Andy Zou and
Mantas Mazeika and
Dawn Song and
Jacob Steinhardt},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/HendrycksBBZMSS21.bib},
    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
    publisher = {OpenReview.net},
    timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
    title = {Measuring Massive Multitask Language Understanding},
    url = {https://openreview.net/forum?id=d7KBjmI3GmQ},
    year = {2021}
}

@inproceedings{sakaguchi2019winogrande,
    author = {Keisuke Sakaguchi and
Ronan Le Bras and
Chandra Bhagavatula and
Yejin Choi},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/SakaguchiBBC20.bib},
    booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
    pages = {8732--8740},
    publisher = {{AAAI} Press},
    timestamp = {Tue, 02 Feb 2021 00:00:00 +0100},
    title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    url = {https://aaai.org/ojs/index.php/AAAI/article/view/6399},
    year = {2020}
}

@inproceedings{hartvigsen-etal-2022-toxigen,
    address = {Dublin, Ireland},
    author = {Hartvigsen, Thomas  and
Gabriel, Saadia  and
Palangi, Hamid  and
Sap, Maarten  and
Ray, Dipankar  and
Kamar, Ece},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.234},
    pages = {3309--3326},
    publisher = {Association for Computational Linguistics},
    title = {{T}oxi{G}en: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection},
    url = {https://aclanthology.org/2022.acl-long.234},
    year = {2022}
}

@inproceedings{ea01b9c0db064caca6986b925d75f2bb,
    abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
    author = {Levesque, {Hector J.} and Ernest Davis and Leora Morgenstern},
    booktitle = {13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012},
    isbn = {9781577355601},
    language = {English (US)},
    note = {13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012},
    pages = {552--561},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    series = {Proceedings of the International Conference on Knowledge Representation and Reasoning},
    title = {The winograd schema challenge},
    year = {2012}
}

@inproceedings{sennrich-etal-2016-edinburgh,
    address = {Berlin, Germany},
    author = {Sennrich, Rico  and
Haddow, Barry  and
Birch, Alexandra},
    booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
    doi = {10.18653/v1/W16-2323},
    pages = {371--376},
    publisher = {Association for Computational Linguistics},
    title = {{E}dinburgh Neural Machine Translation Systems for {WMT} 16},
    url = {https://aclanthology.org/W16-2323},
    year = {2016}
}

@inproceedings{bojar-etal-2014-findings,
    address = {Athens, Greece},
    author = {Callison-Burch, Chris  and
Koehn, Philipp  and
Monz, Christof  and
Schroeder, Josh},
    booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
    pages = {1--28},
    publisher = {Association for Computational Linguistics},
    title = {Findings of the 2009 {W}orkshop on {S}tatistical {M}achine {T}ranslation},
    url = {https://aclanthology.org/W09-0401},
    year = {2009}
}

@inproceedings{mihaylov-etal-2018-suit,
    address = {Brussels, Belgium},
    author = {Mihaylov, Todor  and
Clark, Peter  and
Khot, Tushar  and
Sabharwal, Ashish},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D18-1260},
    pages = {2381--2391},
    publisher = {Association for Computational Linguistics},
    title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
    url = {https://aclanthology.org/D18-1260},
    year = {2018}
}

@inproceedings{paperno-etal-2016-lambada,
    address = {Berlin, Germany},
    author = {Paperno, Denis  and
Kruszewski, Germ{\'a}n  and
Lazaridou, Angeliki  and
Pham, Ngoc Quan  and
Bernardi, Raffaella  and
Pezzelle, Sandro  and
Baroni, Marco  and
Boleda, Gemma  and
Fern{\'a}ndez, Raquel},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P16-1144},
    pages = {1525--1534},
    publisher = {Association for Computational Linguistics},
    title = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
    url = {https://aclanthology.org/P16-1144},
    year = {2016}
}

@inproceedings{hendrycks2021ethics,
    author = {Dan Hendrycks and
Collin Burns and
Steven Basart and
Andrew Critch and
Jerry Li and
Dawn Song and
Jacob Steinhardt},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/HendrycksBBC0SS21.bib},
    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
    publisher = {OpenReview.net},
    timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
    title = {Aligning {AI} With Shared Human Values},
    url = {https://openreview.net/forum?id=dNy\_RKzJacY},
    year = {2021}
}

@article{reddy-etal-2019-coqa,
    address = {Cambridge, MA},
    author = {Reddy, Siva  and
Chen, Danqi  and
Manning, Christopher D.},
    doi = {10.1162/tacl_a_00266},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {249--266},
    publisher = {MIT Press},
    title = {{C}o{QA}: A Conversational Question Answering Challenge},
    url = {https://aclanthology.org/Q19-1016},
    volume = {7},
    year = {2019}
}

@article{Clark2018ThinkYH,
    author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
    journal = {ArXiv preprint},
    title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
    url = {https://arxiv.org/abs/1803.05457},
    volume = {abs/1803.05457},
    year = {2018}
}

@inproceedings{hendrycks2021aligning,
    author = {Dan Hendrycks and
Collin Burns and
Steven Basart and
Andrew Critch and
Jerry Li and
Dawn Song and
Jacob Steinhardt},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/HendrycksBBC0SS21.bib},
    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
    publisher = {OpenReview.net},
    timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
    title = {Aligning {AI} With Shared Human Values},
    url = {https://openreview.net/forum?id=dNy\_RKzJacY},
    year = {2021}
}

@article{chen2021codex,
    author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
    journal = {ArXiv preprint},
    title = {Evaluating Large Language Models Trained on Code},
    url = {https://arxiv.org/abs/2107.03374},
    volume = {abs/2107.03374},
    year = {2021}
}

@inproceedings{Bisk2020piqa,
    author = {Yonatan Bisk and
Rowan Zellers and
Ronan LeBras and
Jianfeng Gao and
Yejin Choi},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
    booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
    pages = {7432--7439},
    publisher = {{AAAI} Press},
    timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
    title = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
    url = {https://aaai.org/ojs/index.php/AAAI/article/view/6239},
    year = {2020}
}

@inproceedings{lin-etal-2022-truthfulqa,
    address = {Dublin, Ireland},
    author = {Lin, Stephanie  and
Hilton, Jacob  and
Evans, Owain},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.229},
    pages = {3214--3252},
    publisher = {Association for Computational Linguistics},
    title = {{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods},
    url = {https://aclanthology.org/2022.acl-long.229},
    year = {2022}
}

@inproceedings{zellers-etal-2019-hellaswag,
    address = {Florence, Italy},
    author = {Zellers, Rowan  and
Holtzman, Ari  and
Bisk, Yonatan  and
Farhadi, Ali  and
Choi, Yejin},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1472},
    pages = {4791--4800},
    publisher = {Association for Computational Linguistics},
    title = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
    url = {https://aclanthology.org/P19-1472},
    year = {2019}
}

@inproceedings{suzgun-etal-2023-challenging,
    abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65{\%} of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
    address = {Toronto, Canada},
    author = {Suzgun, Mirac  and
Scales, Nathan  and
Sch{\"a}rli, Nathanael  and
Gehrmann, Sebastian  and
Tay, Yi  and
Chung, Hyung Won  and
Chowdhery, Aakanksha  and
Le, Quoc  and
Chi, Ed  and
Zhou, Denny  and
Wei, Jason},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
    doi = {10.18653/v1/2023.findings-acl.824},
    editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
    pages = {13003--13051},
    publisher = {Association for Computational Linguistics},
    title = {Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
    url = {https://aclanthology.org/2023.findings-acl.824},
    year = {2023}
}

@misc{cobbe2021training,
    author = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
    journal = {ArXiv preprint},
    title = {Training Verifiers to Solve Math Word Problems},
    url = {https://arxiv.org/abs/2110.14168},
    volume = {abs/2110.14168},
    year = {2021}
}

@inproceedings{margatina-etal-2022-importance,
    address = {Dublin, Ireland},
    author = {Margatina, Katerina  and
Barrault, Loic  and
Aletras, Nikolaos},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    doi = {10.18653/v1/2022.acl-short.93},
    pages = {825--836},
    publisher = {Association for Computational Linguistics},
    title = {On the Importance of Effectively Adapting Pretrained Language Models for Active Learning},
    url = {https://aclanthology.org/2022.acl-short.93},
    year = {2022}
}

@inproceedings{asai2022attempt,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Asai, Akari  and
Salehi, Mohammadreza  and
Peters, Matthew  and
Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {6655--6672},
    publisher = {Association for Computational Linguistics},
    title = {{ATTEMPT}: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts},
    url = {https://aclanthology.org/2022.emnlp-main.446},
    year = {2022}
}

@inproceedings{qin-eisner-2021-learning,
    address = {Online},
    author = {Qin, Guanghui  and
Eisner, Jason},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.410},
    pages = {5203--5212},
    publisher = {Association for Computational Linguistics},
    title = {Learning How to Ask: Querying {LM}s with Mixtures of Soft Prompts},
    url = {https://aclanthology.org/2021.naacl-main.410},
    year = {2021}
}

@inproceedings{su-etal-2022-transferability,
    address = {Seattle, United States},
    author = {Su, Yusheng  and
Wang, Xiaozhi  and
Qin, Yujia  and
Chan, Chi-Min  and
Lin, Yankai  and
Wang, Huadong  and
Wen, Kaiyue  and
Liu, Zhiyuan  and
Li, Peng  and
Li, Juanzi  and
Hou, Lei  and
Sun, Maosong  and
Zhou, Jie},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2022.naacl-main.290},
    pages = {3949--3969},
    publisher = {Association for Computational Linguistics},
    title = {On Transferability of Prompt Tuning for Natural Language Processing},
    url = {https://aclanthology.org/2022.naacl-main.290},
    year = {2022}
}

@inproceedings{shi2023dept,
    author = {Zhengxiang Shi and Aldo Lipani},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {De{PT}: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning},
    url = {https://openreview.net/forum?id=KjegfPGRde},
    year = {2024}
}

@article{openai2023gpt4,
    author = {OpenAI},
    journal = {ArXiv preprint},
    title = {GPT-4 Technical Report},
    url = {https://arxiv.org/abs/2303.08774},
    volume = {abs/2303.08774},
    year = {2023}
}

@article{touvron2023llama,
    author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
    journal = {ArXiv preprint},
    title = {Llama 2: Open foundation and fine-tuned chat models},
    url = {https://arxiv.org/abs/2307.09288},
    volume = {abs/2307.09288},
    year = {2023}
}

@inproceedings{pmlr-v139-zhao21c,
    author = {Zihao Zhao and
Eric Wallace and
Shi Feng and
Dan Klein and
Sameer Singh},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/ZhaoWFK021.bib},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning,
{ICML} 2021, 18-24 July 2021, Virtual Event},
    editor = {Marina Meila and
Tong Zhang},
    pages = {12697--12706},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Wed, 25 Aug 2021 01:00:00 +0200},
    title = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
    url = {http://proceedings.mlr.press/v139/zhao21c.html},
    volume = {139},
    year = {2021}
}

@inproceedings{vu-etal-2022-spot,
    address = {Dublin, Ireland},
    author = {Vu, Tu  and
Lester, Brian  and
Constant, Noah  and
Al-Rfou{'}, Rami  and
Cer, Daniel},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.346},
    pages = {5039--5059},
    publisher = {Association for Computational Linguistics},
    title = {{SP}o{T}: Better Frozen Model Adaptation through Soft Prompt Transfer},
    url = {https://aclanthology.org/2022.acl-long.346},
    year = {2022}
}

@inproceedings{li-liang-2021-prefix,
    address = {Online},
    author = {Li, Xiang Lisa  and
Liang, Percy},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.353},
    pages = {4582--4597},
    publisher = {Association for Computational Linguistics},
    title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
    url = {https://aclanthology.org/2021.acl-long.353},
    year = {2021}
}

@inproceedings{xu2024wizardlm,
    author = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Qingwei Lin and Daxin Jiang},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {Wizard{LM}: Empowering Large Pre-Trained Language Models to Follow Complex Instructions},
    url = {https://openreview.net/forum?id=CfXh93NDgH},
    year = {2024}
}

@misc{dao2023flashattention2,
    archiveprefix = {arXiv},
    author = {Tri Dao},
    eprint = {2307.08691},
    primaryclass = {cs.LG},
    title = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
    year = {2023}
}

@inproceedings{hosking2024human,
    author = {Tom Hosking and Phil Blunsom and Max Bartolo},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {Human Feedback is not Gold Standard},
    url = {https://openreview.net/forum?id=7W3GLNImfS},
    year = {2024}
}

@article{singh2024aya,
    author = {Singh, Shivalika and Vargus, Freddie and Dsouza, Daniel and Karlsson, B{\"o}rje F and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and OMahony, Laura and others},
    journal = {ArXiv preprint},
    title = {Aya dataset: An open-access collection for multilingual instruction tuning},
    url = {https://arxiv.org/abs/2402.06619},
    volume = {abs/2402.06619},
    year = {2024}
}

@article{jha2023limit,
    author = {Jha, Aditi and Havens, Sam and Dohmann, Jeremey and Trott, Alex and Portes, Jacob},
    journal = {ArXiv preprint},
    title = {LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms},
    url = {https://arxiv.org/abs/2311.13133},
    volume = {abs/2311.13133},
    year = {2023}
}

@inproceedings{shin-etal-2020-autoprompt,
    address = {Online},
    author = {Shin, Taylor  and
Razeghi, Yasaman  and
Logan IV, Robert L.  and
Wallace, Eric  and
Singh, Sameer},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.346},
    pages = {4222--4235},
    publisher = {Association for Computational Linguistics},
    title = {{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts},
    url = {https://aclanthology.org/2020.emnlp-main.346},
    year = {2020}
}

@inproceedings{lester-etal-2021-power,
    address = {Online and Punta Cana, Dominican Republic},
    author = {Lester, Brian  and
Al-Rfou, Rami  and
Constant, Noah},
    booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/2021.emnlp-main.243},
    pages = {3045--3059},
    publisher = {Association for Computational Linguistics},
    title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
    url = {https://aclanthology.org/2021.emnlp-main.243},
    year = {2021}
}

@inproceedings{gu-etal-2022-ppt,
    address = {Dublin, Ireland},
    author = {Gu, Yuxian  and
Han, Xu  and
Liu, Zhiyuan  and
Huang, Minlie},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.576},
    pages = {8410--8423},
    publisher = {Association for Computational Linguistics},
    title = {{PPT}: Pre-trained Prompt Tuning for Few-shot Learning},
    url = {https://aclanthology.org/2022.acl-long.576},
    year = {2022}
}

@inproceedings{longpre-etal-2020-effective,
    address = {Online},
    author = {Longpre, Shayne  and
Wang, Yu  and
DuBois, Chris},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
    doi = {10.18653/v1/2020.findings-emnlp.394},
    pages = {4401--4411},
    publisher = {Association for Computational Linguistics},
    title = {How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?},
    url = {https://aclanthology.org/2020.findings-emnlp.394},
    year = {2020}
}

@inproceedings{min-etal-2022-rethinking,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Min, Sewon  and
Lyu, Xinxi  and
Holtzman, Ari  and
Artetxe, Mikel  and
Lewis, Mike  and
Hajishirzi, Hannaneh  and
Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {11048--11064},
    publisher = {Association for Computational Linguistics},
    title = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
    url = {https://aclanthology.org/2022.emnlp-main.759},
    year = {2022}
}

@inproceedings{chen-etal-2022-adaprompt,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Chen, Yulong  and
Liu, Yang  and
Dong, Li  and
Wang, Shuohang  and
Zhu, Chenguang  and
Zeng, Michael  and
Zhang, Yue},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
    pages = {6057--6068},
    publisher = {Association for Computational Linguistics},
    title = {{A}da{P}rompt: Adaptive Model Training for Prompt-based {NLP}},
    url = {https://aclanthology.org/2022.findings-emnlp.448},
    year = {2022}
}

@inproceedings{le-scao-rush-2021-many,
    address = {Online},
    author = {Le Scao, Teven  and
Rush, Alexander},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.208},
    pages = {2627--2636},
    publisher = {Association for Computational Linguistics},
    title = {How many data points is a prompt worth?},
    url = {https://aclanthology.org/2021.naacl-main.208},
    year = {2021}
}

@article{10.1162/tacl_a_00662,
    abstract = {{Large pretrained language models are widely used in downstream NLP tasks via task- specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating much fewer parameters than full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: We first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimization in a low-cost setup, we then discover a Pareto-optimal set of configurations with strong performance-cost trade-offs across different numbers of parameters that are also highly transferable across different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly outperform existing PEFT methods and are on par or better than FFT without incurring substantial training efficiency costs.}},
    author = {Zhou, Han and Wan, Xingchen and Vulić, Ivan and Korhonen, Anna},
    doi = {10.1162/tacl_a_00662},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00662/2369530/tacl\_a\_00662.pdf},
    issn = {2307-387X},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {525-542},
    title = {{AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning}},
    url = {https://doi.org/10.1162/tacl\_a\_00662},
    volume = {12},
    year = {2024}
}

@inproceedings{alajrami-aletras-2022-pre,
    address = {Dublin, Ireland},
    author = {Alajrami, Ahmed  and
Aletras, Nikolaos},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    doi = {10.18653/v1/2022.acl-short.16},
    pages = {131--147},
    publisher = {Association for Computational Linguistics},
    title = {How does the pre-training objective affect what large language models learn about linguistic properties?},
    url = {https://aclanthology.org/2022.acl-short.16},
    year = {2022}
}

@inproceedings{yamaguchi-etal-2021-frustratingly,
    address = {Online and Punta Cana, Dominican Republic},
    author = {Yamaguchi, Atsuki  and
Chrysostomou, George  and
Margatina, Katerina  and
Aletras, Nikolaos},
    booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/2021.emnlp-main.249},
    pages = {3116--3125},
    publisher = {Association for Computational Linguistics},
    title = {Frustratingly Simple Pretraining Alternatives to Masked Language Modeling},
    url = {https://aclanthology.org/2021.emnlp-main.249},
    year = {2021}
}

@inproceedings{NEURIPS2019_dc6a7e65,
    author = {Zhilin Yang and
Zihang Dai and
Yiming Yang and
Jaime G. Carbonell and
Ruslan Salakhutdinov and
Quoc V. Le},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/YangDYCSL19.bib},
    booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
    editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
    pages = {5754--5764},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    url = {https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html},
    year = {2019}
}

@inproceedings{wang2022usb,
    author = {Yidong Wang and Hao Chen and Yue Fan and Wang SUN and Ran Tao and Wenxin Hou and Renjie Wang and Linyi Yang and Zhi Zhou and Lan-Zhe Guo and Heli Qi and Zhen Wu and Yu-Feng Li and Satoshi Nakamura and Wei Ye and Marios Savvides and Bhiksha Raj and Takahiro Shinozaki and Bernt Schiele and Jindong Wang and Xing Xie and Yue Zhang},
    booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    title = {{USB}: A Unified Semi-supervised Learning Benchmark for Classification},
    url = {https://openreview.net/forum?id=QeuwINa96C},
    year = {2022}
}

@inproceedings{rasmus2015semi,
    author = {Antti Rasmus and
Mathias Berglund and
Mikko Honkala and
Harri Valpola and
Tapani Raiko},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/RasmusBHVR15.bib},
    booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015, December 7-12, 2015,
Montreal, Quebec, Canada},
    editor = {Corinna Cortes and
Neil D. Lawrence and
Daniel D. Lee and
Masashi Sugiyama and
Roman Garnett},
    pages = {3546--3554},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Semi-supervised Learning with Ladder Networks},
    url = {https://proceedings.neurips.cc/paper/2015/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html},
    year = {2015}
}

@article{chapelle2009semi,
    author = {Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
    journal = {IEEE Transactions on Neural Networks},
    number = {3},
    pages = {542--542},
    publisher = {IEEE},
    title = {Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]},
    url = {https://ieeexplore.ieee.org/abstract/document/4787647},
    volume = {20},
    year = {2009}
}

@inproceedings{sun2019fine,
    author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
    journal = {ArXiv preprint},
    title = {How to fine-tune bert for text classification?},
    url = {https://arxiv.org/abs/1905.05583},
    volume = {abs/1905.05583},
    year = {2019}
}

@inproceedings{howard-ruder-2018-universal,
    address = {Melbourne, Australia},
    author = {Howard, Jeremy  and
Ruder, Sebastian},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P18-1031},
    pages = {328--339},
    publisher = {Association for Computational Linguistics},
    title = {Universal Language Model Fine-tuning for Text Classification},
    url = {https://aclanthology.org/P18-1031},
    year = {2018}
}

@inproceedings{NEURIPS2020_d85b63ef,
    author = {Ekin Dogus Cubuk and
Barret Zoph and
Jon Shlens and
Quoc Le},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/CubukZS020.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {RandAugment: Practical Automated Data Augmentation with a Reduced
Search Space},
    url = {https://proceedings.neurips.cc/paper/2020/hash/d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html},
    year = {2020}
}

@inproceedings{zhang2018mixup,
    author = {Hongyi Zhang and
Moustapha Ciss{\'{e}} and
Yann N. Dauphin and
David Lopez{-}Paz},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/ZhangCDL18.bib},
    booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
    publisher = {OpenReview.net},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {mixup: Beyond Empirical Risk Minimization},
    url = {https://openreview.net/forum?id=r1Ddp1-Rb},
    year = {2018}
}

@inproceedings{sohn2020fixmatch,
    author = {Kihyuk Sohn and
David Berthelot and
Nicholas Carlini and
Zizhao Zhang and
Han Zhang and
Colin Raffel and
Ekin Dogus Cubuk and
Alexey Kurakin and
Chun{-}Liang Li},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/SohnBCZZRCKL20.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Wed, 03 Feb 2021 00:00:00 +0100},
    title = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and
Confidence},
    url = {https://proceedings.neurips.cc/paper/2020/hash/06964dce9addb1c5cb5d6e3d9838f733-Abstract.html},
    year = {2020}
}

@inproceedings{logeswaran-etal-2019-zero,
    address = {Florence, Italy},
    author = {Logeswaran, Lajanugen  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina  and
Devlin, Jacob  and
Lee, Honglak},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1335},
    pages = {3449--3460},
    publisher = {Association for Computational Linguistics},
    title = {Zero-Shot Entity Linking by Reading Entity Descriptions},
    url = {https://aclanthology.org/P19-1335},
    year = {2019}
}

@inproceedings{lee2013pseudo,
    author = {Lee, Dong-Hyun and others},
    booktitle = {Workshop on challenges in representation learning, ICML},
    pages = {896},
    title = {Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
    url = {https://www.kaggle.com/blobs/download/forum-message-attachment-files/746/pseudo_label_final.pdf},
    year = {2013}
}

@article{miyato2018virtual,
    author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
    journal = {ArXiv preprint},
    title = {Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
    url = {https://arxiv.org/abs/1704.03976},
    volume = {abs/1704.03976},
    year = {2017}
}

@inproceedings{berthelot2019mixmatch,
    author = {David Berthelot and
Nicholas Carlini and
Ian J. Goodfellow and
Nicolas Papernot and
Avital Oliver and
Colin Raffel},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/BerthelotCGPOR19.bib},
    booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
    editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
    pages = {5050--5060},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {MixMatch: {A} Holistic Approach to Semi-Supervised Learning},
    url = {https://proceedings.neurips.cc/paper/2019/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html},
    year = {2019}
}

@article{10.1162/tacl_a_00517,
    address = {Cambridge, MA},
    author = {Hou, Zejiang  and
Salazar, Julian  and
Polovets, George},
    doi = {10.1162/tacl_a_00517},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {1249--1265},
    publisher = {MIT Press},
    title = {Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation},
    url = {https://aclanthology.org/2022.tacl-1.72},
    volume = {10},
    year = {2022}
}

@inproceedings{berthelot2019remixmatch,
    author = {David Berthelot and
Nicholas Carlini and
Ekin D. Cubuk and
Alex Kurakin and
Kihyuk Sohn and
Han Zhang and
Colin Raffel},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/BerthelotCCKSZR20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {ReMixMatch: Semi-Supervised Learning with Distribution Matching and
Augmentation Anchoring},
    url = {https://openreview.net/forum?id=HklkeR4KPB},
    year = {2020}
}

@inproceedings{xu2021dash,
    author = {Yi Xu and
Lei Shang and
Jinxing Ye and
Qi Qian and
Yu{-}Feng Li and
Baigui Sun and
Hao Li and
Rong Jin},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/XuSYQLSLJ21.bib},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning,
{ICML} 2021, 18-24 July 2021, Virtual Event},
    editor = {Marina Meila and
Tong Zhang},
    pages = {11525--11536},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Mon, 17 Jan 2022 00:00:00 +0100},
    title = {Dash: Semi-Supervised Learning with Dynamic Thresholding},
    url = {http://proceedings.mlr.press/v139/xu21e.html},
    volume = {139},
    year = {2021}
}

@inproceedings{li2021comatch,
    author = {Junnan Li and
Caiming Xiong and
Steven C. H. Hoi},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iccv/0001XH21.bib},
    booktitle = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
2021, Montreal, QC, Canada, October 10-17, 2021},
    doi = {10.1109/ICCV48922.2021.00934},
    pages = {9455--9464},
    publisher = {{IEEE}},
    timestamp = {Fri, 11 Mar 2022 00:00:00 +0100},
    title = {CoMatch: Semi-supervised Learning with Contrastive Graph Regularization},
    url = {https://doi.org/10.1109/ICCV48922.2021.00934},
    year = {2021}
}

@inproceedings{fan2021revisiting,
    author = {Fan, Yue and Kukleva, Anna and Schiele, Bernt},
    booktitle = {DAGM German Conference on Pattern Recognition},
    organization = {Springer},
    pages = {63--78},
    title = {Revisiting Consistency Regularization for Semi-Supervised Learning},
    year = {2021}
}

@inproceedings{zhang2021flexmatch,
    author = {Bowen Zhang and
Yidong Wang and
Wenxin Hou and
Hao Wu and
Jindong Wang and
Manabu Okumura and
Takahiro Shinozaki},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/ZhangWHWWOS21.bib},
    booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
    editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
    pages = {18408--18419},
    timestamp = {Tue, 03 May 2022 01:00:00 +0200},
    title = {FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo
Labeling},
    url = {https://proceedings.neurips.cc/paper/2021/hash/995693c15f439e3d189b06e89d145dd5-Abstract.html},
    year = {2021}
}

@inproceedings{berthelot2021adamatch,
    author = {David Berthelot and
Rebecca Roelofs and
Kihyuk Sohn and
Nicholas Carlini and
Alexey Kurakin},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/BerthelotRSCK22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {AdaMatch: {A} Unified Approach to Semi-Supervised Learning and Domain
Adaptation},
    url = {https://openreview.net/forum?id=Q5uh1Nvv5dm},
    year = {2022}
}

@inproceedings{zheng2022simmatch,
    author = {Mingkai Zheng and
Shan You and
Lang Huang and
Fei Wang and
Chen Qian and
Chang Xu},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cvpr/ZhengYHWQX22.bib},
    booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
    doi = {10.1109/CVPR52688.2022.01407},
    pages = {14451--14461},
    publisher = {{IEEE}},
    timestamp = {Sat, 04 Feb 2023 00:00:00 +0100},
    title = {SimMatch: Semi-supervised Learning with Similarity Matching},
    url = {https://doi.org/10.1109/CVPR52688.2022.01407},
    year = {2022}
}

@article{mikolov2013efficient,
    author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal = {Proceedings of Workshop at ICLR},
    title = {Efficient estimation of word representations in vector space},
    url = {https://arxiv.org/abs/1301.3781},
    year = {2013}
}

@misc{Yelp,
    title = {Yelp Review},
    url = {https://www.yelp.com/dataset},
    year = {2014}
}

@inproceedings{chang2008importance,
    author = {Chang, Ming-Wei and Ratinov, Lev and Roth, Dan and Srikumar, Vivek},
    booktitle = {Proceedings of the 23rd national conference on Artificial intelligence-Volume 2},
    pages = {830--835},
    title = {Importance of semantic representation: dataless classification},
    url = {https://www.aaai.org/Papers/AAAI/2008/AAAI08-132.pdf},
    year = {2008}
}

@inproceedings{10.5555/2969239.2969312,
    author = {Xiang Zhang and
Junbo Jake Zhao and
Yann LeCun},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/ZhangZL15.bib},
    booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015, December 7-12, 2015,
Montreal, Quebec, Canada},
    editor = {Corinna Cortes and
Neil D. Lawrence and
Daniel D. Lee and
Masashi Sugiyama and
Roman Garnett},
    pages = {649--657},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Character-level Convolutional Networks for Text Classification},
    url = {https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html},
    year = {2015}
}

@inproceedings{10.1145/2507157.2507163,
    author = {Julian J. McAuley and
Jure Leskovec},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/recsys/McAuleyL13.bib},
    booktitle = {Seventh {ACM} Conference on Recommender Systems, RecSys '13, Hong
Kong, China, October 12-16, 2013},
    doi = {10.1145/2507157.2507163},
    editor = {Qiang Yang and
Irwin King and
Qing Li and
Pearl Pu and
George Karypis},
    pages = {165--172},
    publisher = {{ACM}},
    timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
    title = {Hidden factors and hidden topics: understanding rating dimensions
with review text},
    url = {https://doi.org/10.1145/2507157.2507163},
    year = {2013}
}

@inproceedings{maas-etal-2011-learning,
    address = {Portland, Oregon, USA},
    author = {Maas, Andrew L.  and
Daly, Raymond E.  and
Pham, Peter T.  and
Huang, Dan  and
Ng, Andrew Y.  and
Potts, Christopher},
    booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
    pages = {142--150},
    publisher = {Association for Computational Linguistics},
    title = {Learning Word Vectors for Sentiment Analysis},
    url = {https://aclanthology.org/P11-1015},
    year = {2011}
}

@inproceedings{li-etal-2021-semi-supervised,
    address = {Online},
    author = {Li, Changchun  and
Li, Ximing  and
Ouyang, Jihong},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.391},
    pages = {5044--5053},
    publisher = {Association for Computational Linguistics},
    title = {Semi-Supervised Text Classification with Balanced Deep Representation Distributions},
    url = {https://aclanthology.org/2021.acl-long.391},
    year = {2021}
}

@inproceedings{kedzie-mckeown-2019-good,
    address = {Tokyo, Japan},
    author = {Kedzie, Chris  and
McKeown, Kathleen},
    booktitle = {Proceedings of the 12th International Conference on Natural Language Generation},
    doi = {10.18653/v1/W19-8672},
    pages = {584--593},
    publisher = {Association for Computational Linguistics},
    title = {A Good Sample is Hard to Find: Noise Injection Sampling and Self-Training for Neural Language Generation Models},
    url = {https://aclanthology.org/W19-8672},
    year = {2019}
}

@inproceedings{He2020Revisiting,
    author = {Junxian He and
Jiatao Gu and
Jiajun Shen and
Marc'Aurelio Ranzato},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/HeGSR20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {Revisiting Self-Training for Neural Sequence Generation},
    url = {https://openreview.net/forum?id=SJgdnAVKDH},
    year = {2020}
}

@inproceedings{chen-etal-2020-local,
    address = {Online},
    author = {Chen, Jiaao  and
Wang, Zhenghui  and
Tian, Ran  and
Yang, Zichao  and
Yang, Diyi},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.95},
    pages = {1241--1251},
    publisher = {Association for Computational Linguistics},
    title = {Local Additivity Based Data Augmentation for Semi-supervised {NER}},
    url = {https://aclanthology.org/2020.emnlp-main.95},
    year = {2020}
}

@inproceedings{meng-etal-2020-text,
    address = {Online},
    author = {Meng, Yu  and
Zhang, Yunyi  and
Huang, Jiaxin  and
Xiong, Chenyan  and
Ji, Heng  and
Zhang, Chao  and
Han, Jiawei},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.724},
    pages = {9006--9017},
    publisher = {Association for Computational Linguistics},
    title = {Text Classification Using Label Names Only: A Language Model Self-Training Approach},
    url = {https://aclanthology.org/2020.emnlp-main.724},
    year = {2020}
}

@inproceedings{chen-etal-2020-mixtext,
    address = {Online},
    author = {Chen, Jiaao  and
Yang, Zichao  and
Yang, Diyi},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.194},
    pages = {2147--2157},
    publisher = {Association for Computational Linguistics},
    title = {{M}ix{T}ext: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification},
    url = {https://aclanthology.org/2020.acl-main.194},
    year = {2020}
}

@inproceedings{mcclosky-etal-2006-effective,
    address = {New York City, USA},
    author = {McClosky, David  and
Charniak, Eugene  and
Johnson, Mark},
    booktitle = {Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference},
    pages = {152--159},
    publisher = {Association for Computational Linguistics},
    title = {Effective Self-Training for Parsing},
    url = {https://aclanthology.org/N06-1020},
    year = {2006}
}

@inproceedings{NEURIPS2020_f23d125d,
    author = {Subhabrata Mukherjee and
Ahmed Hassan Awadallah},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/MukherjeeA20.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Uncertainty-aware Self-training for Few-shot Text Classification},
    url = {https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html},
    year = {2020}
}

@inproceedings{gera2022zero,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Gera, Ariel  and
Halfon, Alon  and
Shnarch, Eyal  and
Perlitz, Yotam  and
Ein-Dor, Liat  and
Slonim, Noam},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {1107--1119},
    publisher = {Association for Computational Linguistics},
    title = {Zero-Shot Text Classification with Self-Training},
    url = {https://aclanthology.org/2022.emnlp-main.73},
    year = {2022}
}

@inproceedings{Glorot2011,
    author = {Xavier Glorot and
Antoine Bordes and
Yoshua Bengio},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/GlorotBB11.bib},
    booktitle = {Proceedings of the 28th International Conference on Machine Learning,
{ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
    editor = {Lise Getoor and
Tobias Scheffer},
    pages = {513--520},
    publisher = {Omnipress},
    timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
    title = {Domain Adaptation for Large-Scale Sentiment Classification: {A} Deep
Learning Approach},
    url = {https://icml.cc/2011/papers/342\_icmlpaper.pdf},
    year = {2011}
}

@inproceedings{xie2020self,
    author = {Qizhe Xie and
Minh{-}Thang Luong and
Eduard H. Hovy and
Quoc V. Le},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cvpr/XieLHL20.bib},
    booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
    doi = {10.1109/CVPR42600.2020.01070},
    pages = {10684--10695},
    publisher = {{IEEE}},
    timestamp = {Tue, 11 Aug 2020 01:00:00 +0200},
    title = {Self-Training With Noisy Student Improves ImageNet Classification},
    url = {https://doi.org/10.1109/CVPR42600.2020.01070},
    year = {2020}
}

@inproceedings{dong-de-melo-2019-robust,
    address = {Hong Kong, China},
    author = {Dong, Xin  and
de Melo, Gerard},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1658},
    pages = {6306--6310},
    publisher = {Association for Computational Linguistics},
    title = {A Robust Self-Learning Framework for Cross-Lingual Text Classification},
    url = {https://aclanthology.org/D19-1658},
    year = {2019}
}

@inproceedings{10.3115/1220175.1220218,
    address = {Sydney, Australia},
    author = {McClosky, David  and
Charniak, Eugene  and
Johnson, Mark},
    booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.3115/1220175.1220218},
    pages = {337--344},
    publisher = {Association for Computational Linguistics},
    title = {Reranking and Self-Training for Parser Adaptation},
    url = {https://aclanthology.org/P06-1043},
    year = {2006}
}

@inproceedings{yarowsky-1995-unsupervised,
    address = {Cambridge, Massachusetts, USA},
    author = {Yarowsky, David},
    booktitle = {33rd Annual Meeting of the Association for Computational Linguistics},
    doi = {10.3115/981658.981684},
    pages = {189--196},
    publisher = {Association for Computational Linguistics},
    title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
    url = {https://aclanthology.org/P95-1026},
    year = {1995}
}

@inproceedings{lewis2019bart,
    address = {Online},
    author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.703},
    pages = {7871--7880},
    publisher = {Association for Computational Linguistics},
    title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    url = {https://aclanthology.org/2020.acl-main.703},
    year = {2020}
}

@article{radford2019language,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
    journal = {OpenAI blog},
    number = {8},
    title = {Language models are unsupervised multitask learners},
    url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
    volume = {1},
    year = {2019}
}

@inproceedings{devlin2018bert,
    address = {Minneapolis, Minnesota},
    author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    doi = {10.18653/v1/N19-1423},
    pages = {4171--4186},
    publisher = {Association for Computational Linguistics},
    title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    url = {https://aclanthology.org/N19-1423},
    year = {2019}
}

@article{liu2019roberta,
    author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    journal = {ArXiv preprint},
    title = {Roberta: A robustly optimized bert pretraining approach},
    url = {https://arxiv.org/abs/1907.11692},
    volume = {abs/1907.11692},
    year = {2019}
}

@inproceedings{lan2019albert,
    author = {Zhenzhong Lan and
Mingda Chen and
Sebastian Goodman and
Kevin Gimpel and
Piyush Sharma and
Radu Soricut},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
Representations},
    url = {https://openreview.net/forum?id=H1eA7AEtvS},
    year = {2020}
}

@inproceedings{10.1145/3404835.3463241,
    abstract = {Evaluation is crucial in the development process of task-oriented dialogue systems. As an evaluation method, user simulation allows us to tackle issues such as scalability and cost-efficiency, making it a viable choice for large-scale automatic evaluation. To help build a human-like user simulator that can measure the quality of a dialogue, we propose the following task: simulating user satisfaction for the evaluation of task-oriented dialogue systems. The purpose of the task is to increase the evaluation power of user simulations and to make the simulation more human-like. To overcome a lack of annotated data, we propose a user satisfaction annotation dataset, USS, that includes 6,800 dialogues sampled from multiple domains, spanning real-world e-commerce dialogues, task-oriented dialogues constructed through Wizard-of-Oz experiments, and movie recommendation dialogues. All user utterances in those dialogues, as well as the dialogues themselves, have been labeled based on a 5-level satisfaction scale. We also share three baseline methods for user satisfaction prediction and action prediction tasks. Experiments conducted on the USS dataset suggest that distributed representations outperform feature-based methods. A model based on hierarchical GRUs achieves the best performance in in-domain user satisfaction prediction, while a BERT-based model has better cross-domain generalization ability.},
    address = {New York, NY, USA},
    author = {Sun, Weiwei and Zhang, Shuo and Balog, Krisztian and Ren, Zhaochun and Ren, Pengjie and Chen, Zhumin and de Rijke, Maarten},
    booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    doi = {10.1145/3404835.3463241},
    isbn = {9781450380379},
    keywords = {conversational information access, task-oriented dialogue, conversational recommendation, user simulation},
    location = {Virtual Event, Canada},
    numpages = {8},
    pages = {2499–2506},
    publisher = {Association for Computing Machinery},
    series = {SIGIR '21},
    title = {Simulating User Satisfaction for the Evaluation of Task-Oriented Dialogue Systems},
    url = {https://doi.org/10.1145/3404835.3463241},
    year = {2021}
}

@inproceedings{cai-lapata-2019-semi,
    address = {Hong Kong, China},
    author = {Cai, Rui  and
Lapata, Mirella},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1094},
    pages = {1018--1027},
    publisher = {Association for Computational Linguistics},
    title = {Semi-Supervised Semantic Role Labeling with Cross-View Training},
    url = {https://aclanthology.org/D19-1094},
    year = {2019}
}

@inproceedings{10.5555/3495724.3496047,
    author = {Barret Zoph and
Golnaz Ghiasi and
Tsung{-}Yi Lin and
Yin Cui and
Hanxiao Liu and
Ekin Dogus Cubuk and
Quoc Le},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/ZophGLCLC020.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Rethinking Pre-training and Self-training},
    url = {https://proceedings.neurips.cc/paper/2020/hash/27e9661e033a73a6ad8cefcde965c54d-Abstract.html},
    year = {2020}
}

@inproceedings{kipf2017semi,
    author = {Thomas N. Kipf and
Max Welling},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
    booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
    publisher = {OpenReview.net},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Semi-Supervised Classification with Graph Convolutional Networks},
    url = {https://openreview.net/forum?id=SJU4ayYgl},
    year = {2017}
}

@inproceedings{dehaven2022improving,
    author = {DeHaven, Mitchell and Billa, Jayadev},
    journal = {ArXiv preprint},
    title = {Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training},
    url = {https://arxiv.org/abs/2207.00659},
    volume = {abs/2207.00659},
    year = {2022}
}

@inproceedings{khashabi-etal-2020-unifiedqa,
    address = {Online},
    author = {Khashabi, Daniel  and
Min, Sewon  and
Khot, Tushar  and
Sabharwal, Ashish  and
Tafjord, Oyvind  and
Clark, Peter  and
Hajishirzi, Hannaneh},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
    doi = {10.18653/v1/2020.findings-emnlp.171},
    pages = {1896--1907},
    publisher = {Association for Computational Linguistics},
    title = {{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System},
    url = {https://aclanthology.org/2020.findings-emnlp.171},
    year = {2020}
}

@inproceedings{aribandi2022ext,
    author = {Vamsi Aribandi and
Yi Tay and
Tal Schuster and
Jinfeng Rao and
Huaixiu Steven Zheng and
Sanket Vaibhav Mehta and
Honglei Zhuang and
Vinh Q. Tran and
Dara Bahri and
Jianmo Ni and
Jai Prakash Gupta and
Kai Hui and
Sebastian Ruder and
Donald Metzler},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/AribandiTSRZMZ022.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
    url = {https://openreview.net/forum?id=Vzh1BFUCiIX},
    year = {2022}
}

@inproceedings{ouyang2022training,
    author = {Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Gray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    title = {Training language models to follow instructions with human feedback},
    url = {https://openreview.net/forum?id=TG8KACxEON},
    year = {2022}
}

@inproceedings{min-etal-2022-metaicl,
    address = {Seattle, United States},
    author = {Min, Sewon  and
Lewis, Mike  and
Zettlemoyer, Luke  and
Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2022.naacl-main.201},
    pages = {2791--2809},
    publisher = {Association for Computational Linguistics},
    title = {{M}eta{ICL}: Learning to Learn In Context},
    url = {https://aclanthology.org/2022.naacl-main.201},
    year = {2022}
}

@inproceedings{mishra-etal-2022-cross,
    address = {Dublin, Ireland},
    author = {Mishra, Swaroop  and
Khashabi, Daniel  and
Baral, Chitta  and
Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.244},
    pages = {3470--3487},
    publisher = {Association for Computational Linguistics},
    title = {Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
    url = {https://aclanthology.org/2022.acl-long.244},
    year = {2022}
}

@inproceedings{shi2023rethinking,
    address = {Toronto, Canada},
    author = {Shi, Zhengxiang and Tonolini, Francesco and Aletras, Nikolaos and Yilmaz, Emine and Kazai, Gabriella and Jiao, Yunlong},
    booktitle = {Findings of ACL 2023},
    publisher = {Association for Computational Linguistics},
    title = {Rethinking Semi-supervised Learning with Language Models},
    year = {2023}
}

@inproceedings{10.5555/3495724.3496249,
    author = {Qizhe Xie and
Zihang Dai and
Eduard H. Hovy and
Thang Luong and
Quoc Le},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/XieDHL020.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Unsupervised Data Augmentation for Consistency Training},
    url = {https://proceedings.neurips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html},
    year = {2020}
}

@book{abney2007semisupervised,
    author = {Abney, Steven},
    publisher = {Chapman and Hall/CRC},
    title = {Semisupervised learning for computational linguistics},
    url = {https://www.taylorfrancis.com/books/mono/10.1201/9781420010800/semisupervised-learning-computational-linguistics-steven-abney},
    year = {2007}
}

@inproceedings{sogaard-2010-simple,
    address = {Uppsala, Sweden},
    author = {S{\o}gaard, Anders},
    booktitle = {Proceedings of the {ACL} 2010 Conference Short Papers},
    pages = {205--208},
    publisher = {Association for Computational Linguistics},
    title = {Simple Semi-Supervised Training of Part-Of-Speech Taggers},
    url = {https://aclanthology.org/P10-2038},
    year = {2010}
}

@inproceedings{clark-etal-2018-semi,
    address = {Brussels, Belgium},
    author = {Clark, Kevin  and
Luong, Minh-Thang  and
Manning, Christopher D.  and
Le, Quoc},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D18-1217},
    pages = {1914--1925},
    publisher = {Association for Computational Linguistics},
    title = {Semi-Supervised Sequence Modeling with Cross-View Training},
    url = {https://aclanthology.org/D18-1217},
    year = {2018}
}

@inproceedings{peters-etal-2018-deep,
    address = {New Orleans, Louisiana},
    author = {Peters, Matthew E.  and
Neumann, Mark  and
Iyyer, Mohit  and
Gardner, Matt  and
Clark, Christopher  and
Lee, Kenton  and
Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    doi = {10.18653/v1/N18-1202},
    pages = {2227--2237},
    publisher = {Association for Computational Linguistics},
    title = {Deep Contextualized Word Representations},
    url = {https://aclanthology.org/N18-1202},
    year = {2018}
}

@inproceedings{gururangan-etal-2020-dont,
    address = {Online},
    author = {Gururangan, Suchin  and
Marasovi{\'c}, Ana  and
Swayamdipta, Swabha  and
Lo, Kyle  and
Beltagy, Iz  and
Downey, Doug  and
Smith, Noah A.},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.740},
    pages = {8342--8360},
    publisher = {Association for Computational Linguistics},
    title = {Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks},
    url = {https://aclanthology.org/2020.acl-main.740},
    year = {2020}
}

@inproceedings{gururangan-etal-2019-variational,
    address = {Florence, Italy},
    author = {Gururangan, Suchin  and
Dang, Tam  and
Card, Dallas  and
Smith, Noah A.},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1590},
    pages = {5880--5894},
    publisher = {Association for Computational Linguistics},
    title = {Variational Pretraining for Semi-supervised Text Classification},
    url = {https://aclanthology.org/P19-1590},
    year = {2019}
}

@inproceedings{artetxe-etal-2018-robust,
    address = {Melbourne, Australia},
    author = {Artetxe, Mikel  and
Labaka, Gorka  and
Agirre, Eneko},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P18-1073},
    pages = {789--798},
    publisher = {Association for Computational Linguistics},
    title = {A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings},
    url = {https://aclanthology.org/P18-1073},
    year = {2018}
}

@inproceedings{ott2019fairseq,
    address = {Minneapolis, Minnesota},
    author = {Ott, Myle  and
Edunov, Sergey  and
Baevski, Alexei  and
Fan, Angela  and
Gross, Sam  and
Ng, Nathan  and
Grangier, David  and
Auli, Michael},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)},
    doi = {10.18653/v1/N19-4009},
    pages = {48--53},
    publisher = {Association for Computational Linguistics},
    title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
    url = {https://aclanthology.org/N19-4009},
    year = {2019}
}

@inproceedings{zhou-etal-2022-flipda,
    address = {Dublin, Ireland},
    author = {Zhou, Jing  and
Zheng, Yanan  and
Tang, Jie  and
Jian, Li  and
Yang, Zhilin},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.592},
    pages = {8646--8665},
    publisher = {Association for Computational Linguistics},
    title = {{F}lip{DA}: Effective and Robust Data Augmentation for Few-Shot Learning},
    url = {https://aclanthology.org/2022.acl-long.592},
    year = {2022}
}

@inproceedings{10.5555/3495724.3495883,
    author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Language Models are Few-Shot Learners},
    url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
    year = {2020}
}

@article{goel2022pars,
    author = {Goel, Arushi and Jiao, Yunlong and Massiah, Jordan},
    journal = {ArXiv preprint},
    title = {PARS: Pseudo-label aware robust sample selection for learning with noisy labels},
    url = {https://arxiv.org/abs/2201.10836},
    volume = {abs/2201.10836},
    year = {2022}
}

@article{10.5555/3455716.3455856,
    author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
    journal = {J. Mach. Learn. Res.},
    pages = {140:1--140:67},
    timestamp = {Fri, 05 Feb 2021 00:00:00 +0100},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
    url = {http://jmlr.org/papers/v21/20-074.html},
    volume = {21},
    year = {2020}
}

@inproceedings{wang-etal-2022-super,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Wang, Yizhong  and
Mishra, Swaroop  and
Alipoormolabashi, Pegah  and
Kordi, Yeganeh  and
Mirzaei, Amirreza  and
Naik, Atharva  and
Ashok, Arjun  and
Dhanasekaran, Arut Selvan  and
Arunkumar, Anjana  and
Stap, David  and
Pathak, Eshaan  and
Karamanolakis, Giannis  and
Lai, Haizhi  and
Purohit, Ishan  and
Mondal, Ishani  and
Anderson, Jacob  and
Kuznia, Kirby  and
Doshi, Krima  and
Pal, Kuntal Kumar  and
Patel, Maitreya  and
Moradshahi, Mehrad  and
Parmar, Mihir  and
Purohit, Mirali  and
Varshney, Neeraj  and
Kaza, Phani Rohitha  and
Verma, Pulkit  and
Puri, Ravsehaj Singh  and
Karia, Rushang  and
Doshi, Savan  and
Sampat, Shailaja Keyur  and
Mishra, Siddhartha  and
Reddy A, Sujan  and
Patro, Sumanta  and
Dixit, Tanay  and
Shen, Xudong},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {5085--5109},
    publisher = {Association for Computational Linguistics},
    title = {Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks},
    url = {https://aclanthology.org/2022.emnlp-main.340},
    year = {2022}
}

@inproceedings{wang2021selftuning,
    author = {Ximei Wang and
Jinghan Gao and
Mingsheng Long and
Jianmin Wang},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/WangGLW21.bib},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning,
{ICML} 2021, 18-24 July 2021, Virtual Event},
    editor = {Marina Meila and
Tong Zhang},
    pages = {10738--10748},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Wed, 25 Aug 2021 01:00:00 +0200},
    title = {Self-Tuning for Data-Efficient Deep Learning},
    url = {http://proceedings.mlr.press/v139/wang21g.html},
    volume = {139},
    year = {2021}
}

@inproceedings{arazo2020pseudo,
    author = {Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel E and McGuinness, Kevin},
    journal = {ArXiv preprint},
    title = {Pseudo-labeling and confirmation bias in deep semi-supervised learning},
    url = {https://arxiv.org/abs/1908.02983},
    volume = {abs/1908.02983},
    year = {2019}
}

@inproceedings{DST2022,
    author = {Chen, Baixu and Jiang, Junguang and Wang, Ximei and Wan, Pengfei and Wang, Jianmin and Long, Mingsheng},
    booktitle = {Advances in Neural Information Processing Systems},
    series = {NIPS'22},
    title = {Debiased Self-Training for Semi-Supervised Learning},
    url = {https://openreview.net/forum?id=NI7moUOKtc},
    year = {2022}
}

@inproceedings{li-etal-2021-task-adaptive,
    address = {Punta Cana, Dominican Republic},
    author = {Li, Shiyang  and
Yavuz, Semih  and
Chen, Wenhu  and
Yan, Xifeng},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
    doi = {10.18653/v1/2021.findings-emnlp.86},
    pages = {1006--1015},
    publisher = {Association for Computational Linguistics},
    title = {Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding},
    url = {https://aclanthology.org/2021.findings-emnlp.86},
    year = {2021}
}

@inproceedings{xu2021self,
    author = {Xu, Qiantong and Baevski, Alexei and Likhomanenko, Tatiana and Tomasello, Paden and Conneau, Alexis and Collobert, Ronan and Synnaeve, Gabriel and Auli, Michael},
    booktitle = {ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    organization = {IEEE},
    pages = {3030--3034},
    title = {Self-training and pre-training are complementary for speech recognition},
    url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414641},
    year = {2021}
}

@inproceedings{10.5555/3157096.3157135,
    author = {Konstantinos Bousmalis and
George Trigeorgis and
Nathan Silberman and
Dilip Krishnan and
Dumitru Erhan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/BousmalisTSKE16.bib},
    booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain},
    editor = {Daniel D. Lee and
Masashi Sugiyama and
Ulrike von Luxburg and
Isabelle Guyon and
Roman Garnett},
    pages = {343--351},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Domain Separation Networks},
    url = {https://proceedings.neurips.cc/paper/2016/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html},
    year = {2016}
}

@article{zhou2005tri,
    author = {Zhou, Zhi-Hua and Li, Ming},
    journal = {IEEE Transactions on knowledge and Data Engineering},
    number = {11},
    pages = {1529--1541},
    publisher = {IEEE},
    title = {Tri-training: Exploiting unlabeled data using three classifiers},
    url = {https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkde05.pdf},
    volume = {17},
    year = {2005}
}

@misc{petrak_improving_2022,
    author = {Petrak, Dominic and Moosavi, Nafise Sadat and Gurevych, Iryna},
    journal = {ArXiv preprint},
    title = {Improving the {Numerical} {Reasoning} {Skills} of {Pretrained} {Language} {Models}},
    url = {https://arxiv.org/abs/2205.06733},
    volume = {abs/2205.06733},
    year = {2022}
}

@inproceedings{saito2019semi,
    author = {Kuniaki Saito and
Donghyun Kim and
Stan Sclaroff and
Trevor Darrell and
Kate Saenko},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iccv/SaitoKSDS19.bib},
    booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
2019, Seoul, Korea (South), October 27 - November 2, 2019},
    doi = {10.1109/ICCV.2019.00814},
    pages = {8049--8057},
    publisher = {{IEEE}},
    timestamp = {Thu, 05 Mar 2020 00:00:00 +0100},
    title = {Semi-Supervised Domain Adaptation via Minimax Entropy},
    url = {https://doi.org/10.1109/ICCV.2019.00814},
    year = {2019}
}

@misc{jiang_simple_2022,
    author = {Jiang, Shaojie and Zhang, Ruqing and Vakulenko, Svitlana and de Rijke, Maarten},
    journal = {ArXiv preprint},
    title = {A {Simple} {Contrastive} {Learning} {Objective} for {Alleviating} {Neural} {Text} {Degeneration}},
    url = {https://arxiv.org/abs/2205.02517},
    volume = {abs/2205.02517},
    year = {2022}
}

@inproceedings{lecun_deep_2015,
    author = {Ruslan Salakhutdinov},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/kdd/Salakhutdinov14.bib},
    booktitle = {The 20th {ACM} {SIGKDD} International Conference on Knowledge Discovery
and Data Mining, {KDD} '14, New York, NY, {USA} - August 24 - 27,
2014},
    doi = {10.1145/2623330.2630809},
    editor = {Sofus A. Macskassy and
Claudia Perlich and
Jure Leskovec and
Wei Wang and
Rayid Ghani},
    pages = {1973},
    publisher = {{ACM}},
    timestamp = {Tue, 06 Nov 2018 00:00:00 +0100},
    title = {Deep learning},
    url = {https://doi.org/10.1145/2623330.2630809},
    year = {2014}
}

@misc{liu_visual_2022,
    author = {Liu, Fangyu and Emerson, Guy and Collier, Nigel},
    journal = {ArXiv preprint},
    title = {Visual {Spatial} {Reasoning}},
    url = {https://arxiv.org/abs/2205.00363},
    volume = {abs/2205.00363},
    year = {2022}
}

@inproceedings{rosin_time_2022,
    abstract = {Our world is constantly evolving, and so is the content on the web. Consequently, our languages, often said to mirror the world, are dynamic in nature. However, most current contextual language models are static and cannot adapt to changes over time. In this work, we propose a temporal contextual language model called TempoBERT, which uses time as an additional context of texts. Our technique is based on modifying texts with temporal information and performing time masking—specific masking for the supplementary time information. We leverage our approach for the tasks of semantic change detection and sentence time prediction, experimenting on diverse datasets in terms of time, size, genre, and language. Our extensive evaluation shows that both tasks benefit from exploiting time masking.},
    address = {Virtual Event AZ USA},
    author = {Rosin, Guy D. and Guy, Ido and Radinsky, Kira},
    booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
    doi = {10.1145/3488560.3498529},
    file = {Rosin et al. - 2022 - Time Masking for Temporal Language Models.pdf:files/448/Rosin et al. - 2022 - Time Masking for Temporal Language Models.pdf:application/pdf},
    isbn = {978-1-4503-9132-0},
    language = {en},
    pages = {833--841},
    publisher = {ACM},
    title = {Time {Masking} for {Temporal} {Language} {Models}},
    url = {https://dl.acm.org/doi/10.1145/3488560.3498529},
    urldate = {2022-08-08},
    year = {2022}
}

@inproceedings{liu_things_2022,
    address = {Dublin, Ireland},
    author = {Liu, Xiao  and
Yin, Da  and
Feng, Yansong  and
Zhao, Dongyan},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.168},
    pages = {2365--2376},
    publisher = {Association for Computational Linguistics},
    title = {Things not Written in Text: Exploring Spatial Commonsense from Visual Signals},
    url = {https://aclanthology.org/2022.acl-long.168},
    year = {2022}
}

@inproceedings{zhang_contrastive_2022,
    address = {Seattle, United States},
    author = {Zhang, Rui  and
Ji, Yangfeng  and
Zhang, Yue  and
Passonneau, Rebecca J.},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts},
    doi = {10.18653/v1/2022.naacl-tutorials.6},
    pages = {39--47},
    publisher = {Association for Computational Linguistics},
    title = {Contrastive Data and Learning for Natural Language Processing},
    url = {https://aclanthology.org/2022.naacl-tutorials.6},
    year = {2022}
}

@misc{li_kfcnet_2021,
    author = {Li, Haonan and Gong, Yeyun and Jiao, Jian and Zhang, Ruofei and Baldwin, Timothy and Duan, Nan},
    journal = {ArXiv preprint},
    title = {{KFCNet}: {Knowledge} {Filtering} and {Contrastive} {Learning} {Network} for {Generative} {Commonsense} {Reasoning}},
    url = {https://arxiv.org/abs/2109.06704},
    volume = {abs/2109.06704},
    year = {2021}
}

@inproceedings{klein_contrastive_2020,
    address = {Online},
    author = {Klein, Tassilo  and
Nabi, Moin},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.671},
    pages = {7517--7523},
    publisher = {Association for Computational Linguistics},
    title = {Contrastive Self-Supervised Learning for Commonsense Reasoning},
    url = {https://aclanthology.org/2020.acl-main.671},
    year = {2020}
}

@inproceedings{paranjape_prompting_2021,
    address = {Online},
    author = {Paranjape, Bhargavi  and
Michael, Julian  and
Ghazvininejad, Marjan  and
Hajishirzi, Hannaneh  and
Zettlemoyer, Luke},
    booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
    doi = {10.18653/v1/2021.findings-acl.366},
    pages = {4179--4192},
    publisher = {Association for Computational Linguistics},
    title = {Prompting Contrastive Explanations for Commonsense Reasoning Tasks},
    url = {https://aclanthology.org/2021.findings-acl.366},
    year = {2021}
}

@inproceedings{alsentzer-etal-2019-publicly,
    address = {Minneapolis, Minnesota, USA},
    author = {Alsentzer, Emily  and
Murphy, John  and
Boag, William  and
Weng, Wei-Hung  and
Jindi, Di  and
Naumann, Tristan  and
McDermott, Matthew},
    booktitle = {Proceedings of the 2nd Clinical Natural Language Processing Workshop},
    doi = {10.18653/v1/W19-1909},
    pages = {72--78},
    publisher = {Association for Computational Linguistics},
    title = {Publicly Available Clinical {BERT} Embeddings},
    url = {https://aclanthology.org/W19-1909},
    year = {2019}
}

@inproceedings{han-eisenstein-2019-unsupervised,
    address = {Hong Kong, China},
    author = {Han, Xiaochuang  and
Eisenstein, Jacob},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1433},
    pages = {4238--4248},
    publisher = {Association for Computational Linguistics},
    title = {Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling},
    url = {https://aclanthology.org/D19-1433},
    year = {2019}
}

@article{hoffmann2022training,
    author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
    journal = {ArXiv preprint},
    title = {Training Compute-Optimal Large Language Models},
    url = {https://arxiv.org/abs/2203.15556},
    volume = {abs/2203.15556},
    year = {2022}
}

@article{lee2020biobert,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    journal = {ArXiv preprint},
    title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
    url = {https://arxiv.org/abs/1901.08746},
    volume = {abs/1901.08746},
    year = {2019}
}

@inproceedings{xue-etal-2021-mt5,
    address = {Online},
    author = {Xue, Linting  and
Constant, Noah  and
Roberts, Adam  and
Kale, Mihir  and
Al-Rfou, Rami  and
Siddhant, Aditya  and
Barua, Aditya  and
Raffel, Colin},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.41},
    pages = {483--498},
    publisher = {Association for Computational Linguistics},
    title = {m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
    url = {https://aclanthology.org/2021.naacl-main.41},
    year = {2021}
}

@inproceedings{zhang_metadata-induced_2022,
    abstract = {Large-scale multi-label text classification (LMTC) aims to associate a document with its relevant labels from a large candidate set. Most existing LMTC approaches rely on massive human-annotated training data, which are often costly to obtain and suffer from a long-tailed label distribution (i.e., many labels occur only a few times in the training set). In this paper, we study LMTC under the zero-shot setting, which does not require any annotated documents with labels and only relies on label surface names and descriptions. To train a classifier that calculates the similarity score between a document and a label, we propose a novel metadata-induced contrastive learning (MICoL) method. Different from previous textbased contrastive learning techniques, MICoL exploits document metadata (e.g., authors, venues, and references of research papers), which are widely available on the Web, to derive similar document–document pairs. Experimental results on two large-scale datasets show that: (1) MICoL significantly outperforms strong zero-shot text classification and contrastive learning baselines; (2) MICoL is on par with the state-of-the-art supervised metadata-aware LMTC method trained on 10K–200K labeled documents; and (3) MICoL tends to predict more infrequent labels than supervised methods, thus alleviates the deteriorated performance on long-tailed labels.},
    address = {Virtual Event, Lyon France},
    author = {Zhang, Yu and Shen, Zhihong and Wu, Chieh-Han and Xie, Boya and Hao, Junheng and Wang, Ye-Yi and Wang, Kuansan and Han, Jiawei},
    booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
    doi = {10.1145/3485447.3512174},
    file = {Zhang et al. - 2022 - Metadata-Induced Contrastive Learning for Zero-Sho.pdf:files/460/Zhang et al. - 2022 - Metadata-Induced Contrastive Learning for Zero-Sho.pdf:application/pdf},
    isbn = {978-1-4503-9096-5},
    language = {en},
    pages = {3162--3173},
    publisher = {ACM},
    title = {Metadata-{Induced} {Contrastive} {Learning} for {Zero}-{Shot} {Multi}-{Label} {Text} {Classification}},
    url = {https://dl.acm.org/doi/10.1145/3485447.3512174},
    urldate = {2022-08-08},
    year = {2022}
}

@inproceedings{song_multi-label_nodate,
    author = {Jiaming Song and
Stefano Ermon},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/SongE20.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Multi-label Contrastive Predictive Coding},
    url = {https://proceedings.neurips.cc/paper/2020/hash/5cd5058bca53951ffa7801bcdf421651-Abstract.html},
    year = {2020}
}

@misc{malkinski_multi-label_2020,
    author = {Małkiński, Mikołaj and Mańdziuk, Jacek},
    journal = {ArXiv preprint},
    title = {Multi-{Label} {Contrastive} {Learning} for {Abstract} {Visual} {Reasoning}},
    url = {https://arxiv.org/abs/2012.01944},
    volume = {abs/2012.01944},
    year = {2020}
}

@article{zhang_use_nodate,
    abstract = {Current contrastive learning frameworks focus on leveraging a single supervisory signal to learn representations, which limits the efficacy on unseen data and downstream tasks. In this paper, we present a hierarchical multi-label representation learning framework that can leverage all available labels and preserve the hierarchical relationship between classes. We introduce novel hierarchy preserving losses, which jointly apply a hierarchical penalty to the contrastive loss, and enforce the hierarchy constraint. The loss function is data driven and automatically adapts to arbitrary multi-label structures. Experiments on several datasets show that our relationship-preserving embedding performs well on a variety of tasks and outperform the baseline supervised and self-supervised approaches. Code is available at https://github.com/salesforce/ hierarchicalContrastiveLearning.},
    author = {Zhang, Shu and Xu, Ran and Xiong, Caiming and Ramaiah, Chetan},
    file = {Zhang et al. - Use All the Labels A Hierarchical Multi-Label Con.pdf:files/463/Zhang et al. - Use All the Labels A Hierarchical Multi-Label Con.pdf:application/pdf},
    language = {en},
    pages = {10},
    title = {Use {All} the {Labels}: {A} {Hierarchical} {Multi}-{Label} {Contrastive} {Learning} {Framework}}
}

@inproceedings{su_contrastive_2022,
    address = {Dublin, Ireland},
    author = {Su, Xi{'}ao  and
Wang, Ran  and
Dai, Xinyu},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    doi = {10.18653/v1/2022.acl-short.75},
    pages = {672--679},
    publisher = {Association for Computational Linguistics},
    title = {Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification},
    url = {https://aclanthology.org/2022.acl-short.75},
    year = {2022}
}

@inproceedings{thompson_overcoming_2019,
    address = {Minneapolis, Minnesota},
    author = {Thompson, Brian  and
Gwinnup, Jeremy  and
Khayrallah, Huda  and
Duh, Kevin  and
Koehn, Philipp},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    doi = {10.18653/v1/N19-1209},
    pages = {2062--2068},
    publisher = {Association for Computational Linguistics},
    title = {Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation},
    url = {https://aclanthology.org/N19-1209},
    year = {2019}
}

@misc{dangovski_equivariant_2022,
    author = {Dangovski, Rumen and Jing, Li and Loh, Charlotte and Han, Seungwook and Srivastava, Akash and Cheung, Brian and Agrawal, Pulkit and Soljačić, Marin},
    journal = {ArXiv preprint},
    title = {Equivariant {Contrastive} {Learning}},
    url = {https://arxiv.org/abs/2111.00899},
    volume = {abs/2111.00899},
    year = {2021}
}

@inproceedings{chuang_diffcse_2022,
    address = {Seattle, United States},
    author = {Chuang, Yung-Sung  and
Dangovski, Rumen  and
Luo, Hongyin  and
Zhang, Yang  and
Chang, Shiyu  and
Soljacic, Marin  and
Li, Shang-Wen  and
Yih, Scott  and
Kim, Yoon  and
Glass, James},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2022.naacl-main.311},
    pages = {4207--4218},
    publisher = {Association for Computational Linguistics},
    title = {{D}iff{CSE}: Difference-based Contrastive Learning for Sentence Embeddings},
    url = {https://aclanthology.org/2022.naacl-main.311},
    year = {2022}
}

@inproceedings{valizadeh_ai_2022,
    address = {Dublin, Ireland},
    author = {Valizadeh, Mina  and
Parde, Natalie},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.458},
    pages = {6638--6660},
    publisher = {Association for Computational Linguistics},
    title = {The {AI} Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications},
    url = {https://aclanthology.org/2022.acl-long.458},
    year = {2022}
}

@inproceedings{yang_enhancing_2019,
    address = {Florence, Italy},
    author = {Yang, An  and
Wang, Quan  and
Liu, Jing  and
Liu, Kai  and
Lyu, Yajuan  and
Wu, Hua  and
She, Qiaoqiao  and
Li, Sujian},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1226},
    pages = {2346--2357},
    publisher = {Association for Computational Linguistics},
    title = {Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension},
    url = {https://aclanthology.org/P19-1226},
    year = {2019}
}

@misc{hinton_distilling_2015,
    author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
    journal = {ArXiv preprint},
    title = {Distilling the {Knowledge} in a {Neural} {Network}},
    url = {https://arxiv.org/abs/1503.02531},
    volume = {abs/1503.02531},
    year = {2015}
}

@article{pueschel_small_nodate,
    author = {Pueschel, Markus},
    file = {Pueschel - Small Guide to Making Nice Tables.pdf:files/474/Pueschel - Small Guide to Making Nice Tables.pdf:application/pdf},
    language = {en},
    pages = {16},
    title = {Small {Guide} to {Making} {Nice} {Tables}}
}

@inproceedings{liu_trans-encoder_2022,
    author = {Fangyu Liu and
Yunlong Jiao and
Jordan Massiah and
Emine Yilmaz and
Serhii Havrylov},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/0001JMYH22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Trans-Encoder: Unsupervised sentence-pair modelling through self-
and mutual-distillations},
    url = {https://openreview.net/forum?id=AmUhwTOHgm},
    year = {2022}
}

@inproceedings{garg_leveraging_2022,
    author = {Saurabh Garg and
Sivaraman Balakrishnan and
Zachary Chase Lipton and
Behnam Neyshabur and
Hanie Sedghi},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/GargBLNS22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Leveraging unlabeled data to predict out-of-distribution performance},
    url = {https://openreview.net/forum?id=o\_HsiMPYh\_x},
    year = {2022}
}

@inproceedings{tenney_bert_2019,
    address = {Florence, Italy},
    author = {Tenney, Ian  and
Das, Dipanjan  and
Pavlick, Ellie},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1452},
    pages = {4593--4601},
    publisher = {Association for Computational Linguistics},
    title = {{BERT} Rediscovers the Classical {NLP} Pipeline},
    url = {https://aclanthology.org/P19-1452},
    year = {2019}
}

@misc{tian_understanding_2022,
    author = {Tian, Yuandong},
    journal = {ArXiv preprint},
    title = {Understanding {Deep} {Contrastive} {Learning} via {Coordinate}-wise {Optimization}},
    url = {https://arxiv.org/abs/2201.12680},
    volume = {abs/2201.12680},
    year = {2022}
}

@article{marchi_fagundes_crosslinguistic_2021,
    abstract = {Humans use spatial language on a daily basis, to describe locations, give directions, and ask for information about places. Better understanding of spatial language can assist in developing natural language interfaces and querying tools for GIS and web mapping. However, most previous studies focus on artificial, indoor situations. We conduct cross-l­inguistic experiments to compare natural language relative location descriptions (e.g., the house beside the river) in New Zealand English (NZE) and Brazilian Portuguese (BP) using eight real outdoor locations to discover the differences that occur when people describe the same location in the two languages. Our results show that NZE uses a wider range of spatial relation terms (e.g., beside) and reference objects (e.g., river) than BP, that BP uses more projective spatial relation terms than NZE, which prefers directional terms, and that translation between spatial relation terms is context-­dependent.},
    author = {Marchi Fagundes, Cristiane Kutianski and Stock, Kristin and Delazari, Luciene Stamato},
    doi = {10.1111/tgis.12815},
    file = {Marchi Fagundes et al. - 2021 - A cross‐linguistic study of spatial location descr.pdf:files/480/Marchi Fagundes et al. - 2021 - A cross‐linguistic study of spatial location descr.pdf:application/pdf},
    issn = {1361-1682, 1467-9671},
    journal = {Transactions in GIS},
    language = {en},
    number = {6},
    pages = {3159--3187},
    title = {A cross‐linguistic study of spatial location descriptions in {New} {Zealand} {English} and {Brazilian} {Portuguese} natural language},
    url = {https://onlinelibrary.wiley.com/doi/10.1111/tgis.12815},
    urldate = {2022-08-06},
    volume = {25},
    year = {2021}
}

@misc{akyurek_tracing_2022,
    author = {Akyürek, Ekin and Bolukbasi, Tolga and Liu, Frederick and Xiong, Binbin and Tenney, Ian and Andreas, Jacob and Guu, Kelvin},
    journal = {ArXiv preprint},
    title = {Tracing {Knowledge} in {Language} {Models} {Back} to the {Training} {Data}},
    url = {https://arxiv.org/abs/2205.11482},
    volume = {abs/2205.11482},
    year = {2022}
}

@article{dusek_novel_nodate,
    abstract = {This thesis explores novel approaches to natural language generation (NLG) in spoken dialogue systems (i.e., generating system responses to be presented the user), aiming at simplifying adaptivity of NLG in three respects: domain portability, language portability, and user-adaptive outputs.},
    author = {Dušek, Ondřej},
    file = {Dušek - Novel Methods for Natural Language Generation in S.pdf:files/483/Dušek - Novel Methods for Natural Language Generation in S.pdf:application/pdf},
    language = {en},
    pages = {188},
    title = {Novel {Methods} for {Natural} {Language} {Generation} in {Spoken} {Dialogue} {Systems}}
}

@inproceedings{chen_simple_2020,
    author = {Ting Chen and
Simon Kornblith and
Mohammad Norouzi and
Geoffrey E. Hinton},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/ChenK0H20.bib},
    booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
    pages = {1597--1607},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
    title = {A Simple Framework for Contrastive Learning of Visual Representations},
    url = {http://proceedings.mlr.press/v119/chen20j.html},
    volume = {119},
    year = {2020}
}

@inproceedings{nekvinda_shades_2021,
    address = {Online},
    author = {Nekvinda, Tom{\'a}{\v{s}}  and
Du{\v{s}}ek, Ond{\v{r}}ej},
    booktitle = {Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)},
    doi = {10.18653/v1/2021.gem-1.4},
    pages = {34--46},
    publisher = {Association for Computational Linguistics},
    title = {Shades of {BLEU}, Flavours of Success: The Case of {M}ulti{WOZ}},
    url = {https://aclanthology.org/2021.gem-1.4},
    year = {2021}
}

@inproceedings{mitchell_memory-based_2022,
    author = {Eric Mitchell and
Charles Lin and
Antoine Bosselut and
Christopher D. Manning and
Chelsea Finn},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/MitchellLBMF22.bib},
    booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
    editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
    pages = {15817--15831},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
    title = {Memory-Based Model Editing at Scale},
    url = {https://proceedings.mlr.press/v162/mitchell22a.html},
    volume = {162},
    year = {2022}
}

@inproceedings{jang_towards_2022,
    author = {Joel Jang and
Seonghyeon Ye and
Sohee Yang and
Joongbo Shin and
Janghoon Han and
Gyeonghun Kim and
Stanley Jungkyu Choi and
Minjoon Seo},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/JangYYSHKCS22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Towards Continual Knowledge Learning of Language Models},
    url = {https://openreview.net/forum?id=vfsRB5MImo9},
    year = {2022}
}

@inproceedings{kulhanek_augpt_2021,
    address = {Online},
    author = {Kulh{\'a}nek, Jon{\'a}{\v{s}}  and
Hude{\v{c}}ek, Vojt{\v{e}}ch  and
Nekvinda, Tom{\'a}{\v{s}}  and
Du{\v{s}}ek, Ond{\v{r}}ej},
    booktitle = {Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI},
    doi = {10.18653/v1/2021.nlp4convai-1.19},
    pages = {198--210},
    publisher = {Association for Computational Linguistics},
    title = {{AuGPT}: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models},
    url = {https://aclanthology.org/2021.nlp4convai-1.19},
    year = {2021}
}

@inproceedings{jung_learning_2022,
    address = {Dublin, Ireland},
    author = {Jung, Yong-Ho  and
Park, Jun-Hyung  and
Choi, Joon-Young  and
Lee, Mingyu  and
Kim, Junho  and
Kim, Kang-Min  and
Lee, SangKeun},
    booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
    doi = {10.18653/v1/2022.findings-acl.119},
    pages = {1514--1523},
    publisher = {Association for Computational Linguistics},
    title = {Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference},
    url = {https://aclanthology.org/2022.findings-acl.119},
    year = {2022}
}

@inproceedings{wu_generating_2022,
    address = {Dublin, Ireland},
    author = {Wu, Yuxiang  and
Gardner, Matt  and
Stenetorp, Pontus  and
Dasigi, Pradeep},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.190},
    pages = {2660--2676},
    publisher = {Association for Computational Linguistics},
    title = {Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets},
    url = {https://aclanthology.org/2022.acl-long.190},
    year = {2022}
}

@inproceedings{ghosal_cicero_2022,
    address = {Dublin, Ireland},
    author = {Ghosal, Deepanway  and
Shen, Siqi  and
Majumder, Navonil  and
Mihalcea, Rada  and
Poria, Soujanya},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.344},
    pages = {5010--5028},
    publisher = {Association for Computational Linguistics},
    title = {{CICERO}: A Dataset for Contextualized Commonsense Inference in Dialogues},
    url = {https://aclanthology.org/2022.acl-long.344},
    year = {2022}
}

@inproceedings{okimura_impact_2022,
    address = {Dublin, Ireland},
    author = {Okimura, Itsuki  and
Reid, Machel  and
Kawano, Makoto  and
Matsuo, Yutaka},
    booktitle = {Proceedings of the Third Workshop on Insights from Negative Results in NLP},
    doi = {10.18653/v1/2022.insights-1.12},
    pages = {88--93},
    publisher = {Association for Computational Linguistics},
    title = {On the Impact of Data Augmentation on Downstream Performance in Natural Language Processing},
    url = {https://aclanthology.org/2022.insights-1.12},
    year = {2022}
}

@inproceedings{wang_learning_2016,
    address = {Berlin, Germany},
    author = {Wang, Sida I.  and
Liang, Percy  and
Manning, Christopher D.},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P16-1224},
    pages = {2368--2378},
    publisher = {Association for Computational Linguistics},
    title = {Learning Language Games through Interaction},
    url = {https://aclanthology.org/P16-1224},
    year = {2016}
}

@article{lecun_path_nodate,
    abstract = {How could machines learn as eﬃciently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as conﬁgurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
    author = {LeCun, Yann},
    file = {LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf:files/501/LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf:application/pdf},
    language = {en},
    pages = {62},
    title = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence} {Version} 0.9.2, 2022-06-27}
}

@inproceedings{gao_improving_2022,
    address = {Dublin, Ireland},
    author = {Gao, Jun  and
Wang, Wei  and
Yu, Changlong  and
Zhao, Huan  and
Ng, Wilfred  and
Xu, Ruifeng},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.216},
    pages = {3036--3049},
    publisher = {Association for Computational Linguistics},
    title = {Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering},
    url = {https://aclanthology.org/2022.acl-long.216},
    year = {2022}
}

@inproceedings{kong_mutual_2019,
    author = {Lingpeng Kong and
Cyprien de Masson d'Autume and
Lei Yu and
Wang Ling and
Zihang Dai and
Dani Yogatama},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/KongdYLDY20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {A Mutual Information Maximization Perspective of Language Representation
Learning},
    url = {https://openreview.net/forum?id=Syx79eBKwr},
    year = {2020}
}

@inproceedings{su_tacl_2022,
    address = {Seattle, United States},
    author = {Su, Yixuan  and
Liu, Fangyu  and
Meng, Zaiqiao  and
Lan, Tian  and
Shu, Lei  and
Shareghi, Ehsan  and
Collier, Nigel},
    booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
    doi = {10.18653/v1/2022.findings-naacl.191},
    pages = {2497--2507},
    publisher = {Association for Computational Linguistics},
    title = {{T}a{CL}: Improving {BERT} Pre-training with Token-aware Contrastive Learning},
    url = {https://aclanthology.org/2022.findings-naacl.191},
    year = {2022}
}

@inproceedings{santhanam_colbertv2_2022,
    address = {Seattle, United States},
    author = {Santhanam, Keshav  and
Khattab, Omar  and
Saad-Falcon, Jon  and
Potts, Christopher  and
Zaharia, Matei},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2022.naacl-main.272},
    pages = {3715--3734},
    publisher = {Association for Computational Linguistics},
    title = {{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction},
    url = {https://aclanthology.org/2022.naacl-main.272},
    year = {2022}
}

@article{li_language_2022,
    abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
    author = {Li, Hang},
    doi = {10.1145/3490443},
    file = {Li - 2022 - Language models past, present, and future.pdf:files/511/Li - 2022 - Language models past, present, and future.pdf:application/pdf},
    issn = {0001-0782, 1557-7317},
    journal = {Communications of the ACM},
    language = {en},
    number = {7},
    pages = {56--63},
    shorttitle = {Language models},
    title = {Language models: past, present, and future},
    url = {https://dl.acm.org/doi/10.1145/3490443},
    urldate = {2022-08-06},
    volume = {65},
    year = {2022}
}

@inproceedings{lewis_bart_2020,
    address = {Online},
    author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.703},
    pages = {7871--7880},
    publisher = {Association for Computational Linguistics},
    title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    url = {https://aclanthology.org/2020.acl-main.703},
    year = {2020}
}

@article{liu2022deep,
    author = {Liu, Xiaofeng and Yoo, Chaehwa and Xing, Fangxu and Oh, Hyejin and El Fakhri, Georges and Kang, Je-Won and Woo, Jonghye and others},
    journal = {ArXiv preprint},
    title = {Deep unsupervised domain adaptation: a review of recent advances and perspectives},
    url = {https://arxiv.org/abs/2208.07422},
    volume = {abs/2208.07422},
    year = {2022}
}

@article{Mehri2020DialoGLUEAN,
    author = {Shikib Mehri and Mihail Eric and Dilek Z. Hakkani-T{\"u}r},
    journal = {ArXiv preprint},
    title = {DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue},
    url = {https://arxiv.org/abs/2009.13570},
    volume = {abs/2009.13570},
    year = {2020}
}

@inproceedings{bengio2009curriculum,
    author = {Yoshua Bengio and
J{\'{e}}r{\^{o}}me Louradour and
Ronan Collobert and
Jason Weston},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/BengioLCW09.bib},
    booktitle = {Proceedings of the 26th Annual International Conference on Machine
Learning, {ICML} 2009, Montreal, Quebec, Canada, June 14-18, 2009},
    doi = {10.1145/1553374.1553380},
    editor = {Andrea Pohoreckyj Danyluk and
L{\'{e}}on Bottou and
Michael L. Littman},
    pages = {41--48},
    publisher = {{ACM}},
    series = {{ACM} International Conference Proceeding Series},
    timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
    title = {Curriculum learning},
    url = {https://doi.org/10.1145/1553374.1553380},
    volume = {382},
    year = {2009}
}

@inproceedings{wang-etal-2018-glue,
    author = {Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/WangSMHLB19.bib},
    booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
    publisher = {OpenReview.net},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding},
    url = {https://openreview.net/forum?id=rJ4km2R5t7},
    year = {2019}
}

@inproceedings{zhang2022differentiable,
    author = {Ningyu Zhang and
Luoqiu Li and
Xiang Chen and
Shumin Deng and
Zhen Bi and
Chuanqi Tan and
Fei Huang and
Huajun Chen},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/ZhangLCDBTHC22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Mon, 13 Mar 2023 00:00:00 +0100},
    title = {Differentiable Prompt Makes Pre-trained Language Models Better Few-shot
Learners},
    url = {https://openreview.net/forum?id=ek9a0qIafW},
    year = {2022}
}

@inproceedings{lewis_retrieval-augmented_2020,
    author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
    url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
    year = {2020}
}

@inproceedings{DBLP:conf/iclr/LaineA17,
    author = {Samuli Laine and
Timo Aila},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/LaineA17.bib},
    booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
    publisher = {OpenReview.net},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Temporal Ensembling for Semi-Supervised Learning},
    url = {https://openreview.net/forum?id=BJ6oOfqge},
    year = {2017}
}

@inproceedings{tarvainen2017mean,
    author = {Antti Tarvainen and
Harri Valpola},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/TarvainenV17.bib},
    booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
    editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
    pages = {1195--1204},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results},
    url = {https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
    year = {2017}
}

@inproceedings{park_relational_2019,
    author = {Wonpyo Park and
Dongju Kim and
Yan Lu and
Minsu Cho},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cvpr/ParkKLC19.bib},
    booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
2019, Long Beach, CA, USA, June 16-20, 2019},
    doi = {10.1109/CVPR.2019.00409},
    pages = {3967--3976},
    publisher = {Computer Vision Foundation / {IEEE}},
    timestamp = {Mon, 20 Jan 2020 00:00:00 +0100},
    title = {Relational Knowledge Distillation},
    url = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Park\_Relational\_Knowledge\_Distillation\_CVPR\_2019\_paper.html},
    year = {2019}
}

@inproceedings{sun2016deep,
    author = {Sun, Baochen and Saenko, Kate},
    booktitle = {European conference on computer vision},
    organization = {Springer},
    pages = {443--450},
    title = {Deep coral: Correlation alignment for deep domain adaptation},
    url = {https://link.springer.com/chapter/10.1007/978-3-319-49409-8_35},
    year = {2016}
}

@article{pan2010survey,
    author = {Pan, Sinno Jialin and Yang, Qiang},
    journal = {IEEE Transactions on knowledge and data engineering},
    number = {10},
    pages = {1345--1359},
    publisher = {IEEE},
    title = {A survey on transfer learning},
    url = {https://ieeexplore.ieee.org/document/5288526},
    volume = {22},
    year = {2010}
}

@inproceedings{raina2007self,
    author = {Rajat Raina and
Alexis Battle and
Honglak Lee and
Benjamin Packer and
Andrew Y. Ng},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/RainaBLPN07.bib},
    booktitle = {Machine Learning, Proceedings of the Twenty-Fourth International Conference
{(ICML} 2007), Corvallis, Oregon, USA, June 20-24, 2007},
    doi = {10.1145/1273496.1273592},
    editor = {Zoubin Ghahramani},
    pages = {759--766},
    publisher = {{ACM}},
    series = {{ACM} International Conference Proceeding Series},
    timestamp = {Tue, 06 Nov 2018 00:00:00 +0100},
    title = {Self-taught learning: transfer learning from unlabeled data},
    url = {https://doi.org/10.1145/1273496.1273592},
    volume = {227},
    year = {2007}
}

@inproceedings{intriguing2013,
    author = {Christian Szegedy and
Wojciech Zaremba and
Ilya Sutskever and
Joan Bruna and
Dumitru Erhan and
Ian J. Goodfellow and
Rob Fergus},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
    booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
    editor = {Yoshua Bengio and
Yann LeCun},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Intriguing properties of neural networks},
    url = {http://arxiv.org/abs/1312.6199},
    year = {2014}
}

@inproceedings{jayanthi2021sj_aj,
    address = {Kyiv},
    author = {Jayanthi, Sai Muralidhar  and
Gupta, Akshat},
    booktitle = {Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages},
    pages = {307--312},
    publisher = {Association for Computational Linguistics},
    title = {{SJ}{\_}{AJ}@{D}ravidian{L}ang{T}ech-{EACL}2021: Task-Adaptive Pre-Training of Multilingual {BERT} models for Offensive Language Identification},
    url = {https://aclanthology.org/2021.dravidianlangtech-1.44},
    year = {2021}
}

@inproceedings{saito2018maximum,
    author = {Kuniaki Saito and
Kohei Watanabe and
Yoshitaka Ushiku and
Tatsuya Harada},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cvpr/SaitoWUH18.bib},
    booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
    doi = {10.1109/CVPR.2018.00392},
    pages = {3723--3732},
    publisher = {{IEEE} Computer Society},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Maximum Classifier Discrepancy for Unsupervised Domain Adaptation},
    url = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Saito\_Maximum\_Classifier\_Discrepancy\_CVPR\_2018\_paper.html},
    year = {2018}
}

@article{ben2010theory,
    author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
    journal = {Machine learning},
    number = {1},
    pages = {151--175},
    publisher = {Springer},
    title = {A theory of learning from different domains},
    url = {https://link.springer.com/article/10.1007/s10994-009-5152-4},
    volume = {79},
    year = {2010}
}

@misc{conwell_testing_2022,
    author = {Conwell, Colin and Ullman, Tomer},
    journal = {ArXiv preprint},
    title = {Testing {Relational} {Understanding} in {Text}-{Guided} {Image} {Generation}},
    url = {https://arxiv.org/abs/2208.00005},
    volume = {abs/2208.00005},
    year = {2022}
}

@inproceedings{wettig2022should,
    address = {Dubrovnik, Croatia},
    author = {Wettig, Alexander  and
Gao, Tianyu  and
Zhong, Zexuan  and
Chen, Danqi},
    booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
    pages = {2985--3000},
    publisher = {Association for Computational Linguistics},
    title = {Should You Mask 15{\%} in Masked Language Modeling?},
    url = {https://aclanthology.org/2023.eacl-main.217},
    year = {2023}
}

@misc{li_diffusion-lm_2022,
    author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
    journal = {ArXiv preprint},
    title = {Diffusion-{LM} {Improves} {Controllable} {Text} {Generation}},
    url = {https://arxiv.org/abs/2205.14217},
    volume = {abs/2205.14217},
    year = {2022}
}

@inproceedings{meng_coco-lm_nodate,
    author = {Yu Meng and
Chenyan Xiong and
Payal Bajaj and
Saurabh Tiwary and
Paul Bennett and
Jiawei Han and
Xia Song},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/MengXBTBHS21.bib},
    booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
    editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
    pages = {23102--23114},
    timestamp = {Tue, 03 May 2022 01:00:00 +0200},
    title = {{COCO-LM:} Correcting and Contrasting Text Sequences for Language
Model Pretraining},
    url = {https://proceedings.neurips.cc/paper/2021/hash/c2c2a04512b35d13102459f8784f1a2d-Abstract.html},
    year = {2021}
}

@inproceedings{li_sentence_2020,
    address = {Online},
    author = {Li, Bohan  and
Zhou, Hao  and
He, Junxian  and
Wang, Mingxuan  and
Yang, Yiming  and
Li, Lei},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.733},
    pages = {9119--9130},
    publisher = {Association for Computational Linguistics},
    title = {On the Sentence Embeddings from Pre-trained Language Models},
    url = {https://aclanthology.org/2020.emnlp-main.733},
    year = {2020}
}

@misc{wu_clear_2020,
    author = {Wu, Zhuofeng and Wang, Sinong and Gu, Jiatao and Khabsa, Madian and Sun, Fei and Ma, Hao},
    journal = {ArXiv preprint},
    title = {{CLEAR}: {Contrastive} {Learning} for {Sentence} {Representation}},
    url = {https://arxiv.org/abs/2012.15466},
    volume = {abs/2012.15466},
    year = {2020}
}

@inproceedings{kim_self-guided_2021,
    address = {Online},
    author = {Kim, Taeuk  and
Yoo, Kang Min  and
Lee, Sang-goo},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.197},
    pages = {2528--2540},
    publisher = {Association for Computational Linguistics},
    title = {Self-Guided Contrastive Learning for {BERT} Sentence Representations},
    url = {https://aclanthology.org/2021.acl-long.197},
    year = {2021}
}

@inproceedings{gao_simcse_2022,
    address = {Online and Punta Cana, Dominican Republic},
    author = {Gao, Tianyu  and
Yao, Xingcheng  and
Chen, Danqi},
    booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/2021.emnlp-main.552},
    pages = {6894--6910},
    publisher = {Association for Computational Linguistics},
    title = {{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings},
    url = {https://aclanthology.org/2021.emnlp-main.552},
    year = {2021}
}

@inproceedings{arora_theoretical_nodate,
    author = {Nikunj Saunshi and
Orestis Plevrakis and
Sanjeev Arora and
Mikhail Khodak and
Hrishikesh Khandeparkar},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/SaunshiPAKK19.bib},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning,
{ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
    editor = {Kamalika Chaudhuri and
Ruslan Salakhutdinov},
    pages = {5628--5637},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 11 Jun 2019 01:00:00 +0200},
    title = {A Theoretical Analysis of Contrastive Unsupervised Representation
Learning},
    url = {http://proceedings.mlr.press/v97/saunshi19a.html},
    volume = {97},
    year = {2019}
}

@inproceedings{giorgi_declutr_2021,
    address = {Online},
    author = {Giorgi, John  and
Nitski, Osvald  and
Wang, Bo  and
Bader, Gary},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.72},
    pages = {879--895},
    publisher = {Association for Computational Linguistics},
    title = {{D}e{CLUTR}: Deep Contrastive Learning for Unsupervised Textual Representations},
    url = {https://aclanthology.org/2021.acl-long.72},
    year = {2021}
}

@inproceedings{wang_cline_2021,
    address = {Online},
    author = {Wang, Dong  and
Ding, Ning  and
Li, Piji  and
Zheng, Haitao},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.181},
    pages = {2332--2342},
    publisher = {Association for Computational Linguistics},
    title = {{CLINE}: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding},
    url = {https://aclanthology.org/2021.acl-long.181},
    year = {2021}
}

@inproceedings{yang_xmoco_2021,
    address = {Online},
    author = {Yang, Nan  and
Wei, Furu  and
Jiao, Binxing  and
Jiang, Daxing  and
Yang, Linjun},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.477},
    pages = {6120--6129},
    publisher = {Association for Computational Linguistics},
    title = {x{M}o{C}o: Cross Momentum Contrastive Learning for Open-Domain Question Answering},
    url = {https://aclanthology.org/2021.acl-long.477},
    year = {2021}
}

@inproceedings{iter_pretraining_2020,
    address = {Online},
    author = {Iter, Dan  and
Guu, Kelvin  and
Lansing, Larry  and
Jurafsky, Dan},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.439},
    pages = {4859--4870},
    publisher = {Association for Computational Linguistics},
    title = {Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models},
    url = {https://aclanthology.org/2020.acl-main.439},
    year = {2020}
}

@misc{fang_cert_2020,
    author = {Fang, Hongchao and Wang, Sicheng and Zhou, Meng and Ding, Jiayuan and Xie, Pengtao},
    journal = {ArXiv preprint},
    title = {{CERT}: {Contrastive} {Self}-supervised {Learning} for {Language} {Understanding}},
    url = {https://arxiv.org/abs/2005.12766},
    volume = {abs/2005.12766},
    year = {2020}
}

@article{jaiswal_survey_2020,
    abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-deﬁned pseudolabels as supervision and use the learned representations for several downstream tasks. Speciﬁcally, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classiﬁcation, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
    author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
    doi = {10.3390/technologies9010002},
    file = {Jaiswal et al. - 2020 - A Survey on Contrastive Self-Supervised Learning.pdf:files/352/Jaiswal et al. - 2020 - A Survey on Contrastive Self-Supervised Learning.pdf:application/pdf},
    issn = {2227-7080},
    journal = {Technologies},
    language = {en},
    number = {1},
    pages = {2},
    title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
    url = {https://www.mdpi.com/2227-7080/9/1/2},
    urldate = {2022-08-01},
    volume = {9},
    year = {2020}
}

@inproceedings{zou_multi-level_2022,
    author = {Zou, Ding and Wei, Wei and Mao, Xian-Ling and Wang, Ziyang and Qiu, Minghui and Zhu, Feida and Cao, Xin},
    journal = {ArXiv preprint},
    title = {Multi-level {Cross}-view {Contrastive} {Learning} for {Knowledge}-aware {Recommender} {System}},
    url = {https://arxiv.org/abs/2204.08807},
    volume = {abs/2204.08807},
    year = {2022}
}

@inproceedings{khosla_supervised_2021,
    author = {Prannay Khosla and
Piotr Teterwak and
Chen Wang and
Aaron Sarna and
Yonglong Tian and
Phillip Isola and
Aaron Maschinot and
Ce Liu and
Dilip Krishnan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/KhoslaTWSTIMLK20.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Supervised Contrastive Learning},
    url = {https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html},
    year = {2020}
}

@inproceedings{chen_perfectly_2022,
    author = {Mayee F. Chen and
Daniel Y. Fu and
Avanika Narayan and
Michael Zhang and
Zhao Song and
Kayvon Fatahalian and
Christopher R{\'{e}}},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/ChenFNZ0FR22.bib},
    booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
    editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
    pages = {3090--3122},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
    title = {Perfectly Balanced: Improving Transfer and Robustness of Supervised
Contrastive Learning},
    url = {https://proceedings.mlr.press/v162/chen22d.html},
    volume = {162},
    year = {2022}
}

@misc{zhang_self-supervised_2022,
    author = {Zhang, Xiang and Zhao, Ziyuan and Tsiligkaridis, Theodoros and Zitnik, Marinka},
    journal = {ArXiv preprint},
    title = {Self-{Supervised} {Contrastive} {Pre}-{Training} {For} {Time} {Series} via {Time}-{Frequency} {Consistency}},
    url = {https://arxiv.org/abs/2206.08496},
    volume = {abs/2206.08496},
    year = {2022}
}

@inproceedings{zhong_training_2022,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Zhong, Zexuan  and
Lei, Tao  and
Chen, Danqi},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {5657--5673},
    publisher = {Association for Computational Linguistics},
    title = {Training Language Models with Memory Augmentation},
    url = {https://aclanthology.org/2022.emnlp-main.382},
    year = {2022}
}

@inproceedings{karpukhin_dense_2020,
    address = {Online},
    author = {Karpukhin, Vladimir  and
Oguz, Barlas  and
Min, Sewon  and
Lewis, Patrick  and
Wu, Ledell  and
Edunov, Sergey  and
Chen, Danqi  and
Yih, Wen-tau},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.550},
    pages = {6769--6781},
    publisher = {Association for Computational Linguistics},
    title = {Dense Passage Retrieval for Open-Domain Question Answering},
    url = {https://aclanthology.org/2020.emnlp-main.550},
    year = {2020}
}

@inproceedings{brank_annotating_2017,
    abstract = {We describe an efficient approach for annotating a document with relevant concepts from the Wikipedia. A pagerank-based method is used to identify a coherent set of relevant concepts considering the input document as a whole. The proposed approach is suitable for parallel processing and can support any language for which a sufficiently large Wikipedia is available.},
    author = {Brank, Janez and Leban, Gregor and Grobelnik, Marko},
    booktitle = {Proc. of {Slovenian} {KDD} {Conf}. on {Data} {Mining} and {Data} {Warehouses} ({SiKDD}),},
    file = {Brank et al. - ANNOTATING DOCUMENTS WITH RELEVANT WIKIPEDIA CONCE.pdf:files/173/Brank et al. - ANNOTATING DOCUMENTS WITH RELEVANT WIKIPEDIA CONCE.pdf:application/pdf},
    language = {en},
    pages = {4},
    publisher = {Proc. of Slovenian KDD Conf. on Data Mining and Data Warehouses (SiKDD),},
    title = {{ANNOTATING} {DOCUMENTS} {WITH} {RELEVANT} {WIKIPEDIA} {CONCEPTS}},
    year = {2017}
}

@misc{lynch_interactive_2022,
    author = {Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and Ding, Tianli and Betker, James and Baruch, Robert and Armstrong, Travis and Florence, Pete},
    journal = {ArXiv preprint},
    title = {Interactive {Language}: {Talking} to {Robots} in {Real} {Time}},
    url = {https://arxiv.org/abs/2210.06407},
    volume = {abs/2210.06407},
    year = {2022}
}

@inproceedings{ahmed_towards_nodate,
    abstract = {Extracting useful information from the user history to clearly understand informational needs is a crucial feature of a proactive information retrieval system. Regarding understanding information and relevance, Wikipedia can provide the background knowledge that an intelligent system needs. This work explores how exploiting the context of a query using Wikipedia concepts can improve proactive information retrieval on noisy text. We formulate two models that use entity linking to associate Wikipedia topics with the relevance model. Our experiments around a podcast segment retrieval task demonstrate that there is a clear signal of relevance in Wikipedia concepts while a ranking model can improve precision by incorporating them. We also find Wikifying the background context of a query can help disambiguate the meaning of the query, further helping proactive information retrieval.},
    author = {Ahmed, Tabish and Bulathwela, Sahan},
    file = {Ahmed and Bulathwela - Towards Proactive Information Retrieval in Noisy T.pdf:files/180/Ahmed and Bulathwela - Towards Proactive Information Retrieval in Noisy T.pdf:application/pdf},
    language = {en},
    pages = {12},
    title = {Towards {Proactive} {Information} {Retrieval} in {Noisy} {Text} with {Wikipedia} {Concepts}}
}

@inproceedings{chen_convfinqa_2022,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Chen, Zhiyu  and
Li, Shiyang  and
Smiley, Charese  and
Ma, Zhiqiang  and
Shah, Sameena  and
Wang, William Yang},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {6279--6292},
    publisher = {Association for Computational Linguistics},
    title = {{C}onv{F}in{QA}: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
    url = {https://aclanthology.org/2022.emnlp-main.421},
    year = {2022}
}

@misc{luo_understanding_2022,
    author = {Luo, Calvin},
    journal = {ArXiv preprint},
    title = {Understanding {Diffusion} {Models}: {A} {Unified} {Perspective}},
    url = {https://arxiv.org/abs/2208.11970},
    volume = {abs/2208.11970},
    year = {2022}
}

@misc{drozdov_compositional_2022,
    author = {Drozdov, Andrew and Schärli, Nathanael and Akyuürek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
    journal = {ArXiv preprint},
    title = {Compositional {Semantic} {Parsing} with {Large} {Language} {Models}},
    url = {https://arxiv.org/abs/2209.15003},
    volume = {abs/2209.15003},
    year = {2022}
}

@inproceedings{ruhling_cachay_end--end_2021,
    author = {Salva R{\"{u}}hling Cachay and
Benedikt Boecking and
Artur Dubrawski},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/CachayBD21.bib},
    booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
    editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
    pages = {1845--1857},
    timestamp = {Tue, 03 May 2022 01:00:00 +0200},
    title = {End-to-End Weak Supervision},
    url = {https://proceedings.neurips.cc/paper/2021/hash/0e674a918ebca3f78bfe02e2f387689d-Abstract.html},
    year = {2021}
}

@inproceedings{karamanolakis_self-training_2021,
    address = {Online},
    author = {Karamanolakis, Giannis  and
Mukherjee, Subhabrata  and
Zheng, Guoqing  and
Awadallah, Ahmed Hassan},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.66},
    pages = {845--863},
    publisher = {Association for Computational Linguistics},
    title = {Self-Training with Weak Supervision},
    url = {https://aclanthology.org/2021.naacl-main.66},
    year = {2021}
}

@inproceedings{zhang_wrench_nodate,
    abstract = {Recent Weak Supervision (WS) approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a signiﬁcant "hidden" source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classiﬁcation and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efﬁcacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.},
    author = {Zhang, Jieyu and Yu, Yue and Li, Yinghao and Wang, Yujing and Yang, Yaming and Yang, Mao and Ratner, Alexander},
    booktitle = {({NeurIPS} 2021 {Track} on {Datasets} and {Benchmarks}},
    file = {Zhang et al. - WRENCH A Comprehensive Benchmark for Weak Supervi.pdf:files/190/Zhang et al. - WRENCH A Comprehensive Benchmark for Weak Supervi.pdf:application/pdf},
    language = {en},
    pages = {16},
    publisher = {NIPS 2021},
    title = {{WRENCH}: {A} {Comprehensive} {Benchmark} for {Weak} {Supervision}}
}

@inproceedings{zhou_s3-rec_2020,
    author = {Kun Zhou and
Hui Wang and
Wayne Xin Zhao and
Yutao Zhu and
Sirui Wang and
Fuzheng Zhang and
Zhongyuan Wang and
Ji{-}Rong Wen},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cikm/ZhouWZZWZWW20.bib},
    booktitle = {{CIKM} '20: The 29th {ACM} International Conference on Information
and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020},
    doi = {10.1145/3340531.3411954},
    editor = {Mathieu d'Aquin and
Stefan Dietze and
Claudia Hauff and
Edward Curry and
Philippe Cudr{\'{e}}{-}Mauroux},
    pages = {1893--1902},
    publisher = {{ACM}},
    timestamp = {Mon, 19 Oct 2020 01:00:00 +0200},
    title = {S3-Rec: Self-Supervised Learning for Sequential Recommendation with
Mutual Information Maximization},
    url = {https://doi.org/10.1145/3340531.3411954},
    year = {2020}
}

@inproceedings{ludewig_performance_2019,
    author = {Malte Ludewig and
Noemi Mauro and
Sara Latifi and
Dietmar Jannach},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/recsys/LudewigMLJ19.bib},
    booktitle = {Proceedings of the 13th {ACM} Conference on Recommender Systems, RecSys
2019, Copenhagen, Denmark, September 16-20, 2019},
    doi = {10.1145/3298689.3347041},
    editor = {Toine Bogers and
Alan Said and
Peter Brusilovsky and
Domonkos Tikk},
    pages = {462--466},
    publisher = {{ACM}},
    timestamp = {Tue, 17 Sep 2019 01:00:00 +0200},
    title = {Performance comparison of neural and non-neural approaches to session-based
recommendation},
    url = {https://doi.org/10.1145/3298689.3347041},
    year = {2019}
}

@inproceedings{jain2024neftune,
    author = {Neel Jain and Ping-yeh Chiang and Yuxin Wen and John Kirchenbauer and Hong-Min Chu and Gowthami Somepalli and Brian R. Bartoldson and Bhavya Kailkhura and Avi Schwarzschild and Aniruddha Saha and Micah Goldblum and Jonas Geiping and Tom Goldstein},
    booktitle = {The Twelfth International Conference on Learning Representations},
    title = {{NEFT}une: Noisy Embeddings Improve Instruction Finetuning},
    url = {https://openreview.net/forum?id=0bMmZ3fkCk},
    year = {2024}
}

@article{zhang2022opt,
    author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
    journal = {ArXiv preprint},
    title = {Opt: Open pre-trained transformer language models},
    url = {https://arxiv.org/abs/2205.01068},
    volume = {abs/2205.01068},
    year = {2022}
}

@article{liu-etal-2023-compositional,
    abstract = {Label scarcity is a bottleneck for improving task performance in specialized domains. We propose a novel compositional transfer learning framework (DoT51) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from masked language modelling of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: We simultaneously train natural language generation (NLG) for in-domain label-to-data generation, which enables data augmentation for self-finetuning and natural language understanding (NLU) for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on natural language inference, text summarization, and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current state-of-the-art in zero-shot transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with ablations and a case study demonstrating its ability to solve challenging NLI examples requiring in-domain expertise.},
    address = {Cambridge, MA},
    author = {Liu, Fangyu  and
Liu, Qianchu  and
Bannur, Shruthi  and
P{\'e}rez-Garc{\'\i}a, Fernando  and
Usuyama, Naoto  and
Zhang, Sheng  and
Naumann, Tristan  and
Nori, Aditya  and
Poon, Hoifung  and
Alvarez-Valle, Javier  and
Oktay, Ozan  and
Hyland, Stephanie L.},
    doi = {10.1162/tacl_a_00585},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {1097--1113},
    publisher = {MIT Press},
    title = {Compositional Zero-Shot Domain Transfer with Text-to-Text Models},
    url = {https://aclanthology.org/2023.tacl-1.62},
    volume = {11},
    year = {2023}
}

@inproceedings{xia_self-supervised_2021,
    abstract = {Session-based recommendation targets next-item prediction by exploiting user behaviors within a short time period. Compared with other recommendation paradigms, session-based recommendation suffers more from the problem of data sparsity due to the very limited short-term interactions. Self-supervised learning, which can discover ground-truth samples from the raw data, holds vast potentials to tackle this problem. However, existing self-supervised recommendation models mainly rely on item/segment dropout to augment data, which are not fit for session-based recommendation because the dropout leads to sparser data, creating unserviceable self-supervision signals. In this paper, for informative sessionbased data augmentation, we combine self-supervised learning with co-training, and then develop a framework to enhance sessionbased recommendation. Technically, we first exploit the sessionbased graph to augment two views that exhibit the internal and external connectivities of sessions, and then we build two distinct graph encoders over the two views, which recursively leverage the different connectivity information to generate ground-truth samples to supervise each other by contrastive learning. In contrast to the dropout strategy, the proposed self-supervised graph co-training preserves the complete session information and fulfills genuine data augmentation. Extensive experiments on multiple benchmark datasets show that, session-based recommendation can be remarkably enhanced under the regime of self-supervised graph co-training, achieving the state-of-the-art performance.},
    address = {Virtual Event Queensland Australia},
    author = {Xia, Xin and Yin, Hongzhi and Yu, Junliang and Shao, Yingxia and Cui, Lizhen},
    booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
    doi = {10.1145/3459637.3482388},
    file = {Xia et al. - 2021 - Self-Supervised Graph Co-Training for Session-base.pdf:files/196/Xia et al. - 2021 - Self-Supervised Graph Co-Training for Session-base.pdf:application/pdf},
    isbn = {978-1-4503-8446-9},
    language = {en},
    pages = {2180--2190},
    publisher = {ACM},
    title = {Self-{Supervised} {Graph} {Co}-{Training} for {Session}-based {Recommendation}},
    url = {https://dl.acm.org/doi/10.1145/3459637.3482388},
    urldate = {2022-09-28},
    year = {2021}
}

@inproceedings{10.5555/2283696.2283708,
    author = {Shoushan Li and
Zhongqing Wang and
Guodong Zhou and
Sophia Yat Mei Lee},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ijcai/LiWZL11.bib},
    booktitle = {{IJCAI} 2011, Proceedings of the 22nd International Joint Conference
on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22,
2011},
    doi = {10.5591/978-1-57735-516-8/IJCAI11-306},
    editor = {Toby Walsh},
    pages = {1826--1831},
    publisher = {{IJCAI/AAAI}},
    timestamp = {Sun, 04 Jun 2017 01:00:00 +0200},
    title = {Semi-Supervised Learning for Imbalanced Sentiment Classification},
    url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-306},
    year = {2011}
}

@inproceedings{shalaby_m2trec_2022,
    abstract = {Session-based recommender systems (SBRSs) have shown superior performance over conventional methods. However, they show limited scalability on large-scale industrial datasets since most models learn one embedding per item. This leads to a large memory requirement (of storing one vector per item) and poor performance on sparse sessions with cold-start or unpopular items. Using one public and one large industrial dataset, we experimentally show that state-of-the-art SBRSs have low performance on sparse sessions with sparse items. We propose M2TRec, a Metadata-aware Multitask Transformer model for session-based recommendations. Our proposed method learns a transformation function from item metadata to embeddings, and is thus, item-ID free (i.e., does not need to learn one embedding per item). It integrates item metadata to learn shared representations of diverse item attributes. During inference, new or unpopular items will be assigned identical representations for the attributes they share with items previously observed during training, and thus will have similar representations with those items, enabling recommendations of even cold-start and sparse items. Additionally, M2TRec is trained in a multi-task setting to predict the next item in the session along with its primary category and subcategories. Our multi-task strategy makes the model converge faster and significantly improves the overall performance. Experimental results show significant performance gains using our proposed approach on sparse items on the two datasets.},
    address = {Seattle WA USA},
    author = {Shalaby, Walid and Oh, Sejoon and Afsharinejad, Amir and Kumar, Srijan and Cui, Xiquan},
    booktitle = {Sixteenth {ACM} {Conference} on {Recommender} {Systems}},
    doi = {10.1145/3523227.3551477},
    file = {Shalaby et al. - 2022 - M2TRec Metadata-aware Multi-task Transformer for .pdf:files/261/Shalaby et al. - 2022 - M2TRec Metadata-aware Multi-task Transformer for .pdf:application/pdf},
    isbn = {978-1-4503-9278-5},
    language = {en},
    pages = {573--578},
    publisher = {ACM},
    shorttitle = {{M2TRec}},
    title = {{M2TRec}: {Metadata}-aware {Multi}-task {Transformer} for {Large}-scale and {Cold}-start free {Session}-based {Recommendations}},
    url = {https://dl.acm.org/doi/10.1145/3523227.3551477},
    urldate = {2022-09-27},
    year = {2022}
}

@inproceedings{michael_asking_2020,
    address = {Online},
    author = {Michael, Julian  and
Botha, Jan A.  and
Tenney, Ian},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.552},
    pages = {6792--6812},
    publisher = {Association for Computational Linguistics},
    title = {Asking without Telling: Exploring Latent Ontologies in Contextual Representations},
    url = {https://aclanthology.org/2020.emnlp-main.552},
    year = {2020}
}

@inproceedings{mikolov_distributed_2013,
    author = {Tom{\'{a}}s Mikolov and
Ilya Sutskever and
Kai Chen and
Gregory S. Corrado and
Jeffrey Dean},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
    booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Systems 2013. Proceedings
of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
    editor = {Christopher J. C. Burges and
L{\'{e}}on Bottou and
Zoubin Ghahramani and
Kilian Q. Weinberger},
    pages = {3111--3119},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Distributed Representations of Words and Phrases and their Compositionality},
    url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
    year = {2013}
}

@inproceedings{conneau_what_2018,
    address = {Melbourne, Australia},
    author = {Conneau, Alexis  and
Kruszewski, German  and
Lample, Guillaume  and
Barrault, Lo{\"\i}c  and
Baroni, Marco},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P18-1198},
    pages = {2126--2136},
    publisher = {Association for Computational Linguistics},
    title = {What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties},
    url = {https://aclanthology.org/P18-1198},
    year = {2018}
}

@inproceedings{yang_assessing_2019,
    address = {Florence, Italy},
    author = {Yang, Baosong  and
Wang, Longyue  and
Wong, Derek F.  and
Chao, Lidia S.  and
Tu, Zhaopeng},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1354},
    pages = {3635--3644},
    publisher = {Association for Computational Linguistics},
    title = {Assessing the Ability of Self-Attention Networks to Learn Word Order},
    url = {https://aclanthology.org/P19-1354},
    year = {2019}
}

@inproceedings{reif_visualizing_2019,
    author = {Emily Reif and
Ann Yuan and
Martin Wattenberg and
Fernanda B. Vi{\'{e}}gas and
Andy Coenen and
Adam Pearce and
Been Kim},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/ReifYWVCPK19.bib},
    booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
    editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
    pages = {8592--8600},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Visualizing and Measuring the Geometry of {BERT}},
    url = {https://proceedings.neurips.cc/paper/2019/hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html},
    year = {2019}
}

@inproceedings{lin_does_2022,
    address = {Dublin, Ireland},
    author = {Lin, Ruixi  and
Ng, Hwee Tou},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    doi = {10.18653/v1/2022.acl-short.11},
    pages = {94--99},
    publisher = {Association for Computational Linguistics},
    title = {Does {BERT} Know that the {IS}-A Relation Is Transitive?},
    url = {https://aclanthology.org/2022.acl-short.11},
    year = {2022}
}

@inproceedings{ethayarajh_how_2019,
    address = {Hong Kong, China},
    author = {Ethayarajh, Kawin},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1006},
    pages = {55--65},
    publisher = {Association for Computational Linguistics},
    title = {How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings},
    url = {https://aclanthology.org/D19-1006},
    year = {2019}
}

@inproceedings{jawahar_what_2019,
    address = {Florence, Italy},
    author = {Jawahar, Ganesh  and
Sagot, Beno{\^\i}t  and
Seddah, Djam{\'e}},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1356},
    pages = {3651--3657},
    publisher = {Association for Computational Linguistics},
    title = {What Does {BERT} Learn about the Structure of Language?},
    url = {https://aclanthology.org/P19-1356},
    year = {2019}
}

@inproceedings{hewitt_structural_2019,
    address = {Minneapolis, Minnesota},
    author = {Hewitt, John  and
Manning, Christopher D.},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    doi = {10.18653/v1/N19-1419},
    pages = {4129--4138},
    publisher = {Association for Computational Linguistics},
    title = {{A} Structural Probe for Finding Syntax in Word Representations},
    url = {https://aclanthology.org/N19-1419},
    year = {2019}
}

@inproceedings{cirik_visual_2018,
    address = {New Orleans, Louisiana},
    author = {Cirik, Volkan  and
Morency, Louis-Philippe  and
Berg-Kirkpatrick, Taylor},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    doi = {10.18653/v1/N18-2123},
    pages = {781--787},
    publisher = {Association for Computational Linguistics},
    title = {Visual Referring Expression Recognition: What Do Systems Actually Learn?},
    url = {https://aclanthology.org/N18-2123},
    year = {2018}
}

@inproceedings{su_contrastive_2022-1,
    author = {Su, Yixuan and Lan, Tian and Wang, Yan and Yogatama, Dani and Kong, Lingpeng and Collier, Nigel},
    journal = {ArXiv preprint},
    title = {A {Contrastive} {Framework} for {Neural} {Text} {Generation}},
    url = {https://arxiv.org/abs/2202.06417},
    volume = {abs/2202.06417},
    year = {2022}
}

@inproceedings{montazeralghaem_reinforcement_2020,
    author = {Ali Montazeralghaem and
Hamed Zamani and
James Allan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/sigir/Montazeralghaem20.bib},
    booktitle = {Proceedings of the 43rd International {ACM} {SIGIR} conference on
research and development in Information Retrieval, {SIGIR} 2020, Virtual
Event, China, July 25-30, 2020},
    doi = {10.1145/3397271.3401099},
    editor = {Jimmy Huang and
Yi Chang and
Xueqi Cheng and
Jaap Kamps and
Vanessa Murdock and
Ji{-}Rong Wen and
Yiqun Liu},
    pages = {59--68},
    publisher = {{ACM}},
    timestamp = {Mon, 27 Jul 2020 01:00:00 +0200},
    title = {A Reinforcement Learning Framework for Relevance Feedback},
    url = {https://doi.org/10.1145/3397271.3401099},
    year = {2020}
}

@article{izacard_unsupervised_2022,
    author = {Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
    journal = {ArXiv preprint},
    title = {Unsupervised {Dense} {Information} {Retrieval} with {Contrastive} {Learning}},
    url = {https://arxiv.org/abs/2112.09118},
    volume = {abs/2112.09118},
    year = {2021}
}

@inproceedings{aliannejadi_asking_2019,
    author = {Mohammad Aliannejadi and
Hamed Zamani and
Fabio Crestani and
W. Bruce Croft},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/sigir/AliannejadiZCC19.bib},
    booktitle = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on
Research and Development in Information Retrieval, {SIGIR} 2019, Paris,
France, July 21-25, 2019},
    doi = {10.1145/3331184.3331265},
    editor = {Benjamin Piwowarski and
Max Chevalier and
{\'{E}}ric Gaussier and
Yoelle Maarek and
Jian{-}Yun Nie and
Falk Scholer},
    pages = {475--484},
    publisher = {{ACM}},
    timestamp = {Sun, 21 Jul 2019 01:00:00 +0200},
    title = {Asking Clarifying Questions in Open-Domain Information-Seeking Conversations},
    url = {https://doi.org/10.1145/3331184.3331265},
    year = {2019}
}

@inproceedings{izacard_distilling_2021,
    author = {Gautier Izacard and
Edouard Grave},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/IzacardG21.bib},
    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
    publisher = {OpenReview.net},
    timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
    title = {Distilling Knowledge from Reader to Retriever for Question Answering},
    url = {https://openreview.net/forum?id=NTEz-6wysdb},
    year = {2021}
}

@inproceedings{sachan_end--end_2021,
    address = {Online},
    author = {Sachan, Devendra  and
Patwary, Mostofa  and
Shoeybi, Mohammad  and
Kant, Neel  and
Ping, Wei  and
Hamilton, William L.  and
Catanzaro, Bryan},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.519},
    pages = {6648--6662},
    publisher = {Association for Computational Linguistics},
    title = {End-to-End Training of Neural Retrievers for Open-Domain Question Answering},
    url = {https://aclanthology.org/2021.acl-long.519},
    year = {2021}
}

@inproceedings{sachan_end--end_2021-1,
    author = {Devendra Singh Sachan and
Siva Reddy and
William L. Hamilton and
Chris Dyer and
Dani Yogatama},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/SachanRHDY21.bib},
    booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
    editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
    pages = {25968--25981},
    timestamp = {Tue, 03 May 2022 01:00:00 +0200},
    title = {End-to-End Training of Multi-Document Reader and Retriever for Open-Domain
Question Answering},
    url = {https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html},
    year = {2021}
}

@inproceedings{clark_electra_2020,
    author = {Kevin Clark and
Minh{-}Thang Luong and
Quoc V. Le and
Christopher D. Manning},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/ClarkLLM20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
Generators},
    url = {https://openreview.net/forum?id=r1xMH1BtvB},
    year = {2020}
}

@inproceedings{liu_generated_2022,
    address = {Dublin, Ireland},
    author = {Liu, Jiacheng  and
Liu, Alisa  and
Lu, Ximing  and
Welleck, Sean  and
West, Peter  and
Le Bras, Ronan  and
Choi, Yejin  and
Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/2022.acl-long.225},
    pages = {3154--3169},
    publisher = {Association for Computational Linguistics},
    title = {Generated Knowledge Prompting for Commonsense Reasoning},
    url = {https://aclanthology.org/2022.acl-long.225},
    year = {2022}
}

@inproceedings{anonymous_247_file_paper_nodate,
    author = {Anonymous},
    file = {247_file_Paper.pdf:files/240/247_file_Paper.pdf:application/pdf},
    publisher = {EMNLP22 Reivew Paper},
    title = {247\_file\_Paper: {Exploring} {Dense} {Retrieval} for {Dialogue} {Response} {Selection}}
}

@inproceedings{anonymous_811_file_paper_nodate,
    author = {Anonymous},
    file = {811_file_Paper.pdf:files/239/811_file_Paper.pdf:application/pdf},
    publisher = {EMNLP22 Reivew Paper},
    title = {811\_file\_Paper: {Exploring} {Dense} {Retrieval} for {Dialogue} {Response} {Selection}}
}

@inproceedings{anonymous_3468_file_paper_nodate,
    author = {Anonymous},
    file = {3468_file_Paper.pdf:files/238/3468_file_Paper.pdf:application/pdf},
    publisher = {EMNLP22 Reivew Paper},
    title = {3468\_file\_Paper: {Know} {Thy} {Strengths}: {Comprehensive} {Dialogue} {State} {Tracking} {Diagnostics}}
}

@inproceedings{anonymous_id_nodate,
    author = {Anonymous},
    file = {Submission 2931.pdf:files/237/Submission 2931.pdf:application/pdf},
    publisher = {AAAI23 Review Paper},
    title = {{ID} 2931: {Generic} {Dependency} {Modeling} for {Multi}-{Party} {Conversation} {LATEX}}
}

@inproceedings{anonymous_id_nodate-1,
    author = {Anonymous},
    file = {Submission 7066.pdf:files/236/Submission 7066.pdf:application/pdf},
    publisher = {AAAI23 Review Paper},
    title = {{ID} 7066: {CAB}: {Empathetic} {Dialogue} {Generation} with {Cognition}, {Affection} and {Behavior}}
}

@inproceedings{anonymous_id_nodate-2,
    author = {Anonymous},
    file = {Submission 8222.pdf:files/235/Submission 8222.pdf:application/pdf},
    publisher = {AAAI23 Review Paper},
    title = {{ID} 8222: {Bridging} the {Generalization} {Gap}: {Towards} {Benchmarking} {Robust} {Spoken} {Language} {Understanding}}
}

@article{luan_sparse_2021,
    address = {Cambridge, MA},
    author = {Luan, Yi  and
Eisenstein, Jacob  and
Toutanova, Kristina  and
Collins, Michael},
    doi = {10.1162/tacl_a_00369},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {329--345},
    publisher = {MIT Press},
    title = {Sparse, Dense, and Attentional Representations for Text Retrieval},
    url = {https://aclanthology.org/2021.tacl-1.20},
    volume = {9},
    year = {2021}
}

@inproceedings{sorscher_beyond_2022,
    author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
    journal = {ArXiv preprint},
    title = {Beyond neural scaling laws: beating power law scaling via data pruning},
    url = {https://arxiv.org/abs/2206.14486},
    volume = {abs/2206.14486},
    year = {2022}
}

@inproceedings{izacard_few-shot_2022,
    author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
    journal = {ArXiv preprint},
    title = {Few-shot {Learning} with {Retrieval} {Augmented} {Language} {Models}},
    url = {https://arxiv.org/abs/2208.03299},
    volume = {abs/2208.03299},
    year = {2022}
}

@inproceedings{li_graph_2022,
    author = {Li, Juanhui and Ma, Yao and Zeng, Wei and Cheng, Suqi and Tang, Jiliang and Wang, Shuaiqiang and Yin, Dawei},
    journal = {ArXiv preprint},
    title = {Graph {Enhanced} {BERT} for {Query} {Understanding}},
    url = {https://arxiv.org/abs/2204.06522},
    volume = {abs/2204.06522},
    year = {2022}
}

@inproceedings{xiong_approximate_2021,
    author = {Lee Xiong and
Chenyan Xiong and
Ye Li and
Kwok{-}Fung Tang and
Jialin Liu and
Paul N. Bennett and
Junaid Ahmed and
Arnold Overwijk},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/XiongXLTLBAO21.bib},
    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
    publisher = {OpenReview.net},
    timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
    title = {Approximate Nearest Neighbor Negative Contrastive Learning for Dense
Text Retrieval},
    url = {https://openreview.net/forum?id=zeFrfgyZln},
    year = {2021}
}

@inproceedings{yu_improving_2021,
    abstract = {Dense retrieval systems conduct first-stage retrieval using embedded representations and simple similarity metrics to match a query to documents. Its effectiveness depends on encoded embeddings to capture the semantics of queries and documents, a challenging task due to the shortness and ambiguity of search queries. This paper proposes ANCE-PRF, a new query encoder that uses pseudo relevance feedback (PRF) to improve query representations for dense retrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top retrieved documents from a dense retrieval model, ANCE, and it learns to produce better query embeddings directly from relevance labels. It also keeps the document index unchanged to reduce overhead. ANCE-PRF significantly outperforms ANCE and other recent dense retrieval systems on several datasets. Analysis shows that the PRF encoder effectively captures the relevant and complementary information from PRF documents, while ignoring the noise with its learned attention mechanism.},
    author = {Yu, HongChien},
    booktitle = {{CIKM}},
    file = {Yu - 2021 - Improving Query Representations for Dense Retrieva.pdf:files/386/Yu - 2021 - Improving Query Representations for Dense Retrieva.pdf:application/pdf},
    language = {en},
    pages = {5},
    publisher = {ACM CIKM},
    title = {Improving {Query} {Representations} for {Dense} {Retrieval} with {Pseudo} {Relevance} {Feedback}},
    year = {2021}
}

@inproceedings{pradeep_neural_2022,
    abstract = {In this work, we propose an effective multi-stage neural ranking system for the clinical trial matching problem. First, we introduce NQS, a neural query synthesis method that leverages a zero-shot document expansion model to generate multiple sentence-long queries from lengthy patient descriptions. These queries are independently issued to a search engine and the results are fused. We find that on the TREC 2021 Clinical Trials Track, this method outperforms strong traditional baselines like BM25 and BM25 + RM3 by about 12 points in nDCG@10, a relative improvement of 34\%. This simple method is so effective that even a state-of-the-art neural relevance ranking method trained on the medical subset of MS MARCO passage, when reranking the results of NQS, fails to improve on the ranked list. Second, we introduce a two-stage neural reranking pipeline trained on clinical trial matching data using tailored ranking templates. In this setting, we can train a pointwise reranker using just 1.1k positive examples and obtain effectiveness improvements over NQS by 24 points. This end-to-end multi-stage system demonstrates a 20\% relative effectiveness gain compared to the second-best submission at TREC 2021, making it an important step towards better automated clinical trial matching.},
    author = {Pradeep, Ronak},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    file = {Pradeep - 2022 - Neural Query Synthesis and Domain-Specific Ranking.pdf:files/571/Pradeep - 2022 - Neural Query Synthesis and Domain-Specific Ranking.pdf:application/pdf},
    language = {en},
    pages = {6},
    publisher = {ACM SIGIR},
    title = {Neural {Query} {Synthesis} and {Domain}-{Specific} {Ranking} {Templates} for {Multi}-{Stage} {Clinical} {Trial} {Matching}},
    year = {2022}
}

@inproceedings{treviso_efficient_2022,
    author = {Treviso, Marcos and Ji, Tianchu and Lee, Ji-Ung and van Aken, Betty and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Martins, Pedro H. and Martins, André F. T. and Milder, Peter and Raffel, Colin and Simpson, Edwin and Slonim, Noam and Balasubramanian, Niranjan and Derczynski, Leon and Schwartz, Roy},
    journal = {ArXiv preprint},
    title = {Efficient {Methods} for {Natural} {Language} {Processing}: {A} {Survey}},
    url = {https://arxiv.org/abs/2209.00099},
    volume = {abs/2209.00099},
    year = {2022}
}

@inproceedings{wang_multi-level_2022,
    author = {Wang, Ziyang and Liu, Huoyu and Wei, Wei and Hu, Yue and Mao, Xian-Ling and He, Shaojian and Fang, Rui and chen, Dangyang},
    journal = {ArXiv preprint},
    title = {Multi-level {Contrastive} {Learning} {Framework} for {Sequential} {Recommendation}},
    url = {https://arxiv.org/abs/2208.13007},
    volume = {abs/2208.13007},
    year = {2022}
}

@inproceedings{zhang_uni-retriever_2022,
    address = {Washington DC USA},
    author = {Zhang, Jianjin and Liu, Zheng and Han, Weihao and Xiao, Shitao and Zheng, Ruicheng and Shao, Yingxia and Sun, Hao and Zhu, Hanqing and Srinivasan, Premkumar and Deng, Weiwei and Zhang, Qi and Xie, Xing},
    booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
    doi = {10.1145/3534678.3539212},
    file = {Zhang et al. - 2022 - Uni-Retriever Towards Learning the Unified Embedd.pdf:files/220/Zhang et al. - 2022 - Uni-Retriever Towards Learning the Unified Embedd.pdf:application/pdf},
    isbn = {978-1-4503-9385-0},
    language = {en},
    pages = {4493--4501},
    publisher = {ACM},
    shorttitle = {Uni-{Retriever}},
    title = {Uni-{Retriever}: {Towards} {Learning} the {Unified} {Embedding} {Based} {Retriever} in {Bing} {Sponsored} {Search}},
    url = {https://dl.acm.org/doi/10.1145/3534678.3539212},
    urldate = {2022-09-06},
    year = {2022}
}

@misc{ou_clarifying_2020,
    author = {Ou, Wenjie and Lin, Yue},
    journal = {ArXiv preprint},
    title = {A {Clarifying} {Question} {Selection} {System} from {NTES}\_ALONG in {Convai3} {Challenge}},
    url = {https://arxiv.org/abs/2010.14202},
    volume = {abs/2010.14202},
    year = {2020}
}

@inproceedings{zhang_mie_2020,
    address = {Online},
    author = {Zhang, Yuanzhe  and
Jiang, Zhongtao  and
Zhang, Tao  and
Liu, Shiwan  and
Cao, Jiarun  and
Liu, Kang  and
Liu, Shengping  and
Zhao, Jun},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.576},
    pages = {6460--6469},
    publisher = {Association for Computational Linguistics},
    title = {{MIE}: A Medical Information Extractor towards Medical Dialogues},
    url = {https://aclanthology.org/2020.acl-main.576},
    year = {2020}
}

@inproceedings{yan_remedi_2022,
    abstract = {Medical dialogue systems (MDSs) aim to assist doctors and patients with a range of professional medical services, i.e., diagnosis, treatment and consultation. The development of MDSs is hindered because of a lack of resources. In particular. (1) there is no dataset with large-scale medical dialogues that covers multiple medical services and contains fine-grained medical labels (i.e., intents, actions, slots, values), and (2) there is no set of established benchmarks for MDSs for multi-domain, multi-service medical dialogues.},
    address = {Madrid Spain},
    author = {Yan, Guojun and Pei, Jiahuan and Ren, Pengjie and Ren, Zhaochun and Xin, Xin and Liang, Huasheng and de Rijke, Maarten and Chen, Zhumin},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531809},
    file = {Yan et al. - 2022 - ReMeDi Resources for Multi-domain, Multi-service,.pdf:files/232/Yan et al. - 2022 - ReMeDi Resources for Multi-domain, Multi-service,.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {3013--3024},
    publisher = {ACM},
    shorttitle = {{ReMeDi}},
    title = {{ReMeDi}: {Resources} for {Multi}-domain, {Multi}-service, {Medical} {Dialogues}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531809},
    urldate = {2022-08-31},
    year = {2022}
}

@inproceedings{liu_self-alignment_2021,
    address = {Online},
    author = {Liu, Fangyu  and
Shareghi, Ehsan  and
Meng, Zaiqiao  and
Basaldella, Marco  and
Collier, Nigel},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.334},
    pages = {4228--4238},
    publisher = {Association for Computational Linguistics},
    title = {Self-Alignment Pretraining for Biomedical Entity Representations},
    url = {https://aclanthology.org/2021.naacl-main.334},
    year = {2021}
}

@inproceedings{beltagy-etal-2019-scibert,
    address = {Hong Kong, China},
    author = {Beltagy, Iz  and
Lo, Kyle  and
Cohan, Arman},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1371},
    pages = {3615--3620},
    publisher = {Association for Computational Linguistics},
    title = {{S}ci{BERT}: A Pretrained Language Model for Scientific Text},
    url = {https://aclanthology.org/D19-1371},
    year = {2019}
}

@inproceedings{basaldella_cometa_2020,
    address = {Online},
    author = {Basaldella, Marco  and
Liu, Fangyu  and
Shareghi, Ehsan  and
Collier, Nigel},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    doi = {10.18653/v1/2020.emnlp-main.253},
    pages = {3122--3137},
    publisher = {Association for Computational Linguistics},
    title = {{COMETA}: A Corpus for Medical Entity Linking in the Social Media},
    url = {https://aclanthology.org/2020.emnlp-main.253},
    year = {2020}
}

@article{marcus_very_nodate,
    abstract = {The DALL-E 2 system generates original synthetic images corresponding to an input text as caption. We report here on the outcome of fourteen tests of this system designed to assess its common sense, reasoning and ability to understand complex texts. All of our prompts were intentionally much more challenging than the typical ones that have been showcased in recent weeks. Nevertheless, for 5 out of the 14 prompts, at least one of the ten images fully satisfied our requests. On the other hand, on no prompt did all of the ten images satisfy our requests.},
    author = {Marcus, Gary and Davis, Ernest and Aaronson, Scott},
    file = {Marcus et al. - A very preliminary analysis of DALL-E 2.pdf:files/444/Marcus et al. - A very preliminary analysis of DALL-E 2.pdf:application/pdf},
    language = {en},
    pages = {14},
    title = {A very preliminary analysis of {DALL}-{E} 2}
}

@inproceedings{gu_incorporating_2016,
    address = {Berlin, Germany},
    author = {Gu, Jiatao  and
Lu, Zhengdong  and
Li, Hang  and
Li, Victor O.K.},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P16-1154},
    pages = {1631--1640},
    publisher = {Association for Computational Linguistics},
    title = {Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
    url = {https://aclanthology.org/P16-1154},
    year = {2016}
}

@inproceedings{zhu_lol_2022,
    address = {Madrid Spain},
    author = {Zhu, Yunchang and Pang, Liang and Lan, Yanyan and Shen, Huawei and Cheng, Xueqi},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3532017},
    file = {Zhu et al. - 2022 - LoL A Comparative Regularization Loss over Query .pdf:files/569/Zhu et al. - 2022 - LoL A Comparative Regularization Loss over Query .pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {825--836},
    publisher = {ACM},
    shorttitle = {{LoL}},
    title = {{LoL}: {A} {Comparative} {Regularization} {Loss} over {Query} {Reformulation} {Losses} for {Pseudo}-{Relevance} {Feedback}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3532017},
    urldate = {2022-08-29},
    year = {2022}
}

@inproceedings{erbacher_interactive_2022,
    abstract = {When users initiate search sessions, their query are often ambiguous or might lack of context; this resulting in non-efficient document ranking. Multiple approaches have been proposed by the Information Retrieval community to add context and retrieve documents aligned with users’ intents. While some work focus on query disambiguation using users’ browsing history, a recent line of work proposes to interact with users by asking clarification questions or/and proposing clarification panels. However, these approaches count either a limited number (i.e., 1) of interactions with user or log-based interactions. In this paper, we propose and evaluate a fully simulated query clarification framework allowing multi-turn interactions between IR systems and user agents.},
    address = {Madrid Spain},
    author = {Erbacher, Pierre and Denoyer, Ludovic and Soulier, Laure},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531871},
    file = {Erbacher et al. - 2022 - Interactive Query Clarification and Refinement via.pdf:files/570/Erbacher et al. - 2022 - Interactive Query Clarification and Refinement via.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {2420--2425},
    publisher = {ACM},
    title = {Interactive {Query} {Clarification} and {Refinement} via {User} {Simulation}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531871},
    urldate = {2022-08-29},
    year = {2022}
}

@inproceedings{krasakis_zero-shot_2022,
    abstract = {Current conversational passage retrieval systems cast conversational search into ad-hoc search by using an intermediate query resolution step that places the user’s question in context of the conversation. While the proposed methods have proven effective, they still assume the availability of large-scale question resolution and conversational search datasets. To waive the dependency on the availability of such data, we adapt a pre-trained token-level dense retriever on ad-hoc search data to perform conversational search with no additional fine-tuning. The proposed method allows to contextualize the user question within the conversation history, but restrict the matching only between question and potential answer. Our experiments demonstrate the effectiveness of the proposed approach. We also perform an analysis that provides insights of how contextualization works in the latent space, in essence introducing a bias towards salient terms from the conversation.},
    address = {Madrid Spain},
    author = {Krasakis, Antonios Minas and Yates, Andrew and Kanoulas, Evangelos},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531769},
    file = {Krasakis et al. - 2022 - Zero-shot Query Contextualization for Conversation.pdf:files/572/Krasakis et al. - 2022 - Zero-shot Query Contextualization for Conversation.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {1880--1884},
    publisher = {ACM},
    title = {Zero-shot {Query} {Contextualization} for {Conversational} {Search}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531769},
    urldate = {2022-08-29},
    year = {2022}
}

@inproceedings{khattab_colbert_2020,
    author = {Omar Khattab and
Matei Zaharia},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/sigir/KhattabZ20.bib},
    booktitle = {Proceedings of the 43rd International {ACM} {SIGIR} conference on
research and development in Information Retrieval, {SIGIR} 2020, Virtual
Event, China, July 25-30, 2020},
    doi = {10.1145/3397271.3401075},
    editor = {Jimmy Huang and
Yi Chang and
Xueqi Cheng and
Jaap Kamps and
Vanessa Murdock and
Ji{-}Rong Wen and
Yiqun Liu},
    pages = {39--48},
    publisher = {{ACM}},
    timestamp = {Mon, 27 Jul 2020 01:00:00 +0200},
    title = {ColBERT: Efficient and Effective Passage Search via Contextualized
Late Interaction over {BERT}},
    url = {https://doi.org/10.1145/3397271.3401075},
    year = {2020}
}

@inproceedings{gao_coil_2021,
    address = {Online},
    author = {Gao, Luyu  and
Dai, Zhuyun  and
Callan, Jamie},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.241},
    pages = {3030--3042},
    publisher = {Association for Computational Linguistics},
    title = {{COIL}: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List},
    url = {https://aclanthology.org/2021.naacl-main.241},
    year = {2021}
}

@inproceedings{mallia_learning_2021,
    abstract = {Neural information retrieval systems typically use a cascading pipeline, in which a first-stage model retrieves a candidate set of documents and one or more subsequent stages re-rank this set using contextualized language models such as BERT. In this paper, we propose DeepImpact, a new document term-weighting scheme suitable for efficient retrieval using a standard inverted index. Compared to existing methods, DeepImpact improves impact-score modeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact leverages DocT5Query to enrich the document collection and, using a contextualized language model, directly estimates the semantic importance of tokens in a document, producing a single-value representation for each token in each document. Our experiments show that DeepImpact significantly outperforms prior first-stage retrieval approaches by up to 17\% on effectiveness metrics w.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the same effectiveness of state-of-the-art approaches with up to 5.1× speedup in efficiency.},
    address = {Virtual Event Canada},
    author = {Mallia, Antonio and Khattab, Omar and Suel, Torsten and Tonellotto, Nicola},
    booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3404835.3463030},
    file = {Mallia et al. - 2021 - Learning Passage Impacts for Inverted Indexes.pdf:files/578/Mallia et al. - 2021 - Learning Passage Impacts for Inverted Indexes.pdf:application/pdf},
    isbn = {978-1-4503-8037-9},
    language = {en},
    pages = {1723--1727},
    publisher = {ACM},
    title = {Learning {Passage} {Impacts} for {Inverted} {Indexes}},
    url = {https://dl.acm.org/doi/10.1145/3404835.3463030},
    urldate = {2022-08-28},
    year = {2021}
}

@misc{lin_few_2021,
    author = {Lin, Jimmy and Ma, Xueguang},
    journal = {ArXiv preprint},
    title = {A {Few} {Brief} {Notes} on {DeepImpact}, {COIL}, and a {Conceptual} {Framework} for {Information} {Retrieval} {Techniques}},
    url = {https://arxiv.org/abs/2106.14807},
    volume = {abs/2106.14807},
    year = {2021}
}

@article{beydokhti_qualitative_2022,
    abstract = {Although geospatial question answering systems have received increasing attention in recent years, existing prototype systems struggle to properly answer qualitative spatial questions. In this work, we propose a unique framework for answering qualitative spatial questions, which comprises three main components: a geoparser that takes the input questions and extracts place semantic information from text, a reasoning system which is embedded with a crisp reasoner, and finally, answer extraction, which refines the solution space and generates final answers. We present an experimental design to evaluate our framework for point-based cardinal direction calculus (CDC) relations by developing an automated approach for generating three types of synthetic qualitative spatial questions. The initial evaluations of generated answers in our system are promising because a high proportion of answers were labelled correct.},
    author = {Beydokhti, Mohammad Kazemi and Duckham, Matt and Tao, Yaguang and Vasardani, Maria and Griffin, Amy},
    file = {Beydokhti et al. - 2022 - Qualitative Spatial Reasoning over Questions.pdf:files/368/Beydokhti et al. - 2022 - Qualitative Spatial Reasoning over Questions.pdf:application/pdf},
    language = {en},
    pages = {7},
    title = {Qualitative {Spatial} {Reasoning} over {Questions}},
    year = {2022}
}

@inproceedings{li_nprf_2018,
    address = {Brussels, Belgium},
    author = {Li, Canjia  and
Sun, Yingfei  and
He, Ben  and
Wang, Le  and
Hui, Kai  and
Yates, Andrew  and
Sun, Le  and
Xu, Jungang},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D18-1478},
    pages = {4482--4491},
    publisher = {Association for Computational Linguistics},
    title = {{NPRF}: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval},
    url = {https://aclanthology.org/D18-1478},
    year = {2018}
}

@misc{li_improving_2021,
    author = {Li, Hang and Zhuang, Shengyao and Mourad, Ahmed and Ma, Xueguang and Lin, Jimmy and Zuccon, Guido},
    journal = {ArXiv preprint},
    title = {Improving {Query} {Representations} for {Dense} {Retrieval} with {Pseudo} {Relevance} {Feedback}: {A} {Reproducibility} {Study}},
    url = {https://arxiv.org/abs/2112.06400},
    volume = {abs/2112.06400},
    year = {2021}
}

@misc{ou_clarifying_2020-1,
    author = {Ou, Wenjie and Lin, Yue},
    journal = {ArXiv preprint},
    title = {A {Clarifying} {Question} {Selection} {System} from {NTES}\_ALONG in {Convai3} {Challenge}},
    url = {https://arxiv.org/abs/2010.14202},
    volume = {abs/2010.14202},
    year = {2020}
}

@inproceedings{formal_splade_2021,
    abstract = {In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-ofthe-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization.},
    address = {Virtual Event Canada},
    author = {Formal, Thibault and Piwowarski, Benjamin and Clinchant, Stéphane},
    booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3404835.3463098},
    file = {Formal et al. - 2021 - SPLADE Sparse Lexical and Expansion Model for Fir.pdf:files/385/Formal et al. - 2021 - SPLADE Sparse Lexical and Expansion Model for Fir.pdf:application/pdf},
    isbn = {978-1-4503-8037-9},
    language = {en},
    pages = {2288--2292},
    publisher = {ACM},
    shorttitle = {{SPLADE}},
    title = {{SPLADE}: {Sparse} {Lexical} and {Expansion} {Model} for {First} {Stage} {Ranking}},
    url = {https://dl.acm.org/doi/10.1145/3404835.3463098},
    urldate = {2022-08-22},
    year = {2021}
}

@inproceedings{li_interpolate_2022,
    abstract = {Current pre-trained language model approaches to information retrieval can be broadly divided into two categories: sparse retrievers (to which belong also non-neural approaches such as bag-of-words methods, e.g., BM25) and dense retrievers. Each of these categories appears to capture di erent characteristics of relevance. Previous work has investigated how relevance signals from sparse retrievers could be combined with those from dense retrievers via interpolation. Such interpolation would generally lead to higher retrieval e ectiveness. In this paper we consider the problem of combining the relevance signals from sparse and dense retrievers in the context of Pseudo Relevance Feedback (PRF). This context poses two key challenges: (1) When should interpolation occur: before, after, or both before and after the PRF process? (2) Which sparse representation should be considered: a zero-shot bag-of-words model (BM25), or a learned sparse representation? To answer these questions we perform a thorough empirical evaluation considering an e ective and scalable neural PRF approach (Vector-PRF), three e ective dense retrievers (ANCE, TCTv2, DistillBERT), and one state-of-the-art learned sparse retriever (uniCOIL). The empirical ndings from our experiments suggest that, regardless of sparse representation and dense retriever, interpolation both before and after PRF achieves the highest e ectiveness across most datasets and metrics.},
    address = {Madrid Spain},
    author = {Li, Hang and Wang, Shuai and Zhuang, Shengyao and Mourad, Ahmed and Ma, Xueguang and Lin, Jimmy and Zuccon, Guido},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531884},
    file = {Li et al. - 2022 - To Interpolate or not to Interpolate PRF, Dense a.pdf:files/390/Li et al. - 2022 - To Interpolate or not to Interpolate PRF, Dense a.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {2495--2500},
    publisher = {ACM},
    shorttitle = {To {Interpolate} or not to {Interpolate}},
    title = {To {Interpolate} or not to {Interpolate}: {PRF}, {Dense} and {Sparse} {Retrievers}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531884},
    urldate = {2022-08-21},
    year = {2022}
}

@inproceedings{reimers_sentence-bert_2019,
    address = {Hong Kong, China},
    author = {Reimers, Nils  and
Gurevych, Iryna},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1410},
    pages = {3982--3992},
    publisher = {Association for Computational Linguistics},
    title = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
    url = {https://aclanthology.org/D19-1410},
    year = {2019}
}

@inproceedings{elgohary_can_2019,
    address = {Hong Kong, China},
    author = {Elgohary, Ahmed  and
Peskov, Denis  and
Boyd-Graber, Jordan},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    doi = {10.18653/v1/D19-1605},
    pages = {5918--5924},
    publisher = {Association for Computational Linguistics},
    title = {Can You Unpack That? Learning to Rewrite Questions-in-Context},
    url = {https://aclanthology.org/D19-1605},
    year = {2019}
}

@inproceedings{wan_fast_2022,
    address = {Hybrid: Seattle, Washington + Online},
    author = {Wan, Hui  and
Patel, Siva Sankalp  and
Murdock, J William  and
Potdar, Saloni  and
Joshi, Sachindra},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
    doi = {10.18653/v1/2022.naacl-industry.37},
    pages = {334--343},
    publisher = {Association for Computational Linguistics},
    title = {Fast and Light-Weight Answer Text Retrieval in Dialogue Systems},
    url = {https://aclanthology.org/2022.naacl-industry.37},
    year = {2022}
}

@inproceedings{yu_few-shot_2021,
    abstract = {Dense retrieval (DR) has the potential to resolve the query understanding challenge in conversational search by matching in the learned embedding space. However, this adaptation is challenging due to DR models’ extra needs for supervision signals and the longtail nature of conversational search. In this paper, we present a Conversational Dense Retrieval system, ConvDR, that learns contextualized embeddings for multi-turn conversational queries and retrieves documents solely using embedding dot products. In addition, we grant ConvDR few-shot ability using a teacher-student framework, where we employ an ad hoc dense retriever as the teacher, inherit its document encodings, and learn a student query encoder to mimic the teacher embeddings on oracle reformulated queries. Our experiments on TREC CAsT and OR-QuAC demonstrate ConvDR’s effectiveness in both few-shot and fully-supervised settings. It outperforms previous systems that operate in the sparse word space, matches the retrieval accuracy of oracle query reformulations, and is also more efficient thanks to its simplicity. Our analyses reveal that the advantages of ConvDR come from its ability to capture informative context while ignoring the unrelated context in previous conversation rounds. This makes ConvDR more effective as conversations evolve while previous systems may get confused by the increased noise from previous turns. Our code is publicly available at https://github.com/thunlp/ConvDR.},
    address = {Virtual Event Canada},
    author = {Yu, Shi and Liu, Zhenghao and Xiong, Chenyan and Feng, Tao and Liu, Zhiyuan},
    booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3404835.3462856},
    file = {Yu et al. - 2021 - Few-Shot Conversational Dense Retrieval.pdf:files/327/Yu et al. - 2021 - Few-Shot Conversational Dense Retrieval.pdf:application/pdf},
    isbn = {978-1-4503-8037-9},
    language = {en},
    pages = {829--838},
    publisher = {ACM},
    title = {Few-{Shot} {Conversational} {Dense} {Retrieval}},
    url = {https://dl.acm.org/doi/10.1145/3404835.3462856},
    urldate = {2022-08-16},
    year = {2021}
}

@inproceedings{hu_learning_2022,
    author = {Hu, Weihua and Bansal, Rajas and Cao, Kaidi and Rao, Nikhil and Subbian, Karthik and Leskovec, Jure},
    journal = {ArXiv preprint},
    title = {Learning {Backward} {Compatible} {Embeddings}},
    url = {https://arxiv.org/abs/2206.03040},
    volume = {abs/2206.03040},
    year = {2022}
}

@inproceedings{vakulenko_question_2021,
    abstract = {Conversational question answering (QA) requires the ability to correctly interpret a question in the context of previous conversation turns. We address the conversational QA task by decomposing it into question rewriting and question answering subtasks. The question rewriting (QR) subtask is specifically designed to reformulate ambiguous questions, which depend on the conversational context, into unambiguous questions that can be correctly interpreted outside of the conversational context. We introduce a conversational QA architecture that sets the new state of the art on the TREC CAsT 2019 passage retrieval dataset. Moreover, we show that the same QR model improves QA performance on the QuAC dataset with respect to answer span extraction, which is the next step in QA after passage retrieval. Our evaluation results indicate that the QR model we proposed achieves near human-level performance on both datasets and the gap in performance on the end-to-end conversational QA task is attributed mostly to the errors in QA.},
    address = {Virtual Event Israel},
    author = {Vakulenko, Svitlana and Longpre, Shayne and Tu, Zhucheng and Anantha, Raviteja},
    booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
    doi = {10.1145/3437963.3441748},
    file = {Vakulenko et al. - 2021 - Question Rewriting for Conversational Question Ans.pdf:files/331/Vakulenko et al. - 2021 - Question Rewriting for Conversational Question Ans.pdf:application/pdf},
    isbn = {978-1-4503-8297-7},
    language = {en},
    pages = {355--363},
    publisher = {ACM WSDM},
    title = {Question {Rewriting} for {Conversational} {Question} {Answering}},
    url = {https://dl.acm.org/doi/10.1145/3437963.3441748},
    urldate = {2022-08-15},
    year = {2021}
}

@inproceedings{mao_curriculum_2022,
    abstract = {Conversational search is a crucial and promising branch in information retrieval. In this paper, we reveal that not all historical conversational turns are necessary for understanding the intent of the current query. The redundant noisy turns in the context largely hinder the improvement of search performance. However, enhancing the context denoising ability for conversational search is quite challenging due to data scarcity and the steep difficulty for simultaneously learning conversational query encoding and context denoising. To address these issues, in this paper, we present a novel Curriculum cOntrastive conTExt Denoising framework, COTED, towards few-shot conversational dense retrieval. Under a curriculum training order, we progressively endow the model with the capability of context denoising via contrastive learning between noised samples and denoised samples generated by a new conversation data augmentation strategy. Three curriculums tailored to conversational search are exploited in our framework. Extensive experiments on two few-shot conversational search datasets, i.e., CAsT-19 and CAsT-20, validate the effectiveness and superiority of our method compared with the state-of-the-art baselines.},
    address = {Madrid Spain},
    author = {Mao, Kelong and Dou, Zhicheng and Qian, Hongjin},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531961},
    file = {Mao et al. - 2022 - Curriculum Contrastive Context Denoising for Few-s.pdf:files/332/Mao et al. - 2022 - Curriculum Contrastive Context Denoising for Few-s.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {176--186},
    publisher = {ACM},
    title = {Curriculum {Contrastive} {Context} {Denoising} for {Few}-shot {Conversational} {Dense} {Retrieval}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531961},
    urldate = {2022-08-15},
    year = {2022}
}

@inproceedings{zhang_analyzing_2022,
    abstract = {User simulation has been a cost-effective technique for evaluating conversational recommender systems. However, building a humanlike simulator is still an open challenge. In this work, we focus on how users reformulate their utterances when a conversational agent fails to understand them. First, we perform a user study, involving five conversational agents across different domains, to identify common reformulation types and their transition relationships. A common pattern that emerges is that persistent users would first try to rephrase, then simplify, before giving up. Next, to incorporate the observed reformulation behavior in a user simulator, we introduce the task of reformulation sequence generation: to generate a sequence of reformulated utterances with a given intent (rephrase or simplify). We develop methods by extending transformer models guided by the reformulation type and perform further filtering based on estimated reading difficulty. We demonstrate the effectiveness of our approach using both automatic and human evaluation.},
    address = {Madrid Spain},
    author = {Zhang, Shuo and Wang, Mu-Chun and Balog, Krisztian},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531936},
    file = {Zhang et al. - 2022 - Analyzing and Simulating User Utterance Reformulat.pdf:files/333/Zhang et al. - 2022 - Analyzing and Simulating User Utterance Reformulat.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {133--143},
    publisher = {ACM},
    title = {Analyzing and {Simulating} {User} {Utterance} {Reformulation} in {Conversational} {Recommender} {Systems}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531936},
    urldate = {2022-08-15},
    year = {2022}
}

@inproceedings{ren_variational_2022,
    abstract = {Conversational recommender systems (CRSs) provide recommendations through interactive conversations. CRSs typically provide recommendations through relatively straightforward interactions, where the system continuously inquires about a user’s explicit attribute-aware preferences and then decides which items to recommend. In addition, topic tracking is often used to provide naturally sounding responses. However, merely tracking topics is not enough to recognize a user’s real preferences in a dialogue. In this paper, we address the problem of accurately recognizing and maintaining user preferences in CRSs. Three challenges come with this problem: (1) An ongoing dialogue only provides the user’s short-term feedback; (2) Annotations of user preferences are not available; and (3) There may be complex semantic correlations among items that feature in a dialogue. We tackle these challenges by proposing an end-to-end variational reasoning approach to the task of conversational recommendation. We model both long-term preferences and short-term preferences as latent variables with topical priors for explicit long-term and short-term preference exploration, respectively. We use an efficient stochastic gradient variational Bayesian (SGVB) estimator for optimizing the derived evidence lower bound. A policy network is then used to predict topics for a clarification utterance or items for a recommendation response. The use of explicit sequences of preferences with multi-hop reasoning in a heterogeneous knowledge graph helps to provide more accurate conversational recommendation results.},
    address = {Madrid Spain},
    author = {Ren, Zhaochun and Tian, Zhi and Li, Dongdong and Ren, Pengjie and Yang, Liu and Xin, Xin and Liang, Huasheng and de Rijke, Maarten and Chen, Zhumin},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3532077},
    file = {Ren et al. - 2022 - Variational Reasoning about User Preferences for C.pdf:files/334/Ren et al. - 2022 - Variational Reasoning about User Preferences for C.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {165--175},
    publisher = {ACM},
    title = {Variational {Reasoning} about {User} {Preferences} for {Conversational} {Recommendation}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3532077},
    urldate = {2022-08-15},
    year = {2022}
}

@inproceedings{zhang_new_2022,
    abstract = {Sequential prediction is one of the key components in recommendation. In online e-commerce recommendation system, user behavior consists of the sequential visiting logs and item behavior contains the interacted user list in order. Most of the existing state-of-theart sequential prediction methods only consider the user behavior while ignoring the item behavior. In addition, we find that user behavior varies greatly at different time, and most existing models fail to characterize the rich temporal information. To address the above problems, we propose a transformer-based spatial-temporal recommendation framework (STEM). In the STEM framework, we first utilize attention mechanisms to model user behavior and item behavior, and then exploit spatial and temporal information through a transformer-based model. The STEM framework, as a plug-in, is able to be incorporated into many neural network-based sequential recommendation methods to improve performance. We conduct extensive experiments on three real-world Amazon datasets. The results demonstrate the effectiveness of our proposed framework.},
    address = {Madrid Spain},
    author = {Zhang, Jihai and Lin, Fangquan and Yang, Cheng and Jiang, Wei},
    booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3477495.3531846},
    file = {Zhang et al. - 2022 - A New Sequential Prediction Framework with Spatial.pdf:files/336/Zhang et al. - 2022 - A New Sequential Prediction Framework with Spatial.pdf:application/pdf},
    isbn = {978-1-4503-8732-3},
    language = {en},
    pages = {2282--2286},
    publisher = {ACM},
    title = {A {New} {Sequential} {Prediction} {Framework} with {Spatial}-temporal {Embedding}},
    url = {https://dl.acm.org/doi/10.1145/3477495.3531846},
    urldate = {2022-08-15},
    year = {2022}
}

@inproceedings{contractor_answering_2021,
    abstract = {We introduce the novel and challenging task of answering Pointsof-interest (POI) recommendation questions, using a collection of reviews that describe candidate answer entities (POIs). We harvest a QA dataset that contains 47,124 paragraph-sized user questions from travelers seeking POI recommendations for hotels, attractions and restaurants. Each question can have thousands of candidate entities to choose from and each candidate is associated with a collection of unstructured reviews. Questions can include requirements based on physical location, budget, timings as well as other subjective considerations related to ambience, quality of service etc. Our dataset requires reasoning over a large number of candidate answer entities (over 5300 per question on average) and we find that running commonly used neural architectures for QA is prohibitively expensive. Further, commonly used retriever-ranker based methods also do not work well for our task due to the nature of review-documents. Thus, as a first attempt at addressing some of the novel challenges of reasoning-at-scale posed by our task, we present a task specific baseline model that uses a three-stage cluster-select-rerank architecture. The model first clusters text for each entity to identify exemplar sentences describing an entity. It then uses a neural information retrieval (IR) module to select a set of potential entities from the large candidate set. A reranker uses a deeper attention-based architecture to pick the best answers from the selected entities. This strategy performs better than a pure retrieval or a pure attention-based reasoning approach yielding nearly 25\% relative improvement in Hits@3 over both approaches. To the best of our knowledge we are the first to present an unstructured QA-style task for POI-recommendation, using real-world tourism questions and POI-reviews.},
    address = {Virtual Event Queensland Australia},
    author = {Contractor, Danish and Shah, Krunal and Partap, Aditi and Singla, Parag and Mausam, Mausam},
    booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
    doi = {10.1145/3459637.3482320},
    file = {Contractor et al. - 2021 - Answering POI-recommendation Questions using Touri.pdf:files/337/Contractor et al. - 2021 - Answering POI-recommendation Questions using Touri.pdf:application/pdf},
    isbn = {978-1-4503-8446-9},
    language = {en},
    pages = {281--291},
    publisher = {ACM},
    title = {Answering {POI}-recommendation {Questions} using {Tourism} {Reviews}},
    url = {https://dl.acm.org/doi/10.1145/3459637.3482320},
    urldate = {2022-08-15},
    year = {2021}
}

@inproceedings{contractor_joint_2021,
    abstract = {Our goal is to answer real-world tourism questions that seek Pointsof-Interest (POI) recommendations. Such questions express various kinds of spatial and non-spatial constraints, necessitating a combination of textual and spatial reasoning. In response, we develop the first joint spatio-textual reasoning model, which combines geospatial knowledge with information in textual corpora to answer questions. We first develop a modular spatial-reasoning network that uses geo-coordinates of location names mentioned in a question, and of candidate answer POIs, to reason over only spatial constraints. We then combine our spatial-reasoner with a textual reasoner in a joint model and present experiments on a real world POI recommendation task. We report substantial improvements over existing models without joint spatio-textual reasoning. To the best of our knowledge, we are the first to develop a joint QA model that combines reasoning over external geo-spatial knowledge along with textual reasoning.},
    address = {Ljubljana Slovenia},
    author = {Contractor, Danish and Goel, Shashank and {Mausam} and Singla, Parag},
    booktitle = {Proceedings of the {Web} {Conference} 2021},
    doi = {10.1145/3442381.3449857},
    file = {Contractor et al. - 2021 - Joint Spatio-Textual Reasoning for Answering Touri.pdf:files/338/Contractor et al. - 2021 - Joint Spatio-Textual Reasoning for Answering Touri.pdf:application/pdf},
    isbn = {978-1-4503-8312-7},
    language = {en},
    pages = {1978--1989},
    publisher = {ACM},
    title = {Joint {Spatio}-{Textual} {Reasoning} for {Answering} {Tourism} {Questions}},
    url = {https://dl.acm.org/doi/10.1145/3442381.3449857},
    urldate = {2022-08-15},
    year = {2021}
}

@inproceedings{wang_understanding_2021,
    author = {Feng Wang and
Huaping Liu},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cvpr/WangL21a.bib},
    booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
2021, virtual, June 19-25, 2021},
    doi = {10.1109/CVPR46437.2021.00252},
    pages = {2495--2504},
    publisher = {Computer Vision Foundation / {IEEE}},
    timestamp = {Mon, 18 Jul 2022 01:00:00 +0200},
    title = {Understanding the Behaviour of Contrastive Loss},
    url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang\_Understanding\_the\_Behaviour\_of\_Contrastive\_Loss\_CVPR\_2021\_paper.html},
    year = {2021}
}

@misc{anand_supervised_2022,
    author = {Anand, Abhijit and Leonhardt, Jurek and Rudra, Koustav and Anand, Avishek},
    journal = {ArXiv preprint},
    title = {Supervised {Contrastive} {Learning} {Approach} for {Contextual} {Ranking}},
    url = {https://arxiv.org/abs/2207.03153},
    volume = {abs/2207.03153},
    year = {2022}
}

@inproceedings{jang_towards_2022-1,
    author = {Joel Jang and
Seonghyeon Ye and
Sohee Yang and
Joongbo Shin and
Janghoon Han and
Gyeonghun Kim and
Stanley Jungkyu Choi and
Minjoon Seo},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/JangYYSHKCS22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Towards Continual Knowledge Learning of Language Models},
    url = {https://openreview.net/forum?id=vfsRB5MImo9},
    year = {2022}
}

@inproceedings{akula_words_2020,
    address = {Online},
    author = {Akula, Arjun  and
Gella, Spandana  and
Al-Onaizan, Yaser  and
Zhu, Song-Chun  and
Reddy, Siva},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.586},
    pages = {6555--6565},
    publisher = {Association for Computational Linguistics},
    title = {Words Aren{'}t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions},
    url = {https://aclanthology.org/2020.acl-main.586},
    year = {2020}
}

@misc{thrush_winoground_2022,
    author = {Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
    journal = {ArXiv preprint},
    title = {Winoground: {Probing} {Vision} and {Language} {Models} for {Visio}-{Linguistic} {Compositionality}},
    url = {https://arxiv.org/abs/2204.03162},
    volume = {abs/2204.03162},
    year = {2022}
}

@misc{ramesh_hierarchical_2022,
    author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
    journal = {ArXiv preprint},
    title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
    url = {https://arxiv.org/abs/2204.06125},
    volume = {abs/2204.06125},
    year = {2022}
}

@inproceedings{yunlong_snora_2021,
    author = {Yunlong, Jiao},
    booktitle = {Amazon {Internal} {Work}},
    file = {20220107_weak_supervision_rca_naacl22.pdf:files/1196/20220107_weak_supervision_rca_naacl22.pdf:application/pdf},
    publisher = {Amazon},
    title = {{SNORA}: {A} {Weak} {Supervision} {Framework} for {AI} {Assistants}},
    year = {2021}
}

@inproceedings{wang_meta_2021,
    abstract = {Neural sequence labeling is widely adopted for many Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) and slot tagging for dialog systems and semantic parsing. Recent advances with large-scale pre-trained language models have shown remarkable success in these tasks when fine-tuned on large amounts of task-specific labeled data. However, obtaining such large-scale labeled training data is not only costly, but also may not be feasible in many sensitive user applications due to data access and privacy constraints. This is exacerbated for sequence labeling tasks requiring such annotations at token-level. In this work, we develop techniques to address the label scarcity challenge for neural sequence labeling models. Specifically, we propose a meta self-training framework which leverages very few manually annotated labels for training neural sequence models. While self-training serves as an effective mechanism to learn from large amounts of unlabeled data via iterative knowledge exchange – meta-learning helps in adaptive sample re-weighting to mitigate error propagation from noisy pseudo-labels. Extensive experiments on six benchmark datasets including two for massive multilingual NER and four slot tagging datasets for task-oriented dialog systems demonstrate the effectiveness of our method. With only 10 labeled examples for each class in each task, the proposed method achieves 10\% improvement over state-of-the-art methods demonstrating its effectiveness for limited training labels regime1.},
    address = {Virtual Event Singapore},
    author = {Wang, Yaqing and Mukherjee, Subhabrata and Chu, Haoda and Tu, Yuancheng and Wu, Ming and Gao, Jing and Awadallah, Ahmed Hassan},
    booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
    doi = {10.1145/3447548.3467235},
    file = {Wang et al. - 2021 - Meta Self-training for Few-shot Neural Sequence La.pdf:files/1199/Wang et al. - 2021 - Meta Self-training for Few-shot Neural Sequence La.pdf:application/pdf},
    isbn = {978-1-4503-8332-5},
    language = {en},
    pages = {1737--1747},
    publisher = {ACM},
    title = {Meta {Self}-training for {Few}-shot {Neural} {Sequence} {Labeling}},
    url = {https://dl.acm.org/doi/10.1145/3447548.3467235},
    urldate = {2022-10-19},
    year = {2021}
}

@inproceedings{zheng_walnut_2022,
    address = {Seattle, United States},
    author = {Zheng, Guoqing  and
Karamanolakis, Giannis  and
Shu, Kai  and
Awadallah, Ahmed},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2022.naacl-main.64},
    pages = {873--899},
    publisher = {Association for Computational Linguistics},
    title = {{WALNUT}: A Benchmark on Semi-weakly Supervised Learning for Natural Language Understanding},
    url = {https://aclanthology.org/2022.naacl-main.64},
    year = {2022}
}

@article{ratner_snorkel_2017,
    abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a ﬁrst-of-its-kind system that enables users to train stateof-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the ﬁrst end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a ﬂexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8× faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoﬀs in this new setting and propose an optimizer for automating tradeoﬀ decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Aﬀairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
    author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
    doi = {10.14778/3157794.3157797},
    file = {Ratner et al. - 2017 - Snorkel rapid training data creation with weak su.pdf:files/1203/Ratner et al. - 2017 - Snorkel rapid training data creation with weak su.pdf:application/pdf},
    issn = {2150-8097},
    journal = {Proceedings of the VLDB Endowment},
    language = {en},
    number = {3},
    pages = {269--282},
    shorttitle = {Snorkel},
    title = {Snorkel: rapid training data creation with weak supervision},
    url = {https://dl.acm.org/doi/10.14778/3157794.3157797},
    urldate = {2022-10-19},
    volume = {11},
    year = {2017}
}

@inproceedings{awasthi_learning_2020,
    author = {Abhijeet Awasthi and
Sabyasachi Ghosh and
Rasna Goyal and
Sunita Sarawagi},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/AwasthiGGS20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {Learning from Rules Generalizing Labeled Exemplars},
    url = {https://openreview.net/forum?id=SkeuexBtDr},
    year = {2020}
}

@inproceedings{noauthor_make_2022,
    booktitle = {{UCL} {Master} {Thesis}},
    file = {COMP0158_SKRS6.pdf:files/76/COMP0158_SKRS6.pdf:application/pdf},
    publisher = {UCL Master Thesis},
    title = {Make the {Noise} {Wik}(i) : {Proactive} {Information} {Retrieval} using {Wikipedia} {Concepts}},
    year = {2022}
}

@inproceedings{grandvalet_semi-supervised_2004,
    author = {Yves Grandvalet and
Yoshua Bengio},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/GrandvaletB04.bib},
    booktitle = {Advances in Neural Information Processing Systems 17 [Neural Information
Processing Systems, {NIPS} 2004, December 13-18, 2004, Vancouver,
British Columbia, Canada]},
    pages = {529--536},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Semi-supervised Learning by Entropy Minimization},
    url = {https://proceedings.neurips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html},
    year = {2004}
}

@inproceedings{xia_self-supervised_2021-1,
    author = {Xin Xia and
Hongzhi Yin and
Junliang Yu and
Qinyong Wang and
Lizhen Cui and
Xiangliang Zhang},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/0013YYWC021.bib},
    booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
2021},
    pages = {4503--4511},
    publisher = {{AAAI} Press},
    timestamp = {Sun, 02 Oct 2022 01:00:00 +0200},
    title = {Self-Supervised Hypergraph Convolutional Networks for Session-based
Recommendation},
    url = {https://ojs.aaai.org/index.php/AAAI/article/view/16578},
    year = {2021}
}

@inproceedings{sun_simulating_2021,
    abstract = {Evaluation is crucial in the development process of task-oriented dialogue systems. As an evaluation method, user simulation allows us to tackle issues such as scalability and cost-efficiency, making it a viable choice for large-scale automatic evaluation. To help build a human-like user simulator that can measure the quality of a dialogue, we propose the following task: simulating user satisfaction for the evaluation of task-oriented dialogue systems. The purpose of the task is to increase the evaluation power of user simulations and to make the simulation more human-like. To overcome a lack of annotated data, we propose a user satisfaction annotation dataset, User Satisfaction Simulation (USS), that includes 6,800 dialogues sampled from multiple domains, spanning real-world e-commerce dialogues, task-oriented dialogues constructed through Wizard-ofOz experiments, and movie recommendation dialogues. All user utterances in those dialogues, as well as the dialogues themselves, have been labeled based on a 5-level satisfaction scale. We also share three baseline methods for user satisfaction prediction and action prediction tasks. Experiments conducted on the USS dataset suggest that distributed representations outperform feature-based methods. A model based on hierarchical GRUs achieves the best performance in in-domain user satisfaction prediction, while a BERT-based model has better cross-domain generalization ability.},
    address = {Virtual Event Canada},
    author = {Sun, Weiwei and Zhang, Shuo and Balog, Krisztian and Ren, Zhaochun and Ren, Pengjie and Chen, Zhumin and de Rijke, Maarten},
    booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    doi = {10.1145/3404835.3463241},
    file = {Sun et al. - 2021 - Simulating User Satisfaction for the Evaluation of.pdf:files/1284/Sun et al. - 2021 - Simulating User Satisfaction for the Evaluation of.pdf:application/pdf},
    isbn = {978-1-4503-8037-9},
    language = {en},
    pages = {2499--2506},
    publisher = {ACM},
    title = {Simulating {User} {Satisfaction} for the {Evaluation} of {Task}-oriented {Dialogue} {Systems}},
    url = {https://dl.acm.org/doi/10.1145/3404835.3463241},
    urldate = {2022-10-24},
    year = {2021}
}

@inproceedings{deng_user_2022,
    abstract = {User Satisfaction Estimation (USE) is an important yet challenging task in goal-oriented conversational systems. Whether the user is satisfied with the system largely depends on the fulfillment of the user’s needs, which can be implicitly reflected by users’ dialogue acts. However, existing studies often neglect the sequential transitions of dialogue act or rely heavily on annotated dialogue act labels when utilizing dialogue acts to facilitate USE. In this paper, we propose a novel framework, namely USDA, to incorporate the sequential dynamics of dialogue acts for predicting user satisfaction, by jointly learning User Satisfaction Estimation and Dialogue Act Recognition tasks. In specific, we first employ a Hierarchical Transformer to encode the whole dialogue context, with two taskadaptive pre-training strategies to be a second-phase in-domain pre-training for enhancing the dialogue modeling ability. In terms of the availability of dialogue act labels, we further develop two variants of USDA to capture the dialogue act information in either supervised or unsupervised manners. Finally, USDA leverages the sequential transitions of both content and act features in the dialogue to predict the user satisfaction. Experimental results on four benchmark goal-oriented dialogue datasets across different applications show that the proposed method substantially and consistently outperforms existing methods on USE, and validate the important role of dialogue act sequences in USE.},
    address = {Virtual Event, Lyon France},
    author = {Deng, Yang and Zhang, Wenxuan and Lam, Wai and Cheng, Hong and Meng, Helen},
    booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
    doi = {10.1145/3485447.3512020},
    file = {Deng et al. - 2022 - User Satisfaction Estimation with Sequential Dialo.pdf:files/1288/Deng et al. - 2022 - User Satisfaction Estimation with Sequential Dialo.pdf:application/pdf},
    isbn = {978-1-4503-9096-5},
    language = {en},
    pages = {2998--3008},
    publisher = {ACM},
    title = {User {Satisfaction} {Estimation} with {Sequential} {Dialogue} {Act} {Modeling} in {Goal}-oriented {Conversational} {Systems}},
    url = {https://dl.acm.org/doi/10.1145/3485447.3512020},
    urldate = {2022-10-24},
    year = {2022}
}

@article{kaplan2020scaling,
    author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
    journal = {ArXiv preprint},
    title = {Scaling laws for neural language models},
    url = {https://arxiv.org/abs/2001.08361},
    volume = {abs/2001.08361},
    year = {2020}
}

@inproceedings{sun_bert4rec_2019,
    author = {Fei Sun and
Jun Liu and
Jian Wu and
Changhua Pei and
Xiao Lin and
Wenwu Ou and
Peng Jiang},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cikm/SunLWPLOJ19.bib},
    booktitle = {Proceedings of the 28th {ACM} International Conference on Information
and Knowledge Management, {CIKM} 2019, Beijing, China, November 3-7,
2019},
    doi = {10.1145/3357384.3357895},
    editor = {Wenwu Zhu and
Dacheng Tao and
Xueqi Cheng and
Peng Cui and
Elke A. Rundensteiner and
David Carmel and
Qi He and
Jeffrey Xu Yu},
    pages = {1441--1450},
    publisher = {{ACM}},
    timestamp = {Sun, 25 Oct 2020 01:00:00 +0200},
    title = {BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations
from Transformer},
    url = {https://doi.org/10.1145/3357384.3357895},
    year = {2019}
}

@inproceedings{li_dividemix_2020,
    author = {Junnan Li and
Richard Socher and
Steven C. H. Hoi},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/LiSH20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {DivideMix: Learning with Noisy Labels as Semi-supervised Learning},
    url = {https://openreview.net/forum?id=HJgExaVtwr},
    year = {2020}
}

@inproceedings{li_learning_2021,
    author = {Bo Li and
Yezhen Wang and
Shanghang Zhang and
Dongsheng Li and
Kurt Keutzer and
Trevor Darrell and
Han Zhao},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/cvpr/0080WZLKD021.bib},
    booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
2021, virtual, June 19-25, 2021},
    doi = {10.1109/CVPR46437.2021.00116},
    pages = {1104--1113},
    publisher = {Computer Vision Foundation / {IEEE}},
    timestamp = {Wed, 03 Aug 2022 01:00:00 +0200},
    title = {Learning Invariant Representations and Risks for Semi-Supervised Domain
Adaptation},
    url = {https://openaccess.thecvf.com/content/CVPR2021/html/Li\_Learning\_Invariant\_Representations\_and\_Risks\_for\_Semi-Supervised\_Domain\_Adaptation\_CVPR\_2021\_paper.html},
    year = {2021}
}

@inproceedings{ao_fast_2017,
    author = {Shuang Ao and
Xiang Li and
Charles X. Ling},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/AoLL17.bib},
    booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
February 4-9, 2017, San Francisco, California, {USA}},
    editor = {Satinder P. Singh and
Shaul Markovitch},
    pages = {1719--1725},
    publisher = {{AAAI} Press},
    timestamp = {Sat, 03 Feb 2018 00:00:00 +0100},
    title = {Fast Generalized Distillation for Semi-Supervised Domain Adaptation},
    url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14538},
    year = {2017}
}

@inproceedings{ramponi_neural_2020,
    address = {Barcelona, Spain (Online)},
    author = {Ramponi, Alan  and
Plank, Barbara},
    booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
    doi = {10.18653/v1/2020.coling-main.603},
    pages = {6838--6855},
    publisher = {International Committee on Computational Linguistics},
    title = {Neural Unsupervised Domain Adaptation in {NLP}{---}{A} Survey},
    url = {https://aclanthology.org/2020.coling-main.603},
    year = {2020}
}

@inproceedings{saito2017asymmetric,
    author = {Kuniaki Saito and
Yoshitaka Ushiku and
Tatsuya Harada},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/SaitoUH17.bib},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning,
{ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
    editor = {Doina Precup and
Yee Whye Teh},
    pages = {2988--2997},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
    title = {Asymmetric Tri-training for Unsupervised Domain Adaptation},
    url = {http://proceedings.mlr.press/v70/saito17a.html},
    volume = {70},
    year = {2017}
}

@inproceedings{ruder_strong_2018,
    address = {Melbourne, Australia},
    author = {Ruder, Sebastian  and
Plank, Barbara},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P18-1096},
    pages = {1044--1054},
    publisher = {Association for Computational Linguistics},
    title = {Strong Baselines for Neural Semi-Supervised Learning under Domain Shift},
    url = {https://aclanthology.org/P18-1096},
    year = {2018}
}

@inproceedings{karisani_semi-supervised_2021,
    abstract = {We present a neural semi-supervised learning model termed SelfPretraining. Our model is inspired by the classic self-training algorithm. However, as opposed to self-training, Self-Pretraining is threshold-free, it can potentially update its belief about previously labeled documents, and can cope with the semantic drift problem. Self-Pretraining is iterative and consists of two classifiers. In each iteration, one classifier draws a random set of unlabeled documents and labels them. This set is used to initialize the second classifier, to be further trained by the set of labeled documents. The algorithm proceeds to the next iteration and the classifiers’ roles are reversed. To improve the flow of information across the iterations and also to cope with the semantic drift problem, SelfPretraining employs an iterative distillation process, transfers hypotheses across the iterations, utilizes a two-stage training model, uses an efficient learning rate schedule, and employs a pseudo-label transformation heuristic. We have evaluated our model in three publicly available social media datasets. Our experiments show that Self-Pretraining outperforms the existing state-of-the-art semisupervised classifiers across multiple settings. Our code is available at https://github.com/p-karisani/self-pretraining.},
    address = {Virtual Event Israel},
    author = {Karisani, Payam and Karisani, Negin},
    booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
    doi = {10.1145/3437963.3441814},
    file = {Karisani and Karisani - 2021 - Semi-Supervised Text Classification via Self-Pretr.pdf:files/1391/Karisani and Karisani - 2021 - Semi-Supervised Text Classification via Self-Pretr.pdf:application/pdf},
    isbn = {978-1-4503-8297-7},
    language = {en},
    pages = {40--48},
    publisher = {ACM},
    title = {Semi-{Supervised} {Text} {Classification} via {Self}-{Pretraining}},
    url = {https://dl.acm.org/doi/10.1145/3437963.3441814},
    urldate = {2022-10-30},
    year = {2021}
}

@inproceedings{sun_return_2016,
    author = {Baochen Sun and
Jiashi Feng and
Kate Saenko},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/SunFS16.bib},
    booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence,
February 12-17, 2016, Phoenix, Arizona, {USA}},
    editor = {Dale Schuurmans and
Michael P. Wellman},
    pages = {2058--2065},
    publisher = {{AAAI} Press},
    timestamp = {Thu, 21 Apr 2016 01:00:00 +0200},
    title = {Return of Frustratingly Easy Domain Adaptation},
    url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12443},
    year = {2016}
}

@inproceedings{lim_semi-supervised_2020,
    author = {KyungTae Lim and
Jay Yoon Lee and
Jaime G. Carbonell and
Thierry Poibeau},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/LimLCP20.bib},
    booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
    pages = {8344--8351},
    publisher = {{AAAI} Press},
    timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
    title = {Semi-Supervised Learning on Meta Structure: Multi-Task Tagging and
Parsing in Low-Resource Scenarios},
    url = {https://aaai.org/ojs/index.php/AAAI/article/view/6351},
    year = {2020}
}

@inproceedings{li_semi-supervised_2019,
    address = {Florence, Italy},
    author = {Li, Zhenghua  and
Peng, Xue  and
Zhang, Min  and
Wang, Rui  and
Si, Luo},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1229},
    pages = {2386--2395},
    publisher = {Association for Computational Linguistics},
    title = {Semi-supervised Domain Adaptation for Dependency Parsing},
    url = {https://aclanthology.org/P19-1229},
    year = {2019}
}

@inproceedings{ma_normalized_2020,
    author = {Xingjun Ma and
Hanxun Huang and
Yisen Wang and
Simone Romano and
Sarah M. Erfani and
James Bailey},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/MaH00E020.bib},
    booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
    pages = {6543--6553},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
    title = {Normalized Loss Functions for Deep Learning with Noisy Labels},
    url = {http://proceedings.mlr.press/v119/ma20c.html},
    volume = {119},
    year = {2020}
}

@inproceedings{kim_nlnl_2019,
    author = {Youngdong Kim and
Junho Yim and
Juseung Yun and
Junmo Kim},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iccv/KimYYK19.bib},
    booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
2019, Seoul, Korea (South), October 27 - November 2, 2019},
    doi = {10.1109/ICCV.2019.00019},
    pages = {101--110},
    publisher = {{IEEE}},
    timestamp = {Thu, 05 Mar 2020 00:00:00 +0100},
    title = {{NLNL:} Negative Learning for Noisy Labels},
    url = {https://doi.org/10.1109/ICCV.2019.00019},
    year = {2019}
}

@inproceedings{zhang_generalized_2018,
    author = {Zhilu Zhang and
Mert R. Sabuncu},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/ZhangS18.bib},
    booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montr{\'{e}}al, Canada},
    editor = {Samy Bengio and
Hanna M. Wallach and
Hugo Larochelle and
Kristen Grauman and
Nicol{\`{o}} Cesa{-}Bianchi and
Roman Garnett},
    pages = {8792--8802},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Generalized Cross Entropy Loss for Training Deep Neural Networks with
Noisy Labels},
    url = {https://proceedings.neurips.cc/paper/2018/hash/f2925f97bc13ad2852a7a551802feea0-Abstract.html},
    year = {2018}
}

@inproceedings{ghosh_robust_2017,
    author = {Aritra Ghosh and
Himanshu Kumar and
P. S. Sastry},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/GhoshKS17.bib},
    booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
February 4-9, 2017, San Francisco, California, {USA}},
    editor = {Satinder P. Singh and
Shaul Markovitch},
    pages = {1919--1925},
    publisher = {{AAAI} Press},
    timestamp = {Mon, 29 Jun 2020 01:00:00 +0200},
    title = {Robust Loss Functions under Label Noise for Deep Neural Networks},
    url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14759},
    year = {2017}
}

@inproceedings{chen_weakly_2022,
    author = {Chen, Maximillian and Papangelis, Alexandros and Tao, Chenyang and Rosenbaum, Andy and Kim, Seokhwan and Liu, Yang and Yu, Zhou and Hakkani-Tur, Dilek},
    journal = {ArXiv preprint},
    title = {Weakly {Supervised} {Data} {Augmentation} {Through} {Prompting} for {Dialogue} {Understanding}},
    url = {https://arxiv.org/abs/2210.14169},
    volume = {abs/2210.14169},
    year = {2022}
}

@inproceedings{csaky_improving_2019,
    address = {Florence, Italy},
    author = {Cs{\'a}ky, Rich{\'a}rd  and
Purgai, Patrik  and
Recski, G{\'a}bor},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1567},
    pages = {5650--5669},
    publisher = {Association for Computational Linguistics},
    title = {Improving Neural Conversational Models with Entropy-Based Data Filtering},
    url = {https://aclanthology.org/P19-1567},
    year = {2019}
}

@inproceedings{wang_global_2020,
    author = {Ziyang Wang and
Wei Wei and
Gao Cong and
Xiao{-}Li Li and
Xianling Mao and
Minghui Qiu},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/sigir/Wang0CLMQ20.bib},
    booktitle = {Proceedings of the 43rd International {ACM} {SIGIR} conference on
research and development in Information Retrieval, {SIGIR} 2020, Virtual
Event, China, July 25-30, 2020},
    doi = {10.1145/3397271.3401142},
    editor = {Jimmy Huang and
Yi Chang and
Xueqi Cheng and
Jaap Kamps and
Vanessa Murdock and
Ji{-}Rong Wen and
Yiqun Liu},
    pages = {169--178},
    publisher = {{ACM}},
    timestamp = {Mon, 04 Jan 2021 00:00:00 +0100},
    title = {Global Context Enhanced Graph Neural Networks for Session-based Recommendation},
    url = {https://doi.org/10.1145/3397271.3401142},
    year = {2020}
}

@inproceedings{diwan_why_2022,
    address = {Abu Dhabi, United Arab Emirates},
    author = {Diwan, Anuj  and
Berry, Layne  and
Choi, Eunsol  and
Harwath, David  and
Mahowald, Kyle},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    pages = {2236--2250},
    publisher = {Association for Computational Linguistics},
    title = {Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality},
    url = {https://aclanthology.org/2022.emnlp-main.143},
    year = {2022}
}

@inproceedings{wu_session-based_2019,
    author = {Shu Wu and
Yuyuan Tang and
Yanqiao Zhu and
Liang Wang and
Xing Xie and
Tieniu Tan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/aaai/WuT0WXT19.bib},
    booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
2019, The Thirty-First Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
USA, January 27 - February 1, 2019},
    doi = {10.1609/aaai.v33i01.3301346},
    pages = {346--353},
    publisher = {{AAAI} Press},
    timestamp = {Tue, 02 Feb 2021 00:00:00 +0100},
    title = {Session-Based Recommendation with Graph Neural Networks},
    url = {https://doi.org/10.1609/aaai.v33i01.3301346},
    year = {2019}
}

@inproceedings{schick-schutze-2021-exploiting,
    address = {Online},
    author = {Schick, Timo  and
Sch{\"u}tze, Hinrich},
    booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
    doi = {10.18653/v1/2021.eacl-main.20},
    pages = {255--269},
    publisher = {Association for Computational Linguistics},
    title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
    url = {https://aclanthology.org/2021.eacl-main.20},
    year = {2021}
}

@article{austin2021program,
    author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
    journal = {ArXiv preprint},
    title = {Program synthesis with large language models},
    url = {https://arxiv.org/abs/2108.07732},
    volume = {abs/2108.07732},
    year = {2021}
}

@inproceedings{hu2022lora,
    author = {Edward J. Hu and
Yelong Shen and
Phillip Wallis and
Zeyuan Allen{-}Zhu and
Yuanzhi Li and
Shean Wang and
Lu Wang and
Weizhu Chen},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
    booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
    publisher = {OpenReview.net},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {LoRA: Low-Rank Adaptation of Large Language Models},
    url = {https://openreview.net/forum?id=nZeVKeeFYf9},
    year = {2022}
}

@article{chowdhery2022palm,
    author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
    journal = {ArXiv preprint},
    title = {Palm: Scaling language modeling with pathways},
    url = {https://arxiv.org/abs/2204.02311},
    volume = {abs/2204.02311},
    year = {2022}
}

@inproceedings{carlini2021extracting,
    author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
    booktitle = {USENIX Security Symposium},
    title = {Extracting Training Data from Large Language Models.},
    volume = {6},
    year = {2021}
}

@inproceedings{10.1145/3442188.3445922,
    abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
    address = {New York, NY, USA},
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    doi = {10.1145/3442188.3445922},
    isbn = {9781450383097},
    location = {Virtual Event, Canada},
    numpages = {14},
    pages = {610–623},
    publisher = {Association for Computing Machinery},
    series = {FAccT '21},
    title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
    url = {https://doi.org/10.1145/3442188.3445922},
    year = {2021}
}

@inproceedings{bender-koller-2020-climbing,
    address = {Online},
    author = {Bender, Emily M.  and
Koller, Alexander},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.463},
    pages = {5185--5198},
    publisher = {Association for Computational Linguistics},
    title = {Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data},
    url = {https://aclanthology.org/2020.acl-main.463},
    year = {2020}
}

@inproceedings{gao-etal-2021-making,
    address = {Online},
    author = {Gao, Tianyu  and
Fisch, Adam  and
Chen, Danqi},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.295},
    pages = {3816--3830},
    publisher = {Association for Computational Linguistics},
    title = {Making Pre-trained Language Models Better Few-shot Learners},
    url = {https://aclanthology.org/2021.acl-long.295},
    year = {2021}
}

@inproceedings{bowman-etal-2015-large,
    address = {Lisbon, Portugal},
    author = {Bowman, Samuel R.  and
Angeli, Gabor  and
Potts, Christopher  and
Manning, Christopher D.},
    booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D15-1075},
    pages = {632--642},
    publisher = {Association for Computational Linguistics},
    title = {A large annotated corpus for learning natural language inference},
    url = {https://aclanthology.org/D15-1075},
    year = {2015}
}

@article{t5,
    author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
    journal = {J. Mach. Learn. Res.},
    pages = {140:1--140:67},
    timestamp = {Fri, 05 Feb 2021 00:00:00 +0100},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
    url = {http://jmlr.org/papers/v21/20-074.html},
    volume = {21},
    year = {2020}
}

@article{liu2021gpt,
    author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
    journal = {ArXiv preprint},
    title = {GPT Understands, Too},
    url = {https://arxiv.org/abs/2103.10385},
    volume = {abs/2103.10385},
    year = {2021}
}

@article{warstadt2019neural_cola,
    address = {Cambridge, MA},
    author = {Warstadt, Alex  and
Singh, Amanpreet  and
Bowman, Samuel R.},
    doi = {10.1162/tacl_a_00290},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {625--641},
    publisher = {MIT Press},
    title = {Neural Network Acceptability Judgments},
    url = {https://aclanthology.org/Q19-1040},
    volume = {7},
    year = {2019}
}

@inproceedings{socher2013recursive_sst-2,
    address = {Seattle, Washington, USA},
    author = {Socher, Richard  and
Perelygin, Alex  and
Wu, Jean  and
Chuang, Jason  and
Manning, Christopher D.  and
Ng, Andrew  and
Potts, Christopher},
    booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
    pages = {1631--1642},
    publisher = {Association for Computational Linguistics},
    title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
    url = {https://aclanthology.org/D13-1170},
    year = {2013}
}

@inproceedings{dolan2005automatically_mrpc,
    author = {Dolan, William B.  and
Brockett, Chris},
    booktitle = {Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)},
    title = {Automatically Constructing a Corpus of Sentential Paraphrases},
    url = {https://aclanthology.org/I05-5002},
    year = {2005}
}

@inproceedings{cer2017semeval_sts-b,
    address = {Vancouver, Canada},
    author = {Cer, Daniel  and
Diab, Mona  and
Agirre, Eneko  and
Lopez-Gazpio, I{\~n}igo  and
Specia, Lucia},
    booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)},
    doi = {10.18653/v1/S17-2001},
    pages = {1--14},
    publisher = {Association for Computational Linguistics},
    title = {{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},
    url = {https://aclanthology.org/S17-2001},
    year = {2017}
}

@inproceedings{williams2018broad_mnli,
    address = {New Orleans, Louisiana},
    author = {Williams, Adina  and
Nangia, Nikita  and
Bowman, Samuel},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    doi = {10.18653/v1/N18-1101},
    pages = {1112--1122},
    publisher = {Association for Computational Linguistics},
    title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
    url = {https://aclanthology.org/N18-1101},
    year = {2018}
}

@inproceedings{dagan2005pascal_rte1,
    author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
    booktitle = {the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment},
    title = {The {PASCAL} Recognising Textual Entailment Challenge},
    year = {2005}
}

@misc{yang2024bayesian1,
      title={Bayesian Reward Models for LLM Alignment}, 
      author={Adam X. Yang and Maxime Robeyns and Thomas Coste and Jun Wang and Haitham Bou-Ammar and Laurence Aitchison},
      year={2024},
      eprint={2402.13210},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{giampiccolo2007third_rte3,
    address = {Prague},
    author = {Giampiccolo, Danilo  and
Magnini, Bernardo  and
Dagan, Ido  and
Dolan, Bill},
    booktitle = {Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing},
    pages = {1--9},
    publisher = {Association for Computational Linguistics},
    title = {The Third {PASCAL} Recognizing Textual Entailment Challenge},
    url = {https://aclanthology.org/W07-1401},
    year = {2007}
}

@inproceedings{bentivogli2009fifth_rte4,
    author = {Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
    booktitle = {TAC},
    title = {The Fifth {PASCAL} Recognizing Textual Entailment Challenge.},
    year = {2009}
}

@inproceedings{levesque2012winograd_wnli,
    author = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
    booktitle = {Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
    title = {The winograd schema challenge},
    year = {2012}
}

@inproceedings{rajpurkar2016squad,
    address = {Austin, Texas},
    author = {Rajpurkar, Pranav  and
Zhang, Jian  and
Lopyrev, Konstantin  and
Liang, Percy},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D16-1264},
    pages = {2383--2392},
    publisher = {Association for Computational Linguistics},
    title = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
    url = {https://aclanthology.org/D16-1264},
    year = {2016}
}

@inproceedings{bao2020fewshot,
    author = {Yujia Bao and
Menghua Wu and
Shiyu Chang and
Regina Barzilay},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/BaoWCB20.bib},
    booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
    publisher = {OpenReview.net},
    timestamp = {Thu, 07 May 2020 01:00:00 +0200},
    title = {Few-shot Text Classification with Distributional Signatures},
    url = {https://openreview.net/forum?id=H1emfT4twB},
    year = {2020}
}

@article{Beltagy2020Longformer,
    author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
    journal = {ArXiv preprint},
    title = {Longformer: The Long-Document {Transformer}},
    url = {https://arxiv.org/abs/2004.05150},
    volume = {abs/2004.05150},
    year = {2020}
}

@inproceedings{pang2004sentimental_subj,
    address = {Barcelona, Spain},
    author = {Pang, Bo  and
Lee, Lillian},
    booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04)},
    doi = {10.3115/1218955.1218990},
    pages = {271--278},
    title = {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},
    url = {https://aclanthology.org/P04-1035},
    year = {2004}
}

@inproceedings{pang2005seeing_mr,
    address = {Ann Arbor, Michigan},
    author = {Pang, Bo  and
Lee, Lillian},
    booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)},
    doi = {10.3115/1219840.1219855},
    pages = {115--124},
    publisher = {Association for Computational Linguistics},
    title = {Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales},
    url = {https://aclanthology.org/P05-1015},
    year = {2005}
}

@inproceedings{voorhees2000building_trec,
    author = {Voorhees, Ellen M and Tice, Dawn M},
    booktitle = {the 23rd annual international ACM SIGIR conference on Research and development in information retrieval},
    title = {Building a question answering test collection},
    year = {2000}
}

@inproceedings{hu2004mining_cr,
    author = {Hu, Minqing and Liu, Bing},
    booktitle = {ACM SIGKDD international conference on Knowledge discovery and data mining},
    title = {Mining and summarizing customer reviews},
    year = {2004}
}

@misc{zhao2024long,
      title={Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning}, 
      author={Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
      year={2024},
      eprint={2402.04833},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wiebe2005annotating_mpqa,
    author = {Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
    journal = {Language resources and evaluation},
    number = {2-3},
    publisher = {Springer},
    title = {Annotating expressions of opinions and emotions in language},
    volume = {39},
    year = {2005}
}

@article{he2023preserving,
    author = {He, Guande and Chen, Jianfei and Zhu, Jun},
    journal = {ArXiv preprint},
    title = {Preserving pre-trained features helps calibrate fine-tuned language models},
    url = {https://arxiv.org/abs/2305.19249},
    volume = {abs/2305.19249},
    year = {2023}
}

@inproceedings{yang2024bayesian,
    author = {Yang, Adam X and Robeyns, Maxime and Wang, Xi and Aitchison, Laurence},
    booktitle = {ICLR},
    title = {Bayesian low-rank adaptation for large language models},
    year = {2024}
}
