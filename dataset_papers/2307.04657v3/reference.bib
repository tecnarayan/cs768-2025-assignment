@article{ppo-lag-2019,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal={arXiv preprint arXiv:1910.01708},
  volume={7},
  pages={1},
  year={2019}
}

@article{Perez2022-mk,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{wu2023tidybot,
  title={Tidybot: Personalized robot assistance with large language models},
  author={Wu, Jimmy and Antonova, Rika and Kan, Adam and Lepert, Marion and Zeng, Andy and Song, Shuran and Bohg, Jeannette and Rusinkiewicz, Szymon and Funkhouser, Thomas},
  journal={arXiv preprint arXiv:2305.05658},
  year={2023}
}

@article{katz2023gpt,
  title={Gpt-4 passes the bar exam},
  author={Katz, Daniel Martin and Bommarito, Michael James and Gao, Shang and Arredondo, Pablo},
  journal={Available at SSRN 4389233},
  year={2023}
}

@MISC{Christian2023-dt,
  title        = "Amazing ``jailbreak'' bypasses {ChatGPT's} ethics safeguards",
  booktitle    = "Futurism",
  author       = "Christian, Jon",
  abstract     = "A brilliant ChatGPT jailbreak lets you bypass many of its
                  guardrails against unethical outputs -- and it has some
                  interesting implications.",
  month        =  feb,
  year         =  2023,
  howpublished = "\url{https://futurism.com/amazing-jailbreak-chatgpt}",
  note         = "Accessed: 2023-6-7",
  language     = "en"
}

@ARTICLE{Chilton2023-uz,
  title    = "The New Risks {ChatGPT} Poses to Cybersecurity",
  author   = "Chilton, Jim",
  abstract = "The FBI's 2021 Internet Crime Report found that phishing is the
              most common IT threat in America. From a hacker's perspective,
              ChatGPT is a game changer, affording hackers from all over the
              globe a near fluency in English to bolster their phishing
              campaigns. Bad actors may also be able to trick the AI into
              generating hacking code. And, of course, there's the potential
              for ChatGPT itself to be hacked, disseminating dangerous
              misinformation and political propaganda. This article examines
              these new risks, explores the needed training and tools for
              cybersecurity professionals to respond, and calls for government
              oversight to ensure that AI usage doesn't become detrimental to
              cybersecurity efforts.",
  journal  = "Harvard Business Review",
  month    =  apr,
  year     =  2023
}

@ARTICLE{Newman2023-hs,
  title    = "{ChatGPT} Scams Are Infiltrating the App Store and Google Play",
  author   = "Newman, Lily Hay",
  abstract = "An explosion of interest in OpenAI's sophisticated chatbot means
              a proliferation of ``fleeceware'' apps that trick users with
              sneaky in-app subscriptions.",
  journal  = "Wired",
  month    =  may,
  year     =  2023,
  language = "en"
}

@MISC{noauthor_undated-ae,
  title        = "{ChatGPT} plugins",
  abstract     = "We've implemented initial support for plugins in ChatGPT.
                  Plugins are tools designed specifically for language models
                  with safety as a core principle, and help ChatGPT access
                  up-to-date information, run computations, or use third-party
                  services.",
  howpublished = "\url{https://openai.com/blog/chatgpt-plugins}",
  note         = "Accessed: 2023-6-7",
  language     = "en"
}




@misc{anil2023palm,
      title={PaLM 2 Technical Report}, 
      author={Rohan Anil and Andrew M. Dai and et. al},
      year={2023},
      eprint={2305.10403},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{shah2023lm,
  title={Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action},
  author={Shah, Dhruv and Osi{\'n}ski, B{\l}a{\.z}ej and Levine, Sergey and others},
  booktitle={Conference on Robot Learning},
  pages={492--504},
  year={2023},
  organization={PMLR}
}

@article{vemprala2023chatgpt,
  title={Chatgpt for robotics: Design principles and model abilities},
  author={Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  journal={Microsoft Auton. Syst. Robot. Res},
  volume={2},
  pages={20},
  year={2023}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and Individual Differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

@article{kung2023performance,
  title={Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models},
  author={Kung, Tiffany H and Cheatham, Morgan and Medenilla, Arielle and Sillos, Czarina and De Leon, Lorie and Elepa{\~n}o, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and others},
  journal={PLoS digital health},
  volume={2},
  number={2},
  pages={e0000198},
  year={2023},
  publisher={Public Library of Science}
}

@article{arora2023promise,
  title={The promise of large language models in health care},
  author={Arora, Anmol and Arora, Ananya},
  journal={The Lancet},
  volume={401},
  number={10377},
  pages={641},
  year={2023},
  publisher={Elsevier}
}

@article{moor2023foundation,
  title={Foundation models for generalist medical artificial intelligence},
  author={Moor, Michael and Banerjee, Oishi and Abad, Zahra Shakeri Hossein and Krumholz, Harlan M and Leskovec, Jure and Topol, Eric J and Rajpurkar, Pranav},
  journal={Nature},
  volume={616},
  number={7956},
  pages={259--265},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{yang2022large,
  title={A large language model for electronic health records},
  author={Yang, Xi and Chen, Aokun and PourNejatian, Nima and Shin, Hoo Chang and Smith, Kaleb E and Parisien, Christopher and Compas, Colin and Martin, Cheryl and Costa, Anthony B and Flores, Mona G and others},
  journal={npj Digital Medicine},
  volume={5},
  number={1},
  pages={194},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
@inproceedings{ousidhoum2021probing,
  title={Probing toxic content in large pre-trained language models},
  author={Ousidhoum, Nedjma and Zhao, Xinran and Fang, Tianqing and Song, Yangqiu and Yeung, Dit-Yan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4262--4274},
  year={2021}
}

@article{deshpande2023toxicity,
  title={Toxicity in chatgpt: Analyzing persona-assigned language models},
  author={Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2304.05335},
  year={2023}
}

@misc{srivastava2022imitation,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and et. al.},
      year={2022},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{huang2019reducing,
  title={Reducing sentiment bias in language models via counterfactual evaluation},
  author={Huang, Po-Sen and Zhang, Huan and Jiang, Ray and Stanforth, Robert and Welbl, Johannes and Rae, Jack and Maini, Vishal and Yogatama, Dani and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1911.03064},
  year={2019}
}

@misc{deroy2023ready,
      title={How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?}, 
      author={Aniket Deroy and Kripabandhu Ghosh and Saptarshi Ghosh},
      year={2023},
      eprint={2306.01248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2019text,
  title={Text summarization with pretrained encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}

@article{Bai2022-mt,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


@INPROCEEDINGS{Xu2021-ce,
  title     = "{Bot-Adversarial} Dialogue for Safe Conversational Agents",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  author    = "Xu, Jing and Ju, Da and Li, Margaret and Boureau, Y-Lan and
               Weston, Jason and Dinan, Emily",
  abstract  = "Conversational agents trained on large unlabeled corpora of
               human interactions will learn patterns and mimic behaviors
               therein, which include offensive or otherwise toxic behavior. We
               introduce a new human-and-model-in-the-loop framework for
               evaluating the toxicity of such models, and compare a variety of
               existing methods in both the cases of non-adversarial and
               adversarial users that expose their weaknesses. We then go on to
               propose two novel methods for safe conversational agents, by
               either training on data from our new human-and-model-in-the-loop
               framework in a two-stage system, or ''baking-in'' safety to the
               generative model itself. We find our new techniques are (i)
               safer than existing models; while (ii) maintaining usability
               metrics such as engagingness relative to state-of-the-art
               chatbots. In contrast, we expose serious safety issues in
               existing standard systems like GPT2, DialoGPT, and BlenderBot.",
  publisher = "Association for Computational Linguistics",
  pages     = "2950--2968",
  month     =  jun,
  year      =  2021,
  address   = "Online"
}


@INPROCEEDINGS{Lin2022-ys,
  title     = "{{T}ruthful{QA}}: Measuring How Models Mimic Human Falsehoods",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  author    = "Lin, Stephanie and Hilton, Jacob and Evans, Owain",
  abstract  = "We propose a benchmark to measure whether a language model is
               truthful in generating answers to questions. The benchmark
               comprises 817 questions that span 38 categories, including
               health, law, finance and politics. We crafted questions that
               some humans would answer falsely due to a false belief or
               misconception. To perform well, models must avoid generating
               false answers learned from imitating human texts. We tested
               GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was
               truthful on 58\% of questions, while human performance was 94\%.
               Models generated many false answers that mimic popular
               misconceptions and have the potential to deceive humans. The
               largest models were generally the least truthful. This contrasts
               with other NLP tasks, where performance improves with model
               size. However, this result is expected if false answers are
               learned from the training distribution. We suggest that scaling
               up models alone is less promising for improving truthfulness
               than fine-tuning using training objectives other than imitation
               of text from the web.",
  publisher = "Association for Computational Linguistics",
  pages     = "3214--3252",
  month     =  may,
  year      =  2022,
  address   = "Dublin, Ireland"
}

@article{hosseini2017deceiving,
  title={Deceiving google's perspective api built for detecting toxic comments},
  author={Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
  journal={arXiv preprint arXiv:1702.08138},
  year={2017}
}

@article{rieder2021fabrics,
  title={The fabrics of machine moderation: Studying the technical, normative, and organizational structure of Perspective API},
  author={Rieder, Bernhard and Skop, Yarden},
  journal={Big Data \& Society},
  volume={8},
  number={2},
  pages={20539517211046181},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{Parrish2021-fi,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2110.08193},
  year={2021}
}

@article{Rauh2022-ac,
  title={Characteristics of harmful text: Towards rigorous benchmarking of language models},
  author={Rauh, Maribeth and Mellor, John and Uesato, Jonathan and Huang, Po-Sen and Welbl, Johannes and Weidinger, Laura and Dathathri, Sumanth and Glaese, Amelia and Irving, Geoffrey and Gabriel, Iason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24720--24739},
  year={2022}
}

@INPROCEEDINGS{Gehman2020-zq,
  title     = "{{R}eal{T}oxicity{P}rompts}: Evaluating Neural Toxic
               Degeneration in Language Models",
  booktitle = "Findings of the Association for Computational Linguistics:
               {EMNLP} 2020",
  author    = "Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi,
               Yejin and Smith, Noah A",
  abstract  = "Pretrained neural language models (LMs) are prone to generating
               racist, sexist, or otherwise toxic language which hinders their
               safe deployment. We investigate the extent to which pretrained
               LMs can be prompted to generate toxic language, and the
               effectiveness of controllable text generation algorithms at
               preventing such toxic degeneration. We create and release
               RealToxicityPrompts, a dataset of 100K naturally occurring,
               sentence-level prompts derived from a large corpus of English
               web text, paired with toxicity scores from a widely-used
               toxicity classifier. Using RealToxicityPrompts, we find that
               pretrained LMs can degenerate into toxic text even from
               seemingly innocuous prompts. We empirically assess several
               controllable generation methods, and find that while data- or
               compute-intensive methods (e.g., adaptive pretraining on
               non-toxic data) are more effective at steering away from
               toxicity than simpler solutions (e.g., banning ``bad'' words),
               no current method is failsafe against neural toxic degeneration.
               To pinpoint the potential cause of such persistent toxic
               degeneration, we analyze two web text corpora used to pretrain
               several LMs (including GPT-2; Radford et. al, 2019), and find a
               significant amount of offensive, factually unreliable, and
               otherwise toxic content. Our work provides a test bed for
               evaluating toxic generations by LMs and stresses the need for
               better data selection processes for pretraining.",
  publisher = "Association for Computational Linguistics",
  pages     = "3356--3369",
  month     =  nov,
  year      =  2020,
  address   = "Online"
}

@article{Schulman2017-mq,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{Ouyang2022-lg,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@inproceedings{Lees2022-at,
  title={A new generation of perspective api: Efficient multilingual character-level transformers},
  author={Lees, Alyssa and Tran, Vinh Q and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3197--3207},
  year={2022}
}

@MISC{Jigsaw2017-jh,
  title        = "Perspective {API}",
  author       = "Jigsaw, Google",
  year         =  2017,
  howpublished = "\url{https://www.perspectiveapi.com/}",
  note         = "Accessed: 2023-06-05"
}


@MISC{OpenAI2023-ws,
  title        = "Moderation {API}",
  booktitle    = "{OpenAI} {API}",
  author       = "{OpenAI}",
  year         =  2023,
  howpublished = "\url{https://platform.openai.com/docs/guides/moderation/overview}",
  note         = "Accessed: 2023-6-5"
}

@misc{SHP,
  author = {Ethayarajh, Kawin and Zhang, Heidi and Wang, Yizhong and Jurafsky, Dan},
  title = {Stanford Human Preferences Dataset},
  year = {2023},
  url = {https://huggingface.co/datasets/stanfordnlp/SHP}
}


@article{Weidinger2021-mm,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}


@ARTICLE{Gor2021-iz,
  title         = "Toward Deconfounding the Influence of Entity Demographics
                   for Question Answering Accuracy",
  author        = "Gor, Maharshi and Webster, Kellie and Boyd-Graber, Jordan",
  abstract      = "The goal of question answering (QA) is to answer any
                   question. However, major QA datasets have skewed
                   distributions over gender, profession, and nationality.
                   Despite that skew, model accuracy analysis reveals little
                   evidence that accuracy is lower for people based on gender
                   or nationality; instead, there is more variation on
                   professions (question topic). But QA's lack of
                   representation could itself hide evidence of bias,
                   necessitating QA datasets that better represent global
                   diversity.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.07571"
}


@article{Shevlane2023-by,
  title={Model evaluation for extreme risks},
  author={Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and others},
  journal={arXiv preprint arXiv:2305.15324},
  year={2023}
}

@article{Ganguli2022-td,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{3h-2021,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}
@article{text-summarize-2020,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{follow-instruction-2023,
  title={The Wisdom of Hindsight Makes Language Models Better Instruction Followers},
  author={Zhang, Tianjun and Liu, Fangchen and Wong, Justin and Abbeel, Pieter and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2302.05206},
  year={2023}
}
@article{cai-2022,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
@misc{wu2023finegrained,
      title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training}, 
      author={Zeqiu Wu and Yushi Hu and Weijia Shi and Nouha Dziri and Alane Suhr and Prithviraj Ammanabrolu and Noah A. Smith and Mari Ostendorf and Hannaneh Hajishirzi},
      year={2023},
      eprint={2306.01693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{safe-rlhf,
      title={Safe RLHF: Safe Reinforcement Learning from Human Feedback}, 
      author={Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
      year={2023},
      eprint={2310.12773},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{ray2019benchmarking,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal={arXiv preprint arXiv:1910.01708},
  volume={7},
  number={1},
  pages={2},
  year={2019}
}

@misc{ji2023omnisafe,
      title={OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research}, 
      author={Jiaming Ji and Jiayi Zhou and Borong Zhang and Juntao Dai and Xuehai Pan and Ruiyang Sun and Weidong Huang and Yiran Geng and Mickel Liu and Yaodong Yang},
      year={2023},
      eprint={2305.09304},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{altman2021constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  year={2021},
  publisher={Routledge}
}

@misc{ji2023safetygymnasium,
      title={Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark}, 
      author={Jiaming Ji and Borong Zhang and Jiayi Zhou and Xuehai Pan and Weidong Huang and Ruiyang Sun and Yiran Geng and Yifan Zhong and Juntao Dai and Yaodong Yang},
      year={2023},
      eprint={2310.12567},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{sun2023safety,
      title={Safety Assessment of Chinese Large Language Models}, 
      author={Hao Sun and Zhexin Zhang and Jiawen Deng and Jiale Cheng and Minlie Huang},
      journal={arXiv preprint arXiv:2304.10436},
      year={2023}
}

@MISC{noauthor_undated-nz,
  title        = "China: hourly minimum wage by region 2023",
  booktitle    = "Statista",
  abstract     = "In 2023, the minimum hourly wage in Beijing was the highest
                  in China at 25.3 yuan per hour.",
  howpublished = "\url{https://www.statista.com/statistics/233886/minimum-wage-per-hour-in-china-by-city-and-province/}",
  note         = "Accessed: 2023-6-7",
  language     = "en"
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and Yang, Fan and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@inproceedings{ci2022proactive,
  title={Proactive Multi-Camera Collaboration for 3D Human Pose Estimation},
  author={Ci, Hai and Liu, Mickel and Pan, Xuehai and Wang, Yizhou and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{pan2022mate,
  title={Mate: Benchmarking multi-agent reinforcement learning in distributed target coverage control},
  author={Pan, Xuehai and Liu, Mickel and Zhong, Fangwei and Yang, Yaodong and Zhu, Song-Chun and Wang, Yizhou},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27862--27879},
  year={2022}
}


@ARTICLE{Tallamraju2020-uf,
  title    = "{AirCapRL}: Autonomous Aerial Human Motion Capture Using Deep
              Reinforcement Learning",
  author   = "Tallamraju, Rahul and Saini, Nitin and Bonetto, Elia and Pabst,
              Michael and Liu, Yu Tang and Black, Michael J and Ahmad, Aamir",
  abstract = "In this letter, we introduce a deep reinforcement learning (DRL)
              based multi-robot formation controller for the task of autonomous
              aerial human motion capture (MoCap). We focus on vision-based
              MoCap, where the objective is to estimate the trajectory of body
              pose, and shape of a single moving person using multiple micro
              aerial vehicles. State-of-the-art solutions to this problem are
              based on classical control methods, which depend on hand-crafted
              system, and observation models. Such models are difficult to
              derive, and generalize across different systems. Moreover, the
              non-linearities, and non-convexities of these models lead to
              sub-optimal controls. In our work, we formulate this problem as a
              sequential decision making task to achieve the vision-based
              motion capture objectives, and solve it using a deep neural
              network-based RL method. We leverage proximal policy optimization
              (PPO) to train a stochastic decentralized control policy for
              formation control. The neural network is trained in a
              parallelized setup in synthetic environments. We performed
              extensive simulation experiments to validate our approach.
              Finally, real-robot experiments demonstrate that our policies
              generalize to real world conditions.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  5,
  number   =  4,
  pages    = "6678--6685",
  month    =  oct,
  year     =  2020,
  keywords = "Cameras;Three-dimensional displays;Shape;Trajectory;Unmanned
              aerial vehicles;Robot vision systems;Computational
              modeling;Reinforecment learning;aerial systems: perception and
              autonomy;multi-robot systems;visual tracking"
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={OpenAI and Ilge Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and Nikolas Tezak and Jerry Tworek and Peter Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei Zhang},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@article{miki2022learning,
  title={Learning robust perceptive locomotion for quadrupedal robots in the wild},
  author={Miki, Takahiro and Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
  journal={Science Robotics},
  volume={7},
  number={62},
  pages={eabk2822},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}