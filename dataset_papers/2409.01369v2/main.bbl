\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2024)Agarwal, Vieillard, Zhou, Stanczyk, Garea, Geist,
  and Bachem]{agarwal2024policy}
Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela~Ramos
  Garea, Matthieu Geist, and Olivier Bachem.
\newblock On-policy distillation of language models: Learning from
  self-generated mistakes.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Al-Hafez et~al.(2023)Al-Hafez, Tateo, Arenz, Zhao, and
  Peters]{al2023ls}
Firas Al-Hafez, Davide Tateo, Oleg Arenz, Guoping Zhao, and Jan Peters.
\newblock Ls-iq: Implicit reward regularization for inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2303.00599}, 2023.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, Chu, Clark, Shafey, Huang, Meier-Hellstern, Mishra,
  Moreira, Omernick, Robinson, Ruder, Tay, Xiao, Xu, Zhang, Abrego, Ahn,
  Austin, Barham, Botha, Bradbury, Brahma, Brooks, Catasta, Cheng, Cherry,
  Choquette-Choo, Chowdhery, Crepy, Dave, Dehghani, Dev, Devlin, Díaz, Du,
  Dyer, Feinberg, Feng, Fienber, Freitag, Garcia, Gehrmann, Gonzalez, Gur-Ari,
  Hand, Hashemi, Hou, Howland, Hu, Hui, Hurwitz, Isard, Ittycheriah, Jagielski,
  Jia, Kenealy, Krikun, Kudugunta, Lan, Lee, Lee, Li, Li, Li, Li, Li, Lim, Lin,
  Liu, Liu, Maggioni, Mahendru, Maynez, Misra, Moussalem, Nado, Nham, Ni,
  Nystrom, Parrish, Pellat, Polacek, Polozov, Pope, Qiao, Reif, Richter, Riley,
  Ros, Roy, Saeta, Samuel, Shelby, Slone, Smilkov, So, Sohn, Tokumine, Valter,
  Vasudevan, Vodrahalli, Wang, Wang, Wang, Wang, Wieting, Wu, Xu, Xu, Xue, Yin,
  Yu, Zhang, Zheng, Zheng, Zhou, Zhou, Petrov, and Wu]{anil2023palm}
Rohan Anil, Andrew~M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
  Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
  Eric Chu, Jonathan~H. Clark, Laurent~El Shafey, Yanping Huang, Kathy
  Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,
  Sebastian Ruder, Yi~Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
  Gustavo~Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,
  James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng,
  Colin Cherry, Christopher~A. Choquette-Choo, Aakanksha Chowdhery, Clément
  Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
  Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus
  Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
  Steven Hand, Hadi Hashemi, Le~Hou, Joshua Howland, Andrea Hu, Jeffrey Hui,
  Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
  Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine
  Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek
  Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma
  Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
  Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,
  Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker
  Riley, Alex~Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee
  Shelby, Ambrose Slone, Daniel Smilkov, David~R. So, Daniel Sohn, Simon
  Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
  Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan
  Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
  Ce~Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.
\newblock {PaLM 2} technical report, 2023.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Brenna~D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Arora et~al.(2022)Arora, Asri, Bahuleyan, and Cheung]{Arora2022WhyEB}
Kushal Arora, Layla~El Asri, Hareesh Bahuleyan, and Jackie Chi~Kit Cheung.
\newblock Why exposure bias matters: An imitation learning perspective of error
  accumulation in language generation.
\newblock In \emph{Findings}, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:247939224}.

\bibitem[Bachmann and Nagarajan(2024)]{Bachmann2024ThePO}
Gregor Bachmann and Vaishnavh Nagarajan.
\newblock The pitfalls of next-token prediction.
\newblock \emph{ArXiv}, abs/2403.06963, 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:268364153}.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Bain and Sammut(1995)]{bain1995framework}
Michael Bain and Claude Sammut.
\newblock A framework for behavioural cloning.
\newblock In \emph{Machine Intelligence 15}, pages 103--129, 1995.

\bibitem[Barnes et~al.(2024)Barnes, Abueg, Lange, Deeds, Trader, Molitor,
  Wulfmeier, and O'Banion]{barnes2024massively}
Matt Barnes, Matthew Abueg, Oliver~F. Lange, Matt Deeds, Jason Trader, Denali
  Molitor, Markus Wulfmeier, and Shawn O'Banion.
\newblock Massively scalable inverse reinforcement learning in google maps.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=z3L59iGALM}.

\bibitem[Biderman et~al.(2024)Biderman, Ortiz, Portes, Paul, Greengard,
  Jennings, King, Havens, Chiley, Frankle, Blakeney, and
  Cunningham]{biderman2024lora}
Dan Biderman, Jose~Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip
  Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan
  Frankle, Cody Blakeney, and John~P. Cunningham.
\newblock Lora learns less and forgets less, 2024.

\bibitem[Caccia et~al.(2018)Caccia, Caccia, Fedus, Larochelle, Pineau, and
  Charlin]{caccia2018language}
Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau,
  and Laurent Charlin.
\newblock Language gans falling short.
\newblock \emph{arXiv preprint arXiv:1811.02549}, 2018.

\bibitem[Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando,
  Freedman, Korbak, Lindner, Freire, Wang, Marks, S{\'e}gerie, Carroll, Peng,
  Christoffersen, Damani, Slocum, Anwar, Siththaranjan, Nadeau, Michaud, Pfau,
  Krasheninnikov, Chen, di~Langosco, Hase, Biyik, Dragan, Krueger, Sadigh, and
  Hadfield-Menell]{Casper2023OpenPA}
Stephen Casper, Xander Davies, Claudia Shi, Thomas~Krendl Gilbert, J'er'emy
  Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro
  Freire, Tony Wang, Samuel Marks, Charbel-Rapha{\"e}l S{\'e}gerie, Micah
  Carroll, Andi Peng, Phillip J.~K. Christoffersen, Mehul Damani, Stewart
  Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric~J. Michaud, Jacob
  Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro~Langosco di~Langosco, Peter
  Hase, Erdem Biyik, Anca~D. Dragan, David Krueger, Dorsa Sadigh, and Dylan
  Hadfield-Menell.
\newblock Open problems and fundamental limitations of reinforcement learning
  from human feedback.
\newblock \emph{ArXiv}, abs/2307.15217, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:260316010}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul Christiano, Jan Leike, Tom~B Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{arXiv preprint arXiv:1706.03741}, 2017.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cundy and Ermon(2024)]{cundy2024sequencematch}
Chris Cundy and Stefano Ermon.
\newblock Sequencematch: Imitation learning for autoregressive sequence
  modelling with backtracking, 2024.

\bibitem[Dawid and LeCun(2023)]{Dawid2023IntroductionTL}
Anna Dawid and Yann LeCun.
\newblock Introduction to latent variable energy-based models: A path towards
  autonomous machine intelligence.
\newblock \emph{ArXiv}, abs/2306.02572, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:259075148}.

\bibitem[et~al.(2024)]{geminiteam2024gemini}
Gemini~Team et~al.
\newblock Gemini: A family of highly capable multimodal models, 2024.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{Finn2016GuidedCL}
Chelsea Finn, Sergey Levine, and P.~Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock \emph{ArXiv}, abs/1603.00448, 2016.

\bibitem[Fu et~al.(2017)Fu, Luo, and Levine]{fu2017learning}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1710.11248}, 2017.

\bibitem[Garg et~al.(2022{\natexlab{a}})Garg, Chakraborty, Cundy, Song, Geist,
  and Ermon]{garg2022iqlearn}
Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, Matthieu Geist,
  and Stefano Ermon.
\newblock Iq-learn: Inverse soft-q learning for imitation, 2022{\natexlab{a}}.

\bibitem[Garg et~al.(2022{\natexlab{b}})Garg, Tsipras, Liang, and
  Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30583--30598, 2022{\natexlab{b}}.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin.
\newblock A theory of regularized markov decision processes.
\newblock \emph{CoRR}, abs/1901.11275, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.11275}.

\bibitem[Ghasemipour et~al.(2019)Ghasemipour, Zemel, and
  Gu]{ghasemipour2019divergence}
Seyed Kamyar~Seyed Ghasemipour, Richard~S. Zemel, and Shixiang Gu.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock \emph{CoRR}, abs/1911.02256, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.02256}.

\bibitem[Hejna and Sadigh(2023)]{Hejna2023InversePL}
Joey Hejna and Dorsa Sadigh.
\newblock Inverse preference learning: Preference-based rl without a reward
  function.
\newblock \emph{ArXiv}, abs/2305.15363, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:258865730}.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4565--4573, 2016.

\bibitem[Hormann and Sokolov(2021)]{hormann2021fixing}
Luca Hormann and Artem Sokolov.
\newblock Fixing exposure bias with imitation learning needs powerful oracles.
\newblock \emph{arXiv preprint arXiv:2109.04114}, 2021.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.

\bibitem[Hubert et~al.(2023)Hubert, Sokolov, and Riezler]{hubert2023improving}
Rebekka Hubert, Artem Sokolov, and Stefan Riezler.
\newblock Improving end-to-end speech translation by imitation-based knowledge
  distillation with synthetic transcripts.
\newblock In \emph{Int. Workshop on Spoken Language Translation}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.08426}.

\bibitem[Hussein et~al.(2017)Hussein, Gaber, Elyan, and Jayne]{hussein2018}
Ahmed Hussein, Mohamed~Medhat Gaber, Eyad Elyan, and Chrisina Jayne.
\newblock Imitation learning: A survey of learning methods.
\newblock \emph{ACM Comput. Surv.}, 50\penalty0 (2), apr 2017.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/3054912}.
\newblock URL \url{https://doi.org/10.1145/3054912}.

\bibitem[Ji et~al.(2023)Ji, Lee, Frieske, Yu, Su, Xu, Ishii, Bang, Madotto, and
  Fung]{hallucinate2023}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
  Ye~Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Comput. Surv.}, 55\penalty0 (12), mar 2023.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/3571730}.
\newblock URL \url{https://doi.org/10.1145/3571730}.

\bibitem[Kaufmann et~al.(2023)Kaufmann, Weng, Bengs, and
  H{\"u}llermeier]{kaufmann2023survey}
Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H{\"u}llermeier.
\newblock A {{Survey}} of {{Reinforcement Learning}} from {{Human Feedback}},
  2023.

\bibitem[Ke et~al.(2019)Ke, Huang, Huang, and Zhu]{ke2019araml}
Pei Ke, Fei Huang, Minlie Huang, and Xiaoyan Zhu.
\newblock Araml: A stable adversarial training framework for text generation.
\newblock \emph{arXiv preprint arXiv:1908.07195}, 2019.

\bibitem[Kim et~al.(2022)Kim, Seo, Lee, Jeon, Hwang, Yang, and
  Kim]{kim2022demodice}
Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang,
  Hongseok Yang, and Kee-Eung Kim.
\newblock Demodice: Offline imitation learning with supplementary imperfect
  demonstrations.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kocmi et~al.(2022)Kocmi, Bawden, Bojar, Dvorkovich, Federmann, Fishel,
  Gowda, Graham, Grundkiewicz, Haddow, Knowles, Koehn, Monz, Morishita, Nagata,
  Nakazawa, Nov{\'a}k, Popel, and Popovi{\'c}]{wmt22}
Tom Kocmi, Rachel Bawden, Ond{\v{r}}ej Bojar, Anton Dvorkovich, Christian
  Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz,
  Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto
  Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Nov{\'a}k, Martin Popel,
  and Maja Popovi{\'c}.
\newblock {Findings of the 2022 Conference on Machine Translation ({WMT}22)}.
\newblock In \emph{Proceedings of the Seventh Conference on Machine Translation
  (WMT)}, 2022.
\newblock URL \url{https://aclanthology.org/2022.wmt-1.1}.

\bibitem[Kostrikov et~al.(2019{\natexlab{a}})Kostrikov, Agrawal, Dwibedi,
  Levine, and Tompson]{kostrikov2019dac}
Ilya Kostrikov, Kumar~Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and
  Jonathan Tompson.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Kostrikov et~al.(2019{\natexlab{b}})Kostrikov, Nachum, and
  Tompson]{kostrikov2019imitation}
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson.
\newblock Imitation learning via off-policy distribution matching.
\newblock \emph{arXiv preprint arXiv:1912.05032}, 2019{\natexlab{b}}.

\bibitem[Lee et~al.(2023)Lee, Phatale, Mansoor, Mesnard, Ferret, Lu, Bishop,
  Hall, Carbune, Rastogi, and Prakash]{lee2023rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret,
  Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and
  Sushant Prakash.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai
  feedback.
\newblock \emph{arXiv}, abs/2309.00267, 2023.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Zeng, Wai, Li, Garc{\'i}a, and
  Hong]{Li2024GettingMJ}
Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garc{\'i}a, and
  Mingyi Hong.
\newblock Getting more juice out of the sft data: Reward learning from human
  demonstration improves sft for llm alignment.
\newblock \emph{ArXiv}, abs/2405.17888, 2024{\natexlab{a}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:270067914}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Chen, Xu, Qin, Xiao, Sun, and
  Luo]{Li2024EntropicDM}
Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, and
  Zhimin Luo.
\newblock Entropic distribution matching in supervised fine-tuning of llms:
  Less overfitting and better diversity, 2024{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Wohlwend, Chen, and Lei]{lin2020autoregressive}
Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei.
\newblock Autoregressive knowledge distillation through imitation learning.
\newblock \emph{arXiv preprint arXiv:2009.07253}, 2020.

\bibitem[Ma et~al.(2022)Ma, Shen, Jayaraman, and Bastani]{ma2022versatile}
Yecheng Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani.
\newblock Versatile offline imitation from observations and examples via
  regularized state-occupancy matching.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2022.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{Narayan2018DontGM}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don't give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock \emph{ArXiv}, abs/1808.08745, 2018.

\bibitem[Ng and Russell(2000)]{ng2000inverse}
Andrew~Y. Ng and Stuart~J. Russell.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, ICML '00, page 663–670, San Francisco, CA, USA, 2000.
  Morgan Kaufmann Publishers Inc.
\newblock ISBN 1558607072.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Pillutla et~al.(2022)Pillutla, Liu, Thickstun, Welleck, Swayamdipta,
  Zellers, Oh, Choi, and Harchaoui]{Pillutla2022MAUVESF}
Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta,
  Rowan Zellers, Sewoong Oh, Yejin Choi, and Za{\"i}d Harchaoui.
\newblock Mauve scores for generative models: Theory and practice.
\newblock \emph{ArXiv}, abs/2212.14578, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:255341265}.

\bibitem[Piot et~al.(2014)Piot, Geist, and Pietquin]{piot2014}
Bilal Piot, Matthieu Geist, and Olivier Pietquin.
\newblock Boosted and reward-regularized classification for apprenticeship
  learning.
\newblock In \emph{Proceedings of the 2014 International Conference on
  Autonomous Agents and Multi-Agent Systems}, AAMAS '14, page 1249–1256,
  Richland, SC, 2014. International Foundation for Autonomous Agents and
  Multiagent Systems.
\newblock ISBN 9781450327381.

\bibitem[Pomerleau(1988)]{pomerleau1988alvinn}
Dean~A Pomerleau.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock \emph{Advances in neural information processing systems}, 1, 1988.

\bibitem[Rafailov et~al.(2024)Rafailov, Hejna, Park, and
  Finn]{Rafailov2024FromT}
Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn.
\newblock From r to q*: Your language model is secretly a q-function, 2024.

\bibitem[Raffel et~al.(2023)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2023exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer, 2023.

\bibitem[Ranzato et~al.(2016)Ranzato, Chopra, Auli, and
  Zaremba]{ranzato2016sequence}
Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba.
\newblock Sequence level training with recurrent neural networks, 2016.

\bibitem[Razin et~al.(2023)Razin, Zhou, Saremi, Thilak, Bradley, Nakkiran,
  Susskind, and Littwin]{razin2023vanishing}
Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum
  Nakkiran, Joshua Susskind, and Etai Littwin.
\newblock Vanishing gradients in reinforcement finetuning of language models.
\newblock \emph{arXiv preprint arXiv:2310.20703}, 2023.

\bibitem[Reddy et~al.(2019)Reddy, Dragan, and Levine]{reddy2019sqil}
Siddharth Reddy, Anca~D. Dragan, and Sergey Levine.
\newblock Sqil: Imitation learning via reinforcement learning with sparse
  rewards, 2019.

\bibitem[Ren et~al.(2024)Ren, Swamy, Wu, Bagnell, and
  Choudhury]{Ren2024HybridIR}
Juntao Ren, Gokul Swamy, Zhiwei~Steven Wu, J.~Andrew Bagnell, and Sanjiban
  Choudhury.
\newblock Hybrid inverse reinforcement learning.
\newblock \emph{ArXiv}, abs/2402.08848, 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:267658009}.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Sikchi et~al.(2023)Sikchi, Zheng, Zhang, and Niekum]{Sikchi2023DualRU}
Harshit Sikchi, Qinqing Zheng, Amy Zhang, and Scott Niekum.
\newblock Dual rl: Unification and new methods for reinforcement and imitation
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Sun(2024)]{sun2024supervised}
Hao Sun.
\newblock Supervised fine-tuning as inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2403.12017}, 2024.

\bibitem[Syed and Schapire(2007)]{Syed2007}
Umar Syed and Robert~E. Schapire.
\newblock A game-theoretic approach to apprenticeship learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Viano et~al.(2022)Viano, Kamoutsi, Neu, Krawczuk, and
  Cevher]{viano2022proximal}
Luca Viano, Angeliki Kamoutsi, Gergely Neu, Igor Krawczuk, and Volkan Cevher.
\newblock Proximal point imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Watson et~al.(2023)Watson, Huang, and Heess]{watson2023coherent}
Joe Watson, Sandy~H. Huang, and Nicolas Heess.
\newblock Coherent soft imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2023.

\bibitem[Williams and Zipser(1989)]{Williams1989ALA}
Ronald~J. Williams and David Zipser.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural Computation}, 1:\penalty0 270--280, 1989.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:14711886}.

\bibitem[Wu et~al.(2021)Wu, Li, and Yu]{wu2021textgail}
Qingyang Wu, Lei Li, and Zhou Yu.
\newblock Textgail: Generative adversarial imitation learning for text
  generation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, 2021.

\bibitem[Wulfmeier et~al.(2016)Wulfmeier, Wang, and
  Posner]{Wulfmeier2016WatchTS}
Markus Wulfmeier, Dominic~Zeng Wang, and Ingmar Posner.
\newblock Watch this: Scalable cost-function learning for path planning in
  urban environments.
\newblock \emph{2016 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 2089--2095, 2016.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:206944802}.

\bibitem[Wulfmeier et~al.(2017)Wulfmeier, Posner, and
  Abbeel]{Wulfmeier2017MutualAT}
Markus Wulfmeier, Ingmar Posner, and P.~Abbeel.
\newblock Mutual alignment transfer learning.
\newblock \emph{ArXiv}, abs/1707.07907, 2017.

\bibitem[Wulfmeier et~al.(2023)Wulfmeier, Byravan, Bechtle, Hausman, and
  Heess]{Wulfmeier2023FoundationsFT}
Markus Wulfmeier, Arunkumar Byravan, Sarah Bechtle, Karol Hausman, and Nicolas
  Manfred~Otto Heess.
\newblock Foundations for transfer in reinforcement learning: A taxonomy of
  knowledge modalities.
\newblock \emph{ArXiv}, abs/2312.01939, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:265609417}.

\bibitem[Xing et~al.(2021)Xing, Song, and Cheng]{NEURIPS2021_df1f1d20}
Yue Xing, Qifan Song, and Guang Cheng.
\newblock On the algorithmic stability of adversarial training.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 26523--26535. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf}.

\bibitem[Xu et~al.(2020)Xu, Li, and Yu]{xu2020error}
Tian Xu, Ziniu Li, and Yang Yu.
\newblock Error bounds of imitating policies and environments.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yu et~al.(2017)Yu, Zhang, Wang, and Yu]{yu2017seqgan}
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Seqgan: Sequence generative adversarial nets with policy gradient.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~31, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and
  Artzi]{Zhang2019BERTScoreET}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q. Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{ArXiv}, abs/1904.09675, 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:127986044}.

\bibitem[Zhu et~al.(2018)Zhu, Lu, Zheng, Guo, Zhang, Wang, and
  Yu]{zhu2018texygen}
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong
  Yu.
\newblock Texygen: A benchmarking platform for text generation models.
\newblock In \emph{The 41st international ACM SIGIR conference on research \&
  development in information retrieval}, pages 1097--1100, 2018.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Brian~D Ziebart.
\newblock \emph{Modeling Purposeful Adaptive Behavior with the Principle of
  Maximum Causal Entropy}.
\newblock PhD thesis, Carnegie Mellon University, 2010.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
