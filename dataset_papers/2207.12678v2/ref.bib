@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{wang2021large,
  title={Large learning rate tames homogeneity: Convergence and balancing effect},
  author={Wang, Yuqing and Chen, Minshuo and Zhao, Tuo and Tao, Molei},
  journal={arXiv preprint arXiv:2110.03677},
  year={2021}
}

@article{papyan2019measurements,
  title={Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1901.08244},
  year={2019}
}

@book{martens2016second,
  title={Second-order optimization for neural networks},
  author={Martens, James},
  year={2016},
  publisher={University of Toronto (Canada)}
}


@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{karakida2019pathological,
  title={Pathological spectra of the fisher information metric and its variants in deep neural networks},
  author={Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
  journal={arXiv preprint arXiv:1910.05992},
  year={2019}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@article{elkabetz2021continuous,
  title={Continuous vs. discrete optimization of deep neural networks},
  author={Elkabetz, Omer and Cohen, Nadav},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{arora2018convergence,
  title={A convergence analysis of gradient descent for deep linear neural networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  journal={arXiv preprint arXiv:1810.02281},
  year={2018}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{sagun2016eigenvalues,
  title={Eigenvalues of the hessian in deep learning: Singularity and beyond},
  author={Sagun, Levent and Bottou, Leon and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2016}
}

@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@article{papyan2018full,
  title={The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}

@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{jastrzebski2020break,
  title={The break-even point on optimization trajectories of deep neural networks},
  author={Jastrzebski, Stanislaw and Szymczak, Maciej and Fort, Stanislav and Arpit, Devansh and Tabor, Jacek and Cho, Kyunghyun and Geras, Krzysztof},
  journal={arXiv preprint arXiv:2002.09572},
  year={2020}
}

@article{gilmer2021loss,
  title={A Loss Curvature Perspective on Training Instability in Deep Learning},
  author={Gilmer, Justin and Ghorbani, Behrooz and Garg, Ankush and Kudugunta, Sneha and Neyshabur, Behnam and Cardoze, David and Dahl, George and Nado, Zachary and Firat, Orhan},
  journal={arXiv preprint arXiv:2110.04369},
  year={2021}
}

@article{ma2022multiscale,
  title={The Multiscale Structure of Neural Network Loss Functions: The Effect on Optimization and Origin},
  author={Ma, Chao and Wu, Lei and Ying, Lexing},
  journal={arXiv preprint arXiv:2204.11326},
  year={2022}
}

@article{karakida2019normalization,
  title={The normalization method for alleviating pathological sharpness in wide neural networks},
  author={Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{ahn2022understanding,
  title={Understanding the unstable convergence of gradient descent},
  author={Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit},
  journal={arXiv preprint arXiv:2204.01050},
  year={2022}
}

@article{li2021happens,
  title={What Happens after SGD Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2110.06914},
  year={2021}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@inproceedings{ghorbani2019investigation,
  title={An investigation into neural net optimization via hessian eigenvalue density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  booktitle={International Conference on Machine Learning},
  pages={2232--2241},
  year={2019},
  organization={PMLR}
}

@article{wu2020dissecting,
  title={Dissecting hessian: Understanding common structure of hessian in neural networks},
  author={Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie and Ge, Rong},
  journal={arXiv preprint arXiv:2010.04261},
  year={2020}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{xing2018walk,
  title={A walk with sgd},
  author={Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1802.08770},
  year={2018}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{mulayoff2021implicit,
  title={The Implicit Bias of Minima Stability: A View from Function Space},
  author={Mulayoff, Rotem and Michaeli, Tomer and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{golmantpytorchhessian,
  title={pytorchhessian-eigentings: efficient pytorch hessian eigendecomposition, October 2018},
  author={Golmant, Noah and Yao, Zhewei and Gholami, Amir and Mahoney, Michael and Gonzalez, Joseph},
  journal={URL https://github. com/noahgolmant/pytorch-hessian-eigenthings}
}

@article{hu2020surprising,
  title={The surprising simplicity of the early-time learning dynamics of neural networks},
  author={Hu, Wei and Xiao, Lechao and Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17116--17128},
  year={2020}
}

@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{arora2022understanding,
  title={Understanding Gradient Descent on Edge of Stability in Deep Learning},
  author={Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  journal={arXiv preprint arXiv:2205.09745},
  year={2022}
}

@article{lyu2022understanding,
  title={Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction},
  author={Lyu, Kaifeng and Li, Zhiyuan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2206.07085},
  year={2022}
}