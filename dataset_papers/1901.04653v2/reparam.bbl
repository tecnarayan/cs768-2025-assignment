\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille \& Soatto(2018)Achille and Soatto]{EmergenceOfInvariance}
Achille, A. and Soatto, S.
\newblock {Emergence of Invariance and Disentanglement in Deep
  Representations}.
\newblock \emph{Journal of Machine Learning Research}, 19, 2018.

\bibitem[Alquier et~al.(2016)Alquier, Ridgway, and Chopin]{PropertyVarApp}
Alquier, P., Ridgway, J., and Chopin, N.
\newblock {On the properties of variational approximations of Gibbs
  posteriors}.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (239):\penalty0 1--41, 2016.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{CompressionBound}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock {Stronger Generalization Bounds for Deep Nets via a Compression
  Approach}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\
   254--263. PMLR, 10--15 Jul 2018.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{SpectralMargin}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock {Spectrally-normalized margin bounds for neural networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  6240--6249. Curran Associates, Inc., 2017.

\bibitem[Blier \& Ollivier(2018)Blier and Ollivier]{MDLofDeep}
Blier, L. and Ollivier, Y.
\newblock {The Description Length of Deep Learning Models LÃ©onard}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  2220--2230. Curran Associates, Inc., 2018.

\bibitem[Catoni(2007)]{PACBayesSupervisedClassification}
Catoni, O.
\newblock \emph{{Pac-Bayesian Supervised Classification: The Thermodynamics of
  Statistical Learning}}.
\newblock {Institute of Mathematical Statistics}, 2007.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{EntropySGD}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and
  Bengio]{SharpMinimaGeneralize}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock {Sharp Minima Can Generalize For Deep Nets}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\
   1019--1028. PMLR, 06--11 Aug 2017.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{NoBarriers}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\
   1309--1318. PMLR, 10--15 Jul 2018.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{Nonvacuous}
Dziugaite, G.~K. and Roy, D.~M.
\newblock {Computing Nonvacuous Generalization Bounds for Deep (Stochastic)
  Neural Networks with Many More Parameters than Training Data}.
\newblock In \emph{Proceedings of the Thirty-Third Conference on Uncertainty in
  Artificial Intelligence}, 2017.

\bibitem[Germain et~al.(2016)Germain, Bach, Lacoste, and
  Lacoste-Julien]{PACBayesInference}
Germain, P., Bach, F., Lacoste, A., and Lacoste-Julien, S.
\newblock {PAC-Bayesian Theory Meets Bayesian Inference}.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pp.\
  1884--1892. Curran Associates, Inc., 2016.

\bibitem[Hinton \& van Camp(1993)Hinton and van Camp]{KeepNNSimple}
Hinton, G.~E. and van Camp, D.
\newblock {Keeping the Neural Networks Simple by Minimizing the Description
  Length of the Weights}.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, COLT '93, pp.\  5--13. ACM, 1993.
\newblock ISBN 0-89791-611-5.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{FlatMinima}
Hochreiter, S. and Schmidhuber, J.
\newblock {Flat Minima}.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{TrainLonger}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock {Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  1731--1741. Curran Associates, Inc., 2017.

\bibitem[Honkela \& Valpola(2004)Honkela and Valpola]{BitsBack}
Honkela, A. and Valpola, H.
\newblock {Variational Learning and Bits-back Coding: An Information-theoretic
  View to Bayesian Learning}.
\newblock \emph{Trans. Neur. Netw.}, 15\penalty0 (4):\penalty0 800--810, July
  2004.
\newblock ISSN 1045-9227.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{BatchNorm}
Ioffe, S. and Szegedy, C.
\newblock {Batch Normalization: Accelerating Deep Network Training by Reducing
  Internal Covariate Shift}.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, volume~37 of \emph{Proceedings of Machine Learning Research}, pp.\
   448--456. PMLR, 07--09 Jul 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{AverageWeights}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D.~P., and Wilson, A.~G.
\newblock {Averaging Weights Leads to Wider Optima and Better Generalization}.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2018.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{LargeBatchTraining}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock {On Large-Batch Training for Deep Learning: Generalization Gap and
  Sharp Minima}.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{VariationalDropout}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock {Variational Dropout and the Local Reparameterization Trick}.
\newblock In \emph{Advances in Neural Information Processing Systems 28}, pp.\
  2575--2583. Curran Associates, Inc., 2015.

\bibitem[Krizhevsky(2009)]{CIFAR}
Krizhevsky, A.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock 2009.

\bibitem[Langford \& Caruana(2002)Langford and Caruana]{NotBounding}
Langford, J. and Caruana, R.
\newblock {(Not) Bounding the True Error}.
\newblock In \emph{Advances in Neural Information Processing Systems 14}, pp.\
  809--816. MIT Press, 2002.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{LeNet}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock {{Gradient-based Learning Applied to Document Recognition}}.
\newblock In \emph{Proceedings of the IEEE}, pp.\  2278--2324, 1998.

\bibitem[LeCun et~al.(1998)LeCun, Cortes, and Burges]{MNIST}
LeCun, Y., Cortes, C., and Burges, C. J.~C.
\newblock {The MNIST Database of Handwritten Digits}.
\newblock 1998.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, and Goldstein]{VisualizeLoss}
Li, H., Xu, Z., Taylor, G., and Goldstein, T.
\newblock {Visualizing the Loss Landscape of Neural Nets}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  6391--6401. Curran Associates, Inc., 2018.

\bibitem[Liang et~al.(2017)Liang, Poggio, Rakhlin, and Stokes]{FisherRaoNorm}
Liang, T., Poggio, T.~A., Rakhlin, A., and Stokes, J.
\newblock {Fisher-Rao Metric, Geometry, and Complexity of Neural Networks}.
\newblock \emph{CoRR}, abs/1711.01530, 2017.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{AdamW}
Loshchilov, I. and Hutter, F.
\newblock {Decoupled Weight Decay Regularization}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[McAllester(1999)]{SomePACTheorems}
McAllester, D.~A.
\newblock {Some PAC-Bayesian Theorems}.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363, Dec
  1999.
\newblock ISSN 1573-0565.

\bibitem[McAllester(2003)]{PACBayesStochasticModelSel}
McAllester, D.~A.
\newblock {PAC-Bayesian Stochastic Model Selection}.
\newblock \emph{Machine Learning}, 51\penalty0 (1):\penalty0 5--21, Apr 2003.
\newblock ISSN 1573-0565.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, Mcallester, and
  Srebro]{ExploreGeneralization}
Neyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N.
\newblock {Exploring Generalization in Deep Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  5947--5956. Curran Associates, Inc., 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{PACBayesianSpectralMargin}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock {A {PAC}-Bayesian Approach to Spectrally-Normalized Margin Bounds for
  Neural Networks}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Neyshabur et~al.(2019)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{OverParam}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock {The role of over-parametrization in generalization of neural
  networks}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Rissanen(1986)]{MDL}
Rissanen, J.
\newblock Stochastic complexity and modeling.
\newblock \emph{Ann. Statist.}, 14\penalty0 (3):\penalty0 1080--1100, 09 1986.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{WeightNorm}
Salimans, T. and Kingma, D.~P.
\newblock {Weight Normalization: A Simple Reparameterization to Accelerate
  Training of Deep Neural Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pp.\
  901--909. Curran Associates, Inc., 2016.

\bibitem[Sun et~al.(2019)Sun, Zhang, Shi, and Grosse]{FunctionalBBN}
Sun, S., Zhang, G., Shi, J., and Grosse, R.
\newblock {Functional Variational Bayesian Neural Networks}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{Intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock {Intriguing properties of neural networks}.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Tishby et~al.(1999)Tishby, Pereira, and Bialek]{InformationBottleneck}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock In \emph{Proceedings of the 37-th Annual Allerton Conference on
  Communication, Control and Computing}, pp.\  368--377, 1999.

\bibitem[Wang et~al.(2018)Wang, Shirish~Keskar, Xiong, and Socher]{IdentifyGen}
Wang, H., Shirish~Keskar, N., Xiong, C., and Socher, R.
\newblock {Identifying Generalization Properties in Neural Networks}.
\newblock \emph{ArXiv e-prints}, 2018.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'{a}}r, Tu, and He]{ResNext}
Xie, S., Girshick, R.~B., Doll{\'{a}}r, P., Tu, Z., and He, K.
\newblock {Aggregated Residual Transformations for Deep Neural Networks}.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pp.\  5987--5995, 2017.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and
  Mahoney]{HessianAnalysis}
Yao, Z., Gholami, A., Lei, Q., Keutzer, K., and Mahoney, M.~W.
\newblock {Hessian-based Analysis of Large Batch Training and Robustness to
  Adversaries}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  4954--4964. Curran Associates, Inc., 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{WideResNet}
Zagoruyko, S. and Komodakis, N.
\newblock {Wide Residual Networks}.
\newblock In \emph{Proceedings of the British Machine Vision Conference}, pp.\
  87.1--87.12, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Xu, and
  Grosse]{ThreeMechanismWeightDecay}
Zhang, G., Wang, C., Xu, B., and Grosse, R.
\newblock {Three Mechanisms of Weight Decay Regularization}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and
  Orbanz]{NonvacuousImageNet}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock {Non-vacuous Generalization Bounds at the ImageNet Scale: a
  {PAC}-Bayesian Compression Approach}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{NASNet}
Zoph, B., Vasudevan, V., Shlens, J., and Le, Q.~V.
\newblock {Learning Transferable Architectures for Scalable Image Recognition}.
\newblock In \emph{2018 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pp.\  8697--8710, 2018.

\end{thebibliography}
