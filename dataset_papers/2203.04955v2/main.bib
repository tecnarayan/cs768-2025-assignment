@article{Lillicrap2016ContinuousCW,
	title        = {Continuous control with deep reinforcement learning},
	author       = {T. Lillicrap and J. Hunt and A. Pritzel and N. Heess and T. Erez and Y. Tassa and D. Silver and Daan Wierstra},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1509.02971},
}
@article{Haarnoja2018SoftAA,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and G. Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and P. Abbeel and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.05905}
}
@article{mnih2013playing,
	title        = {Playing atari with deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.5602},
}
@article{Sutton1988TD,
author = {Sutton, Richard},
year = {1988},
month = {08},
pages = {9-44},
title = {Learning to Predict by the Method of Temporal Differences},
volume = {3},
journal = {Machine Learning},
doi = {10.1007/BF00115009}
}
@inproceedings{Abdolmaleki2020ADV,
  title={A Distributional View on Multi-Objective Policy Optimization},
  author={Abbas Abdolmaleki and Sandy H. Huang and Leonard Hasenclever and Michael Neunert and H. Francis Song and Martina Zambelli and Murilo Fernandes Martins and Nicolas Manfred Otto Heess and Raia Hadsell and Martin A. Riedmiller},
  booktitle={ICML},
  year={2020}
}
@article{Chen2021ExploringSS,
  title={Exploring Simple Siamese Representation Learning},
  author={Xinlei Chen and Kaiming He},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15745-15753}
}
@article{yarats2021drqv2,
  title={Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning},
  author={Denis Yarats and Rob Fergus and Alessandro Lazaric and Lerrel Pinto},
  journal={arXiv preprint arXiv:2107.09645},
  year={2021}
}
@book{Sutton2018,
	title        = {Reinforcement Learning: An Introduction},
	author       = {Sutton, Richard S. and Barto, Andrew G.},
	year         = 2018,
	publisher    = {The MIT Press},
	edition      = {Second},
}
@inproceedings{hansen2021deployment,
	title={Self-Supervised Policy Adaptation during Deployment},
	author={Nicklas Hansen and Rishabh Jangir and Yu Sun and Guillem Aleny√† and Pieter Abbeel and Alexei A. Efros and Lerrel Pinto and Xiaolong Wang},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2021},
}
@inproceedings{hansen2021softda,
	title={Generalization in Reinforcement Learning by Soft Data Augmentation},
	author={Nicklas Hansen and Xiaolong Wang},
	booktitle={International Conference on Robotics and Automation (ICRA)},
	year={2021},
}
@inproceedings{Hasselt2016DeepRL,
	title        = {Deep Reinforcement Learning with Double Q-Learning},
	author       = {H. V. Hasselt and A. Guez and D. Silver},
	year         = 2016,
	booktitle    = {Aaai},
}
@article{Fujimoto2018AddressingFA,
	title        = {Addressing Function Approximation Error in Actor-Critic Methods},
	author       = {Scott Fujimoto and H. V. Hoof and D. Meger},
	year         = 2018,
	journal      = {ArXiv},
	volume       = {abs/1802.09477},
}
@article{Hafner2020DreamTC,
  title={Dream to Control: Learning Behaviors by Latent Imagination},
  author={Danijar Hafner and Timothy P. Lillicrap and Jimmy Ba and Mohammad Norouzi},
  journal={ArXiv},
  year={2020},
  volume={abs/1912.01603}
}
@article{Espeholt2018IMPALASD,
  title={IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},
  author={Lasse Espeholt and Hubert Soyer and R{\'e}mi Munos and Karen Simonyan and Volodymyr Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and Shane Legg and Koray Kavukcuoglu},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.01561}
}
@article{Nagabandi2018NeuralND,
  title={Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning},
  author={Anusha Nagabandi and Gregory Kahn and Ronald S. Fearing and Sergey Levine},
  journal={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018},
  pages={7559-7566}
}
@article{Hafez2019CuriousMA,
  title={Curious Meta-Controller: Adaptive Alternation between Model-Based and Model-Free Control in Deep Reinforcement Learning},
  author={Muhammad Burhan Hafez and Cornelius Weber and Matthias Kerzel and Stefan Wermter},
  journal={2019 International Joint Conference on Neural Networks (IJCNN)},
  year={2019},
  pages={1-8}
}
@inproceedings{Margolis2021LearningTJ,
  title={Learning to Jump from Pixels},
  author={G. Margolis and Tao Chen and Kartik Paigwar and Xiang Fu and Donghyun Kim and Sangbae Kim and Pulkit Agrawal},
  booktitle={CoRL},
  year={2021}
}
@article{Pong2018TemporalDM,
  title={Temporal Difference Models: Model-Free Deep RL for Model-Based Control},
  author={Vitchyr H. Pong and Shixiang Shane Gu and Murtaza Dalal and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.09081}
}
@inproceedings{Nguyen2021TemporalPC,
  title={Temporal Predictive Coding For Model-Based Planning In Latent Space},
  author={Tung D. Nguyen and Rui Shu and Tu Pham and Hung Hai Bui and Stefano Ermon},
  booktitle={ICML},
  year={2021}
}
@article{Kalashnikov2021MTOptCM,
  title={MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale},
  author={Dmitry Kalashnikov and Jacob Varley and Yevgen Chebotar and Benjamin Swanson and Rico Jonschkowski and Chelsea Finn and Sergey Levine and Karol Hausman},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08212}
}
@article{Pourchot2019CEMRLCE,
  title={CEM-RL: Combining evolutionary and gradient-based methods for policy search},
  author={Alo{\"i}s Pourchot and Olivier Sigaud},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.01222}
}
@article{Sekar2020PlanningTE,
  title={Planning to Explore via Self-Supervised World Models},
  author={Ramanan Sekar and Oleh Rybkin and Kostas Daniilidis and P. Abbeel and Danijar Hafner and Deepak Pathak},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.05960}
}
@article{Kidambi2020MOReLM,
  title={MOReL : Model-Based Offline Reinforcement Learning},
  author={Rahul Kidambi and Aravind Rajeswaran and Praneeth Netrapalli and Thorsten Joachims},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.05951}
}
@article{shao2020grac,
  title={GRAC: Self-Guided and Self-Regularized Actor-Critic},
  author={Shao, Lin and You, Yifan and Yan, Mengyuan and Sun, Qingyun and Bohg, Jeannette},
  journal={arXiv preprint arXiv:2009.08973},
  year={2020}
 }
 @article{Zhang2018SOLARDS,
  title={SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning},
  author={Marvin Zhang and Sharad Vikram and Laura Smith and P. Abbeel and Matthew J. Johnson and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.09105}
}
 @article{Yu2020MOPOMO,
  title={MOPO: Model-based Offline Policy Optimization},
  author={Tianhe Yu and Garrett Thomas and Lantao Yu and Stefano Ermon and James Y. Zou and Sergey Levine and Chelsea Finn and Tengyu Ma},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.13239}
}
@inproceedings{hafner2019planet,
  title={Learning Latent Dynamics for Planning from Pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International Conference on Machine Learning},
  pages={2555--2565},
  year={2019}
}
@article{Hatch2021TheVO,
  title={The Value of Planning for Infinite-Horizon Model Predictive Control},
  author={Nathan Hatch and Byron Boots},
  journal={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2021},
  pages={7372-7378}
}
@article{Bhardwaj2020InformationTM,
  title={Information Theoretic Model Predictive Q-Learning},
  author={Mohak Bhardwaj and Ankur Handa and Dieter Fox and Byron Boots},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.02153}
}
@article{Lowrey2019PlanOL,
  title={Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control},
  author={Kendall Lowrey and Aravind Rajeswaran and Sham M. Kakade and Emanuel Todorov and Igor Mordatch},
  journal={ArXiv},
  year={2019},
  volume={abs/1811.01848}
}
@article{Ebert2018VisualFM,
  title={Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control},
  author={Frederik Ebert and Chelsea Finn and Sudeep Dasari and Annie Xie and Alex X. Lee and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.00568}
}
@article{negenborn2005,
title = {Learning-Based Model Predictive Control for Markov Decision Processes},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {354-359},
year = {2005},
note = {16th IFAC World Congress},
author = {Rudy R. Negenborn and Bart {De Schutter} and Marco A. Wiering and Hans Hellendoorn},
}
@article{Kalashnikov2018QTOptSD,
  title={QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  author={Dmitry Kalashnikov and Alex Irpan and Peter Pastor and Julian Ibarz and Alexander Herzog and Eric Jang and Deirdre Quillen and Ethan Holly and Mrinal Kalakrishnan and Vincent Vanhoucke and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.10293}
}
@inproceedings{Mnih2016AsynchronousMF,
  title={Asynchronous Methods for Deep Reinforcement Learning},
  author={Volodymyr Mnih and Adri{\`a} Puigdom{\`e}nech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle={ICML},
  year={2016}
}
@article{Rubinstein1997OptimizationOC,
  title={Optimization of computer simulation models with rare events},
  author={Reuven Y. Rubinstein},
  journal={European Journal of Operational Research},
  year={1997},
  volume={99},
  pages={89-112}
}
@article{kostrikov2020image,
	title        = {Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels},
	author       = {Ilya Kostrikov and Denis Yarats and Rob Fergus},
	year         = 2020,
	journal      = {International Conference on Learning Representations},
	eprint       = {2004.13649},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
}
@article{Janner2019WhenTT,
  title={When to Trust Your Model: Model-Based Policy Optimization},
  author={Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.08253}
}
@article{Argenson2021ModelBasedOP,
  title={Model-Based Offline Planning},
  author={Arthur Argenson and Gabriel Dulac-Arnold},
  journal={ArXiv},
  year={2021},
  volume={abs/2008.05556}
}
@article{Bhardwaj2021BlendingM,
  title={Blending MPC \& Value Function Approximation for Efficient Reinforcement Learning},
  author={Mohak Bhardwaj and Sanjiban Choudhury and Byron Boots},
  journal={ArXiv},
  year={2021},
  volume={abs/2012.05909}
}
@article{Morgan2021ModelPA,
  title={Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning},
  author={Andrew S. Morgan and Daljeet Nandha and Georgia Chalvatzaki and Carlo D'Eramo and Aaron M. Dollar and Jan Peters},
  journal={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2021},
  pages={6672-6678}
}
@article{Schaul2016PrioritizedER,
  title={Prioritized Experience Replay},
  author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  journal={CoRR},
  year={2016},
  volume={abs/1511.05952}
}
@misc{pytorch_sac,
  author = {Yarats, Denis and Kostrikov, Ilya},
  title = {Soft Actor-Critic (SAC) implementation in PyTorch},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/denisyarats/pytorch_sac}},
}
@inproceedings{ziebart2008maximum,
	title        = {Maximum Entropy Inverse Reinforcement Learning},
	author       = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	year         = 2008,
	booktitle    = {Proceedings of the 23rd National Conference on Artificial Intelligence},
	volume       = 3,
}
@article{Kaiser2020ModelBasedRL,
  title={Model-Based Reinforcement Learning for Atari},
  author={Lukasz Kaiser and Mohammad Babaeizadeh and Piotr Milos and Blazej Osinski and Roy H. Campbell and K. Czechowski and D. Erhan and Chelsea Finn and Piotr Kozakowski and Sergey Levine and Ryan Sepassi and G. Tucker and Henryk Michalewski},
  journal={ArXiv},
  year={2020},
  volume={abs/1903.00374}
}
@inproceedings{Sikchi2020LearningOW,
  title={Learning off-policy with online planning},
  author={Sikchi, Harshit and Zhou, Wenxuan and Held, David},
  booktitle={Conference on Robot Learning},
  pages={1622--1633},
  year={2022},
  organization={PMLR}
}
@article{Clavera2020ModelAugmentedAB,
  title={Model-Augmented Actor-Critic: Backpropagating through Paths},
  author={Ignasi Clavera and Yao Fu and P. Abbeel},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.08068}
}
@article{Wang2020ExploringMP,
  title={Exploring Model-based Planning with Policy Networks},
  author={Tingwu Wang and Jimmy Ba},
  journal={ArXiv},
  year={2020},
  volume={abs/1906.08649}
}
@inproceedings{Buckman2018SampleEfficientRL,
  title={Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion},
  author={Jacob Buckman and Danijar Hafner and G. Tucker and Eugene Brevdo and Honglak Lee},
  booktitle={NeurIPS},
  year={2018}
}
@article{srinivas2020curl,
	title        = {Curl: Contrastive unsupervised representations for reinforcement learning},
	author       = {Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.04136},
}
@article{Ye2021MasteringAG,
  title={Mastering Atari Games with Limited Data},
  author={Weirui Ye and Shaohuai Liu and Thanard Kurutach and P. Abbeel and Yang Gao},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.00210}
}
@article{Schrittwieser2020MasteringAG,
  title={Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  author={Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and L. Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy P. Lillicrap and David Silver},
  journal={Nature},
  year={2020},
  volume={588 7839},
  pages={604-609}
}
@article{hafner2020dreamerv2,
  title={Mastering Atari with Discrete World Models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={arXiv preprint arXiv:2010.02193},
  year={2020}
}
@inproceedings{Cobbe2021PhasicPG,
  title={Phasic Policy Gradient},
  author={Karl Cobbe and Jacob Hilton and Oleg Klimov and John Schulman},
  booktitle={ICML},
  year={2021}
}
@article{Tassa2012SynthesisAS,
  title={Synthesis and stabilization of complex behaviors through online trajectory optimization},
  author={Yuval Tassa and Tom Erez and Emanuel Todorov},
  journal={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year={2012},
  pages={4906-4913}
}
@inproceedings{Chua2018DeepRL,
  title={Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
  author={Kurtland Chua and Roberto Calandra and Rowan McAllister and Sergey Levine},
  booktitle={NeurIPS},
  year={2018}
}
@article{Fujimoto2021AMA,
  title={A Minimalist Approach to Offline Reinforcement Learning},
  author={Scott Fujimoto and Shixiang Shane Gu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.06860}
}
@techreport{deepmindcontrolsuite2018,
	title        = {DeepMind Control Suite},
	author       = {Yuval Tassa and Yotam Doron and Alistair Muldal and Tom Erez and Yazhe Li and Diego de Las Casas and David Budden and Abbas Abdolmaleki and others},
	year         = 2018,
	institution  = {DeepMind},
	archiveprefix = {arXiv},
	eprint       = {1504.04804},
	primaryclass = {cs.LG},
}
@inproceedings{yu2019meta,
  title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Karol Hausman and Chelsea Finn and Sergey Levine},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2019},
  eprint={1910.10897},
  archivePrefix={arXiv},
  primaryClass={cs.LG}}
}
@article{Sutton1988LearningTP,
	title        = {Learning to Predict by the Methods of Temporal Differences},
	author       = {R. Sutton},
	year         = 2005,
	journal      = {Machine Learning},
	volume       = 3,
	pages        = {9--44},
}
@article{Williams2015ModelPP,
  title={Model Predictive Path Integral Control using Covariance Variable Importance Sampling},
  author={Grady Williams and Andrew Aldrich and Evangelos A. Theodorou},
  journal={ArXiv},
  year={2015},
  volume={abs/1509.01149}
}
@incollection{ha2018worldmodels,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages = {2451--2463},
  year = {2018},
  publisher = {Curran Associates, Inc.}
}
@article{agarwal2021deep,
  title={Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel
          and Courville, Aaron and Bellemare, Marc G},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}