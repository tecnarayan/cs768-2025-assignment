\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015}
Mart\'{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dandelion Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris
  Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
  Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas,
  Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
  Xiaoqiang Zheng.
\newblock {TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
  January 2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ahia et~al.(2021)Ahia, Kreutzer, and
  Hooker]{ahia-etal-2021-low-resource}
Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker.
\newblock The low-resource double bind: An empirical study of pruning for
  low-resource machine translation.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pp.\  3316--3333, Punta Cana, Dominican Republic, November 2021.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.282}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.282}.

\bibitem[Aji \& Heafield(2020)Aji and Heafield]{aji-heafield-2020-compressing}
Alham~Fikri Aji and Kenneth Heafield.
\newblock {Compressing Neural Machine Translation Models with 4-bit Precision}.
\newblock In \emph{Proceedings of the Fourth Workshop on Neural Generation and
  Translation}, pp.\  35--42, Online, July 2020. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2020.ngt-1.4}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.ngt-1.4}.

\bibitem[Artetxe et~al.(2022)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer,
  Lin, Du, Iyer, Pasunuru, Anantharaman, Li, Chen, Akin, Baines, Martin, Zhou,
  Koura, O{'}Horo, Wang, Zettlemoyer, Diab, Kozareva, and
  Stoyanov]{artetxe-etal-2022-efficient}
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam
  Shleifer, Xi~Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,
  Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines,
  Louis Martin, Xing Zhou, Punit~Singh Koura, Brian O{'}Horo, Jeffrey Wang,
  Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Veselin Stoyanov.
\newblock Efficient large scale language modeling with mixtures of experts.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  11699--11732, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.804}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{PIQABisk2020}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem[Bondarenko et~al.(2021)Bondarenko, Nagel, and
  Blankevoort]{Bondarenko2021}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Understanding and overcoming the challenges of efficient transformer
  quantization.
\newblock \emph{CoRR}, abs/2109.12948, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.12948}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[{Courbariaux} et~al.(2014){Courbariaux}, {Bengio}, and
  {David}]{2014Courbariaux_low_precision_multiplications}
Matthieu {Courbariaux}, Yoshua {Bengio}, and Jean-Pierre {David}.
\newblock {Training deep neural networks with low precision multiplications}.
\newblock \emph{arXiv e-prints}, art. arXiv:1412.7024, Dec 2014.

\bibitem[Dash \& Mukhopadhyay(2020)Dash and
  Mukhopadhyay]{10.1145/3400302.3415679}
Saurabh Dash and Saibal Mukhopadhyay.
\newblock Hessian-driven unequal protection of dnn parameters for robust
  inference.
\newblock In \emph{Proceedings of the 39th International Conference on
  Computer-Aided Design}, ICCAD '20, New York, NY, USA, 2020. Association for
  Computing Machinery.
\newblock ISBN 9781450380263.
\newblock \doi{10.1145/3400302.3415679}.
\newblock URL \url{https://doi.org/10.1145/3400302.3415679}.

\bibitem[Dash et~al.(2022)Dash, Luo, Lu, Yu, and Mukhopadhyay]{9425549}
Saurabh Dash, Yandong Luo, Anni Lu, Shimeng Yu, and Saibal Mukhopadhyay.
\newblock Robust processing-in-memory with multibit reram using hessian-driven
  mixed-precision computation.
\newblock \emph{IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems}, 41\penalty0 (4):\penalty0 1006--1019, 2022.
\newblock \doi{10.1109/TCAD.2021.3078408}.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, Le, and Ng]{deanbf16}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc\textquotesingle~aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang,
  Quoc Le, and Andrew Ng.
\newblock Large scale distributed deep networks.
\newblock In F.~Pereira, C.J. Burges, L.~Bottou, and K.Q. Weinberger (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~25. Curran
  Associates, Inc., 2012.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf}.

\bibitem[Dehghani et~al.(2021)Dehghani, Arnab, Beyer, Vaswani, and
  Tay]{dehghani2021}
Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi~Tay.
\newblock The efficiency misnomer.
\newblock \emph{CoRR}, abs/2110.12894, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.12894}.

\bibitem[Dettmers \& Zettlemoyer(2022)Dettmers and Zettlemoyer]{dettmers2022}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.09720}.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022gptint}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock {GPT}3.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=dXiGWqBoxaD}.

\bibitem[Du et~al.(2021)Du, Qian, Liu, Ding, Qiu, Yang, and Tang]{GLM-Du2021}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2103.10360.pdf}.

\bibitem[Fader et~al.(2013)Fader, Zettlemoyer, and
  Etzioni]{Fader2013ParaphraseDrivenLF-paralex}
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
\newblock Paraphrase-driven learning for open question answering.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2013.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023}
Elias Frantar and Dan Alistarh.
\newblock Sparsegpt: Massive language models can be accurately pruned in
  one-shot, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.00774}.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.17323}.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks, 2019.
\newblock URL \url{https://arxiv.org/abs/1902.09574}.

\bibitem[Gerganov(2023)]{llamacpp}
Georgi Gerganov.
\newblock llama.cpp.
\newblock \url{https://github.com/ggerganov/llama.cpp}, 2023.

\bibitem[Gholami et~al.(2021)Gholami, Kim, Dong, Yao, Mahoney, and
  Keutzer]{Gholami2021}
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael~W. Mahoney, and Kurt
  Keutzer.
\newblock A survey of quantization methods for efficient neural network
  inference, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.13630}.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{{Deep learning}}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{2015_gupta}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock {Deep Learning with Limited Numerical Precision}.
\newblock \emph{CoRR}, abs/1502.02551, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.02551}.

\bibitem[Hassibi et~al.(1993{\natexlab{a}})Hassibi, Stork, and
  Wolff]{1993optimalbrain}
B.~Hassibi, D.~G. Stork, and G.~J. Wolff.
\newblock {Optimal Brain Surgeon and general network pruning}.
\newblock In \emph{IEEE International Conference on Neural Networks}, pp.\
  293--299 vol.1, March 1993{\natexlab{a}}.
\newblock \doi{10.1109/ICNN.1993.298572}.

\bibitem[Hassibi et~al.(1993{\natexlab{b}})Hassibi, Stork, and
  Com]{Hassibi93secondorder}
Babak Hassibi, David~G. Stork, and Stork Crc.~Ricoh. Com.
\newblock {Second Order Derivatives for Network Pruning: Optimal Brain
  Surgeon}.
\newblock In \emph{Advances in Neural Information Processing Systems 5}, pp.\
  164--171. Morgan Kaufmann, 1993{\natexlab{b}}.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{Hendrycks2016}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus), 2016.
\newblock URL \url{https://arxiv.org/abs/1606.08415}.

\bibitem[{Hinton} et~al.(2015){Hinton}, {Vinyals}, and {Dean}]{2015hinton}
Geoffrey {Hinton}, Oriol {Vinyals}, and Jeff {Dean}.
\newblock {Distilling the Knowledge in a Neural Network}.
\newblock \emph{arXiv e-prints}, art. arXiv:1503.02531, Mar 2015.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012dropout}
Geoffrey~E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R. Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors, 2012.

\bibitem[Hooker(2021)]{Hooker2021}
Sara Hooker.
\newblock The hardware lottery.
\newblock \emph{Commun. ACM}, 64\penalty0 (12):\penalty0 58–65, nov 2021.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/3467017}.
\newblock URL \url{https://doi.org/10.1145/3467017}.

\bibitem[{Howard} et~al.(2017){Howard}, {Zhu}, {Chen}, {Kalenichenko}, {Wang},
  {Weyand}, {Andreetto}, and {Adam}]{2017Howard}
A.~G. {Howard}, M.~{Zhu}, B.~{Chen}, D.~{Kalenichenko}, W.~{Wang}, T.~{Weyand},
  M.~{Andreetto}, and H.~{Adam}.
\newblock {MobileNets: Efficient Convolutional Neural Networks for Mobile
  Vision Applications}.
\newblock \emph{ArXiv e-prints}, April 2017.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El{-}Yaniv, and
  Bengio]{Hubara2016_training_neural_networks_low_precision}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El{-}Yaniv, and Yoshua
  Bengio.
\newblock {Quantized Neural Networks: Training Neural Networks with Low
  Precision Weights and Activations}.
\newblock \emph{CoRR}, abs/1609.07061, 2016.
\newblock URL \url{http://arxiv.org/abs/1609.07061}.

\bibitem[Hubara et~al.(2020)Hubara, Nahshan, Hanani, Banner, and
  Soudry]{hubara2020improving}
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
\newblock Improving post training neural quantization: Layer-wise calibration
  and integer programming.
\newblock \emph{arXiv preprint arXiv:2006.10518}, 2020.

\bibitem[{Iandola} et~al.(2016){Iandola}, {Han}, {Moskewicz}, {Ashraf},
  {Dally}, and {Keutzer}]{2016Squeezenet}
F.~N. {Iandola}, S.~{Han}, M.~W. {Moskewicz}, K.~{Ashraf}, W.~J. {Dally}, and
  K.~{Keutzer}.
\newblock {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and
  0.5MB model size}.
\newblock \emph{ArXiv e-prints}, February 2016.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{Jacob_2018}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock {Quantization and Training of Neural Networks for Efficient
  Integer-Arithmetic-Only Inference}.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, Jun 2018.
\newblock \doi{10.1109/cvpr.2018.00286}.
\newblock URL \url{http://dx.doi.org/10.1109/CVPR.2018.00286}.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, Boyle, Cantin, Chao, Clark, Coriell, Daley,
  Dau, Dean, Gelb, Ghaemmaghami, Gottipati, Gulland, Hagmann, Ho, Hogberg, Hu,
  Hundt, Hurt, Ibarz, Jaffey, Jaworski, Kaplan, Khaitan, Killebrew, Koch,
  Kumar, Lacy, Laudon, Law, Le, Leary, Liu, Lucke, Lundin, MacKean, Maggiore,
  Mahony, Miller, Nagarajan, Narayanaswami, Ni, Nix, Norrie, Omernick,
  Penukonda, Phelps, Ross, Ross, Salek, Samadiani, Severn, Sizikov, Snelham,
  Souter, Steinberg, Swing, Tan, Thorson, Tian, Toma, Tuttle, Vasudevan,
  Walter, Wang, Wilcox, and Yoon]{10.1145/3140659.3080246}
Norman~P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
  Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al~Borchers, Rick
  Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike
  Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara~Vazir Ghaemmaghami, Rajendra
  Gottipati, William Gulland, Robert Hagmann, C.~Richard Ho, Doug Hogberg, John
  Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,
  Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar,
  Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu,
  Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony,
  Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas
  Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt
  Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew
  Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory
  Thorson, Bo~Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter,
  Walter Wang, Eric Wilcox, and Doe~Hyun Yoon.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock \emph{SIGARCH Comput. Archit. News}, 45\penalty0 (2):\penalty0
  1–12, jun 2017.
\newblock ISSN 0163-5964.
\newblock \doi{10.1145/3140659.3080246}.
\newblock URL \url{https://doi.org/10.1145/3140659.3080246}.

\bibitem[Kalamkar et~al.(2019)Kalamkar, Mudigere, Mellempudi, Das, Banerjee,
  Avancha, Vooturi, Jammalamadaka, Huang, Yuen, Yang, Park, Heinecke,
  Georganas, Srinivasan, Kundu, Smelyanskiy, Kaul, and
  Dubey]{kalamkar2019bfloat}
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
  Banerjee, Sasikanth Avancha, Dharma~Teja Vooturi, Nataraj Jammalamadaka,
  Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke,
  Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
  Bharat Kaul, and Pradeep Dubey.
\newblock A study of bfloat16 for deep learning training, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{Kaplan2020}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{CoRR}, abs/2001.08361, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.08361}.

\bibitem[Kim et~al.()Kim, Jang, Lee, Park, Kim, Kim, Kwon, Lee,
  et~al.]{kimwinning}
Yulhwa Kim, Jaeyong Jang, Jehun Lee, Jihoon Park, Jeonghoon Kim, Byeongwook
  Kim, Se~Jung Kwon, Dongsoo Lee, et~al.
\newblock Winning both the accuracy of floating point activation and the
  simplicity of integer arithmetic.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[Krishnamoorthi(2018)]{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock \emph{arXiv preprint arXiv:1806.08342}, 2018.

\bibitem[Kudo \& Richardson(2018)Kudo and
  Richardson]{kudo-richardson-2018-sentencepiece}
Taku Kudo and John Richardson.
\newblock {S}entence{P}iece: A simple and language independent subword
  tokenizer and detokenizer for neural text processing.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  66--71, Brussels,
  Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-2012}.
\newblock URL \url{https://aclanthology.org/D18-2012}.

\bibitem[Kumar et~al.(2017)Kumar, Goyal, and Varma]{kumar17}
Ashish Kumar, Saurabh Goyal, and Manik Varma.
\newblock {Resource-efficient Machine Learning in 2 KB RAM for the Internet of
  Things}.
\newblock In Doina Precup and Yee~Whye Teh (eds.), \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1935--1944,
  International Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/kumar17a.html}.

\bibitem[Kuzmin et~al.(2022)Kuzmin, Baalen, Ren, Nagel, Peters, and
  Blankevoort]{kuzmin2022fp8}
Andrey Kuzmin, Mart~Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and
  Tijmen Blankevoort.
\newblock Fp8 quantization: The power of the exponent, 2022.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{Cun90optimalbrain}
Yann LeCun, John~S. Denker, and Sara~A. Solla.
\newblock {Optimal Brain Damage}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  598--605. Morgan Kaufmann, 1990.

\bibitem[Li et~al.(2021)Li, Gong, Tan, Yang, Hu, Zhang, Yu, Wang, and
  Gu]{BRECQ2021}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei
  Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block
  reconstruction, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.05426}.

\bibitem[Lin et~al.(2019)Lin, Gan, and Han]{lin2019defensive}
Ji~Lin, Chuang Gan, and Song Han.
\newblock Defensive quantization: When efficiency meets robustness, 2019.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{Liu2018GeneratingWB}
Peter~J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam~M. Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock \emph{ICLR}, abs/1801.10198, 2018.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[{Louizos} et~al.(2017){Louizos}, {Welling}, and {Kingma}]{2017l0_reg}
C.~{Louizos}, M.~{Welling}, and D.~P. {Kingma}.
\newblock {Learning Sparse Neural Networks through $L\_0$ Regularization}.
\newblock \emph{ArXiv e-prints}, December 2017.

\bibitem[Luo et~al.(2020)Luo, Kulmizev, and Mao]{Luo2020PositionalAP}
Ziyang Luo, Artur Kulmizev, and Xiao-Xi Mao.
\newblock Positional artefacts propagate through masked language model
  embeddings.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2020.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and
  Wu]{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, and Hao Wu.
\newblock {Mixed Precision Training}, 2017.

\bibitem[Mostafazadeh et~al.(2016)Mostafazadeh, Chambers, He, Parikh, Batra,
  Vanderwende, Kohli, and Allen]{storycloze-mostafazadeh-etal-2016-corpus}
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
  Lucy Vanderwende, Pushmeet Kohli, and James Allen.
\newblock A corpus and cloze evaluation for deeper understanding of commonsense
  stories.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  839--849, San Diego, California, June 2016. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1098}.
\newblock URL \url{https://aclanthology.org/N16-1098}.

\bibitem[Nagel et~al.(2020)Nagel, Amjad, Van~Baalen, Louizos, and
  Blankevoort]{nagel2020up}
Markus Nagel, Rana~Ali Amjad, Mart Van~Baalen, Christos Louizos, and Tijmen
  Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7197--7206. PMLR, 2020.

\bibitem[Nagel et~al.(2021)Nagel, Fournarakis, Amjad, Bondarenko, Van~Baalen,
  and Blankevoort]{nagel2021white}
Markus Nagel, Marios Fournarakis, Rana~Ali Amjad, Yelysei Bondarenko, Mart
  Van~Baalen, and Tijmen Blankevoort.
\newblock A white paper on neural network quantization.
\newblock \emph{arXiv preprint arXiv:2106.08295}, 2021.

\bibitem[{Narang} et~al.(2017){Narang}, {Elsen}, {Diamos}, and
  {Sengupta}]{2017Narang}
Sharan {Narang}, Erich {Elsen}, Gregory {Diamos}, and Shubho {Sengupta}.
\newblock {Exploring Sparsity in Recurrent Neural Networks}.
\newblock \emph{arXiv e-prints}, art. arXiv:1704.05119, Apr 2017.

\bibitem[Nvidia()]{nvidiaa100}
Nvidia.
\newblock Nvidia a100.
\newblock URL
  \url{https://resources.nvidia.com/en-us-genomics-ep/ampere-architecture-white-paper?xs=169656#page=1}.

\bibitem[Ogueji et~al.(2022)Ogueji, Ahia, Onilude, Gehrmann, Hooker, and
  Kreutzer]{ogueji2022}
Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian Gehrmann, Sara
  Hooker, and Julia Kreutzer.
\newblock Intriguing properties of compression on multilingual models.
\newblock 2022.
\newblock \doi{10.48550/ARXIV.2211.02738}.
\newblock URL \url{https://arxiv.org/abs/2211.02738}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno-etal-2016-lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse
  context.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1525--1534,
  Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1144}.
\newblock URL \url{https://aclanthology.org/P16-1144}.

\bibitem[Park et~al.(2022{\natexlab{a}})Park, Park, Lee, Kim, Kim, Kwon, Lee,
  and Lee]{nuQMM2022}
Gunho Park, Baeseong Park, Sungjae Lee, Minsub Kim, Byeongwook Kim, Se~Jung
  Kwon, Youngjoo Lee, and Dongsoo Lee.
\newblock nuqmm: Quantized matmul for efficient inference of large-scale
  generative language models, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2206.09557}.

\bibitem[Park et~al.(2022{\natexlab{b}})Park, You, Nagel, and
  Chang]{park2022quadapter}
Minseop Park, Jaeseong You, Markus Nagel, and Simyung Chang.
\newblock Quadapter: Adapter for gpt-2 quantization.
\newblock \emph{arXiv preprint arXiv:2211.16912}, 2022{\natexlab{b}}.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{pmlr-v28-pascanu13}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In Sanjoy Dasgupta and David McAllester (eds.), \emph{Proceedings of
  the 30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1310--1318, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/pascanu13.html}.

\bibitem[Puccetti et~al.(2022)Puccetti, Rogers, Drozd, and
  Dell'Orletta]{puccetti2022}
Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell'Orletta.
\newblock Outliers dimensions that disrupt transformers are driven by
  frequency, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.11380}.

\bibitem[Quinn \& Ballesteros(2018)Quinn and
  Ballesteros]{quinn-ballesteros-2018-pieces}
Jerry Quinn and Miguel Ballesteros.
\newblock {Pieces of Eight: 8-bit Neural Machine Translation}.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 3 (Industry Papers)}, pp.\  114--120, New Orleans -
  Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-3014}.
\newblock URL \url{https://aclanthology.org/N18-3014}.

\bibitem[Radford et~al.()Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radfordlanguage}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang,
  Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, Casas, Guy, Jones, Bradbury,
  Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell,
  Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and
  Irving]{rae2021}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
  Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
  Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson
  d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan
  Clark, Diego de~Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew
  Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac,
  Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem
  Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and
  Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock {Exploring the Limits of Transfer Learning with a Unified
  Text-to-Text Transformer}, 2020.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{10.5555/3433701.3433727}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, SC '20. IEEE Press,
  2020.
\newblock ISBN 9781728199986.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{arXiv preprint arXiv:1907.10641}, 2019.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander~M. Rush.
\newblock {Movement Pruning: Adaptive Sparsity by Fine-Tuning}, 2020.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[{See} et~al.(2016){See}, {Luong}, and {Manning}]{2016abigail}
Abigail {See}, Minh-Thang {Luong}, and Christopher~D. {Manning}.
\newblock {Compression of Neural Machine Translation Models via Pruning}.
\newblock \emph{arXiv e-prints}, art. arXiv:1606.09274, Jun 2016.

\bibitem[Sheng et~al.(2023)Sheng, Zheng, Yuan, Li, Ryabinin, Fu, Xie, Chen,
  Barrett, Gonzalez, et~al.]{sheng2023high}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel~Y Fu,
  Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph~E Gonzalez, et~al.
\newblock High-throughput generative inference of large language models with a
  single gpu.
\newblock \emph{arXiv preprint arXiv:2303.06865}, 2023.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout-journal}
Nitish Srivastava, Geoffrey~E. Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock {Dropout: a simple way to prevent neural networks from overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Ström(1997)]{Strom97sparseconnection}
Nikko Ström.
\newblock {Sparse Connection And Pruning In Large Dynamic Artificial Neural
  Networks}, 1997.

\bibitem[Treviso et~al.(2022)Treviso, Ji, Lee, van Aken, Cao, Ciosici, Hassid,
  Heafield, Hooker, Martins, Martins, Milder, Raffel, Simpson, Slonim,
  Balasubramanian, Derczynski, and Schwartz]{treviso2022}
Marcos Treviso, Tianchu Ji, Ji-Ung Lee, Betty van Aken, Qingqing Cao, Manuel~R.
  Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Pedro~H. Martins,
  André F.~T. Martins, Peter Milder, Colin Raffel, Edwin Simpson, Noam Slonim,
  Niranjan Balasubramanian, Leon Derczynski, and Roy Schwartz.
\newblock Efficient methods for natural language processing: A survey, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.00099}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock {Attention is all you need}.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman]{SuperGLUE}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang,
  Dean, and Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Zhang, Zhang, Gong, Zhang, Zhang,
  Yu, and Liu]{wei2022outlier}
Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
  Qi~Zhang, Fengwei Yu, and Xianglong Liu.
\newblock Outlier suppression: Pushing the limit of low-bit transformer
  language models.
\newblock \emph{arXiv preprint arXiv:2209.13325}, 2022{\natexlab{b}}.

\bibitem[{Wen} et~al.(2016){Wen}, {Wu}, {Wang}, {Chen}, and
  {Li}]{2016learnedSparsity}
W.~{Wen}, C.~{Wu}, Y.~{Wang}, Y.~{Chen}, and H.~{Li}.
\newblock {Learning Structured Sparsity in Deep Neural Networks}.
\newblock \emph{ArXiv e-prints}, August 2016.

\bibitem[Wortsman et~al.(2023)Wortsman, Dettmers, Zettlemoyer, Morcos, Farhadi,
  and Schmidt]{wortsman2023stable}
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and
  Ludwig Schmidt.
\newblock Stable and low-precision training for large-scale vision-language
  models, 2023.

\bibitem[Wu et~al.(2020)Wu, Judd, Zhang, Isaev, and
  Micikevicius]{wu2020integer}
Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
\newblock Integer quantization for deep learning inference: Principles and
  empirical evaluation.
\newblock \emph{arXiv preprint arXiv:2004.09602}, 2020.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and Han]{Guangxuan2022}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.10438}.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, Wu, Li, and
  He]{yao2022zeroquant}
Zhewei Yao, Reza Yazdani~Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
  Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27168--27183, 2022.

\bibitem[Yao et~al.(2023)Yao, Li, Wu, Youn, and He]{yao2023comprehensive}
Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He.
\newblock A comprehensive study on post-training quantization for large
  language models, 2023.

\bibitem[Yoo et~al.(2022)Yoo, Perlin, Kamalakara, and Araújo]{Yoo2022}
Joanna Yoo, Kuba Perlin, Siddhartha~Rao Kamalakara, and João G.~M. Araújo.
\newblock Scalable training of language models using jax pjit and tpuv4, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.06514}.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat]{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock In \emph{2019 Fifth Workshop on Energy Efficient Machine Learning and
  Cognitive Computing-NeurIPS Edition (EMC2-NIPS)}, pp.\  36--39. IEEE, 2019.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{HellaSwagCAZellers2019}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2019.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia,
  Tam, Ma, Xue, Zhai, Chen, Zhang, Dong, and Tang]{Zeng2022}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue,
  Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang.
\newblock Glm-130b: An open bilingual pre-trained model, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.02414}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer]{OPT-zhang2022}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.01068}.

\bibitem[Zhuo et~al.(2022)Zhuo, Chen, Ramakrishnan, Chen, Feng, Lin, Zhang, and
  Shen]{zhuo2022empirical}
Shaojie Zhuo, Hongyu Chen, Ramchalam~Kinattinkara Ramakrishnan, Tommy Chen,
  Chen Feng, Yicheng Lin, Parker Zhang, and Liang Shen.
\newblock An empirical study of low precision quantization for tinyml.
\newblock \emph{arXiv preprint arXiv:2203.05492}, 2022.

\end{thebibliography}
