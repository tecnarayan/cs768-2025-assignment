\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An et~al.(2022)An, Li, Lin, Liu, Chen, Fu, Chen, Zheng, and Lou]{an2022input}
S.~An, Y.~Li, Z.~Lin, Q.~Liu, B.~Chen, Q.~Fu, W.~Chen, N.~Zheng, and J.-G. Lou.
\newblock Input-tuning: Adapting unfamiliar inputs to frozen pretrained models.
\newblock \emph{arXiv preprint arXiv:2203.03131}, 2022.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim, Bari, Fevry, Alyafeai, Dey, Santilli, Sun, Ben-David, Xu, Chhablani, Wang, Fries, Al-shaibani, Sharma, Thakker, Almubarak, Tang, Tang, Jiang, and Rush]{bach2022promptsource}
S.~H. Bach, V.~Sanh, Z.-X. Yong, A.~Webson, C.~Raffel, N.~V. Nayak, A.~Sharma, T.~Kim, M.~S. Bari, T.~Fevry, Z.~Alyafeai, M.~Dey, A.~Santilli, Z.~Sun, S.~Ben-David, C.~Xu, G.~Chhablani, H.~Wang, J.~A. Fries, M.~S. Al-shaibani, S.~Sharma, U.~Thakker, K.~Almubarak, X.~Tang, X.~Tang, M.~T.-J. Jiang, and A.~M. Rush.
\newblock Promptsource: An integrated development environment and repository for natural language prompts, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2024)Chen, Han, Wu, and Jiang]{chen2024multi}
H.~Chen, X.~Han, Z.~Wu, and Y.-G. Jiang.
\newblock Multi-prompt alignment for multi-source unsupervised domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning]{clark2019does}
K.~Clark, U.~Khandelwal, O.~Levy, and C.~D. Manning.
\newblock What does bert look at? an analysis of bert's attention.
\newblock \emph{arXiv preprint arXiv:1906.04341}, 2019.

\bibitem[Davison et~al.(2019)Davison, Feldman, and Rush]{davison2019commonsense}
J.~Davison, J.~Feldman, and A.~M. Rush.
\newblock Commonsense knowledge mining from pretrained models.
\newblock In \emph{Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)}, pages 1173--1178, 2019.

\bibitem[Deng et~al.(2022)Deng, Wang, Hsieh, Wang, Guo, Shu, Song, Xing, and Hu]{deng2022rlprompt}
M.~Deng, J.~Wang, C.-P. Hsieh, Y.~Wang, H.~Guo, T.~Shu, M.~Song, E.~P. Xing, and Z.~Hu.
\newblock Rlprompt: Optimizing discrete text prompts with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2205.12548}, 2022.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey}
Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui.
\newblock A survey on in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022.

\bibitem[Dulac-Arnold et~al.(2019)Dulac-Arnold, Mankowitz, and Hester]{dulac2019challenges}
G.~Dulac-Arnold, D.~Mankowitz, and T.~Hester.
\newblock Challenges of real-world reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1904.12901}, 2019.

\bibitem[Gao et~al.(2020)Gao, Fisch, and Chen]{gao2020making}
T.~Gao, A.~Fisch, and D.~Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2012.15723}, 2020.

\bibitem[Ge et~al.(2023)Ge, Huang, Xie, Lai, Song, Li, and Huang]{ge2023domain}
C.~Ge, R.~Huang, M.~Xie, Z.~Lai, S.~Song, S.~Li, and G.~Huang.
\newblock Domain adaptation via prompt learning.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2023.

\bibitem[Gu et~al.(2021)Gu, Han, Liu, and Huang]{gu2021ppt}
Y.~Gu, X.~Han, Z.~Liu, and M.~Huang.
\newblock Ppt: Pre-trained prompt tuning for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2109.04332}, 2021.

\bibitem[Guo et~al.(2022)Guo, Li, and Yu]{guo2022improving}
X.~Guo, B.~Li, and H.~Yu.
\newblock Improving the sample efficiency of prompt tuning with domain adaptation.
\newblock \emph{arXiv preprint arXiv:2210.02952}, 2022.

\bibitem[Haviv et~al.(2021)Haviv, Berant, and Globerson]{haviv2021bertese}
A.~Haviv, J.~Berant, and A.~Globerson.
\newblock Bertese: Learning to speak to bert.
\newblock \emph{arXiv preprint arXiv:2103.05327}, 2021.

\bibitem[Htut et~al.(2019)Htut, Phang, Bordia, and Bowman]{htut2019attention}
P.~M. Htut, J.~Phang, S.~Bordia, and S.~R. Bowman.
\newblock Do attention heads in bert track syntactic dependencies?
\newblock \emph{arXiv preprint arXiv:1911.12246}, 2019.

\bibitem[Hu and Liu(2004)]{hu2004mining}
M.~Hu and B.~Liu.
\newblock Mining and summarizing customer reviews.
\newblock In \emph{Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining}, pages 168--177, 2004.

\bibitem[Jiang et~al.(2020)Jiang, Xu, Araki, and Neubig]{jiang2020can}
Z.~Jiang, F.~F. Xu, J.~Araki, and G.~Neubig.
\newblock How can we know what language models know?
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 423--438, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
B.~Lester, R.~Al-Rfou, and N.~Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Li et~al.(2024)Li, Liu, Wang, Li, Lan, and Shen]{li2024dialogue}
C.~Li, X.~Liu, Y.~Wang, D.~Li, Y.~Lan, and C.~Shen.
\newblock Dialogue for prompting: A policy-gradient-based discrete prompt generation for few-shot learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 18481--18489, 2024.

\bibitem[Li and Liang(2021)]{li2021prefix}
X.~L. Li and P.~Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Lin et~al.(2019)Lin, Tan, and Frank]{lin2019open}
Y.~Lin, Y.~C. Tan, and R.~Frank.
\newblock Open sesame: Getting inside bert's linguistic knowledge.
\newblock \emph{arXiv preprint arXiv:1906.01698}, 2019.

\bibitem[Liu et~al.(2022)Liu, Xiao, Ma, Li, Qi, Meng, and Meng]{liu2022prompt}
J.~Liu, J.~Xiao, H.~Ma, X.~Li, Z.~Qi, X.~Meng, and L.~Meng.
\newblock Prompt learning with cross-modal feature alignment for visual domain adaptation.
\newblock In \emph{CAAI International Conference on Artificial Intelligence}, pages 416--428. Springer, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Yuan, Fu, Jiang, Hayashi, and Neubig]{liu2023pre}
P.~Liu, W.~Yuan, J.~Fu, Z.~Jiang, H.~Hayashi, and G.~Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (9):\penalty0 1--35, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2021)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu2021p}
X.~Liu, K.~Ji, Y.~Fu, W.~L. Tam, Z.~Du, Z.~Yang, and J.~Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
\newblock \emph{arXiv preprint arXiv:2110.07602}, 2021.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Zheng, Du, Ding, Qian, Yang, and Tang]{liu2023gpt}
X.~Liu, Y.~Zheng, Z.~Du, M.~Ding, Y.~Qian, Z.~Yang, and J.~Tang.
\newblock Gpt understands, too.
\newblock \emph{AI Open}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2024)Liu, Liu, Zhang, Li, Wang, Lan, and Shen]{liu2024stablept}
X.~Liu, C.~Liu, Z.~Zhang, C.~Li, L.~Wang, Y.~Lan, and C.~Shen.
\newblock Stablept: Towards stable prompting for few-shot learning via input separation.
\newblock \emph{arXiv preprint arXiv:2404.19335}, 2024.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2022)Lu, Qiu, Chang, Wu, Zhu, Rajpurohit, Clark, and Kalyan]{lu2022dynamic}
P.~Lu, L.~Qiu, K.-W. Chang, Y.~N. Wu, S.-C. Zhu, T.~Rajpurohit, P.~Clark, and A.~Kalyan.
\newblock Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2209.14610}, 2022.

\bibitem[Lu et~al.(2021)Lu, Bartolo, Moore, Riedel, and Stenetorp]{lu2021fantastically}
Y.~Lu, M.~Bartolo, A.~Moore, S.~Riedel, and P.~Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\newblock \emph{arXiv preprint arXiv:2104.08786}, 2021.

\bibitem[Mao et~al.(2023)Mao, Mohri, and Zhong]{mao2023cross}
A.~Mao, M.~Mohri, and Y.~Zhong.
\newblock Cross-entropy loss functions: Theoretical analysis and applications.
\newblock In \emph{International Conference on Machine Learning}, pages 23803--23828. PMLR, 2023.

\bibitem[OpenAI(2023)]{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.

\bibitem[Pang and Lee(2005)]{pang2005seeing}
B.~Pang and L.~Lee.
\newblock Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.
\newblock \emph{arXiv preprint cs/0506075}, 2005.

\bibitem[Perez et~al.(2021)Perez, Kiela, and Cho]{perez2021true}
E.~Perez, D.~Kiela, and K.~Cho.
\newblock True few-shot learning with language models.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 11054--11070, 2021.

\bibitem[Prasad et~al.(2022)Prasad, Hase, Zhou, and Bansal]{prasad2022grips}
A.~Prasad, P.~Hase, X.~Zhou, and M.~Bansal.
\newblock Grips: Gradient-free, edit-based instruction search for prompting large language models.
\newblock \emph{arXiv preprint arXiv:2203.07281}, 2022.

\bibitem[Qian et~al.(2022)Qian, Dong, Shen, Wei, and Chen]{qian2022controllable}
J.~Qian, L.~Dong, Y.~Shen, F.~Wei, and W.~Chen.
\newblock Controllable natural language generation with contrastive prefixes.
\newblock \emph{arXiv preprint arXiv:2202.13257}, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Schick and Sch{\"u}tze(2020{\natexlab{a}})]{schick2020exploiting}
T.~Schick and H.~Sch{\"u}tze.
\newblock Exploiting cloze questions for few shot text classification and natural language inference.
\newblock \emph{arXiv preprint arXiv:2001.07676}, 2020{\natexlab{a}}.

\bibitem[Schick and Sch{\"u}tze(2020{\natexlab{b}})]{schick2020s}
T.~Schick and H.~Sch{\"u}tze.
\newblock It's not just size that matters: Small language models are also few-shot learners.
\newblock \emph{arXiv preprint arXiv:2009.07118}, 2020{\natexlab{b}}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher2013recursive}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Y. Ng, and C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in natural language processing}, pages 1631--1642, 2013.

\bibitem[Sun et~al.(2023)Sun, H{\"u}y{\"u}k, and van~der Schaar]{sun2023query}
H.~Sun, A.~H{\"u}y{\"u}k, and M.~van~der Schaar.
\newblock Query-dependent prompt evaluation and optimization with offline inverse rl.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
R.~Taori, I.~Gulrajani, T.~Zhang, Y.~Dubois, X.~Li, C.~Guestrin, P.~Liang, and T.~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vig(2019)]{vig-2019-multiscale}
J.~Vig.
\newblock A multiscale visualization of attention in the transformer model.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, pages 37--42, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-3007}.
\newblock URL \url{https://www.aclweb.org/anthology/P19-3007}.

\bibitem[Vu et~al.(2021)Vu, Lester, Constant, Al-Rfou, and Cer]{vu2021spot}
T.~Vu, B.~Lester, N.~Constant, R.~Al-Rfou, and D.~Cer.
\newblock Spot: Better frozen model adaptation through soft prompt transfer.
\newblock \emph{arXiv preprint arXiv:2110.07904}, 2021.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2023)Wang, Li, Dai, Chen, Zhou, Meng, Zhou, and Sun]{wang2023label}
L.~Wang, L.~Li, D.~Dai, D.~Chen, H.~Zhou, F.~Meng, J.~Zhou, and X.~Sun.
\newblock Label words are anchors: An information flow perspective for understanding in-context learning.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Wang et~al.(2022)Wang, Chen, Ren, Liang, Yan, and Ren]{wang2022paying}
S.~Wang, Z.~Chen, Z.~Ren, H.~Liang, Q.~Yan, and P.~Ren.
\newblock Paying more attention to self-attention: Improving pre-trained language models via attention guiding.
\newblock \emph{arXiv preprint arXiv:2204.02922}, 2022.

\bibitem[Wang and Zhao(2023)]{wang2023tram}
Y.~Wang and Y.~Zhao.
\newblock Tram: Benchmarking temporal reasoning for large language models.
\newblock \emph{arXiv preprint arXiv:2310.00835}, 2023.

\bibitem[Wu and Shi(2022)]{wu2022adversarial}
H.~Wu and X.~Shi.
\newblock Adversarial soft prompt tuning for cross-domain sentiment analysis.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2438--2447, 2022.

\bibitem[Xu et~al.(2023)Xu, Wang, Qiu, Luo, Xu, Huang, and Huang]{xu2023making}
Z.~Xu, C.~Wang, M.~Qiu, F.~Luo, R.~Xu, S.~Huang, and J.~Huang.
\newblock Making pre-trained language models end-to-end few-shot learners with contrastive prompt tuning.
\newblock In \emph{Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining}, pages 438--446, 2023.

\bibitem[Yu et~al.(2022)Yu, Velu, Vinitsky, Gao, Wang, Bayen, and Wu]{yu2022surprising}
C.~Yu, A.~Velu, E.~Vinitsky, J.~Gao, Y.~Wang, A.~Bayen, and Y.~Wu.
\newblock The surprising effectiveness of ppo in cooperative multi-agent games.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24611--24624, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Wang, Zhou, Schuurmans, and Gonzalez]{zhang2022tempera}
T.~Zhang, X.~Wang, D.~Zhou, D.~Schuurmans, and J.~E. Gonzalez.
\newblock Tempera: Test-time prompting via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2211.11890}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Zheng, Zeng, He, Geng, Jiang, Wu, and Xu]{zhao2022adpl}
L.~Zhao, F.~Zheng, W.~Zeng, K.~He, R.~Geng, H.~Jiang, W.~Wu, and W.~Xu.
\newblock Adpl: Adversarial prompt-based domain adaptation for dialogue summarization with knowledge disentanglement.
\newblock In \emph{Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 245--255, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
L.~Zheng, W.-L. Chiang, Y.~Sheng, S.~Zhuang, Z.~Wu, Y.~Zhuang, Z.~Lin, Z.~Li, D.~Li, E.~Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 46595--46623, 2023.

\end{thebibliography}
