
@inproceedings{10.1109/INFOCOM48880.2022.9796974,
	author = {Liu, Yang and Fan, Mingyuan and Chen, Cen and Liu, Ximeng and Ma, Zhuo and Wang, Li and Ma, Jianfeng},
	title = {Backdoor Defense with Machine Unlearning},
	year = {2022},
	publisher = {IEEE Press},
	url = {https://doi.org/10.1109/INFOCOM48880.2022.9796974},
	doi = {10.1109/INFOCOM48880.2022.9796974},
	abstract = {Backdoor injection attack is an emerging threat to the security of neural networks, however, there still exist limited effective defense methods against the attack. In this paper, we propose BAERASER, a novel method that can erase the backdoor injected into the victim model through machine unlearning. Specifically, BAERASER mainly implements backdoor defense in two key steps. First, trigger pattern recovery is conducted to extract the trigger patterns infected by the victim model. Here, the trigger pattern recovery problem is equivalent to the one of extracting an unknown noise distribution from the victim model, which can be easily resolved by the entropy maximization based generative model. Subsequently, BAERASER leverages these recovered trigger patterns to reverse the backdoor injection procedure and induce the victim model to erase the polluted memories through a newly designed gradient ascent based machine unlearning method. Compared with the previous machine unlearning solutions, the proposed approach gets rid of the reliance on the full access to training data for retraining and shows higher effectiveness on backdoor erasing than existing fine-tuning or pruning methods. Moreover, experiments show that BAERASER can averagely lower the attack success rates of three kinds of state-of-the-art backdoor attacks by 99% on four benchmark datasets.},
	booktitle = {IEEE INFOCOM 2022 - IEEE Conference on Computer Communications},
	pages = {280–289},
	numpages = {10},
	location = {London, United Kingdom}
}

@inproceedings{NEURIPS2018_280cf18b,
	author = {Tran, Brandon and Li, Jerry and Madry, Aleksander},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	title = {Spectral Signatures in Backdoor Attacks},
	url = {https://proceedings.neurips.cc/paper/2018/file/280cf18baf4311c92aa5a042336587d3-Paper.pdf},
	volume = {31},
	year = {2018}
}


@inproceedings{
	li2021neural,
	title={Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks},
	author={Yige Li and Xixiang Lyu and Nodens Koren and Lingjuan Lyu and Bo Li and Xingjun Ma},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=9l0K4OM-oXE}
}

@inproceedings{Trojannn,
  author    = {Yingqi Liu and
               Shiqing Ma and
               Yousra Aafer and
               Wen-Chuan Lee and
               Juan Zhai and
               Weihang Wang and
               Xiangyu Zhang},
  title     = {Trojaning Attack on Neural Networks},
  booktitle = {25th Annual Network and Distributed System Security Symposium, {NDSS}
               2018, San Diego, California, USA, February 18-221, 2018},
  publisher = {The Internet Society},
  year      = {2018},
}

@ARTICLE{badnets_2019,
	author={Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
	journal={IEEE Access}, 
	title={BadNets: Evaluating Backdooring Attacks on Deep Neural Networks}, 
	year={2019},
	volume={7},
	number={},
	pages={47230-47244},
	doi={10.1109/ACCESS.2019.2909068}}

@misc{https://doi.org/10.48550/arxiv.1708.06733,
  doi = {10.48550/ARXIV.1708.06733},
  
  url = {https://arxiv.org/abs/1708.06733},
  
  author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  
  keywords = {Cryptography and Security (cs.CR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{NIPS2014_375c7134,
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	title = {How transferable are features in deep neural networks?},
	url = {https://proceedings.neurips.cc/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf},
	volume = {27},
	year = {2014}
}


@InProceedings{liu2018fine-pruning,
	author="Liu, Kang
	and Dolan-Gavitt, Brendan
	and Garg, Siddharth",
	title="Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
	booktitle="Research in Attacks, Intrusions, and Defenses",
	year="2018",
	pages="273--294",
}

@INPROCEEDINGS{NeuralCleanse,
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)}, 
  title={Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={707-723},
  doi={10.1109/SP.2019.00031}}

@misc{https://doi.org/10.48550/arxiv.1710.00942,
  doi = {10.48550/ARXIV.1710.00942},
  
  url = {https://arxiv.org/abs/1710.00942},
  
  author = {Liu, Yuntao and Xie, Yang and Srivastava, Ankur},
  
  keywords = {Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Trojans},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{neural_trojans2017,
	author={Liu, Yuntao and Xie, Yang and Srivastava, Ankur},
	booktitle={2017 IEEE International Conference on Computer Design (ICCD)}, 
	title={Neural Trojans}, 
	year={2017},
	volume={},
	number={},
	pages={45-48},
	doi={10.1109/ICCD.2017.16}}


@article{Saha_Subramanya_Pirsiavash_2020, title={Hidden Trigger Backdoor Attacks}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6871}, DOI={10.1609/aaai.v34i07.6871}, abstractNote={&lt;p&gt;With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.&lt;/p&gt;}, number={07}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed}, year={2020}, month={Apr.}, pages={11957-11965} }


@inproceedings{DBLP:conf/aaai/ChenCBLELMS19,
	author={Bryant Chen and Wilka Carvalho and Nathalie Baracaldo and Heiko Ludwig and Benjamin Edwards and Taesung Lee and Ian Molloy and Biplav Srivastava},
	title={Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering},
	year={2019},
	cdate={1546300800000},
	url={http://ceur-ws.org/Vol-2301/paper_18.pdf},
	booktitle={SafeAI@AAAI}
}

@inproceedings{gao2019strip,
	
	title={STRIP: A Defence Against Trojan Attacks on Deep Neural Networks},
	
	author={Gao, Yansong and Xu, Chang and Wang, Derui and Chen, Shiping and Ranasinghe, Damith C and Nepal, Surya},
	
	booktitle={35th Annual Computer Security Applications Conference (ACSAC)},
	
	year={2019} }

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@article{10.1093/biomet/asv008,
    author = {Yu, Y. and Wang, T. and Samworth, R. J.},
    title = "{A useful variant of the Davis–Kahan theorem for statisticians}",
    journal = {Biometrika},
    volume = {102},
    number = {2},
    pages = {315-323},
    year = {2014},
    month = {04},
    abstract = "{The Davis–Kahan theorem is used in the analysis of many statistical procedures to bound the distance between subspaces spanned by population eigenvectors and their sample versions. It relies on an eigenvalue separation condition between certain population and sample eigenvalues. We present a variant of this result that depends only on a population eigenvalue separation condition, making it more natural and convenient for direct application in statistical contexts, and provide an improvement in many cases to the usual bound in the statistical literature. We also give an extension to situations where the matrices under study may be asymmetric or even non-square, and where interest is in the distance between subspaces spanned by corresponding singular vectors.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asv008},
    url = {https://doi.org/10.1093/biomet/asv008},
    eprint = {https://academic.oup.com/biomet/article-pdf/102/2/315/9642505/asv008.pdf},
}

@article{DBLP:journals/corr/ClementeMC17,
  author    = {Alfredo V. Clemente and
               Humberto Nicol{\'{a}}s Castej{\'{o}}n Mart{\'{\i}}nez and
               Arjun Chandra},
  title     = {Efficient Parallel Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1705.04862},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.04862},
  eprinttype = {arXiv},
  eprint    = {1705.04862},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ClementeMC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{https://doi.org/10.48550/arxiv.1707.06347,
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  title = {Proximal Policy Optimization Algorithms},
  journal = {arXiv preprint arXiv:1707.06347},
  year = {2017},
}


@inproceedings{trojdrl_2019,
	author = {Kiourti, Panagiota and Wardega, Kacper and Jha, Susmit and Li, Wenchao},
	title = {TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning},
	year = {2020},
	isbn = {9781450367257},
	publisher = {IEEE Press},
	abstract = {We present TrojDRL, a tool for exploring and evaluating backdoor attacks on deep reinforcement learning agents. TrojDRL exploits the sequential nature of deep reinforcement learning (DRL) and considers different gradations of threat models. We show that untargeted attacks on state-of-the-art actor-critic algorithms can circumvent existing defenses built on the assumption of backdoors being targeted. We evaluated TrojDRL on a broad set of DRL benchmarks and showed that the attacks require only poisoning as little as 0.025% of the training data. Compared with existing works of backdoor attacks on classification models, TrojDRL provides a first step towards understanding the vulnerability of DRL agents.},
	booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
	articleno = {31},
	numpages = {6},
	keywords = {C.1.3.i neural nets, K.4.4.f security, G.4.g reliability and robustness, I.2.6.g machine learning},
	location = {Virtual Event, USA},
	series = {DAC '20}
}

@article{Karra2020TheTS,
  title={The {TrojAI} software framework: An opensource tool for embedding trojans into deep learning models},
  author={Karra, Kiran and Ashcraft, Chace and Fendley, Neil},
  journal={arXiv preprint arXiv:2003.07233},
  year={2020}
}


@inproceedings{ijcai2021p509,
	title     = {BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning},
	author    = {Wang, Lun and Javed, Zaynah and Wu, Xian and Guo, Wenbo and Xing, Xinyu and Song, Dawn},
	booktitle = {Proceedings of the Thirtieth International Joint Conference on
	Artificial Intelligence, {IJCAI-21}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	editor    = {Zhi-Hua Zhou},
	pages     = {3699--3705},
	year      = {2021},
	month     = {8},
	note      = {Main Track},
	doi       = {10.24963/ijcai.2021/509},
	url       = {https://doi.org/10.24963/ijcai.2021/509},
}


@article{https://doi.org/10.48550/arxiv.2202.03609,
  title={Backdoor detection in reinforcement learning},
  author={Guo, Junfeng and Li, Ang and Liu, Cong},
  journal={arXiv preprint arXiv:2202.03609},
  year={2022}
}

@inproceedings{NEURIPS2019_95e1533e,
	author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks},
	url = {https://proceedings.neurips.cc/paper/2019/file/95e1533eb1b20a97777749fb94fdb944-Paper.pdf},
	volume = {32},
	year = {2019}
}


@inproceedings{10.5555/3327144.3327299,
	author = {Scaman, Kevin and Virmaux, Aladin},
	title = {Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation},
	year = {2018},
	abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	pages = {3839–3848},
	numpages = {10},
	location = {Montr\'{e}al, Canada},
	series = {NIPS'18}
}

