\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Basu, Mianjy, and
  Mukherjee]{arora2016understanding}
Arora, R., Basu, A., Mianjy, P., and Mukherjee, A.
\newblock Understanding deep neural networks with rectified linear units.
\newblock In \emph{ICLR}, 2018.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Ba \& Caruana(2014)Ba and Caruana]{ba2014deep}
Ba, J. and Caruana, R.
\newblock Do deep nets really need to be deep?
\newblock In \emph{NeurIPS}, pp.\  2654--2662, 2014.

\bibitem[Bianchini \& Scarselli(2014)Bianchini and
  Scarselli]{bianchini2014complexity}
Bianchini, M. and Scarselli, F.
\newblock On the complexity of neural network classifiers: A comparison between
  shallow and deep architectures.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  25\penalty0 (8):\penalty0 1553--1565, 2014.

\bibitem[Eldan \& Shamir(2016)Eldan and Shamir]{eldan2016power}
Eldan, R. and Shamir, O.
\newblock The power of depth for feedforward neural networks.
\newblock In \emph{COLT}, pp.\  907--940, 2016.

\bibitem[Hanin(2018)]{hanin2018neural}
Hanin, B.
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Hanin \& Nica(2018)Hanin and Nica]{hanin2018products}
Hanin, B. and Nica, M.
\newblock Products of many large random matrices and gradients in deep neural
  networks.
\newblock \emph{Preprint arXiv:1812.05994}, 2018.

\bibitem[Hanin \& Rolnick(2018)Hanin and Rolnick]{hanin2018start}
Hanin, B. and Rolnick, D.
\newblock How to start training: The effect of initialization and architecture.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Lin et~al.(2017)Lin, Tegmark, and Rolnick]{lin2017does}
Lin, H.~W., Tegmark, M., and Rolnick, D.
\newblock Why does deep and cheap learning work so well?
\newblock \emph{Journal of Statistical Physics}, 168\penalty0 (6):\penalty0
  1223--1247, 2017.

\bibitem[Mhaskar et~al.(2016)Mhaskar, Liao, and Poggio]{mhaskar2016learning}
Mhaskar, H., Liao, Q., and Poggio, T.
\newblock Learning functions: when is deep better than shallow.
\newblock \emph{Preprint arXiv:1603.00988}, 2016.

\bibitem[Mhaskar \& Poggio(2016)Mhaskar and Poggio]{mhaskar2016deep}
Mhaskar, H.~N. and Poggio, T.
\newblock Deep vs.~shallow networks: An approximation theory perspective.
\newblock \emph{Analysis and Applications}, 14\penalty0 (06):\penalty0
  829--848, 2016.

\bibitem[Montufar et~al.(2014)Montufar, Pascanu, Cho, and
  Bengio]{montufar2014number}
Montufar, G.~F., Pascanu, R., Cho, K., and Bengio, Y.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{NeurIPS}, pp.\  2924--2932, 2014.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{NeurIPS}, pp.\  5947--5956, 2017.

\bibitem[Novak et~al.(2018)Novak, Bahri, Abolafia, Pennington, and
  Sohl-Dickstein]{novak2018sensitivity}
Novak, R., Bahri, Y., Abolafia, D.~A., Pennington, J., and Sohl-Dickstein, J.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock In \emph{ICLR}, 2018.

\bibitem[Petersen \& Voigtlaender(2018)Petersen and
  Voigtlaender]{petersen2018optimal}
Petersen, P. and Voigtlaender, F.
\newblock Optimal approximation of piecewise smooth functions using deep {ReLU}
  neural networks.
\newblock \emph{Neural Networks}, 108:\penalty0 296--330, 2018.

\bibitem[Poggio et~al.(2017)Poggio, Mhaskar, Rosasco, Miranda, and
  Liao]{poggio2017and}
Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q.
\newblock Why and when can deep -- but not shallow -- networks avoid the curse
  of dimensionality: a review.
\newblock \emph{International Journal of Automation and Computing}, 14\penalty0
  (5):\penalty0 503--519, 2017.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole2016exponential}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{NeurIPS}, pp.\  3360--3368, 2016.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Dickstein]{raghu2017expressive}
Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Dickstein, J.~S.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{ICML}, pp.\  2847--2854, 2017.

\bibitem[Rolnick \& Tegmark(2018)Rolnick and Tegmark]{rolnick2017power}
Rolnick, D. and Tegmark, M.
\newblock The power of deeper networks for expressing natural functions.
\newblock In \emph{ICLR}, 2018.

\bibitem[Serra et~al.(2018)Serra, Tjandraatmadja, and
  Ramalingam]{serra2017bounding}
Serra, T., Tjandraatmadja, C., and Ramalingam, S.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Shalev-Shwartz et~al.(2018)Shalev-Shwartz, Shamir, and
  Shammah]{shalev2017failures}
Shalev-Shwartz, S., Shamir, O., and Shammah, S.
\newblock Failures of gradient-based deep learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Telgarsky(2015)]{telgarsky2015representation}
Telgarsky, M.
\newblock Representation benefits of deep feedforward networks.
\newblock \emph{Preprint arXiv:1509.08101}, 2015.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Telgarsky, M.
\newblock Benefits of depth in neural networks.
\newblock In \emph{COLT}, 2016.

\bibitem[Yarotsky(2017)]{yarotsky2017error}
Yarotsky, D.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\bibitem[Yarotsky(2018)]{yarotsky2018optimal}
Yarotsky, D.
\newblock Optimal approximation of continuous functions by very deep {ReLU}
  networks.
\newblock In \emph{COLT}, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\end{thebibliography}
