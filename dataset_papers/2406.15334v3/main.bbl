\begin{thebibliography}{105}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2024)Agarwal, Singh, Zhang, Bohnet, Chan, Anand, Abbas, Nova, Co-Reyes, Chu, Behbahani, Faust, and Larochelle]{Agarwal2024ManyShotIL}
Rishabh Agarwal, Avi Singh, Lei~M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John~D. Co-Reyes, Eric Chu, Feryal M.~P. Behbahani, Aleksandra Faust, and Hugo Larochelle.
\newblock Many-shot in-context learning.
\newblock 2024.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei, Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski, Barreira, Vinyals, Zisserman, and Simonyan]{Alayrac2022FlamingoAV}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{ArXiv}, abs/2204.14198, 2022.

\bibitem[Alfassy et~al.(2022)Alfassy, Arbelle, Halimi, Harary, Herzig, Schwartz, Panda, Dolfi, Auer, Staar, Saenko, Feris, and Karlinsky]{alfassy2022feta}
Amit Alfassy, Assaf Arbelle, Oshri Halimi, Sivan Harary, Roei Herzig, Eli Schwartz, Rameswar Panda, Michele Dolfi, Christoph Auer, Peter W.~J. Staar, Kate Saenko, Rogerio Feris, and Leonid Karlinsky.
\newblock {FETA}: Towards specializing foundational models for expert task applications.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and Parikh]{antol2015vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C~Lawrence Zitnick, and Devi Parikh.
\newblock Vqa: Visual question answering.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 2425--2433, 2015.

\bibitem[Avraham et~al.(2022)Avraham, Herzig, Mangalam, Bar, Rohrbach, Karlinsky, Darrell, and Globerson]{avraham2022svit}
Elad~Ben Avraham, Roei Herzig, Karttikeya Mangalam, Amir Bar, Anna Rohrbach, Leonid Karlinsky, Trevor Darrell, and Amir Globerson.
\newblock Bringing image scene structure to video via frame-clip consistency of object tokens.
\newblock In \emph{Thirty-Sixth Conference on Neural Information Processing Systems}, 2022.

\bibitem[Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{Bai2023QwenVLAF}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock \emph{ArXiv}, abs/2308.12966, 2023.

\bibitem[Bertsch et~al.(2024)Bertsch, Ivgi, Alon, Berant, Gormley, and Neubig]{Bertsch2024InContextLW}
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew~R. Gormley, and Graham Neubig.
\newblock In-context learning with long-context models: An in-depth exploration.
\newblock 2024.

\bibitem[Besta et~al.(2023)Besta, Blach, Kubicek, Gerstenberger, Gianinazzi, Gajda, Lehmann, Podstawski, Niewiadomski, Nyczyk, and Hoefler]{Besta2023GraphOT}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler.
\newblock Graph of thoughts: Solving elaborate problems with large language models.
\newblock \emph{ArXiv}, abs/2308.09687, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{Brown2020OGICL}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}, abs/2005.14165, 2020.

\bibitem[Chen et~al.(2023)Chen, Wong, Chen, and Tian]{Chen2023PosInter}
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{ArXiv}, abs/2306.15595, 2023.

\bibitem[Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and Chen]{ChevalierAutoCOmp}
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.
\newblock Adapting language models to compress contexts.
\newblock \emph{ArXiv}, abs/2305.14788, 2023.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
  and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{J. Mach. Learn. Res.}, 24:\penalty0 240:1--240:113, 2022.

\bibitem[Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{Dettmers2023QLoRAEF}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{ArXiv}, abs/2305.14314, 2023.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{Dong2022ASO}
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui.
\newblock A survey on in-context learning.
\newblock 2022.

\bibitem[Doveh et~al.(2024)Doveh, Perek, Mirza, Alfassy, Arbelle, Ullman, and Karlinsky]{Doveh2024TowardsMM-ICL}
Sivan Doveh, Shaked Perek, Muhammad~Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and Leonid Karlinsky.
\newblock Towards multimodal in-context learning for vision \& language models.
\newblock \emph{ArXiv}, abs/2403.12736, 2024.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, Huang, Chebotar, Sermanet, Duckworth, Levine, Vanhoucke, Hausman, Toussaint, Greff, Zeng, Mordatch, and Florence]{Driess2023PaLMEAE}
Danny Driess, F. Xia, Mehdi S.~M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan~Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter~R. Florence.
\newblock Palm-e: An embodied multimodal language model.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Gao et~al.(2023)Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, Li, and Qiao]{Gao2023LLaMAAdapter2}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu~Jiao Qiao.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock \emph{ArXiv}, abs/2304.15010, 2023.

\bibitem[Ge et~al.(2025)Ge, Subramanian, Shi, Herzig, and Darrell]{Ge2023RecursiveVP}
Jiaxin Ge, Sanjay Subramanian, Baifeng Shi, Roei Herzig, and Trevor Darrell.
\newblock Recursive visual programming.
\newblock In \emph{European Conference on Computer Vision}, pages 1--18. Springer, 2025.

\bibitem[Ge et~al.(2023)Ge, Hu, Wang, Chen, and Wei]{Ge2023IncontextAF}
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei.
\newblock In-context autoencoder for context compression in a large language model.
\newblock \emph{ArXiv}, abs/2307.06945, 2023.

\bibitem[Gong et~al.(2023)Gong, Lyu, Zhang, Wang, Zheng, Zhao, Liu, Zhang, Luo, and Chen]{Gong2023MultiModalGPTAV}
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qianmengke Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.
\newblock Multimodal-gpt: A vision and language model for dialogue with humans.
\newblock \emph{ArXiv}, abs/2305.04790, 2023.

\bibitem[Gupta and Kembhavi(2022)]{Gupta2022VisualPC}
Tanmay Gupta and Aniruddha Kembhavi.
\newblock Visual programming: Compositional visual reasoning without training.
\newblock \emph{2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 14953--14962, 2022.

\bibitem[Gurari et~al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and Bigham]{Gurari2018VizWizGC}
Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey~P. Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3608--3617, 2018.

\bibitem[He et~al.(2019)He, Fan, Wu, Xie, and Girshick]{He2019MomentumCF}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross~B. Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 9726--9735, 2019.

\bibitem[Hendel et~al.(2023)Hendel, Geva, and Globerson]{Hendel2023InContextLC}
Roee Hendel, Mor Geva, and Amir Globerson.
\newblock In-context learning creates task vectors.
\newblock \emph{ArXiv}, abs/2310.15916, 2023.

\bibitem[Herzig et~al.(2023)Herzig, Mendelson, Karlinsky, Arbelle, Feris, Darrell, and Globerson]{herzig2023incorporating}
Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, and Amir Globerson.
\newblock Incorporating structured representations into pretrained vision {\textbackslash}\& language models using scene graphs.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Hojel et~al.(2025)Hojel, Bai, Darrell, Globerson, and Bar]{Hojel2024FindingVT}
Alberto Hojel, Yutong Bai, Trevor Darrell, Amir Globerson, and Amir Bar.
\newblock Finding visual task vectors.
\newblock In \emph{European Conference on Computer Vision}, pages 257--273. Springer, 2025.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby19adapters}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning}, pages 2790--2799, 2019.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and Chen]{Hu2021LoRALA}
J.~Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{ArXiv}, abs/2106.09685, 2021.

\bibitem[Hu et~al.(2023)Hu, Lan, Wang, Xu, Lim, Lee, Bing, and Poria]{Hu2023LLMAdaptersAA}
Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria.
\newblock Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.
\newblock \emph{ArXiv}, abs/2304.01933, 2023.

\bibitem[Hudson and Manning(2019)]{Hudson2019GQAAN}
Drew~A. Hudson and Christopher~D. Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional question answering.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 6693--6702, 2019.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{Jia2021ScalingUV}
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc~V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Jiang et~al.(2024)Jiang, He, Zeng, Wei, Ku, Liu, and Chen]{Jiang2024MANTISIM}
Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max~W.F. Ku, Qian Liu, and Wenhu Chen.
\newblock Mantis: Interleaved multi-image instruction tuning.
\newblock arXiv2405.01483, 2024.

\bibitem[Jiang et~al.(2023)Jiang, Wu, Lin, Yang, and Qiu]{Jiang2023LLMLinguaCP}
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Llmlingua: Compressing prompts for accelerated inference of large language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Jiang et~al.()Jiang, Irvin, Wang, Chaudhry, Chen, and Ng]{jiang2024many}
Yixing Jiang, Jeremy~Andrew Irvin, Ji~Hun Wang, Muhammad~Ahmed Chaudhry, Jonathan~H Chen, and Andrew~Y Ng.
\newblock Many-shot in-context learning in multimodal foundation models.
\newblock In \emph{ICML 2024 Workshop on In-Context Learning}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{Kojima2022LargeLM}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{ArXiv}, abs/2205.11916, 2022.

\bibitem[Krishna et~al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen, Kalantidis, Li, Shamma, et~al.]{krishna2017visual}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced dense image annotations.
\newblock \emph{International Journal of Computer Vision}, 123\penalty0 (1):\penalty0 32--73, 2017.

\bibitem[Laurenccon et~al.(2023)Laurenccon, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, Kiela, Cord, and Sanh]{Laurenccon2023IDEFICS}
Hugo Laurenccon, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh.
\newblock Obelisc: An open web-scale filtered dataset of interleaved image-text documents.
\newblock \emph{ArXiv}, abs/2306.16527, 2023.

\bibitem[Laurenccon et~al.(2024)Laurenccon, Tronchon, Cord, and Sanh]{Laurenccon2024Idefics2}
Hugo Laurenccon, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?
\newblock 2024.

\bibitem[Lei et~al.(2023)Lei, Lin, Liao, and Ding]{Lei2023BoostingLR}
Bin Lei, Pei-Hung Lin, Chunhua Liao, and Caiwen Ding.
\newblock Boosting logical reasoning in large language models through a new framework: The graph of thought.
\newblock \emph{ArXiv}, abs/2308.08614, 2023.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{Lester2021ThePO}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2021.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock \emph{arXiv preprint arXiv:2201.12086}, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Li, Savarese, and Hoi]{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{ICML}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Gong, Feng, Xu, Zhang, Wu, and Kong]{Li2023InContextLW}
Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jinchao Zhang, Zhiyong Wu, and Lingpeng Kong.
\newblock In-context learning with many demonstration examples.
\newblock \emph{ArXiv}, abs/2302.04931, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2024)Li, Zhang, Do, Yue, and Chen]{Li2024LongcontextLS}
Tianle Li, Ge Zhang, Quy~Duc Do, Xiang Yue, and Wenhu Chen.
\newblock Long-context llms struggle with long in-context learning.
\newblock 2024.

\bibitem[Li and Liang(2021)]{Li2021PrefixTuningOC}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, abs/2101.00190, 2021.

\bibitem[Lin et~al.(2023)Lin, Yin, Ping, Lu, Molchanov, Tao, Mao, Kautz, Shoeybi, and Han]{lin2023vila}
Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.
\newblock Vila: On pre-training for visual language models, 2023.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{Lin2014MSCOCO}
Tsung-Yi Lin, M. Maire, Serge~J. Belongie, James Hays, P. Perona, D. Ramanan, Piotr Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{liu2023llava15}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{Liu2023LostIT}
Nelson~F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 157--173, 2023{\natexlab{c}}.

\bibitem[Lu et~al.(2023)Lu, Peng, Cheng, Galley, Chang, Wu, Zhu, and Gao]{Lu2023ChameleonPC}
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying~Nian Wu, Song-Chun Zhu, and Jianfeng Gao.
\newblock Chameleon: Plug-and-play compositional reasoning with large language models.
\newblock \emph{ArXiv}, abs/2304.09842, 2023.

\bibitem[Ma et~al.(2023)Ma, Zhang, Bian, Liu, Zhang, Zhao, Zhang, Fu, Hu, and Wu]{Ma2023FairnessguidedFP}
Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, H. Fu, Qinghua Hu, and Bing Wu.
\newblock Fairness-guided few-shot prompting for large language models.
\newblock \emph{ArXiv}, abs/2303.13217, 2023.

\bibitem[Marino et~al.(2019)Marino, Rastegari, Farhadi, and Mottaghi]{Marino2019OKVQAAV}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external knowledge.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 3190--3199, 2019.

\bibitem[Meta et~al.(2024)Meta, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{meta2024llama3}
AI Meta, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2, 2024.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{Min2022RethinkingTR}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock \emph{ArXiv}, abs/2202.12837, 2022.

\bibitem[Mitra et~al.(2024)Mitra, Huang, Darrell, and Herzig]{MitraCCoT}
Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig.
\newblock Compositional chain of thought prompting for large multimodal models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024.

\bibitem[Mu et~al.(2023)Mu, Li, and Goodman]{Mu2023LearningTC}
Jesse Mu, Xiang~Lisa Li, and Noah~D. Goodman.
\newblock Learning to compress prompts with gist tokens.
\newblock \emph{ArXiv}, abs/2304.08467, 2023.

\bibitem[Nilsback and Zisserman(2008)]{Flowers}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing}, pages 722--729, 2008.

\bibitem[OpenAI(2023)]{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pearl(2001)]{Pearl2001DirectAI}
Judea Pearl.
\newblock Direct and indirect effects.
\newblock \emph{Probabilistic and Causal Inference}, 2001.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{Peng2023YaRNEC}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{ArXiv}, abs/2309.00071, 2023.

\bibitem[Qin et~al.(2023)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, Zhao, Tian, Xie, Zhou, Gerstein, Li, Liu, and Sun]{Qin2023ToolLLMFL}
Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc~H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis.
\newblock \emph{ArXiv}, abs/2307.16789, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International Conference on Machine Learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{Raffel2019ExploringTL}
Colin Raffel, Noam~M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2019.

\bibitem[Saikh et~al.(2022)Saikh, Ghosal, Mittal, Ekbal, and Bhattacharyya]{Saikh2022ScienceQAAN}
Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya.
\newblock Scienceqa: a novel resource for question answering on scholarly articles.
\newblock \emph{International Journal on Digital Libraries}, 23:\penalty0 289 -- 301, 2022.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan, Scao, Biderman, Gao, Wolf, and Rush]{sanh2022multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`i}, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom]{Schick2023ToolformerLM}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`i}, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{ArXiv}, abs/2302.04761, 2023.

\bibitem[Schreiber et~al.(2020)Schreiber, Bilmes, and Noble]{schreiber2020apricot}
Jacob Schreiber, Jeffrey Bilmes, and William~Stafford Noble.
\newblock apricot: Submodular selection for data summarization in python.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (161):\penalty0 1--6, 2020.

\bibitem[Shang et~al.(2024)Shang, You, Subramanian, Darrell, and Herzig]{Shang2024TraveLERAM}
Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, and Roei Herzig.
\newblock Traveler: A multi-lmm agent framework for video question-answering.
\newblock \emph{ArXiv}, abs/2404.01476, 2024.

\bibitem[Shen et~al.(2023)Shen, Song, Tan, Li, Lu, and Zhuang]{Shen2023HuggingGPTSA}
Yongliang Shen, Kaitao Song, Xu Tan, Dong~Sheng Li, Weiming Lu, and Yue~Ting Zhuang.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
\newblock \emph{ArXiv}, abs/2303.17580, 2023.

\bibitem[Snell et~al.(2022)Snell, Klein, and Zhong]{Snell2022LearningBD}
Charles~Burton Snell, Dan Klein, and Ruiqi Zhong.
\newblock Learning by distilling context.
\newblock \emph{ArXiv}, abs/2209.15189, 2022.

\bibitem[Subramanian et~al.(2023)Subramanian, Narasimhan, Khangaonkar, Yang, Nagrani, Schmid, Zeng, Darrell, and Klein]{Subramanian2023ModularVQ}
Sanjay Subramanian, Medhini~G. Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.
\newblock Modular visual question answering via code generation.
\newblock \emph{ArXiv}, abs/2306.05392, 2023.

\bibitem[Sun et~al.(2023)Sun, Cui, Zhang, Zhang, Yu, Luo, Wang, Rao, Liu, Huang, and Wang]{Sun2023Emu2}
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
\newblock Generative multimodal models are in-context learners.
\newblock \emph{ArXiv}, abs/2312.13286, 2023.

\bibitem[Sur'is et~al.(2023)Sur'is, Menon, and Vondrick]{Suris2023ViperGPTVI}
D'idac Sur'is, Sachit Menon, and Carl Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock \emph{ArXiv}, abs/2303.08128, 2023.

\bibitem[Tan et~al.(2024)Tan, Li, Patil, Wu, Zhang, Keutzer, Gonzalez, and Popa]{Tan2024LLoCOLL}
Sijun Tan, Xiuyu Li, Shishir~G. Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph~E. Gonzalez, and Raluca~A. Popa.
\newblock Lloco: Learning long contexts offline.
\newblock 2024.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Tran, Garc{\'i}a, Wei, Wang, Chung, Bahri, Schuster, Zheng, Zhou, Houlsby, and Metzler]{Tay2022UL2UL}
Yi Tay, Mostafa Dehghani, Vinh~Q. Tran, Xavier Garc{\'i}a, Jason Wei, Xuezhi Wang, Hyung~Won Chung, Dara Bahri, Tal Schuster, Huaixiu~Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.
\newblock Ul2: Unifying language learning paradigms.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Team(2024)]{Reid2024Gemini1.5}
Gemini Team.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock \emph{ArXiv}, abs/2403.05530, 2024.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Todd et~al.(2023)Todd, Li, Sharma, Mueller, Wallace, and Bau]{Todd2023FunctionVI}
Eric Todd, Millicent Li, Arnab~Sen Sharma, Aaron Mueller, Byron~C. Wallace, and David Bau.
\newblock Function vectors in large language models.
\newblock \emph{ArXiv}, abs/2310.15213, 2023.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{Birds}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge~J. Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem[Wan et~al.(2023)Wan, Sun, Dai, Arik, and Pfister]{Wan2023BetterZR}
Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan~{\"O}. Arik, and Tomas Pfister.
\newblock Better zero-shot reasoning with self-adaptive prompting.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Hu, He, Xu, Liu, juan Liu, and Shen]{Wang2023TSciQTM}
Lei Wang, Yilang Hu, Jiabang He, Xingdong Xu, Ning Liu, Hui juan Liu, and Hengtao Shen.
\newblock T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering.
\newblock \emph{ArXiv}, abs/2305.03453, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Xu, Lan, Hu, Lan, Lee, and Lim]{Wang2023PlanandSolvePI}
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
\newblock Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Wei, Schuurmans, Le, hsin Chi, and Zhou]{Wang2022SelfConsistencyIC}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Huai hsin Chi, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{ArXiv}, abs/2203.11171, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Li, Xu, Zhou, Lei, Lin, Wang, Yang, Zhu, Hoiem, Chang, Bansal, and Ji]{Wang2022LanguageMW}
Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, and Heng Ji.
\newblock Language models with image descriptors are strong few-shot video-language learners.
\newblock \emph{ArXiv}, abs/2205.10747, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{Wei2021FinetunedLM}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{ArXiv}, abs/2109.01652, 2021.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, hsin Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{Wei2022EmergentAO}
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{Trans. Mach. Learn. Res.}, 2022, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, hsin Chi, Xia, Le, and Zhou]{Wei2022ChainOT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock \emph{ArXiv}, abs/2201.11903, 2022{\natexlab{b}}.

\bibitem[Williams(2004)]{Williams2004SimpleSG}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Machine Learning}, 8:\penalty0 229--256, 2004.

\bibitem[Wu et~al.(2023)Wu, Yin, Qi, Wang, Tang, and Duan]{Wu2023VisualCT}
Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation models.
\newblock \emph{ArXiv}, abs/2303.04671, 2023.

\bibitem[Xu et~al.(2023)Xu, Yang, Lin, Wang, Zhou, Zhang, and Mao]{Xu2023ExpertPromptingIL}
Benfeng Xu, An Yang, Junyang Lin, Quang Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao.
\newblock Expertprompting: Instructing large language models to be distinguished experts.
\newblock \emph{ArXiv}, abs/2305.14688, 2023.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{Yao2023TreeOT}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{ArXiv}, abs/2305.10601, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Li, and Zhao]{Yao2023BeyondCE}
Yao Yao, Z. Li, and Hai Zhao.
\newblock Beyond chain-of-thought, effective graph-of-thought reasoning in large language models.
\newblock \emph{ArXiv}, abs/2305.16582, 2023{\natexlab{b}}.

\bibitem[Ye et~al.(2023{\natexlab{a}})Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi, Li, Xu, Chen, Tian, Qi, Zhang, and Huang]{Ye2023mPLUGOwlME}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang.
\newblock mplug-owl: Modularization empowers large language models with multimodality.
\newblock \emph{ArXiv}, abs/2304.14178, 2023{\natexlab{a}}.

\bibitem[Ye et~al.(2023{\natexlab{b}})Ye, Xu, Ye, Yan, Hu, Liu, Qian, Zhang, Huang, and Zhou]{Ye2023mPLUGOwl2RM}
Qinghao Ye, Haiyang Xu, Jiabo Ye, Mingshi Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.
\newblock \emph{ArXiv}, abs/2311.04257, 2023{\natexlab{b}}.

\bibitem[Zhai et~al.(2023)Zhai, Mustafa, Kolesnikov, and Beyer]{zhai2023sigmoid}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11975--11986, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Chen, Bukharin, He, Cheng, Chen, and Zhao]{Zhang2023AdaLoRA}
Qingru Zhang, Minshuo Chen, Alexander~W. Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
\newblock Adaptive budget allocation for parameter-efficient fine-tuning.
\newblock \emph{ArXiv}, abs/2303.10512, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Han, Zhou, Hu, Yan, Lu, Li, Gao, and Qiao]{Zhang2023LLaMAAdapter1}
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu~Jiao Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with zero-init attention.
\newblock \emph{ArXiv}, abs/2303.16199, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Li, and Smola]{Zhang2022AutomaticCO}
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexander~J. Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock \emph{ArXiv}, abs/2210.03493, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Li, Zhao, Karypis, and Smola]{Zhang2023MultimodalCR}
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alexander~J. Smola.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock \emph{ArXiv}, abs/2302.00923, 2023{\natexlab{c}}.

\bibitem[Zhao et~al.(2023)Zhao, Cai, Si, Ma, An, Chen, Liu, Wang, Han, and Chang]{Zhao2023MMICLEV}
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.
\newblock Mmicl: Empowering vision-language model with multi-modal in-context learning.
\newblock \emph{ArXiv}, abs/2309.07915, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Yang, Tang, Zhou, and Yang]{Zheng2023DDCoTDC}
Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang.
\newblock Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models.
\newblock \emph{ArXiv}, abs/2310.16436, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
