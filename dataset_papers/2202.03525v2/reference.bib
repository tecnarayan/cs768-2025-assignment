@article{ahn2020sgd,
  title={SGD with shuffling: optimal rates without component convexity and large epoch requirements},
  author={Ahn, Kwangjun and Yun, Chulhee and Sra, Suvrit},
  journal={arXiv preprint arXiv:2006.06946},
  year={2020}
}

@Article{Nesterov1983,
  Title                    = {A method for unconstrained convex minimization problem with the rate of convergence $\mathcal{O}(1/k^2)$},
  Author                   = {Y. Nesterov},
  Journal                  = {Doklady AN SSSR},
  Year                     = {1983},
  Note                     = {Translated as Soviet Math. Dokl.},
  Pages                    = {543--547},
  Volume                   = {269},
  File                     = {:Papers/Nesterov1983.pdf:PDF},
  Groups                   = {Imported},
  Owner                    = {quoctd},
  Timestamp                = {2012.04.09}
}

@Article{chen2018convergence,
  author       = {X. Chen and S. Liu and R. Sun and M. Hong},
  date         = {2018},
  title        = {On the convergence of a class of adam-type algorithms for non-convex optimization},
  journal      = {ICLR},
  year         = {2018},
}

@Article{Kingma2014,
  author    = {D. P. Kingma and J. Ba},
  title     = {{ADAM}: {A} {M}ethod for {S}tochastic {O}ptimization.},
  journal   = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year      = {2014},
  volume    = {abs/1412.6980},
  added-at  = {2015-01-01T00:00:00.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/23b0328784dbfce338ba0dd2618a7a059/dblp},
  ee        = {http://arxiv.org/abs/1412.6980},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {3b0328784dbfce338ba0dd2618a7a059},
  keywords  = {dblp},
  timestamp = {2015-06-18T04:22:29.000+0200},
}

@article{Bottou2018,
  author =        {L. Bottou and F. E. Curtis and J. Nocedal},
  journal =       {SIAM Rev.},
  number =        {2},
  pages =         {223--311},
  title =         {{O}ptimization {M}ethods for {L}arge-{S}cale
                   {M}achine {L}earning},
  volume =        {60},
  year =          {2018},
}

@article{ghadimi2013stochastic,
  author =        {S. Ghadimi and G. Lan},
  journal =       {SIAM J. Optim.},
  number =        {4},
  pages =         {2341--2368},
  publisher =     {SIAM},
  title =         {Stochastic first-and zeroth-order methods for
                   nonconvex stochastic programming},
  volume =        {23},
  year =          {2013},
}

@article{RM1951,
  author =        {Robbins, Herbert and Monro, Sutton},
  journal =       {The Annals of Mathematical Statistics},
  number =        {3},
  pages =         {400--407},
  title =         {A Stochastic Approximation Method},
  volume =        {22},
  year =          {1951},
}

@inproceedings{bottou2009curiously,
  title={Curiously fast convergence of some stochastic gradient descent algorithms},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of the symposium on learning and data science, Paris},
  volume={8},
  pages={2624--2633},
  year={2009}
}

@book{Nesterov2004,
  author =        {Y. Nesterov},
  publisher =     {Kluwer Academic Publishers},
  series =        {Applied Optimization},
  title =         {{I}ntroductory lectures on convex optimization: {A}
                   basic course},
  volume =        {87},
  year =          {2004},
}

@article{dozat2016incorporating,
  author =        {T. Dozat},
  journal =       {ICLR Workshop},
  pages =         {2013–-2016},
  title =         {Incorporating nesterov momentum into {ADAM}},
  volume =        {1},
  year =          {2016},
}

@article{wang2020scheduled,
  author =        {B. Wang and T. M. Nguyen and A. L. Bertozzi and
                   R. G. Baraniuk and S. J. Osher},
  journal =       {arXiv preprint arXiv:2002.10583},
  title =         {Scheduled Restart Momentum for Accelerated Stochastic
                   Gradient Descent},
  year =          {2020},
}

@article{nguyen2020unified,
  author  = {Lam M. Nguyen and Quoc Tran-Dinh and Dzung T. Phan and Phuong Ha Nguyen and Marten van Dijk},
  title   = {A Unified Convergence Analysis for Shuffling-Type Gradient Methods},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {207},
  pages   = {1-44}
}

@inproceedings{shamir2016without,
  author =        {O. Shamir},
  booktitle =     {Advances in neural information processing systems},
  pages =         {46--54},
  title =         {Without-replacement sampling for stochastic gradient
                   methods},
  year =          {2016},
}

@incollection{bottou2012stochastic,
  author =        {L. Bottou},
  booktitle =     {Neural networks: Tricks of the trade},
  pages =         {421--436},
  publisher =     {Springer},
  title =         {Stochastic gradient descent tricks},
  year =          {2012},
}

@inproceedings{nagaraj2019sgd,
  author =        {Nagaraj, Dheeraj and Jain, Prateek and
                   Netrapalli, Praneeth},
  booktitle =     {International Conference on Machine Learning},
  pages =         {4703--4711},
  title =         {SGD without Replacement: Sharper Rates for General
                   Smooth Convex Functions},
  year =          {2019},
}

@article{nedic2001incremental,
  author =        {A. Nedic and D. P. Bertsekas},
  journal =       {SIAM J. on Optim.},
  number =        {1},
  pages =         {109--138},
  publisher =     {SIAM},
  title =         {Incremental subgradient methods for nondifferentiable
                   optimization},
  volume =        {12},
  year =          {2001},
}

@incollection{nedic2001convergence,
  author =        {A. Nedic and D. Bertsekas},
  booktitle =     {Stochastic optimization: algorithms and applications},
  pages =         {223--264},
  publisher =     {Springer},
  title =         {Convergence rate of incremental subgradient
                   algorithms},
  year =          {2001},
}

@article{ying2017convergence,
  author =        {B. Ying and K. Yuan and A. H. Sayed},
  journal =       {arXiv preprint arXiv:1708.01383},
  number =        {3},
  pages =         {6},
  title =         {Convergence of variance-reduced stochastic learning
                   under random reshuffling},
  volume =        {2},
  year =          {2017},
}

@inproceedings{SAGA,
  author =        {Defazio, Aaron and Bach, Francis and
                   Lacoste-Julien, Simon},
  booktitle =     {Advances in Neural Information Processing Systems},
  pages =         {1646--1654},
  title =         {SAGA: A fast incremental gradient method with support
                   for non-strongly convex composite objectives},
  year =          {2014},
}

@inproceedings{SVRG,
  author =        {Johnson, Rie and Zhang, Tong},
  booktitle =     {NIPS},
  pages =         {315-323},
  title =         {Accelerating Stochastic Gradient Descent Using
                   Predictive Variance Reduction},
  year =          {2013},
}

@article{meng2019convergence,
  author =        {Q. Meng and W. Chen and Y. Wang and Z.-M. Ma and
                   T.-Y. Liu},
  journal =       {Neurocomputing},
  pages =         {46--57},
  publisher =     {Elsevier},
  title =         {Convergence analysis of distributed stochastic
                   gradient descent with shuffling},
  volume =        {337},
  year =          {2019},
}

@article{li2019incremental,
  author =        {X. Li and Z. Zhu and A. So and J. D. Lee},
  journal =       {arXiv preprint arXiv:1907.11687},
  title =         {Incremental Methods for Weakly Convex Optimization},
  year =          {2019},
}


@misc{shu2020metalrschedulenet,
  author =        {Jun Shu and Yanwen Zhu and Qian Zhao and Deyu Meng and
                   Zongben Xu},
  title =         {Meta-LR-Schedule-Net: Learned LR Schedules that Scale
                   and Generalize},
  year =          {2020},
}

@article{li2020exponential,
  author =        {L. Li and Z. Zhuang and F. Orabona},
  journal =       {arXiv preprint arXiv:2002.05273},
  title =         {Exponential Step Sizes for Non-Convex Optimization},
  year =          {2020},
}

@misc{li2019exponential,
  author =        {Zhiyuan Li and Sanjeev Arora},
  title =         {An Exponential Learning Rate Schedule for Deep
                   Learning},
  year =          {2019},
}

@misc{huang2017snapshot,
  author =        {Gao Huang and Yixuan Li and Geoff Pleiss and
                   Zhuang Liu and John E. Hopcroft and
                   Kilian Q. Weinberger},
  title =         {Snapshot Ensembles: Train 1, get M for free},
  year =          {2017},
}

@misc{loshchilov10sgdr,
      title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1608.03983},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{smith2017cyclical,
  author =        {L. N. Smith},
  booktitle =     {2017 IEEE Winter Conference on Applications of
                   Computer Vision (WACV)},
  organization =  {IEEE},
  pages =         {464--472},
  title =         {Cyclical learning rates for training neural networks},
  year =          {2017},
}

@article{Bertsekas2011,
  author =        {Bertsekas, D.P.},
  journal =       {Math. Program.},
  number =        {2},
  pages =         {163--195},
  title =         {Incremental proximal methods for large scale convex
                   optimization},
  volume =        {129},
  year =          {2011},
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}
@misc{ruder2017overview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{chollet2015keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  url={https://github.com/fchollet/keras},
  journal={GitHub},
}

@article{Gurbuzbalaban2019,
   title={Why random reshuffling beats stochastic gradient descent},
   ISSN={1436-4646},
   DOI={10.1007/s10107-019-01440-w},
   journal={Mathematical Programming},
   publisher={Springer Science and Business Media LLC},
   author={Gürbüzbalaban, M. and Ozdaglar, A. and Parrilo, P. A.},
   year={2019},
   month={Oct}
}

@article{mishchenko2020random,
  title={Random reshuffling: Simple analysis with vast improvements},
  author={Mishchenko, Konstantin and Khaled Ragab Bayoumi, Ahmed and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{safran2020good,
  title={How good is SGD with random shuffling?},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3250--3284},
  year={2020},
  organization={PMLR}
}

@inproceedings{rajput2020closing,
  title={Closing the convergence gap of SGD without replacement},
  author={Rajput, Shashank and Gupta, Anant and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={7964--7973},
  year={2020},
  organization={PMLR}
}

@inproceedings{haochen2019random,
  title={Random shuffling beats sgd after finite epochs},
  author={Haochen, Jeff and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={2624--2633},
  year={2019},
  organization={PMLR}
}

@article{wang2019spiderboost,
  title={Spiderboost and momentum: Faster variance reduction algorithms},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={2406--2416},
  year={2019}
}

@article{tran2019hybrid,
  title={Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization},
  author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
  journal={arXiv preprint arXiv:1905.05920},
  year={2019}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
} 


@misc{shu2020metalrschedulenet,
    title={Meta-LR-Schedule-Net: Learned LR Schedules that Scale and Generalize},
    author={Jun Shu and Yanwen Zhu and Qian Zhao and Deyu Meng and Zongben Xu},
    year={2020},
    eprint={2007.14546},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{li2019exponential,
    title={An Exponential Learning Rate Schedule for Deep Learning},
    author={Zhiyuan Li and Sanjeev Arora},
    year={2019},
    eprint={1910.07454},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{huang2017snapshot,
    title={Snapshot Ensembles: Train 1, get M for free},
    author={Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
    year={2017},
    eprint={1704.00109},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@book{sra2012optimization,
  author =        {S. Sra and S. Nowozin and S. J. Wright},
  publisher =     {MIT Press},
  title =         {{O}ptimization for {M}achine {L}earning},
  year =          {2012},
}

@article{Polyak1992,
  author =        {B. Polyak and A. Juditsky},
  journal =       {SIAM J. Control Optim.},
  number =        {4},
  pages =         {838--855},
  publisher =     {SIAM},
  title =         {Acceleration of stochastic approximation by
                   averaging},
  volume =        {30},
  year =          {1992},
}

@article{Nemirovski2009,
  author =        {Nemirovski, A. and Juditsky, A. and Lan, G. and
                   Shapiro, A.},
  journal =       {SIAM J. on Optimization},
  number =        {4},
  pages =         {1574--1609},
  title =         {Robust Stochastic Approximation Approach to
                   Stochastic Programming},
  volume =        {19},
  year =          {2009},
}

@inproceedings{SAG,
  title={A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets},
  author={Le Roux, Nicolas and Schmidt, Mark and Bach, Francis},
  booktitle={NIPS},
  year={2012},
  pages={2663--2671}
}

@article{gurbuzbalaban2015convergence,
  author =        {M. G{\"u}rb{\"u}zbalaban and A. Ozdaglar and
                   P. Parrilo},
  journal =       {arXiv preprint arXiv:1510.08562},
  title =         {Convergence rate of incremental gradient and {N}ewton
                   methods},
  year =          {2015},
}

@inproceedings{Defazio2014,
  author =        {A. Defazio and F. Bach and S. Lacoste-Julien},
  booktitle =     {Advances in Neural Information Processing Systems
                   (NIPS)},
  pages =         {1646--1654},
  title =         {{SAGA}: {A} Fast Incremental Gradient Method With
                   Support for Non-Strongly Convex Composite Objectives},
  year =          {2014},
}

@inproceedings{defazio2014finito,
  author =        {A. Defazio and T. Caetano and J. Domke},
  booktitle =     {International Conference on Machine Learning},
  pages =         {1125--1133},
  title =         {Finito: A faster, permutable incremental gradient
                   method for big data problems},
  year =          {2014},
}

@article{Tran-Dinh2019a,
  author =        {Q. Tran-Dinh and N. H. Pham and D. T. Phan and
                   L. M. Nguyen},
  journal =       {Preprint: UNC-STOR 07.10.2019},
  title =         {A Hybrid Stochastic Optimization Framework for
                   Stochastic Composite Nonconvex Optimization},
  year =          {2019},
}

@article{Cutkosky2019,
  author =        {A. Cutkosky and F. Orabona},
  journal =       {arxiv:1905.10018},
  title =         {Momentum-Based Variance Reduction in Non-Convex
                   {SGD}},
  year =          {2019},
}

@article{Polyak1964,
  author =        {Polyak, Boris T.},
  journal =       {{USSR} Computational Mathematics and Mathematical
                   Physics},
  number =        {5},
  pages =         {1--17},
  title =         {Some methods of speeding up the convergence of
                   iteration methods},
  volume =        {4},
  year =          {1964},
}

@article{nesterov2006cubic,
  author =        {Nesterov, Yurii and Polyak, Boris T},
  journal =       {Mathematical Programming},
  number =        {1},
  pages =         {177--205},
  publisher =     {Springer-Verlag},
  title =         {Cubic regularization of {N}ewton method and its
                   global performance},
  volume =        {108},
  year =          {2006},
}

@inproceedings{polyak_condition,
  address =       {Cham},
  author =        {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle =     {Machine Learning and Knowledge Discovery in
                   Databases},
  editor =        {Frasconi, Paolo and Landwehr, Niels and
                   Manco, Giuseppe and Vreeken, Jilles},
  pages =         {795--811},
  publisher =     {Springer International Publishing},
  title =         {Linear Convergence of Gradient and Proximal-Gradient
                   Methods Under the {P}olyak-{{\L}}ojasiewicz
                   Condition},
  year =          {2016},
}

@article{bottou2016optimization,
  author =        {Bottou, L{\'e}on and Curtis, Frank E and
                   Nocedal, Jorge},
  journal =       {Siam Review},
  number =        {2},
  pages =         {223--311},
  publisher =     {SIAM},
  title =         {Optimization methods for large-scale machine
                   learning},
  volume =        {60},
  year =          {2018},
}

@article{LIBSVM,
  author =        {Chang, Chih-Chung and Lin, Chih-Jen},
  journal =       {ACM Transactions on Intelligent Systems and
                   Technology},
  note =          {Software available at
                   \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}},
  pages =         {27:1--27:27},
  title =         {{LIBSVM}: A library for support vector machines},
  volume =        {2},
  year =          {2011},
}

@misc{tensorflow2015-whitepaper,
  author =        {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and
                   Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and
                   Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and
                   Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and
                   Andrew~Harp and Geoffrey~Irving and Michael~Isard and
                   Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and
                   Manjunath~Kudlur and Josh~Levenberg and
                   Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and
                   Derek~Murray and Chris~Olah and Mike~Schuster and
                   Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and
                   Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and
                   Vijay~Vasudevan and Fernanda~Vi\'{e}gas and
                   Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and
                   Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note =          {Software available from tensorflow.org},
  title =         {{TensorFlow}: Large-Scale Machine Learning on
                   Heterogeneous Systems},
  year =          {2015},
  url =           {https://www.tensorflow.org/},
}

@article{MNIST,
  author =        {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and
                   Haffner, Patrick},
  journal =       {Proceedings of the IEEE},
  number =        {11},
  pages =         {2278--2324},
  publisher =     {IEEE},
  title =         {Gradient-based learning applied to document
                   recognition},
  volume =        {86},
  year =          {1998},
}

@misc{BertsekasSurvey,
  author =        {Dimitri P. Bertsekas},
  title =         {Incremental Gradient, Subgradient, and Proximal
                   Methods for Convex Optimization: A Survey},
  year =          {2015},
}

@techreport{CIFAR10,
  author =        {Krizhevsky, Alex and Hinton, Geoffrey},
  institution =   {Citeseer},
  title =         {Learning multiple layers of features from tiny
                   images},
  year =          {2009},
}

@article{JMLR:v18:17-632,
  author  = {Hiroyuki Kasai},
  title   = {{SGDLibrary}: A {MATLAB} library for stochastic optimization algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {215},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v18/17-632.html}
}

@article{Nguyen2019_sgd_new_aspects,
  author  = {Lam M. Nguyen and Phuong Ha Nguyen and Peter Richt{{\'a}}rik and Katya Scheinberg and Martin Tak{{\'a}}{\v{c}} and Marten van Dijk},
  title   = {New Convergence Aspects of Stochastic Gradient Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {176},
  pages   = {1-49},
  url     = {http://jmlr.org/papers/v20/18-759.html}
}

@InProceedings{Nguyen2018_sgdhogwild,
  title = 	 {{SGD} and {H}ogwild! Convergence Without the Bounded Gradients Assumption},
  author = 	 {Nguyen, Lam and Nguyen, Phuong Ha and van Dijk, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning-Volume 80},
  pages={3747--3755}, 
  year = 	 {2018}
}

@inproceedings{Nguyen2017sarah,
  title={{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2613--2621},
  year={2017},
  organization={JMLR. org}
}

@article{KingmaB14,
  added-at = {2015-01-01T00:00:00.000+0100},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/23b0328784dbfce338ba0dd2618a7a059/dblp},
  ee = {http://arxiv.org/abs/1412.6980},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {3b0328784dbfce338ba0dd2618a7a059},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2015-06-18T04:22:29.000+0200},
  title = {Adam: A Method for Stochastic Optimization.},
  volume = {abs/1412.6980},
  year = 2014
}

@article{AdaGrad,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 journal = {Journal of Machine Learning Research},
 volume = {12},
 year = {2011},
 pages = {2121--2159}
}



@article{Pham2019,
  author  = {Nhan H. Pham and Lam M. Nguyen and Dzung T. Phan and Quoc Tran-Dinh},
  title   = {ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {110},
  pages   = {1-48},
  url     = {http://jmlr.org/papers/v21/19-248.html}
}

@inproceedings{Reddi2016a,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={International conference on machine learning},
  pages={314--323},
  year={2016}
}


@InProceedings{pmlr-v89-vaswani19a,
  title = 	 {Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron},
  author =       {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1195--1204},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/vaswani19a/vaswani19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/vaswani19a.html},
  
}
@article{Recht2011,
author = {Recht, Benjamin and Ré, Christopher},
year = {2011},
month = {04},
pages = {},
title = {Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion},
volume = {5},
journal = {Mathematical Programming Computation},
doi = {10.1007/s12532-013-0053-8}
}

@InProceedings{pmlr-v28-shamir13,
  title = 	 {Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes},
  author = 	 {Shamir, Ohad and Zhang, Tong},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {71--79},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/shamir13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/shamir13.html},
  
}


@InProceedings{pmlr-v28-sutskever13,
  title = 	 {On the importance of initialization and momentum in deep learning},
  author = 	 {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1139--1147},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/sutskever13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/sutskever13.html},
}
@article{Lan2012,
  title={An optimal method for stochastic composite optimization},
  author={Guanghui Lan},
  journal={Mathematical Programming},
  year={2012},
  volume={133},
  pages={365-397}
}

@inproceedings{NIPS_cotter2011,
 author = {Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Better Mini-Batch Algorithms via Accelerated Gradient Methods},
 url = {https://proceedings.neurips.cc/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},
 volume = {24},
 year = {2011}
}
@article{JMLR_yuan2016,
  author  = {Kun Yuan and Bicheng Ying and Ali H. Sayed},
  title   = {On the Influence of Momentum Acceleration on Online Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {192},
  pages   = {1-66},
  url     = {http://jmlr.org/papers/v17/16-157.html}
}

@misc{schmidt2013fast,
      title={Fast Convergence of Stochastic Gradient Descent under a Strong Growth Condition}, 
      author={Mark Schmidt and Nicolas Le Roux},
      year={2013},
      eprint={1308.6370},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@inproceedings{NIPS_hu2009,
 author = {Hu, Chonghai and Pan, Weike and Kwok, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Accelerated Gradient Methods for Stochastic Optimization and Online Learning},
 url = {https://proceedings.neurips.cc/paper/2009/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf},
 volume = {22},
 year = {2009}
}


@article{siam_tseng1998,
author = {Tseng, Paul},
title = {An Incremental Gradient(-Projection) Method with Momentum Term and Adaptive Stepsize Rule},
journal = {SIAM Journal on Optimization},
volume = {8},
number = {2},
pages = {506-531},
year = {1998},
doi = {10.1137/S1052623495294797},

URL = { 
        https://doi.org/10.1137/S1052623495294797
    
},
eprint = { 
        https://doi.org/10.1137/S1052623495294797
    
}

}

@article{siam_ghadimi2012,
author = {Ghadimi, Saeed and Lan, Guanghui},
title = {Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic Composite Optimization I: A Generic Algorithmic Framework},
journal = {SIAM Journal on Optimization},
volume = {22},
number = {4},
pages = {1469-1492},
year = {2012},
doi = {10.1137/110848864},

URL = { 
        https://doi.org/10.1137/110848864
    
},
eprint = { 
        https://doi.org/10.1137/110848864
    
}

}
@InProceedings{pmlr-v33-zhong14,
  title = 	 {{Accelerated Stochastic Gradient Method for Composite Regularization}},
  author = 	 {Zhong, Wenliang and Kwok, James},
  booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1086--1094},
  year = 	 {2014},
  editor = 	 {Kaski, Samuel and Corander, Jukka},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Reykjavik, Iceland},
  month = 	 {22--25 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v33/zhong14.pdf},
  url = 	 {https://proceedings.mlr.press/v33/zhong14.html},
  
}

@article{siam_lessard2016,
author = {Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
title = {Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
journal = {SIAM Journal on Optimization},
volume = {26},
number = {1},
pages = {57-95},
year = {2016},
doi = {10.1137/15M1009597},

URL = { 
        https://doi.org/10.1137/15M1009597
    
},
eprint = { 
        https://doi.org/10.1137/15M1009597
    
}

}
@article{springer_devolder2014, 
    author = {Devolder, Olivier and Glineur, Fran\c{c}ois and Nesterov, Yurii}, 
    title = {First-Order Methods of Smooth Convex Optimization with Inexact Oracle}, year = {2014}, 
    issue_date = {August 2014}, 
    publisher = {Springer-Verlag}, 
    journal = {Springer-Verlag}, 
    address = {Berlin, Heidelberg}, 
    volume = {146}, number = {1–2}, 
    issn = {0025-5610}, 
    url = {https://doi.org/10.1007/s10107-013-0677-5}, 
    doi = {10.1007/s10107-013-0677-5}
}

@article{corr_liu2018,
  author    = {Chaoyue Liu and
               Mikhail Belkin},
  title     = {MaSS: an Accelerated Stochastic Method for Over-parametrized Learning},
  journal   = {CoRR},
  volume    = {abs/1810.13395},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.13395},
  eprinttype = {arXiv},
  eprint    = {1810.13395},
  timestamp = {Mon, 07 Sep 2020 12:51:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-13395.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v139-tran21b,
  title = 	 {{SMG}: A Shuffling Gradient-Based Method with Momentum},
  author =       {Tran, Trang H and Nguyen, Lam M and Tran-Dinh, Quoc},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10379--10389},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/tran21b/tran21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/tran21b.html},

}

@misc{mishchenko2021proximal,
      title={Proximal and Federated Random Reshuffling}, 
      author={Konstantin Mishchenko and Ahmed Khaled and Peter Richtárik},
      year={2021},
      eprint={2102.06704},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deng2012mnist, 
  title={The mnist database of handwritten digit images for machine learning research}, 
  author={Deng, Li}, 
  journal={IEEE Signal Processing Magazine}, 
  volume={29}, 
  number={6}, 
  pages={141--142}, 
  year={2012}, 
  publisher={IEEE} 
}

@ARTICLE{Nguyen2022_OptDL,
  author = {Lam M. Nguyen and Trang H. Tran and Marten van Dijk},
  title = {Finite-Sum Optimization: A New Perspective for Convergence to a Global Solution},
  journal = {arXiv preprint arXiv:2202.03524},
  year = {2022}
}