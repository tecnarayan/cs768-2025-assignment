\begin{thebibliography}{10}

\bibitem{optnet}
B.~Amos and J.~Z. Kolter.
\newblock \href{https://arxiv.org/abs/1703.00443} {Opt{N}et: Differentiable
  optimization as a layer in neural networks}.
\newblock In {\em Proc. of ICML}, 2017.

\bibitem{nmt_bahdanau}
D.~Bahdanau, K.~Cho, and Y.~Bengio.
\newblock \href{https://arxiv.org/abs/1409.0473} {Neural machine translation by
  jointly learning to align and translate}.
\newblock In {\em Proc. of ICLR}, 2015.

\bibitem{ball_1994}
K.~Ball, E.~A. Carlen, and E.~H. Lieb.
\newblock \href{https://link.springer.com/article/10.1007/BF01231769} {Sharp
  uniform convexity and smoothness inequalities for trace norms}.
\newblock {\em Inventiones Mathematicae}, 115(1):463--482, 1994.

\bibitem{fista}
A.~Beck and M.~Teboulle.
\newblock \href{http://epubs.siam.org/doi/abs/10.1137/080716542} {A fast
  iterative shrinkage-thresholding algorithm for linear inverse problems}.
\newblock {\em SIAM Journal on Imaging Sciences}, 2(1):183--202, 2009.

\bibitem{convex_nn_bengio}
Y.~Bengio, N.~Le~Roux, P.~Vincent, O.~Delalleau, and P.~Marcotte.
\newblock \href{http://papers.nips.cc/paper/2800-convex-neural-networks}
  {Convex neural networks}.
\newblock In {\em Proc. of NIPS}, 2005.

\bibitem{conditional_bengio}
Y.~Bengio, N.~L{\'e}onard, and A.~Courville.
\newblock \href{https://arxiv.org/abs/1305.2982} {Estimating or propagating
  gradients through stochastic neurons for conditional computation}.
\newblock In {\em Proc. of NIPS}, 2013.

\bibitem{oscar}
H.~D. Bondell and B.~J. Reich.
\newblock
  \href{http://www.stat.ncsu.edu/information/library/papers/Oscar_2583.pdf}
  {Simultaneous regression shrinkage, variable selection, and supervised
  clustering of predictors with {OSCAR}}.
\newblock {\em Biometrics}, 64(1):115--123, 2008.

\bibitem{snli}
S.~R. Bowman, G.~Angeli, C.~Potts, and C.~D. Manning.
\newblock \href{https://arxiv.org/abs/1508.05326} {A large annotated corpus for
  learning natural language inference}.
\newblock In {\em Proc. of EMNLP}, 2015.

\bibitem{boyd_book}
S.~Boyd and L.~Vandenberghe.
\newblock {\em \href{https://web.stanford.edu/~boyd/cvxbook/}{Convex
  optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem{attention_speech}
J.~K. Chorowski, D.~Bahdanau, D.~Serdyuk, K.~Cho, and Y.~Bengio.
\newblock \href{https://arxiv.org/abs/1506.07503}{Attention-based models for
  speech recognition}.
\newblock In {\em Proc. of NIPS}, 2015.

\bibitem{clarke_book}
F.~H. Clarke.
\newblock {\em \href{http://epubs.siam.org/doi/book/10.1137/1.9781611971309}
  {Optimization and nonsmooth analysis}}.
\newblock SIAM, 1990.

\bibitem{cohn}
T.~Cohn, C.~D.~V. Hoang, E.~Vymolova, K.~Yao, C.~Dyer, and G.~Haffari.
\newblock \href{https://arxiv.org/abs/1601.01085} {Incorporating structural
  alignment biases into an attentional neural translation model}.
\newblock In {\em Proc. of NAACL-HLT}, 2016.

\bibitem{tv1d_prox}
L.~Condat.
\newblock \href{http://ieeexplore.ieee.org/document/6579659/} {A direct
  algorithm for 1-d total variation denoising}.
\newblock {\em IEEE Signal Processing Letters}, 20(11):1054--1057, 2013.

\bibitem{dagan_nle}
I.~Dagan, B.~Dolan, B.~Magnini, and D.~Roth.
\newblock
  \href{https://experts.illinois.edu/en/publications/recognizing-textual-entailment-rational-evaluation-and-approaches-2}
  {Recognizing textual entailment: Rational, evaluation and approaches}.
\newblock {\em Natural Language Engineering}, 15(4):i--xvii, 2009.

\bibitem{duchi}
J.~Duchi, S.~Shalev-Shwartz, Y.~Singer, and T.~Chandra.
\newblock \href{https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf}
  {Efficient projections onto the $\ell_1$-ball for learning in high
  dimensions}.
\newblock In {\em Proc. of ICML}, 2008.

\bibitem{softplus}
C.~Dugas, Y.~Bengio, F.~B{\'e}lisle, C.~Nadeau, and R.~Garcia.
\newblock
  \href{http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing}
  {Incorporating second-order functional knowledge for better option pricing}.
\newblock {\em Proc. of NIPS}, 2001.

\bibitem{pathwise}
J.~Friedman, T.~Hastie, H.~H{\"o}fling, and R.~Tibshirani.
\newblock \href{https://arxiv.org/abs/0708.1485} {Pathwise coordinate
  optimization}.
\newblock {\em The Annals of Applied Statistics}, 1(2):302--332, 2007.

\bibitem{NTM}
A.~Graves, G.~Wayne, and I.~Danihelka.
\newblock \href{https://arxiv.org/abs/1410.5401}{Neural {T}uring {M}achines}.
\newblock In {\em Proc. of NIPS}, 2014.

\bibitem{diff_neural_computer}
A.~Graves, G.~Wayne, M.~Reynolds, T.~Harley, I.~Danihelka,
  A.~Grabska-Barwi{\'n}ska, S.~G. Colmenarejo, E.~Grefenstette, T.~Ramalho,
  J.~Agapiou, et~al.
\newblock
  \href{https://www.nature.com/nature/journal/v538/n7626/full/nature20101.html}
  {Hybrid computing using a neural network with dynamic external memory}.
\newblock {\em Nature}, 538(7626):471--476, 2016.

\bibitem{ozan_thesis}
O.~Irsoy.
\newblock {\em \href{https://search.proquest.com/docview/1865340139} {Deep
  sequential and structural neural models of compositionality}}.
\newblock PhD thesis, Cornell University, 2017.

\bibitem{gumbel_softmax}
E.~Jang, S.~Gu, and B.~Poole.
\newblock \href{https://arxiv.org/abs/1611.01144} {Categorical
  reparameterization with {G}umbel-{S}oftmax}.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{kakade_jmlr}
S.~M. Kakade, S.~Shalev-Shwartz, and A.~Tewari.
\newblock \href{https://arxiv.org/abs/0910.0610} {Regularization techniques for
  learning with matrices}.
\newblock {\em Journal of Machine Learning Research}, 13:1865--1890, 2012.

\bibitem{structured_attn}
Y.~Kim, C.~Denton, L.~Hoang, and A.~M. Rush.
\newblock \href{https://arxiv.org/abs/1702.00887}{Structured attention
  networks}.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{opennmt}
G.~{Klein}, Y.~{Kim}, Y.~{Deng}, J.~{Senellart}, and A.~M. {Rush}.
\newblock \href{https://arxiv.org/abs/1701.02810} {{OpenNMT:} Open-source
  toolkit for neural machine translation}.
\newblock {\em arXiv e-prints}, 2017.

\bibitem{moses}
P.~Koehn, H.~Hoang, A.~Birch, C.~Callison-Burch, M.~Federico, N.~Bertoldi,
  B.~Cowan, W.~Shen, C.~Moran, R.~Zens, C.~Dyer, O.~Bojar, A.~Constantin, and
  E.~Herbst.
\newblock \href{http://www.aclweb.org/anthology/P07-2045} {Moses: Open source
  toolkit for statistical machine translation}.
\newblock In {\em Proc. of ACL}, 2007.

\bibitem{taolei}
T.~Lei, R.~Barzilay, and T.~Jaakkola.
\newblock \href{https://arxiv.org/abs/1606.04155} {Rationalizing neural
  predictions}.
\newblock In {\em Proc. of EMNLP}, 2016.

\bibitem{jiwei}
J.~Li, X.~Chen, E.~Hovy, and D.~Jurafsky.
\newblock \href{https://arxiv.org/abs/1506.01066} {Visualizing and
  understanding neural models in NLP}.
\newblock In {\em Proc. of NAACL-HLT}, 2016.

\bibitem{sparse_cnn}
B.~Liu, M.~Wang, H.~Foroosh, M.~Tappen, and M.~Pensky.
\newblock \href{http://ieeexplore.ieee.org/document/7298681/} {Sparse
  convolutional neural networks}.
\newblock In {\em Proc. of ICCVPR}, 2015.

\bibitem{nmt_luong}
M.-T. Luong, H.~Pham, and C.~D. Manning.
\newblock \href{https://arxiv.org/abs/1508.04025} {Effective approaches to
  attention-based neural machine translation}.
\newblock In {\em Proc. of EMNLP}, 2015.

\bibitem{concrete_distribution}
C.~J. Maddison, A.~Mnih, and Y.~W. Teh.
\newblock \href{https://arxiv.org/abs/1611.00712} {The concrete distribution: A
  continuous relaxation of discrete random variables}.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{sparsemax}
A.~F. Martins and R.~F. Astudillo.
\newblock \href{https://arxiv.org/abs/1602.02068} {From softmax to sparsemax: A
  sparse model of attention and multi-label classification}.
\newblock In {\em Proc. of ICML}, 2016.

\bibitem{andre-easy}
A.~F. Martins and J.~Kreutzer.
\newblock \href{https://aclweb.org/anthology/D/D17/D17-1036.pdf} {Learning
  what's easy: {F}ully differentiable neural easy-first taggers}.
\newblock In {\em Proc. of EMNLP}, 2017.

\bibitem{smooth_and_strong}
O.~Meshi, M.~Mahdavi, and A.~G. Schwing.
\newblock
  \href{https://papers.nips.cc/paper/5710-smooth-and-strong-map-inference-with-linear-convergence}
  {Smooth and strong: {MAP} inference with linear convergence}.
\newblock In {\em Proc. of NIPS}, 2015.

\bibitem{michelot}
C.~Michelot.
\newblock \href{https://link.springer.com/article/10.1007/BF00938486} {A finite
  algorithm for finding the projection of a point onto the canonical simplex of
  $\mathbb{R}^n$}.
\newblock {\em Journal of Optimization Theory and Applications},
  50(1):195--200, 1986.

\bibitem{nesterov_smooth}
Y.~Nesterov.
\newblock \href{https://link.springer.com/article/10.1007/s10107-004-0552-5}
  {Smooth minimization of non-smooth functions}.
\newblock {\em Mathematical Programming}, 103(1):127--152, 2005.

\bibitem{proximal_algorithms}
N.~Parikh and S.~Boyd.
\newblock \href{http://web.stanford.edu/~boyd/papers/prox_algs.html} {Proximal
  algorithms}.
\newblock {\em Foundations and Trends{\textregistered} in Optimization},
  1(3):127--239, 2014.

\bibitem{pytorch}
PyTorch.
\newblock \url{http://pytorch.org}, 2017.

\bibitem{rockt}
T.~Rockt{\"a}schel, E.~Grefenstette, K.~M. Hermann, T.~Kocisky, and P.~Blunsom.
\newblock \href{https://arxiv.org/abs/1509.06664} {Reasoning about entailment
  with neural attention}.
\newblock In {\em Proc. of ICLR}, 2016.

\bibitem{rush_summary}
A.~M. Rush, S.~Chopra, and J.~Weston.
\newblock \href{https://arxiv.org/abs/1509.00685} {A neural attention model for
  abstractive sentence summarization}.
\newblock In {\em Proc. of EMNLP}, 2015.

\bibitem{group_sparse_dnn}
S.~Scardapane, D.~Comminiello, A.~Hussain, and A.~Uncini.
\newblock \href{https://arxiv.org/abs/1607.00485} {Group sparse regularization
  for deep neural networks}.
\newblock {\em Neurocomputing}, 241:81--89, 2017.

\bibitem{prox_sdca}
S.~Shalev-Shwartz and T.~Zhang.
\newblock \href{https://arxiv.org/abs/1309.2375} {Accelerated proximal
  stochastic dual coordinate ascent for regularized loss minimization}.
\newblock {\em Mathematical Programming}, 155(1):105--145, 2016.

\bibitem{fused_lasso}
R.~Tibshirani, M.~Saunders, S.~Rosset, J.~Zhu, and K.~Knight.
\newblock
  \href{https://web.stanford.edu/group/SOL/papers/fused-lasso-JRSSB.pdf}
  {Sparsity and smoothness via the fused lasso}.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 67(1):91--108, 2005.

\bibitem{structured_dnn}
W.~Wen, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock \href{https://arxiv.org/abs/1608.03665} {Learning structured sparsity
  in deep neural networks}.
\newblock In {\em Proc. of NIPS}, 2016.

\bibitem{show_attent_tell}
K.~Xu, J.~Ba, R.~Kiros, K.~Cho, A.~Courville, R.~Salakhudinov, R.~Zemel, and
  Y.~Bengio.
\newblock \href{https://arxiv.org/abs/1502.03044} {Show, attend and tell:
  Neural image caption generation with visual attention}.
\newblock In {\em Proc. of ICML}, 2015.

\bibitem{decomposing_prox}
Y.~Yu.
\newblock
  \href{https://papers.nips.cc/paper/4863-on-decomposing-the-proximal-map} {On
  decomposing the proximal map}.
\newblock In {\em Proc. of NIPS}, 2013.

\bibitem{zalinescu}
C.~Zalinescu.
\newblock {\em \href{http://www.worldscientific.com/worldscibooks/10.1142/5021}
  {Convex analysis in general vector spaces}}.
\newblock World Scientific, 2002.

\bibitem{oscar_prox}
X.~Zeng and M.~A. Figueiredo.
\newblock \href{https://arxiv.org/abs/1309.6301} {Solving {OSCAR}
  regularization problems by fast approximate proximal splitting algorithms}.
\newblock {\em Digital Signal Processing}, 31:124--135, 2014.

\bibitem{owl}
X.~Zeng and F.~A. Mario.
\newblock \href{https://arxiv.org/abs/1409.4271} {The ordered weighted $\ell_1$
  norm: Atomic formulation, dual norm, and projections}.
\newblock {\em arXiv e-prints}, 2014.

\bibitem{oscar_proof}
L.~W. Zhong and J.~T. Kwok.
\newblock \href{http://www.icml-2011.org/papers/9_icmlpaper.pdf} {Efficient
  sparse modeling with automatic feature grouping}.
\newblock {\em IEEE transactions on neural networks and learning systems},
  23(9):1436--1447, 2012.

\end{thebibliography}
