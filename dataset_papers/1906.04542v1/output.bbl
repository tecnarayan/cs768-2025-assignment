\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Audibert et~al.(2007)Audibert, Tsybakov, et~al.]{audibert2007fast}
Audibert, J.-Y., Tsybakov, A.~B., et~al.
\newblock Fast learning rates for plug-in classifiers.
\newblock \emph{The Annals of statistics}, 35\penalty0 (2):\penalty0 608--633,
  2007.

\bibitem[Biau \& Devroye(2015)Biau and Devroye]{Biau}
Biau, G. and Devroye, L.
\newblock \emph{Lectures on the Nearest Neighbor Method}.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2015.

\bibitem[Blanchard et~al.(2010)Blanchard, Lee, and Scott]{blanchard2010semi}
Blanchard, G., Lee, G., and Scott, C.
\newblock Semi-supervised novelty detection.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Nov):\penalty0 2973--3009, 2010.

\bibitem[Blanchard et~al.(2016)Blanchard, Flaska, Handy, Pozzi, and
  Scott]{blanchard2016}
Blanchard, G., Flaska, M., Handy, G., Pozzi, S., and Scott, C.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock \emph{Electron. J. Statist.}, 10\penalty0 (2):\penalty0 2780--2824,
  2016.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
Boucheron, S., Lugosi, G., and Massart, P.
\newblock \emph{Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem[{Cannings} et~al.(2018){Cannings}, {Fan}, and
  {Samworth}]{cannings2018}
{Cannings}, T.~I., {Fan}, Y., and {Samworth}, R.~J.
\newblock {Classification with imperfect training labels}.
\newblock \emph{ArXiv e-prints}, May 2018.

\bibitem[Chaudhuri \& Dasgupta(2014)Chaudhuri and Dasgupta]{chaudhuri2014rates}
Chaudhuri, K. and Dasgupta, S.
\newblock Rates of convergence for nearest neighbor classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3437--3445, 2014.

\bibitem[Dasgupta \& Kpotufe(2014)Dasgupta and Kpotufe]{DasKpo}
Dasgupta, S. and Kpotufe, S.
\newblock Optimal rates for k-nn density and mode estimation.
\newblock In \emph{Proceedings of the 27th International Conference on Neural
  Information Processing Systems - Volume 2}, NIPS'14, pp.\  2555--2563,
  Cambridge, MA, USA, 2014. MIT Press.

\bibitem[D{\"{o}}ring et~al.(2017)D{\"{o}}ring, Gy{\"{o}}rfi, and
  Walk]{doring2018}
D{\"{o}}ring, M., Gy{\"{o}}rfi, L., and Walk, H.
\newblock Rate of convergence of k-nearest-neighbor classification rule.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0
  227:1--227:16, 2017.

\bibitem[Elkan \& Noto(2008)Elkan and Noto]{ElkanNoto}
Elkan, C. and Noto, K.
\newblock Learning classifiers from only positive and unlabeled data.
\newblock In \emph{Proceedings of the 14th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '08, New York, NY, USA, 2008.
  ACM.

\bibitem[Fr{\'{e}}nay \& Verleysen(2014)Fr{\'{e}}nay and Verleysen]{Frenay1}
Fr{\'{e}}nay, B. and Verleysen, M.
\newblock Classification in the presence of label noise: {A} survey.
\newblock \emph{{IEEE} Trans. Neural Netw. Learning Syst.}, 25\penalty0
  (5):\penalty0 845--869, 2014.

\bibitem[Gao et~al.(2018)Gao, Niu, and Zhou]{Gao}
Gao, W., Niu, X., and Zhou, Z.
\newblock On the consistency of exact and approximate nearest neighbor with
  noisy data.
\newblock \emph{Arxiv}, abs/1607.07526, 2018.

\bibitem[Giné \& Nickl(2015)Giné and Nickl]{gine_nickl_2015}
Giné, E. and Nickl, R.
\newblock \emph{Mathematical Foundations of Infinite-Dimensional Statistical
  Models}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2015.

\bibitem[Inouye et~al.(2017)Inouye, Ravikumar, Das, and Dutta]{Inouye}
Inouye, D.~I., Ravikumar, P., Das, P., and Dutta, A.
\newblock Hyperparameter selection under localized label noise via corrupt
  validation.
\newblock In \emph{NIPS Workshop}. 2017.

\bibitem[Jiang(2019)]{Jiang}
Jiang, H.
\newblock Non-asymptotic uniform rates of consistency for k-nn regression.
\newblock In \emph{Proceedings of the 33rd AAAI Conference on Artificial
  Intelligence}. AAAI, 2019.

\bibitem[Jiang \& Kpotufe(2017)Jiang and Kpotufe]{jiang2017modal}
Jiang, H. and Kpotufe, S.
\newblock Modal-set estimation with an application to clustering.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1197--1206,
  2017.

\bibitem[Kpotufe(2011)]{Kpotufe}
Kpotufe, S.
\newblock k-nn regression adapts to local intrinsic dimension.
\newblock In Shawe-Taylor, J., Zemel, R.~S., Bartlett, P.~L., Pereira, F., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 24}, pp.\  729--737. 2011.

\bibitem[Mammen \& Tsybakov(1999)Mammen and Tsybakov]{mammen1999}
Mammen, E. and Tsybakov, A.~B.
\newblock Smooth discrimination analysis.
\newblock \emph{Ann. Statist.}, 27\penalty0 (6):\penalty0 1808--1829, 12 1999.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Menon, A., Van~Rooyen, B., Ong, C.~S., and Williamson, B.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  125--134, 2015.

\bibitem[Menon et~al.(2018)Menon, van Rooyen, and Natarajan]{MenonMLJ2018}
Menon, A.~K., van Rooyen, B., and Natarajan, N.
\newblock Learning from binary labels with instance-dependent noise.
\newblock \emph{Machine Learning}, 107\penalty0 (8):\penalty0 1561--1595, Sep
  2018.

\bibitem[Mitzenmacher \& Upfal(2005)Mitzenmacher and
  Upfal]{mitzenmacher2005probability}
Mitzenmacher, M. and Upfal, E.
\newblock \emph{Probability and computing: Randomized algorithms and
  probabilistic analysis}.
\newblock Cambridge university press, 2005.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1196--1204, 2013.

\bibitem[Natarajan et~al.(2018)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2018}
Natarajan, N., Dhillon, I.~S., Ravikumar, P., and Tewari, A.
\newblock Cost-sensitive learning with noisy labels.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (155):\penalty0 1--33, 2018.

\bibitem[Reeve \& Kaban(2019)Reeve and Kaban]{reeve2019COLT}
Reeve, H.~W. and Kaban, A.
\newblock Classification with unknown class conditional label noise on
  non-compact feature spaces.
\newblock \emph{Accepted to COLT}, 2019.

\bibitem[Scott(2015)]{scott2015rate}
Scott, C.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  838--846,
  2015.

\bibitem[Scott et~al.(2013)Scott, Blanchard, and
  Handy]{scott2013classification}
Scott, C., Blanchard, G., and Handy, G.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In \emph{Conference On Learning Theory}, pp.\  489--511, 2013.

\bibitem[Tarlow et~al.(2013)Tarlow, Swersky, Charlin, Sutskever, and
  Zemel]{Tarlow}
Tarlow, D., Swersky, K., Charlin, L., Sutskever, I., and Zemel, R.
\newblock Stochastic k-neighborhood selection for supervised and unsupervised
  learning.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  199--207, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.

\bibitem[Tsybakov(2008)]{Tsybakov}
Tsybakov, A.~B.
\newblock \emph{Introduction to Nonparametric Estimation}.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2008.

\end{thebibliography}
