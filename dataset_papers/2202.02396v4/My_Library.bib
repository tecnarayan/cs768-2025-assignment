
@inproceedings{fujimoto_off-policy_2019,
	title = {Off-{Policy} {Deep} {Reinforcement} {Learning} without {Exploration}},
	booktitle = {Proceeding of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Fujimoto, Scott and Meger, David and Precup, Doina},
	year = {2019},
	pages = {2052--2062},
	file = {Fujimoto et al. - 2019 - Off-Policy Deep Reinforcement Learning without Exp.pdf:/Users/samueletosatto/Zotero/storage/AV5A63A6/Fujimoto et al. - 2019 - Off-Policy Deep Reinforcement Learning without Exp.pdf:application/pdf},
}

@inproceedings{engel_reinforcement_2005,
	title = {Reinforcement {Learning} with {Gaussian} {Processes}},
	booktitle = {Proceedings of the 22nd {International} {Conference} {On} {Machine} {Learning}},
	publisher = {ACM},
	author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
	year = {2005},
	pages = {201--208},
	file = {Engel et al. - 2005 - Reinforcement Learning with Gaussian Processes.pdf:/Users/samueletosatto/Zotero/storage/X3EC665G/Engel et al. - 2005 - Reinforcement Learning with Gaussian Processes.pdf:application/pdf},
}

@inproceedings{taylor_kernelized_2009,
	series = {{ICML} '09},
	title = {Kernelized {Value} {Function} {Approximation} for {Reinforcement} {Learning}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553504},
	doi = {10.1145/1553374.1553504},
	abstract = {A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a modelbased approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also discuss the relationship between our modelbased approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.},
	urldate = {2019-10-02},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Taylor, Gavin and Parr, Ronald},
	year = {2009},
	note = {event-place: Montreal, Quebec, Canada},
	pages = {1017--1024},
	file = {ACM Full Text PDF:/Users/samueletosatto/Zotero/storage/VVZPYKGS/Taylor and Parr - 2009 - Kernelized Value Function Approximation for Reinfo.pdf:application/pdf},
}

@inproceedings{lu_non-delusional_2018,
	title = {Non-{Delusional} {Q}-learning and {Value}-{Iteration}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lu, Tyler and Schuurmans, Dale and Boutilier, Craig},
	year = {2018},
	pages = {9949--9959},
	file = {Lu et al. - 2018 - Non-Delusional Q-learning and Value-Iteration.pdf:/Users/samueletosatto/Zotero/storage/PVDN9ZJV/Lu et al. - 2018 - Non-Delusional Q-learning and Value-Iteration.pdf:application/pdf},
}

@inproceedings{liu_breaking_2018,
	title = {Breaking the {Curse} of {Horizon}: {Infinite}-{Horizon} {Off}-{Policy} {Estimation}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
	year = {2018},
	pages = {5356--5366},
	file = {Liu et al. - 2018 - Breaking the Curse of Horizon Infinite-Horizon Of.pdf:/Users/samueletosatto/Zotero/storage/VZYTLSA9/Liu et al. - 2018 - Breaking the Curse of Horizon Infinite-Horizon Of.pdf:application/pdf},
}

@inproceedings{shelton_policy_2001,
	series = {{UAI}'01},
	title = {Policy {Improvement} for {POMDPs} {Using} {Normalized} {Importance} {Sampling}},
	isbn = {978-1-55860-800-9},
	abstract = {We present a new method for estimating the expected return of a POMDP from experience. The estimator does not assume any knowledge of the POMDP, can estimate the returns for finite state controllers, allows experience to be gathered from arbitrary sequences of policies, and estimates the return for any new policy. We motivate the estimator from function-approximation and importance sampling points-of-view and derive its bias and variance. Although the estimator is biased, it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons. We conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to the REINFORCE algorithm showing an order of magnitude reduction in the number of trials required.},
	urldate = {2019-10-02},
	booktitle = {Proceedings of the {Seventeenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Shelton, Christian R.},
	year = {2001},
	note = {event-place: Seattle, Washington},
	pages = {496--503},
	file = {ACM Full Text PDF:/Users/samueletosatto/Zotero/storage/8ZMT3M2E/Shelton - 2001 - Policy Improvement for POMDPs Using Normalized Imp.pdf:application/pdf},
}

@article{nadaraya_estimating_1964,
	title = {On {Estimating} {Regression}},
	volume = {9},
	number = {1},
	journal = {Theory of Probability \& Its Applications},
	author = {Nadaraya, Elizbar A},
	year = {1964},
	pages = {141--142},
	file = {Nadaraya - 1964 - On Estimating Regression.pdf:/Users/samueletosatto/Zotero/storage/KECL7CGZ/Nadaraya - 1964 - On Estimating Regression.pdf:application/pdf},
}

@article{fan_design-adaptive_1992,
	title = {Design-{Adaptive} {Nonparametric} {Regression}},
	volume = {87},
	number = {420},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing},
	year = {1992},
	pages = {998--1004},
	file = {Fan - Design-adaptive Nonparametric Regression.pdf:/Users/samueletosatto/Zotero/storage/SIPXI2XR/Fan - Design-adaptive Nonparametric Regression.pdf:application/pdf},
}

@inproceedings{imani_off-policy_2018,
	title = {An {Off}-{Policy} {Policy} {Gradient} {Theorem} {Using} {Emphatic} {Weightings}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Imani, Ehsan and Graves, Eric and White, Martha},
	year = {2018},
	pages = {96--106},
	file = {NIPS Full Text PDF:/Users/samueletosatto/Zotero/storage/W3KXR7CK/Imani et al. - 2018 - An Off-policy Policy Gradient Theorem Using Emphat.pdf:application/pdf;NIPS Snapshot:/Users/samueletosatto/Zotero/storage/SBR2VDCI/7295-an-off-policy-policy-gradient-theorem-using-emphatic-weightings.html:text/html},
}

@inproceedings{riedmiller_neural_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Fitted} {Q} {Iteration} – {First} {Experiences} with a {Data} {Efficient} {Neural} {Reinforcement} {Learning} {Method}},
	isbn = {978-3-540-31692-3},
	abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
	language = {en},
	booktitle = {European {Conference} of {Machine} {Learning}},
	publisher = {Springer Berlin Heidelberg},
	author = {Riedmiller, Martin},
	year = {2005},
	pages = {317--328},
	file = {Springer Full Text PDF:/Users/samueletosatto/Zotero/storage/45D59KUQ/Riedmiller - 2005 - Neural Fitted Q Iteration – First Experiences with.pdf:application/pdf},
}

@inproceedings{kroemer_non-parametric_2011,
	title = {A {Non}-{Parametric} {Approach} to {Dynamic} {Programming}},
	url = {http://papers.nips.cc/paper/4182-a-non-parametric-approach-to-dynamic-programming.pdf},
	urldate = {2019-10-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kroemer, Oliver B. and Peters, Jan R.},
	year = {2011},
	pages = {1719--1727},
	file = {NIPS Full Text PDF:/Users/samueletosatto/Zotero/storage/SINYQPER/Kroemer and Peters - 2011 - A Non-Parametric Approach to Dynamic Programming.pdf:application/pdf;NIPS Snapshot:/Users/samueletosatto/Zotero/storage/FBQZSU33/4182-a-non-parametric-approach-to-dynamic-programming.html:text/html},
}

@incollection{metelli_policy_2018,
	title = {Policy {Optimization} via {Importance} {Sampling}},
	urldate = {2019-10-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Metelli, Alberto Maria and Papini, Matteo and Faccio, Francesco and Restelli, Marcello},
	year = {2018},
	pages = {5442--5454},
	file = {NIPS Full Text PDF:/Users/samueletosatto/Zotero/storage/F52ATEXA/Metelli et al. - 2018 - Policy Optimization via Importance Sampling.pdf:application/pdf;NIPS Snapshot:/Users/samueletosatto/Zotero/storage/S9P5D4VS/7789-policy-optimization-via-importance-sampling.html:text/html},
}

@inproceedings{deisenroth_pilco:_2011,
	series = {{ICML}'11},
	title = {{PILCO}: {A} {Model}-based and {Data}-efficient {Approach} to {Policy} {Search}},
	isbn = {978-1-4503-0619-5},
	shorttitle = {{PILCO}},
	url = {http://dl.acm.org/citation.cfm?id=3104482.3104541},
	abstract = {Loading...},
	urldate = {2019-10-02},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	year = {2011},
	note = {event-place: Bellevue, Washington, USA},
	pages = {465--472},
	file = {Deisenroth and Rasmussen - 2011 - PILCO A Model-based and Data-efficient Approach t.pdf:/Users/samueletosatto/Zotero/storage/J2G2YXJW/Deisenroth and Rasmussen - 2011 - PILCO A Model-based and Data-efficient Approach t.pdf:application/pdf},
}

@article{liu_off-policy_2019,
	title = {Off-{Policy} {Policy} {Gradient} with {State} {Distribution} {Correction}},
	url = {http://arxiv.org/abs/1904.08473},
	abstract = {We study the problem of off-policy policy optimization in Markov decision processes, and develop a novel off-policy policy gradient method. Prior off-policy policy gradient approaches have generally ignored the mismatch between the distribution of states visited under the behavior policy used to collect data, and what would be the distribution of states under the learned policy. Here we build on recent progress for estimating the ratio of the state distributions under behavior and evaluation policies for policy evaluation, and present an off-policy policy gradient optimization technique that can account for this mismatch in distributions. We present an illustrative example of why this is important and a theoretical convergence guarantee for our approach. Empirically, we compare our method in simulations to several strong baselines which do not correct for this mismatch, significantly improving in the quality of the policy discovered.},
	urldate = {2019-10-02},
	journal = {arXiv:1904.08473},
	author = {Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
	year = {2019},
	note = {arXiv: 1904.08473},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: to appear at UAI 18; camera-ready version},
	file = {arXiv\:1904.08473 PDF:/Users/samueletosatto/Zotero/storage/56Z3CKWU/Liu et al. - 2019 - Off-Policy Policy Gradient with State Distribution.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/6NADQ5ZH/1904.html:text/html},
}

@article{ecoffet_go-explore:_2019,
	title = {Go-{Explore}: a {New} {Approach} for {Hard}-{Exploration} {Problems}},
	shorttitle = {Go-{Explore}},
	url = {http://arxiv.org/abs/1901.10995},
	abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).},
	urldate = {2019-10-02},
	journal = {arXiv:1901.10995},
	author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	year = {2019},
	note = {arXiv: 1901.10995},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 37 pages, 14 figures; added references to Goyal et al. and Oh et al., updated reference to Colas et al},
	file = {arXiv\:1901.10995 PDF:/Users/samueletosatto/Zotero/storage/WCX8R2C2/Ecoffet et al. - 2019 - Go-Explore a New Approach for Hard-Exploration Pr.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/X9S659Q7/1901.html:text/html},
}

@article{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2019-10-02},
	journal = {arXiv:1606.01540},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv\:1606.01540 PDF:/Users/samueletosatto/Zotero/storage/PC92K4YB/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/WH8FKG5Y/1606.html:text/html},
}

@misc{dhariwal_openai_2017,
	title = {{OpenAI} {Baselines}},
	url = {https://github.com/openai/baselines},
	publisher = {GitHub},
	author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
	year = {2017},
}

@article{dhariwal_openai_2017-1,
	title = {Openai {Baselines}},
	journal = {GitHub, GitHub repository},
	author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
	year = {2017},
}

@article{williams_simple_1992,
	title = {Simple {Statistical} {Gradient}-{Following} {Algorithms} for {Connectionist} {Reinforcement} {Learning}},
	volume = {8},
	number = {3-4},
	journal = {Machine learning},
	author = {Williams, Ronald J},
	year = {1992},
	pages = {229--256},
	file = {Williams - 1992 - Simple Statistical Gradient-Following Algorithms f.pdf:/Users/samueletosatto/Zotero/storage/85DP76NJ/Williams - 1992 - Simple Statistical Gradient-Following Algorithms f.pdf:application/pdf},
}

@inproceedings{sutton_policy_2000,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
	year = {2000},
	pages = {1057--1063},
	file = {Sutton et al. - 2000 - Policy Gradient Methods for Reinforcement Learning.pdf:/Users/samueletosatto/Zotero/storage/PNM783B3/Sutton et al. - 2000 - Policy Gradient Methods for Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{lillicrap_continuous_2016,
	title = {Continuous {Control} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2019-10-02},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	year = {2016},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary},
	file = {arXiv\:1509.02971 PDF:/Users/samueletosatto/Zotero/storage/JIRFCUTT/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/8EU3EJVF/1509.html:text/html},
}

@article{mnih_human-level_2015,
	title = {Human-{Level} {Control} {Through} {Deep} {Reinforcement} {Learning}},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2019-10-02},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	year = {2015},
	keywords = {deep; general rl},
	pages = {529--533},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/Users/samueletosatto/Zotero/storage/S9BRUL5C/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@article{silver_mastering_2017,
	title = {Mastering the {Game} of {Go} without {Human} {Knowledge}},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2019-10-02},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	year = {2017},
	pages = {354--359},
	file = {Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:/Users/samueletosatto/Zotero/storage/78YKFC65/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@inproceedings{kober_policy_2009,
	title = {Policy {Search} for {Motor} {Primitives} in {Robotics}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kober, Jens and Peters, Jan R},
	year = {2009},
	pages = {849--856},
	file = {Kober and Peters - 2009 - Policy Search for Motor Primitives in Robotics.pdf:/Users/samueletosatto/Zotero/storage/3UMUJQV5/Kober and Peters - 2009 - Policy Search for Motor Primitives in Robotics.pdf:application/pdf},
}

@inproceedings{degris_off-policy_2012,
	title = {Off-{Policy} {Actor}-{Critic}},
	booktitle = {Proceedings of the 29th {International} {Coference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Degris, Thomas and White, Martha and Sutton, Richard S},
	year = {2012},
	pages = {179--186},
	file = {Degris et al. - 2012 - Off-Policy Actor-Critic.pdf:/Users/samueletosatto/Zotero/storage/HUUGD3YY/Degris et al. - 2012 - Off-Policy Actor-Critic.pdf:application/pdf},
}

@article{watson_smooth_1964,
	title = {Smooth {Regression} {Analysis}},
	journal = {Sankhyā: The Indian Journal of Statistics, Series A},
	author = {Watson, Geoffrey S},
	year = {1964},
	pages = {359--372},
	file = {Watson - 1964 - Smooth Regression Analysis.pdf:/Users/samueletosatto/Zotero/storage/ASJ5WQZW/Watson - 1964 - Smooth Regression Analysis.pdf:application/pdf},
}

@book{wasserman_all_2006,
	title = {All of {Nonparametric} {Statistics}},
	url = {https://books.google.it/books?hl=it&lr=&id=MRFlzQfRg7UC&oi=fnd&pg=PA2&dq=wasserman+2006+all&ots=SPSQp53XJz&sig=R9JPan0NnS8GkezXCj85U2ndFmc#v=onepage&q=wasserman%202006%20all&f=false},
	urldate = {2019-10-02},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2006},
	file = {All of Nonparametric Statistics - Larry Wasserman - Google Libri:/Users/samueletosatto/Zotero/storage/MLKMMYM9/books.html:text/html},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
	language = {en},
	number = {3},
	urldate = {2019-10-02},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Springer Full Text PDF:/Users/samueletosatto/Zotero/storage/8LSH6VNY/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{xu_kernel-based_2007,
	title = {Kernel-{Based} {Least} {Squares} {Policy} {Iteration} for {Reinforcement} {Learning}},
	volume = {18},
	number = {4},
	journal = {IEEE Transactions on Neural Networks},
	author = {Xu, Xin and Hu, Dewen and Lu, Xicheng},
	year = {2007},
	pages = {973--992},
	file = {Xu et al. - 2007 - Kernel-Based Least Squares Policy Iteration for Re.pdf:/Users/samueletosatto/Zotero/storage/DDC9SVMP/Xu et al. - 2007 - Kernel-Based Least Squares Policy Iteration for Re.pdf:application/pdf},
}

@inproceedings{silver_deterministic_2014,
	title = {Deterministic {Policy} {Gradient} {Algorithms}},
	abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efﬁciently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can signiﬁcantly outperform their stochastic counterparts in high-dimensional action spaces.},
	language = {en},
	booktitle = {Proceedings of the 31 st {International} {Conference} on {Machine} {Learning}},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	year = {2014},
	file = {Silver et al. - Deterministic Policy Gradient Algorithms.pdf:/Users/samueletosatto/Zotero/storage/HHPXCDSN/Silver et al. - Deterministic Policy Gradient Algorithms.pdf:application/pdf},
}

@inproceedings{peshkin_learning_2002,
	title = {Learning from {Scarce} {Experience}},
	url = {http://arxiv.org/abs/cs/0204043},
	abstract = {Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.},
	urldate = {2019-10-02},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Machine} {Learning}},
	author = {Peshkin, Leonid and Shelton, Christian R.},
	year = {2002},
	note = {arXiv: cs/0204043},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, G.1.6, I.2, I.2.11, I.2.6, I.2.8},
	annote = {Comment: 8 pages 4 figures},
	file = {arXiv\:cs/0204043 PDF:/Users/samueletosatto/Zotero/storage/RT8ISEGE/Peshkin and Shelton - 2002 - Learning from Scarce Experience.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/GQEEM86C/0204043.html:text/html},
}

@misc{noauthor_off-policy_nodate,
	title = {Off-policy actor-critic},
	url = {https://dl.acm.org/citation.cfm?id=3042600},
	urldate = {2019-10-02},
	file = {Off-policy actor-critic:/Users/samueletosatto/Zotero/storage/H6ASM93L/citation.html:text/html},
}

@misc{noauthor_design-adaptive_nodate,
	title = {Design-adaptive {Nonparametric} {Regression}: {Journal} of the {American} {Statistical} {Association}: {Vol} 87, {No} 420},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1992.10476255},
	urldate = {2019-10-02},
	file = {Design-adaptive Nonparametric Regression\: Journal of the American Statistical Association\: Vol 87, No 420:/Users/samueletosatto/Zotero/storage/5FVU68RZ/01621459.1992.html:text/html},
}

@misc{noauthor_kernel-based_nodate,
	title = {Kernel-{Based} {Least} {Squares} {Policy} {Iteration} for {Reinforcement} {Learning} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/abstract/document/4267723/},
	urldate = {2019-10-02},
	file = {Kernel-Based Least Squares Policy Iteration for Reinforcement Learning - IEEE Journals & Magazine:/Users/samueletosatto/Zotero/storage/M36NZQWA/4267723.html:text/html},
}

@book{borrelli_predictive_2017,
	title = {Predictive {Control} for {Linear} and {Hybrid} {Systems}},
	isbn = {978-1-108-15829-9},
	abstract = {Model Predictive Control (MPC), the dominant advanced control approach in industry over the past twenty-five years, is presented comprehensively in this unique book. With a simple, unified approach, and with attention to real-time implementation, it covers predictive control theory including the stability, feasibility, and robustness of MPC controllers. The theory of explicit MPC, where the nonlinear optimal feedback controller can be calculated efficiently, is presented in the context of linear systems with linear constraints, switched linear systems, and, more generally, linear hybrid systems. Drawing upon years of practical experience and using numerous examples and illustrative applications, the authors discuss the techniques required to design predictive control laws, including algorithms for polyhedral manipulations, mathematical and multiparametric programming and how to validate the theoretical properties and to implement predictive control policies. The most important algorithms feature in an accompanying free online MATLAB toolbox, which allows easy access to sample solutions. Predictive Control for Linear and Hybrid Systems is an ideal reference for graduate, postgraduate and advanced control practitioners interested in theory and/or implementation aspects of predictive control.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Borrelli, Francesco and Bemporad, Alberto and Morari, Manfred},
	month = jun,
	year = {2017},
	note = {Google-Books-ID: 7NUoDwAAQBAJ},
	keywords = {Mathematics / Linear \& Nonlinear Programming, Mathematics / Optimization, Technology \& Engineering / Chemical \& Biochemical},
}

@article{degris_off-policy_2012-1,
	title = {Off-{Policy} {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1205.4839},
	abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
	urldate = {2019-10-02},
	journal = {arXiv:1205.4839 [cs]},
	author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
	month = may,
	year = {2012},
	note = {arXiv: 1205.4839},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Full version of the paper, appendix and errata included; Proceedings of the 2012 International Conference on Machine Learning},
	file = {arXiv\:1205.4839 PDF:/Users/samueletosatto/Zotero/storage/62EX5G74/Degris et al. - 2012 - Off-Policy Actor-Critic.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/J7T2GTVC/1205.html:text/html},
}

@article{ernst_tree-based_2005,
	title = {Tree-{Based} {Batch} {Mode} {Reinforcement} {Learning}},
	volume = {6},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v6/ernst05a.html},
	number = {Apr},
	urldate = {2019-10-02},
	journal = {Journal of Machine Learning Research},
	author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
	year = {2005},
	pages = {503--556},
	file = {Full Text PDF:/Users/samueletosatto/Zotero/storage/KQL3YPM3/Ernst et al. - 2005 - Tree-Based Batch Mode Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/samueletosatto/Zotero/storage/U8I9XSTB/ernst05a.html:text/html},
}

@article{neu_unified_2017,
	title = {A unified view of entropy-regularized {Markov} decision processes},
	url = {http://arxiv.org/abs/1705.07798},
	abstract = {We propose a general framework for entropy-regularized average-reward reinforcement learning in Markov decision processes (MDPs). Our approach is based on extending the linear-programming formulation of policy optimization in MDPs to accommodate convex regularization functions. Our key result is showing that using the conditional entropy of the joint state-action distributions as regularization yields a dual optimization problem closely resembling the Bellman optimality equations. This result enables us to formalize a number of state-of-the-art entropy-regularized reinforcement learning algorithms as approximate variants of Mirror Descent or Dual Averaging, and thus to argue about the convergence properties of these methods. In particular, we show that the exact version of the TRPO algorithm of Schulman et al. (2015) actually converges to the optimal policy, while the entropy-regularized policy gradient methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally, we illustrate empirically the effects of using various regularization techniques on learning performance in a simple reinforcement learning setup.},
	urldate = {2018-12-11},
	journal = {arXiv:1705.07798 [cs, stat]},
	author = {Neu, Gergely and Jonsson, Anders and Gómez, Vicenç},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07798},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1705.07798 PDF:/Users/samueletosatto/Zotero/storage/C3WNHZZK/Neu et al. - 2017 - A unified view of entropy-regularized Markov decis.pdf:application/pdf;arXiv.org Snapshot:/Users/samueletosatto/Zotero/storage/NLDU5EEF/1705.html:text/html},
}

@article{fox_taming_nodate,
	title = {Taming the {Noise} in {Reinforcement} {Learning} via {Soft} {Updates}},
	abstract = {Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in signiﬁcant improvements of the convergence rate and the cost of the learning process.},
	language = {en},
	author = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
	pages = {10},
	file = {Fox et al. - Taming the Noise in Reinforcement Learning via Sof.pdf:/Users/samueletosatto/Zotero/storage/2V4AVB64/Fox et al. - Taming the Noise in Reinforcement Learning via Sof.pdf:application/pdf},
}

@inproceedings{kumar_stabilizing_2019,
	address = {Vancouver, Canada},
	title = {Stabilizing {Off}-{Policy} {Q}-{Learning} via {Bootstrapping} {Error} {Reduction}},
	booktitle = {Proceedings of the 33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
	year = {2019},
}

@article{kidambi_morel_2020,
	title = {{MOReL}: {Model}-based {Offline} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2005.05951},
	author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
	year = {2020},
}

@inproceedings{argenson_model-based_2020,
	title = {Model-{Based} {Offline} {Planning}},
	booktitle = {Proceeding of the 9th {International} {Conference} on {Learning} {Representations}},
	author = {Argenson, Arthur and Dulac-Arnold, Gabriel},
	year = {2020},
}

@inproceedings{yu_mopo_2020,
	title = {{MOPO}: {Model}-based {Offline} {Policy} {Optimization}},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
	year = {2020},
}

@article{fu_d4rl_2020,
	title = {D4rl: {Datasets} for {Deep} {Data}-{Driven} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2004.07219},
	author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
	year = {2020},
}

@article{kumar_stabilizing_2019-1,
	title = {Stabilizing {Off}-{Policy} {Q}-{Learning} via {Bootstrapping} {Error} {Reduction}},
	journal = {arXiv preprint arXiv:1906.00949},
	author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
	year = {2019},
}

@article{wu_behavior_2019,
	title = {Behavior {Regularized} {Offline} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:1911.11361},
	author = {Wu, Yifan and Tucker, George and Nachum, Ofir},
	year = {2019},
}

@article{kumar_conservative_2020,
	title = {Conservative {Q}-{Learning} for {Offline} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2006.04779},
	author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
	year = {2020},
}

@article{peters_natural_2008,
	title = {Natural {Actor}-{Critic}},
	volume = {71},
	number = {7-9},
	journal = {Neurocomputing},
	author = {Peters, J and Schaal, S},
	year = {2008},
	note = {Publisher: Elsevier},
	pages = {1180--1190},
}

@inproceedings{kakade_natural_2001,
	title = {A {Natural} {Policy} {Gradient}},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Kakade, Sham},
	year = {2001},
	pages = {1531--1538},
}

@inproceedings{antos_fitted_2007,
	title = {Fitted {Q}-{Iteration} in {Continuous} {Action}-{Space} {MDPs}},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Antos, Andras and Munos, Rémi and Szepesvari, Csaba},
	year = {2007},
}

@article{wood_estimation_1996,
	title = {Estimation of the {Lipschitz} {Constant} of a {Function}},
	volume = {8},
	number = {1},
	journal = {Journal of Global Optimization},
	author = {Wood, GR and Zhang, BP},
	year = {1996},
	note = {Publisher: Springer},
	pages = {91--103},
}

@inproceedings{kumaraswamy_context-dependent_2018,
	title = {Context-{Dependent} {Upper}-{Confidence} {Bounds} for {Directed} {Exploration}},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Kumaraswamy, Raksha and Schlegel, Matthew and White, Adam and White, Martha},
	year = {2018},
	pages = {4784--4794},
}

@inproceedings{noauthor_notitle_nodate,
}

@inproceedings{szita_many_2008,
	title = {The {Many} {Faces} of {Optimism}: a {Unifying} {Approach}},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	author = {Szita, István and Lorincz, András},
	year = {2008},
	pages = {1048--1055},
}

@article{clements_estimating_2019,
	title = {Estimating {Risk} and {Uncertainty} in {Deep} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:1905.09638},
	author = {Clements, William R and Robaglia, Benoît-Marie and Van Delft, Bastien and Slaoui, Reda Bahi and Toth, Sébastien},
	year = {2019},
}

@article{garcia_comprehensive_2015,
	title = {A {Comprehensive} {Survey} on {Safe} {Reinforcement} {Learning}},
	volume = {16},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Garcia, Javier and Fernández, Fernando},
	year = {2015},
	pages = {1437--1480},
}

@inproceedings{hadfield-menell_inverse_2017,
	title = {Inverse {Reward} {Design}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
	year = {2017},
	pages = {6765--6774},
}

@inproceedings{hadfield-menell_cooperative_2016,
	title = {Cooperative {Inverse} {Reinforcement} {Learning}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
	year = {2016},
	pages = {3909--3917},
}

@article{nota_is_2019,
	title = {Is the {Policy} {Gradient} a {Gradient}?},
	journal = {arXiv preprint arXiv:1906.07073},
	author = {Nota, Chris and Thomas, Philip S},
	year = {2019},
}

@article{ude_task-specific_2010,
	title = {Task-{Specific} {Generalization} of {Discrete} and {Periodic} {Dynamic} {Movement} {Primitives}},
	volume = {26},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Ude, Aleš and Gams, Andrej and Asfour, Tamim and Morimoto, Jun},
	year = {2010},
	note = {Publisher: IEEE},
	pages = {800--815},
}

@inproceedings{osband_more_2013,
	title = {({More}) {Efficient} {Reinforcement} {Learning} via {Posterior} {Sampling}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
	year = {2013},
	pages = {3003--3011},
}

@article{opitz_popular_1999,
	title = {Popular {Ensemble} {Methods}: {An} {Empirical} {Study}},
	volume = {11},
	journal = {Journal of Artificial Intelligence Research},
	author = {Opitz, David and Maclin, Richard},
	year = {1999},
	pages = {169--198},
}

@inproceedings{odonoghue_uncertainty_2018,
	title = {The {Uncertainty} {Bellman} {Equation} and {Exploration}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {O’Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
	year = {2018},
	pages = {3836--3845},
}

@article{wilde_improving_2020,
	title = {Improving {User} {Specifications} for {Robot} {Behavior} {Through} {Active} {Preference} {Learning}: {Framework} and {Evaluation}},
	volume = {39},
	number = {6},
	journal = {The International Journal of Robotics Research},
	author = {Wilde, Nils and Blidaru, Alexandru and Smith, Stephen L and Kulić, Dana},
	year = {2020},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {651--667},
}

@article{tosatto_technical_2018,
	title = {Technical {Report}:“{Exploration} {Driven} by an {Optimistic} {Bellman} {Equation}”},
	author = {Tosatto, Samuele and D’eramo, Carlo and Pajarinen, Joni and Restelli, Marcello and Peters, Jan},
	year = {2018},
}

@inproceedings{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Machine} {Learning}},
	author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	year = {2016},
	pages = {1928--1937},
}

@book{gyorfi_distribution-free_2006,
	title = {A {Distribution}-{Free} {Theory} of {Nonparametric} {Regression}},
	publisher = {Springer Science \& Business Media},
	author = {Györfi, László and Kohler, Michael and Krzyzak, Adam and Walk, Harro},
	year = {2006},
}

@inproceedings{grande_sample_2014,
	title = {Sample {Efficient} {Reinforcement} {Learning} with {Gaussian} {Processes}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Grande, Robert and Walsh, Thomas and How, Jonathan},
	year = {2014},
	pages = {1332--1340},
}

@inproceedings{dearden_bayesian_1998,
	title = {Bayesian {Q}-{Learning}},
	booktitle = {{AAAI}},
	author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
	year = {1998},
	pages = {761--768},
}

@article{melo_convergence_2001,
	title = {Convergence of {Q}-{Learning}: {A} {Simple} {Proof}},
	journal = {Institute Of Systems and Robotics, Tech. Rep},
	author = {Melo, Francisco S},
	year = {2001},
	pages = {1--4},
}

@incollection{marcus_risk_1997,
	title = {Risk {Sensitive} {Markov} {Decision} {Processes}},
	booktitle = {Systems and {Control} in the {Twenty}-{First} {Century}},
	publisher = {Springer},
	author = {Marcus, Steven I and Fernández-Gaucherand, Emmanual and Hernández-Hernandez, Daniel and Coraluppi, Stefano and Fard, Pedram},
	year = {1997},
	pages = {263--279},
}

@article{kroemer_review_2019,
	title = {A {Review} of {Robot} {Learning} for {Manipulation}: {Challenges}, {Representations}, and {Algorithms}},
	journal = {arXiv preprint arXiv:1907.03146},
	author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
	year = {2019},
}

@inproceedings{klink_self-paced_2020,
	title = {Self-{Paced} {Contextual} {Reinforcement} {Learning}},
	booktitle = {Conference on {Robot} {Learning}},
	author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and Peters, Jan},
	year = {2020},
	pages = {513--529},
}

@inproceedings{kang_policy_2018,
	title = {Policy {Optimization} with {Demonstrations}},
	booktitle = {Proceeding of the 36th {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Kang, Bingyi and Jie, Zequn and Feng, Jiashi},
	year = {2018},
	pages = {2469--2478},
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement {Learning}: {A} {Survey}},
	volume = {4},
	journal = {Journal of Artificial Intelligence Research},
	author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
	year = {1996},
	pages = {237--285},
}

@inproceedings{jaakkola_convergence_1994,
	title = {Convergence of {Stochastic} {Iterative} {Dynamic} {Programming} {Algorithms}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
	year = {1994},
	pages = {703--710},
}

@article{howard_risk-sensitive_1972,
	title = {Risk-{Sensitive} {Markov} {Decision} {Processes}},
	volume = {18},
	number = {7},
	journal = {Management Science},
	author = {Howard, Ronald A and Matheson, James E},
	year = {1972},
	note = {Publisher: INFORMS},
	pages = {356--369},
}

@inproceedings{balakrishna_-policy_2020,
	title = {On-{Policy} {Robot} {Imitation} {Learning} {From} a {Converging} {Supervisor}},
	booktitle = {Conference on {Robot} {Learning}},
	author = {Balakrishna, Ashwin and Thananjeyan, Brijen and Lee, Jonathan and Li, Felix and Zahed, Arsh and Gonzalez, Joseph E and Goldberg, Ken},
	year = {2020},
	pages = {24--41},
}

@inproceedings{delgado-guerrero_sample-efficient_2020,
	title = {Sample-{Efficient} {Robot} {Motion} {Learning} {Using} {Gaussian} {Process} {Latent} {Variable} {Models}},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Delgado-Guerrero, Juan Antonio and Colomé, Adrià and Torras, Carme},
	year = {2020},
	pages = {314--320},
}

@book{deisenroth_survey_2013,
	title = {A {Survey} on {Policy} {Search} for {Robotics}},
	publisher = {Now Publishers},
	author = {Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan},
	year = {2013},
}

@inproceedings{cheng_fast_2018,
	title = {Fast and {Accurate} {Online} {Video} {Object} {Segmentation} via {Tracking} {Parts}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cheng, Jingchun and Tsai, Yi-Hsuan and Hung, Wei-Chih and Wang, Shengjin and Yang, Ming-Hsuan},
	year = {2018},
	pages = {7415--7424},
}

@inproceedings{dermy_prediction_2018,
	title = {Prediction of {Human} {Whole}-{Body} {Movements} with {AE}-{ProMPs}},
	booktitle = {2018 {IEEE}-{RAS} 18th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	publisher = {IEEE},
	author = {Dermy, Oriane and Chaveroche, Maxime and Colas, Francis and Charpillet, François and Ivaldi, Serena},
	year = {2018},
	pages = {572--579},
}

@techreport{calinon_learning_2007,
	title = {Learning of {Gestures} by {Imitation} in a {Humanoid} {Robot}},
	institution = {Cambridge University Press},
	author = {Calinon, Sylvain and Billard, Aude},
	year = {2007},
}

@article{ijspeert_dynamical_2013,
	title = {Dynamical {Movement} {Primitives}: {Learning} {Attractor} {Models} for {Motor} {Behaviors}},
	volume = {25},
	number = {2},
	journal = {Neural Computation},
	author = {Ijspeert, Auke Jan and Nakanishi, Jun and Hoffmann, Heiko and Pastor, Peter and Schaal, Stefan},
	year = {2013},
	note = {Publisher: MIT Press},
	pages = {328--373},
}

@inproceedings{balakrishna_-policy_2020-1,
	title = {On-{Policy} {Robot} {Imitation} {Learning} {From} a {Converging} {Supervisor}},
	booktitle = {Conference on {Robot} {Learning}},
	author = {Balakrishna, Ashwin and Thananjeyan, Brijen and Lee, Jonathan and Li, Felix and Zahed, Arsh and Gonzalez, Joseph E and Goldberg, Ken},
	year = {2020},
	pages = {24--41},
}

@inproceedings{atkeson_robot_1997,
	title = {Robot {Learning} {From} {Demonstration}},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Atkeson, Christopher G and Schaal, Stefan},
	year = {1997},
	pages = {12--20},
}

@article{jaksch_near-optimal_2010,
	title = {Near-{Optimal} {Regret} {Bounds} for {Reinforcement} {Learning}.},
	volume = {11},
	number = {4},
	journal = {Journal of Machine Learning Research},
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	year = {2010},
}

@article{kearns_near-optimal_2002,
	title = {Near-{Optimal} {Reinforcement} {Learning} in {Polynomial} {Time}},
	volume = {49},
	number = {2-3},
	journal = {Machine learning},
	author = {Kearns, Michael and Singh, Satinder},
	year = {2002},
	note = {Publisher: Springer},
	pages = {209--232},
}

@article{lai_asymptotic_1995,
	title = {Asymptotic {Normality} of a {Class} of {Adaptive} {Statistics} with {Applications} to {Synthetic} {Data} {Methods} for {Censored} {Regression}},
	volume = {52},
	number = {2},
	journal = {Journal of Multivariate analysis},
	author = {Lai, Tze Leung and Ying, ZL and Zheng, ZK},
	year = {1995},
	note = {Publisher: Elsevier},
	pages = {259--279},
}

@article{brafman_r-max_2002,
	title = {R-{Max}: {A} {General} {Polynomial} {Time} {Algorithm} for {Near}-{Optimal} {Reinforcement} {Learning}},
	volume = {3},
	number = {Oct},
	journal = {Journal of Machine Learning Research},
	author = {Brafman, Ronen I and Tennenholtz, Moshe},
	year = {2002},
	pages = {213--231},
}

@inproceedings{bitzer_using_2010,
	title = {Using {Dimensionality} {Reduction} to {Exploit} {Constraints} in {Reinforcement} {Learning}},
	booktitle = {2010 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Bitzer, Sebastian and Howard, Matthew and Vijayakumar, Sethu},
	year = {2010},
	pages = {3219--3225},
}

@article{ruszczynski_risk-averse_2010,
	title = {Risk-{Averse} {Dynamic} {Programming} for {Markov} {Decision} {Processes}},
	volume = {125},
	number = {2},
	journal = {Mathematical Programming},
	author = {Ruszczyński, Andrzej},
	year = {2010},
	note = {Publisher: Springer},
	pages = {235--261},
}

@article{ruszczynski_risk-averse_2010-1,
	title = {Risk-{Averse} dynamic programming for {Markov} decision processes},
	volume = {125},
	number = {2},
	journal = {Mathematical programming},
	author = {Ruszczyński, Andrzej},
	year = {2010},
	note = {Publisher: Springer},
	pages = {235--261},
}

@inproceedings{sutton_generalization_1996,
	title = {Generalization in {Reinforcement} {Learning}: {Successful} {Examples} {Using} {Sparse} {Coarse} {Coding}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sutton, Richard S},
	year = {1996},
	pages = {1038--1044},
}

@inproceedings{theodorou_reinforcement_2010,
	title = {Reinforcement {Learning} of {Motor} {Skills} in {High} {Dimensions}: {A} {Path} {Integral} {Approach}},
	booktitle = {2010 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
	year = {2010},
	pages = {2397--2403},
}

@inproceedings{asadi_alternative_2017,
	title = {An {Alternative} {Softmax} {Operator} for {Reinforcement} {Learning}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Asadi, Kavosh and Littman, Michael L},
	year = {2017},
	pages = {243--252},
}

@inproceedings{abdolmaleki_contextual_2015,
	title = {Contextual {Policy} {Search} for {Generalizing} a {Parameterized} {Biped} {Walking} {Controller}},
	booktitle = {2015 {IEEE} {International} {Conference} on {Autonomous} {Robot} {Systems} and {Competitions}},
	publisher = {IEEE},
	author = {Abdolmaleki, Abbas and Lau, Nuno and Reis, Luis Paulo and Peters, Jan and Neumann, Gerhard},
	year = {2015},
	pages = {17--22},
}

@inproceedings{white_interval_2010,
	title = {Interval {Estimation} for {Reinforcement}-{Learning} {Algorithms} in {Continuous}-{State} {Domains}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {White, Martha and White, Adam},
	year = {2010},
	pages = {2433--2441},
}

@inproceedings{schmidhuber_driven_2008,
	title = {Driven by {Compression} {Progress}: {A} {Simple} {Principle} {Explains} {Essential} {Aspects} of {Subjective} {Beauty}, {Novelty}, {Surprise}, {Interestingness}, {Attention}, {Curiosity}, {Creativity}, {Art}, {Science}, {Music}, {Jokes}},
	booktitle = {Workshop on anticipatory behavior in adaptive learning systems},
	publisher = {Springer},
	author = {Schmidhuber, Jürgen},
	year = {2008},
	pages = {48--76},
}

@article{chen_ucb_2017,
	title = {{UCB} {Exploration} via {Q}-{Ensembles}},
	journal = {arXiv preprint arXiv:1706.01502},
	author = {Chen, Richard Y and Sidor, Szymon and Abbeel, Pieter and Schulman, John},
	year = {2017},
}

@article{meuleau_exploration_1999,
	title = {Exploration of {Multi}-{State} {Environments}: {Local} {Measures} and {Back}-{Propagation} of {Uncertainty}},
	volume = {35},
	number = {2},
	journal = {Machine Learning},
	author = {Meuleau, Nicolas and Bourgine, Paul},
	year = {1999},
	note = {Publisher: Springer},
	pages = {117--154},
}

@inproceedings{even-dar_convergence_2002,
	title = {Convergence of {Optimistic} and {Incremental} {Q}-{Learning}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Even-Dar, Eyal and Mansour, Yishay},
	year = {2002},
	pages = {1499--1506},
}

@inproceedings{pathak_curiosity-driven_2017,
	title = {Curiosity-{Driven} {Exploration} by {Self}-{Supervised} {Prediction}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
	year = {2017},
	pages = {16--17},
}

@inproceedings{ostrovski_count-based_2017,
	title = {Count-{Based} {Exploration} with {Neural} {Density} {Models}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Ostrovski, Georg and Bellemare, Marc G and Oord, Aäron and Munos, Rémi},
	year = {2017},
	pages = {2721--2730},
}

@inproceedings{bellemare_unifying_2016,
	title = {Unifying {Count}-{Based} {Exploration} and {Intrinsic} {Motivation}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	year = {2016},
	pages = {1471--1479},
}

@article{chentanez_intrinsically_2004,
	title = {Intrinsically {Motivated} {Reinforcement} {Learning}},
	volume = {17},
	journal = {Advances in Neural Information Processing Systems (NIPS)},
	author = {Chentanez, Nuttapong and Barto, Andrew and Singh, Satinder},
	year = {2004},
	pages = {1281--1288},
}

@inproceedings{strens_bayesian_2000,
	title = {A {Bayesian} {Framework} for {Reinforcement} {Learning}},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Strens, Malcolm JA},
	year = {2000},
	pages = {943--950},
}

@inproceedings{osband_deep_2016,
	title = {Deep {Exploration} via {Bootstrapped} {DQN}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	year = {2016},
	pages = {4026--4034},
}

@inproceedings{azizzadenesheli_efficient_2018,
	title = {Efficient {Exploration} {Through} {Bayesian} {Deep} {Q}-{Networks}},
	booktitle = {2018 {Information} {Theory} and {Applications} {Workshop} ({ITA})},
	publisher = {IEEE},
	author = {Azizzadenesheli, Kamyar and Brunskill, Emma and Anandkumar, Animashree},
	year = {2018},
	pages = {1--9},
}

@incollection{vlassis_bayesian_2012,
	title = {Bayesian {Reinforcement} {Learning}},
	booktitle = {Reinforcement learning},
	publisher = {Springer},
	author = {Vlassis, Nikos and Ghavamzadeh, Mohammad and Mannor, Shie and Poupart, Pascal},
	year = {2012},
	pages = {359--386},
}

@book{rummery_-line_1994,
	title = {On-{Line} {Q}-{Learning} using {Connectionist} {Systems}},
	volume = {37},
	publisher = {University of Cambridge, Department of Engineering Cambridge, UK},
	author = {Rummery, Gavin A and Niranjan, Mahesan},
	year = {1994},
}

@article{tipping_probabilistic_1999,
	title = {Probabilistic {Principal} {Component} {Analysis}},
	volume = {61},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tipping, Michael E and Bishop, Christopher M},
	year = {1999},
	note = {Publisher: Wiley Online Library},
	pages = {611--622},
}

@article{hotelling_analysis_1933,
	title = {Analysis of a {Complex} of {Statistical} {Variables} into {Principal} {Components}.},
	volume = {24},
	number = {6},
	journal = {Journal of educational psychology},
	author = {Hotelling, Harold},
	year = {1933},
	note = {Publisher: Warwick \& York},
	pages = {417},
}

@incollection{jolliffe_principal_1986,
	title = {Principal {Component} {Analysis}},
	booktitle = {Principal {Component} {Analysis}},
	publisher = {Springer},
	author = {Jolliffe, Ian T},
	year = {1986},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: {A} {2D} {Graphics} {Environment}},
	volume = {9},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, John D},
	year = {2007},
	note = {Publisher: IEEE Computer Society},
	pages = {90--95},
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in pytorch},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
}

@inproceedings{abadi_tensorflow_2016,
	title = {Tensorflow: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
	booktitle = {12th {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDIS})},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and {others}},
	year = {2016},
	pages = {265--283},
}

@book{bressert_scipy_2012,
	title = {{SciPy} and {NumPy}: an {Overview} for {Developers}},
	publisher = {" O'Reilly Media, Inc."},
	author = {Bressert, Eli},
	year = {2012},
}

@article{oliphant_python_2007,
	title = {Python for {Scientific} {Computing}},
	volume = {9},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Oliphant, Travis E},
	year = {2007},
	note = {Publisher: IEEE},
	pages = {10--20},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-{Learn}: {Machine} learning in {Python}},
	volume = {12},
	journal = {the Journal of machine Learning research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and {others}},
	year = {2011},
	note = {Publisher: JMLR. org},
	pages = {2825--2830},
}

@article{paszke_automatic_2017-1,
	title = {Automatic {Differentiation} in {PyTorch}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
}

@book{bishop_pattern_2006,
	title = {Pattern {Recognition} and {Machine} {Learning}},
	publisher = {springer},
	author = {Bishop, Christopher M},
	year = {2006},
}

@inproceedings{carpentier_upper-confidence-bound_2011,
	title = {Upper-{Confidence}-{Bound} {Algorithms} for {Active} {Learning} in {Multi}-{Armed} {Bandits}},
	booktitle = {International {Conference} on {Algorithmic} {Learning} {Theory}},
	publisher = {Springer},
	author = {Carpentier, Alexandra and Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, Rémi and Auer, Peter},
	year = {2011},
	pages = {189--203},
}

@article{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	journal = {arXiv preprint arXiv:2005.01643},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	year = {2020},
}

@article{tosatto_contextual_2020,
	title = {Contextual {Latent}-{Movements} {Off}-{Policy} {Optimization} for {Robotic} {Manipulation} {Skills}},
	journal = {arXiv preprint arXiv:2010.13766},
	author = {Tosatto, Samuele and Chalvatzaki, Georgia and Peters, Jan},
	year = {2020},
}

@book{lee_nonlinear_2007,
	title = {Nonlinear {Dimensionality} {Reduction}},
	publisher = {Springer Science \& Business Media},
	author = {Lee, John A and Verleysen, Michel},
	year = {2007},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	publisher = {MIT press},
	author = {Sutton, Richard S and Barto, Andrew G},
	year = {2018},
}

@article{cheng_rise_2019,
	title = {The {Rise} of {Robots} in {China}},
	volume = {33},
	number = {2},
	journal = {Journal of Economic Perspectives},
	author = {Cheng, Hong and Jia, Ruixue and Li, Dandan and Li, Hongbin},
	year = {2019},
	pages = {71--88},
}

@article{silver_general_2018,
	title = {A {General} {Reinforcement} {Learning} {Algorithm} that {Masters} {Chess}, {Shogi}, and {Go} {Through} {Self}-{Play}},
	volume = {362},
	number = {6419},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and {others}},
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1140--1144},
}

@article{james_rlbench_2020,
	title = {{RLBench}: {The} {Robot} {Learning} {Benchmark} \& {Learning} {Environment}},
	volume = {5},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {James, Stephen and Ma, Zicong and Arrojo, David Rovick and Davison, Andrew J},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {3019--3026},
}

@article{tipping_mixtures_1999,
	title = {Mixtures of {Probabilistic} {Principal} {Component} {Analyzers}},
	volume = {11},
	number = {2},
	journal = {Neural Computation},
	author = {Tipping, Michael E and Bishop, Christopher M},
	year = {1999},
	pages = {443--482},
}

@article{colome_dimensionality_2018,
	title = {Dimensionality {Reduction} in {Learning} {Gaussian} {Mixture} {Models} of {Movement} {Primitives} for {Contextualized} {Action} {Selection} and {Adaptation}},
	volume = {3},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Colomé, Adrià and Torras, Carme},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {3922--3929},
}

@inproceedings{riedmiller_neural_2005-1,
	title = {Neural {Fitted} {Q} iteration–{First} {Experiences} with a {Data} {Efficient} {Neural} {Reinforcement} {Learning} {Method}},
	booktitle = {European {Conference} on {Machine} {Learning}},
	publisher = {Springer},
	author = {Riedmiller, Martin},
	year = {2005},
	pages = {317--328},
}

@book{deramo_mushroomrl_2020,
	title = {{MushroomRL}: {Simplifying} {Reinforcement} {Learning} {Research}},
	url = {https://github.com/MushroomRL/mushroom-rl},
	author = {D'Eramo, Carlo and Tateo, Davide and Bonarini, Andrea and Restelli, Marcello and Peters, Jan},
	year = {2020},
	note = {Publication Title: arXiv preprint arXiv:2001.01102},
}

@inproceedings{tosatto_nonparametric_2020,
	address = {Palermo, Italy},
	title = {A {Nonparametric} {Off}-{Policy} {Policy} {Gradient}},
	abstract = {Reinforcement learning (RL) algorithms still suffer from high sample complexity despite outstanding recent successes. The need for intensive interactions with the environment is especially observed in many widely popular policy gradient algorithms that perform updates using on-policy samples. The price of such inefficiency becomes evident in real-world scenarios such as interaction-driven robot learning, where the success of RL has been rather limited. We address this issue by building on the general sample efficiency of off-policy algorithms. With nonparametric regression and density estimation methods we construct a nonparametric Bellman equation in a principled manner, which allows us to obtain closed-form estimates of the value function, and to analytically express the full policy gradient. We provide a theoretical analysis of our estimate to show that it is consistent under mild smoothness assumptions and empirically show that our approach has better sample efficiency than state-of-the-art policy gradient methods.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Tosatto, Samuele and Carvalho, Joao and Abdulsamad, Hany and Peters, Jan},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	year = {2020},
	file = {Tosatto et al. - 2020 - A Nonparametric Off-Policy Policy Gradient.pdf:/Users/samueletosatto/Zotero/storage/W6VGGMGP/Tosatto et al. - 2020 - A Nonparametric Off-Policy Policy Gradient.pdf:application/pdf},
}

@article{baxter_infinite-horizon_2001,
	title = {Infinite-{Horizon} {Policy}-{Gradient} {Estimation}},
	volume = {15},
	journal = {Journal of Artificial Intelligence Research},
	author = {Baxter, Jonathan and Bartlett, Peter L},
	year = {2001},
	pages = {319--350},
}

@incollection{chua_deep_2018,
	title = {Deep {Reinforcement} {Learning} in a {Handful} of {Trials} using {Probabilistic} {Dynamics} {Models}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
	year = {2018},
	pages = {4754--4765},
}

@inproceedings{nota_is_2020,
	title = {Is the {Policy} {Gradient} a {Gradient}?},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	author = {Nota, Chris and Thomas, Philip S},
	year = {2020},
}

@inproceedings{thomas_bias_2014,
	title = {Bias in {Natural} {Actor}-{Critic} {Algorithms}},
	booktitle = {Proceeding of the 31st {International} {Conference} on {Machine} {Learning}},
	author = {Thomas, Philip},
	year = {2014},
	pages = {441--448},
}

@article{sutton_emphatic_2016,
	title = {An {Emphatic} {Approach} to the {Problem} of {Off}-{Policy} {Temporal}-{Difference} {Learning}},
	volume = {17},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Sutton, Richard S and Mahmood, A Rupam and White, Martha},
	year = {2016},
	note = {Publisher: JMLR. org},
	pages = {2603--2631},
}

@article{mahmood_emphatic_2015,
	title = {Emphatic {Temporal}-{Difference} {Learning}},
	journal = {arXiv preprint arXiv:1507.01569},
	author = {Mahmood, A Rupam and Yu, Huizhen and White, Martha and Sutton, Richard S},
	year = {2015},
}

@book{rubinstein_simulation_2016,
	title = {Simulation and the {Monte} {Carlo} {Method}},
	volume = {10},
	publisher = {John Wiley \& Sons},
	author = {Rubinstein, Reuven Y and Kroese, Dirk P},
	year = {2016},
}

@article{shelton_policy_2013,
	title = {Policy {Improvement} for {POMDPs} {Using} {Normalized} {Importance} {Sampling}},
	journal = {arXiv preprint arXiv:1301.2310},
	author = {Shelton, Christian R},
	year = {2013},
}

@article{fujimoto_addressing_2018,
	title = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
	volume = {80},
	journal = {Journal of Machine Learning Research},
	author = {Fujimoto, S and van Hoof, H and Meger, D},
	year = {2018},
}

@inproceedings{jie_connection_2010,
	title = {On a {Connection} {Between} {Importance} {Sampling} and the {Likelihood} {Ratio} {Policy} {Gradient}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jie, Tang and Abbeel, Pieter},
	year = {2010},
	pages = {1000--1008},
}

@inproceedings{peters_relative_2010,
	title = {Relative {Entropy} {Policy} {Search}},
	booktitle = {Twenty-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Peters, Jan and Mulling, Katharina and Altun, Yasemin},
	year = {2010},
}

@techreport{meuleau_exploration_2001,
	title = {Exploration in {Gradient}-{Based} {Reinforcement} {Learning}},
	url = {https://dspace.mit.edu/handle/1721.1/6076},
	urldate = {2019-10-02},
	institution = {Massachusetts Institute of Technology},
	author = {Meuleau, Nicolas and Peshkin, Leonid and Kim, Kee-Eung},
	year = {2001},
	file = {Full Text PDF:/Users/samueletosatto/Zotero/storage/3Q2C4QIR/Meuleau et al. - 2001 - Exploration in Gradient-Based Reinforcement Learni.pdf:application/pdf;Snapshot:/Users/samueletosatto/Zotero/storage/5A9K5ACR/6076.html:text/html},
}

@inproceedings{white_unifying_2017,
	title = {Unifying {Task} {Specification} in {Reinforcement} {Learning}},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR. org},
	author = {White, Martha},
	year = {2017},
	pages = {3742--3750},
}

@article{davella_combinations_2003,
	title = {Combinations of {Muscle} {Synergies} in the {Construction} of a {Natural} {Motor} {Behavior}},
	volume = {6},
	number = {3},
	journal = {Nature Neuroscience},
	author = {D'Avella, Andrea and Saltiel, Philippe and Bizzi, Emilio},
	year = {2003},
	pages = {300--308},
}

@article{khansari-zadeh_learning_2011,
	title = {Learning {Stable} {Nonlinear} {Dynamical} {Systems} with {Gaussian} {Mixture} {Models}},
	volume = {27},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Khansari-Zadeh, S Mohammad and Billard, Aude},
	year = {2011},
	pages = {943--957},
}

@inproceedings{rana_towards_2018,
	title = {Towards {Robust} {Skill} {Generalization}: {Unifying} {Learning} from {Demonstration} and {Motion} {Planning}},
	booktitle = {Intelligent robots and systems},
	author = {Rana, M and Mukadam, Mustafa and Ahmadzadeh, Seyed Reza and Chernova, Sonia and Boots, Byron},
	year = {2018},
}

@article{billard_robot_2004,
	title = {Robot {Learning} from {Demonstration}},
	volume = {2},
	number = {47},
	journal = {Robotics and Autonomous Systems},
	author = {Billard, Aude and Siegwart, Roland},
	year = {2004},
	pages = {65--67},
}

@article{argall_survey_2009,
	title = {A {Survey} of {Robot} {Learning} from {Demonstration}},
	volume = {57},
	number = {5},
	journal = {Robotics and autonomous systems},
	author = {Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
	year = {2009},
	pages = {469--483},
}

@article{schaal_is_1999,
	title = {Is {Imitation} {Learning} the {Route} to {Humanoid} {Robots}?},
	volume = {3},
	number = {6},
	journal = {Trends in cognitive sciences},
	author = {Schaal, Stefan},
	year = {1999},
	pages = {233--242},
}

@inproceedings{colome_dimensionality_2014,
	title = {Dimensionality {Reduction} for {Probabilistic} {Movement} {Primitives}},
	booktitle = {International {Conference} on {Humanoid} {Robots}},
	publisher = {IEEE},
	author = {Colomé, Adria and Neumann, Gerhard and Peters, Jan and Torras, Carme},
	year = {2014},
	pages = {794--800},
	file = {Colomé et al. - 2014 - Dimensionality Reduction for Probabilistic Movemen.pdf:/Users/samueletosatto/Zotero/storage/4A4QX72M/Colomé et al. - 2014 - Dimensionality Reduction for Probabilistic Movemen.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate-1,
}

@book{golub_matrix_2012,
	edition = {3},
	title = {Matrix {Computations}},
	publisher = {JHU press},
	author = {Golub, Gene H and Van Loan, Charles F},
	year = {2012},
}

@article{pearson_lines_1901,
	title = {On {Lines} and {Planes} of {Closest} fit to {Systems} of {Points} in {Space}},
	volume = {2},
	number = {11},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {Pearson, Karl},
	year = {1901},
	pages = {559--572},
}

@inproceedings{rueckert_extracting_2015,
	title = {Extracting {Low}-{Dimensional} {Control} {Variables} for {Movement} {Primitives}},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Rueckert, Elmar and Mundo, Jan and Paraschos, Alexandros and Peters, Jan and Neumann, Gerhard},
	year = {2015},
	pages = {1511--1518},
}

@inproceedings{kupcsik_data-efficient_2013,
	title = {Data-{Efficient} {Generalization} of {Robot} {Skills} with {Contextual} {Policy} {Search}},
	booktitle = {Twenty-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Kupcsik, Andras Gabor and Deisenroth, Marc Peter and Peters, Jan and Neumann, Gerhard},
	year = {2013},
	file = {Kupcsik et al. - 2013 - Data-Efficient Generalization of Robot Skills with.pdf:/Users/samueletosatto/Zotero/storage/I9897D9F/Kupcsik et al. - 2013 - Data-Efficient Generalization of Robot Skills with.pdf:application/pdf},
}

@inproceedings{amor_interaction_2014,
	title = {Interaction {Primitives} for {Human}-{Robot} {Cooperation} {Tasks}},
	booktitle = {{IEEE} international conference on robotics and automation ({ICRA})},
	publisher = {IEEE},
	author = {Amor, Heni Ben and Neumann, Gerhard and Kamthe, Sanket and Kroemer, Oliver and Peters, Jan},
	year = {2014},
	pages = {2831--2837},
	file = {Amor et al. - 2014 - Interaction primitives for human-robot cooperation.pdf:/Users/samueletosatto/Zotero/storage/5XZG75BI/Amor et al. - 2014 - Interaction primitives for human-robot cooperation.pdf:application/pdf},
}

@inproceedings{park_movement_2008,
	title = {Movement {Reproduction} and {Obstacle} {Avoidance} with {Dynamic} {Movement} {Primitives} and {Potential} {Fields}},
	booktitle = {Humanoids 2008-8th {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	publisher = {IEEE},
	author = {Park, Dae-Hyung and Hoffmann, Heiko and Pastor, Peter and Schaal, Stefan},
	year = {2008},
	pages = {91--98},
}

@article{wang_probabilistic_2013,
	title = {Probabilistic {Movement} {Modeling} for {Intention} {Inference} in {Human}-{Robot} {Interaction}},
	volume = {32},
	number = {7},
	journal = {The International Journal of Robotics Research},
	author = {Wang, Zhikun and Mülling, Katharina and Deisenroth, Marc Peter and Ben Amor, Heni and Vogt, David and Schölkopf, Bernhard and Peters, Jan},
	year = {2013},
	pages = {841--858},
	file = {Wang et al. - 2013 - Probabilistic movement modeling for intention infe.pdf:/Users/samueletosatto/Zotero/storage/7SL8B4DC/Wang et al. - 2013 - Probabilistic movement modeling for intention infe.pdf:application/pdf},
}

@inproceedings{koert_demonstration_2016,
	title = {Demonstration {Based} {Trajectory} {Optimization} for {Generalizable} {Robot} {Motions}},
	booktitle = {{IEEE}-{RAS} 16th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	publisher = {IEEE},
	author = {Koert, Dorothea and Maeda, Guilherme and Lioutikov, Rudolf and Neumann, Gerhard and Peters, Jan},
	year = {2016},
	pages = {515--522},
	file = {Koert et al. - 2016 - Demonstration based trajectory optimization for ge.pdf:/Users/samueletosatto/Zotero/storage/UFL5TJIC/Koert et al. - 2016 - Demonstration based trajectory optimization for ge.pdf:application/pdf},
}

@inproceedings{maeda_learning_2014,
	title = {Learning {Interaction} for {Collaborative} {Tasks} with {Probabilistic} {Movement} {Primitives}},
	booktitle = {{IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	publisher = {IEEE},
	author = {Maeda, Guilherme and Ewerton, Marco and Lioutikov, Rudolf and Amor, Heni Ben and Peters, Jan and Neumann, Gerhard},
	year = {2014},
	pages = {527--534},
	file = {Maeda et al. - 2014 - Learning interaction for collaborative tasks with .pdf:/Users/samueletosatto/Zotero/storage/GDPH3H24/Maeda et al. - 2014 - Learning interaction for collaborative tasks with .pdf:application/pdf},
}

@article{stark_experience_2019,
	title = {Experience {Reuse} with {Probabilistic} {Movement} {Primitives}},
	journal = {arXiv preprint arXiv:1908.03936},
	author = {Stark, Svenja and Peters, Jan and Rueckert, Elmar},
	year = {2019},
	file = {Stark et al. - 2019 - Experience Reuse with Probabilistic Movement Primi.pdf:/Users/samueletosatto/Zotero/storage/6M8MGQDW/Stark et al. - 2019 - Experience Reuse with Probabilistic Movement Primi.pdf:application/pdf},
}

@article{peters_reinforcement_2008,
	title = {Reinforcement {Learning} of {Motor} {Skills} with {Policy} {Gradients}},
	volume = {21},
	number = {4},
	journal = {Neural networks},
	author = {Peters, Jan and Schaal, Stefan},
	year = {2008},
	pages = {682--697},
	file = {Peters and Schaal - 2008 - Reinforcement Learning of Motor Skills with Policy.pdf:/Users/samueletosatto/Zotero/storage/9T5UCR5A/Peters and Schaal - 2008 - Reinforcement Learning of Motor Skills with Policy.pdf:application/pdf},
}

@inproceedings{kober_policy_2009-1,
	title = {Policy {Search} for {Motor} {Primitives} in {Robotics}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kober, Jens and Peters, Jan R},
	year = {2009},
	pages = {849--856},
	file = {Kober and Peters - 2009 - Policy Search for Motor Primitives in Robotics.pdf:/Users/samueletosatto/Zotero/storage/5J6MSIKN/Kober and Peters - 2009 - Policy Search for Motor Primitives in Robotics.pdf:application/pdf},
}

@article{mulling_learning_2013,
	title = {Learning to {Select} and {Generalize} {Striking} {Movements} in {Robot} {Table} {Tennis}},
	volume = {32},
	number = {3},
	journal = {The International Journal of Robotics Research},
	author = {Mülling, Katharina and Kober, Jens and Kroemer, Oliver and Peters, Jan},
	year = {2013},
	pages = {263--279},
	file = {Mülling et al. - 2013 - Learning to Select and Generalize Striking Movemen.pdf:/Users/samueletosatto/Zotero/storage/9FMJ7EEW/Mülling et al. - 2013 - Learning to Select and Generalize Striking Movemen.pdf:application/pdf},
}

@inproceedings{kober_reinforcement_2011,
	title = {Reinforcement {Learning} to {Adjust} {Robot} {Movements} to {New} {Situations}},
	booktitle = {Twenty-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Kober, Jens and Oztop, Erhan and Peters, Jan},
	year = {2011},
	file = {Kober et al. - 2011 - Reinforcement Learning to Adjust Robot Movements t.pdf:/Users/samueletosatto/Zotero/storage/EJNL3G8Q/Kober et al. - 2011 - Reinforcement Learning to Adjust Robot Movements t.pdf:application/pdf},
}

@article{tosatto_upper_2020,
	title = {An {Upper} {Bound} of the {Bias} of {Nadaraya}-{Watson} {Kernel} {Regression} under {Lipschitz} {Assumptions}},
	journal = {arXiv preprint arXiv:2001.10972},
	author = {Tosatto, Samuele and Akrour, Riad and Peters, Jan},
	year = {2020},
	file = {Tosatto et al. - 2020 - An Upper Bound of the Bias of Nadaraya-Watson Kern.pdf:/Users/samueletosatto/Zotero/storage/NPEILFDU/Tosatto et al. - 2020 - An Upper Bound of the Bias of Nadaraya-Watson Kern.pdf:application/pdf},
}

@inproceedings{schaal_learning_2005,
	title = {Learning {Movement} {Primitives}},
	booktitle = {Robotics research. {The} eleventh international symposium},
	publisher = {Springer},
	author = {Schaal, Stefan and Peters, Jan and Nakanishi, Jun and Ijspeert, Auke},
	year = {2005},
	pages = {561--572},
}

@inproceedings{paraschos_probabilistic_2013,
	title = {Probabilistic {Movement} {Primitives}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	publisher = {mit press},
	author = {Paraschos, A. and Daniel, C. and Peters, J. and Neumann, G},
	year = {2013},
	file = {Paraschos et al. - 2013 - Probabilistic Movement Primitives.pdf:/Users/samueletosatto/Zotero/storage/D73V9ZRP/Paraschos et al. - 2013 - Probabilistic Movement Primitives.pdf:application/pdf},
}

@article{paraschos_using_2018,
	title = {Using {Probabilistic} {Movement} {Primitives} in {Robotics}},
	number = {3},
	journal = {Autonomous Robots (AURO)},
	author = {Paraschos, A. and Daniel, C. and Peters, J. and Neumann, G.},
	year = {2018},
	pages = {529--551},
	file = {Paraschos et al. - 2018 - Using Probabilistic Movement Primitives in Robotic.pdf:/Users/samueletosatto/Zotero/storage/5K2R8I9S/Paraschos et al. - 2018 - Using Probabilistic Movement Primitives in Robotic.pdf:application/pdf;Paraschos Using Probabilistic Movement Primitives in Robotic.pdf:/Users/samueletosatto/Zotero/storage/4J7J3THU/Paraschos Using Probabilistic Movement Primitives in Robotic.pdf:application/pdf},
}

@article{ziegler_approximations_2001,
	title = {On {Approximations} to the {Bias} of the {Nadaraya}-{Watson} {Regression} {Estimator}},
	volume = {13},
	url = {https://doi.org/10.1080/10485250108832866},
	doi = {10.1080/10485250108832866},
	number = {4},
	journal = {Journal of Nonparametric Statistics},
	author = {Ziegler, Klaus},
	year = {2001},
	pages = {583--589},
	file = {Ziegler - 2001 - On approximations to the bias of the nadaraya-wats.pdf:/Users/samueletosatto/Zotero/storage/FPACSGEP/Ziegler - 2001 - On approximations to the bias of the nadaraya-wats.pdf:application/pdf},
}

@article{skold_kernel_1999,
	title = {Kernel {Regression} in the {Presence} of {Size}-{Bias}},
	volume = {12},
	url = {https://doi.org/10.1080/10485259908832797},
	doi = {10.1080/10485259908832797},
	number = {1},
	journal = {Journal of Nonparametric Statistics},
	author = {Sköld, Martin},
	year = {1999},
	pages = {41--51},
	file = {Sköld - 1999 - Kernel regression in the presence of size-bias.pdf:/Users/samueletosatto/Zotero/storage/IGKFWS5X/Sköld - 1999 - Kernel regression in the presence of size-bias.pdf:application/pdf},
}

@inproceedings{maeda_active_2017,
	title = {Active {Incremental} {Learning} of {Robot} {Movement} {Primitives}},
	author = {Maeda, Guilherme and Ewerton, Marco and Osa, Takayuki and Busch, Baptiste and Peters, Jan},
	year = {2017},
	file = {Maeda et al. - 2017 - Active Incremental Learning of Robot Movement Prim.pdf:/Users/samueletosatto/Zotero/storage/TZG3XQH2/Maeda et al. - 2017 - Active Incremental Learning of Robot Movement Prim.pdf:application/pdf},
}

@inproceedings{chen_dynamic_2016,
	title = {Dynamic {Movement} {Primitives} in {Latent} {Space} of {Time}-{Dependent} {Variational} {Autoencoders}},
	booktitle = {2016 {IEEE}-{RAS} 16th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	publisher = {IEEE},
	author = {Chen, Nutan and Karl, Maximilian and Van Der Smagt, Patrick},
	year = {2016},
	pages = {629--636},
	file = {Chen et al. - 2016 - Dynamic Movement Primitives in Latent Space of Tim.pdf:/Users/samueletosatto/Zotero/storage/B5LQG8CL/Chen et al. - 2016 - Dynamic Movement Primitives in Latent Space of Tim.pdf:application/pdf},
}

@article{colome_dimensionality_2018-1,
	title = {Dimensionality {Reduction} for {Dynamic} {Movement} {Primitives} and {Application} to {Bimanual} {Manipulation} of {Clothes}},
	volume = {34},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Colomé, Adria and Torras, Carme},
	year = {2018},
	pages = {602--615},
	file = {Colomé and Torras - 2018 - Dimensionality Reduction for Dynamic Movement Prim.pdf:/Users/samueletosatto/Zotero/storage/9946988F/Colomé and Torras - 2018 - Dimensionality Reduction for Dynamic Movement Prim.pdf:application/pdf},
}

@book{owen_monte_2013,
	title = {Monte {Carlo} {Theory}, {Methods} and {Examples}},
	author = {Owen, Art B},
	year = {2013},
	file = {Owen - 2013 - Monte Carlo Theory, Methods and Examples.pdf:/Users/samueletosatto/Zotero/storage/SQE9W65M/Owen - 2013 - Monte Carlo Theory, Methods and Examples.pdf:application/pdf},
}

@article{miculescu_sufficient_2000,
	title = {A {Sufficient} {Condition} for a {Function} to {Satisfy} a {Weak} {Lipschitz} {Condition}},
	journal = {Mathematical Reports},
	author = {Miculescu, Radu},
	year = {2000},
}

@book{nachum_reinforcement_2020,
	title = {Reinforcement {Learning} via {Fenchel}-{Rockafellar} {Duality}},
	author = {Nachum, Ofir and Dai, Bo},
	year = {2020},
	file = {Nachum and Dai - 2020 - Reinforcement Learning via Fenchel-Rockafellar Dua.pdf:/Users/samueletosatto/Zotero/storage/P7YU7WVM/Nachum and Dai - 2020 - Reinforcement Learning via Fenchel-Rockafellar Dua.pdf:application/pdf},
}

@inproceedings{agostini_reinforcement_2010,
	title = {Reinforcement {Learning} with a {Gaussian} {Mixture} {Model}},
	booktitle = {The 2010 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Agostini, Alejandro and Celaya, Enric},
	year = {2010},
	pages = {1--8},
	file = {Agostini and Celaya - 2010 - Reinforcement Learning with a Gaussian Mixture Mod.pdf:/Users/samueletosatto/Zotero/storage/KBJJSJIU/Agostini and Celaya - 2010 - Reinforcement Learning with a Gaussian Mixture Mod.pdf:application/pdf},
}

@phdthesis{sung_gaussian_2004,
	type = {{PhD} {Thesis}},
	title = {Gaussian {Mixture} {Regression} and {Classification}},
	school = {Rice University},
	author = {Sung, Hsi Guang},
	year = {2004},
	file = {Sung - 2004 - Gaussian Mixture Regression and Classification.pdf:/Users/samueletosatto/Zotero/storage/3M32BN36/Sung - 2004 - Gaussian Mixture Regression and Classification.pdf:application/pdf},
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	journal = {arXiv preprint arXiv:1707.06347},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year = {2017},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/Users/samueletosatto/Zotero/storage/RF4SXEWT/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@inproceedings{moosavi-dezfooli_deepfool:_2016,
	title = {Deepfool: a {Simple} and {Accurate} {Method} to {Fool} {Deep} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	year = {2016},
	pages = {2574--2582},
}

@inproceedings{charikar_hashing-based-estimators_2017,
	title = {Hashing-{Based}-{Estimators} for {Kernel} {Density} in {High} {Dimensions}},
	booktitle = {58th {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	publisher = {IEEE},
	author = {Charikar, Moses and Siminelakis, Paris},
	year = {2017},
	pages = {1032--1043},
	file = {Charikar and Siminelakis - 2017 - Hashing-Based-Estimators for Kernel Density in Hig.pdf:/Users/samueletosatto/Zotero/storage/IZMZYNZL/Charikar and Siminelakis - 2017 - Hashing-Based-Estimators for Kernel Density in Hig.pdf:application/pdf},
}

@inproceedings{backurs_space_2019,
	title = {Space and {Time} {Efficient} {Kernel} {Density} {Estimation} in {High} {Dimensions}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Backurs, Arturs and Indyk, Piotr and Wagner, Tal},
	year = {2019},
	pages = {15773--15782},
	file = {Backurs et al. - 2019 - Space and Time Efficient Kernel Density Estimation.pdf:/Users/samueletosatto/Zotero/storage/FFTVGNGN/Backurs et al. - 2019 - Space and Time Efficient Kernel Density Estimation.pdf:application/pdf},
}

@article{cai_weighted_2001,
	title = {Weighted {Nadaraya}–{Watson} {Regression} {Estimation}},
	volume = {51},
	number = {3},
	journal = {Statistics {\textbackslash}\& Probability Letters},
	author = {Cai, Zongwu},
	year = {2001},
	pages = {307--318},
	file = {Cai - 2001 - Weighted Nadaraya–Watson Regression Estimation.pdf:/Users/samueletosatto/Zotero/storage/N2ME8S56/Cai - 2001 - Weighted Nadaraya–Watson Regression Estimation.pdf:application/pdf},
}

@article{ray_bandwidth_1997,
	title = {Bandwidth {Selection} for {Kernel} {Regression} with {Long}-{Range} {Dependent} {Errors}},
	volume = {84},
	number = {4},
	journal = {Biometrika},
	author = {Ray, Bonnie K and Tsay, Ruey S},
	year = {1997},
	pages = {791--802},
	file = {Ray and Tsay - 1997 - Bandwidth Selection for Kernel Regression with Lon.pdf:/Users/samueletosatto/Zotero/storage/WWJRLWFU/Ray and Tsay - 1997 - Bandwidth Selection for Kernel Regression with Lon.pdf:application/pdf},
}

@article{herrmann_choice_1992,
	title = {Choice of {Bandwidth} for {Kernel} {Regression} when {Residuals} are {Correlated}},
	volume = {79},
	number = {4},
	journal = {Biometrika},
	author = {Herrmann, Eva and Gasser, Theo and Kneip, Alois},
	year = {1992},
	pages = {783--795},
	file = {Herrmann et al. - 1992 - Choice of Bandwidth for Kernel Regression when Res.pdf:/Users/samueletosatto/Zotero/storage/4L57ANZB/Herrmann et al. - 1992 - Choice of Bandwidth for Kernel Regression when Res.pdf:application/pdf},
}

@article{hardle_asymptotic_1985,
	title = {Asymptotic {Nonequivalence} of {Some} {Bandwidth} {Selectors} in {Nonparametric} {Regression}},
	volume = {72},
	number = {2},
	journal = {Biometrika},
	author = {Härdle, W and Marron, JS},
	year = {1985},
	pages = {481--484},
	file = {Härdle and Marron - 1985 - Asymptotic Nonequivalence of Some Bandwidth Select.pdf:/Users/samueletosatto/Zotero/storage/KYJLCJEX/Härdle and Marron - 1985 - Asymptotic Nonequivalence of Some Bandwidth Select.pdf:application/pdf},
}

@article{cheng_nonparametric_1981,
	title = {Nonparametric {Estimation} of a {Regression} {Function}},
	volume = {57},
	number = {2},
	journal = {Probability Theory and Related Fields},
	author = {Cheng, Kuang-Fu and Lin, Pi-Erh},
	year = {1981},
	pages = {223--233},
	file = {Cheng and Lin - 1981 - Nonparametric estimation of a regression function.pdf:/Users/samueletosatto/Zotero/storage/98GSHXYP/Cheng and Lin - 1981 - Nonparametric estimation of a regression function.pdf:application/pdf},
}

@article{kohler_review_2014,
	title = {A {Review} and {Comparison} of {Bandwidth} {Selection} {Methods} for {Kernel} {Regression}},
	volume = {82},
	number = {2},
	journal = {International Statistical Review},
	author = {Köhler, Max and Schindler, Anja and Sperlich, Stefan},
	year = {2014},
	pages = {243--274},
	file = {Köhler et al. - 2014 - A Review and Comparison of Bandwidth Selection Met.pdf:/Users/samueletosatto/Zotero/storage/7XGDKJVL/Köhler et al. - 2014 - A Review and Comparison of Bandwidth Selection Met.pdf:application/pdf},
}

@article{bansal_nonparametric_1995,
	title = {Nonparametric {Estimation} of {Structural} {Models} for {High}-{Frequency} {Currency} {Market} {Data}},
	volume = {66},
	number = {1-2},
	journal = {Journal of Econometrics},
	author = {Bansal, Ravi and Gallant, A Ronald and Hussey, Robert and Tauchen, George},
	year = {1995},
	pages = {251--287},
	file = {Bansal et al. - 1995 - Nonparametric Estimation of Structural Models for .pdf:/Users/samueletosatto/Zotero/storage/N372KVVI/Bansal et al. - 1995 - Nonparametric Estimation of Structural Models for .pdf:application/pdf},
}

@inproceedings{wang_gaussian_2006,
	title = {Gaussian {Process} {Dynamical} {Models}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Jack and Hertzmann, Aaron and Fleet, David J},
	year = {2006},
	pages = {1441--1448},
	file = {Wang et al. - 2006 - Gaussian Process Dynamical Models.pdf:/Users/samueletosatto/Zotero/storage/RYMRNLVC/Wang et al. - 2006 - Gaussian Process Dynamical Models.pdf:application/pdf},
}

@inproceedings{nguyen-tuong_using_2010,
	title = {Using {Model} {Knowledge} for {Learning} {Inverse} {Dynamics}},
	booktitle = {International {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Nguyen-Tuong, Duy and Peters, Jan},
	year = {2010},
	pages = {2677--2682},
	file = {Nguyen-Tuong and Peters - 2010 - Using Model Knowledge for Learning Inverse Dynamic.pdf:/Users/samueletosatto/Zotero/storage/QYLMF73A/Nguyen-Tuong and Peters - 2010 - Using Model Knowledge for Learning Inverse Dynamic.pdf:application/pdf},
}

@inproceedings{kakade_approximately_2002,
	title = {Approximately {Optimal} {Approximate} {Reinforcement} {Learning}},
	volume = {2},
	booktitle = {Proceeding of the 19th {International} {Conference} on {Machine} {Learning}},
	author = {Kakade, Sham and Langford, John},
	year = {2002},
	pages = {267--274},
	file = {Kakade and Langford - 2002 - Approximately Optimal Approximate Reinforcement Le.pdf:/Users/samueletosatto/Zotero/storage/N9MALDJC/Kakade and Langford - 2002 - Approximately Optimal Approximate Reinforcement Le.pdf:application/pdf},
}

@article{bu_comprehensive_2008,
	title = {A {Comprehensive} {Survey} of {Multiagent} {Reinforcement} {Learning}},
	volume = {38},
	number = {2},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Bu, Lucian and Babu, Robert and De Schutter, Bart},
	year = {2008},
	pages = {156--172},
	file = {Bu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf:/Users/samueletosatto/Zotero/storage/XUFN954K/Bu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf:application/pdf},
}

@inproceedings{boutilier_planning_1996,
	title = {Planning, {Learning} and {Coordination} in {Multiagent} {Decision} {Processes}},
	booktitle = {Proceedings of the 6th {Conference} on {Theoretical} {Aspects} of {Rationality} and {Knowledge}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Boutilier, Craig},
	year = {1996},
	pages = {195--210},
	file = {Boutilier - 1996 - Planning, Learning and Coordination in Multiagent .pdf:/Users/samueletosatto/Zotero/storage/8AWUKF3Y/Boutilier - 1996 - Planning, Learning and Coordination in Multiagent .pdf:application/pdf},
}

@inproceedings{uppal_nonparametric_2019,
	title = {Nonparametric {Density} {Estimation} and {Convergence} {Rates} for {GANs} under {Besov} {IPM} {Losses}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Uppal, Ananya and Singh, Shashank and Poczos, Barnabas},
	year = {2019},
	pages = {9086--9097},
	file = {Uppal et al. - 2019 - Nonparametric Density Estimation and Convergence R.pdf:/Users/samueletosatto/Zotero/storage/AZRBSSD6/Uppal et al. - 2019 - Nonparametric Density Estimation and Convergence R.pdf:application/pdf},
}

@book{bertsekas_nonlinear_nodate,
	edition = {Second},
	title = {Nonlinear {Programming}},
	publisher = {Athena Scientific, Belmont, Massachussets},
	author = {Bertsekas, Dimitri P.},
}

@article{nguyen_estimating_2010,
	title = {Estimating {Divergence} {Functionals} and the {Likelihood} {Ratio} by {Convex} {Risk} {Minimization}},
	volume = {56},
	number = {11},
	journal = {IEEE Transactions on Information Theory},
	author = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael I},
	year = {2010},
	pages = {5847--5861},
	file = {Nguyen et al. - 2010 - Estimating Divergence Functionals and the Likeliho.pdf:/Users/samueletosatto/Zotero/storage/Z3DKKC8A/Nguyen et al. - 2010 - Estimating Divergence Functionals and the Likeliho.pdf:application/pdf},
}

@article{belousov_entropic_2019,
	title = {Entropic {Regularization} of {Markov} {Decision} {Processes}},
	url = {https://www.ias.informatik.tu-darmstadt.de/uploads/Team/BorisBelousov/entropy19_belousov.pdf},
	number = {7},
	journal = {Entropy},
	author = {Belousov, B. and Peters, J.},
	year = {2019},
	file = {Belousov and Peters - 2019 - Entropic Regularization of Markov Decision Process.pdf:/Users/samueletosatto/Zotero/storage/JCARK9PZ/Belousov and Peters - 2019 - Entropic Regularization of Markov Decision Process.pdf:application/pdf},
}

@article{nachum_algaedice:_2019,
	title = {{AlgaeDICE}: {Policy} {Gradient} from {Arbitrary} {Experience}},
	abstract = {In many real-world applications of reinforcement learning (RL), interactions with
the environment are limited due to cost or feasibility. This presents a challenge to
traditional RL algorithms since the max-return objective involves an expectation
over on-policy samples. We introduce a new formulation of max-return optimization
that allows the problem to be re-expressed by an expectation over an arbitrary
behavior-agnostic and off-policy data distribution. We first derive this result by
considering a regularized version of the dual max-return objective before extending
our findings to unregularized objectives through the use of a Lagrangian formulation
of the linear programming characterization of Q-values. We show that, if auxiliary
dual variables of the objective are optimized, then the gradient of the off-policy
objective is exactly the on-policy policy gradient, without any use of importance
weighting. In addition to revealing the appealing theoretical properties of this
approach, we also show that it delivers good practical performance.},
	journal = {arXiv:1912.02074v1},
	author = {Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
	year = {2019},
	file = {Nachum et al. - 2019 - AlgaeDICE Policy Gradient from Arbitrary Experien.pdf:/Users/samueletosatto/Zotero/storage/TYSRBY2R/Nachum et al. - 2019 - AlgaeDICE Policy Gradient from Arbitrary Experien.pdf:application/pdf},
}

@article{nachum_dualdice:_nodate,
	title = {{DualDICE}: {Behavior}-{Agnostic} {Estimation} of {Discounted} {Stationary} {Distribution} {Corrections}},
	author = {Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
	file = {Nachum et al. - DualDICE Behavior-Agnostic Estimation of Discount.pdf:/Users/samueletosatto/Zotero/storage/CFJP3I5L/Nachum et al. - DualDICE Behavior-Agnostic Estimation of Discount.pdf:application/pdf},
}

@article{lample_deep_nodate,
	title = {Deep {Learning} for {Symbolic} {Mathematics}},
	journal = {arXiv},
	author = {Lample, Gullaume and Charton, Francois},
	file = {Deep Learning for Symbolic Mathematics:/Users/samueletosatto/Zotero/storage/AFEIL2FF/Deep Learning for Symbolic Mathematics:application/pdf;Deep Learning for Symbolic Mathematics:/home/samuele/Zotero/local files/Deep Learning for Symbolic Mathematics:application/pdf;Deep Learning for Symbolic Mathematics.pdf:/Users/samueletosatto/Zotero/storage/3LE5JUS8/Deep Learning for Symbolic Mathematics.pdf:application/pdf},
}

@article{ormoneit_kernel-based_2002,
	title = {Kernel-{Based} {Reinforcement} {Learning}},
	volume = {49},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1017928328829},
	doi = {10.1023/A:1017928328829},
	abstract = {We present a kernel-based approach to reinforcement learning that overcomes the stability problems of temporal-difference learning in continuous state-spaces. First, our algorithm converges to a unique solution of an approximate Bellman's equation regardless of its initialization values. Second, the method is consistent in the sense that the resulting policy converges asymptotically to the optimal policy. Parametric value function estimates such as neural networks do not possess this property. Our kernel-based approach also allows us to show that the limiting distribution of the value function estimate is a Gaussian process. This information is useful in studying the bias-variance tradeoff in reinforcement learning. We find that all reinforcement learning approaches to estimating the value function, parametric or non-parametric, are subject to a bias. This bias is typically larger in reinforcement learning than in a comparable regression problem.},
	language = {en},
	number = {2},
	urldate = {2019-10-02},
	journal = {Machine Learning},
	author = {Ormoneit, Dirk and Sen, Saunak},
	year = {2002},
	keywords = {reinforcement learning, kernel smoothing, kernel-based learning, lazy learning, local averaging, Markov decision process},
	pages = {161--178},
	file = {Springer Full Text PDF:/Users/samueletosatto/Zotero/storage/Y54UA4Q4/Ormoneit and Sen - 2002 - Kernel-Based Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{chen_efficient_2015,
	title = {Efficient {Movement} {Representation} by {Embedding} {Dynamic} {Movement} {Primitives} in {Deep} {Autoencoders}},
	booktitle = {{IEEE}-{RAS} 15th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	publisher = {IEEE},
	author = {Chen, Nutan and Bayer, Justin and Urban, Sebastian and Van Der Smagt, Patrick},
	year = {2015},
	pages = {434--440},
	file = {Chen et al. - 2015 - Efficient Movement Representation by Embedding Dyn.pdf:/Users/samueletosatto/Zotero/storage/ZHXAQGIG/Chen et al. - 2015 - Efficient Movement Representation by Embedding Dyn.pdf:application/pdf},
}

@incollection{park_movement_2004,
	title = {Movement {Primitives} and {Principal} {Component} {Analysis}},
	booktitle = {On {Advances} in {Robot} {Kinematics}},
	publisher = {Springer},
	author = {Park, Frank C and Jo, Kyoosang},
	year = {2004},
	pages = {421--430},
	file = {Park and Jo - 2004 - Movement Primitives and Principal Component Analys.pdf:/Users/samueletosatto/Zotero/storage/9RI6MCP5/Park and Jo - 2004 - Movement Primitives and Principal Component Analys.pdf:application/pdf},
}

@article{linton_estimation_1996,
	title = {Estimation of {Additive} {Regression} {Models} with {Known} {Links}},
	volume = {83},
	number = {3},
	journal = {Biometrika},
	author = {Linton, OB and Härdle, W},
	year = {1996},
	pages = {529--540},
}

@article{akritas_nonparametric_2000,
	title = {Nonparametric {Models} and {Methods} for {Nonlinear} {Analysis} of {Covariance}},
	volume = {87},
	number = {3},
	journal = {Biometrika},
	author = {Akritas, Michael G and Arnold, Steven F and Du, Yunling},
	year = {2000},
	pages = {507--526},
	file = {Akritas et al. - 2000 - Nonparametric Models and Methods for Nonlinear Ana.pdf:/Users/samueletosatto/Zotero/storage/YNHLVDXL/Akritas et al. - 2000 - Nonparametric Models and Methods for Nonlinear Ana.pdf:application/pdf},
}

@inproceedings{nicolle_facial_2015,
	title = {Facial {Action} {Unit} {Intensity} {Prediction} via {Hard} {Multi}-{Task} {Metric} {Learning} for {Kernel} {Regression}},
	volume = {6},
	booktitle = {{IEEE} {International} {Conference} and {Workshops} on {Automatic} {Face} and {Gesture} {Recognition} ({FG})},
	publisher = {IEEE},
	author = {Nicolle, Jeremie and Bailly, Kevin and Chetouani, Mohamed},
	year = {2015},
	pages = {1--6},
}

@techreport{ho_data-driven_2019,
	title = {On {Data}-{Driven} {Prescriptive} {Analytics} with {Side} {Information}: {A} {Regularized} {Nadaraya}-{Watson} {Approach}},
	institution = {Technical report, March},
	author = {Ho, Chin Pang and Hanasusanto, Grani A},
	year = {2019},
}

@inproceedings{andriyana_modelling_2019,
	title = {Modelling the {Percentage} of {Poverty} {Based} on an {Open} {Unemployment} {Rate} using some {Nonparametric} {Regression} {Techniques}},
	volume = {1265},
	booktitle = {Journal of {Physics}: {Conference} {Series}},
	publisher = {IOP Publishing},
	author = {Andriyana, Y and Suprijadi, J and Suparman, Y and Winarni, S and Jaya, IGNM},
	year = {2019},
	pages = {012023},
}

@article{rosenblatt_remarks_1956,
	title = {Remarks on {Some} {Nonparametric} {Estimates} of a {Density} {Function}},
	journal = {The Annals of Mathematical Statistics},
	author = {Rosenblatt, Murray},
	year = {1956},
	pages = {832--837},
	file = {Rosenblatt - 1956 - Remarks on Some Nonparametric Estimates of a Densi.pdf:/Users/samueletosatto/Zotero/storage/LGXHA3JU/Rosenblatt - 1956 - Remarks on Some Nonparametric Estimates of a Densi.pdf:application/pdf},
}

@article{parzen_estimation_1962,
	title = {On {Estimation} of a {Probability} {Density} {Function} and {Mode}},
	volume = {33},
	number = {3},
	journal = {The annals of mathematical statistics},
	author = {Parzen, Emanuel},
	year = {1962},
	pages = {1065--1076},
	file = {Parzen - 1962 - On estimation of a probability density function an.pdf:/Users/samueletosatto/Zotero/storage/MY3QQG7V/Parzen - 1962 - On estimation of a probability density function an.pdf:application/pdf},
}

@article{fan_variable_1992,
	title = {Variable {Bandwidth} and {Local} {Linear} {Regression} {Smoothers}},
	journal = {The Annals of Statistics},
	author = {Fan, Jianqing and Gijbels, Irene},
	year = {1992},
	pages = {2008--2036},
	file = {Fan and Gijbels - 1992 - Variable Bandwidth and Local Linear Regression Smo.pdf:/Users/samueletosatto/Zotero/storage/FCX2VFVT/Fan and Gijbels - 1992 - Variable Bandwidth and Local Linear Regression Smo.pdf:application/pdf},
}

@inproceedings{colome_dimensionality_2014-1,
	title = {Dimensionality {Reduction} and {Motion} {Coordination} in {Learning} {Trajectories} with {Dynamic} {Movement} {Primitives}},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Colomé, Adria and Torras, Carme},
	year = {2014},
	pages = {1414--1420},
	file = {Colomé and Torras - 2014 - Dimensionality Reduction and Motion Coordination i.pdf:/Users/samueletosatto/Zotero/storage/RPCQK7SF/Colomé and Torras - 2014 - Dimensionality Reduction and Motion Coordination i.pdf:application/pdf},
}

@inproceedings{tosatto_boosted_2017,
	title = {Boosted {Fitted} {Q}-{Iteration}},
	copyright = {All rights reserved},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR},
	author = {Tosatto, Samuele and Pirotta, Matteo and D'Eramo, Carlo and Restelli, Marcello},
	year = {2017},
	pages = {3434--3443},
	file = {Tosatto et al. - 2017 - Boosted Fitted Q-Iteration.pdf:/Users/samueletosatto/Zotero/storage/QKXPPQDH/Tosatto et al. - 2017 - Boosted Fitted Q-Iteration.pdf:application/pdf},
}

@inproceedings{kroemer_kernel-based_2012,
	title = {A {Kernel}-{Based} {Approach} to {Direct} {Action} {Perception}},
	booktitle = {International {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Kroemer, Oliver and Ugur, Emre and Oztop, Erhan and Peters, Jan},
	year = {2012},
	pages = {2605--2610},
	file = {Kroemer et al. - 2012 - A Kernel-Based Approach to Direct Action Perceptio.pdf:/Users/samueletosatto/Zotero/storage/N5VPC9U5/Kroemer et al. - 2012 - A Kernel-Based Approach to Direct Action Perceptio.pdf:application/pdf},
}

@inproceedings{rozo_force-based_2013,
	title = {Force-{Based} {Robot} {Learning} of {Pouring} {Skills} using {Parametric} {Hidden} {Markov} {Models}},
	booktitle = {9th {International} {Workshop} on {Robot} {Motion} and {Control}},
	publisher = {IEEE},
	author = {Rozo, Leonel and Jiménez, Pablo and Torras, Carme},
	year = {2013},
	pages = {227--232},
	file = {Rozo et al. - 2013 - Force-Based Robot Learning of Pouring Skills using.pdf:/Users/samueletosatto/Zotero/storage/4YIFU2RC/Rozo et al. - 2013 - Force-Based Robot Learning of Pouring Skills using.pdf:application/pdf},
}

@inproceedings{muhlig_task-level_2009,
	title = {Task-{Level} {Imitation} {Learning} {Using} {Variance}-{Based} {Movement} {Optimization}},
	booktitle = {International {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Muhlig, Manuel and Gienger, Michael and Hellbach, Sven and Steil, Jochen J and Goerick, Christian},
	year = {2009},
	pages = {1177--1184},
	file = {Muhlig et al. - 2009 - Task-Level Imitation Learning Using Variance-Based.pdf:/Users/samueletosatto/Zotero/storage/JQ3TSK77/Muhlig et al. - 2009 - Task-Level Imitation Learning Using Variance-Based.pdf:application/pdf},
}

@inproceedings{pastor_learning_2009,
	title = {Learning and {Generalization} of {Motor} {Skills} by {Learning} from {Demonstration}},
	booktitle = {International {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Pastor, Peter and Hoffmann, Heiko and Asfour, Tamim and Schaal, Stefan},
	year = {2009},
	pages = {763--768},
	file = {Pastor et al. - 2009 - Learning and Generalization of Motor Skills by Lea.pdf:/Users/samueletosatto/Zotero/storage/GB3YPTNT/Pastor et al. - 2009 - Learning and Generalization of Motor Skills by Lea.pdf:application/pdf},
}

@inproceedings{tosatto_exploration_2019,
	title = {Exploration {Driven} by an {Optimistic} {Bellman} {Equation}},
	copyright = {All rights reserved},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Tosatto, Samuele and D'Eramo, Carlo and Pajarinen, Joni and Restelli, Marcello and Peters, Jan},
	year = {2019},
	pages = {1--8},
	file = {Tosatto et al. - 2019 - Exploration Driven by an Optimistic Bellman Equati.pdf:/Users/samueletosatto/Zotero/storage/PPLIFMXT/Tosatto et al. - 2019 - Exploration Driven by an Optimistic Bellman Equati.pdf:application/pdf},
}

@inproceedings{rueckert_learning_2017,
	title = {Learning {Inverse} {Dynamics} {Models} in {O}(n) {Time} with {LSTM} {Networks}},
	booktitle = {2017 {IEEE}-{RAS} 17th {International} {Conference} on {Humanoid} {Robotics} ({Humanoids})},
	publisher = {IEEE},
	author = {Rueckert, Elmar and Nakatenus, Moritz and Tosatto, Samuele and Peters, Jan},
	year = {2017},
	pages = {811--816},
	file = {Rueckert et al. - 2017 - Learning Inverse Dynamics Models in O(n) Time with.pdf:/Users/samueletosatto/Zotero/storage/UG8GZ8K6/Rueckert et al. - 2017 - Learning Inverse Dynamics Models in O(n) Time with.pdf:application/pdf},
}

@article{rosenblatt_conditional_1969,
	title = {Conditional {Probability} {Density} and {Regression} {Estimators}},
	volume = {25},
	journal = {Multivariate analysis II},
	author = {Rosenblatt, Murray},
	year = {1969},
	pages = {31},
}

@phdthesis{johnston_smooth_1979,
	title = {Smooth {Nonparametric} {Regression} {Analysis}.},
	school = {NORTH CAROLINA UNIV AT CHAPEL HILL INST OF STATISTICS},
	author = {Johnston, Gordon J},
	year = {1979},
}

@article{hastie_local_1993,
	title = {Local {Regression}: {Automatic} {Kernel} {Carpentry}},
	journal = {Statistical Science},
	author = {Hastie, Trevor and Loader, Clive},
	year = {1993},
	pages = {120--129},
	file = {Hastie and Loader - 1993 - Local Regression Automatic Kernel Carpentry.pdf:/Users/samueletosatto/Zotero/storage/NB2NTMXG/Hastie and Loader - 1993 - Local Regression Automatic Kernel Carpentry.pdf:application/pdf},
}

@incollection{gasser_kernel_1979,
	title = {Kernel {Estimation} of {Regression} {Functions}},
	booktitle = {Smoothing {Techniques} for {Curve} {Estimation}},
	publisher = {Springer},
	author = {Gasser, Theo and Müller, Hans-Georg},
	year = {1979},
	pages = {23--68},
}

@article{gasser_estimating_1984,
	title = {Estimating {Regression} {Functions} and their {Derivatives} by the {Kernel} {Method}},
	journal = {Scandinavian Journal of Statistics},
	author = {Gasser, Theo and Müller, Hans-Georg},
	year = {1984},
	pages = {171--185},
	file = {Gasser and Müller - 1984 - Estimating regression functions and their derivati.pdf:/Users/samueletosatto/Zotero/storage/78CXQWTM/Gasser and Müller - 1984 - Estimating regression functions and their derivati.pdf:application/pdf},
}

@article{mack_convolution_1988,
	title = {Convolution {Type} {Estimators} for {Nonparametric} {Regression}},
	volume = {7},
	number = {3},
	journal = {Statistics \& probability letters},
	author = {Mack, YP and Müller, Hans-Georg},
	year = {1988},
	pages = {229--239},
	file = {Mack and Müller - 1988 - Convolution Type Estimators for Nonparametric Regr.pdf:/Users/samueletosatto/Zotero/storage/J6M7P8U9/Mack and Müller - 1988 - Convolution Type Estimators for Nonparametric Regr.pdf:application/pdf},
}

@article{chu_choosing_1991,
	title = {Choosing a {Kernel} {Regression} {Estimator}},
	volume = {6},
	number = {4},
	journal = {Statistical Science},
	author = {Chu, C-K and Marron, James S},
	year = {1991},
	pages = {404--419},
	file = {Chu and Marron - 1991 - Choosing a Kernel Regression Estimator.pdf:/Users/samueletosatto/Zotero/storage/XZUHTKD8/Chu and Marron - 1991 - Choosing a Kernel Regression Estimator.pdf:application/pdf},
}

@inproceedings{julier_scaled_2002,
	title = {The {Scaled} {Unscented} {Transformation}},
	volume = {6},
	doi = {10.1109/ACC.2002.1025369},
	booktitle = {Proceedings of the 2002 {American} {Control} {Conference}},
	author = {Julier, S. J.},
	year = {2002},
	keywords = {angular uncertainties, Cartesian coordinates, Cities and towns, coordinate conversion, covariance prediction, Filtering, filtering theory, Filters, Gaussian distribution, identification, Jacobian matrices, mean prediction, nonlinear systems, Nonlinear systems, polar coordinates, probability distribution, Random variables, Sampling methods, scaled unscented transformation, sigma points, statistical analysis, Statistics, truncated filter, Uncertainty, UT},
	pages = {4555--4559 vol.6},
	file = {Julier - 2002 - The Scaled Unscented Transformation.pdf:/Users/samueletosatto/Zotero/storage/IXN79MKM/Julier - 2002 - The Scaled Unscented Transformation.pdf:application/pdf},
}

@inproceedings{schulman_trust_2015,
	title = {Trust {Region} {Policy} {Optimization}},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
	year = {2015},
	pages = {1889--1897},
	file = {Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:/Users/samueletosatto/Zotero/storage/PM4YK37Z/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:application/pdf},
}

@inproceedings{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	booktitle = {Proceeding of the 35th {International} {Conference} on {Machine} {Learning}},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year = {2018},
	pages = {1856--1865},
	file = {Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:/Users/samueletosatto/Zotero/storage/GLEJ6QCJ/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf},
}

@article{baird_residual_1995,
	title = {Residual {Algorithms}: {Reinforcement} {Learning} with {Function} {Approximation}},
	journal = {Machine Learning Proceedings},
	author = {Baird, Leemon},
	year = {1995},
	pages = {30--37},
	file = {Baird - 1995 - Residual Algorithms Reinforcement Learning with F.pdf:/Users/samueletosatto/Zotero/storage/RZQ4W88E/Baird - 1995 - Residual Algorithms Reinforcement Learning with F.pdf:application/pdf},
}

@article{heess_learning_2015,
	title = {Learning {Continuous} {Control} {Policies} by {Stochastic} {Value} {Gradients}},
	volume = {28},
	journal = {Advances in Neural Information Processing Systems},
	author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
	year = {2015},
	pages = {2944--2952},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster {Level} in {StarCraft} {II} using {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {575},
	number = {7782},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and {others}},
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	pages = {350--354},
}

@inproceedings{sutton_convergent_2008,
	title = {A {Convergent} {O}(n) {Algorithm} for {Off}-{Policy} temporal-{Difference} {Learning} with {Linear} {Function} {Approximation}},
	volume = {21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	publisher = {MIT Press},
	author = {Sutton, Richard S and Szepesvári, Csaba and Maei, Hamid Reza},
	year = {2008},
	pages = {1609--1616},
}

@article{rajeswaran_towards_2017,
	title = {Towards {Generalization} and {Simplicity} in {Continuous} {Control}},
	journal = {arXiv preprint arXiv:1703.02660},
	author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel and Kakade, Sham},
	year = {2017},
}

@inproceedings{sutton_fast_2009,
	title = {Fast {Gradient}-{Descent} {Methods} for {Temporal}-{Difference} {Learning} with {Linear} {Function} {Approximation}},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	author = {Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesvári, Csaba and Wiewiora, Eric},
	year = {2009},
	pages = {993--1000},
}

@inproceedings{sutton_fast_2009-1,
	title = {Fast gradient-descent methods for temporal-difference learning with linear function approximation},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	author = {Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesvári, Csaba and Wiewiora, Eric},
	year = {2009},
	pages = {993--1000},
}

@inproceedings{mahmood_setting_2018,
	title = {Setting up a {Reinforcement} {Learning} {Task} with a {Real}-{World} {Robot}},
	doi = {10.1109/IROS.2018.8593894},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Mahmood, Rupam and Korenkevych, Dmytro and Komer, Brent J. and Bergstra, James},
	year = {2018},
	pages = {4635--4640},
}

@article{tsitsiklis_analysis_1997,
	title = {An {Analysis} of {Temporal}-{Difference} {Learning} with {Function} {Approximation}},
	volume = {42},
	number = {5},
	journal = {IEEE Transactions on Automatic Control},
	author = {Tsitsiklis, John N and Van Roy, Benjamin},
	year = {1997},
	note = {Publisher: IEEE},
	pages = {674--690},
}

@inproceedings{tsitsiklis_analysis_1997-1,
	title = {Analysis of {Temporal}-{Diffference} {Learning} with {Function} {Approximation}},
	booktitle = {Advances in neural information processing systems},
	author = {Tsitsiklis, John N and Van Roy, Benjamin},
	year = {1997},
	pages = {1075--1081},
}

@inproceedings{boyan_least-squares_1999,
	title = {Least-{Squares} {Temporal} {Difference} {Learning}},
	booktitle = {Proceedings of the {Sixteenth} {International} {Conference} on {Machine} {Learning}},
	author = {Boyan, Justin A},
	year = {1999},
	pages = {49--56},
}

@article{lagoudakis_least-squares_2003,
	title = {Least-{Squares} {Policy} {Iteration}},
	volume = {4},
	journal = {Journal of Machine Learning Research},
	author = {Lagoudakis, Michail G and Parr, Ronald},
	year = {2003},
	note = {Publisher: JMLR. org},
	pages = {1107--1149},
	file = {Lagoudakis and Parr - 2003 - Least-Squares Policy Iteration.pdf:/Users/samueletosatto/Zotero/storage/96Y557LF/Lagoudakis and Parr - 2003 - Least-Squares Policy Iteration.pdf:application/pdf},
}

@article{hallak_emphatic_2015,
	title = {Emphatic {TD} {Bellman} {Operator} is a {Contraction}},
	journal = {arXiv preprint arXiv:1508.03411},
	author = {Hallak, Assaf and Tamar, Aviv and Mannor, Shie},
	year = {2015},
}

@inproceedings{hallak_generalized_2016,
	title = {Generalized {Emphatic} {Temporal} {Difference} {Learning}: {Bias}-{Variance} {Analysis}},
	booktitle = {Thirtieth {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Hallak, Assaf and Tamar, Aviv and Munos, Rémi and Mannor, Shie},
	year = {2016},
}

@article{sutton_learning_1988,
	title = {Learning to {Predict} by the {Methods} of {Temporal} {Differences}},
	volume = {3},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Sutton, Richard S},
	year = {1988},
	note = {Publisher: Springer},
	pages = {9--44},
}

@inproceedings{kolter_fixed_2011,
	title = {The {Fixed} {Points} of {Off}-{Policy} {TD}},
	volume = {24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kolter, J},
	year = {2011},
	pages = {2169--2177},
	file = {Kolter - 2011 - The Fixed Points of Off-Policy TD.pdf:/Users/samueletosatto/Zotero/storage/EHZ92AC4/Kolter - 2011 - The Fixed Points of Off-Policy TD.pdf:application/pdf},
}

@inproceedings{kolter_regularization_2009,
	title = {Regularization and {Feature} {Selection} in {Least}-{Squares} {Temporal} {Difference} {Learning}},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	author = {Kolter, J Zico and Ng, Andrew Y},
	year = {2009},
	pages = {521--528},
	file = {Kolter and Ng - 2009 - Regularization and Feature Selection in Least-Squa.pdf:/Users/samueletosatto/Zotero/storage/KDC6ZECR/Kolter and Ng - 2009 - Regularization and Feature Selection in Least-Squa.pdf:application/pdf},
}

@inproceedings{ghiassian_gradient_2020,
	title = {Gradient {Temporal}-{Difference} {Learning} with {Regularized} {Corrections}},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ghiassian, Sina and Patterson, Andrew and Garg, Shivam and Gupta, Dhawal and White, Adam and White, Martha},
	year = {2020},
	pages = {3524--3534},
	file = {Ghiassian et al. - 2020 - Gradient Temporal-Difference Learning with Regular.pdf:/Users/samueletosatto/Zotero/storage/BTZA34TU/Ghiassian et al. - 2020 - Gradient Temporal-Difference Learning with Regular.pdf:application/pdf},
}

@inproceedings{mahmood_notes_2021,
	title = {Notes: {A} {Temporal} {Difference} {Approach} to {Policy} {Gradient} {Estimation}},
	author = {Mahmood, Rupam},
	year = {2021},
	file = {Mahmood - 2021 - Notes A Temporal Difference Approach to Policy Gr.pdf:/Users/samueletosatto/Zotero/storage/IFUGBLZU/Mahmood - 2021 - Notes A Temporal Difference Approach to Policy Gr.pdf:application/pdf},
}

@article{zhang_generalized_2019,
	title = {Generalized {Off}-{Policy} {Actor}-{Critic}},
	journal = {arXiv preprint arXiv:1903.11329},
	author = {Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
	year = {2019},
}

@article{tosatto_batch_2021,
	title = {Batch {Reinforcement} {Learning} with a {Nonparametric} {Off}-{Policy} {Policy} {Gradient}},
	doi = {10.1109/TPAMI.2021.3088063},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tosatto, Samuele and Carvalho, Joao and Peters, Jan},
	year = {2021},
	pages = {1--1},
}

@article{kingma_adam_2014,
	title = {{ADAM}: {A} {Method} for {Stochastic} {Optimization}},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@inproceedings{andrychowicz_learning_2016,
	title = {Learning to {Learn} by {Gradient} {Descent} by {Gradient} {Descent}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio Gómez and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	year = {2016},
}

@incollection{bengio_optimization_2013,
	title = {On the {Optimization} of a {Synaptic} {Learning} {Rule}},
	booktitle = {Optimality in {Biological} and {Artificial} {Networks}?},
	publisher = {Routledge},
	author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gescei, Jan},
	year = {2013},
	pages = {281--303},
}

@inproceedings{chen_learning_2017,
	title = {Learning to {Learn} without {Gradient} {Descent} by {Gradient} {Descent}},
	booktitle = {Proceeding of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Yutian and Hoffman, Matthew W and Colmenarejo, Sergio Gómez and Denil, Misha and Lillicrap, Timothy P and Botvinick, Matt and Freitas, Nando},
	year = {2017},
	pages = {748--756},
}

@inproceedings{ji_learning_2019,
	title = {Learning to {Learn} {Gradient} {Aggregation} by {Gradient} {Descent}},
	booktitle = {Proceedings of the {International} {Joint} {Conference} of {Artificial} {Intelligence}},
	author = {Ji, Jinlong and Chen, Xuhui and Wang, Qianlong and Yu, Lixing and Li, Pan},
	year = {2019},
	pages = {2614--2620},
}

@inproceedings{denevi_learning--learn_2019,
	title = {Learning-to-{Learn} {Stochastic} {Gradient} {Descent} with {Biased} {Regularization}},
	booktitle = {Proceeding of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Denevi, Giulia and Ciliberto, Carlo and Grazzi, Riccardo and Pontil, Massimiliano},
	year = {2019},
	pages = {1566--1575},
}

@article{ward_reminiscence_1937,
	title = {Reminiscence and {Rote} {Learning}.},
	volume = {49},
	number = {4},
	journal = {Psychological Monographs},
	author = {Ward, Lewis B},
	year = {1937},
	note = {Publisher: Psychological Review Company},
	pages = {i},
}

@article{harlow_formation_1949,
	title = {The {Formation} of {Learning} {Sets}.},
	volume = {56},
	number = {1},
	journal = {Psychological review},
	author = {Harlow, Harry F},
	year = {1949},
	note = {Publisher: American Psychological Association},
	pages = {51},
}

@inproceedings{noauthor_exploiting_nodate,
	title = {Exploiting {Minimum}-{Variance} {Policy} {Evaluation} for {Policy} {Optimization}},
	file = {Exploiting Minimum-Variance Policy Evaluation for .pdf:/Users/samueletosatto/Zotero/storage/RA5SBWZJ/Exploiting Minimum-Variance Policy Evaluation for .pdf:application/pdf},
}

@inproceedings{noauthor_risk-sensitive_nodate,
	title = {A {Risk}-{Sensitive} {Policy} {Gradient} {Method}},
	file = {A Risk-Sensitive Policy Gradient Method.pdf:/Users/samueletosatto/Zotero/storage/Y9FIQPTS/A Risk-Sensitive Policy Gradient Method.pdf:application/pdf},
}

@inproceedings{noauthor_learning_nodate,
	title = {Learning {Transferable} {Motor} {Skills} with {Hierarchical} {Latent} {Mixture} {Policies}},
	file = {Learning Transferable Motor Skills with Hierarchic.pdf:/Users/samueletosatto/Zotero/storage/5RBMRWZE/Learning Transferable Motor Skills with Hierarchic.pdf:application/pdf},
}

@inproceedings{mahmood_benchmarking_2018,
	title = {Benchmarking {Reinforcement} {Learning} {Algorithms} on {Real}-{World} {Robots}},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Mahmood, A Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
	year = {2018},
	pages = {561--591},
}

@inproceedings{mahmood_setting_2018-1,
	title = {Setting up a {Reinforcement} {Learning} {Task} with a {Real}-{World} {Robot}},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Mahmood, A Rupam and Korenkevych, Dmytro and Komer, Brent J and Bergstra, James},
	year = {2018},
	pages = {4635--4640},
}

@inproceedings{mahmood_representation_2013,
	title = {Representation {Search} {Through} {Generate} and {Test}},
	booktitle = {Workshops at the {Twenty}-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Mahmood, Ashique Rupam and Sutton, Richard S},
	year = {2013},
}

@article{dohare_continual_2021,
	title = {Continual {Backprop}: {Stochastic} {Gradient} {Descent} with {Persistent} {Randomness}},
	journal = {arXiv preprint arXiv:2108.06325},
	author = {Dohare, Shibhansh and Mahmood, A Rupam and Sutton, Richard S},
	year = {2021},
}

@inproceedings{jakobi_noise_1995,
	title = {Noise and the {Reality} {Gap}: {The} use of {Simulation} in {Evolutionary} {Robotics}},
	booktitle = {European {Conference} on {Artificial} {Life}},
	publisher = {Springer},
	author = {Jakobi, Nick and Husbands, Phil and Harvey, Inman},
	year = {1995},
	pages = {704--720},
}

@inproceedings{muratore_domain_2018,
	title = {Domain {Randomization} for {Simulation}-{Based} {Policy} {Optimization} with {Transferability} {Assessment}},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Muratore, Fabio and Treede, Felix and Gienger, Michael and Peters, Jan},
	year = {2018},
	pages = {700--713},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and {Semi}-{MDPs}: {A} {Framework} for {Temporal} {Abstraction} in {Reinforcement} {Learning}},
	volume = {112},
	number = {1-2},
	journal = {Artificial intelligence},
	author = {Sutton, Richard S and Precup, Doina and Singh, Satinder},
	year = {1999},
	note = {Publisher: Elsevier},
	pages = {181--211},
}

@inproceedings{noauthor_yans_nodate,
	title = {Yan's {Proposal}},
	file = {Yan's Proposal.pdf:/Users/samueletosatto/Zotero/storage/9YAEX4VQ/Yan's Proposal.pdf:application/pdf},
}

@article{noauthor_maximum_nodate,
	title = {Maximum {Block} {Energy} {Guided} {Robust} {Subspace} {Clustering}},
	file = {Maximum Block Energy Guided Robust Subspace Cluste.pdf:/Users/samueletosatto/Zotero/storage/P2MPWKBH/Maximum Block Energy Guided Robust Subspace Cluste.pdf:application/pdf},
}

@article{noauthor_maximum_nodate-1,
	title = {Maximum {Block} {Energy} {Guided} {Robust} {Subspace} {Clustering} 2},
	file = {Maximum Block Energy Guided Robust Subspace Cluste.pdf:/Users/samueletosatto/Zotero/storage/LQNLWKSN/Maximum Block Energy Guided Robust Subspace Cluste.pdf:application/pdf},
}

@inproceedings{garg_alternate_nodate,
	title = {Alternate {Gradient}},
	author = {Garg, Shivam},
	file = {Garg - Alternate Gradient.pdf:/Users/samueletosatto/Zotero/storage/H8KLS6ZM/Garg - Alternate Gradient.pdf:application/pdf},
}

@inproceedings{the_beatles_michelle_nodate,
	title = {Michelle},
	author = {{The Beatles}},
	file = {The Beatles - Michelle.pdf:/Users/samueletosatto/Zotero/storage/KMX88ZNS/The Beatles - Michelle.pdf:application/pdf},
}

@inproceedings{the_beatles_hey_nodate,
	title = {Hey {Jude}},
	author = {{The Beatles}},
	file = {The Beatles - Hey Jude.pdf:/Users/samueletosatto/Zotero/storage/I42ENWK3/The Beatles - Hey Jude.pdf:application/pdf},
}

@inproceedings{the_beatles_my_nodate,
	title = {In {My} {Life}},
	author = {{The Beatles}},
	file = {The Beatles - In Mty Life.pdf:/Users/samueletosatto/Zotero/storage/LGTCIFFS/The Beatles - In Mty Life.pdf:application/pdf},
}

@inproceedings{presley_i_nodate,
	title = {I {Can}'t {Help} {Falling} in {Love}},
	author = {Presley, Elvis},
	file = {Presley - I Can't Help Falling in Love.pdf:/Users/samueletosatto/Zotero/storage/NQL4LUDK/Presley - I Can't Help Falling in Love.pdf:application/pdf},
}

@inproceedings{oasis_wonderwall_nodate,
	title = {Wonderwall},
	author = {{Oasis}},
	file = {Oasis - Wonderwall.pdf:/Users/samueletosatto/Zotero/storage/8MFBIRAC/Oasis - Wonderwall.pdf:application/pdf},
}

@inproceedings{coldplay_yellow_nodate,
	title = {Yellow},
	author = {{Coldplay}},
	file = {Coldplay - Yellow.pdf:/Users/samueletosatto/Zotero/storage/SQH9NDJI/Coldplay - Yellow.pdf:application/pdf},
}

@inproceedings{yang_representation_nodate,
	title = {Representation {Matters}: {Offline} {Pretraining} for {Sequential} {Decision} {Making}},
	author = {Yang, Mengjiao and Nachum, Ofir},
	file = {Yang and Nachum - Representation Matters Offline Pretraining for Se.pdf:/Users/samueletosatto/Zotero/storage/RUXU6VQR/Yang and Nachum - Representation Matters Offline Pretraining for Se.pdf:application/pdf},
}

@inproceedings{nachum_provable_nodate,
	title = {Provable {Representation} {Learning} for {Imitation} with {Contrastive} {Fourier} {Features}},
	author = {Nachum, Ofir and Yang, Mengjiao},
	file = {Nachum and Yang - Provable Representation Learning for Imitation wit.pdf:/Users/samueletosatto/Zotero/storage/B7JUY783/Nachum and Yang - Provable Representation Learning for Imitation wit.pdf:application/pdf},
}

@inproceedings{yang_trail_nodate,
	title = {{TRAIL}: {Near}-{Optimal} {Imitation} {Learning} with {Suboptimal} {Data}},
	author = {Yang, Mengjiao and Levine, Sergey and Nachum, Ofir},
	file = {Yang et al. - TRAIL Near-Optimal Imitation Learning with Subopt.pdf:/Users/samueletosatto/Zotero/storage/J8ER7SSG/Yang et al. - TRAIL Near-Optimal Imitation Learning with Subopt.pdf:application/pdf},
}

@article{surovik_learning_2021,
	title = {Learning an {Expert} {Skill}-{Space} for {Replanning} {Dynamic} {Quadruped} {Locomotion} over {Obstacles}},
	author = {Surovik, David and Melon, Oliwier and Geisert, Mathieu and Fallon, Maurice and Havoutis, Ioannis},
	year = {2021},
	note = {Publisher: Journal of Machine Learning Research},
	file = {Surovik et al. - 2021 - Learning an Expert Skill-Space for Replanning Dyna.pdf:/Users/samueletosatto/Zotero/storage/WNGWW7R9/Surovik et al. - 2021 - Learning an Expert Skill-Space for Replanning Dyna.pdf:application/pdf},
}

@article{tanneberg_skid_2021,
	title = {{SKID} {RAW}: {Skill} {Discovery} from {Raw} {Trajectories}},
	volume = {6},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tanneberg, Daniel and Ploeger, Kai and Rueckert, Elmar and Peters, Jan},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {4696--4703},
	file = {tanneberg_skid_2021.pdf:/Users/samueletosatto/Documents/zotero/tanneberg_skid_2021.pdf:application/pdf},
}

@inproceedings{haarnoja_latent_2018,
	title = {Latent {Space} {Policies} for {Hierarchical} {Reinforcement} {Learning}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
	year = {2018},
	pages = {1851--1860},
	file = {Haarnoja et al. - 2018 - Latent Space Policies for Hierarchical Reinforceme.pdf:/Users/samueletosatto/Zotero/storage/5ZC4TQLA/Haarnoja et al. - 2018 - Latent Space Policies for Hierarchical Reinforceme.pdf:application/pdf},
}

@inproceedings{shankar_learning_2020,
	title = {Learning {Robot} {Skills} with {Temporal} {Variational} {Inference}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shankar, Tanmay and Gupta, Abhinav},
	year = {2020},
	pages = {8624--8633},
	file = {Shankar and Gupta - 2020 - Learning Robot Skills with Temporal Variational In.pdf:/Users/samueletosatto/Zotero/storage/KS3JTQJL/Shankar and Gupta - 2020 - Learning Robot Skills with Temporal Variational In.pdf:application/pdf},
}

@inproceedings{noseworthy_task-conditioned_2020,
	title = {Task-{Conditioned} {Variational} {Autoencoders} for {Learning} {Movement} {Primitives}},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Noseworthy, Michael and Paul, Rohan and Roy, Subhro and Park, Daehyung and Roy, Nicholas},
	year = {2020},
	pages = {933--944},
	file = {Noseworthy et al. - 2020 - Task-Conditioned Variational Autoencoders for Lear.pdf:/Users/samueletosatto/Zotero/storage/TDK4KTZE/Noseworthy et al. - 2020 - Task-Conditioned Variational Autoencoders for Lear.pdf:application/pdf},
}

@inproceedings{chen_dynamic_2016-1,
	title = {Dynamic {Movement} {Primitives} in {Latent} {Space} of {Time}-{Dependent} {Variational} {Autoencoders}},
	booktitle = {2016 {IEEE}-{RAS} 16th international conference on humanoid robots ({Humanoids})},
	publisher = {IEEE},
	author = {Chen, Nutan and Karl, Maximilian and Van Der Smagt, Patrick},
	year = {2016},
	pages = {629--636},
	file = {Chen et al. - 2016 - Dynamic Movement Primitives in Latent Space of Tim.pdf:/Users/samueletosatto/Zotero/storage/J7ZTZITJ/Chen et al. - 2016 - Dynamic Movement Primitives in Latent Space of Tim.pdf:application/pdf},
}

@article{graves_lispr_2020,
	title = {Lispr: {An} {Options} {Framework} for {Policy} {Reuse} with {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2012.14942},
	author = {Graves, Daniel and Jin, Jun and Luo, Jun},
	year = {2020},
	file = {Graves et al. - 2020 - Lispr An Options Framework for Policy Reuse with .pdf:/Users/samueletosatto/Zotero/storage/W8EXHSUN/Graves et al. - 2020 - Lispr An Options Framework for Policy Reuse with .pdf:application/pdf},
}

@article{jin_offline_2020,
	title = {Offline {Learning} of {Counterfactual} {Perception} as {Prediction} for {Real}-{World} {Robotic} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2011.05857},
	author = {Jin, Jun and Graves, Daniel and Haigh, Cameron and Luo, Jun and Jagersand, Martin},
	year = {2020},
	file = {Jin et al. - 2020 - Offline Learning of Counterfactual Perception as P.pdf:/Users/samueletosatto/Zotero/storage/B2UFF4NA/Jin et al. - 2020 - Offline Learning of Counterfactual Perception as P.pdf:application/pdf},
}

@misc{noauthor_learning_nodate-1,
	title = {Learning transferable motor skills with hierarchical latent mixture policies},
	file = {Learning transferable motor skills with hierarchic.pdf:/Users/samueletosatto/Zotero/storage/2XY3YISW/Learning transferable motor skills with hierarchic.pdf:application/pdf},
}

@article{galashov_information_2019,
	title = {Information {Asymmetry} in {KL}-{Regularized} {RL}},
	journal = {arXiv preprint arXiv:1905.01240},
	author = {Galashov, Alexandre and Jayakumar, Siddhant M and Hasenclever, Leonard and Tirumala, Dhruva and Schwarz, Jonathan and Desjardins, Guillaume and Czarnecki, Wojciech M and Teh, Yee Whye and Pascanu, Razvan and Heess, Nicolas},
	year = {2019},
}

@article{pateria_hierarchical_2021,
	title = {Hierarchical {Reinforcement} {Learning}: {A} {Comprehensive} {Survey}},
	volume = {54},
	number = {5},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
	year = {2021},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--35},
	file = {Pateria et al. - 2021 - Hierarchical Reinforcement Learning A Comprehensi.pdf:/Users/samueletosatto/Zotero/storage/3DLJRQTQ/Pateria et al. - 2021 - Hierarchical Reinforcement Learning A Comprehensi.pdf:application/pdf},
}

@inproceedings{vezhnevets_feudal_2017,
	title = {Feudal {Networks} for {Hierarchical} {Reinforcement} {Learning}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
	year = {2017},
	pages = {3540--3549},
	file = {Vezhnevets et al. - 2017 - Feudal Networks for Hierarchical Reinforcement Lea.pdf:/Users/samueletosatto/Zotero/storage/VRX9FYST/Vezhnevets et al. - 2017 - Feudal Networks for Hierarchical Reinforcement Lea.pdf:application/pdf},
}

@inproceedings{ramapuram_lifelong_nodate,
	title = {Lifelong {Generative} {Modelling}},
	author = {Ramapuram, Jason and Gregorova, Magda and Kalousis, Alexandros},
	file = {Ramapuram et al. - Lifelong Generative Modelling.pdf:/Users/samueletosatto/Zotero/storage/VG78SBTX/Ramapuram et al. - Lifelong Generative Modelling.pdf:application/pdf},
}

@inproceedings{shin_continual_2017,
	title = {Continual {Learning} with {Deep} {Generative} {Replay}},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	year = {2017},
	pages = {2994--3003},
	file = {Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:/Users/samueletosatto/Zotero/storage/27SAJK9K/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:application/pdf},
}

@article{kamra_deep_2017,
	title = {Deep {Generative} {Dual} {Memory} {Network} for {Continual} {Learning}},
	journal = {arXiv preprint arXiv:1710.10368},
	author = {Kamra, Nitin and Gupta, Umang and Liu, Yan},
	year = {2017},
	file = {Kamra et al. - 2017 - Deep Generative Dual Memory Network for Continual .pdf:/Users/samueletosatto/Zotero/storage/MCHTAVQW/Kamra et al. - 2017 - Deep Generative Dual Memory Network for Continual .pdf:application/pdf},
}

@article{bai_deep_2019,
	title = {Deep {Equilibrium} {Models}},
	journal = {arXiv preprint arXiv:1909.01377},
	author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	year = {2019},
	file = {Bai et al. - 2019 - Deep Equilibrium Models.pdf:/Users/samueletosatto/Zotero/storage/EMQWQ7RR/Bai et al. - 2019 - Deep Equilibrium Models.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} {You} {Need}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
	file = {Vaswani et al. - 2017 - Attention is All You Need.pdf:/Users/samueletosatto/Zotero/storage/48X2LEG2/Vaswani et al. - 2017 - Attention is All You Need.pdf:application/pdf},
}

@article{singh_parrot_2020,
	title = {Parrot: {Data}-{Driven} {Behavioral} {Priors} for {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:2011.10024},
	author = {Singh, Avi and Liu, Huihan and Zhou, Gaoyue and Yu, Albert and Rhinehart, Nicholas and Levine, Sergey},
	year = {2020},
	file = {Singh et al. - 2020 - Parrot Data-driven behavioral priors for reinforc.pdf:/Users/samueletosatto/Zotero/storage/DSLPPDHY/Singh et al. - 2020 - Parrot Data-driven behavioral priors for reinforc.pdf:application/pdf},
}

@article{pertsch_accelerating_2020,
	title = {Accelerating {Reinforcement} {Learning} with {Learned} {Skill} {Priors}},
	journal = {arXiv preprint arXiv:2010.11944},
	author = {Pertsch, Karl and Lee, Youngwoon and Lim, Joseph J},
	year = {2020},
	file = {Pertsch et al. - 2020 - Accelerating Reinforcement Learning with Learned S.pdf:/Users/samueletosatto/Zotero/storage/2BS44NE2/Pertsch et al. - 2020 - Accelerating Reinforcement Learning with Learned S.pdf:application/pdf},
}

@article{bahl_neural_2020,
	title = {Neural {Dynamic} {Policies} for {End}-to-{End} {Sensorimotor} {Learning}},
	journal = {arXiv preprint arXiv:2012.02788},
	author = {Bahl, Shikhar and Mukadam, Mustafa and Gupta, Abhinav and Pathak, Deepak},
	year = {2020},
	file = {Bahl et al. - 2020 - Neural Dynamic Policies for End-to-End Sensorimoto.pdf:/Users/samueletosatto/Zotero/storage/TT3TSJRC/Bahl et al. - 2020 - Neural Dynamic Policies for End-to-End Sensorimoto.pdf:application/pdf},
}

@article{noauthor_notitle_nodate-2,
}

@article{schrittwieser_mastering_2020,
	title = {Mastering {Atari}, {Go}, {Chess} and {Shogi} by {Planning} with a {Learned} {Model}},
	volume = {588},
	number = {7839},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and {others}},
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {604--609},
}

@article{graves_off-policy_2021,
	title = {Off-{Policy} {Actor}-{Critic} with {Emphatic} {Weightings}},
	journal = {arXiv preprint arXiv:2111.08172},
	author = {Graves, Eric and Imani, Ehsan and Kumaraswamy, Raksha and White, Martha},
	year = {2021},
	file = {Graves et al. - 2021 - Off-Policy Actor-Critic with Emphatic Weightings.pdf:/Users/samueletosatto/Zotero/storage/NHM3DZZI/Graves et al. - 2021 - Off-Policy Actor-Critic with Emphatic Weightings.pdf:application/pdf},
}

@article{graves_off-policy_2021-1,
	title = {Off-{Policy} {Actor}-{Critic} with {Emphatic} {Weightings} - copy},
	journal = {arXiv preprint arXiv:2111.08172},
	author = {Graves, Eric and Imani, Ehsan and Kumaraswamy, Raksha and White, Martha},
	year = {2021},
	file = {Graves et al. - 2021 - Off-Policy Actor-Critic with Emphatic Weightings -.pdf:/Users/samueletosatto/Zotero/storage/PHB2KS6T/Graves et al. - 2021 - Off-Policy Actor-Critic with Emphatic Weightings -.pdf:application/pdf},
}

@inproceedings{noauthor_temporal_nodate,
	title = {A temporal {Difference} {Approach} to {Policy} {Gradient} {Estimation}},
	file = {A temporal Difference Approach to Policy Gradient .pdf:/Users/samueletosatto/Zotero/storage/GY6G8SIJ/A temporal Difference Approach to Policy Gradient .pdf:application/pdf},
}

@article{graves_off-policy_2021-2,
	title = {Off-{Policy} {Actor}-{Critic} with {Emphatic} {Weightings}},
	journal = {arXiv preprint arXiv:2111.08172},
	author = {Graves, Eric and Imani, Ehsan and Kumaraswamy, Raksha and White, Martha},
	year = {2021},
}

@inproceedings{xie_deep_2021,
	title = {Deep {Reinforcement} {Learning} amidst {Continual} {Structured} {Non}-{Stationarity}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xie, Annie and Harrison, James and Finn, Chelsea},
	year = {2021},
	pages = {11393--11403},
}

@incollection{thrun_lifelong_1998,
	title = {Lifelong {Learning} {Algorithms}},
	booktitle = {Learning to {Learn}},
	publisher = {Springer},
	author = {Thrun, Sebastian},
	year = {1998},
	pages = {181--209},
}

@inproceedings{silver_lifelong_2013,
	title = {Lifelong {Machine} {Learning} {Systems}: {Beyond} {Learning} {Algorithms}},
	booktitle = {2013 {AAAI} {Spring} {Symposium} {Series}},
	author = {Silver, Daniel L and Yang, Qiang and Li, Lianghao},
	year = {2013},
}

@inproceedings{kober_learning_2009,
	title = {Learning {Motor} {Primitives} for {Robotics}},
	booktitle = {2009 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Kober, Jens and Peters, Jan},
	year = {2009},
	pages = {2112--2118},
}

@article{ploeger_high_2020,
	title = {High {Acceleration} {Reinforcement} {Learning} for {Real}-{World} {Juggling} with {Binary} {Rewards}},
	journal = {arXiv preprint arXiv:2010.13483},
	author = {Ploeger, Kai and Lutter, Michael and Peters, Jan},
	year = {2020},
}

@article{dalal_accelerating_2021,
	title = {Accelerating {Robotic} {Reinforcement} {Learning} via {Parameterized} {Action} {Primitives}},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dalal, Murtaza and Pathak, Deepak and Salakhutdinov, Russ R},
	year = {2021},
}

@article{liu_skill_2020,
	title = {Skill {Transfer} {Learning} for {Autonomous} {Robots} and {Human}–{Robot} {Cooperation}: {A} {Survey}},
	volume = {128},
	journal = {Robotics and Autonomous Systems},
	author = {Liu, Yueyue and Li, Zhijun and Liu, Huaping and Kan, Zhen},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {103515},
}

@inproceedings{bacon_option-critic_2017,
	title = {The {Option}-{Critic} {Architecture}},
	volume = {31},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	year = {2017},
	note = {Issue: 1},
}

@inproceedings{kingma_stochastic_2014,
	title = {Stochastic {Gradient} {VB} and the {Variational} {Auto}-{Encoder}},
	volume = {19},
	booktitle = {Second {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Kingma, Diederik P and Welling, Max},
	year = {2014},
	pages = {121},
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	journal = {arXiv preprint arXiv:1906.02691},
	author = {Kingma, Diederik P and Welling, Max},
	year = {2019},
}

@article{girin_dynamical_2020,
	title = {Dynamical {Variational} {Autoencoders}: {A} {Comprehensive} {Review}},
	journal = {arXiv preprint arXiv:2008.12595},
	author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
	year = {2020},
	file = {Girin et al. - 2020 - Dynamical Variational Autoencoders A Comprehensiv.pdf:/Users/samueletosatto/Zotero/storage/R99GV8QV/Girin et al. - 2020 - Dynamical Variational Autoencoders A Comprehensiv.pdf:application/pdf},
}

@article{lim_time-series_2021,
	title = {Time-{Series} {Forecasting} with {Deep} {Learning}: a {Survey}},
	volume = {379},
	number = {2194},
	journal = {Philosophical Transactions of the Royal Society A},
	author = {Lim, Bryan and Zohren, Stefan},
	year = {2021},
	note = {Publisher: The Royal Society Publishing},
	pages = {20200209},
}

@article{ha_hypernetworks_2016,
	title = {{HyperNetworks}},
	journal = {arXiv preprint arXiv:1609.09106},
	author = {Ha, David and Dai, Andrew and Le, Quoc V},
	year = {2016},
}

@article{bahl_hierarchical_2021,
	title = {Hierarchical {Neural} {Dynamic} {Policies}},
	journal = {arXiv preprint arXiv:2107.05627},
	author = {Bahl, Shikhar and Gupta, Abhinav and Pathak, Deepak},
	year = {2021},
}

@inproceedings{carvalho_empirical_2021,
	title = {An {Empirical} {Analysis} of {Measure}-{Valued} {Derivatives} for {Policy} {Gradients}},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Carvalho, João and Tateo, Davide and Muratore, Fabio and Peters, Jan},
	year = {2021},
	pages = {1--10},
}

@inproceedings{kipf_compile_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{CompILE}: {Compositional} {Imitation} {Learning} and {Execution}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/kipf19a.html},
	abstract = {We introduce Compositional Imitation Learning and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. CompILE uses a novel unsupervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-composed and executed to perform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate CompILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsupervised manner. Latent codes and associated behavior policies discovered by CompILE can be used by a hierarchical agent, where the high-level policy selects actions in the latent code space, and the low-level, task-specific policies are simply the learned decoders. We found that our CompILE-based agent could learn given only sparse rewards, where agents without task-specific policies struggle.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kipf, Thomas and Li, Yujia and Dai, Hanjun and Zambaldi, Vinicius and Sanchez-Gonzalez, Alvaro and Grefenstette, Edward and Kohli, Pushmeet and Battaglia, Peter},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {3418--3428},
}

@inproceedings{gupta_relay_2020,
	title = {Relay {Policy} {Learning}: {Solving} {Long}-{Horizon} {Tasks} via {Imitation} and {Reinforcement} {Learning}},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
	year = {2020},
	pages = {1025--1037},
}

@inproceedings{sharma_directed-info_2018,
	title = {Directed-{Info} {GAIL}: {Learning} {Hierarchical} {Policies} from {Unsegmented} {Demonstrations} using {Directed} {Information}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sharma, Mohit and Sharma, Arjun and Rhinehart, Nicholas and Kitani, Kris M},
	year = {2018},
}

@article{ijspeert_learning_2002,
	title = {Learning {Attractor} {Landscapes} for {Learning} {Motor} {Primitives}},
	volume = {15},
	journal = {Advances in neural information processing systems},
	author = {Ijspeert, Auke and Nakanishi, Jun and Schaal, Stefan},
	year = {2002},
}

@inproceedings{urain_imitationflow_2020,
	title = {Imitationflow: {Learning} {Deep} {Stable} {Stochastic} {Dynamic} {Systems} by {Normalizing} {Flows}},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Urain, Julen and Ginesi, Michele and Tateo, Davide and Peters, Jan},
	year = {2020},
	pages = {5231--5237},
}

@article{tosatto_temporal-difference_2022,
	title = {A {Temporal}-{Difference} {Approach} to {Policy} {Gradient} {Estimation}},
	journal = {arXiv preprint arXiv:2202.02396},
	author = {Tosatto, Samuele and Patterson, Andrew and White, Martha and Mahmood, A Rupam},
	year = {2022},
}

@article{lachiheb_hierarchical_2018,
	title = {A {Hierarchical} {Deep} {Neural} {Network} {Design} for {Stock} {Returns} {Prediction}},
	volume = {126},
	journal = {Procedia Computer Science},
	author = {Lachiheb, Oussama and Gouider, Mohamed Salah},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {264--272},
}

@article{li_hierarchical_2015,
	title = {A {Hierarchical} {Neural} {Autoencoder} for {Paragraphs} and {Documents}},
	journal = {arXiv preprint arXiv:1506.01057},
	author = {Li, Jiwei and Luong, Minh-Thang and Jurafsky, Dan},
	year = {2015},
}

@inproceedings{guo_recurrent_2020,
	title = {Recurrent {Hierarchical} {Topic}-{Guided} {RNN} for {Language} {Generation}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Guo, Dandan and Chen, Bo and Lu, Ruiying and Zhou, Mingyuan},
	year = {2020},
	pages = {3810--3821},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	number = {8},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	note = {Publisher: MIT Press},
	pages = {1735--1780},
}

@article{ridge_training_2020,
	title = {Training of deep neural networks for the generation of dynamic movement primitives},
	volume = {127},
	journal = {Neural Networks},
	author = {Ridge, Barry and Gams, Andrej and Morimoto, Jun and Ude, Aleš and {others}},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {121--131},
}

@article{ridge_training_2020-1,
	title = {Training of {Deep} {Neural} {Networks} for the {Generation} of {Dynamic} {Movement} {Primitives}},
	volume = {127},
	journal = {Neural Networks},
	author = {Ridge, Barry and Gams, Andrej and Morimoto, Jun and Ude, Aleš and {others}},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {121--131},
}

@inproceedings{pervez_learning_2017,
	title = {Learning {Deep} {Movement} {Primitives} using {Convolutional} {Neural} {Networks}},
	booktitle = {2017 {IEEE}-{RAS} 17th international conference on humanoid robotics ({Humanoids})},
	publisher = {IEEE},
	author = {Pervez, Affan and Mao, Yuecheng and Lee, Dongheui},
	year = {2017},
	pages = {191--197},
}

@article{garnelo_neural_2018,
	title = {Neural {Processes}},
	journal = {arXiv preprint arXiv:1807.01622},
	author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
	year = {2018},
	file = {Garnelo et al. - 2018 - Neural Processes.pdf:/Users/samueletosatto/Zotero/storage/ZUSV2MY6/Garnelo et al. - 2018 - Neural Processes.pdf:application/pdf},
}

@inproceedings{garnelo_conditional_2018,
	title = {Conditional {Neural} {Processes}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, SM Ali},
	year = {2018},
	pages = {1704--1713},
	file = {Garnelo et al. - 2018 - Conditional Neural Processes.pdf:/Users/samueletosatto/Zotero/storage/ML3R6U7P/Garnelo et al. - 2018 - Conditional Neural Processes.pdf:application/pdf},
}

@article{gordon_convolutional_2019,
	title = {Convolutional {Conditional} {Neural} {Processes}},
	journal = {arXiv preprint arXiv:1910.13556},
	author = {Gordon, Jonathan and Bruinsma, Wessel P and Foong, Andrew YK and Requeima, James and Dubois, Yann and Turner, Richard E},
	year = {2019},
	file = {Gordon et al. - 2019 - Convolutional Conditional Neural Processes.pdf:/Users/samueletosatto/Zotero/storage/4J3RMZ9Y/Gordon et al. - 2019 - Convolutional Conditional Neural Processes.pdf:application/pdf},
}

@article{zaheer_deep_2017,
	title = {Deep {Sets}},
	volume = {30},
	journal = {Advances in neural information processing systems},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
	year = {2017},
}

@inproceedings{zaheer_deep_2017-1,
	title = {Deep {Sets}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	file = {Zaheer et al. - 2017 - Deep Sets.pdf:/Users/samueletosatto/Zotero/storage/EDXAEFPW/Zaheer et al. - 2017 - Deep Sets.pdf:application/pdf},
}

@inproceedings{seker_conditional_2019,
	title = {Conditional {Neural} {Movement} {Primitives}.},
	volume = {10},
	booktitle = {Robotics: {Science} and {Systems}},
	author = {Seker, Muhammet Yunus and Imre, Mert and Piater, Justus H and Ugur, Emre},
	year = {2019},
	file = {Seker et al. - 2019 - Conditional Neural Movement Primitives..pdf:/Users/samueletosatto/Zotero/storage/WZM797UQ/Seker et al. - 2019 - Conditional Neural Movement Primitives..pdf:application/pdf},
}

@inproceedings{pervez_learning_2017-1,
	title = {Learning deep movement primitives using convolutional neural networks},
	booktitle = {2017 {IEEE}-{RAS} 17th international conference on humanoid robotics ({Humanoids})},
	publisher = {IEEE},
	author = {Pervez, Affan and Mao, Yuecheng and Lee, Dongheui},
	year = {2017},
	pages = {191--197},
}

@inproceedings{james_coarse--fine_nodate,
	title = {Coarse-to-fine {Q}-attention with {Tree} {Expansion}},
	author = {James, Stephen and Abbeel, Pieter},
	file = {James and Abbeel - Coarse-to-fine Q-attention with Tree Expansion.pdf:/Users/samueletosatto/Zotero/storage/IB74DNW3/James and Abbeel - Coarse-to-fine Q-attention with Tree Expansion.pdf:application/pdf},
}

@article{ni_optimal_2022,
	title = {Optimal {Estimation} of {Off}-{Policy} {Policy} {Gradient} via {Double} {Fitted} {Iteration}},
	journal = {arXiv preprint arXiv:2202.00076},
	author = {Ni, Chengzhuo and Zhang, Ruiqi and Ji, Xiang and Zhang, Xuezhou and Wang, Mengdi},
	year = {2022},
}

@inproceedings{lan_model-free_2022,
	address = {Virtual},
	title = {Model-{Free} {Policy} {Learning} with {Reward} {Gradients}},
	booktitle = {{arXiv} preprint {arXiv}:2103.05147},
	author = {Lan, Qingfeng and Tosatto, Samuele and Farrahi, Homayoon and Mahmood, A Rupam},
	year = {2022},
}
