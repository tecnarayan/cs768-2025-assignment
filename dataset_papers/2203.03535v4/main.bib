n@string{aaai = "Association for the Advancement of Artificial Intelligence (AAAI)"}
@string{nips = "Neural Information Processing Systems (NeurIPS)"}
@string{iclr = "International Conference on Learning Representations (ICLR)"}
@string{icml = "International Conference on Machine Learning (ICML)"}
@string{aamas = "International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)"}

@article{papoudakis19nonstationarity,
  author    = {Georgios Papoudakis and
               Filippos Christianos and
               Arrasy Rahman and
               Stefano V. Albrecht},
  title     = {Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1906.04737},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.04737},
  archivePrefix = {arXiv},
  eprint    = {1906.04737},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-04737},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{tesauro2004extending,
  title={Extending Q-learning to general adaptive multi-agent systems},
  author={Tesauro, Gerald},
  booktitle={Advances in neural information processing systems},
  pages={871--878},
  year={2004}
}

@article{bu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Bu, Lucian and Babu, Robert and De Schutter, Bart and others},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}

@article{stone00ma_survey,
 author = {Stone, Peter and Veloso, Manuela},
 title = {Multiagent Systems: A Survey from a Machine Learning Perspective},
 journal = {Auton. Robots},
 issue_date = {June 2000},
 volume = {8},
 number = {3},
 month = jun,
 year = {2000},
 issn = {0929-5593},
 pages = {345--383},
 numpages = {39},
 url = {https://doi.org/10.1023/A:1008942012299},
 doi = {10.1023/A:1008942012299},
 acmid = {591815},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {communicating agents, heterogeneous agents, homogeneous agents, intelligent agents, machine learning, multiagent systems, pursuit domain, robotic soccer, robotics, survey},
} 
[download]

@Article{page2007difference,
  title={The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies},
  author={Scott E. Page},
  isbn={9780691128382},
  lccn={2006044678},
  year={2007},
  journal={Princeton University Press}
}

@article{tuyls2012multiagent,
  title={Multiagent learning: Basics, challenges, and prospects},
  author={Tuyls, Karl and Weiss, Gerhard},
  journal={Ai Magazine},
  volume={33},
  number={3},
  pages={41--41},
  year={2012}
}

@inproceedings{sukhbaatar2016learning,
  title={Learning multiagent communication with backpropagation},
  author={Sukhbaatar, Sainbayar and Fergus, Rob and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2244--2252},
  year={2016}
}

@inproceedings{foerster2016learning,
  title={Learning to communicate with deep multi-agent reinforcement learning},
  author={Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2137--2145},
  year={2016}
}

@INPROCEEDINGS{Tan93indep,
    author = {Ming Tan},
    title = {Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents},
    booktitle = {In Proceedings of the Tenth International Conference on Machine Learning},
    year = {1993},
    pages = {330--337},
    publisher = {Morgan Kaufmann}
}

@inproceedings{foerster2017stabilising,
  title={Stabilising experience replay for deep multi-agent reinforcement learning},
  author={Foerster, Jakob and Nardelli, Nantas and Farquhar, Gregory and Afouras, Triantafyllos and Torr, Philip HS and Kohli, Pushmeet and Whiteson, Shimon},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1146--1155},
  year={2017},
  organization={JMLR. org}
}

@article{yang2018mean,
  title={Mean field multi-agent reinforcement learning},
  author={Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:1802.05438},
  year={2018}
}

@article{shoham2003multi,
  title={Multi-agent reinforcement learning: a critical survey},
  author={Shoham, Yoav and Powers, Rob and Grenager, Trond},
  journal={Web manuscript},
  year={2003}
}


@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1311--1320},
  year={2017},
  organization={JMLR. org}
}

@article{balduzzi2018mechanics,
  title={The mechanics of n-player differentiable games},
  author={Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
  journal={arXiv preprint arXiv:1802.05642},
  year={2018}
}

@incollection{bowling05convergence,
title = {Convergence and No-Regret in Multiagent Learning},
author = {Bowling, Michael},
booktitle = nips,
pages = {209--216},
year = {2005},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2673-convergence-and-no-regret-in-multiagent-learning.pdf}
}


@article{hernandezLealK17survey,
  author    = {Pablo Hernandez{-}Leal and
               Michael Kaisers and
               Tim Baarslag and
               Enrique Munoz de Cote},
  title     = {A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity},
  journal   = {CoRR},
  volume    = {abs/1707.09183},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.09183},
  archivePrefix = {arXiv},
  eprint    = {1707.09183},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Hernandez-LealK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tuyls12survey,
author = {Tuyls, Karl and Weiss, Gerhard},
year = {2012},
month = {12},
pages = {41-52},
title = {Multiagent Learning: Basics, Challenges, and Prospects},
volume = {33},
journal = {Ai Magazine},
doi = {10.1609/aimag.v33i3.2426}
}

@article{Goldberg10equilibrium,
  author    = {Paul W. Goldberg and
               Christos H. Papadimitriou and
               Rahul Savani},
  title     = {The Complexity of the Homotopy Method, Equilibrium Selection, and
               Lemke-Howson Solutions},
  journal   = {CoRR},
  volume    = {abs/1006.5352},
  year      = {2010},
  url       = {http://arxiv.org/abs/1006.5352},
  archivePrefix = {arXiv},
  eprint    = {1006.5352},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1006-5352},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{omidshafiei2017deep,
  title={Deep decentralized multi-task multi-agent reinforcement learning under partial observability},
  author={Omidshafiei, Shayegan and Pazis, Jason and Amato, Christopher and How, Jonathan P and Vian, John},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2681--2690},
  year={2017},
  organization={JMLR. org}
}

@article{omidshafiei2018learning,
  title={Learning to teach in cooperative multiagent reinforcement learning},
  author={Omidshafiei, Shayegan and Kim, Dong-Ki and Liu, Miao and Tesauro, Gerald and Riemer, Matthew and Amato, Christopher and Campbell, Murray and How, Jonathan P},
  journal={arXiv preprint arXiv:1805.07830},
  year={2018}
}

@book{OliehoekAmato16book,
    author =        {Frans A. Oliehoek and Christopher Amato},
    title =         {A Concise Introduction to Decentralized POMDPs},
    year =          2016,
    month =         may,
    url =           {http://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf},
    publisher =     {Springer},
    series =        {SpringerBriefs in Intelligent Systems},
    OPTurl =           {http://www.springer.com/us/book/9783319289274},
    wwwnote =          {Authors' pre-print. Final version availabe at <a href="http://www.springer.com/us/book/9783319289274">Springer</a>.},
    keywords =   {nonrefereed, book},
    doi =           {10.1007/978-3-319-28929-8}
}

@article{constantinos09nash,
author = {Daskalakis, Constantinos and Goldberg, Paul and H. Papadimitriou, Christos},
year = {2009},
month = {02},
pages = {195-259},
title = {The Complexity of Computing a Nash Equilibrium},
volume = {39},
journal = {SIAM J. Comput.},
doi = {10.1137/070699652}
}

@BOOK{brown08gametheory, 
author={K. {Leyton-Brown} and Y. {Shoham}}, 
booktitle={Essentials of Game Theory: A Concise Multidisciplinary Introduction}, 
title={Essentials of Game Theory: A Concise Multidisciplinary Introduction}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
keywords={Game theory;multiagent systems;competition;coordination;Prisoner's Dilemma: zero-sum games;Nash equilibrium;extensive form;repeated games;stochastic games;Bayesian games;coalitional games}, 
doi={}, 
ISSN={}, 
publisher={Morgan \& Claypool}, 
isbn={9781598295948}, 
url={https://ieeexplore.ieee.org/document/6812710},}

@inbook{nowe12gameandmarl,
author = {Nowe, Ann and Vrancx, Peter and De Hauwere, Yann-Michaël},
year = {2012},
month = {01},
pages = {30},
title = {Game Theory and Multi-agent Reinforcement Learning},
isbn = {978-3-642-27645-3},
journal = {Adaptation, Learning, and Optimization},
doi = {10.1007/978-3-642-27645-3_14}
}

@article{dayan93sr, 
author={P. {Dayan}}, 
journal={Neural Computation}, 
title={Improving Generalization for Temporal Difference Learning: The Successor Representation}, 
year={1993}, 
volume={5}, 
number={4}, 
pages={613-624}, 
keywords={}, 
doi={10.1162/neco.1993.5.4.613}, 
ISSN={0899-7667}, 
month={July},}

@article{Jaderberg17pbt,
  author    = {Max Jaderberg and
               Valentin Dalibard and
               Simon Osindero and
               Wojciech M. Czarnecki and
               Jeff Donahue and
               Ali Razavi and
               Oriol Vinyals and
               Tim Green and
               Iain Dunning and
               Karen Simonyan and
               Chrisantha Fernando and
               Koray Kavukcuoglu},
  title     = {Population Based Training of Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1711.09846},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.09846},
  archivePrefix = {arXiv},
  eprint    = {1711.09846},
  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-09846},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2018emergent,
title={Emergent Coordination Through Competition},
author={Siqi Liu and Guy Lever and Nicholas Heess and Josh Merel and Saran Tunyasuvunakool and Thore Graepel},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BkG8sjR5Km},
}

@article{hernandezlean18survey,
  author={Pablo Hernandez{-}Leal and Bilal Kartal and Matthew E. Taylor},
  title={Is multiagent deep reinforcement learning the answer or the question? {A} brief survey},
  journal={CoRR},
  volume={abs/1810.05587},
  year={2018},
  url={http://arxiv.org/abs/1810.05587},
  archivePrefix={arXiv},
  eprint={1810.05587},
  timestamp={Tue, 30 Oct 2018 20:39:56 +0100},
  biburl={https://dblp.org/rec/bib/journals/corr/abs-1810-05587},
  bibsource={dblp computer science bibliography, https://dblp.org}
}

@misc{OpenAI_dota,
      author = {OpenAI},
      title = {OpenAI Five},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}}

@misc{alphastarblog,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojciech M. and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Wu, Yuhuai and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}

@article{Jaderberg18ftw,
  author    = {Max Jaderberg and
               Wojciech M. Czarnecki and
               Iain Dunning and
               Luke Marris and
               Guy Lever and
               Antonio Garc{\'{\i}}a Casta{\~{n}}eda and
               Charles Beattie and
               Neil C. Rabinowitz and
               Ari S. Morcos and
               Avraham Ruderman and
               Nicolas Sonnerat and
               Tim Green and
               Louise Deason and
               Joel Z. Leibo and
               David Silver and
               Demis Hassabis and
               Koray Kavukcuoglu and
               Thore Graepel},
  title     = {Human-level performance in first-person multiplayer games with population-based
               deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1807.01281},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.01281},
  archivePrefix = {arXiv},
  eprint    = {1807.01281},
  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-01281},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lowe17maddpg,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, OpenAI Pieter and Mordatch, Igor},
  booktitle=nips,
  pages={6382--6393},
  year={2017}
}

@article{rabinowitz18tom,
  author    = {Neil C. Rabinowitz and
               Frank Perbet and
               H. Francis Song and
               Chiyuan Zhang and
               S. M. Ali Eslami and
               Matthew Botvinick},
  title     = {Machine Theory of Mind},
  journal   = {CoRR},
  volume    = {abs/1802.07740},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.07740},
  archivePrefix = {arXiv},
  eprint    = {1802.07740},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-07740},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{grover18opponent,
author={Aditya Grover and Maruan Al{-}Shedivat and Jayesh K. Gupta and Yura Burda and Harrison Edwards},
title={Learning Policy Representations in Multiagent Systems},
journal={CoRR},
volume={abs/1806.06464},
year={2018},
url={http://arxiv.org/abs/1806.06464},
archivePrefix={arXiv},
eprint={1806.06464},
timestamp={Mon, 13 Aug 2018 16:45:59 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1806-06464},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{ponsen09evolutionarygame,
title = "An evolutionary game-theoretic analysis of poker strategies",
journal = "Entertainment Computing",
volume = "1",
number = "1",
pages = "39 - 45",
year = "2009",
issn = "1875-9521",
doi = "https://doi.org/10.1016/j.entcom.2009.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S1875952109000056",
author = "Marc Ponsen and Karl Tuyls and Michael Kaisers and Jan Ramon",
keywords = "No Limit Texas Hold’em poker, Replicator Dynamics, Poker strategies, Simplex analysis",
abstract = "In this paper we investigate the evolutionary dynamics of strategic behavior in the game of poker by means of data gathered from a large number of real world poker games. We perform this study from an evolutionary game theoretic perspective using two Replicator Dynamics models. First we consider the basic selection model on this data, secondly we use a model which includes both selection and mutation. We investigate the dynamic properties by studying how rational players switch between different strategies under different circumstances, what the basins of attraction of the equilibria look like, and what the stability properties of the attractors are. We illustrate the dynamics using a simplex analysis. Our experimental results confirm existing domain knowledge of the game, namely that certain strategies are clearly inferior while others can be successful given certain game conditions."
}

@inproceedings{amir2016interactive,
  title={Interactive teaching strategies for agent training},
  author={Amir, Ofra and Kamar, Ece and Kolobov, Andrey and Grosz, Barbara J},
  year={2016},
  organization={International Joint Conferences on Artificial Intelligence (IJCAI)}
}

@article{clouse1996integrating,
 author = {Clouse, J.},
 title = {On integrating apprentice learning and reinforcement learning},
 year = {1997},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
} 

@inproceedings{torrey2013teaching,
  title={Teaching on a budget: Agents advising agents in reinforcement learning},
  author={Torrey, Lisa and Taylor, Matthew},
  booktitle={Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems},
  pages={1053--1060},
  year={2013},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@article{he16dron,
author={He He and Jordan L. Boyd{-}Graber and Kevin Kwok and Hal Daum{\'{e}} III},
title={Opponent Modeling in Deep Reinforcement Learning},
journal={CoRR},
volume={abs/1609.05559},
year={2016},
url={http://arxiv.org/abs/1609.05559},
archivePrefix={arXiv},
eprint={1609.05559},
timestamp={Mon, 13 Aug 2018 16:48:46 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/HeBKD16},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@inproceedings{SilvaGC17,
  author    = {Felipe Leno da Silva and
              Ruben Glatt and
              Anna Helena Reali Costa},
  title     = {Simultaneously Learning and Advising in Multiagent Reinforcement Learning},
  booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  pages     = {1100--1108},
  year      = {2017},
}

@article{tuyls18empiricalgame,
  author    = {Karl Tuyls and
               Julien P{\'{e}}rolat and
               Marc Lanctot and
               Joel Z. Leibo and
               Thore Graepel},
  title     = {A Generalised Method for Empirical Game Theoretic Analysis},
  journal   = {CoRR},
  volume    = {abs/1803.06376},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.06376},
  archivePrefix = {arXiv},
  eprint    = {1803.06376},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-06376},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lanctot17unified,
  author    = {Marc Lanctot and
               Vin{\'{\i}}cius Flores Zambaldi and
               Audrunas Gruslys and
               Angeliki Lazaridou and
               Karl Tuyls and
               Julien P{\'{e}}rolat and
               David Silver and
               Thore Graepel},
  title     = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1711.00832},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00832},
  archivePrefix = {arXiv},
  eprint    = {1711.00832},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00832},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{omidshafiei18teach,
  author    = {Shayegan Omidshafiei and
              Dong{-}Ki Kim and
              Miao Liu and
              Gerald Tesauro and
              Matthew Riemer and
              Christopher Amato and
              Murray Campbell and
              Jonathan P. How},
  title     = {Learning to Teach in Cooperative Multiagent Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1805.07830},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.07830},
  archivePrefix = {arXiv},
  eprint    = {1805.07830},
  timestamp = {Mon, 13 Aug 2018 16:48:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-07830},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Taylor:2009:TLR:1577069.1755839,
 author = {Taylor, Matthew E. and Stone, Peter},
 title = {Transfer Learning for Reinforcement Learning Domains: A Survey},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = dec,
 year = {2009},
 issn = {1532-4435},
 pages = {1633--1685},
 numpages = {53},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1755839},
 acmid = {1755839},
 publisher = {JMLR.org},
} 

@Article{Lin1992,
author="Lin, Long-Ji",
title="Self-improving reactive agents based on reinforcement learning, planning and teaching",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="293--321",
abstract="To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.",
issn="1573-0565",
doi="10.1007/BF00992699",
url="https://doi.org/10.1007/BF00992699"
}

@Article{Avis2010,
author="Avis, David
and Rosenberg, Gabriel D.
and Savani, Rahul
and von Stengel, Bernhard",
title="Enumeration of Nash equilibria for two-player games",
journal="Economic Theory",
year="2010",
month="Jan",
day="01",
volume="42",
number="1",
pages="9--37",
issn="1432-0479",
doi="10.1007/s00199-009-0449-x",
url="https://doi.org/10.1007/s00199-009-0449-x"
}

@article{omidshafiei19alpha,
  author    = {Shayegan Omidshafiei and
               Christos H. Papadimitriou and
               Georgios Piliouras and
               Karl Tuyls and
               Mark Rowland and
               Jean{-}Baptiste Lespiau and
               Wojciech M. Czarnecki and
               Marc Lanctot and
               Julien P{\'{e}}rolat and
               R{\'{e}}mi Munos},
  title     = {{\(\alpha\)}-Rank: Multi-Agent Evaluation by Evolution},
  journal   = {CoRR},
  volume    = {abs/1903.01373},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.01373},
  archivePrefix = {arXiv},
  eprint    = {1903.01373},
  timestamp = {Tue, 28 May 2019 13:51:17 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-01373},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{littman94markov,
 author = {Littman, Michael L.},
 title = {Markov Games As a Framework for Multi-agent Reinforcement Learning},
 booktitle = icml,
 year = {1994},
 isbn = {1-55860-335-2},
 location = {New Brunswick, NJ, USA},
 pages = {157--163},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3091574.3091594},
 acmid = {3091594},
 publisher = {Morgan Kaufmann Publishers Inc.},
} 

@incollection{chang04nips,
title = {All learning is Local: Multi-agent Learning in Global Reward Games},
author = {Yu-han Chang and Tracey Ho and Leslie P. Kaelbling},
booktitle = {Advances in Neural Information Processing Systems 16},
editor = {S. Thrun and L. K. Saul and B. Sch\"{o}lkopf},
pages = {807--814},
year = {2004},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2476-all-learning-is-local-multi-agent-learning-in-global-reward-games.pdf}
}

@article{forester17coma,
title={Counterfactual Multi-Agent Policy Gradients}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11794}, abstractNote={ &lt;p&gt; Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. &lt;/p&gt; }, number={1}, journal=aaai, author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon}, year={2018}, month={Apr.} }

@Inbook{Busoniu2010,
author="Bu{\c{s}}oniu, Lucian and Babu{\v{s}}ka, Robert and De Schutter, Bart",
title="Multi-agent Reinforcement Learning: An Overview",
bookTitle="Innovations in Multi-Agent Systems and Applications - 1",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="183--221",
isbn="978-3-642-14435-6",
doi="10.1007/978-3-642-14435-6_7",
url="https://doi.org/10.1007/978-3-642-14435-6_7"
}

@article{Matignon2012IndependentRL,
  title={Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems},
  author={La{\"e}titia Matignon and Guillaume J. Laurent and Nadine Le Fort-Piat},
  journal={Knowledge Engineering Review},
  year={2012},
  volume={27},
  pages={1-31}
}

@inproceedings{alshedivat2018continuous,
title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},
author={Maruan Al-Shedivat and Trapit Bansal and Yura Burda and Ilya Sutskever and Igor Mordatch and Pieter Abbeel},
booktitle=iclr,
year={2018},
url={https://openreview.net/forum?id=Sk2u1g-0-},
}

@incollection{lopezpak17episodic,
title={Gradient Episodic Memory for Continual Learning},
author={Lopez-Paz, David and Ranzato, Marc Aurelio},
booktitle={Advances in Neural Information Processing Systems 30},
editor={I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages={6467--6476},
year={2017},
publisher={Curran Associates, Inc.},
url={http://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf}
}

@inproceedings{riemer2018learning,
title={Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference},
author={Matthew Riemer and Ignacio Cases and Robert Ajemian and Miao Liu and Irina Rish and Yuhai Tu and and Gerald Tesauro},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1gTShAct7},
}

@article{lomonaco19continual,
author={Vincenzo Lomonaco and Karan Desai and Eugenio Culurciello and Davide Maltoni},
title={Continual Reinforcement Learning in 3D Non-stationary Environments},
journal={CoRR},
volume={abs/1905.10112},
year={2019},
url={http://arxiv.org/abs/1905.10112},
archivePrefix={arXiv},
eprint={1905.10112},
timestamp={Wed, 29 May 2019 11:27:50 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1905-10112},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{ring1997,
author="Ring, Mark B.",
title="{CHILD}: A First Step Towards Continual Learning",
journal="Machine Learning",
year="1997",
month="Jul",
day="01",
volume="28",
number="1",
pages="77--104",
abstract="Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.",
issn="1573-0565",
doi="10.1023/A:1007331723572",
url="https://doi.org/10.1023/A:1007331723572"
}

@inproceedings{schlimmer86continual,
author={Schlimmer, Jeffrey C. and Fisher, Douglas},
title={A Case Study of Incremental Concept Induction},
booktitle={Proceedings of the Fifth AAAI National Conference on Artificial Intelligence},
series = {AAAI'86},
year = {1986},
location = {Philadelphia, Pennsylvania},
pages = {496--501},
numpages = {6},
url = {http://dl.acm.org/citation.cfm?id=2887770.2887853},
acmid = {2887853},
publisher = {AAAI Press},
} 

@article{rakelly19oyster,
author={Kate Rakelly and Aurick Zhou and Deirdre Quillen and Chelsea Finn and Sergey Levine},
title={Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
journal={CoRR},
volume={abs/1903.08254},
year={2019},
url={http://arxiv.org/abs/1903.08254},
archivePrefix = {arXiv},
eprint={1903.08254},
timestamp={Mon, 01 Apr 2019 14:07:37 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1903-08254},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{carpenter87stabilityplasticity,
title="A massively parallel architecture for a self-organizing neural pattern recognition machine",
journal="Computer Vision, Graphics, and Image Processing",
volume="37",
number="1",
pages="54 - 115",
year="1987",
issn="0734-189X",
doi="https://doi.org/10.1016/S0734-189X(87)80014-2",
url="http://www.sciencedirect.com/science/article/pii/S0734189X87800142",
author="Gail A. Carpenter and Stephen Grossberg"
}

@article{li16catastrophic,
author={Zhizhong Li and Derek Hoiem},
title={Learning without Forgetting},
journal={CoRR},
volume={abs/1606.09282},
year={2016},
url={http://arxiv.org/abs/1606.09282},
archivePrefix={arXiv},
eprint={1606.09282},
timestamp={Mon, 13 Aug 2018 16:47:54 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/LiH16e},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cases2019rrn,
title="Recursive Routing Networks: Learning to Compose Modules for Language Understanding",
author="Cases, Ignacio  and Rosenbaum, Clemens and Riemer, Matthew  and Geiger, Atticus  and Klinger, Tim  and Tamkin, Alex  and Li, Olivia  and Agarwal, Sandhini  and Greene, Joshua D.  and Jurafsky, Dan  and Potts, Christopher  and Karttunen, Lauri",
booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
month = jun,
year = "2019",
address = "Minneapolis, Minnesota",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/N19-1365",
pages = "3631--3648",
}

@inproceedings{amato13bayes,
author={Amato, Christopher and Oliehoek, Frans},
year={2013},
month={01},
pages={76-83},
title={Bayesian Reinforcement Learning for Multiagent Systems with State Uncertainty}
}

@inproceedings{nguyen18variational,
title={Variational Continual Learning},
author={Cuong V. Nguyen and Yingzhen Li and Thang D. Bui and Richard E. Turner},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkQqq0gRb},
}

@inproceedings{kingma14vae,
author={Diederik P. Kingma and Max Welling},
title={Auto-Encoding Variational Bayes},
booktitle={International Conference on Learning Representations},
year={2014},
url={http://arxiv.org/abs/1312.6114},
timestamp={Thu, 04 Apr 2019 13:20:07 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/KingmaW13},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{shazeer17moe,
author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc V. Le and Geoffrey E. Hinton and Jeff Dean},
title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
journal={CoRR},
volume={abs/1701.06538},
year={2017},
url={http://arxiv.org/abs/1701.06538},
archivePrefix={arXiv},
eprint={1701.06538},
timestamp={Mon, 13 Aug 2018 16:46:11 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/ShazeerMMDLHD17},
bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{jacobs91moe,
title="Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks",
journal="Cognitive Science",
volume="15",
number="2",
pages="219 - 250",
year="1991",
issn="0364-0213",
doi="https://doi.org/10.1016/0364-0213(91)80006-Q",
url="http://www.sciencedirect.com/science/article/pii/036402139180006Q",
author="Robert A. Jacobs and Michael I. Jordan and Andrew G. Barto",
}

@inproceedings{foerster17lola,
author = {Foerster, Jakob and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
title = {Learning with Opponent-Learning Awareness},
year = {2018},
address = {Richland, SC},
booktitle = aamas,
pages = {122–130},
numpages = {9},
keywords = {multi-agent learning, iterated prisoners dilemma, deep reinforcement learning, general-sum},
location = {Stockholm, Sweden},
}

@book{sutton98rlbook,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@inproceedings{wen2018probabilistic,
title={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},
author={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},
booktitle={International Conference on Learning Representations (ICLR)},
year={2019},
url={https://openreview.net/forum?id=rkl6As0cF7},
}

@inproceedings{zhang10lookahead,
  title={Multi-Agent Learning with Policy Prediction},
  author={Chongjie Zhang and Victor R. Lesser},
  booktitle=aaai,
  year={2010}
}

@article{wei18multiagentsoftQ,
  author    = {Ermo Wei and
               Drew Wicke and
               David Freelan and
               Sean Luke},
  title     = {Multiagent Soft Q-Learning},
  journal   = {CoRR},
  volume    = {abs/1804.09817},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.09817},
  archivePrefix = {arXiv},
  eprint    = {1804.09817},
  timestamp = {Mon, 13 Aug 2018 16:49:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-09817},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{foerster2018dice,
  title={{D}i{CE}: The Infinitely Differentiable {M}onte {C}arlo Estimator},
  author={Foerster, Jakob and Farquhar, Gregory and Al-Shedivat, Maruan and Rockt{\"a}schel, Tim and Xing, Eric and Whiteson, Shimon},
  booktitle = icml,
  pages={1524--1533},
  year={2018},
  volume={80},
  publisher={PMLR},
  pdf={http://proceedings.mlr.press/v80/foerster18a/foerster18a.pdf},
  url={http://proceedings.mlr.press/v80/foerster18a.html},
}

@inproceedings{letcher2018stable,
title={Stable Opponent Shaping in Differentiable Games},
author={Alistair Letcher and Jakob Foerster and David Balduzzi and Tim Rocktäschel and Shimon Whiteson},
booktitle=iclr,
year={2019},
url={https://openreview.net/forum?id=SyGjjsC5tQ},
}

@article{lecarpentier19,
  author    = {Erwan Lecarpentier and
               Emmanuel Rachelson},
  title     = {Non-Stationary Markov Decision Processes a Worst-Case Approach using
               Model-Based Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1904.10090},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10090},
  archivePrefix = {arXiv},
  eprint    = {1904.10090},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-10090.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kim20metamapg,
  author    = {Dong{-}Ki Kim and
               Miao Liu and
               Matthew Riemer and
               Chuangchuang Sun and
               Marwa Abdulhai and
               Golnaz Habibi and
               Sebastian Lopez{-}Cot and
               Gerald Tesauro and
               Jonathan P. How},
  title     = {A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/2011.00382},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.00382},
  archivePrefix = {arXiv},
  eprint    = {2011.00382},
  timestamp = {Fri, 06 Nov 2020 15:32:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-00382.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xie20lili,
 title={Learning Latent Representations to Influence Multi-Agent Interaction},
 author={Xie, Annie and Losey, Dylan and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
 booktitle={Conference on Robot Learning (CoRL)},
 year={2020}
}

@inproceedings{chalkiadakis03bayesmarl,
author = {Chalkiadakis, Georgios and Boutilier, Craig},
title = {Coordination in Multiagent Reinforcement Learning: A Bayesian Approach},
year = {2003},
isbn = {1581136838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/860575.860689},
doi = {10.1145/860575.860689},
abstract = {Much emphasis in multiagent reinforcement learning (MARL) research is placed on ensuring that MARL algorithms (eventually) converge to desirable equilibria. As in standard reinforcement learning, convergence generally requires sufficient exploration of strategy space. However, exploration often comes at a price in the form of penalties or foregone opportunities. In multiagent settings, the problem is exacerbated by the need for agents to "coordinate" their policies on equilibria. We propose a Bayesian model for optimal exploration in MARL problems that allows these exploration costs to be weighed against their expected benefits using the notion of value of information. Unlike standard RL models, this model requires reasoning about how one's actions will influence the behavior of other agents. We develop tractable approximations to optimal Bayesian exploration, and report on experiments illustrating the benefits of this approach in identical interest games.},
booktitle = {Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {709–716},
numpages = {8},
keywords = {multiagent learning, Bayesian methods, reinforcement learning},
location = {Melbourne, Australia},
series = {AAMAS '03}
}

@InProceedings{haarnoja18sac,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = icml,
  pages = 	 {1861--1870},
  year = 	 {2018},
  volume = 	 {80},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/haarnoja18b.html},
}

@article{fudenberg91equilibrium,
title = {Perfect Bayesian equilibrium and sequential equilibrium},
journal = {Journal of Economic Theory},
volume = {53},
number = {2},
pages = {236-260},
year = {1991},
issn = {0022-0531},
doi = {https://doi.org/10.1016/0022-0531(91)90155-W},
url = {https://www.sciencedirect.com/science/article/pii/002205319190155W},
author = {Drew Fudenberg and Jean Tirole},
abstract = {We introduce a formal definition of perfect Bayesian equilibrium (PBE) for multi-period games with observed actions. In a PBE, (P) the strategies form a Bayesian equilibrium for each continuation game, given the specified beliefs, and (B) beliefs are updated from period to period in accordance with Bayes rule whenever possible, and satisfy a “no-signaling-what-you-don't-know” condition. PBE is equivalent to sequential equilibrium if each player has only two types, or there are only two periods, but differs otherwise. Equivalence is restored by requiring that (B) apply to the relative probabilities of types with posterior probability zero.}
}

@article{shapley53stochastic,
	author = {Shapley, L. S.},
	title = {Stochastic Games},
	volume = {39},
	number = {10},
	pages = {1095--1100},
	year = {1953},
	doi = {10.1073/pnas.39.10.1095},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/39/10/1095},
	eprint = {https://www.pnas.org/content/39/10/1095.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@Inbook{Nowe2012,
author="Now{\'e}, Ann
and Vrancx, Peter
and De Hauwere, Yann-Micha{\"e}l",
title="Game Theory and Multi-agent Reinforcement Learning",
bookTitle="Reinforcement Learning: State-of-the-Art",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="441--470",
isbn="978-3-642-27645-3",
doi="10.1007/978-3-642-27645-3_14",
url="https://doi.org/10.1007/978-3-642-27645-3_14"
}

@article{Nash48,
	author = {Nash, John F.},
	title = {Equilibrium points in n-person games},
	volume = {36},
	number = {1},
	pages = {48--49},
	year = {1950},
	doi = {10.1073/pnas.36.1.48},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/36/1/48},
	eprint = {https://www.pnas.org/content/36/1/48.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{correlated87,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1911154},
 abstract = {Correlated equilibrium is formulated in a manner that does away with the dichotomy usually perceived between the "Bayesian" and the "game-theoretic" view of the world. From the Bayesian viewpoint, probabilities should be assignable to everything, including the prospect of a player choosing a certain strategy in a certain game. The so-called "game-theoretic" viewpoint holds that probabilities can only be assigned to events not governed by rational decision makers; for the latter, one must substitute an equilibrium (or other game-theoretic) notion. The current formulation synthesizes the two viewpoints: Correlated equilibrium is viewed as the result of Bayesian rationality; the equilibrium condition appears as a simple maximization of utility on the part of each player, given his information. A feature of this approach is that it does not require explicit randomization on the part of the players. Each player always chooses a definite pure strategy,with no attempt to randomize; the probabilistic nature of the strategies reflects the uncertainty of other players about his choice. Examples are given.},
 author = {Robert J. Aumann},
 journal = {Econometrica},
 number = {1},
 pages = {1--18},
 publisher = {[Wiley, Econometric Society]},
 title = {Correlated Equilibrium as an Expression of Bayesian Rationality},
 volume = {55},
 year = {1987}
}

@book{brown08gametheorybook,
author = {Leyton-Brown, Kevin and Shoham, Yoav},
title = {Essentials of Game Theory: A Concise, Multidisciplinary Introduction},
year = {2008},
isbn = {1598295934},
publisher = {Morgan and Claypool Publishers},
edition = {1st}
}

@InProceedings{kim21metamapg,
  title = 	 {A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning},
  author =       {Kim, Dong Ki and Liu, Miao and Riemer, Matthew D and Sun, Chuangchuang and Abdulhai, Marwa and Habibi, Golnaz and Lopez-Cot, Sebastian and Tesauro, Gerald and How, Jonathan},
  booktitle = icml,
  pages = 	 {5541--5550},
  year = 	 {2021},
  volume = 	 {139},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21g/kim21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/kim21g.html},
}

@inproceedings{hasselt16doubledqn,
author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
title = {Deep Reinforcement Learning with Double Q-Learning},
year = {2016},
publisher = {AAAI Press},
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain
conditions. It was not previously known whether, in practice, such overestimations
are common, whether they harm performance, and whether they can generally be prevented.
In this paper, we answer all these questions affirmatively. In particular, we first
show that the recent DQN algorithm, which combines Q-learning with a deep neural network,
suffers from substantial overestimations in some games in the Atari 2600 domain. We
then show that the idea behind the Double Q-learning algorithm, which was introduced
in a tabular setting, can be generalized to work with large-scale function approximation.
We propose a specific adaptation to the DQN algorithm and show that the resulting
algorithm not only reduces the observed overestimations, as hypothesized, but that
this also leads to much better performance on several games.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {2094–2100},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{mnih13dqn,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hessel18rainbow, 
title={Rainbow: Combining Improvements in Deep Reinforcement Learning}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11796}, abstractNote={ &lt;p&gt; The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David}, year={2018}, month={Apr.} }

@inproceedings{tesauro04hyperq,
 author = {Tesauro, Gerald},
 booktitle = nips,
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Extending {Q}-Learning to General Adaptive Multi-Agent Systems},
 url = {https://proceedings.neurips.cc/paper/2003/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf},
 volume = {16},
 year = {2004}
}

@article{hu03nashq,
author = {Hu, Junling and Wellman, Michael P.},
title = {Nash Q-Learning for General-Sum Stochastic Games},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
abstract = {We extend Q-learning to a noncooperative multiagent context, using the framework of
general-sum stochastic games. A learning agent maintains Q-functions over joint actions,
and performs updates based on assuming Nash equilibrium behavior over the current
Q-values. This learning protocol provably converges given certain restrictions on
the stage games (defined by Q-values) that arise during learning. Experiments with
a pair of two-player grid games suggest that such restrictions on the game structure
are not necessarily required. Stage games encountered during learning in both grid
environments violate the conditions. However, learning consistently converges in the
first grid game, which has a unique equilibrium Q-function, but sometimes fails to
converge in the second, which has three different equilibrium Q-functions. In a comparison
of offline learning performance in both games, we find agents are more likely to reach
a joint optimal path with Nash Q-learning than with a single-agent Q-learning method.
When at least one agent adopts Nash Q-learning, the performance of both agents is
better than using single-agent Q-learning. We have also implemented an online version
of Nash Q-learning that balances exploration with exploitation, yielding improved
performance.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1039–1069},
numpages = {31}
}

@inproceedings{wang2021influencing,
title={Influencing Towards Stable Multi-Agent Interactions},
author={Woodrow Zhouyuan Wang and Andy Shih and Annie Xie and Dorsa Sadigh},
booktitle={Conference on Robot Learning (CoRL)},
year={2021},
url={https://openreview.net/forum?id=n6xYib0irVR}
}

@InProceedings{he16opponent-modeling,
  title = 	 {Opponent Modeling in Deep Reinforcement Learning},
  author = 	 {He He and Jordan Boyd-Graber and Kevin Kwok and Hal Daumé III},
  booktitle =icml, 	
  pages = 	 {1804--1813},
  year = 	 {2016},
  volume = 	 {48},
  month = 	 {20--22 Jun},
  pdf = 	 {http://proceedings.mlr.press/v48/he16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/he16.html},
}

@InProceedings{raileanu18opponent-modeling,
  title = 	 {Modeling Others using Oneself in Multi-Agent Reinforcement Learning},
  author = 	 {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  booktitle = icml, 
  pages = 	 {4257--4266},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/raileanu18a.html},
}

@InProceedings{grover18policy-representation,
  title = 	 {Learning Policy Representations in Multiagent Systems},
  author = 	 {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh and Burda, Yuri and Edwards, Harrison},
  booktitle = icml,
  pages = 	 {1802--1811},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/grover18a/grover18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/grover18a.html},
}

@InProceedings{yang18mean-field-marl,
  title = 	 {Mean Field Multi-Agent Reinforcement Learning},
  author = 	 {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle =icml, 
  pages = 	 {5571--5580},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/yang18d/yang18d.pdf},
  url = 	 {http://proceedings.mlr.press/v80/yang18d.html},
  abstract = 	 {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent’s optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.}
}

@inproceedings{omidshafiei19teach,
author = {Omidshafiei, Shayegan and Kim, Dong-Ki and Liu, Miao and Tesauro, Gerald and Riemer, Matthew and Amato, Christopher and Campbell, Murray and How, Jonathan P.},
title = {Learning to Teach in Cooperative Multiagent Reinforcement Learning},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33016128},
doi = {10.1609/aaai.v33i01.33016128},
booktitle = aaai,
articleno = {752},
numpages = {9},
}

@InProceedings{xie21continual,
  title = 	 {Deep Reinforcement Learning amidst Continual Structured Non-Stationarity},
  author =       {Xie, Annie and Harrison, James and Finn, Chelsea},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11393--11403},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xie21c/xie21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xie21c.html},
  abstract = 	 {As humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift.}
}

@book{puterman94mdp,
author = {Puterman, Martin L.},
title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
year = {1994},
isbn = {0471619779},
publisher = {John Wiley \& Sons, Inc.},
address = {USA},
edition = {1st},
}

@misc{naik2019discounted,
      title={Discounted Reinforcement Learning Is Not an Optimization Problem}, 
      author={Abhishek Naik and Roshan Shariff and Niko Yasui and Hengshuai Yao and Richard S. Sutton},
      year={2019},
      eprint={1910.02140},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@InProceedings{wan21average,
  title = 	 {Learning and Planning in Average-Reward Markov Decision Processes},
  author =       {Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  booktitle = icml,
  pages = 	 {10653--10662},
  year = 	 {2021},
  volume = 	 {139},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wan21a/wan21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/wan21a.html},
  abstract = 	 {We introduce learning and planning algorithms for average-reward MDPs, including 1) the first general proven-convergent off-policy model-free control algorithm without reference states, 2) the first proven-convergent off-policy model-free prediction algorithm, and 3) the first off-policy learning algorithm that converges to the actual value function rather than to the value function plus an offset. All of our algorithms are based on using the temporal-difference error rather than the conventional error when updating the estimate of the average reward. Our proof techniques are a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments with an Access-Control Queuing Task, we show some of the difficulties that can arise when using methods that rely on reference states and argue that our new algorithms are significantly easier to use.}
}

@inproceedings{zinkevich06cyclic,
 author = {Zinkevich, Martin and Greenwald, Amy and Littman, Michael},
 booktitle = nips,
 pages = {},
 publisher = {MIT Press},
 title = {Cyclic Equilibria in Markov Games},
 url = {https://proceedings.neurips.cc/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf},
 volume = {18},
 year = {2006}
}

@article{mahadevan96average,
author = {Mahadevan, Sridhar},
title = {Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results},
year = {1996},
issue_date = {Jan./Feb./March 1996},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00114727},
doi = {10.1007/BF00114727},
journal = {Mach. Learn.},
month = {jan},
pages = {159–195},
numpages = {37},
keywords = {Markov decision processes, reinforcement learning}
}

@article{wei2020modelfree,
  author    = {Chen{-}Yu Wei and
               Mehdi Jafarnia{-}Jahromi and
               Haipeng Luo and
               Hiteshi Sharma and
               Rahul Jain},
  title     = {Model-free Reinforcement Learning in Infinite-horizon Average-reward
               Markov Decision Processes},
  journal   = {CoRR},
  volume    = {abs/1910.07072},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.07072},
  eprinttype = {arXiv},
  eprint    = {1910.07072},
  timestamp = {Tue, 22 Oct 2019 18:17:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-07072.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{christodoulou2019soft,
  author    = {Petros Christodoulou},
  title     = {Soft Actor-Critic for Discrete Action Settings},
  journal   = {CoRR},
  volume    = {abs/1910.07207},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.07207},
  eprinttype = {arXiv},
  eprint    = {1910.07207},
  timestamp = {Tue, 22 Oct 2019 18:17:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-07207.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{blei17variational,
   title={Variational Inference: A Review for Statisticians},
   volume={112},
   ISSN={1537-274X},
   url={http://dx.doi.org/10.1080/01621459.2017.1285773},
   DOI={10.1080/01621459.2017.1285773},
   number={518},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
   year={2017},
   month={Apr},
   pages={859–877}
  }
  
@inproceedings{kim20hmat,
author = {Kim, Dong-Ki and Liu, Miao and Omidshafiei, Shayegan and Lopez-Cot, Sebastian and Riemer, Matthew and Habibi, Golnaz and Tesauro, Gerald and Mourad, Sami and Campbell, Murray and How, Jonathan P.},
title = {Learning Hierarchical Teaching Policies for Cooperative Agents},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = aamas,
pages = {620–628},
numpages = {9},
keywords = {learning agent-to-agent interactions, deep reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{wang02nash,
author = {Wang, Xiaofeng and Sandholm, Tuomas},
title = {Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games},
year = {2002},
publisher = {MIT Press},
abstract = {Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conflicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the first algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm's parameters are easy to set to meet the convergence conditions.},
booktitle = nips,
pages = {1603–1610},
numpages = {8},
}

@inproceedings{littman01friendfoe,
author = {Littman, Michael L.},
title = {Friend-or-Foe Q-Learning in General-Sum Games},
year = {2001},
isbn = {1558607781},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = icml,
pages = {322–328},
numpages = {7},
}

@inproceedings{greenwald03correlated,
author = {Greenwald, Amy and Hall, Keith},
title = {Correlated-{Q} Learning},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {This paper introduces Correlated-Q (CE-Q) learning, a multiagent Q-learning algorithm based on the correlated equilibrium (CE) solution concept. CE-Q generalizes both Nash-Q and Friend-and-Foe-Q: in general-sum games, the set of correlated equilibria contains the set of Nash equilibria; in constant-sum games, the set of correlated equilibria contains the set of minimax equilibria. This paper describes experiments with four variants of CE-Q, demonstrating empirical convergence to equilibrium policies on a testbed of general-sum Markov games.},
booktitle = icml,
pages = {242–249},
numpages = {8},
location = {Washington, DC, USA},
}

@InProceedings{iqbal19masac,
  title =    {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  author =   {Iqbal, Shariq and Sha, Fei},
  booktitle =  icml,
  pages =    {2961--2970},
  year =     {2019},
  volume =   {97},
  month =    {09--15 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  url =      {http://proceedings.mlr.press/v97/iqbal19a.html},
}

@INPROCEEDINGS{Watkins92q-learning,
    author = {Christopher J. C. H. Watkins and Peter Dayan},
    title = {Q-learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {279--292}
}

@article{foster90stochastic,
title = {Stochastic evolutionary game dynamics},
journal = {Theoretical Population Biology},
volume = {38},
number = {2},
pages = {219-232},
year = {1990},
issn = {0040-5809},
doi = {https://doi.org/10.1016/0040-5809(90)90011-J},
url = {https://www.sciencedirect.com/science/article/pii/004058099090011J},
author = {Dean Foster and Peyton Young},
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2},
  year={2002},
  publisher={Springer}
}

@inproceedings{Zintgraf2020VariBAD,
title={VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning},
author={Luisa Zintgraf and Kyriacos Shiarlis and Maximilian Igl and Sebastian Schulze and Yarin Gal and Katja Hofmann and Shimon Whiteson},
booktitle=iclr,
year={2020},
url={https://openreview.net/forum?id=Hkl9JlBYvr}
}

@inproceedings{chris20pg,
author = {Nota, Chris and Thomas, Philip S.},
title = {Is the Policy Gradient a Gradient?},
year = {2020},
isbn = {9781450375184},
address = {Richland, SC},
abstract = {The policy gradient theorem describes the gradient of the expected discounted return with respect to an agent's policy parameters. However, most policy gradient methods drop the discount factor from the state distribution and therefore do not optimize the discounted objective. What do they optimize instead? This has been an open question for several years, and this lack of theoretical clarity has lead to an abundance of misstatements in the literature. We answer this question by proving that the update direction approximated by most methods is not the gradient of any function. Further, we argue that algorithms that follow this direction are not guaranteed to converge to a "reasonable'' fixed point by constructing a counterexample wherein the fixed point is globally pessimal with respect to both the discounted and undiscounted objectives. We motivate this work by surveying the literature and showing that there remains a widespread misunderstanding regarding discounted policy gradient methods, with errors present even in highly-cited papers published at top conferences.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {939–947},
numpages = {9},
keywords = {reinforcement learning, policy gradients},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@ARTICLE{chasparis19perturb,
  author={Chasparis, Georgios C.},
  journal={IEEE Transactions on Automatic Control}, 
  title={Stochastic Stability of Perturbed Learning Automata in Positive-Utility Games}, 
  year={2019},
  volume={64},
  number={11},
  pages={4454-4469},
  doi={10.1109/TAC.2019.2895300}}
  
@book{freidlin2012random,
  added-at = {2014-11-14T03:12:52.000+0100},
  author = {Freidlin, M.I. and Sz{\"u}cs, J. and Wentzell, A.D.},
  biburl = {https://www.bibsonomy.org/bibtex/26df179288a704c367421a6f9801fe949/peter.ralph},
  interhash = {0926f80cc9f0d77d5bce51922d3ff9ea},
  intrahash = {6df179288a704c367421a6f9801fe949},
  isbn = {9783642258473},
  keywords = {Freidlin-Wentzell_theory dynamical_systems large_deviations stochastic_perturbation},
  publisher = {Springer},
  series = {Grundlehren der mathematischen Wissenschaften},
  timestamp = {2014-11-14T03:15:07.000+0100},
  title = {Random Perturbations of Dynamical Systems},
  url = {http://books.google.de/books?id=p8LFMILAiMEC},
  year = 2012
}

@article{chasparis12stochastic,
title = "Distributed Dynamic Reinforcement of Efficient Outcomes in Multiagent Coordination and Network Formation",
keywords = "Evolutionary games, Reinforcement learning, Endogenous network formation, Dynamic reinforcement, Coordination games",
author = "Georgios Chasparis and Shamma, {Jeff S.}",
year = "2012",
doi = "10.1007/s13235-011-0038-z",
language = "English",
volume = "2",
pages = "18--50",
journal = "Dynamic Games and Applications",
issn = "2153-0793",
publisher = "Springer",
number = "1",
}

@InProceedings{jaques19social,
  title = 	 {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},
  author =       {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, Dj and Leibo, Joel Z. and De Freitas, Nando},
  booktitle = icml,
  pages = 	 {3040--3049},
  year = 	 {2019},
  volume = 	 {97},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jaques19a/jaques19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jaques19a.html},
  abstract = 	 {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents’ behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.}
}

@article{balaguer2022good,
  title={The Good Shepherd: An Oracle Agent for Mechanism Design},
  author={Balaguer, Jan and Koster, Raphael and Summerfield, Christopher and Tacchetti, Andrea},
  journal={arXiv preprint arXiv:2202.10135},
  year={2022}
}

@book{bass_2011, 
place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={Stochastic Processes}, DOI={10.1017/CBO9780511997044}, publisher={Cambridge University Press}, author={Bass, Richard F.}, year={2011}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}


@InProceedings{lu2022modelfree,
  title = 	 {Model-Free Opponent Shaping},
  author =       {Lu, Christopher and Willi, Timon and De Witt, Christian A Schroeder and Foerster, Jakob},
  booktitle = icml,
  pages = 	 {14398--14411},
  year = 	 {2022},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lu22d/lu22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lu22d.html},
  abstract = 	 {In general-sum games the interaction of self-interested learning agents commonly leads to collectively worst-case outcomes, such as defect-defect in the iterated prisoner’s dilemma (IPD). To overcome this, some methods, such as Learning with Opponent-Learning Awareness (LOLA), directly shape the learning process of their opponents. However, these methods are myopic since only a small number of steps can be anticipated, are asymmetric since they treat other agents as naive learners, and require the use of higher-order derivatives, which are calculated through white-box access to an opponent’s differentiable learning algorithm. To address these issues, we propose Model-Free Opponent Shaping (M-FOS). M-FOS learns in a meta-game in which each meta-step is an episode of the underlying game. The meta-state consists of the policies in the underlying game and the meta-policy produces a new policy to be used in the next episode. M-FOS then uses generic model-free optimisation methods to learn meta-policies that accomplish long-horizon opponent shaping. Empirically, M-FOS near-optimally exploits naive learners and other, more sophisticated algorithms from the literature. For example, to the best of our knowledge, it is the first method to learn the well-known ZD extortion strategy in the IPD. In the same settings, M-FOS leads to socially optimal outcomes under meta-self-play. Finally, we show that M-FOS can be scaled to high-dimensional settings.}
}

@inproceedings{zheng2018magent,
  title={MAgent: A many-agent reinforcement learning platform for artificial collective intelligence},
  author={Zheng, Lianmin and Yang, Jiacheng and Cai, Han and Zhou, Ming and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle=aaai,
  year={2018}
}

@article{deleu2022continuous,
  title={Continuous-Time Meta-Learning with Forward Mode Differentiation},
  author={Deleu, Tristan and Kanaa, David and Feng, Leo and Kerg, Giancarlo and Bengio, Yoshua and Lajoie, Guillaume and Bacon, Pierre-Luc},
  journal={arXiv preprint arXiv:2203.01443},
  year={2022}
}

@article{imaml,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  journal=nips,
  volume={32},
  year={2019}
}

@inproceedings{wicks05ssd,
author = {Wicks, John R. and Greenwald, Amy},
title = {An Algorithm for Computing Stochastically Stable Distributions with Applications to Multiagent Learning in Repeated Games},
year = {2005},
isbn = {0974903914},
abstract = {One of the proposed solutions to the equilibrium selection problem for agents learning in repeated games is obtained via the notion of stochastic stability. Learning algorithms are perturbed so that the Markov chain underlying the learning dynamics is necessarily irreducible and yields a unique stable distribution. The stochastically stable distribution is the limit of these stable distributions as the perturbation rate tends to zero. We present the first exact algorithm for computing the stochastically stable distribution of a Markov chain. We use our algorithm to predict the long-term dynamics of simple learning algorithms in sample repeated games.},
booktitle = {Conference on Uncertainty in Artificial Intelligence (UAI)},
pages = {623–632},
numpages = {10},
location = {Edinburgh, Scotland},
series = {UAI'05}
}

@inproceedings{leibo17socialdilemmas,
author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
title = {Multi-Agent Reinforcement Learning in Sequential Social Dilemmas},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
booktitle = aamas,
pages = {464–473},
numpages = {10},
keywords = {agent-based social simulation, cooperation, markov games, non-cooperative games, social dilemmas},
location = {S\~{a}o Paulo, Brazil},
}

@inproceedings{hughes18,
 author = {Hughes, Edward and Leibo, Joel Z and Phillips, Matthew and Tuyls, Karl and Due\~{n}ez-Guzman, Edgar and Garc\'{\i}a Casta\~{n}eda, Antonio and Dunning, Iain and Zhu, Tina and McKee, Kevin and Koster, Raphael and Roff, Heather and Graepel, Thore},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inequity aversion improves cooperation in intertemporal social dilemmas},
 url = {https://proceedings.neurips.cc/paper/2018/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{yang20,
 author = {Yang, Jiachen and Li, Ang and Farajtabar, Mehrdad and Sunehag, Peter and Hughes, Edward and Zha, Hongyuan},
 booktitle = nips,
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15208--15219},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Incentivize Other Learning Agents},
 url = {https://proceedings.neurips.cc/paper/2020/file/ad7ed5d47b9baceb12045a929e7e2f66-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{yang22,
author = {Yang, Jiachen and Wang, Ethan and Trivedi, Rakshit and Zhao, Tuo and Zha, Hongyuan},
title = {Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
address = {Richland, SC},
abstract = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
booktitle = aamas,
pages = {1436–1445},
numpages = {10},
keywords = {incentive design, multi-agent reinforcement learning},
location = {Virtual Event, New Zealand},
}

@inproceedings{wang19,
author = {Wang, Jane X. and Hughes, Edward and Fernando, Chrisantha and Czarnecki, Wojciech M. and Du\'{e}\~{n}ez-Guzm\'{a}n, Edgar A. and Leibo, Joel Z.},
title = {Evolving Intrinsic Motivations for Altruistic Behavior},
year = {2019},
isbn = {9781450363099},
address = {Richland, SC},
abstract = {Multi-agent cooperation is an important feature of the natural world. Many tasks involve individual incentives that are misaligned with the common good, yet a wide range of organisms from bacteria to insects and humans are able to overcome their differences and collaborate. Therefore, the emergence of cooperative behavior amongst self-interested individuals is an important question for the fields of multi-agent reinforcement learning (MARL) and evolutionary theory. Here, we study a particular class of multi-agent problems called intertemporal social dilemmas (ISDs), where the conflict between the individual and the group is particularly sharp. By combining MARL with appropriately structured natural selection, we demonstrate that individual inductive biases for cooperation can be learned in a model-free way. To achieve this, we introduce an innovative modular architecture for deep reinforcement learning agents which supports multi-level selection. We present results in two challenging environments, and interpret these in the context of cultural and ecological evolution.},
booktitle = aamas,
pages = {683–692},
numpages = {10},
keywords = {multi-agent, social dilemmas, altruism, evolution},
location = {Montreal QC, Canada},
}

@inproceedings{zheng18bayesian,
author = {Zheng, Yan and Meng, Zhaopeng and Hao, Jianye and Zhang, Zongzhang and Yang, Tianpei and Fan, Changjie},
title = {A Deep Bayesian Policy Reuse Approach against Non-Stationary Agents},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In multiagent domains, coping with non-stationary agents that change behaviors from time to time is a challenging problem, where an agent is usually required to be able to quickly detect the other agent's policy during online interaction, and then adapt its own policy accordingly. This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. We propose a new deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the rectified belief model taking advantage of the opponent model to infer the other agent's policy from reward signals and its behaviors. Instead of directly storing individual policies as BPR+, we introduce distilled policy network that serves as the policy library in BPR+, using policy distillation to achieve efficient online policy learning and reuse. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {962–972},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{wang19ssd,
author = {Wang, Weixun and Hao, Jianye and Wang, Yixi and Taylor, Matthew},
title = {Achieving Cooperation through Deep Multiagent Reinforcement Learning in Sequential Prisoner's Dilemmas},
year = {2019},
isbn = {9781450376563},
url = {https://doi.org/10.1145/3356464.3357712},
doi = {10.1145/3356464.3357712},
abstract = {The Iterated Prisoner's Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner's dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement-learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The first phase is offline: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the Apple-Pear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents.},
booktitle = {International Conference on Distributed Artificial Intelligence (DAI)},
articleno = {11},
numpages = {7},
keywords = {opponent model, sequential prisoner's dilemmas, deep multiagent reinforcement learning, mutual cooperation},
location = {Beijing, China},
}

@article{daskalakis22gametheory,
  author = {Daskalakis, Constantinos and Golowich, Noah and Zhang, Kaiqing},
  title = {The Complexity of Markov Equilibrium in Stochastic Games},
  journal   = {CoRR},
  volume    = {abs/2204.03991},
  year      = {2022},
  url = {https://arxiv.org/abs/2204.03991},
  archivePrefix = {arXiv},
  eprint    = {2204.03991},
}

@article{daskalakis13gametheory,
author = {Daskalakis, Constantinos},
title = {On the Complexity of Approximating a Nash Equilibrium},
year = {2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1549-6325},
url = {https://doi.org/10.1145/2483699.2483703},
doi = {10.1145/2483699.2483703},
abstract = {We show that computing a relatively (i.e., multiplicatively as opposed to additively) approximate Nash equilibrium in two-player games is PPAD-complete, even for constant values of the approximation. Our result is the first constant inapproximability result for Nash equilibrium, since the original results on the computational complexity of the problem [Daskalakis et al. 2006a; Chen and Deng 2006]. Moreover, it provides an apparent---assuming that PPAD is not contained in TIME(nO(log n))---dichotomy between the complexities of additive and relative approximations, as for constant values of additive approximation a quasi-polynomial-time algorithm is known [Lipton et al. 2003]. Such a dichotomy does not exist for values of the approximation that scale inverse-polynomially with the size of the game, where both relative and additive approximations are PPAD-complete [Chen et al. 2006]. As a byproduct, our proof shows that (unconditionally) the sparse-support lemma [Lipton et al. 2003] cannot be extended to relative notions of constant approximation.},
journal = {ACM Transactions on Algorithms},
articleno = {23},
numpages = {35},
keywords = {game theory, PPAD-completeness, Complexity, approximation, Nash equilibrium}
}

@INPROCEEDINGS{wadhwania2019policy,
  author={Wadhwania, Samir and Kim, Dong-Ki and Omidshafiei, Shayegan and How, Jonathan P.},
  booktitle={International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Policy Distillation and Value Matching in Multiagent Reinforcement Learning}, 
  year={2019},
  volume={},
  number={},
  pages={8193-8200},
  doi={10.1109/IROS40897.2019.8967849}}