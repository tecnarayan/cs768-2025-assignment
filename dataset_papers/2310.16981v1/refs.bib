@article{pleiss2020identifying,
  title={Identifying mislabeled data using the area under the margin ranking},
  author={Pleiss, Geoff and Zhang, Tianyi and Elenberg, Ethan and Weinberger, Kilian Q},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17044--17056},
  year={2020}
}

@article{paul2021deep,
  title={Deep learning on a data diet: Finding important examples early in training},
  author={Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20596--20607},
  year={2021}
}

@inproceedings{agarwal2022estimating,
  title={Estimating example difficulty using variance of gradients},
  author={Agarwal, Chirag and D'souza, Daniel and Hooker, Sara},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10368--10378},
  year={2022}
}

@inproceedings{swayamdipta2020dataset,
  title={Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},
  author={Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A and Choi, Yejin},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9275--9293},
  year={2020}
}

@inproceedings{jain2020overview,
  title={Overview and importance of data quality for machine learning tasks},
  author={Jain, Abhinav and Patel, Hima and Nagalapatti, Lokesh and Gupta, Nitin and Mehta, Sameep and Guttula, Shanmukha and Mujumdar, Shashank and Afzal, Shazia and Sharma Mittal, Ruhi and Munigala, Vitobha},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3561--3562},
  year={2020}
}

@article{gupta2021dataB,
  title={Data Quality Toolkit: Automatic assessment of data quality and remediation for machine learning datasets},
  author={Gupta, Nitin and Patel, Hima and Afzal, Shazia and Panwar, Naveen and Mittal, Ruhi Sharma and Guttula, Shanmukha and Jain, Abhinav and Nagalapatti, Lokesh and Mehta, Sameep and Hans, Sandeep and others},
  journal={arXiv preprint arXiv:2108.05935},
  year={2021}
}

@article{renggli2021data,
  title={A Data Quality-Driven View of MLOps},
  author={Renggli, Cedric and Rimanic, Luka and G{\"u}rel, Nezihe Merve and Karlas, Bojan and Wu, Wentao and Zhang, Ce},
  journal={IEEE Data Engineering Bulletin},
  year={2021}
}


@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
  publisher={JMLR. org}
}

@article{qian2023synthcity,
  title={Synthcity: facilitating innovative use cases of synthetic data in different data modalities},
  author={Qian, Zhaozhi and Cebere, Bogdan-Constantin and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2301.07573},
  year={2023}
}

@article{lu2023machine,
  title={Machine Learning for Synthetic Data Generation: a Review},
  author={Lu, Yingzhou and Wang, Huazheng and Wei, Wenqi},
  journal={arXiv preprint arXiv:2302.04062},
  year={2023}
}

@article{shen2023study,
  title={A Study on Improving Realism of Synthetic Data for Machine Learning},
  author={Shen, Tingwei and Zhao, Ganning and You, Suya},
  journal={arXiv preprint arXiv:2304.12463},
  year={2023}
}

@article{alkhalifah2022mlreal,
  title={MLReal: Bridging the gap between training on synthetic data and real data applications in machine learning},
  author={Alkhalifah, Tariq and Wang, Hanchen and Ovcharenko, Oleg},
  journal={Artificial Intelligence in Geosciences},
  volume={3},
  pages={101--114},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{alaa2022faithful,
  title={How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models},
  author={Alaa, Ahmed and Van Breugel, Boris and Saveliev, Evgeny S and van der Schaar, Mihaela},
  booktitle={International Conference on Machine Learning},
  pages={290--306},
  year={2022},
  organization={PMLR}
}

@article{snoke2018general,
  title={General and specific utility measures for synthetic data},
  author={Snoke, Joshua and Raab, Gillian M and Nowok, Beata and Dibben, Chris and Slavkovic, Aleksandra},
  journal={Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume={181},
  number={3},
  pages={663--688},
  year={2018},
  publisher={JSTOR}
}

@article{young2009using,
  title={Using Bayesian networks to create synthetic data},
  author={Young, Jim and Graham, Patrick and Penny, Richard},
  journal={Journal of Official Statistics},
  volume={25},
  number={4},
  pages={549},
  year={2009}
}

@article{kotelnikov2022tabddpm,
  title={TabDDPM: Modelling Tabular Data with Diffusion Models},
  author={Kotelnikov, Akim and Baranchuk, Dmitry and Rubachev, Ivan and Babenko, Artem},
  journal={arXiv preprint arXiv:2209.15421},
  year={2022}
}

@inproceedings{lee2022differentially,
  title={Differentially Private Normalizing Flows for Synthetic Tabular Data Generation},
  author={Lee, Jaewoo and Kim, Minjung and Jeong, Yonghyun and Ro, Youngmin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={7345--7353},
  year={2022}
}

@inproceedings{rezende2015variational,
  title={Variational inference with normalizing flows},
  author={Rezende, Danilo and Mohamed, Shakir},
  booktitle={International conference on machine learning},
  pages={1530--1538},
  year={2015},
  organization={PMLR}
}

@article{bond2021deep,
  title={Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models},
  author={Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2021},
  publisher={IEEE}
}

@inproceedings{patki2016synthetic,
  title={The synthetic data vault},
  author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  pages={399--410},
  year={2016},
  organization={IEEE}
}

@misc{gretal,
	author = {},
	title = {{G}it{H}ub - gretelai/gretel-synthetics: {S}ynthetic data generators for structured and unstructured text, featuring differentially private learning. --- github.com},
	howpublished = {\url{https://github.com/gretelai/gretel-synthetics}},
	year = {},
	note = {[Accessed 06-Jun-2023]},
}

@misc{ydata,
	author = {},
	title = {{G}it{H}ub - ydataai/ydata-synthetic: {S}ynthetic data generators for tabular and time-series data --- github.com},
	howpublished = {\url{https://github.com/ydataai/ydata-synthetic}},
	year = {},
	note = {[Accessed 06-Jun-2023]},
}


@article{borisov2021deep,
  title={Deep neural networks and tabular data: A survey},
  author={Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={arXiv preprint arXiv:2110.01889},
  year={2021}
}

@article{shwartz2022tabular,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}

@misc{kaggle_2017, title={ Kaggle Machine Learning and Data Science Survey}, url={https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017}, author={Kaggle}, year={2017}} 

@article{dahmen2019synsys,
  title={SynSys: A synthetic data generation system for healthcare applications},
  author={Dahmen, Jessamyn and Cook, Diane},
  journal={Sensors},
  volume={19},
  number={5},
  pages={1181},
  year={2019},
  publisher={MDPI}
}


@article{akrami2020robust,
  title={Robust variational autoencoder for tabular data with beta divergence},
  author={Akrami, Haleh and Aydore, Sergul and Leahy, Richard M and Joshi, Anand A},
  journal={arXiv preprint arXiv:2006.08204},
  year={2020}
}

@article{xu2019modeling,
  title={Modeling tabular data using conditional gan},
  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{ganev2022robin,
  title={Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data},
  author={Ganev, Georgi and Oprisanu, Bristena and De Cristofaro, Emiliano},
  booktitle={International Conference on Machine Learning},
  pages={6944--6959},
  year={2022},
  organization={PMLR}
}

@article{liang2022advances,
  title={Advances, challenges and opportunities in creating data for trustworthy AI},
  author={Liang, Weixin and Tadesse, Girmaw Abebe and Ho, Daniel and Fei-Fei, L and Zaharia, Matei and Zhang, Ce and Zou, James},
  journal={Nature Machine Intelligence},
  volume={4},
  number={8},
  pages={669--677},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{seedat2022dc,
  title={DC-Check: A Data-Centric AI checklist to guide the development of reliable machine learning systems},
  author={Seedat, Nabeel and Imrie, Fergus and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2211.05764},
  year={2022}
}

@article{decaf,
  title={Decaf: Generating fair synthetic data using causally-aware generative networks},
  author={van Breugel, Boris and Kyono, Trent and Berrevoets, Jeroen and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22221--22233},
  year={2021}
}

@inproceedings{assefa2020generating,
  title={Generating synthetic data in finance: opportunities, challenges and pitfalls},
  author={Assefa, Samuel A and Dervovic, Danial and Mahfouz, Mahmoud and Tillman, Robert E and Reddy, Prashant and Veloso, Manuela},
  booktitle={Proceedings of the First ACM International Conference on AI in Finance},
  pages={1--8},
  year={2020}
}


@inproceedings{xu2019achieving,
  title={Achieving causal fairness through generative adversarial networks},
  author={Xu, Depeng and Wu, Yongkai and Yuan, Shuhan and Zhang, Lu and Wu, Xintao},
  booktitle={Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence},
  year={2019}
}

@article{dina2022effect,
  title={Effect of Balancing Data Using Synthetic Data on the Performance of Machine Learning Classifiers for Intrusion Detection in Computer Networks},
  author={Dina, Ayesha S and Siddique, AB and Manivannan, D},
  journal={arXiv preprint arXiv:2204.00144},
  year={2022}
}

@article{bing2022conditional,
    doi = {10.1371/journal.pdig.0000074},
    author = {Bing, Simon AND Dittadi, Andrea AND Bauer, Stefan AND Schwab, Patrick},
    journal = {PLOS Digital Health},
    publisher = {Public Library of Science},
    title = {Conditional generation of medical time series for extrapolation to underrepresented populations},
    year = {2022},
    month = {07},
    volume = {1},
    pages = {1-26},
    number = {7},
}


@inproceedings{das2022conditional,
  title={Conditional synthetic data generation for robust machine learning applications with limited pandemic data},
  author={Das, Hari Prasanna and Tran, Ryan and Singh, Japjot and Yue, Xiangyu and Tison, Geoffrey and Sangiovanni-Vincentelli, Alberto and Spanos, Costas J},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={11792--11800},
  year={2022}
}


@article{antoniou2017data,
  title={Data augmentation generative adversarial networks},
  author={Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
  journal={arXiv preprint arXiv:1711.04340},
  year={2017}
}

@inproceedings{jordon2018pate,
  title={{PATE-GAN}: Generating synthetic data with differential privacy guarantees},
  author={Jordon, James and Yoon, Jinsung and Van Der Schaar, Mihaela},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{competitions,
  title={Measuring the quality of synthetic data for use in competitions},
  author={Jordon, James and Yoon, Jinsung and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:1806.11345},
  year={2018}
}


@inproceedings{triage2023,
  title={TRIAGE: Characterizing and auditing training data for improved regression},
  author={Seedat, Nabeel and Crabb{\'e}, Jonathan and Qian, Zhaozhi and van der Schaar, Mihaela},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}




@inproceedings{seedat2022ds,
  title={Data-SUITE: Data-centric identification of in-distribution incongruous examples},
  author={Seedat, Nabeel and Crabb{\'e}, Jonathan and van der Schaar, Mihaela},
  booktitle={International Conference on Machine Learning},
  pages={19467--19496},
  year={2022},
  organization={PMLR}
}


@inproceedings{seedatdata,
  title={Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data},
  author={Seedat, Nabeel and Crabb{\'e}, Jonathan and Bica, Ioana and van der Schaar, Mihaela},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{northcutt2021confident,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}


@misc{grinsztajn_why_2022,
	title = {Why do tree-based models still outperform deep learning on tabular data?},
	url = {http://arxiv.org/abs/2207.08815},
	doi = {10.48550/arXiv.2207.08815},
	abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as {XGBoost} and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\${\textbackslash}sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks ({NNs}). This leads to a series of challenges which should guide researchers aiming to build tabular-specific {NNs}: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
	number = {{arXiv}:2207.08815},
	publisher = {{arXiv}},
	author = {Grinsztajn, Léo and Oyallon, Edouard and Varoquaux, Gaël},
	urldate = {2023-03-24},
	date = {2022-07-18},
	eprinttype = {arxiv},
	eprint = {2207.08815 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/au554730/Zotero/storage/5BH8PB49/2207.html:text/html;Grinsztajn et al_2022_Why do tree-based models still outperform deep learning on tabular data.pdf:/Users/au554730/Library/Mobile Documents/com~apple~CloudDocs/zotero_storage/Grinsztajn et al_2022_Why do tree-based models still outperform deep learning on tabular data.pdf:application/pdf},
}

@article{baqui_ethnic_2020,
  title={Ethnic and regional variations in hospital mortality from COVID-19 in Brazil: a cross-sectional observational study},
  author={Baqui, Pedro and Bica, Ioana and Marra, Valerio and Ercole, Ari and van Der Schaar, Mihaela},
  journal={The Lancet Global Health},
  volume={8},
  number={8},
  pages={e1018--e1026},
  year={2020},
  publisher={Elsevier}
}


@misc{akiba_optuna_2019,
	title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
	url = {http://arxiv.org/abs/1907.10902},
	doi = {10.48550/arXiv.1907.10902},
	shorttitle = {Optuna},
	abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run {API} that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the {MIT} license (https://github.com/pfnet/optuna/).},
	number = {{arXiv}:1907.10902},
	publisher = {{arXiv}},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	urldate = {2023-06-13},
	date = {2019-07-25},
	eprinttype = {arxiv},
	eprint = {1907.10902 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Akiba et al_2019_Optuna.pdf:/Users/au554730/Library/Mobile Documents/com~apple~CloudDocs/zotero_storage/Akiba et al_2019_Optuna.pdf:application/pdf;arXiv.org Snapshot:/Users/au554730/Zotero/storage/3BWK96HH/1907.html:text/html},
}


@misc{bello_dagma_2023,
	title = {{DAGMA}: Learning {DAGs} via M-matrices and a Log-Determinant Acyclicity Characterization},
	url = {http://arxiv.org/abs/2209.08037},
	doi = {10.48550/arXiv.2209.08037},
	shorttitle = {{DAGMA}},
	abstract = {The combinatorial problem of learning directed acyclic graphs ({DAGs}) from data was recently framed as a purely continuous optimization problem by leveraging a differentiable acyclicity characterization of {DAGs} based on the trace of a matrix exponential function. Existing acyclicity characterizations are based on the idea that powers of an adjacency matrix contain information about walks and cycles. In this work, we propose a new acyclicity characterization based on the log-determinant (log-det) function, which leverages the nilpotency property of {DAGs}. To deal with the inherent asymmetries of a {DAG}, we relate the domain of our log-det characterization to the set of \${\textbackslash}textit\{M-matrices\}\$, which is a key difference to the classical log-det function defined over the cone of positive definite matrices. Similar to acyclicity functions previously proposed, our characterization is also exact and differentiable. However, when compared to existing characterizations, our log-det function: (1) Is better at detecting large cycles; (2) Has better-behaved gradients; and (3) Its runtime is in practice about an order of magnitude faster. From the optimization side, we drop the typically used augmented Lagrangian scheme and propose {DAGMA} (\${\textbackslash}textit\{{DAGs} via M-matrices for Acyclicity\}\$), a method that resembles the central path for barrier methods. Each point in the central path of {DAGMA} is a solution to an unconstrained problem regularized by our log-det function, then we show that at the limit of the central path the solution is guaranteed to be a {DAG}. Finally, we provide extensive experiments for \${\textbackslash}textit\{linear\}\$ and \${\textbackslash}textit\{nonlinear\}\$ {SEMs} and show that our approach can reach large speed-ups and smaller structural Hamming distances against state-of-the-art methods. Code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/dagma.},
	number = {{arXiv}:2209.08037},
	publisher = {{arXiv}},
	author = {Bello, Kevin and Aragam, Bryon and Ravikumar, Pradeep},
	urldate = {2023-06-14},
	date = {2023-01-15},
	eprinttype = {arxiv},
	eprint = {2209.08037 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/au554730/Zotero/storage/B5JUKEZG/2209.html:text/html;Bello et al_2023_DAGMA.pdf:/Users/au554730/Library/Mobile Documents/com~apple~CloudDocs/zotero_storage/Bello et al_2023_DAGMA.pdf:application/pdf},
}


@misc{misc_adult_2,
  author       = {Becker,Barry and Kohavi,Ronny},
  title        = {{Adult}},
  year         = {1996},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5XW20}
}

@misc{northcutt2021pervasive,
      title={Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks}, 
      author={Curtis G. Northcutt and Anish Athalye and Jonas Mueller},
      year={2021},
      eprint={2103.14749},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}