\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and
  Eckstein]{boyd2011distributed}
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin]{chen2020lottery}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{arXiv preprint arXiv:2007.12223}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yin, and Jha]{dai2019nest}
Dai, X., Yin, H., and Jha, N.
\newblock Nest: A neural network synthesis tool based on a grow-and-prune
  paradigm.
\newblock \emph{IEEE Transactions on Computers}, 2019.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2019stabilizing}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Stabilizing the lottery ticket hypothesis.
\newblock \emph{arXiv preprint arXiv:1903.01611}, 2019.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020pruning}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020{\natexlab{a}}.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Schwab, and
  Morcos]{frankle2020early}
Frankle, J., Schwab, D.~J., and Morcos, A.~S.
\newblock The early phase of neural network training.
\newblock \emph{arXiv preprint arXiv:2002.10365}, 2020{\natexlab{b}}.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Pool, Tran, and Dally]{han2015}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1135--1143, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pp.\  1135--1143, 2015{\natexlab{b}}.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deep_compression}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  1389--1397, 2017.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{he2019filter}
He, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  4340--4349, 2019.

\bibitem[He et~al.(2020)He, Ding, Liu, Zhu, Zhang, and Yang]{He_2020_CVPR}
He, Y., Ding, Y., Liu, P., Zhu, L., Zhang, H., and Yang, Y.
\newblock Learning filter pruning criteria for deep convolutional neural
  networks acceleration.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2020.

\bibitem[Lin et~al.(2020)Lin, Ji, Wang, Zhang, Zhang, Tian, and
  Shao]{Lin_2020_CVPR}
Lin, M., Ji, R., Wang, Y., Zhang, Y., Zhang, B., Tian, Y., and Shao, L.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2020.

\bibitem[Liu et~al.(2020)Liu, Ma, Xu, Wang, Tang, and Ye]{Liu2020Autocompress}
Liu, N., Ma, X., Xu, Z., Wang, Y., Tang, J., and Ye, J.
\newblock Autocompress: An automatic dnn structured pruning framework for
  ultra-high compression rates.
\newblock In \emph{AAAI}, 2020.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Schwartz, and
  Shamir]{malach2020proving}
Malach, E., Yehudai, G., Shalev-Schwartz, S., and Shamir, O.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6682--6691. PMLR, 2020.

\bibitem[Min et~al.(2018)Min, Wang, Chen, Xu, and Chen]{min20182pfpce}
Min, C., Wang, A., Chen, Y., Xu, W., and Chen, X.
\newblock 2pfpce: Two-phase filter pruning based on conditional entropy.
\newblock \emph{arXiv preprint arXiv:1809.02220}, 2018.

\bibitem[Morcos et~al.(2019)Morcos, Yu, Paganini, and Tian]{morcos2019one}
Morcos, A.~S., Yu, H., Paganini, M., and Tian, Y.
\newblock One ticket to win them all: generalizing lottery ticket
  initializations across datasets and optimizers.
\newblock \emph{arXiv preprint arXiv:1906.02773}, 2019.

\bibitem[Ouyang et~al.(2013)Ouyang, He, Tran, and Gray]{ouyang2013stochastic}
Ouyang, H., He, N., Tran, L., and Gray, A.
\newblock Stochastic alternating direction method of multipliers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  80--88.
  PMLR, 2013.

\bibitem[Ren et~al.(2019)Ren, Zhang, Ye, Xu, Qian, Lin, and
  Wang]{ren2019ADMMNN}
Ren, A., Zhang, T., Ye, S., Xu, W., Qian, X., Lin, X., and Wang, Y.
\newblock Admm-nn: an algorithm-hardware co-design framework of dnns using
  alternating direction methods of multipliers.
\newblock In \emph{ASPLOS}, 2019.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{renda2020comparing}
Renda, A., Frankle, J., and Carbin, M.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock \emph{arXiv preprint arXiv:2003.02389}, 2020.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Tan \& Motani(2020)Tan and Motani]{tan2020dropnet}
Tan, C. M.~J. and Motani, M.
\newblock Dropnet: Reducing neural network complexity via iterative pruning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9356--9366. PMLR, 2020.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and Ganguli]{SynFlow}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{arXiv preprint arXiv:2006.05467}, 2020.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pp.\  2074--2082, 2016.

\bibitem[Ye et~al.(2020)Ye, Gong, Nie, Zhou, Klivans, and Liu]{ye2020good}
Ye, M., Gong, C., Nie, L., Zhou, D., Klivans, A., and Liu, Q.
\newblock Good subnetworks provably exist: Pruning via greedy forward
  selection.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10820--10830. PMLR, 2020.

\bibitem[You et~al.(2019)You, Li, Xu, Fu, Wang, Chen, Baraniuk, Wang, and
  Lin]{you2019drawing}
You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.~G., Wang, Z.,
  and Lin, Y.
\newblock Drawing early-bird tickets: Towards more efficient training of deep
  networks.
\newblock \emph{arXiv preprint arXiv:1909.11957}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Ye, Zhang, Wang, and
  Fardad]{zhang2018systematic}
Zhang, T., Ye, S., Zhang, Y., Wang, Y., and Fardad, M.
\newblock Systematic weight pruning of dnns using alternating direction method
  of multipliers.
\newblock \emph{arXiv preprint arXiv:1802.05747}, 2018.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}, 2017.

\end{thebibliography}
