\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{ESAP{\etalchar{+}}20}

\bibitem[ADH{\etalchar{+}}19]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8139--8148, 2019.

\bibitem[ASB16]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1120--1128, 2016.

\bibitem[AWBB20]{alemohammad2020recurrent}
Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk.
\newblock The recurrent neural tangent kernel.
\newblock {\em arXiv preprint arXiv:2006.10246}, 2020.

\bibitem[AZLS18]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[BFT{\etalchar{+}}18]{benjamin2018modern}
Ari~S Benjamin, Hugo~L Fernandes, Tucker Tomlinson, Pavan Ramkumar, Chris
  VerSteeg, Raeed~H Chowdhury, Lee~E Miller, and Konrad~P Kording.
\newblock Modern machine learning as a benchmark for fitting neural responses.
\newblock {\em Frontiers in computational neuroscience}, 12:56, 2018.

\bibitem[BHMM19]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proc. National Academy of Sciences}, 116(32):15849--15854, 2019.

\bibitem[BKM{\etalchar{+}}19]{Barbier_2019}
Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka
  Zdeborová.
\newblock Optimal errors and phase transitions in high-dimensional generalized
  linear models.
\newblock {\em Proc. National Academy of Sciences}, 116(12):5451–5460, March
  2019.

\bibitem[BM11]{bayati2011dynamics}
Mohsen Bayati and Andrea Montanari.
\newblock The dynamics of message passing on dense graphs, with applications to
  compressed sensing.
\newblock {\em IEEE Transactions on Information Theory}, 57(2):764--785, 2011.

\bibitem[BSF94]{bengio1994learning}
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE transactions on neural networks}, 5(2):157--166, 1994.

\bibitem[BY86]{bai1986limiting}
Zhi~Dong Bai and Yong~Quan Yin.
\newblock Limiting behavior of the norm of products of random matrices and two
  problems of geman-hwang.
\newblock {\em Probability theory and related fields}, 73(4):555--569, 1986.

\bibitem[CPS18]{chen2018dynamical}
Minmin Chen, Jeffrey Pennington, and Samuel~S Schoenholz.
\newblock Dynamical isometry and a mean field theory of rnns: Gating enables
  signal propagation in recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1806.05394}, 2018.

\bibitem[Dan17]{daniely2017sgd}
Amit Daniely.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem[DFS16]{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem[DL20]{dou2020training}
Xialiang Dou and Tengyuan Liang.
\newblock Training neural networks as learning data-adaptive kernels: Provable
  representation and approximation benefits.
\newblock {\em Journal of the American Statistical Association}, pages 1--14,
  2020.

\bibitem[DLL{\etalchar{+}}18]{du2018gradient}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem[DZPS18]{du2018gradient2}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[EARF19]{emami2019input}
Melikasadat Emami, Mojtaba~Sahraee Ardakan, Sundeep Rangan, and Alyson~K
  Fletcher.
\newblock Input-output equivalence of unitary and contractive rnns.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15342--15352, 2019.

\bibitem[EMW20]{E_2020}
Weinan E, Chao Ma, and Lei Wu.
\newblock A comparative analysis of optimization and generalization properties
  of two-layer neural network and random feature models under gradient descent
  dynamics.
\newblock {\em Science China Mathematics}, Jan 2020.

\bibitem[ESAP{\etalchar{+}}20]{emami2020generalization}
Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, and
  Alyson~K Fletcher.
\newblock Generalization error of generalized linear models in high dimensions.
\newblock {\em arXiv preprint arXiv:2005.00180}, 2020.

\bibitem[GAK20]{gerbelot2020asymptotic}
C{\'e}dric Gerbelot, Alia Abbara, and Florent Krzakala.
\newblock Asymptotic errors for convex penalized linear regression beyond
  gaussian matrices.
\newblock {\em arXiv preprint arXiv:2002.04372}, 2020.

\bibitem[GARA19]{garriga-alonso2018deep}
Adrià Garriga-Alonso, Carl~Edward Rasmussen, and Laurence Aitchison.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[GS{\etalchar{+}}84]{givens1984class}
Clark~R Givens, Rae~Michael Shortt, et~al.
\newblock A class of wasserstein metrics for probability distributions.
\newblock {\em The Michigan Mathematical Journal}, 31(2):231--240, 1984.

\bibitem[HMRT19]{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[JSD{\etalchar{+}}17]{jing2017tunable}
Li~Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max
  Tegmark, and Marin Solja{\v{c}}i{\'c}.
\newblock Tunable efficient unitary neural networks (eunn) and their
  application to rnns.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1733--1741. JMLR. org, 2017.

\bibitem[Kai80]{kailath1980linear}
Thomas Kailath.
\newblock {\em Linear systems}, volume 156.
\newblock Prentice-Hall Englewood Cliffs, NJ, 1980.

\bibitem[Kat06]{katayama2006subspace}
Tohru Katayama.
\newblock {\em Subspace methods for system identification}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Len99]{lennart1999system}
Ljung Lennart.
\newblock System identification: theory for the user.
\newblock {\em PTR Prentice Hall, Upper Saddle River, NJ}, pages 1--14, 1999.

\bibitem[LG94]{ljung1994modeling}
L.~Ljung and T.~Glad.
\newblock {\em Modeling of Dynamic Systems}.
\newblock Prentice-Hall information and system sciences series. PTR Prentice
  Hall, 1994.

\bibitem[Lju99]{ljung1999system}
Lennart Ljung.
\newblock System identification.
\newblock {\em Wiley encyclopedia of electrical and electronics engineering},
  pages 1--19, 1999.

\bibitem[LL18]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem[LSdP{\etalchar{+}}18]{lee2018deep}
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam
  Schoenholz, and Yasaman Bahri.
\newblock Deep neural networks as gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[LXS{\etalchar{+}}19]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  8570--8581, 2019.

\bibitem[MMN18]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671, 2018.

\bibitem[MRH{\etalchar{+}}18]{matthews2018gaussian}
Alexander G de~G Matthews, Mark Rowland, Jiri Hron, Richard~E Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock {\em arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[MRSY19]{montanari2019generalization}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock {\em arXiv preprint arXiv:1911.01544}, 2019.

\bibitem[MW{\etalchar{+}}19]{ma2019comparative}
Chao Ma, Lei Wu, et~al.
\newblock A comparative analysis of the optimization and generalization
  property of two-layer neural network and random feature models under gradient
  descent dynamics.
\newblock {\em arXiv preprint arXiv:1904.04326}, 2019.

\bibitem[Nea96]{Neal_1996}
Radford~M. Neal.
\newblock {\em Bayesian Learning for Neural Networks}.
\newblock Springer New York, 1996.

\bibitem[NXB{\etalchar{+}}19]{novak2019bayesian}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[PS12]{pintelon2012system}
Rik Pintelon and Johan Schoukens.
\newblock {\em System identification: a frequency domain approach}.
\newblock John Wiley \& Sons, 2012.

\bibitem[RSF19]{rangan2019vector}
Sundeep Rangan, Philip Schniter, and Alyson~K Fletcher.
\newblock Vector approximate message passing.
\newblock {\em IEEE Transactions on Information Theory}, 65(10):6664--6684,
  2019.

\bibitem[RVE18]{rotskoff2018neural}
Grant~M Rotskoff and Eric Vanden-Eijnden.
\newblock Neural networks as interacting particle systems: Asymptotic convexity
  of the loss landscape and universal scaling of the approximation error.
\newblock {\em arXiv preprint arXiv:1805.00915}, 2018.

\bibitem[SKS{\etalchar{+}}20]{saha2020rnnpool}
Oindrila Saha, Aditya Kusupati, Harsha~Vardhan Simhadri, Manik Varma, and
  Prateek Jain.
\newblock Rnnpool: Efficient non-linear pooling for ram constrained inference.
\newblock {\em arXiv preprint arXiv:2002.11921}, 2020.

\bibitem[SS89]{soderstrom1989system}
Torsten S{\"o}derstr{\"o}m and Petre Stoica.
\newblock {\em System identification}.
\newblock Prentice-Hall International, 1989.

\bibitem[SS20]{sirignano2020mean}
Justin Sirignano and Konstantinos Spiliopoulos.
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock {\em Stochastic Processes and their Applications}, 130(3):1820--1852,
  2020.

\bibitem[SZL{\etalchar{+}}95]{sjoberg1995nonlinear}
Jonas Sj{\"o}berg, Qinghua Zhang, Lennart Ljung, Albert Benveniste, Bernard
  Deylon, Pierre-Yves Glorennec, H{\aa}kan Hjalmarsson, and Anatoli Juditsky.
\newblock {\em Nonlinear black-box modeling in system identification: a unified
  overview}.
\newblock Link{\"o}ping University, 1995.

\bibitem[Vib95]{viberg1995subspace}
Mats Viberg.
\newblock Subspace-based methods for the identification of linear
  time-invariant systems.
\newblock {\em Automatica}, 31(12):1835--1851, 1995.

\bibitem[Vil08]{villani2008optimal}
C{\'e}dric Villani.
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[WLLM19]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9709--9721, 2019.

\bibitem[WPH{\etalchar{+}}16]{wisdom2016full}
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le~Roux, and Les Atlas.
\newblock Full-capacity unitary recurrent neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4880--4888, 2016.

\bibitem[Yan19a]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yan19b]{yang2019wide}
Greg Yang.
\newblock Wide feedforward or recurrent neural networks of any architecture are
  gaussian processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9951--9960, 2019.

\bibitem[ZCZG18]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock {\em arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
