\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartunov \& Vetrov(2018)Bartunov and Vetrov]{bartunov2018few}
Bartunov, S. and Vetrov, D.
\newblock Few-shot generative modelling with generative matching networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  670--678, 2018.

\bibitem[Bloem-Reddy \& Teh(2019)Bloem-Reddy and Teh]{bloem2019probabilistic}
Bloem-Reddy, B. and Teh, Y.~W.
\newblock Probabilistic symmetry and invariant neural networks.
\newblock \emph{arXiv preprint arXiv:1901.06082}, 2019.

\bibitem[Bloem-Reddy \& Teh(2020)Bloem-Reddy and Teh]{bloem2020probabilistic}
Bloem-Reddy, B. and Teh, Y.~W.
\newblock Probabilistic symmetries and invariant neural networks.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (90):\penalty0 1--61, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Child(2020)]{child2020very}
Child, R.
\newblock Very deep vaes generalize autoregressive models and can outperform
  them on images.
\newblock \emph{arXiv preprint arXiv:2011.10650}, 2020.

\bibitem[De~Finetti(2020)]{de20209}
De~Finetti, B.
\newblock 9. on the condition of partial exchangeability.
\newblock In \emph{Studies in Inductive Logic and Probability Volume 2}, pp.\
  193--206. University of California Press, 2020.

\bibitem[Edwards \& Storkey(2016)Edwards and Storkey]{edwards2016towards}
Edwards, H. and Storkey, A.
\newblock Towards a neural statistician.
\newblock \emph{arXiv preprint arXiv:1606.02185}, 2016.

\bibitem[Eslami et~al.(2018)Eslami, Rezende, Besse, Viola, Morcos, Garnelo,
  Ruderman, Rusu, Danihelka, Gregor, et~al.]{eslami2018neural}
Eslami, S.~A., Rezende, D.~J., Besse, F., Viola, F., Morcos, A.~S., Garnelo,
  M., Ruderman, A., Rusu, A.~A., Danihelka, I., Gregor, K., et~al.
\newblock Neural scene representation and rendering.
\newblock \emph{Science}, 360\penalty0 (6394):\penalty0 1204--1210, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{arXiv preprint arXiv:1703.03400}, 2017.

\bibitem[Garnelo et~al.(2018{\natexlab{a}})Garnelo, Rosenbaum, Maddison,
  Ramalho, Saxton, Shanahan, Teh, Rezende, and Eslami]{garnelo2018conditional}
Garnelo, M., Rosenbaum, D., Maddison, C.~J., Ramalho, T., Saxton, D., Shanahan,
  M., Teh, Y.~W., Rezende, D.~J., and Eslami, S.
\newblock Conditional neural processes.
\newblock \emph{arXiv preprint arXiv:1807.01613}, 2018{\natexlab{a}}.

\bibitem[Garnelo et~al.(2018{\natexlab{b}})Garnelo, Schwarz, Rosenbaum, Viola,
  Rezende, Eslami, and Teh]{garnelo2018neural}
Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D.~J., Eslami, S.,
  and Teh, Y.~W.
\newblock Neural processes.
\newblock \emph{arXiv preprint arXiv:1807.01622}, 2018{\natexlab{b}}.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and
  Griffiths]{grant2018recasting}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock \emph{arXiv preprint arXiv:1801.08930}, 2018.

\bibitem[Graves et~al.(2018)Graves, Menick, and van~den Oord]{graves2018acn}
Graves, A., Menick, J., and van~den Oord, A.
\newblock Associative compression networks for representation learning.
\newblock \emph{CoRR}, abs/1804.02476, 2018.
\newblock URL \url{http://arxiv.org/abs/1804.02476}.

\bibitem[Gregor et~al.(2015)Gregor, Danihelka, Graves, Rezende, and
  Wierstra]{gregor2015draw}
Gregor, K., Danihelka, I., Graves, A., Rezende, D.~J., and Wierstra, D.
\newblock Draw: A recurrent neural network for image generation.
\newblock \emph{arXiv preprint arXiv:1502.04623}, 2015.

\bibitem[Hewitt et~al.(2018)Hewitt, Nye, Gane, Jaakkola, and
  Tenenbaum]{hewitt2018variational}
Hewitt, L.~B., Nye, M.~I., Gane, A., Jaakkola, T., and Tenenbaum, J.~B.
\newblock The variational homoencoder: Learning to learn high capacity
  generative models from few examples.
\newblock \emph{arXiv preprint arXiv:1807.08919}, 2018.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Hoffman, M.~D., Blei, D.~M., Wang, C., and Paisley, J.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Hospedales et~al.(2020)Hospedales, Antoniou, Micaelli, and
  Storkey]{hospedales2020meta}
Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{arXiv preprint arXiv:2004.05439}, 2020.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999introduction}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine learning}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Kim et~al.(2019)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum,
  Vinyals, and Teh]{kim2019attentive}
Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D.,
  Vinyals, O., and Teh, Y.~W.
\newblock Attentive neural processes.
\newblock \emph{arXiv preprint arXiv:1901.05761}, 2019.

\bibitem[Kim et~al.(2021)Kim, Yoo, Lee, and Hong]{kim2021setvae}
Kim, J., Yoo, J., Lee, J., and Hong, S.
\newblock Setvae: Learning hierarchical composition for generative modeling of
  set-structured data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  15059--15068, 2021.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and
  Tenenbaum]{lake2011one}
Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the annual meeting of the cognitive science
  society}, volume~33, 2011.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[Lake et~al.(2017)Lake, Ullman, Tenenbaum, and
  Gershman]{lake2017building}
Lake, B.~M., Ullman, T.~D., Tenenbaum, J.~B., and Gershman, S.~J.
\newblock Building machines that learn and think like people.
\newblock \emph{Behavioral and brain sciences}, 40, 2017.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y.~W.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3744--3753. PMLR, 2019.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
Maal{\o}e, L., Fraccaro, M., Li{\'e}vin, V., and Winther, O.
\newblock Biva: A very deep hierarchy of latent variables for generative
  modeling.
\newblock \emph{arXiv preprint arXiv:1902.02102}, 2019.

\bibitem[Oord et~al.(2016)Oord, Kalchbrenner, and Kavukcuoglu]{oord2016pixel}
Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K.
\newblock Pixel recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1601.06759}, 2016.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodr{\'\i}guez~L{\'o}pez, and
  Lacoste]{oreshkin2018tadam}
Oreshkin, B., Rodr{\'\i}guez~L{\'o}pez, P., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 721--731, 2018.

\bibitem[Perez et~al.(2017)Perez, Strub, De~Vries, Dumoulin, and
  Courville]{perez2017film}
Perez, E., Strub, F., De~Vries, H., Dumoulin, V., and Courville, A.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock \emph{arXiv preprint arXiv:1709.07871}, 2017.

\bibitem[Qi et~al.(2017)Qi, Su, Mo, and Guibas]{qi2017pointnet}
Qi, C.~R., Su, H., Mo, K., and Guibas, L.~J.
\newblock Pointnet: Deep learning on point sets for 3d classification and
  segmentation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  652--660, 2017.

\bibitem[Ravi \& Beatson(2018)Ravi and Beatson]{ravi2018amortized}
Ravi, S. and Beatson, A.
\newblock Amortized bayesian meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ravi \& Larochelle(2016)Ravi and Larochelle]{ravi2016optimization}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock 2016.

\bibitem[Reed et~al.(2017)Reed, Chen, Paine, Oord, Eslami, Rezende, Vinyals,
  and de~Freitas]{reed2017few}
Reed, S., Chen, Y., Paine, T., Oord, A. v.~d., Eslami, S., Rezende, D.,
  Vinyals, O., and de~Freitas, N.
\newblock Few-shot autoregressive density estimation: Towards learning to learn
  distributions.
\newblock \emph{arXiv preprint arXiv:1710.10304}, 2017.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Rezende et~al.(2016)Rezende, Mohamed, Danihelka, Gregor, and
  Wierstra]{rezende2016one}
Rezende, D.~J., Mohamed, S., Danihelka, I., Gregor, K., and Wierstra, D.
\newblock One-shot generalization in deep generative models.
\newblock \emph{arXiv preprint arXiv:1603.05106}, 2016.

\bibitem[Rusu et~al.(2018)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{rusu2018meta}
Rusu, A.~A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S.,
  and Hadsell, R.
\newblock Meta-learning with latent embedding optimization.
\newblock \emph{arXiv preprint arXiv:1807.05960}, 2018.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Schaul \& Schmidhuber(2010)Schaul and
  Schmidhuber]{schaul2010metalearning}
Schaul, T. and Schmidhuber, J.
\newblock Metalearning.
\newblock \emph{Scholarpedia}, 5\penalty0 (6):\penalty0 4650, 2010.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4077--4087, 2017.

\bibitem[S{\o}nderby et~al.(2016)S{\o}nderby, Raiko, Maal{\o}e, S{\o}nderby,
  and Winther]{sonderby2016ladder}
S{\o}nderby, C.~K., Raiko, T., Maal{\o}e, L., S{\o}nderby, S.~K., and Winther,
  O.
\newblock Ladder variational autoencoders.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3738--3746, 2016.

\bibitem[Sun(2019)]{mulitdigitmnist}
Sun, S.-H.
\newblock Multi-digit mnist for few-shot learning, 2019.
\newblock URL \url{https://github.com/shaohua0116/MultiDigitMNIST}.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{sung2018learning}
Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.~H., and Hospedales, T.~M.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1199--1208, 2018.

\bibitem[Tenenbaum(1999)]{tenenbaum1999bayesian}
Tenenbaum, J.~B.
\newblock \emph{A Bayesian framework for concept learning}.
\newblock PhD thesis, Massachusetts Institute of Technology, 1999.

\bibitem[Ullman \& Tenenbaum(2020)Ullman and Tenenbaum]{ullman2020bayesian}
Ullman, T.~D. and Tenenbaum, J.~B.
\newblock Bayesian models of conceptual development: Learning as building
  models of the world.
\newblock \emph{Annual Review of Developmental Psychology}, 2:\penalty0
  533--558, 2020.

\bibitem[Vahdat \& Kautz(2020)Vahdat and Kautz]{vahdat2020NVAE}
Vahdat, A. and Kautz, J.
\newblock {NVAE}: A deep hierarchical variational autoencoder.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Velickovic et~al.(2017)Velickovic, Cucurull, Casanova, Romero, Lio,
  and Bengio]{velickovic2017graph}
Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 1\penalty0 (2), 2017.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3630--3638, 2016.

\bibitem[Wu et~al.(2020)Wu, Choi, Goodman, and Ermon]{wu2020meta}
Wu, M., Choi, K., Goodman, N.~D., and Ermon, S.
\newblock Meta-amortized variational inference and learning.
\newblock In \emph{AAAI}, pp.\  6404--6412, 2020.

\bibitem[Xu et~al.(2019)Xu, Ton, Kim, Kosiorek, and Teh]{xu2019metafun}
Xu, J., Ton, J.-F., Kim, H., Kosiorek, A.~R., and Teh, Y.~W.
\newblock Metafun: Meta-learning with iterative functional updates.
\newblock \emph{arXiv preprint arXiv:1912.02738}, 2019.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{zaheer2017deep}
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.~R., and
  Smola, A.~J.
\newblock Deep sets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3391--3401, 2017.

\end{thebibliography}
