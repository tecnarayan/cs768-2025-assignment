\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem[Ambrosio et~al.(2008)Ambrosio, Gigli, and
  Savar{\'e}]{ambrosio2008gradient}
L.~Ambrosio, N.~Gigli, and G.~Savar{\'e}.
\newblock \emph{Gradient flows: in metric spaces and in the space of
  probability measures}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Andras(2002)]{andras2002equivalence}
P.~Andras.
\newblock The equivalence of support vector machine and regularization neural
  networks.
\newblock \emph{Neural Processing Letters}, 15\penalty0 (2):\penalty0 97--104,
  2002.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8141--8150, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Du, Li, Salakhutdinov, Wang,
  and Yu]{arora2019harnessing}
S.~Arora, S.~S. Du, Z.~Li, R.~Salakhutdinov, R.~Wang, and D.~Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock \emph{arXiv preprint arXiv:1910.01663}, 2019{\natexlab{c}}.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
P.~L. Bartlett, N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2285--2301, 2019.

\bibitem[Bertsekas and Scientific(2015)]{bertsekas2015convex}
D.~P. Bertsekas and A.~Scientific.
\newblock \emph{Convex optimization algorithms}.
\newblock Athena Scientific Belmont, 2015.

\bibitem[Boser et~al.(1992)Boser, Guyon, and Vapnik]{boser1992training}
B.~E. Boser, I.~M. Guyon, and V.~N. Vapnik.
\newblock A training algorithm for optimal margin classifiers.
\newblock In \emph{Proceedings of the fifth annual workshop on Computational
  learning theory}, pages 144--152, 1992.

\bibitem[Boyd et~al.(2003)Boyd, Xiao, and Mutapcic]{boyd2003subgradient}
S.~Boyd, L.~Xiao, and A.~Mutapcic.
\newblock Subgradient methods.
\newblock \emph{lecture notes of EE392o, Stanford University, Autumn Quarter},
  2004:\penalty0 2004--2005, 2003.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock \emph{arXiv preprint arXiv:1905.13210}, 2019.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
L.~Chizat and F.~Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Chizat et~al.(2018)Chizat, Oyallon, and Bach]{chizat2018lazy}
L.~Chizat, E.~Oyallon, and F.~Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{arXiv preprint arXiv:1812.07956}, 2018.

\bibitem[Cho(2012)]{cho2012kernel}
Y.~Cho.
\newblock \emph{Kernel methods for deep learning}.
\newblock PhD thesis, UC San Diego, 2012.

\bibitem[Cortes and Vapnik(1995)]{cortes1995support}
C.~Cortes and V.~Vapnik.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20\penalty0 (3):\penalty0 273--297, 1995.

\bibitem[Cristianini et~al.(2000)Cristianini, Shawe-Taylor,
  et~al.]{cristianini2000introduction}
N.~Cristianini, J.~Shawe-Taylor, et~al.
\newblock \emph{An introduction to support vector machines and other
  kernel-based learning methods}.
\newblock Cambridge university press, 2000.

\bibitem[Domingos(2020)]{domingos2020every}
P.~Domingos.
\newblock Every model learned by gradient descent is approximately a kernel
  machine.
\newblock \emph{arXiv preprint arXiv:2012.00152}, 2020.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1675--1685. PMLR, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Hou, P{\'o}czos, Salakhutdinov, Wang,
  and Xu]{du2019graph}
S.~S. Du, K.~Hou, B.~P{\'o}czos, R.~Salakhutdinov, R.~Wang, and K.~Xu.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock \emph{arXiv preprint arXiv:1905.13192}, 2019{\natexlab{b}}.

\bibitem[Gowal et~al.(2018)Gowal, Dvijotham, Stanforth, Bunel, Qin, Uesato,
  Arandjelovic, Mann, and Kohli]{gowal2018effectiveness}
S.~Gowal, K.~Dvijotham, R.~Stanforth, R.~Bunel, C.~Qin, J.~Uesato,
  R.~Arandjelovic, T.~Mann, and P.~Kohli.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock \emph{arXiv preprint arXiv:1810.12715}, 2018.

\bibitem[Hsieh et~al.(2008)Hsieh, Chang, Lin, Keerthi, and
  Sundararajan]{hsieh2008dual}
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.~S. Keerthi, and S.~Sundararajan.
\newblock A dual coordinate descent method for large-scale linear svm.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 408--415, 2008.

\bibitem[Hu et~al.(2021)Hu, Wang, Lin, and Cheng]{hu2021regularization}
T.~Hu, W.~Wang, C.~Lin, and G.~Cheng.
\newblock Regularization matters: A nonparametric perspective on
  overparametrized neural network.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 829--837. PMLR, 2021.

\bibitem[Hu et~al.(2019)Hu, Li, and Yu]{hu2019simple}
W.~Hu, Z.~Li, and D.~Yu.
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock \emph{arXiv preprint arXiv:1905.11368}, 2019.

\bibitem[Huang et~al.(2020)Huang, Du, and Da~Xu]{huang2020neural}
W.~Huang, W.~Du, and R.~Y. Da~Xu.
\newblock On the neural tangent kernel of deep networks with orthogonal
  initialization.
\newblock \emph{arXiv preprint arXiv:2004.05867}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Ji and Telgarsky(2018)]{ji2018risk}
Z.~Ji and M.~Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
J.~Lee, Y.~Bahri, R.~Novak, S.~S. Schoenholz, J.~Pennington, and
  J.~Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1711.00165}, 2017.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  8572--8583, 2019.

\bibitem[Liang et~al.(2017)Liang, Wang, Lei, Liao, and Li]{liang2017soft}
X.~Liang, X.~Wang, Z.~Lei, S.~Liao, and S.~Z. Li.
\newblock Soft-margin softmax for deep classification.
\newblock In \emph{International Conference on Neural Information Processing},
  pages 413--421. Springer, 2017.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Zhu, and Belkin]{liu2020linearity}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhu, and Belkin]{liu2020loss}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{arXiv preprint arXiv:2003.00307}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2016)Liu, Wen, Yu, and Yang]{liu2016large}
W.~Liu, Y.~Wen, Z.~Yu, and M.~Yang.
\newblock Large-margin softmax loss for convolutional neural networks.
\newblock In \emph{ICML}, volume~2, page~7, 2016.

\bibitem[Long and Sedghi(2019)]{long2019generalization}
P.~M. Long and H.~Sedghi.
\newblock Generalization bounds for deep convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1905.12600}, 2019.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
K.~Lyu and J.~Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
A.~G. d.~G. Matthews, M.~Rowland, J.~Hron, R.~E. Turner, and Z.~Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
M.~Mohri, A.~Rostamizadeh, and A.~Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Nacson et~al.(2019)Nacson, Gunasekar, Lee, Srebro, and
  Soudry]{nacson2019lexicographic}
M.~S. Nacson, S.~Gunasekar, J.~Lee, N.~Srebro, and D.~Soudry.
\newblock Lexicographic and depth-sensitive margins in homogeneous and
  non-homogeneous deep models.
\newblock In \emph{International Conference on Machine Learning}, pages
  4683--4692. PMLR, 2019.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
R.~Novak, L.~Xiao, J.~Hron, J.~Lee, A.~A. Alemi, J.~Sohl-Dickstein, and S.~S.
  Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{arXiv preprint arXiv:1912.01703}, 2019.

\bibitem[Pellegrini and Biroli(2020)]{pellegrini2020analytic}
F.~Pellegrini and G.~Biroli.
\newblock An analytic theory of shallow networks dynamics for hinge loss
  classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Poggio and Girosi(1990)]{poggio1990networks}
T.~Poggio and F.~Girosi.
\newblock Networks for approximation and learning.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (9):\penalty0 1481--1497,
  1990.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach,
  et~al.]{scholkopf2002learning}
B.~Sch{\"o}lkopf, A.~J. Smola, F.~Bach, et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shalev-Shwartz et~al.(2011)Shalev-Shwartz, Singer, Srebro, and
  Cotter]{shalev2011pegasos}
S.~Shalev-Shwartz, Y.~Singer, N.~Srebro, and A.~Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock \emph{Mathematical programming}, 127\penalty0 (1):\penalty0 3--30,
  2011.

\bibitem[Shawe-Taylor et~al.(2004)Shawe-Taylor, Cristianini,
  et~al.]{shawe2004kernel}
J.~Shawe-Taylor, N.~Cristianini, et~al.
\newblock \emph{Kernel methods for pattern analysis}.
\newblock Cambridge university press, 2004.

\bibitem[Smola et~al.(1998)Smola, Sch{\"o}lkopf, and
  M{\"u}ller]{smola1998connection}
A.~J. Smola, B.~Sch{\"o}lkopf, and K.-R. M{\"u}ller.
\newblock The connection between regularization operators and support vector
  kernels.
\newblock \emph{Neural networks}, 11\penalty0 (4):\penalty0 637--649, 1998.

\bibitem[Sokoli{\'c} et~al.(2017)Sokoli{\'c}, Giryes, Sapiro, and
  Rodrigues]{sokolic2017robust}
J.~Sokoli{\'c}, R.~Giryes, G.~Sapiro, and M.~R. Rodrigues.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4265--4280, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Steinwart and Christmann(2008)]{steinwart2008support}
I.~Steinwart and A.~Christmann.
\newblock \emph{Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Sun et~al.(2016)Sun, Chen, Wang, Liu, and Liu]{sun2016depth}
S.~Sun, W.~Chen, L.~Wang, X.~Liu, and T.-Y. Liu.
\newblock On the depth of deep neural networks: A theoretical view.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Tang(2013)]{tang2013deep}
Y.~Tang.
\newblock Deep learning using linear support vector machines.
\newblock \emph{arXiv preprint arXiv:1306.0239}, 2013.

\bibitem[Towns et~al.(2014)Towns, Cockerill, Dahan, Foster, Gaither, Grimshaw,
  Hazlewood, Lathrop, Lifka, Peterson, Roskies, Scott, and
  Wilkins-Diehr]{xsede}
J.~Towns, T.~Cockerill, M.~Dahan, I.~Foster, K.~Gaither, A.~Grimshaw,
  V.~Hazlewood, S.~Lathrop, D.~Lifka, G.~D. Peterson, R.~Roskies, J.~R. Scott,
  and N.~Wilkins-Diehr.
\newblock Xsede: Accelerating scientific discovery.
\newblock \emph{Computing in Science \& Engineering}, 16\penalty0 (5):\penalty0
  62--74, Sept.-Oct. 2014.
\newblock ISSN 1521-9615.
\newblock \doi{10.1109/MCSE.2014.80}.
\newblock URL \url{doi.ieeecomputersociety.org/10.1109/MCSE.2014.80}.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
C.~Wei, J.~Lee, Q.~Liu, and T.~Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock 2019.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Song, Hsieh, Daniel, Boning, and
  Dhillon]{weng2018towards}
L.~Weng, H.~Zhang, H.~Chen, Z.~Song, C.-J. Hsieh, L.~Daniel, D.~Boning, and
  I.~Dhillon.
\newblock Towards fast computation of certified robustness for relu networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5276--5285. PMLR, 2018.

\bibitem[Yang and Hu(2020)]{yang2020feature}
G.~Yang and E.~J. Hu.
\newblock Feature learning in infinite-width neural networks.
\newblock \emph{arXiv preprint arXiv:2011.14522}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang et~al.(2018)Zhang, Weng, Chen, Hsieh, and
  Daniel]{zhang2018efficient}
H.~Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L.~Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock \emph{arXiv preprint arXiv:1811.00866}, 2018.

\end{thebibliography}
