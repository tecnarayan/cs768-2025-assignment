\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Bottou(2015)]{agarwal2015lower}
Alekh Agarwal and Leon Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock In \emph{ICML}, 2015.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 4431--4506, 2021.

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (1):\penalty0 8194--8244, 2017.

\bibitem[Allen-Zhu and Hazan(2016)]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{ICML}, 2016.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{ICML}, 2019.

\bibitem[Arioli and Scott(2014)]{arioli2014chebyshev}
Mario Arioli and Jennifer Scott.
\newblock Chebyshev acceleration of iterative refinement.
\newblock \emph{Numerical Algorithms}, 66\penalty0 (3):\penalty0 591--608, 2014.

\bibitem[Arjevani et~al.(2022)Arjevani, Carmon, Duchi, Foster, Srebro, and Woodworth]{ACDFSW2019}
Yossi Arjevani, Yair Carmon, John~C. Duchi, Dylan~J. Foster, Nathan Srebro, and Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{Mathematical Programming}, pages 1--50, 2022.

\bibitem[Attouch and Bolte(2009)]{attouch2009convergence}
Hedy Attouch and J{\'e}r{\^o}me Bolte.
\newblock On the convergence of the proximal algorithm for nonsmooth functions involving analytic features.
\newblock \emph{Mathematical Programming}, 116:\penalty0 5--16, 2009.

\bibitem[Bi et~al.(2022)Bi, Zhang, and Lavaei]{bi2022local}
Yingjie Bi, Haixiang Zhang, and Javad Lavaei.
\newblock Local and global linear convergence of general low-rank matrix recovery problems.
\newblock In \emph{AAAI}, 2022.

\bibitem[Bolte et~al.(2007)Bolte, Daniilidis, and Lewis]{bolte2007lojasiewicz}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, and Adrian Lewis.
\newblock The {{\L}}ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems.
\newblock \emph{SIAM Journal on Optimization}, 17\penalty0 (4):\penalty0 1205--1223, 2007.

\bibitem[Bolte et~al.(2014)Bolte, Sabach, and Teboulle]{bolte2014proximal}
J{\'e}r{\^o}me Bolte, Shoham Sabach, and Marc Teboulle.
\newblock Proximal alternating linearized minimization for nonconvex and nonsmooth problems.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1-2):\penalty0 459--494, 2014.

\bibitem[Bu et~al.(2019)Bu, Mesbahi, Fazel, and Mesbahi]{bu2019lqr}
Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi.
\newblock {LQR} through the lens of first order methods: Discrete-time case.
\newblock \emph{arXiv preprint arXiv:1907.08921}, 2019.

\bibitem[Carmon and Duchi(2020)]{carmon2020first}
Yair Carmon and John~C. Duchi.
\newblock First-order methods for nonconvex quadratic minimization.
\newblock \emph{SIAM Review}, 62\penalty0 (2):\penalty0 395--436, 2020.

\bibitem[Carmon et~al.(2020)Carmon, Duchi, Hinder, and Sidford]{carmon2020lower}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points {I}.
\newblock \emph{Mathematical Programming}, 184\penalty0 (1):\penalty0 71--120, 2020.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives.
\newblock In \emph{NIPS}, 2014.

\bibitem[Diaz-Chito et~al.(2016)Diaz-Chito, Hern{\'a}ndez-Sabat{\'e}, and L{\'o}pez]{diaz2016reduced}
Katerine Diaz-Chito, Aura Hern{\'a}ndez-Sabat{\'e}, and Antonio~M. L{\'o}pez.
\newblock A reduced feature set for driver head pose estimation.
\newblock \emph{Applied Soft Computing}, 45:\penalty0 98--107, 2016.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock {SPIDER}: Near-optimal non-convex optimization via stochastic path-integrated differential estimator.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Fatkhullin and Polyak(2021)]{fatkhullin2021optimizing}
Ilyas Fatkhullin and Boris Polyak.
\newblock Optimizing static linear feedback: Gradient method.
\newblock \emph{SIAM Journal on Control and Optimization}, 59\penalty0 (5):\penalty0 3887--3911, 2021.

\bibitem[Fatkhullin et~al.(2022)Fatkhullin, Etesami, He, and Kiyavash]{fatkhullin2022sharp}
Ilyas Fatkhullin, Jalal Etesami, Niao He, and Negar Kiyavash.
\newblock Sharp analysis of stochastic optimization under global {K}urdyka-{{\L}}ojasiewicz inequality.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for the linear quadratic regulator.
\newblock In \emph{ICML}, 2018.

\bibitem[Hardt and Ma(2016)]{hardt2016identity}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock In \emph{ICLR}, 2016.

\bibitem[Hendrikx et~al.(2021)Hendrikx, Bach, and Massoulie]{hendrikx2021optimal}
Hadrien Hendrikx, Francis Bach, and Laurent Massoulie.
\newblock An optimal algorithm for decentralized finite-sum optimization.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (4):\penalty0 2753--2783, 2021.

\bibitem[Jiang and Li(2022)]{jiang2022holderian}
Rujun Jiang and Xudong Li.
\newblock H{\"o}lderian error bounds and {K}urdyka-{{\L}}ojasiewicz inequality for the trust region subproblem.
\newblock \emph{Mathematics of Operations Research}, 47\penalty0 (4):\penalty0 3025--3050, 2022.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance reduction.
\newblock In \emph{NIPS}, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under the {P}olyak--{{\L}}ojasiewicz condition.
\newblock In \emph{ECML/PKDD}, 2016.

\bibitem[Kovalev et~al.(2020{\natexlab{a}})Kovalev, Horv{\'a}th, and Richt{\'a}rik]{kovalev2020don}
Dmitry Kovalev, Samuel Horv{\'a}th, and Peter Richt{\'a}rik.
\newblock Donâ€™t jump through hoops and remove those loops: {SVRG} and {K}atyusha are better without the outer loop.
\newblock In \emph{ALT}, 2020{\natexlab{a}}.

\bibitem[Kovalev et~al.(2020{\natexlab{b}})Kovalev, Salim, and Richt{\'a}rik]{kovalev2020optimal}
Dmitry Kovalev, Adil Salim, and Peter Richt{\'a}rik.
\newblock Optimal and practical algorithms for smooth and strongly convex decentralized optimization.
\newblock \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I. Jordan.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{NIPS}, 2017.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, and Chi]{li2022destress}
Boyue Li, Zhize Li, and Yuejie Chi.
\newblock {DESTRESS}: Computation-optimal and communication-efficient decentralized nonconvex finite-sum optimization.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0 (3):\penalty0 1031--1051, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Lin, and Fang]{li2022variance}
Huan Li, Zhouchen Lin, and Yongchun Fang.
\newblock Variance reduced {EXTRA} and {DIG}ing and their optimal acceleration for strongly convex decentralized optimization.
\newblock \emph{Journal of Machine Learning Research}, 23:\penalty0 1--41, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pages 2--47. PMLR, 2018.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{li2021page}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt{\'a}rik.
\newblock {PAGE}: A simple and optimal probabilistic gradient estimator for nonconvex optimization.
\newblock In \emph{ICML}, 2021.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0 85--116, 2022.

\bibitem[{{\L}}ojasiewicz(1963)]{lojasiewicz1963topological}
Stanislaw {{\L}}ojasiewicz.
\newblock A topological property of real analytic subsets.
\newblock \emph{Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles}, 117\penalty0 (87-89):\penalty0 2, 1963.

\bibitem[Lu and De~Sa(2021)]{lu2021optimal}
Yucheng Lu and Christopher De~Sa.
\newblock Optimal complexity in decentralized training.
\newblock In \emph{ICML}, 2021.

\bibitem[Luo and Ye(2022)]{luo2022optimal}
Luo Luo and Haishan Ye.
\newblock An optimal stochastic algorithm for decentralized nonconvex finite-sum optimization.
\newblock \emph{arXiv preprint arXiv:2210.13931}, 2022.

\bibitem[Maranjyan et~al.(2022)Maranjyan, Safaryan, and Richt{\'a}rik]{maranjyan2022gradskip}
Artavazd Maranjyan, Mher Safaryan, and Peter Richt{\'a}rik.
\newblock {GradSkip}: Communication-accelerated local gradient methods with better computational complexity.
\newblock \emph{arXiv preprint arXiv:2210.16402}, 2022.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pages 6820--6829. PMLR, 2020.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and Richt{\'a}rik]{mishchenko2022proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richt{\'a}rik.
\newblock Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally!
\newblock In \emph{ICML}, 2022.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0 (1):\penalty0 48--61, 2009.

\bibitem[Nemirovskij and Yudin(1983)]{nemirovskij1983problem}
Arkadij~Semenovi{\v{c}} Nemirovskij and David~Borisovich Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley-Interscience, 1983.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Yurii Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using stochastic recursive gradient.
\newblock In \emph{ICML}, 2017.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, and Tran-Dinh]{pham2020proxsarah}
Nhan~H. Pham, Lam~M. Nguyen, Dzung~T. Phan, and Quoc Tran-Dinh.
\newblock {ProxSARAH}: An efficient algorithmic framework for stochastic composite nonconvex optimization.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (110):\penalty0 1--48, 2020.

\bibitem[Polyak(1963)]{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki}, 3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Qian et~al.(2021)Qian, Qu, and Richt{\'a}rik]{qian2021svrg}
Xun Qian, Zheng Qu, and Peter Richt{\'a}rik.
\newblock {L}-{SVRG} and {L}-{Katyusha} with arbitrary sampling.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 4991--5039, 2021.

\bibitem[Qu and Li(2017)]{qu2017harnessing}
Guannan Qu and Na~Li.
\newblock Harnessing smoothness to accelerate distributed optimization.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 5\penalty0 (3):\penalty0 1245--1260, 2017.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and Smola]{reddi2016stochastic}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{ICML}, 2016.

\bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{roux2012stochastic}
Nicolas Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for finite training sets.
\newblock In \emph{NIPS}, 2012.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and Massouli{\'e}]{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed optimization in networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0 83--112, 2017.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock {EXTRA}: An exact first-order algorithm for decentralized consensus optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0 944--966, 2015.

\bibitem[Song et~al.(2023)Song, Shi, Pu, and Yan]{song2023optimal}
Zhuoqing Song, Lei Shi, Shi Pu, and Ming Yan.
\newblock Optimal gradient tracking for decentralized optimization.
\newblock \emph{Mathematical Programming}, pages 1--53, 2023.

\bibitem[Sun et~al.(2020)Sun, Lu, and Hong]{sun2020improving}
Haoran Sun, Songtao Lu, and Mingyi Hong.
\newblock Improving the sample and communication complexity for decentralized non-convex optimization: Joint gradient estimation and tracking.
\newblock In \emph{ICML}, 2020.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and Tarokh]{wang2019spiderboost}
Zhe Wang, Kaiyi Ji, Yi~Zhou, Yingbin Liang, and Vahid Tarokh.
\newblock {SpiderBoost} and momentum: Faster variance reduction algorithms.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Woodworth and Srebro(2016)]{woodworth2016tight}
Blake~E Woodworth and Nati Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock In \emph{NIPS}, 2016.

\bibitem[Xin et~al.(2022)Xin, Khan, and Kar]{xin2022fast}
Ran Xin, Usman~A. Khan, and Soummya Kar.
\newblock Fast decentralized nonconvex finite-sum optimization with recursive variance reduction.
\newblock \emph{SIAM Journal on Optimization}, 32\penalty0 (1):\penalty0 1--28, 2022.

\bibitem[Ye et~al.(2023)Ye, Luo, Zhou, and Zhang]{ye2023multi}
Haishan Ye, Luo Luo, Ziang Zhou, and Tong Zhang.
\newblock Multi-consensus decentralized accelerated gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (306):\penalty0 1--50, 2023.

\bibitem[Yuan et~al.(2022{\natexlab{a}})Yuan, Huang, Chen, Zhang, Zhang, and Pan]{yuan2022revisiting}
Kun Yuan, Xinmeng Huang, Yiming Chen, Xiaohan Zhang, Yingya Zhang, and Pan Pan.
\newblock Revisiting optimal convergence rate for smooth and non-convex stochastic decentralized optimization.
\newblock In \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Yuan et~al.(2022{\natexlab{b}})Yuan, Gower, and Lazaric]{yuan2022general}
Rui Yuan, Robert~M Gower, and Alessandro Lazaric.
\newblock A general sample complexity analysis of vanilla policy gradient.
\newblock In \emph{AISTATS}, 2022{\natexlab{b}}.

\bibitem[Yue et~al.(2023)Yue, Fang, and Lin]{yue2023lower}
Pengyun Yue, Cong Fang, and Zhouchen Lin.
\newblock On the lower bound of minimizing {P}olyak-{{\L}}ojasiewicz functions.
\newblock In \emph{COLT}, 2023.

\bibitem[Zeng et~al.(2018)Zeng, Ouyang, Lau, Lin, and Yao]{zeng2018global}
Jinshan Zeng, Shikang Ouyang, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao.
\newblock Global convergence in deep learning with variable splitting via the {K}urdyka-{{\L}}ojasiewicz property.
\newblock \emph{arXiv preprint arXiv:1803.00225}, 9, 2018.

\bibitem[Zhan et~al.(2022)Zhan, Wu, and Gao]{zhan2022efficient}
Wenkang Zhan, Gang Wu, and Hongchang Gao.
\newblock Efficient decentralized stochastic gradient descent method for nonconvex finite-sum optimization problems.
\newblock In \emph{AAAI}, 2022.

\bibitem[Zhang et~al.(2013)Zhang, Mahdavi, and Jin]{zhang2013linear}
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin.
\newblock Linear convergence with condition number independent access of full gradients.
\newblock In \emph{NIPS}, 2013.

\bibitem[Zhou and Gu(2019)]{zhou2019lower}
Dongruo Zhou and Quanquan Gu.
\newblock Lower bounds for smooth nonconvex finite-sum optimization.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhou et~al.(2019)Zhou, Yuan, and Feng]{zhou2019faster}
Pan Zhou, Xiao-Tong Yuan, and Jiashi Feng.
\newblock Faster first-order methods for stochastic non-convex optimization on {Riemannian} manifolds.
\newblock In \emph{AISTATS}, 2019.

\bibitem[Zhou et~al.(2018)Zhou, Wang, and Liang]{zhou2018convergence}
Yi~Zhou, Zhe Wang, and Yingbin Liang.
\newblock Convergence of cubic regularization for nonconvex optimization under {KL} property.
\newblock \emph{NeurIPS}, 2018.

\end{thebibliography}
