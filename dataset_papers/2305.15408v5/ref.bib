@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{
dehghani2019universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
pérez2019on,
title={On the Turing Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barceló},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
yun2020are,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{bhattamishra2020computational,
  title={On the Computational Power of Transformers and Its Implications in Sequence Modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  booktitle={Proceedings of the 24th Conference on Computational Natural Language Learning},
  pages={455--475},
  year={2020}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{bhattamishra2020ability,
  title={On the Ability and Limitations of Transformers to Recognize Formal Languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7096--7116},
  year={2020}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}

@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@inproceedings{
muller2022transformers,
title={Transformers Can Do Bayesian Inference},
author={Samuel M{\"u}ller and Noah Hollmann and Sebastian Pineda Arango and Josif Grabocka and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{
xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{nguyen2022transformer,
  title={Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling},
  author={Nguyen, Tung and Grover, Aditya},
  booktitle={International Conference on Machine Learning},
  pages={16569--16594},
  year={2022},
  organization={PMLR}
}

@inproceedings{min2022rethinking,
    title = "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    pages = "11048--11064",
}

@inproceedings{
akyurek2023what,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@inproceedings{
garg2022what,
title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{von2022transformers,
  title={Transformers learn in-context by gradient descent},
  author={von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  journal={arXiv preprint arXiv:2212.07677},
  year={2022}
}

@inproceedings{
chan2022data,
title={Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
author={Stephanie C.Y. Chan and Adam Santoro and Andrew Kyle Lampinen and Jane X Wang and Aaditya K Singh and Pierre Harvey Richemond and James McClelland and Felix Hill},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@article{
wei2022emergent,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
}

@article{dai2022can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@inproceedings{
nye2022show,
title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
booktitle={Deep Learning for Code Workshop},
year={2022},
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@inproceedings{wang2022iteratively,
  title={Iteratively prompt pre-trained language models for chain of thought},
  author={Wang, Boshi and Deng, Xiang and Sun, Huan},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={2714--2730},
  year={2022}
}

@inproceedings{
zhang2023automatic,
title={Automatic Chain of Thought Prompting in Large Language Models},
author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@inproceedings{
liu2023transformers,
title={Transformers Learn Shortcuts to Automata},
author={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{hahn2023theory,
  title={A Theory of Emergent In-Context Learning as Implicit Structure Induction},
  author={Hahn, Michael and Goyal, Navin},
  journal={arXiv preprint arXiv:2303.07971},
  year={2023}
}

@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@article{prystawski2023think,
  title={Why think step-by-step? Reasoning emerges from the locality of experience},
  author={Prystawski, Ben and Goodman, Noah D},
  journal={arXiv preprint arXiv:2304.03843},
  year={2023}
}

@article{li2023transformers,
  title={Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author={Li, Yingcong and Ildiz, M Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  journal={arXiv preprint arXiv:2301.07067},
  year={2023}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@inproceedings{
goel2022recurrent,
title={Recurrent Convolutional Neural Networks Learn Succinct Learning Algorithms},
author={Surbhi Goel and Sham M. Kakade and Adam Tauman Kalai and Cyril Zhang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=BCBac5kkg5G}
}

@article{merrill2022saturated,
  title={Saturated transformers are constant-depth threshold circuits},
  author={Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={843--856},
  year={2022},
  publisher={MIT Press}
}

@article{merrill2023parallelism,
  title={The Parallelism Tradeoff: Limitations of Log-Precision Transformers},
  author={Merrill, William and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023}
}

@article{hao2022formal,
  title={Formal language recognition by hard attention transformers: Perspectives from circuit complexity},
  author={Hao, Yiding and Angluin, Dana and Frank, Robert},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={800--810},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186"
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{
kojima2022large,
title={Large Language Models are Zero-Shot Reasoners},
author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@article{jiang2020can,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with {GPT}-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
      year={2023},
      journal={arXiv preprint arXiv:2305.20050},
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{bellman1954theory,
  title={The theory of dynamic programming},
  author={Bellman, Richard},
  journal={Bulletin of the American Mathematical Society},
  volume={60},
  number={6},
  pages={503--515},
  year={1954}
}

@book{cormen2022introduction,
  title={Introduction to algorithms},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  year={2022},
  publisher={MIT press}
}

@inproceedings{barrington1986bounded,
  title={Bounded-width polynomial-size branching programs recognize exactly those languages in NC},
  author={Barrington, David A},
  booktitle={Proceedings of the eighteenth annual ACM symposium on Theory of computing},
  pages={1--5},
  year={1986}
}
@article{barrington1988finite,
  title={Finite monoids and the fine structure of NC},
  author={Barrington, David A Mix and Therien, Denis},
  journal={Journal of the ACM (JACM)},
  volume={35},
  number={4},
  pages={941--952},
  year={1988},
  publisher={ACM New York, NY, USA}
}

@inproceedings{buss1987boolean,
  title={The Boolean formula value problem is in ALOGTIME},
  author={Buss, Samuel R},
  booktitle={Proceedings of the nineteenth annual ACM symposium on Theory of computing},
  pages={123--131},
  year={1987}
}

@inproceedings{jones1974complete,
  title={Complete problems for deterministic polynomial time},
  author={Jones, Neil D and Laaser, William T},
  booktitle={Proceedings of the sixth annual ACM symposium on Theory of computing},
  pages={40--46},
  year={1974}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{leshno1993multilayer,
  title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  journal={Neural networks},
  volume={6},
  number={6},
  pages={861--867},
  year={1993},
  publisher={Elsevier}
}

@article{loshchilov2017fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2017}
}

@inproceedings{yun2020n,
  title={O (n) connections are expressive enough: Universal approximability of sparse transformers},
  author={Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13783--13794},
  year={2020}
}

@article{jiang2023brief,
  title={A Brief Survey on the Approximation Theory for Sequence Modelling},
  author={Jiang, Haotian and Li, Qianxiao and Li, Zhong and Wang, Shida},
  journal={arXiv preprint arXiv:2302.13752},
  year={2023}
}

@inproceedings{
luo2022your,
title={Your Transformer May Not be as Powerful as You Expect},
author={Shengjie Luo and Shanda Li and Shuxin Zheng and Tie-Yan Liu and Liwei Wang and Di He},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@article{perez2021attention,
  title={Attention is turing complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={3463--3497},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{wei2022statistically,
  title={Statistically meaningful approximation: a case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022}
}

@inproceedings{sakai1961syntax,
  title={Syntax in universal translation},
  author={Sakai, Itiroo},
  booktitle={Proceedings of the International Conference on Machine Translation and Applied Language Analysis},
  year={1961}
}

@article{sipser1996introduction,
  title={Introduction to the Theory of Computation},
  author={Sipser, Michael},
  journal={ACM Sigact News},
  volume={27},
  number={1},
  pages={27--29},
  year={1996}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@inproceedings{
press2022train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@article{ieee754,
  title={IEEE Standard for Floating-Point Arithmetic},
  author={{IEEE Computer Society}},
  journal={{IEEE Std 754-2019}},
  year={2019},
  doi={10.1109/IEEESTD.2019.8766229},
  publisher={{IEEE}},
  url={https://ieeexplore.ieee.org/document/8766229}
}

@inproceedings{
wies2023subtask,
title={Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks},
author={Noam Wies and Yoav Levine and Amnon Shashua},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=BrJATVZDWEH}
}

@inproceedings{yao1989circuits,
  title={Circuits and local computation},
  author={Yao, Andrew C},
  booktitle={Proceedings of the twenty-first annual ACM symposium on Theory of computing},
  pages={186--196},
  year={1989}
}

@article{chiu2001division,
  title={Division in logspace-uniform NC1},
  author={Chiu, Andrew and Davida, George and Litow, Bruce},
  journal={RAIRO-Theoretical Informatics and Applications},
  volume={35},
  number={3},
  pages={259--275},
  year={2001},
  publisher={EDP Sciences}
}

@inproceedings{hesse2001division,
  title={Division is in uniform TC0},
  author={Hesse, William},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={104--114},
  year={2001},
  organization={Springer}
}

@inproceedings{yao2021self,
  title={Self-Attention Networks Can Process Bounded Hierarchical Languages},
  author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3770--3785},
  year={2021}
}

@inproceedings{hewitt2020rnns,
  title={RNNs can generate bounded hierarchical languages with optimal memory},
  author={Hewitt, John and Hahn, Michael and Ganguli, Surya and Liang, Percy and Manning, Christopher D},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1978--2010},
  year={2020}
}


@inproceedings{david2023tighter,
  title={Tighter Bounds on the Expressivity of Transformer Encoders},
  author={Chiang, David and Cholak, Peter and Pillay, Anand},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={5544--5562},
  year={2023}
}

@article{bai2023transformers,
  title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={arXiv preprint arXiv:2306.04637},
  year={2023}
}