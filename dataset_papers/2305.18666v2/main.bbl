\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beck(2017)]{beck2017first}
Amir Beck.
\newblock \emph{First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem[Borsos et~al.(2020)Borsos, Mutny, and Krause]{borsos2020coresets}
Zal{\'a}n Borsos, Mojmir Mutny, and Andreas Krause.
\newblock Coresets via bilevel optimization for continual learning and streaming.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14879--14890, 2020.

\bibitem[Chang and Lin(2011)]{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: a library for support vector machines.
\newblock \emph{ACM transactions on intelligent systems and technology (TIST)}, 2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021tighter}
Tianyi Chen, Yuejiao Sun, and Wotao Yin.
\newblock Tighter analysis of alternating stochastic gradient method for stochastic nested problems, 2021.

\bibitem[Chen et~al.(2022)Chen, Sun, Xiao, and Yin]{chen2022singletimescale}
Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin.
\newblock A single-timescale method for stochastic bilevel optimization, 2022.

\bibitem[Cutkosky and Orabona(2020)]{cutkosky2020momentumbased}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd, 2020.

\bibitem[Dagréou et~al.(2022)Dagréou, Ablin, Vaiter, and Moreau]{dagréou2022framework}
Mathieu Dagréou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau.
\newblock A framework for bilevel optimization that enables stochastic and global variance reduction algorithms, 2022.

\bibitem[Dagréou et~al.(2023)Dagréou, Moreau, Vaiter, and Ablin]{dagréou2023lower}
Mathieu Dagréou, Thomas Moreau, Samuel Vaiter, and Pierre Ablin.
\newblock A lower bound and a near-optimal algorithm for bilevel empirical risk minimization, 2023.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for non-strongly convex composite objectives, 2014.

\bibitem[Falk and Liu(1995)]{falk1995bilevel}
James~E Falk and Jiming Liu.
\newblock On bilevel programming, part i: general nonlinear cases.
\newblock \emph{Mathematical Programming}, 70:\penalty0 47--72, 1995.

\bibitem[Fan et~al.(2021)Fan, Ram, and Liu]{fan2021signmaml}
Chen Fan, Parikshit Ram, and Sijia Liu.
\newblock Sign-maml: Efficient model-agnostic meta-learning by signsgd, 2021.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017modelagnostic}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks, 2017.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and Pontil]{franceschi2017forward}
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil.
\newblock Forward and reverse gradient-based hyperparameter optimization, 2017.

\bibitem[Ghadimi and Wang(2018)]{ghadimi2018approximation}
Saeed Ghadimi and Mengdi Wang.
\newblock Approximation methods for bilevel programming, 2018.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and Richt{\'a}rik]{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richt{\'a}rik.
\newblock Sgd: General analysis and improved rates.
\newblock In \emph{International conference on machine learning}, pages 5200--5209. PMLR, 2019.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and Salzo]{grazzi2020iteration}
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo.
\newblock On the iteration complexity of hypergradient computation, 2020.

\bibitem[Grazzi et~al.(2021)Grazzi, Pontil, and Salzo]{grazzi2021convergence}
Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo.
\newblock Convergence properties of stochastic hypergradients, 2021.

\bibitem[Hong et~al.(2022)Hong, Wai, Wang, and Yang]{hong2022twotimescale}
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
\newblock A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic, 2022.

\bibitem[Huang et~al.(2023)Huang, Li, and Gao]{huang2023biadam}
Feihu Huang, Junyi Li, and Shangqian Gao.
\newblock Biadam: Fast adaptive bilevel optimization methods, 2023.

\bibitem[Ishizuka and Aiyoshi(1992)]{ishizuka1992double}
Yo~Ishizuka and Eitaro Aiyoshi.
\newblock Double penalty method for bilevel optimization problems.
\newblock \emph{Annals of Operations Research}, 34\penalty0 (1):\penalty0 73--88, 1992.

\bibitem[Ji et~al.(2021)Ji, Yang, and Liang]{ji2021bilevel}
Kaiyi Ji, Junjie Yang, and Yingbin Liang.
\newblock Bilevel optimization: Convergence analysis and enhanced design, 2021.

\bibitem[Ji et~al.(2022)Ji, Liu, Liang, and Ying]{ji2022bilevel}
Kaiyi Ji, Mingrui Liu, Yingbin Liang, and Lei Ying.
\newblock Will bilevel optimizers benefit from loops, 2022.

\bibitem[Khanduri et~al.(2021)Khanduri, Zeng, Hong, Wai, Wang, and Yang]{khanduri2021nearoptimal}
Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
\newblock A near-optimal algorithm for stochastic bilevel optimization via double-momentum, 2021.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Li and Orabona(2019)]{li2019convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive stepsizes, 2019.

\bibitem[Liu et~al.(2019)Liu, Simonyan, and Yang]{liu2019darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search, 2019.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and Lacoste-Julien]{loizou2021stochastic}
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien.
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence, 2021.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and Duvenaud]{lorraine2020optimizing}
Jonathan Lorraine, Paul Vicol, and David Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1540--1552. PMLR, 2020.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2019decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate, 2019.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0 1574--1609, 2009.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course}, volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic recursive gradient.
\newblock In \emph{International Conference on Machine Learning}, pages 2613--2621. PMLR, 2017.

\bibitem[Orvieto et~al.(2022)Orvieto, Lacoste-Julien, and Loizou]{orvieto2022dynamics}
Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou.
\newblock Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution, 2022.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and Levine]{rajeswaran2019metalearning}
Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients, 2019.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Ren et~al.(2021)Ren, Xiao, Chang, Huang, Li, Chen, and Wang]{ren2021comprehensive}
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang.
\newblock A comprehensive survey of neural architecture search: Challenges and solutions.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (4):\penalty0 1--34, 2021.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and Boots]{shaban2019truncated}
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pages 1723--1732. PMLR, 2019.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and Srebro]{shalev2007pegasos}
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock In \emph{Proceedings of the 24th international conference on Machine learning}, pages 807--814, 2007.

\bibitem[Sow et~al.(2022)Sow, Ji, and Liang]{sow2022convergence}
Daouda Sow, Kaiyi Ji, and Yingbin Liang.
\newblock On the convergence theory for hessian-free bilevel algorithms, 2022.

\bibitem[Vaswani et~al.(2020)Vaswani, Laradji, Kunstner, Meng, Schmidt, and Lacoste-Julien]{vaswani2020adaptive}
Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si~Yi Meng, Mark Schmidt, and Simon Lacoste-Julien.
\newblock Adaptive gradient methods converge faster with over-parameterization (but you should do a line-search).
\newblock \emph{arXiv preprint arXiv:2006.06835}, 2020.

\bibitem[Vaswani et~al.(2021)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and Lacoste-Julien]{vaswani2021painless}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and convergence rates, 2021.

\bibitem[Vicente et~al.(1994)Vicente, Savard, and J{\'u}dice]{vicente1994descent}
Luis Vicente, Gilles Savard, and Joaquim J{\'u}dice.
\newblock Descent approaches for quadratic bilevel programming.
\newblock \emph{Journal of optimization theory and applications}, 81\penalty0 (2):\penalty0 379--399, 1994.

\bibitem[Wang et~al.(2020)Wang, Zhu, Torralba, and Efros]{wang2020dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A. Efros.
\newblock Dataset distillation, 2020.

\bibitem[Ward et~al.(2021)Ward, Wu, and Bottou]{ward2021adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, 2021.

\bibitem[Wright(2006)]{wright2006numerical}
Jorge Nocedal Stephen~J Wright.
\newblock Numerical optimization, 2006.

\bibitem[Yang et~al.(2021)Yang, Ji, and Liang]{yang2021provably}
Junjie Yang, Kaiyi Ji, and Yingbin Liang.
\newblock Provably faster algorithms for bilevel optimization, 2021.

\bibitem[Yu et~al.(2023)Yu, Liu, and Wang]{yu2023dataset}
Ruonan Yu, Songhua Liu, and Xinchao Wang.
\newblock Dataset distillation: A comprehensive review, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Khanduri, Hong, Chang, and Liu]{zhang2022revisiting}
Yihua Zhang, Guanhua Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, and Sijia Liu.
\newblock Revisiting and advancing fast adversarial training through the lens of bi-level optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 26693--26712. PMLR, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Yao, Ram, Zhao, Chen, Hong, Wang, and Liu]{zhang2023advancing}
Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu~Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu.
\newblock Advancing model pruning via bi-level optimization, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Nezhadarya, and Ba]{zhou2022dataset}
Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba.
\newblock Dataset distillation using neural feature regression.
\newblock \emph{arXiv preprint arXiv:2206.00719}, 2022.

\end{thebibliography}
