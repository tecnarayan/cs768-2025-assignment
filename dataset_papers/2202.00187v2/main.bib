@article{rubing2021does,
  title={Does the Data Induce Capacity Control in Deep Learning?},
  author={Yang, Rubing and Mao, Jialin and Chaudhari, Pratik},
  journal={arXiv preprint arXiv:2110.14163},
  year={2021}
}

@article{kim2021selfmatch,
  title={SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning},
  author={Kim, Byoungjip and Choo, Jinho and Kwon, Yeong-Dae and Joe, Seongho and Min, Seungjai and Gwon, Youngjune},
  journal={arXiv preprint arXiv:2101.06480},
  year={2021}
}

@inproceedings{xu2021dash,
  title={Dash: Semi-supervised learning with dynamic thresholding},
  author={Xu, Yi and Shang, Lei and Ye, Jinxing and Qian, Qi and Li, Yu-Feng and Sun, Baigui and Li, Hao and Jin, Rong},
  booktitle={International Conference on Machine Learning},
  pages={11525--11536},
  year={2021},
  organization={PMLR}
}

@article {Quinn13762,
  author = {Quinn, Katherine N. and Clement, Colin B. and De Bernardis, Francesco and Niemack, Michael D. and Sethna, James P.},
  title = {Visualizing probabilistic models and data with Intensive Principal Component Analysis},
  volume = {116},
  number = {28},
  pages = {13762--13767},
  year = {2019},
  doi = {10.1073/pnas.1817218116},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  URL = {https://www.pnas.org/content/116/28/13762},
  eprint = {https://www.pnas.org/content/116/28/13762.full.pdf},
  journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{rameshModelZooGrowing2022,
  title = {Model {{Zoo}}: {{A Growing}} "{{Brain}}" {{That Learns Continually}}},
  booktitle = {Proc. of {{International Conference}} of {{Learning}} and {{Representations}} ({{ICLR}})},
  author = {Ramesh, Rahul and Chaudhari, Pratik},
  year = {2022},
  eprint = {2106.03027},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@phdthesis{zhang1994discrete,
  title={Discrete noninformative priors},
  author={Zhang, Zhongxin},
  year={1994},
  school={Yale University}
}

@Article{	  abbottscalinglawdiscrete2019,
  title		= {A {{Scaling Law From Discrete}} to {{Continuous
		  Solutions}} of {{Channel Capacity Problems}} in the
		  {{Low}}-{{Noise Limit}}},
  author	= {Abbott, Michael C. and Machta, Benjamin B.},
  year		= {2019},
  month		= jul,
  journal	= {Journal of Statistical Physics},
  volume	= {176},
  number	= {1},
  pages		= {214--227},
  issn		= {1572-9613},
  doi		= {10.1007/s10955-019-02296-2},
  abstract	= {An analog communication channel typically achieves its
		  full capacity when the distribution of inputs is discrete,
		  composed of just K symbols, such as voltage levels or
		  wavelengths. As the effective noise level goes to zero, for
		  example by sending the same message multiple times, it is
		  known that optimal codes become continuous. Here we derive
		  a scaling law for the optimal number of symbols in this
		  limit, finding a novel rational scaling exponent. The
		  number of symbols in the optimal code grows as
		  \$\$\textbackslash log K\textbackslash sim
		  I\^\{4/3\}\$\$logK∼I4/3, where the channel capacity I
		  increases with decreasing noise. The same scaling applies
		  to other problems equivalent to maximizing channel capacity
		  over a continuous distribution.},
  language	= {en}
}

@InProceedings{	  alemi2020variational,
  title		= {Variational Predictive Information Bottleneck},
  booktitle	= {Symposium on Advances in Approximate Bayesian Inference},
  author	= {Alemi, Alexander A},
  year		= {2020},
  pages		= {1--6},
  organization	= {{PMLR}}
}

@Book{		  amariinformationgeometryits2016,
  title		= {Information {{Geometry}} and {{Its Applications}}},
  author	= {Amari, Shun-ichi},
  year		= {2016},
  series	= {Applied {{Mathematical Sciences}}},
  volume	= {194},
  publisher	= {{Springer Japan}},
  address	= {{Tokyo}},
  language	= {en},
  file		= {/Users/prtic/Dropbox/zotero/storage/RJ5L6GUX/[Shun-ichi_Amari_(auth.)]_Information_Geometry_and(b-ok.org).pdf}
}

@Article{	  arimoto1972algorithm,
  title		= {An Algorithm for Computing the Capacity of Arbitrary
		  Discrete Memoryless Channels},
  author	= {Arimoto, Suguru},
  year		= {1972},
  journal	= {IEEE Transactions on Information Theory},
  volume	= {18},
  number	= {1},
  pages		= {14--20},
  publisher	= {{IEEE}}
}

@Book{		  berger1988priors,
  title		= {On Priors That Maximize Expected Information},
  author	= {Berger, James O and Bernardo, Jos M and Mendoza, Manuel},
  year		= {1988},
  publisher	= {{Purdue University. Department of Statistics}},
  file		= {/Users/pratik/Dropbox/zotero/berger_et_al_1988_on_priors_that_maximize_expected_information.pdf}
}

@Article{	  berger2009formal,
  title		= {The Formal Definition of Reference Priors},
  author	= {Berger, James O and Bernardo, José M and Sun, Dongchu},
  year		= {2009},
  journal	= {The Annals of Statistics},
  volume	= {37},
  number	= {2},
  pages		= {905--938},
  publisher	= {{Institute of Mathematical Statistics}}
}

@Article{	  bernardo1979reference,
  title		= {Reference Posterior Distributions for {{Bayesian}}
		  Inference},
  author	= {Bernardo, Jose M},
  year		= {1979},
  journal	= {Journal of the Royal Statistical Society: Series B
		  (Methodological)},
  volume	= {41},
  number	= {2},
  pages		= {113--128},
  publisher	= {{Wiley Online Library}},
  file		= {/Users/pratik/Dropbox/zotero/bernardo_1979_reference_posterior_distributions_for_bayesian_inference.pdf}
}

@Article{	  berthelot2019mixmatch,
  title		= {{{MixMatch}}: {{A}} Holistic Approach to Semi-Supervised
		  Learning},
  author	= {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian
		  and Papernot, Nicolas and Oliver, Avital and Raffel, Colin
		  A},
  year		= {2019},
  journal	= {Advances in Neural Information Processing Systems},
  volume	= {32}
}

@Article{	  bialek2001predictability,
  title		= {Predictability, Complexity, and Learning},
  author	= {Bialek, William and Nemenman, Ilya and Tishby, Naftali},
  year		= {2001},
  journal	= {Neural computation},
  volume	= {13},
  number	= {11},
  pages		= {2409--2463},
  publisher	= {{MIT Press}}
}

@Article{	  blahut1972computation,
  title		= {Computation of Channel Capacity and Rate-Distortion
		  Functions},
  author	= {Blahut, Richard},
  year		= {1972},
  journal	= {IEEE transactions on Information Theory},
  volume	= {18},
  number	= {4},
  pages		= {460--473},
  publisher	= {{IEEE}}
}

@InProceedings{	  bousquet2003introduction,
  title		= {Introduction to Statistical Learning Theory},
  booktitle	= {Summer School on Machine Learning},
  author	= {Bousquet, Olivier and Boucheron, Stéphane and Lugosi,
		  Gábor},
  year		= {2003},
  pages		= {169--207},
  organization	= {{Springer}},
  file		= {/Users/pratik/Dropbox/zotero/bousquet_et_al_2003_introduction_to_statistical_learning_theory.pdf}
}

@Article{	  chen2020big,
  title		= {Big Self-Supervised Models Are Strong Semi-Supervised
		  Learners},
  author	= {Chen, Ting and Kornblith, Simon and Swersky, Kevin and
		  Norouzi, Mohammad and Hinton, Geoffrey E},
  year		= {2020},
  journal	= {Advances in Neural Information Processing Systems},
  volume	= {33},
  pages		= {22243--22255}
}

@Article{	  clarke1994jeffreys,
  title		= {Jeffreys' Prior Is Asymptotically Least Favorable under
		  Entropy Risk},
  author	= {Clarke, Bertrand S and Barron, Andrew R},
  year		= {1994},
  journal	= {Journal of Statistical planning and Inference},
  volume	= {41},
  number	= {1},
  pages		= {37--60},
  publisher	= {{Elsevier}}
}

@Article{	  davatzikos2019machine,
  title		= {Machine Learning in Neuroimaging: {{Progress}} and
		  Challenges},
  author	= {Davatzikos, Christos},
  year		= {2019},
  journal	= {NeuroImage},
  volume	= {197},
  pages		= {652},
  publisher	= {{NIH Public Access}}
}

@InProceedings{	  devlin2019bert,
  ids		= {47751},
  title		= {{{BERT}}: {{Pre}}-Training of Deep Bidirectional
		  Transformers for Language Understanding},
  booktitle	= {{{NAACL HLT}}},
  author	= {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
		  Toutanova, Kristina},
  year		= {2019}
}

@InProceedings{	  dhillon2019a,
  title		= {A Baseline for Few-Shot Image Classification},
  booktitle	= {Proc. of {{International Conference}} of {{Learning}} and
		  {{Representations}} ({{ICLR}})},
  author	= {Dhillon, Guneet S and Chaudhari, Pratik and Ravichandran,
		  Avinash and Soatto, Stefano},
  year		= {2020},
  copyright	= {All rights reserved}
}

@InProceedings{	  gao2020free,
  title		= {A Free-Energy Principle for Representation Learning},
  booktitle	= {Proc. of {{International Conference}} of {{Machine
		  Learning}} ({{ICML}})},
  author	= {Gao, Yansong and Chaudhari, Pratik},
  year		= {2020},
  copyright	= {All rights reserved}
}

@InProceedings{	  kingma2014adam,
  title		= {Adam: {{A}} Method for Stochastic Optimization},
  booktitle	= {International Conference for Learning Representations},
  author	= {Kingma, Diederik and Ba, Jimmy},
  year		= {2015}
}

@Article{	  kolesnikovbigtransferbit2020,
  title		= {Big {{Transfer}} ({{BiT}}): {{General Visual
		  Representation Learning}}},
  shorttitle	= {Big {{Transfer}} ({{BiT}})},
  author	= {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua
		  and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain
		  and Houlsby, Neil},
  year		= {2020},
  month		= may,
  journal	= {arXiv:1912.11370 [cs]},
  eprint	= {1912.11370},
  eprinttype	= {arxiv},
  primaryclass	= {cs},
  abstract	= {Transfer of pre-trained representations improves sample
		  efficiency and simplifies hyperparameter tuning when
		  training deep neural networks for vision. We revisit the
		  paradigm of pre-training on large supervised datasets and
		  fine-tuning the model on a target task. We scale up
		  pre-training, and propose a simple recipe that we call Big
		  Transfer (BiT). By combining a few carefully selected
		  components, and transferring using a simple heuristic, we
		  achieve strong performance on over 20 datasets. BiT
		  performs well across a surprisingly wide range of data
		  regimes – from 1 example per class to 1M total examples.
		  BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\%
		  on CIFAR-10, and 76.3\% on the 19 task Visual Task
		  Adaptation Benchmark (VTAB). On small datasets, BiT attains
		  76.8\% on ILSVRC-2012 with 10 examples per class, and
		  97.0\% on CIFAR-10 with 10 examples per class. We conduct
		  detailed analysis of the main components that lead to high
		  transfer performance.},
  archiveprefix	= {arXiv},
  keywords	= {Computer Science - Computer Vision and Pattern
		  Recognition,Computer Science - Machine Learning}
}

@PhDThesis{	  krizhevsky2009learning,
  ids		= {krizhevskyLearningMultipleLayers2009},
  title		= {Learning Multiple Layers of Features from Tiny Images},
  author	= {Krizhevsky, A.},
  year		= {2009},
  school	= {Computer Science, University of Toronto}
}

@Article{	  laine2016temporal,
  title		= {Temporal Ensembling for Semi-Supervised Learning},
  author	= {Laine, Samuli and Aila, Timo},
  year		= {2016},
  journal	= {arXiv:1610.02242},
  eprint	= {1610.02242},
  eprinttype	= {arxiv},
  archiveprefix	= {arXiv}
}

@Article{	  laughlin1981simple,
  title		= {A Simple Coding Procedure Enhances a Neuron's Information
		  Capacity},
  author	= {Laughlin, Simon},
  year		= {1981},
  journal	= {Zeitschrift für Naturforschung c},
  volume	= {36},
  number	= {9-10},
  pages		= {910--912},
  publisher	= {{Verlag der Zeitschrift für Naturforschung}}
}

@Article{	  lienyidivergencevariational2016,
  title		= {R\textbackslash 'enyi {{Divergence Variational
		  Inference}}},
  author	= {Li, Yingzhen and Turner, Richard E.},
  year		= {2016},
  month		= oct,
  journal	= {arXiv:1602.02311 [cs, stat]},
  eprint	= {1602.02311},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  abstract	= {This paper introduces the variational R´enyi bound (VR)
		  that extends traditional variational inference to
		  R´enyi’s α-divergences. This new family of variational
		  methods unifies a number of existing approaches, and
		  enables a smooth interpolation from the evidence
		  lower-bound to the log marginal likelihood that is
		  controlled by the value of α that parametrises the
		  divergence. The reparameterization trick, Monte Carlo
		  approximation and stochastic optimisation methods are
		  deployed to obtain a unified framework for optimisation. We
		  further consider negative α values and propose a novel
		  variational inference method as a new special case in the
		  proposed framework. Experiments on Bayesian neural networks
		  and variational auto-encoders demonstrate the wide
		  applicability of the VR bound.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Computer Science - Machine Learning,Statistics - Machine
		  Learning}
}

@Article{	  mattingly2018maximizing,
  title		= {Maximizing the Information Learned from Finite Data
		  Selects a Simple Model},
  author	= {Mattingly, Henry H and Transtrum, Mark K and Abbott,
		  Michael C and Machta, Benjamin B},
  year		= {2018},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {115},
  number	= {8},
  pages		= {1760--1765},
  publisher	= {{National Acad Sciences}},
  file		= {/Users/prtic/Dropbox/zotero/storage/MXFUTV5W/mattingly_et_al_2018_maximizing_the_information_learned_from_finite_data_selects_a_simple_model2.pdf}
}

@Article{	  mayer2015well,
  title		= {How a Well-Adapted Immune System Is Organized},
  author	= {Mayer, Andreas and Balasubramanian, Vijay and Mora,
		  Thierry and Walczak, Aleksandra M},
  year		= {2015},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {112},
  number	= {19},
  pages		= {5950--5955},
  publisher	= {{National Acad Sciences}}
}

@Article{	  nalisnick2017variational,
  title		= {Variational Reference Priors},
  author	= {Nalisnick, Eric and Smyth, Padhraic},
  year		= {2017}
}

@Article{	  sajjadi2016regularization,
  title		= {Regularization with Stochastic Transformations and
		  Perturbations for Deep Semi-Supervised Learning},
  author	= {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen,
		  Tolga},
  year		= {2016},
  journal	= {Advances in neural information processing systems},
  volume	= {29},
  pages		= {1163--1171}
}

@Article{	  smith1971information,
  title		= {The Information Capacity of Amplitude-and
		  Variance-Constrained Sclar {{Gaussian}} Channels},
  author	= {Smith, Joel G},
  year		= {1971},
  journal	= {Information and control},
  volume	= {18},
  number	= {3},
  pages		= {203--219},
  publisher	= {{Elsevier}}
}

@Article{	  sohn2020fixmatch,
  title		= {{{FixMatch}}: {{Simplifying}} Semi-Supervised Learning
		  with Consistency and Confidence},
  author	= {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas
		  and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and
		  Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  year		= {2020},
  journal	= {Advances in Neural Information Processing Systems},
  volume	= {33}
}

@InProceedings{	  tishbyinformationbottleneckmethod1999,
  ids		= {Tishby1999-tj,tishby2000information,tishbyInformationBottleneckMethod2000},
  title		= {The Information Bottleneck Method},
  booktitle	= {Proc. of the 37-Th Annual Allerton Conference on
		  Communication, Control and Computing},
  author	= {Tishby, Naftali and Pereira, Fernando C. and Bialek,
		  William},
  year		= {1999},
  eprint	= {physics/0004057},
  eprinttype	= {arxiv},
  pages		= {368--377},
  archiveprefix	= {arXiv},
  date-added	= {2019-10-31 02:10:39 -0400},
  date-modified	= {2019-10-31 02:10:39 -0400},
  keywords	= {Computer Science - Machine Learning,Condensed Matter -
		  Disordered Systems and Neural Networks,Nonlinear Sciences -
		  Adaptation and Self-Organizing Systems,Physics - Data
		  Analysis; Statistics and Probability}
}

@Article{	  tkavcik2008information,
  title		= {Information Flow and Optimization in Transcriptional
		  Regulation},
  author	= {Tkačik, Gašper and Callan, Curtis G and Bialek,
		  William},
  year		= {2008},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {105},
  number	= {34},
  pages		= {12265--12270},
  publisher	= {{National Acad Sciences}}
}

@InProceedings{	  wenzel2020good,
  title		= {How Good Is the Bayes Posterior in Deep Neural Networks
		  Really?},
  booktitle	= {International Conference on Machine Learning},
  author	= {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan and
		  Swiatkowski, Jakub and Tran, Linh and Mandt, Stephan and
		  Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and
		  Nowozin, Sebastian},
  year		= {2020},
  pages		= {10248--10259},
  organization	= {{PMLR}},
  file		= {/Users/prtic/Dropbox/zotero/storage/7G2TXWCF/wenzel_et_al_2020_how_good_is_the_bayes_posterior_in_deep_neural_networks_really.pdf}
}


@InProceedings{	  zagoruyko2016wide,
  title		= {Wide Residual Networks},
  booktitle	= {British Machine Vision Conference 2016},
  author	= {Zagoruyko, Sergey and Komodakis, Nikos},
  year		= {2016},
  organization	= {{British Machine Vision Association}}
}

@Article{	  zhangmixupempiricalrisk2018,
  ids		= {zhang2017mixup,zhangMixupEmpiricalRisk2017},
  title		= {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle	= {Mixup},
  author	= {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N.
		  and {Lopez-Paz}, David},
  year		= {2018},
  month		= apr,
  journal	= {arXiv:1710.09412 [cs, stat]},
  eprint	= {1710.09412},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  abstract	= {Large deep neural networks are powerful, but exhibit
		  undesirable behaviors such as memorization and sensitivity
		  to adversarial examples. In this work, we propose mixup, a
		  simple learning principle to alleviate these issues. In
		  essence, mixup trains a neural network on convex
		  combinations of pairs of examples and their labels. By
		  doing so, mixup regularizes the neural network to favor
		  simple linear behavior in-between training examples. Our
		  experiments on the ImageNet-2012, CIFAR-10, CIFAR-100,
		  Google commands and UCI datasets show that mixup improves
		  the generalization of state-of-the-art neural network
		  architectures. We also find that mixup reduces the
		  memorization of corrupt labels, increases the robustness to
		  adversarial examples, and stabilizes the training of
		  generative adversarial networks.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Computer Science - Machine Learning,Statistics - Machine
		  Learning}
}

@Article{	  zhou2010semi,
  title		= {Semi-Supervised Learning by Disagreement},
  author	= {Zhou, Zhi-Hua and Li, Ming},
  year		= {2010},
  journal	= {Knowledge and Information Systems},
  volume	= {24},
  number	= {3},
  pages		= {415--439},
  publisher	= {{Springer}}
}



@Article{	  xiaojin2008semi,
  title		= {Semi-supervised learning literature survey},
  author	= {Xiaojin, Zhu},
  journal	= {Computer Sciences TR},
  volume	= {1530},
  year		= {2008}
}

@Article{	  zhu2009introduction,
  title		= {Introduction to semi-supervised learning},
  author	= {Zhu, Xiaojin and Goldberg, Andrew B},
  journal	= {Synthesis lectures on artificial intelligence and machine
		  learning},
  volume	= {3},
  number	= {1},
  pages		= {1--130},
  year		= {2009},
  publisher	= {Morgan \& Claypool Publishers}
}

@Article{	  ouali2020overview,
  title		= {An overview of deep semi-supervised learning},
  author	= {Ouali, Yassine and Hudelot, C{\'e}line and Tami, Myriam},
  journal	= {arXiv preprint arXiv:2006.05278},
  year		= {2020}
}

@Article{	  bachman2014learning,
  title		= {Learning with pseudo-ensembles},
  author	= {Bachman, Philip and Alsharif, Ouais and Precup, Doina},
  journal	= {Advances in neural information processing systems},
  volume	= {27},
  pages		= {3365--3373},
  year		= {2014}
}



@Article{	  tarvainen2017mean,
  title		= {Mean teachers are better role models: Weight-averaged
		  consistency targets improve semi-supervised deep learning
		  results},
  author	= {Tarvainen, Antti and Valpola, Harri},
  journal	= {arXiv preprint arXiv:1703.01780},
  year		= {2017}
}



@Article{	  grandvalet2005semi,
  title		= {Semi-supervised learning by entropy minimization.},
  author	= {Grandvalet, Yves and Bengio, Yoshua and others},
  journal	= {CAP},
  volume	= {367},
  pages		= {281--296},
  year		= {2005}
}

@Article{	  miyato2018virtual,
  title		= {Virtual adversarial training: a regularization method for
		  supervised and semi-supervised learning},
  author	= {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori
		  and Ishii, Shin},
  journal	= {IEEE transactions on pattern analysis and machine
		  intelligence},
  volume	= {41},
  number	= {8},
  pages		= {1979--1993},
  year		= {2018},
  publisher	= {IEEE}
}

@InProceedings{	  lee2013pseudo,
  title		= {Pseudo-label: The simple and efficient semi-supervised
		  learning method for deep neural networks},
  author	= {Lee, Dong-Hyun and others},
  booktitle	= {Workshop on challenges in representation learning, ICML},
  volume	= {3},
  number	= {2},
  pages		= {896},
  year		= {2013}
}

@InProceedings{	  sajjadi2016mutual,
  title		= {Mutual exclusivity loss for semi-supervised deep
		  learning},
  author	= {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen,
		  Tolga},
  booktitle	= {2016 IEEE International Conference on Image Processing
		  (ICIP)},
  pages		= {1908--1912},
  year		= {2016},
  organization	= {IEEE}
}

@article{chapelle2009semi,
  title={Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]},
  author={Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
  journal={IEEE Transactions on Neural Networks},
  volume={20},
  number={3},
  pages={542--542},
  year={2009},
  publisher={IEEE}
}


@misc{gao2020freeenergy,
      title={A Free-Energy Principle for Representation Learning}, 
      author={Yansong Gao and Pratik Chaudhari},
      year={2020},
      eprint={2002.12406},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tishby2000information,
      title={The information bottleneck method}, 
      author={Naftali Tishby and Fernando C. Pereira and William Bialek},
      year={2000},
      eprint={physics/0004057},
      archivePrefix={arXiv},
      primaryClass={physics.data-an}
}


@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{henaff2020data,
  title={Data-efficient image recognition with contrastive predictive coding},
  author={Henaff, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={4182--4192},
  year={2020},
  organization={PMLR}
}



%%%%% below are added in 01/18/2022




@inproceedings{tian2020contrastive,
  title={Contrastive multiview coding},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16},
  pages={776--794},
  year={2020},
  organization={Springer}
}

@article{bachman2019learning,
  title={Learning representations by maximizing mutual information across views},
  author={Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
  journal={arXiv preprint arXiv:1906.00910},
  year={2019}
}

@article{hjelm2018learning,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9729--9738},
  year={2020}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{wang2020understanding,
  title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  pages={9929--9939},
  year={2020},
  organization={PMLR}
}

@article{linsker1988self,
  title={Self-organization in a perceptual network},
  author={Linsker, Ralph},
  journal={Computer},
  volume={21},
  number={3},
  pages={105--117},
  year={1988},
  publisher={IEEE}
}
gomes2010discriminative, bridle1992unsupervised,hu2017learning,
@article{gomes2010discriminative,
  title={Discriminative clustering by regularized information maximization},
  author={Gomes, Ryan and Krause, Andreas and Perona, Pietro},
  year={2010},
  publisher={Nueral Information Processing Systems}
}

@article{bridle1992unsupervised,
  title={Unsupervised classifiers, mutual information and'phantom targets'},
  author={Bridle, John S and Heading, Anthony JR and MacKay, David JC},
  year={1992},
  publisher={Morgan Kaufmann}
}

@inproceedings{hu2017learning,
  title={Learning discrete representations via information maximizing self-augmented training},
  author={Hu, Weihua and Miyato, Takeru and Tokui, Seiya and Matsumoto, Eiichi and Sugiyama, Masashi},
  booktitle={International conference on machine learning},
  pages={1558--1567},
  year={2017},
  organization={PMLR}
}

@article{logeswaran2018efficient,
  title={An efficient framework for learning sentence representations},
  author={Logeswaran, Lajanugen and Lee, Honglak},
  journal={arXiv preprint arXiv:1803.02893},
  year={2018}
}

@article{berthelot2019remixmatch,
  title={Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring},
  author={Berthelot, David and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Sohn, Kihyuk and Zhang, Han and Raffel, Colin},
  journal={arXiv preprint arXiv:1911.09785},
  year={2019}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={702--703},
  year={2020}
}

@article{zhang2021flexmatch,
  title={Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling},
  author={Zhang, Bowen and Wang, Yidong and Hou, Wenxin and Wu, Hao and Wang, Jindong and Okumura, Manabu and Shinozaki, Takahiro},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


