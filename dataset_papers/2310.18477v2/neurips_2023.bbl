\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akhtar et~al.(2021)Akhtar, Mian, Kardan, and Shah]{akhtar2021advances}
Naveed Akhtar, Ajmal Mian, Navid Kardan, and Mubarak Shah.
\newblock Advances in adversarial attacks and defenses in computer vision: A
  survey.
\newblock \emph{IEEE Access}, 9:\penalty0 155161--155196, 2021.

\bibitem[Al-Dujaili and O'Reilly(2020)]{al2020sign}
Abdullah Al-Dujaili and Una-May O'Reilly.
\newblock Sign bits are all you need for black-box attacks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and
  Hein]{andriushchenko2020square}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Bai et~al.(2021)Bai, Luo, Zhao, Wen, and Wang]{ijcai2021p591}
Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang.
\newblock Recent advances in adversarial training for adversarial robustness.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, pages 4312--4321, 2021.

\bibitem[Bhambri et~al.(2019)Bhambri, Muku, Tulasi, and
  Buduru]{bhambri2019survey}
Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi, and Arun~Balaji Buduru.
\newblock A survey of black-box adversarial attacks on computer vision models.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Blickle and Thiele(1996)]{blickle1996comparison}
Tobias Blickle and Lothar Thiele.
\newblock A comparison of selection schemes used in evolutionary algorithms.
\newblock \emph{Evolutionary Computation}, 4\penalty0 (4):\penalty0 361--394,
  1996.

\bibitem[Brown et~al.(2005)Brown, Wyatt, Tino, and Bengio]{brown2005managing}
Gavin Brown, Jeremy~L Wyatt, Peter Tino, and Yoshua Bengio.
\newblock Managing diversity in regression ensembles.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 6\penalty0 (9),
  2005.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{IEEE Symposium on Security and Privacy (SP)}, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Duchi, and
  Liang]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{Advances in neural information processing systems (NeurIPS)},
  32, 2019.

\bibitem[Chakraborty et~al.(2021)Chakraborty, Alam, Dey, Chattopadhyay, and
  Mukhopadhyay]{chakraborty2021survey}
Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep
  Mukhopadhyay.
\newblock A survey on adversarial attacks and defences.
\newblock \emph{CAAI Transactions on Intelligence Technology}, 6\penalty0
  (1):\penalty0 25--45, 2021.

\bibitem[Christian et~al.(2014)Christian, Wojciech, Ilya, Bruna, Erhan,
  Goodfellow, and Fergus]{SzegedyZSBEGF13}
Szegedy Christian, Zaremba Wojciech, Sutskever Ilya, Joan Bruna, Dumitru Erhan,
  Ian~J. Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Croce and Hein(2020)]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Croce et~al.(2020)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{croce2020robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock \emph{arXiv preprint}, 2020.

\bibitem[Cui et~al.(2022)Cui, Zhang, Liang, Han, Sugiyama, and
  Zhang]{cui2022synergy}
Sen Cui, Jingfeng Zhang, Jian Liang, Bo~Han, Masashi Sugiyama, and Changshui
  Zhang.
\newblock Synergy-of-experts: Collaborate to improve adversarial robustness.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  35:\penalty0 32552--32567, 2022.

\bibitem[Dong et~al.(2018)Dong, Liao, Pang, Su, Zhu, Hu, and
  Li]{dong2018boosting}
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
  Jianguo Li.
\newblock Boosting adversarial attacks with momentum.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 9185--9193, 2018.

\bibitem[Fawzi et~al.(2018)Fawzi, Moosavi-Dezfooli, Frossard, and
  Soatto]{fawzi2018empirical}
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano
  Soatto.
\newblock Empirical study of the topology and geometry of deep networks.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2018.

\bibitem[Folland(2005)]{folland2005higher}
Gerald~B Folland.
\newblock Higher-order derivatives and taylorâ€™s formula in several variables.
\newblock \emph{Preprint}, pages 1--4, 2005.

\bibitem[Freeman and Bruna(2017)]{FreemanB17Topology}
C.~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2015.

\bibitem[Guo et~al.(2021)Guo, Han, Han, Huang, and Lu]{guo2021label}
Biyang Guo, Songqiao Han, Xiao Han, Hailiang Huang, and Ting Lu.
\newblock Label confusion learning to enhance text classification models.
\newblock In \emph{The AAAI Conference on Artificial Intelligence (AAAI)},
  volume~35, pages 12929--12936, 2021.

\bibitem[Guo et~al.(2019)Guo, Gardner, You, Wilson, and
  Weinberger]{guo2019simple}
Chuan Guo, Jacob Gardner, Yurong You, Andrew~Gordon Wilson, and Kilian
  Weinberger.
\newblock Simple black-box adversarial attacks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2484--2493, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2016.

\bibitem[He et~al.(2022)He, Wang, Dong, and Tan]{he2022revisiting}
Ziwen He, Wei Wang, Jing Dong, and Tieniu Tan.
\newblock Revisiting ensemble adversarial attack.
\newblock \emph{Signal Processing: Image Communication}, 107:\penalty0 116747,
  2022.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and
  Mazeika]{hendrycks2019pretraining}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock \emph{Advances in neural information processing systems (NeurIPS)},
  32, 2019.

\bibitem[Kariyappa and Qureshi(2019)]{kariyappa2019improving}
Sanjay Kariyappa and Moinuddin~K Qureshi.
\newblock Improving adversarial robustness of ensembles with diversity
  training.
\newblock \emph{arXiv preprint arXiv:1901.09981}, 2019.

\bibitem[Lowd and Meek(2005)]{lowd2005adversarial}
Daniel Lowd and Christopher Meek.
\newblock Adversarial learning.
\newblock In \emph{International Conference on Knowledge Discovery and Data
  Mining (KDD)}, 2005.

\bibitem[Lu et~al.(2022)Lu, Hu, Huo, and Li]{lu2022ensemble}
Zhiping Lu, Hongchao Hu, Shumin Huo, and Shuyi Li.
\newblock Ensemble learning methods of adversarial attacks and defenses in
  computer vision: Recent progress.
\newblock In \emph{International Conference on Advanced Computing and
  Endogenous Security (ACES)}, pages 1--10, 2022.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madryMSTV18towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and
  Frossard]{moosavi2016deepfool}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition {CVPR}}, 2016.

\bibitem[Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and
  Frossard]{moosavi2019robustness}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal
  Frossard.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem[Najafi et~al.(2019)Najafi, Maeda, Koyama, and
  Miyato]{najafi2019robustness}
Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato.
\newblock Robustness to adversarial perturbations in learning from incomplete
  data.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  32, 2019.

\bibitem[Pang et~al.(2019)Pang, Xu, Du, Chen, and Zhu]{pang2019improving}
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  4970--4979, 2019.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Jha, Fredrikson, Celik, and
  Swami]{papernokey6limitations}
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z~Berkay
  Celik, and Ananthram Swami.
\newblock The limitations of deep learning in adversarial settings.
\newblock In \emph{IEEE European Symposium on Security and Privacy (EuroSP)},
  2016.

\bibitem[Qin et~al.(2019)Qin, Martens, Gowal, Krishnan, Dvijotham, Fawzi, De,
  Stanforth, and Kohli]{qin2019adversarial}
Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy
  Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli.
\newblock Adversarial robustness through local linearization.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  32, 2019.

\bibitem[Raghunathan et~al.(2020)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2020understanding}
Aditi Raghunathan, Sang~M. Xie, Fanny Yang, John~C. Duchi, and Percy Liang.
\newblock Understanding and mitigating the tradeoff between robustness and
  accuracy.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Ren et~al.(2019)Ren, Wang, Wang, Qin, and Lin]{ren2019security}
Kui Ren, Qian Wang, Cong Wang, Zhan Qin, and Xiaodong Lin.
\newblock The security of autonomous driving: Threats, defenses, and future
  directions.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (2):\penalty0 357--372,
  2019.

\bibitem[Sehwag et~al.(2020)Sehwag, Wang, Mittal, and Jana]{sehwag2020hydra}
Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana.
\newblock Hydra: Pruning adversarially robust neural networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  33:\penalty0 19655--19666, 2020.

\bibitem[Song et~al.(2019)Song, He, Wang, and Hopcroft]{SongHWH19Improve}
Chuanbiao Song, Kun He, Liwei Wang, and John~E. Hopcroft.
\newblock Improving the generalization of adversarial training with domain
  adaptation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Tram{\`e}r et~al.(2018)Tram{\`e}r, Kurakin, Papernot, Goodfellow,
  Boneh, and McDaniel]{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{TsiprasSETM19Robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Ueda and Nakano(1996)]{ueda1996generalization}
Naonori Ueda and Ryohei Nakano.
\newblock Generalization error of ensemble estimators.
\newblock In \emph{International Conference on Neural Networks (ICNN)},
  volume~1, pages 90--95, 1996.

\bibitem[Uesato et~al.(2018)Uesato, Odonoghue, Kohli, and
  Oord]{uesato2018adversarial}
Jonathan Uesato, Brendan Odonoghue, Pushmeet Kohli, and Aaron Oord.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Virmaux and Scaman(2018)]{virmaux2018lipschitz}
Aladin Virmaux and Kevin Scaman.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  31, 2018.

\bibitem[Wang and Liu(2023)]{wang2023adversarial}
Lele Wang and Bin Liu.
\newblock Adversarial ensemble training by jointly learning label dependencies
  and member models.
\newblock In \emph{International Conference on Intelligent Computing (ICIC)},
  pages 3--20, 2023.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Li, Kuang, Tan, and
  Li]{wang2019security}
Xianmin Wang, Jing Li, Xiaohui Kuang, Yu-an Tan, and Jin Li.
\newblock The security of machine learning in an adversarial setting: A survey.
\newblock \emph{The Journal of Parallel and Distributed Computing (JPDC)},
  130:\penalty0 12--23, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Zou, Yi, Bailey, Ma, and
  Gu]{wang2019improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Wood et~al.(2023)Wood, Mu, Webb, Reeve, Lujan, and
  Brown]{wood2023unified}
Danny Wood, Tingting Mu, Andrew Webb, Henry Reeve, Mikel Lujan, and Gavin
  Brown.
\newblock A unified theory of diversity in ensemble learning.
\newblock \emph{arXiv preprint arXiv:2301.03962}, 2023.

\bibitem[Wu et~al.(2021)Wu, Chen, Cai, He, and Gu]{wu2021wider}
Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quanquan Gu.
\newblock Do wider neural networks really help adversarial robustness?
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34:\penalty0 7054--7067, 2021.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  33:\penalty0 2958--2969, 2020.

\bibitem[Xie et~al.(2019)Xie, Wu, Maaten, Yuille, and He]{xie2019feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan~L Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem[Yang et~al.(2020)Yang, Zhang, Dong, Inkawhich, Gardner, Touchet,
  Wilkes, Berry, and Li]{yang2020dverge}
Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner,
  Andrew Touchet, Wesley Wilkes, Heath Berry, and Hai Li.
\newblock Dverge: diversifying vulnerabilities for enhanced robust generation
  of ensembles.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  33:\penalty0 5505--5515, 2020.

\bibitem[Yang et~al.(2021)Yang, Li, Xu, Zuo, Chen, Zhou, Rubinstein, Zhang, and
  Li]{yang2021trs}
Zhuolin Yang, Linyi Li, Xiaojun Xu, Shiliang Zuo, Qian Chen, Pan Zhou, Benjamin
  Rubinstein, Ce~Zhang, and Bo~Li.
\newblock Trs: Transferability reduced ensemble via promoting gradient
  diversity and model smoothness.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Zagoruyko and Komodakis(2016)]{Zagoruyko2016WRN}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference (BMVC)}, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{pmlrv97zhang19p}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent~El Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\end{thebibliography}
