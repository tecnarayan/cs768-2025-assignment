\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WYZY22}

\bibitem[ABTR21]{akhtar2021projection}
Zeeshan Akhtar, Amrit~Singh Bedi, Srujan~Teja Thomdapu, and Ketan Rajawat.
\newblock {Projection-Free Algorithm for Stochastic Bi-level Optimization}.
\newblock {\em arXiv preprint arXiv:2110.11721}, 2021.

\bibitem[AF21]{astudillo2021bayesian}
Raul Astudillo and Peter Frazier.
\newblock Bayesian optimization of function networks.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[BG22]{balasubramanian2021zeroth}
Krishnakumar Balasubramanian and Saeed Ghadimi.
\newblock Zeroth-order nonconvex stochastic optimization: Handling constraints,
  high dimensionality, and saddle points.
\newblock {\em Foundations of Computational Mathematics}, 22(1):35--76, 2022.

\bibitem[BGI{\etalchar{+}}17]{blanchet2017unbiased}
Jose Blanchet, Donald Goldfarb, Garud Iyengar, Fengpei Li, and Chaoxu Zhou.
\newblock Unbiased simulation for optimizing stochastic function compositions.
\newblock {\em arXiv preprint arXiv:1711.07564}, 2017.

\bibitem[BGN22]{balasubramanian2020stochastic}
Krishnakumar Balasubramanian, Saeed Ghadimi, and Anthony Nguyen.
\newblock Stochastic multilevel composition optimization algorithms with
  level-independent convergence rates.
\newblock {\em SIAM Journal on Optimization}, 32(2):519--544, 2022.

\bibitem[BS17]{beck2017linearly}
Amir Beck and Shimrit Shtern.
\newblock Linearly convergent away-step conditional gradient for non-strongly
  convex functions.
\newblock {\em Mathematical Programming}, 164(1-2):1--27, 2017.

\bibitem[CFKM20]{cong2020minimal}
Weilin Cong, Rana Forsati, Mahmut Kandemir, and Mehrdad Mahdavi.
\newblock Minimal variance sampling with provable guarantees for fast training
  of graph neural networks.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1393--1403, 2020.

\bibitem[CSY21]{chen2021solving}
Tianyi Chen, Yuejiao Sun, and Wotao Yin.
\newblock Solving stochastic compositional optimization is nearly as easy as
  solving stochastic optimization.
\newblock {\em IEEE Transactions on Signal Processing}, 69:4937--4948, 2021.

\bibitem[DD19]{davis2019stochastic}
Damek Davis and Dmitriy Drusvyatskiy.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock {\em SIAM Journal on Optimization}, 29(1):207--239, 2019.

\bibitem[DPR17]{dentcheva2017statistical}
Darinka Dentcheva, Spiridon Penev, and Andrzej Ruszczy{\'n}ski.
\newblock Statistical estimation of composite risk functionals and risk
  optimization problems.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  69(4):737--760, 2017.

\bibitem[DR18]{duchi2018stochastic}
John Duchi and Feng Ruan.
\newblock Stochastic methods for composite and weakly convex optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 28(4):3229--3259, 2018.

\bibitem[EN13]{ermoliev2013sample}
Yuri Ermoliev and Vladimir Norkin.
\newblock Sample average approximation method for compound stochastic
  optimization problems.
\newblock {\em SIAM Journal on Optimization}, 23(4):2231--2263, 2013.

\bibitem[Erm76]{Ermolievold}
Yuri Ermoliev.
\newblock Methods of stochastic programming.
\newblock {\em Nauka, Moscow}, 1976.

\bibitem[FMO21]{fallah2021generalization}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Generalization of model-agnostic meta-learning algorithms: Recurring
  and unseen tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[FW56]{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval research logistics quarterly}, 3(1-2):95--110, 1956.

\bibitem[GH15]{garber2015faster}
Dan Garber and Elad Hazan.
\newblock Faster rates for the frank-wolfe method over strongly-convex sets.
\newblock In {\em International Conference on Machine Learning}, pages
  541--549. PMLR, 2015.

\bibitem[GKS21]{garber2021improved}
Dan Garber, Atara Kaplan, and Shoham Sabach.
\newblock Improved complexities of conditional gradient-type methods with
  applications to robust matrix recovery problems.
\newblock {\em Mathematical Programming}, 186(1):185--208, 2021.

\bibitem[GRW20]{ghadimi2020single}
Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang.
\newblock A single timescale stochastic approximation method for nested
  stochastic optimization.
\newblock {\em SIAM Journal on Optimization}, 30(1):960--979, 2020.

\bibitem[GW21]{garber2021frank}
Dan Garber and Noam Wolf.
\newblock {Frank-Wolfe with a nearest extreme point oracle}.
\newblock In {\em Conference on Learning Theory}, pages 2103--2132. PMLR, 2021.

\bibitem[HJN15]{harchaoui2015conditional}
Zaid Harchaoui, Anatoli Juditsky, and Arkadi Nemirovski.
\newblock Conditional gradient algorithms for norm-regularized smooth convex
  optimization.
\newblock {\em Mathematical Programming}, 152(1-2):75--112, 2015.

\bibitem[HK12]{hazan2012projection}
Elad Hazan and Satyen Kale.
\newblock Projection-free online learning.
\newblock In {\em 29th International Conference on Machine Learning, ICML
  2012}, pages 521--528, 2012.

\bibitem[HK14]{hazan2014beyond}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: optimal algorithms for
  stochastic strongly-convex optimization.
\newblock {\em The Journal of Machine Learning Research}, 15(1):2489--2512,
  2014.

\bibitem[HL16]{hazan2016variance}
Elad Hazan and Haipeng Luo.
\newblock Variance-reduced and projection-free stochastic optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1263--1271, 2016.

\bibitem[HLPR19]{harvey2019tight}
Nicholas~JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa.
\newblock Tight analyses for non-smooth stochastic gradient descent.
\newblock In {\em Conference on Learning Theory}, pages 1579--1613. PMLR, 2019.

\bibitem[HZCH20]{hu2020biased}
Yifan Hu, Siqi Zhang, Xin Chen, and Niao He.
\newblock Biased stochastic first-order methods for conditional stochastic
  optimization and applications in meta learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Jag13]{jaggi2013revisiting}
Martin Jaggi.
\newblock {Revisiting Frank-Wolfe: Projection-free sparse convex optimization}.
\newblock In {\em International Conference on Machine Learning}, pages
  427--435. PMLR, 2013.

\bibitem[LJJ15]{lacoste2015global}
Simon Lacoste-Julien and Martin Jaggi.
\newblock On the global linear convergence of {Frank-Wolfe} optimization
  variants.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  496--504, 2015.

\bibitem[LO20]{li2020high}
Xiaoyu Li and Francesco Orabona.
\newblock A high probability analysis of adaptive sgd with momentum.
\newblock {\em arXiv preprint arXiv:2007.14294}, 2020.

\bibitem[LP66]{levitin1966constrained}
Evgeny Levitin and Boris Polyak.
\newblock Constrained minimization methods.
\newblock {\em USSR Computational mathematics and mathematical physics},
  6(5):1--50, 1966.

\bibitem[LZ16]{lan2016conditional}
Guanghui Lan and Yi~Zhou.
\newblock Conditional gradient sliding for convex optimization.
\newblock {\em SIAM Journal on Optimization}, 26(2):1379--1409, 2016.

\bibitem[LZW22]{lou2022beyond}
Zhipeng Lou, Wanrong Zhu, and Wei~Biao Wu.
\newblock Beyond sub-gaussian noises: Sharp concentration analysis for
  stochastic gradient descent.
\newblock {\em Journal of Machine Learning Research}, 23:1--22, 2022.

\bibitem[MDB21]{madden2021high}
Liam Madden, Emiliano Dall'Anese, and Stephen Becker.
\newblock High-probability convergence bounds for non-convex stochastic
  gradient descent.
\newblock {\em arXiv preprint arXiv:2006.05610}, 2021.

\bibitem[MHK20]{mokhtari2020stochastic}
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi.
\newblock Stochastic conditional gradient methods: From convex minimization to
  submodular maximization.
\newblock {\em Journal of machine learning research}, 2020.

\bibitem[Mig94]{migdalas1994regularization}
Athanasios Migdalas.
\newblock A regularization of the frankâ€”wolfe method and unification of
  certain nonlinear programming methods.
\newblock {\em Mathematical Programming}, 65(1):331--345, 1994.

\bibitem[Nes18]{nesterov2018lectures}
Yurii Nesterov.
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[QGX{\etalchar{+}}21]{qi2021online}
Qi~Qi, Zhishuai Guo, Yi~Xu, Rong Jin, and Tianbao Yang.
\newblock An online method for a class of distributionally robust optimization
  with non-convex objectives.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[QHZ{\etalchar{+}}22]{qiu2022large}
Zi-Hao Qiu, Quanqi Hu, Yongjian Zhong, Lijun Zhang, and Tianbao Yang.
\newblock Large-scale stochastic optimization of ndcg surrogates for deep
  learning with provable convergence.
\newblock {\em arXiv preprint arXiv:2202.12183}, 2022.

\bibitem[QLX18]{qu2018non}
Chao Qu, Yan Li, and Huan Xu.
\newblock Non-convex conditional gradient sliding.
\newblock In {\em International Conference on Machine Learning}, pages
  4208--4217. PMLR, 2018.

\bibitem[QLX{\etalchar{+}}21]{qi2021stochastic}
Qi~Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang.
\newblock Stochastic optimization of areas under precision-recall curves with
  provable convergence.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[RS06]{ruszczynski2006optimization}
Andrzej Ruszczy{\'n}ski and Alexander Shapiro.
\newblock Optimization of convex risk functions.
\newblock {\em Mathematics of operations research}, 31(3):433--452, 2006.

\bibitem[RSPS16]{reddi2016stochastic}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Stochastic frank-wolfe methods for nonconvex optimization.
\newblock In {\em 2016 54th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 1244--1251. IEEE, 2016.

\bibitem[Rus87]{ruszczynski1987linearization}
Andrzej Ruszczy{\'n}ski.
\newblock A linearization method for nonsmooth stochastic programming problems.
\newblock {\em Mathematics of Operations Research}, 12(1):32--49, 1987.

\bibitem[Rus21]{ruszczynski2021stochastic}
Andrzej Ruszczynski.
\newblock A stochastic subgradient method for nonsmooth nonconvex multilevel
  composition optimization.
\newblock {\em SIAM Journal on Control and Optimization}, 59(3):2301--2320,
  2021.

\bibitem[RWC03]{ruppert2003semiparametric}
David Ruppert, Matt~P Wand, and Raymond~J Carroll.
\newblock {\em Semiparametric regression}.
\newblock Number~12. Cambridge university press, 2003.

\bibitem[Ver18]{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[WFL17]{wang2017stochastic}
Mengdi Wang, Ethan Fang, and Han Liu.
\newblock Stochastic compositional gradient descent: {A}lgorithms for
  minimizing compositions of expected-value functions.
\newblock {\em Mathematical Programming}, 161(1-2):419--449, 2017.

\bibitem[WGE17]{wang2017solving}
Gang Wang, Georgios~B Giannakis, and Yonina~C Eldar.
\newblock Solving systems of random quadratic equations via truncated amplitude
  flow.
\newblock {\em IEEE Transactions on Information Theory}, 64(2):773--794, 2017.

\bibitem[WLF16]{wang2016accelerating}
Mengdi Wang, Ji~Liu, and Ethan Fang.
\newblock Accelerating stochastic composition optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem[WYZY22]{wang2022momentum}
Guanghui Wang, Ming Yang, Lijun Zhang, and Tianbao Yang.
\newblock Momentum accelerates the convergence of stochastic {AUPRC}
  maximization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3753--3771. PMLR, 2022.

\bibitem[YSC19]{yurtsever2019conditional}
Alp Yurtsever, Suvrit Sra, and Volkan Cevher.
\newblock Conditional gradient methods via stochastic path-integrated
  differential estimator.
\newblock In {\em International Conference on Machine Learning}, pages
  7282--7291. PMLR, 2019.

\bibitem[YWF19]{yang2019multi-level}
Shuoguang Yang, Mengdi Wang, and Ethan Fang.
\newblock Multilevel stochastic gradient methods for nested composition
  optimization.
\newblock {\em SIAM Journal on Optimization}, 29(1):616--659, 2019.

\bibitem[ZCC{\etalchar{+}}18]{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em arXiv preprint arXiv:1808.05671}, 2018.

\bibitem[ZSM{\etalchar{+}}20]{zhang2020one}
Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed Hassani, and Amin Karbasi.
\newblock {One-sample Stochastic Frank-Wolfe}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4012--4023. PMLR, 2020.

\bibitem[ZX21]{zhang2019multi}
Junyu Zhang and Lin Xiao.
\newblock Multilevel composite stochastic optimization via nested variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 31(2):1131--1157, 2021.

\end{thebibliography}
