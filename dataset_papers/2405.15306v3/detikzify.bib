@inproceedings{blecher2024nougat,
title={Nougat: Neural Optical Understanding for Academic Documents},
author={Lukas Blecher and Guillem Cucurull and Thomas Scialom and Robert Stojnic},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=fUtxNAKpdV}
}
@inproceedings{belouadi2024automatikz,
title={{AutomaTikZ}: Text-Guided Synthesis of Scientific Vector Graphics with {TikZ}},
author={Jonas Belouadi and Anne Lauscher and Steffen Eger},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=v3K5TVP8kZ}
}
@misc{ugare2024improving,
      title={Improving {LLM} Code Generation with Grammar Augmentation}, 
      author={Shubham Ugare and Tarun Suresh and Hangoo Kang and Sasa Misailovic and Gagandeep Singh},
      year={2024},
      eprint={2403.01632},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{poesia2022synchromesh,
title={Synchromesh: Reliable Code Generation from Pre-trained Language Models},
author={Gabriel Poesia and Alex Polozov and Vu Le and Ashish Tiwari and Gustavo Soares and Christopher Meek and Sumit Gulwani},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=KmtVD97J43e}
}
@inproceedings{scholak2021picard,
    title = "{PICARD}: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    author = "Scholak, Torsten  and
      Schucher, Nathan  and
      Bahdanau, Dzmitry",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.779",
    doi = "10.18653/v1/2021.emnlp-main.779",
    pages = "9895--9901",
    abstract = "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at \url{https://github.com/ElementAI/picard}), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",
}
@inproceedings{erdweg2010tex,
author="Erdweg, Sebastian Thore
and Ostermann, Klaus",
editor="Malloy, Brian
and Staab, Steffen
and van den Brand, Mark",
title="Featherweight {TeX} and Parser Correctness",
booktitle="Software Language Engineering",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="397--416",
abstract="TeX (and its LaTeX incarnation) is a widely used document preparation system for technical and scientific documents. At the same time, TeX is also an unusual programming language with a quite powerful macro system. Despite the wide range of TeX users (especially in the scientific community), and despite a widely perceived considerable level of ``pain'' in using TeX, there is almost no research on TeX. This paper is an attempt to change that.",
isbn="978-3-642-19440-5"
}

@article{Kartal2016sokoban,
  title={Data Driven {Sokoban} Puzzle Generation with {Monte Carlo} Tree Search},
  volume={12},
  url={https://ojs.aaai.org/index.php/AIIDE/article/view/12859},
  DOI={10.1609/aiide.v12i1.12859},
  abstractNote={In this work, we propose a Monte Carlo Tree Search (MCTS) based approach to procedurally generate Sokoban puzzles. Our method generates puzzles through simulated game play, guaranteeing solvability in all generated puzzles. We perform a user study to infer features that are efficient to compute and are highly correlated with expected puzzle difficulty. We combine several of these features into a data-driven evaluation function for MCTS puzzle creation. The resulting algorithm is efficient and can be run in an anytime manner, capable of quickly generating a variety of challenging puzzles. We perform a second user study to validate the predictive capability of our approach, showing a high correlation between increasing puzzle scores and perceived difficulty.},
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  author={Kartal, Bilal and Sohre, Nick and Guy, Stephen},
  year={2016},
  month={October},
  pages={58-64}
}
@inproceedings{Kartal2016bsokoban,
  title     = {Generating {Sokoban} Puzzle Game Levels with {Monte Carlo} Tree Search},
  author    = {Kartal, Bilal and Sohre, Nick and Guy, Stephen},
  booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on
               Artificial Intelligence, {IJCAI-16}},
  series    = {The {IJCAI-16} Workshop on General Game Playing},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {47--54},
  year      = {2016},
  month     = {7},
  url       = {http://giga16.ru.is/giga16-paper6.pdf},
}
@manual{tantau2023tikz,
   author    = {Till Tantau},
   title     = {The {TikZ} and {PGF} Packages},
   subtitle  = {Manual for version 3.1.10},
   url       = {https://github.com/pgf-tikz/pgf},
   date      = {2023-01-15},
   year      = {2023}
}
@inproceedings{coulom2007mcts,
author="Coulom, R{\'e}mi",
editor="van den Herik, H. Jaap
and Ciancarini, Paolo
and Donkers, H. H. L. M. (Jeroen)",
title="Efficient Selectivity and Backup Operators in {Monte-Carlo} Tree Search",
booktitle="Computers and Games",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="72--83",
abstract="A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9{\texttimes}9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.",
isbn="978-3-540-75538-8"
}
@inproceedings{hu2022planet,
    title = "{PLANET}: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation",
    author = "Hu, Zhe  and
      Chan, Hou Pong  and
      Liu, Jiachen  and
      Xiao, Xinyan  and
      Wu, Hua  and
      Huang, Lifu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.163",
    doi = "10.18653/v1/2022.acl-long.163",
    pages = "2288--2305",
    abstract = "Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow. In this work, we propose PLANET, a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically. To guide the generation of output sentences, our framework enriches the Transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words. Moreover, we introduce a new coherence-based contrastive learning objective to further improve the coherence of output. Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation. Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents.",
}
@article{Summerville2015mcmcts,
  title={{MCMCTS PCG 4 SMB}: {Monte Carlo} Tree Search to Guide Platformer Level Generation}, volume={11},
  url={https://ojs.aaai.org/index.php/AIIDE/article/view/12816},
  DOI={10.1609/aiide.v11i3.12816},
  abstractNote={Markov chains are an enticing option for machine learned generation of platformer levels, but offer poor control for designers and are likely to produce unplayable levels. In this paper we present a method for guiding Markov chain generation using Monte Carlo Tree Search that we call Markov Chain Monte Carlo Tree Search (MCMCTS). We demonstrate an example use for this technique by creating levels trained on a corpus of levels from Super Mario Bros. We then present a player modeling study that was run with the hopes of using the data to better inform the generation of levels in future work.},
  number={3},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  author={Summerville, Adam and Philip, Shweta and Mateas, Michael},
  year={2015},
  month={Nov.},
  pages={68-74}
}
@article{silver2016go,
  author       = {David Silver and
                  Aja Huang and
                  Chris J. Maddison and
                  Arthur Guez and
                  Laurent Sifre and
                  George van den Driessche and
                  Julian Schrittwieser and
                  Ioannis Antonoglou and
                  Vedavyas Panneershelvam and
                  Marc Lanctot and
                  Sander Dieleman and
                  Dominik Grewe and
                  John Nham and
                  Nal Kalchbrenner and
                  Ilya Sutskever and
                  Timothy P. Lillicrap and
                  Madeleine Leach and
                  Koray Kavukcuoglu and
                  Thore Graepel and
                  Demis Hassabis},
  title        = {Mastering the game of Go with deep neural networks and tree search},
  journal      = {Nature},
  volume       = {529},
  number       = {7587},
  pages        = {484--489},
  year         = {2016},
  url          = {https://doi.org/10.1038/nature16961},
  doi          = {10.1038/NATURE16961},
}
@article{silver2017go,
  author       = {David Silver and
                  Julian Schrittwieser and
                  Karen Simonyan and
                  Ioannis Antonoglou and
                  Aja Huang and
                  Arthur Guez and
                  Thomas Hubert and
                  Lucas Baker and
                  Matthew Lai and
                  Adrian Bolton and
                  Yutian Chen and
                  Timothy P. Lillicrap and
                  Fan Hui and
                  Laurent Sifre and
                  George van den Driessche and
                  Thore Graepel and
                  Demis Hassabis},
  title        = {Mastering the game of Go without human knowledge},
  journal      = {Nature},
  volume       = {550},
  number       = {7676},
  pages        = {354--359},
  year         = {2017},
  url          = {https://doi.org/10.1038/nature24270},
  doi          = {10.1038/NATURE24270},
}
@inproceedings{liu2024dont,
title={Don't throw away your value model! Generating more preferable text with Value-Guided {Monte-Carlo} Tree Search decoding},
author={Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=kh9Zt2Ldmn}
}
@misc{brandfonbrener2024verified,
      title={Verified Multi-Step Synthesis using Large Language Models and {Monte Carlo} Tree Search}, 
      author={David Brandfonbrener and Sibi Raja and Tarun Prasad and Chloe Loughridge and Jianang Yang and Simon Henniger and William E. Byrd and Robert Zinkov and Nada Amin},
      year={2024},
      eprint={2402.08147},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@inproceedings{
zhang2023planning,
title={Planning with Large Language Models for Code Generation},
author={Shun Zhang and Zhenfang Chen and Yikang Shen and Mingyu Ding and Joshua B. Tenenbaum and Chuang Gan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Lr8cOOtYbfL}
}
@inproceedings{chaffin2022ppl,
    title = "{PPL-MCTS}: {C}onstrained Textual Generation Through Discriminator-Guided {MCTS} Decoding",
    author = "Chaffin, Antoine  and
      Claveau, Vincent  and
      Kijak, Ewa",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.215",
    doi = "10.18653/v1/2022.naacl-main.215",
    pages = "2953--2967",
    abstract = "Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.",
}
@inproceedings{kocsis2006bandit,
author="Kocsis, Levente and Szepesv{\'a}ri, Csaba",
editor="F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra",
title="Bandit Based {Monte-Carlo} Planning",
booktitle="Machine Learning: ECML 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="282--293",
abstract="For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.",
isbn="978-3-540-46056-5"
}
@inproceedings{soemers2016realtime,
  author={Soemers, Dennis J. N. J. and Sironi, Chiara F. and Schuster, Torsten and Winands, Mark H. M.},
  booktitle={2016 IEEE Conference on Computational Intelligence and Games (CIG)},
  title={Enhancements for real-time Monte-Carlo Tree Search in General Video Game Playing},
  year={2016},
  volume={},
  number={},
  pages={1-8},
  keywords={Games;Monte Carlo methods;History;Real-time systems;Silicon;Knowledge based systems;Avatars},
  doi={10.1109/CIG.2016.7860448}
}
@inproceedings{belouadi2023uscore,
    title = "{US}core: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation",
    author = "Belouadi, Jonas  and
      Eger, Steffen",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.27",
    doi = "10.18653/v1/2023.eacl-main.27",
    pages = "358--374",
    abstract = "The vast majority of evaluation metrics for machine translation are supervised, i.e., (i) are trained on human scores, (ii) assume the existence of reference translations, or (iii) leverage parallel data. This hinders their applicability to cases where such supervision signals are not available. In this work, we develop fully unsupervised evaluation metrics. To do so, we leverage similarities and synergies between evaluation metric induction, parallel corpus mining, and MT systems. In particular, we use an unsupervised evaluation metric to mine pseudo-parallel data, which we use to remap deficient underlying vector spaces (in an iterative manner) and to induce an unsupervised MT system, which then provides pseudo-references as an additional component in the metric. Finally, we also induce unsupervised multilingual sentence embeddings from pseudo-parallel data. We show that our fully unsupervised metrics are effective, i.e., they beat supervised competitors on 4 out of our 5 evaluation datasets. We make our code publicly available.",
}
@inproceedings{song2021sentsim,
    title = "{S}ent{S}im: Crosslingual Semantic Evaluation of Machine Translation",
    author = "Song, Yurun  and
      Zhao, Junchen  and
      Specia, Lucia",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.252",
    doi = "10.18653/v1/2021.naacl-main.252",
    pages = "3143--3156",
    abstract = "Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approaches {--} namely BERTScore and Word Mover{'}s Distance {--} by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information.",
}
@inproceedings{zhao2019moverscore,
    title = "{M}over{S}core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
    author = "Zhao, Wei  and
      Peyrard, Maxime  and
      Liu, Fei  and
      Gao, Yang  and
      Meyer, Christian M.  and
      Eger, Steffen",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1053",
    doi = "10.18653/v1/D19-1053",
    pages = "563--578",
    abstract = "A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service.",
}
@inproceedings{zhao2020limitations,
    title = "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation",
    author = "Zhao, Wei  and
      Glava{\v{s}}, Goran  and
      Peyrard, Maxime  and
      Gao, Yang  and
      West, Robert  and
      Eger, Steffen",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.151",
    doi = "10.18653/v1/2020.acl-main.151",
    pages = "1656--1671",
    abstract = "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish {``}translationese{''}, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.",
}
@INPROCEEDINGS{rubner1998emd,
  author={Rubner, Y. and Tomasi, C. and Guibas, L.J.},
  booktitle={Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)}, 
  title={A metric for distributions with applications to image databases}, 
  year={1998},
  volume={},
  number={},
  pages={59-66},
  keywords={Image databases;Histograms;Image retrieval;Psychology;Frequency;Application software;Computer science;Geoscience;Computer displays;Navigation},
  doi={10.1109/ICCV.1998.710701}
}
@inproceedings{kusner2015wmd,
  title =    {From Word Embeddings To Document Distances},
  author =    {Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
  booktitle =    {Proceedings of the 32nd International Conference on Machine Learning},
  pages =    {957--966},
  year =    {2015},
  editor =    {Bach, Francis and Blei, David},
  volume =    {37},
  series =    {Proceedings of Machine Learning Research},
  address =    {Lille, France},
  month =    {07--09 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v37/kusnerb15.pdf},
  url =    {https://proceedings.mlr.press/v37/kusnerb15.html},
  abstract =    {We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover’s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.}
}
@inproceedings{dou2021word,
    title = "Word Alignment by Fine-tuning Embeddings on Parallel Corpora",
    author = "Dou, Zi-Yi  and
      Neubig, Graham",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.181",
    doi = "10.18653/v1/2021.eacl-main.181",
    pages = "2112--2128",
    abstract = "Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs.",
}
@inproceedings{liu2023llava,
title={Visual Instruction Tuning},
author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=w0H2xGHlkw}
}
@inproceedings{zhang2020bertscore,
title={{BERTScore}: Evaluating Text Generation with {BERT}},
author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}
@inproceedings{zhang2018lpips,
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2018},
  pages = {586--595},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html},
  doi = {10.1109/CVPR.2018.00068},
}
@inproceedings{hessel2021clipscore,
    title = "{CLIPS}core: A Reference-free Evaluation Metric for Image Captioning",
    author = "Hessel, Jack  and
      Holtzman, Ari  and
      Forbes, Maxwell  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.595",
    doi = "10.18653/v1/2021.emnlp-main.595",
    pages = "7514--7528",
    abstract = "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
}
@inproceedings{zhai2023siglip,
    author    = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
    title     = {Sigmoid Loss for Language Image Pre-Training},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {11975-11986},
    url       = {https://doi.org/10.1109/ICCV51070.2023.01100},
    doi       = {10.1109/ICCV51070.2023.01100},
}
@inproceedings{hsu2021scicap,
    title = "{S}ci{C}ap: Generating Captions for Scientific Figures",
    author = "Hsu, Ting-Yao  and
      Giles, C Lee  and
      Huang, Ting-Hao",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.277",
    doi = "10.18653/v1/2021.findings-emnlp.277",
    pages = "3258--3264",
    abstract = "Researchers use figures to communicate rich, complex information in scientific papers. The captions of these figures are critical to conveying effective messages. However, low-quality figure captions commonly occur in scientific articles and may decrease understanding. In this paper, we propose an end-to-end neural framework to automatically generate informative, high-quality captions for scientific figures. To this end, we introduce SCICAP, a large-scale figure-caption dataset based on computer science arXiv papers published between 2010 and 2020. After pre-processing {--} including figure-type classification, sub-figure identification, text normalization, and caption text selection {--} SCICAP contained more than two million figures extracted from over 290,000 papers. We then established baseline models that caption graph plots, the dominant (19.2{\%}) figure type. The experimental results showed both opportunities and steep challenges of generating captions for scientific figures.",
}
@inproceedings{karishma2023aclfig,
  author       = {Zeba Karishma and
                  Shaurya Rohatgi and
                  Kavya Shrinivas Puranik and
                  Jian Wu and
                  C. Lee Giles},
  editor       = {Amir Pouran Ben Veyseh and
                  Franck Dernoncourt and
                  Thien Huu Nguyen and
                  Viet Dack Lai},
  title        = {{ACL-Fig}: A Dataset for Scientific Figure Classification},
  booktitle    = {Proceedings of the Workshop on Scientific Document Understanding co-located
                  with 37th {AAAI} Conference on Artificial Inteligence ({AAAI} 2023),
                  Remote, February 14, 2023},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {3656},
  publisher    = {CEUR-WS.org},
  year         = {2023},
  url          = {https://ceur-ws.org/Vol-3656/paper2.pdf},
}
@inproceedings{rodriguez2023ocrvqgan,
author = {J. A. Rodriguez and D. Vazquez and I. Laradji and M. Pedersoli and P. Rodriguez},
booktitle = {2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
title = {{OCR-VQGAN}: Taming Text-within-Image Generation},
year = {2023},
volume = {},
issn = {},
pages = {3678-3687},
abstract = {Synthetic image generation has recently experienced significant improvements in domains such as natural image or art generation. However, the problem of figure and diagram generation remains unexplored. A challenging aspect of generating figures and diagrams is effectively rendering readable texts within the images. To alleviate this problem, we present OCR-VQGAN, an image encoder, and decoder that leverages OCR pre-trained features to optimize a text perceptual loss, encouraging the architecture to preserve high-fidelity text and diagram structure. To explore our approach, we introduce the Paper2Fig100k dataset, with over 100k images of figures and texts from research papers. The figures show architecture diagrams and methodologies of articles available at arXiv.org from fields like artificial intelligence and computer vision. Figures usually include text and discrete objects, e.g., boxes in a diagram, with lines and arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by conducting several experiments on the task of figure reconstruction. Additionally, we explore the qualitative and quantitative impact of weighting different perceptual metrics in the overall loss function. We release code, models, and dataset at https://github.com/joanrod/ocr-vqgan.},
keywords = {measurement;computer vision;shape;image synthesis;optical character recognition;computer architecture;rendering (computer graphics)},
doi = {10.1109/WACV56688.2023.00368},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV56688.2023.00368},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jan}
}
@mastersthesis{kirsch2010detexify,
  author  = {Kirsch, Daniel},
  title   = {Detexify: Recognition of hand-drawn {LaTeX} symbols},
  school  = {University of Münster},
  address = {Münster, Germany},
  url     = {http://danielkirs.ch/thesis.pdf},
  type    = {Diploma thesis},
  year    = 2010,
  month   = oct
}
@misc{touvron2023llama,
      title={{LLaMA}: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama2,
      title={{LLaMA 2}: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{roziere2023code,
      title={Code {LLaMA}: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2023},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{guo2024deepseekcoder,
      title={{DeepSeek-Coder}: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}, 
      author={Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
      year={2024},
      eprint={2401.14196},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@inproceedings{lester2021power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}
@misc{zhang2024tinyllama,
      title={{TinyLlama}: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{chen2023pali,
title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=mWVoBz4W0u}
}
@misc{chen2023pali3,
      title={Pa{LI}-3 Vision Language Models: Smaller, Faster, Stronger}, 
      author={Xi Chen and Xiao Wang and Lucas Beyer and Alexander Kolesnikov and Jialin Wu and Paul Voigtlaender and Basil Mustafa and Sebastian Goodman and Ibrahim Alabdulmohsin and Piotr Padlewski and Daniel Salz and Xi Xiong and Daniel Vlasic and Filip Pavetic and Keran Rong and Tianli Yu and Daniel Keysers and Xiaohua Zhai and Radu Soricut},
      year={2023},
      eprint={2310.09199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@InProceedings{tong2024eyes,
    author    = {Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
    title     = {Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal {LLMs}},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    url       = {https://doi.org/10.1109/CVPR52733.2024.00914},
    doi       = {10.1109/CVPR52733.2024.00914},
    month     = {June},
    year      = {2024},
    pages     = {9568-9578}
}
@misc{chen2023minigptv2,
      title={{MiniGPT}-v2: large language model as a unified interface for vision-language multi-task learning}, 
      author={Jun Chen and Deyao Zhu and Xiaoqian Shen and Xiang Li and Zechun Liu and Pengchuan Zhang and Raghuraman Krishnamoorthi and Vikas Chandra and Yunyang Xiong and Mohamed Elhoseiny},
      year={2023},
      eprint={2310.09478},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2023improved,
      title={Improved Baselines with Visual Instruction Tuning},
      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
      year={2023},
      eprint={2310.03744},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{dai2023instructblip,
title={Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=vvoWPYqZJA}
}
@inproceedings{brooks2023pix2pix,
    author    = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
    title     = {{InstructPix2Pix}: Learning To Follow Image Editing Instructions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {18392-18402},
    url       = {https://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html},
    doi       = {10.1109/CVPR52729.2023.01764},
}
@inproceedings{loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}
@inproceedings{Rajbhandari2020zero,
author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
title = {{ZeRO}: memory optimizations toward training trillion parameter models},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world's largest language model at the time (17B parameters) with record breaking accuracy.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {20},
numpages = {16},
location = {Atlanta, Georgia},
series = {SC '20}
}
@software{together2023redpajama,
  author = {Together Computer},
  title = {{RedPajama}: An Open Source Recipe to Reproduce {LLaMA} training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
@article{li2023starcoder,
title={{StarCoder}: may the source be with you!},
author={Raymond Li and Loubna Ben allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia LI and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Joel Lamy-Poirier and Joao Monteiro and Nicolas Gontier and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Ben Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason T Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Urvashi Bhattacharyya and Wenhao Yu and Sasha Luccioni and Paulo Villegas and Fedor Zhdanov and Tony Lee and Nadav Timor and Jennifer Ding and Claire S Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro Von Werra and Harm de Vries},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=KoFOg41haE},
note={Reproducibility Certification}
}
@inproceedings{deng2017markup,
  title =    {Image-to-Markup Generation with Coarse-to-Fine Attention},
  author =       {Yuntian Deng and Anssi Kanervisto and Jeffrey Ling and Alexander M. Rush},
  booktitle =    {Proceedings of the 34th International Conference on Machine Learning},
  pages =    {980--989},
  year =    {2017},
  editor =    {Precup, Doina and Teh, Yee Whye},
  volume =    {70},
  series =    {Proceedings of Machine Learning Research},
  month =    {06--11 Aug},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v70/deng17a/deng17a.pdf},
  url =    {https://proceedings.mlr.press/v70/deng17a.html},
  abstract =    {We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.}
}
@misc{rodriguez2023starvector,
      title={{StarVector}: Generating Scalable Vector Graphics Code from Images}, 
      author={Juan A. Rodriguez and Shubham Agarwal and Issam H. Laradji and Pau Rodriguez and David Vazquez and Christopher Pal and Marco Pedersoli},
      year={2023},
      eprint={2312.11556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{zhu2024samvg,
  author={Zhu, Haokun and Ian Chong, Juang and Hu, Teng and Yi, Ran and Lai, Yu-Kun and Rosin, Paul L.},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={{SAMVG}: A Multi-Stage Image Vectorization Model with the Segment-Anything Model}, 
  year={2024},
  volume={},
  number={},
  pages={4350-4354},
  url={https://ieeexplore.ieee.org/document/10447396},
  keywords={Graphics;Deep learning;Image segmentation;Filtering;Computational modeling;Signal processing;Vectors;Image Vectorization;Vector Graphics;Image Segmentation;Computer Graphics;Segment-Anything Model},
  doi={10.1109/ICASSP48485.2024.10447396}
}
@inproceedings{ma2022live,
    author    = {Ma, Xu and Zhou, Yuqian and Xu, Xingqian and Sun, Bin and Filev, Valerii and Orlov, Nikita and Fu, Yun and Shi, Humphrey},
    title     = {Towards Layer-Wise Image Vectorization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16314-16323},
    url       = {https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Towards_Layer-Wise_Image_Vectorization_CVPR_2022_paper.html},
    doi       = {10.1109/CVPR52688.2022.01583},
}
@inproceedings{ellis2018drawing,
  author={Kevin Ellis and Daniel Ritchie and Armando Solar-Lezama and Josh Tenenbaum},
  title={Learning to Infer Graphics Programs from Hand-Drawn Images},
  year={2018},
  cdate={1514764800000},
  pages={6062-6071},
  url={http://papers.nips.cc/paper/7845-learning-to-infer-graphics-programs-from-hand-drawn-images},
  booktitle={Thirty-second Conference on Neural Information Processing Systems},
}
@misc{blecher2020pix2tex,
  title = {pix2tex: Using a {ViT} to convert images of equations into {LaTeX} code},
  copyright = {MIT},
  url = {https://github.com/lukas-blecher/LaTeX-OCR},
  urldate = {2023-02-09},
  author = {Blecher, Lukas},
  month = dec,
  year = {2020},
  note = {GitHub repository},
}
@article{tian2024vector,
  author={Tian, Xingze and Günther, Tobias},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={A Survey of Smooth Vector Graphics: Recent Advances in Representation, Creation, Rasterization, and Image Vectorization}, 
  year={2024},
  volume={30},
  number={3},
  pages={1652-1671},
  keywords={Graphics;Image color analysis;Shape;Mathematical models;Rendering (computer graphics);Splines (mathematics);Solids;Vector graphics;diffusion curves;gradient meshes;survey},
  doi={10.1109/TVCG.2022.3220575}
}
@inproceedings{reddy2021im2vec,
    author    = {Reddy, Pradyumna},
    title     = {{Im2Vec}: Synthesizing Vector Graphics Without Vector Supervision},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2021},
    pages     = {2124-2133},
    url       = {https://openaccess.thecvf.com/content/CVPR2021W/SketchDL/html/Reddy_Im2Vec_Synthesizing_Vector_Graphics_Without_Vector_Supervision_CVPRW_2021_paper.html},
    doi       = {10.1109/CVPRW53098.2021.00241},
}
@phdthesis{diebel2008bayes,
author = {Diebel, James Richard},
advisor = {Thrun, Sebastian},
title = {Bayesian image vectorization: The probabilistic inversion of vector image rasterization},
year = {2008},
isbn = {9780549852773},
school = {Stanford University},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {A common and time-consuming task in the graphic design and print industries is the conversion of bitmap images, which have a fixed resolution and may not be freely scaled, into vector art, in which the shapes in the scene are described by combinations of geometric primitives such as line segments, elliptical arcs, and B\'{e}zier curves. In addition to being scalable, which facilitates crisp high-quality printing, vector art is preferred because its abstract representation makes subsequent editing of the composition much easier than is possible with pixel-based representations. A recent survey of graphic designers and print professionals (conducted as part of this work), indicates that more than 10 million bitmap images are converted to vector form in the U.S. every year and that 65\% of those are converted by manually redrawing (tracing) the image. This thesis presents a new automatic vectorization algorithm based on an accurate and differentiable generative model of the anti-aliased rasterization process used to generate most of the bitmap images that are candidates for vectorization. Bayes' theorem is applied to find the maximum a posteriori (MAP) estimate of the vector image given the original bitmap image, an appropriate generative measurement model, and a well-designed prior distribution over vector images. Posing the problem in this manner results in a highly non-convex mixed continuous-discrete optimization problem. Using the parametrization developed in this work, a typical image results in tens of thousands of continuous variables and hundreds of thousands of discrete variables. The problem is solved by decomposing the optimization into a mostly-discrete phase, which determines the number of shapes present in the image, their colors, their topological arrangement, and rough estimates of the location of the boundaries between them, followed by a continuous phase, which determines the precise locations of the shape boundaries. An optional third step fits the shape boundaries with third-order B\'{e}zier curves. The discrete phase is solved using a modified local agglomerative approach. The continuous variables are optimized using a non-linear conjugate gradient method. For computational complexity reasons, non-intersection constraints are not imposed directly; rather, countermeasures are put into place to correct detected intersections once they are found. The results of the present work compare very favorably with the current state of the art, both qualitatively and quantitatively. In order to test the algorithm using real-world images, a web-based application was developed to make this tool available to the public. During the five months that the site was online, it drew more than 1.5 million visits from 199 countries. All told, more than 1.3 million images were processed.},
note = {AAI3332816}
}
@inproceedings{carlier2020deepsvg,
 author = {Carlier, Alexandre and Danelljan, Martin and Alahi, Alexandre and Timofte, Radu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16351--16361},
 publisher = {Curran Associates, Inc.},
 title = {{DeepSVG}: A Hierarchical Generative Network for Vector Graphics Animation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/bcf9d6bd14a2095866ce8c950b702341-Paper.pdf},
 volume = {33},
 year = {2020}
}
@article{li2020diffvg,
author = {Li, Tzu-Mao and Luk\'{a}\v{c}, Michal and Gharbi, Micha\"{e}l and Ragan-Kelley, Jonathan},
title = {Differentiable vector graphics rasterization for editing and learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417871},
doi = {10.1145/3414685.3417871},
abstract = {We introduce a differentiable rasterizer that bridges the vector graphics and raster image domains, enabling powerful raster-based loss functions, optimization procedures, and machine learning techniques to edit and generate vector content. We observe that vector graphics rasterization is differentiable after pixel prefiltering. Our differentiable rasterizer offers two prefiltering options: an analytical prefiltering technique and a multisampling anti-aliasing technique. The analytical variant is faster but can suffer from artifacts such as conflation. The multisampling variant is still efficient, and can render high-quality images while computing unbiased gradients for each pixel with respect to curve parameters.We demonstrate that our rasterizer enables new applications, including a vector graphics editor guided by image metrics, a painterly rendering algorithm that fits vector primitives to an image by minimizing a deep perceptual loss function, new vector graphics editing algorithms that exploit well-known image processing methods such as seam carving, and deep generative models that generate vector content from raster-only supervision under a VAE or GAN training objective.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {193},
numpages = {15},
keywords = {vector graphics, image vectorization, differentiable rendering}
}
@article{sun2007vectorization,
author = {Sun, Jian and Liang, Lin and Wen, Fang and Shum, Heung-Yeung},
title = {Image vectorization using optimized gradient meshes},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1276377.1276391},
doi = {10.1145/1276377.1276391},
abstract = {Recently, gradient meshes have been introduced as a powerful vector graphics representation to draw multicolored mesh objects with smooth transitions. Using tools from Abode Illustrator and Corel CorelDraw, a user can manually create gradient meshes even for photo-realistic vector arts, which can be further edited, stylized and animated.In this paper, we present an easy-to-use interactive tool, called optimized gradient mesh, to semi-automatically and quickly create gradient meshes from a raster image. We obtain the optimized gradient mesh by formulating an energy minimization problem. The user can also interactively specify a few vector lines to guide the mesh generation. The resulting optimized gradient mesh is an editable and scalable mesh that otherwise would have taken many hours for a user to manually create.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {11–es},
numpages = {8}
}
@inproceedings{lopes2019svgvae,
author = {Lopes, Raphael Gontijo and Ha, David and Eck, Douglas and Shlens, Jonathon},
title = {A Learned Representation for Scalable Vector Graphics},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019},
url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Lopes_A_Learned_Representation_for_Scalable_Vector_Graphics_ICCV_2019_paper.html},
doi = {10.1109/ICCV.2019.00802},
}
@inproceedings{ganin2018program,
  title = 	 {Synthesizing Programs for Images using Reinforced Adversarial Learning},
  author =       {Ganin, Yaroslav and Kulkarni, Tejas and Babuschkin, Igor and Eslami, S. M. Ali and Vinyals, Oriol},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1666--1675},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/ganin18a/ganin18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/ganin18a.html},
  abstract = 	 {Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator’s output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.}
}
@article{wu2020math,
author={Wu, Jin-Wen
and Yin, Fei
and Zhang, Yan-Ming
and Zhang, Xu-Yao
and Liu, Cheng-Lin},
title={Handwritten Mathematical Expression Recognition via Paired Adversarial Learning},
journal={International Journal of Computer Vision},
year={2020},
month={Nov},
day={01},
volume={128},
number={10},
pages={2386-2401},
abstract={Recognition of handwritten mathematical expressions (MEs) is an important problem that has wide applications in practice. Handwritten ME recognition is challenging due to the variety of writing styles and ME formats. As a result, recognizers trained by optimizing the traditional supervision loss do not perform satisfactorily. To improve the robustness of the recognizer with respect to writing styles, in this work, we propose a novel paired adversarial learning method to learn semantic-invariant features. Specifically, our proposed model, named PAL-v2, consists of an attention-based recognizer and a discriminator. During training, handwritten MEs and their printed templates are fed into PAL-v2 simultaneously. The attention-based recognizer is trained to learn semantic-invariant features with the guide of the discriminator. Moreover, we adopt a convolutional decoder to alleviate the vanishing and exploding gradient problems of RNN-based decoder, and further, improve the coverage of decoding with a novel attention method. We conducted extensive experiments on the CROHME dataset to demonstrate the effectiveness of each part of the method and achieved state-of-the-art performance.},
issn={1573-1405},
doi={10.1007/s11263-020-01291-5},
url={https://doi.org/10.1007/s11263-020-01291-5}
}
@inproceedings{suzuki2003infty,
author = {Suzuki, Masakazu and Tamari, Fumikazu and Fukuda, Ryoji and Uchida, Seiichi and Kanahori, Toshihiro},
title = {{INFTY}: an integrated {OCR} system for mathematical documents},
year = {2003},
isbn = {1581137249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/958220.958239},
doi = {10.1145/958220.958239},
abstract = {An integrated OCR system for mathematical documents, called INFTY, is presented. INFTY consists of four procedures, i.e., layout analysis, character recognition, structure analysis of mathematical expressions, and manual error correction. In those procedures, several novel techniques are utilized for better recognition performance. Experimental results on about 500 pages of mathematical documents showed high character recognition rates on both mathematical expressions and ordinary texts, and sufficient performance on the structure analysis of the mathematical expressions.},
booktitle = {Proceedings of the 2003 ACM Symposium on Document Engineering},
pages = {95–104},
numpages = {10},
keywords = {character and symbol recognition, mathematical OCR, structure analysis of mathematical expressions},
location = {Grenoble, France},
series = {DocEng '03}
}
@article{wang2021latex,
author={Wang, Zelun
and Liu, Jyh-Charn},
title={Translating math formula images to {LaTeX} sequences using deep neural networks with sequence-level training},
journal={International Journal on Document Analysis and Recognition (IJDAR)},
year={2021},
month={Jun},
day={01},
volume={24},
number={1},
pages={63-75},
abstract={In this paper, we propose a deep neural network model with an encoder--decoder architecture that translates images of math formulas into their LaTeX markup sequences. The encoder is a convolutional neural network that transforms images into a group of feature maps. To better capture the spatial relationships of math symbols, the feature maps are augmented with 2D positional encoding before being unfolded into a vector. The decoder is a stacked bidirectional long short-term memory model integrated with the soft attention mechanism, which works as a language model to translate the encoder output into a sequence of LaTeX tokens. The neural network is trained in two steps. The first step is token-level training using the maximum likelihood estimation as the objective function. At completion of the token-level training, the sequence-level training objective function is employed to optimize the overall model based on the policy gradient algorithm from reinforcement learning. Our design also overcomes the exposure bias problem by closing the feedback loop in the decoder during sequence-level training, i.e., feeding in the predicted token instead of the ground truth token at every time step. The model is trained and evaluated on the IM2LATEX-100 K dataset and shows state-of-the-art performance on both sequence-based and image-based evaluation metrics.},
issn={1433-2825},
doi={10.1007/s10032-020-00360-2},
url={https://doi.org/10.1007/s10032-020-00360-2}
}
@inproceedings{zhang2017gru,
  author={Zhang, Jianshu and Du, Jun and Dai, Lirong},
  booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)}, 
  title={A {GRU}-Based Encoder-Decoder Approach with Attention for Online Handwritten Mathematical Expression Recognition}, 
  year={2017},
  volume={01},
  number={},
  pages={902-907},
  keywords={Decoding;Trajectory;Handwriting recognition;Logic gates;Character recognition;Recurrent neural networks;Hidden Markov models;Online Handwritten Mathematical Expression Recognition;Encoder-Decoder;Gated Recurrent Unit;Attention},
  doi={10.1109/ICDAR.2017.152}
}
@inproceedings{zhang2019improved,
author = {Zhang, Wei and Bai, Zhiqiang and Zhu, Yuesheng},
title = {An Improved Approach Based on {CNN-RNNs} for Mathematical Expression Recognition},
year = {2019},
isbn = {9781450371711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330393.3330410},
doi = {10.1145/3330393.3330410},
abstract = {Mathematical expression recognition (MER) in images is a challenging task due to formula symbol recognition and structured analysis. Optical character recognition (OCR) has been used in natural language recognition and many areas. However, it is difficult for OCR to recognize some special formula symbols and accurately confirm their positions in MER. In this paper, an improved end-to-end MER approach based on CNN-RNNs (convolutional neural network - recurrent neural networks) is proposed to optimize the processing for the formula symbol recognition and localization. In our proposed approach, we extract the mathematical expression features in the images by CNN and generate the mathematical expression by RNNs. In order to improve the acquisition of fuzzy or small symbolic feature information and get an accurate symbol position, we double the source images and extract image features with preprocessed images by CNN. And we add a double-attention mechanism between the encoder and decoder so that the symbol position can be obtained accurately. In addition, to prevent overfitting, a dropout layer is introduced to improve the generalization of the MER model. We do the experiment on IM2LATEX-100K dataset and obtain the comparative result. A BLEU accuracy of 88.42\% is achieved, which is better than other methods.},
booktitle = {Proceedings of the 2019 4th International Conference on Multimedia Systems and Signal Processing},
pages = {57–61},
numpages = {5},
keywords = {Double-Attention, Dropout, Image Features, Mathematical Expression Recognition},
location = {Guangzhou, China},
series = {ICMSSP '19}
}
@inproceedings{wang2020pdf2latex,
author = {Wang, Zelun and Liu, Jyh-Charn},
title = {{PDF2LaTeX}: A Deep Learning System to Convert Mathematical Documents from {PDF} to {LaTeX}},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419580},
doi = {10.1145/3395027.3419580},
abstract = {The mathematical contents of scientific publications in PDF format cannot be easily analyzed by regular PDF parsers and OCR tools. In this paper, we propose a novel OCR system called PDF2LaTeX, which extracts math expressions and text in both postscript and image-based PDF files and translates them into LaTeX markup. As a preprocessing step, PDF2LaTeX first renders a PDF file into its image format, and then uses projection profile cutting (PPC) to analyze the page layout. The analysis of math expressions and text is based on a series of deep learning algorithms. First, it uses a convolutional neural network (CNN) as a binary classifier to detect math image blocks based on visual features. Next, it uses a conditional random field (CRF) to detect math-text boundaries by incorporating semantics and context information. In the end, the system uses two different models based on a CNN-LSTM neural network architecture to translate image blocks of math expressions and plaintext into the LaTeX representations. For testing, we created a new dataset composed of 102 PDF pages collected from publications on arXiv.org and compared the performance between PDF2LaTeX and the state-of-the-art commercial software InftyReader. The experiment results showed that the proposed system achieved a better recognition accuracy (81.1\%) measured by the string edit distance between the predicted LaTeX and the ground truth.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {4},
numpages = {10},
keywords = {document analysis, deep learning, conditional random field, PDF-to-LaTeX conversion, Mathematical expressions},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}
@misc{lozhkov2024starcoder,
      title={{StarCoder} 2 and {The Stack} v2: The Next Generation}, 
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2024},
      eprint={2402.19173},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@InProceedings{sharma2024vision,
    author    = {Sharma, Pratyusha and Shaham, Tamar Rott and Baradad, Manel and Fu, Stephanie and Rodriguez-Munoz, Adrian and Duggal, Shivam and Isola, Phillip and Torralba, Antonio},
    title     = {A Vision Check-up for Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    url       = {https://doi.org/10.1109/CVPR52733.2024.01366},
    doi       = {10.1109/CVPR52733.2024.01366},
    month     = {June},
    year      = {2024},
    pages     = {14410-14419}
}
@inproceedings{xu2022codeeval,
  author       = {Frank F. Xu and
                  Uri Alon and
                  Graham Neubig and
                  Vincent Josua Hellendoorn},
  editor       = {Swarat Chaudhuri and
                  Charles Sutton},
  title        = {A systematic evaluation of large language models of code},
  booktitle    = {MAPS@PLDI 2022: 6th {ACM} {SIGPLAN} International Symposium on Machine
                  Programming, San Diego, CA, USA, 13 June 2022},
  pages        = {1--10},
  publisher    = {{ACM}},
  year         = {2022},
  url          = {https://doi.org/10.1145/3520312.3534862},
  doi          = {10.1145/3520312.3534862},
}
@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{li2022alphacode,
author = {Yujia Li  and David Choi  and Junyoung Chung  and Nate Kushman  and Julian Schrittwieser  and Rémi Leblond  and Tom Eccles  and James Keeling  and Felix Gimeno  and Agustin Dal Lago  and Thomas Hubert  and Peter Choy  and Cyprien de Masson d’Autume  and Igor Babuschkin  and Xinyun Chen  and Po-Sen Huang  and Johannes Welbl  and Sven Gowal  and Alexey Cherepanov  and James Molloy  and Daniel J. Mankowitz  and Esme Sutherland Robson  and Pushmeet Kohli  and Nando de Freitas  and Koray Kavukcuoglu  and Oriol Vinyals },
title = {Competition-level code generation with {AlphaCode}},
journal = {Science},
volume = {378},
number = {6624},
pages = {1092-1097},
year = {2022},
doi = {10.1126/science.abq1158},
URL = {https://www.science.org/doi/abs/10.1126/science.abq1158},
eprint = {https://www.science.org/doi/pdf/10.1126/science.abq1158},
abstract = {Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.}
}
@inproceedings{fried2023incoder,
title={{InCoder}: A Generative Model for Code Infilling and Synthesis},
author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Scott Yih and Luke Zettlemoyer and Mike Lewis},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=hQwb-lbM6EL}
}
@inproceedings{zan2023lnlcode,
    title = "Large Language Models Meet {NL2Code}: A Survey",
    author = "Zan, Daoguang  and
      Chen, Bei  and
      Zhang, Fengji  and
      Lu, Dianjie  and
      Wu, Bingchao  and
      Guan, Bei  and
      Yongji, Wang  and
      Lou, Jian-Guang",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.411",
    doi = "10.18653/v1/2023.acl-long.411",
    pages = "7443--7464",
    abstract = "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are {``}Large Size, Premium Data, Expert Tuning{''}. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
}
@misc{zhang2023controllable,
      title={Controllable Text-to-Image Generation with {GPT-4}},
      author={Tianjun Zhang and Yi Zhang and Vibhav Vineet and Neel Joshi and Xin Wang},
      year={2023},
      eprint={2305.18583},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with {GPT-4}},
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{fu2023dreamsim,
title={{DreamSim}: Learning New Dimensions of Human Visual Similarity using Synthetic Data},
author={Stephanie Fu and Netanel Yakir Tamir and Shobhita Sundaram and Lucy Chai and Richard Zhang and Tali Dekel and Phillip Isola},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=DEiNSfh1k7}
}
@misc{mckinzie2024mm1,
      title={{MM1}: Methods, Analysis \& Insights from Multimodal {LLM} Pre-training}, 
      author={Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Ankur Jain and Hongyu Hè and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Guoli Yin and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang},
      year={2024},
      eprint={2403.09611},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{mellor2019unsupervised,
      title={Unsupervised Doodling and Painting with Improved {SPIRAL}}, 
      author={John F. J. Mellor and Eunbyung Park and Yaroslav Ganin and Igor Babuschkin and Tejas Kulkarni and Dan Rosenbaum and Andy Ballard and Theophane Weber and Oriol Vinyals and S. M. Ali Eslami},
      year={2019},
      eprint={1910.01007},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{openai2023gptv,
      title={{GPT-4V}(ision) system card},
      author={OpenAI},
      year={2023},
      url={https://cdn.openai.com/papers/GPTV_System_Card.pdf}
}
@misc{openai2023gpt4,
      title={{GPT-4} Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{anthropic2024claude,
      title={The {Claude} 3 Model Family: {Opus}, {Sonnet}, {Haiku}},
      author={Anthropic},
      year={2024},
      url={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}
}
@article{irwin2019latex,
author = {Irwin, Scott H.},
title = {Writing papers in economics using fake {LaTeX}},
journal = {Journal of Economic Surveys},
volume = {33},
number = {4},
pages = {1348-1356},
keywords = {Aesthetics, Economics, LaTeX, Papers, Productivity, Writing},
doi = {https://doi.org/10.1111/joes.12318},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12318},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12318},
abstract = {Abstract LaTeX is a very popular platform for writing papers in economics, mainly due to its superior aesthetics in print. The question is whether the aesthetic (and other) benefits of adopting LaTeX outweigh the opportunity costs of learning and using LaTeX, which can be high even for experienced users. FaKe LaTeX using Microsoft Word has a remarkably similar visual appearance to papers produced in native LaTeX and it is easy to learn and use. This low cost alternative may be appealing to many economists.},
year = {2019}
}
@article{knauf2014latex,
Author = {Knauff, Markus and Nejasmic, Jelica},
Title = {An Efficiency Comparison of Document Preparation Systems Used in Academic Research and Development},
Journal = {PLOS ONE},
Year = {2014},
Volume = {9},
Number = {12},
Month = {DEC 19},
DOI = {10.1371/journal.pone.0115069},
Article-Number = {e115069},
ISSN = {1932-6203},
Unique-ID = {WOS:000347186200011},
}
@inproceedings{madaan2023selfrefine,
title={Self-Refine: Iterative Refinement with Self-Feedback},
author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S37hOerQLB}
}
@inproceedings{eghbali2023bleu,
author = {Eghbali, Aryaz and Pradel, Michael},
title = {{CrystalBLEU}: Precisely and Efficiently Measuring the Similarity of Code},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556903},
doi = {10.1145/3551349.3556903},
abstract = {Recent years have brought a surge of work on predicting pieces of source code, e.g., for code completion, code migration, program repair, or translating natural language into code. All this work faces the challenge of evaluating the quality of a prediction w.r.t. some oracle, typically in the form of a reference solution. A common evaluation metric is the BLEU score, an n-gram-based metric originally proposed for evaluating natural language translation, but adopted in software engineering because it can be easily computed on any programming language and enables automated evaluation at scale. However, a key difference between natural and programming languages is that in the latter, completely unrelated pieces of code may have many common n-grams simply because of the syntactic verbosity and coding conventions of programming languages. We observe that these trivially shared n-grams hamper the ability of the metric to distinguish between truly similar code examples and code examples that are merely written in the same language. This paper presents CrystalBLEU, an evaluation metric based on BLEU, that allows for precisely and efficiently measuring the similarity of code. Our metric preserves the desirable properties of BLEU, such as being language-agnostic, able to handle incomplete or partially incorrect code, and efficient, while reducing the noise caused by trivially shared n-grams. We evaluate CrystalBLEU on two datasets from prior work and on a new, labeled dataset of semantically equivalent programs. Our results show that CrystalBLEU can distinguish similar from dissimilar code examples 1.9–4.5 times more effectively, when compared to the original BLEU score and a previously proposed variant of BLEU for code.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {28},
numpages = {12},
keywords = {Metric, Evaluation, BLEU},
location = {, Rochester, MI, USA, },
series = {ASE '22}
}
@inproceedings{papineni2002bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}
@inproceedings{stanchev2019eed,
    title = "{EED}: Extended Edit Distance Measure for Machine Translation",
    author = "Stanchev, Peter  and
      Wang, Weiyue  and
      Ney, Hermann",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5359",
    doi = "10.18653/v1/W19-5359",
    pages = "514--520",
    abstract = "Over the years a number of machine translation metrics have been developed in order to evaluate the accuracy and quality of machine-generated translations. Metrics such as BLEU and TER have been used for decades. However, with the rapid progress of machine translation systems, the need for better metrics is growing. This paper proposes an extension of the edit distance, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.",
}
@inproceedings{bińkowski2018kid,
title={Demystifying {MMD} {GAN}s},
author={Mikołaj Bińkowski and Dougal J. Sutherland and Michael Arbel and Arthur Gretton},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1lUOzWCW},
}
@article{orme2009maxdiffa,
  title = {{MaxDiff} Analysis: Simple Counting, Individual-Level Logit, and {HB}},
  author = {Bryan K. Orme},
  journal = {Sawtooth Software Research Paper Series},
  year = {2009},
  url = {https://content.sawtoothsoftware.com/assets/8e69929d-a089-4b93-a9f3-d4c64d156642},
}
@inproceedings{kiritchenko2017bws,
    title = "Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation",
    author = "Kiritchenko, Svetlana  and
      Mohammad, Saif",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2074",
    doi = "10.18653/v1/P17-2074",
    pages = "465--470",
    abstract = "Rating scales are a widely used method for data annotation; however, they present several challenges, such as difficulty in maintaining inter- and intra-annotator consistency. Best{--}worst scaling (BWS) is an alternative method of annotation that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of rating scales. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the rating scale.",
}
@inproceedings{kiritchenko2016bws,
    title = "Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best{--}Worst Scaling",
    author = "Kiritchenko, Svetlana  and
      Mohammad, Saif M.",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1095",
    doi = "10.18653/v1/N16-1095",
    pages = "811--817",
}
@book{louviere2015bws,
  place={Cambridge},
  title={Best-Worst Scaling: Theory, Methods and Applications},
  DOI={10.1017/CBO9781107337855},
  publisher={Cambridge University Press},
  author={Louviere, Jordan J. and Flynn, Terry N. and Marley, A. A. J.},
  year={2015}
}
@article{kayal2023table,
author={Kayal, Pratik
and Anand, Mrinal
and Desai, Harsh
and Singh, Mayank},
title={Tables to {LaTeX}: structure and content extraction from scientific tables},
journal={International Journal on Document Analysis and Recognition (IJDAR)},
year={2023},
month={Jun},
day={01},
volume={26},
number={2},
pages={121-130},
abstract={Scientific documents contain tables that list important information in a concise fashion. Structure and content extraction from tables embedded within PDF research documents is a very challenging task due to the existence of visual features like spanning cells and content features like mathematical symbols and equations. Most existing table structure identification methods tend to ignore these academic writing features. In this paper, we adapt the transformer-based language modeling paradigm for scientific table structure and content extraction. Specifically, the proposed model converts a tabular image to its corresponding LaTeX source code. Overall, we outperform the current state-of-the-art baselines and achieve an exact match accuracy of 70.35 and 49.69{\%} on table structure and content extraction, respectively. Further analysis demonstrates that the proposed models efficiently identify the number of rows and columns, the alphanumeric characters, the LaTeX tokens, and symbols.},
issn={1433-2825},
doi={10.1007/s10032-022-00420-9},
url={https://doi.org/10.1007/s10032-022-00420-9}
}
@inproceedings{desai2021table,
author="Desai, Harsh
and Kayal, Pratik
and Singh, Mayank",
editor="Llad{\'o}s, Josep
and Lopresti, Daniel
and Uchida, Seiichi",
title="{TabLeX}: A Benchmark Dataset for Structure and Content Information Extraction from Scientific Tables",
booktitle="Document Analysis and Recognition -- ICDAR 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="554--569",
abstract="Information Extraction (IE) from the tables present in scientific articles is challenging due to complicated tabular representations and complex embedded text. This paper presents TabLeX, a large-scale benchmark dataset comprising table images generated from scientific articles. TabLeX consists of two subsets, one for table structure extraction and the other for table content extraction. Each table image is accompanied by its corresponding LaTeX source code. To facilitate the development of robust table IE tools, TabLeX contains images in different aspect ratios and in a variety of fonts. Our analysis sheds light on the shortcomings of current state-of-the-art table extraction models and shows that they fail on even simple table images. Towards the end, we experiment with a transformer-based existing baseline to report performance scores. In contrast to the static benchmarks, we plan to augment this dataset with more complex and diverse tables at regular intervals.",
isbn="978-3-030-86331-9"
}
@inproceedings{moosavi2021scigen,
title={{SciGen}: a Dataset for Reasoning-Aware Text Generation from Scientific Tables},
author={Nafise Sadat Moosavi and Andreas R{\"u}ckl{\'e} and Dan Roth and Iryna Gurevych},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=Jul-uX7EV_I}
}
@inproceedings{lu2023scitab,
    title = "{SCITAB}: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
    author = "Lu, Xinyuan  and
      Pan, Liangming  and
      Liu, Qian  and
      Nakov, Preslav  and
      Kan, Min-Yen",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.483",
    doi = "10.18653/v1/2023.emnlp-main.483",
    pages = "7787--7813",
    abstract = "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
}
@inproceedings{masry2022chartqa,
    title = "{C}hart{QA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
    author = "Masry, Ahmed  and
      Do, Xuan Long  and
      Tan, Jia Qing  and
      Joty, Shafiq  and
      Hoque, Enamul",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.177",
    doi = "10.18653/v1/2022.findings-acl.177",
    pages = "2263--2279",
    abstract = "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
}
@inproceedings {methani2020plotqa,
author = {N. Methani and P. Ganguly and M. M. Khapra and P. Kumar},
booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
title = {{PlotQA}: Reasoning over Scientific Plots},
year = {2020},
volume = {},
issn = {},
pages = {1516-1525},
abstract = {Existing synthetic datasets (Figure QA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice, this is an unrealistic assumption because many questions require reasoning and thus have real-valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real-world plots. Specifically, we propose Plot QA with 28.9 million question-answer pairs over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary. Analysis of existing models on Plot QA reveals that they cannot deal with OOV questions: their overall accuracy on our dataset is in single digits. This is not surprising given that these models were not designed for such questions. As a step towards a more holistic model which can address fixed vocabulary as well as OOV questions, we propose a hybrid approach: Specific questions are answered by choosing the answer from a fixed vocabulary or by extracting it from a predicted bounding box in the plot, while other questions are answered with a table question-answering engine which is fed with a structured table generated by detecting visual elements from the image. On the existing DVQA dataset, our model has an accuracy of 58%, significantly improving on the highest reported accuracy of 46%. On Plot QA, our model has an accuracy of 22.52%, which is significantly better than state of the art models.},
keywords = {vocabulary;cognition;bars;numerical models;optical character recognition software;data mining;image color analysis},
doi = {10.1109/WACV45572.2020.9093523},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV45572.2020.9093523},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}
@inproceedings{huang2023summaries,
    title = "Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization",
    author = "Huang, Chieh-Yang  and
      Hsu, Ting-Yao  and
      Rossi, Ryan  and
      Nenkova, Ani  and
      Kim, Sungchul  and
      Chan, Gromit Yeuk-Yin  and
      Koh, Eunyee  and
      Giles, C Lee  and
      Huang, Ting-Hao",
    editor = "Keet, C. Maria  and
      Lee, Hung-Yi  and
      Zarrie{\ss}, Sina",
    booktitle = "Proceedings of the 16th International Natural Language Generation Conference",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.inlg-main.6",
    doi = "10.18653/v1/2023.inlg-main.6",
    pages = "80--92",
    abstract = "Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., {``}Figure 3 shows...{''}) into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and data are available at: https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.",
}
@inproceedings{ma2019results,
    title = "Results of the {WMT}19 Metrics Shared Task: Segment-Level and Strong {MT} Systems Pose Big Challenges",
    author = "Ma, Qingsong  and
      Wei, Johnny  and
      Bojar, Ond{\v{r}}ej  and
      Graham, Yvette",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5302",
    doi = "10.18653/v1/W19-5302",
    pages = "62--90",
    abstract = "This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less {``}metrics{''} and constitute submissions to the joint task with WMT19 Quality Estimation Task, {``}QE as a Metric{''}. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.",
}
@article{corey1998averaging,
author = {David M. Corey, William P. Dunlap and Michael J. Burke},
title = {Averaging Correlations: Expected Values and Bias in Combined Pearson rs and Fisher's z Transformations},
journal = {The Journal of General Psychology},
volume = {125},
number = {3},
pages = {245--261},
year = {1998},
publisher = {Routledge},
doi = {10.1080/00221309809595548},
url = {https://doi.org/10.1080/00221309809595548},
eprint = {https://doi.org/10.1080/00221309809595548}
}
@article{sava2006congruence,
author = {Lorenzo-Seva, Urbano and ten Berge, Jos M. F.},
title = {Tucker's Congruence Coefficient as a Meaningful Index of Factor Similarity},
journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
volume = {2},
number = {2},
pages = {57-64},
year = {2006},
doi = {10.1027/1614-2241.2.2.57},
url = {https://doi.org/10.1027/1614-2241.2.2.57},
eprint = {https://doi.org/10.1027/1614-2241.2.2.57},
}
@misc{zou2023representation,
      title={Representation Engineering: A Top-Down Approach to {AI} Transparency}, 
      author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{ding2022dists,
  author={Ding, Keyan and Ma, Kede and Wang, Shiqi and Simoncelli, Eero P.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Image Quality Assessment: Unifying Structure and Texture Similarity},
  year={2020},
  volume={44},
  number={5},
  pages={2567-2581},
  keywords={Visualization;Image quality;Distortion measurement;Nonlinear distortion;Indexes;Databases;Convolution;Image quality assessment;structure similarity;texture similarity;perceptual optimization},
  doi={10.1109/TPAMI.2020.3045810}
}
@manual{hobby2014metapost,
   author    = {John D. Hobby},
   title     = {{MetaPost}},
   subtitle  = {A User's Manual},
   url       = {https://tug.org/metapost},
   date      = {2014-05-21},
   year      = {2014}
}
@manual{zandt2007pstricks,
   author    = {van Zandt, Timothy},
   title     = {{PSTricks}: {PostScript} macros for Generic {TeX}},
   subtitle  = {User's Guide},
   url       = {https://tug.org/PSTricks},
   date      = {2007-12-10},
   year      = {2007}
}
@manual{asymptote2024hammerlindl,
   author    = {Andy Hammerlindl and John Bowman and Tom Prince},
   title     = {Asymptote: The Vector Graphics Language},
   url       = {https://asymptote.sourceforge.io},
   date      = {2024-03-25},
   year      = {2024}
}
@inproceedings{rafailov2023direct,
title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=HPuSIXJaa9}
}
@inproceedings{wu2021balanced,
title={Balanced {Chamfer} Distance as a Comprehensive Metric for Point Cloud Completion},
author={Tong Wu and Liang Pan and Junzhe Zhang and Tai WANG and Ziwei Liu and Dahua Lin},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=B46BjXrLidN}
}
@ARTICLE{wang2009mse,
  author={Wang, Zhou and Bovik, Alan C.},
  journal={IEEE Signal Processing Magazine},
  title={Mean squared error: Love it or leave it? A new look at Signal Fidelity Measures},
  year={2009},
  volume={26},
  number={1},
  pages={98-117},
  keywords={Signal processing;Pollution measurement;PSNR;Signal processing algorithms;Image processing;Distortion measurement;Pixel;Dynamic range;Signal design;Algorithm design and analysis},
  doi={10.1109/MSP.2008.930649}
}
@inproceedings{li2019sketch,
  author={Li, Mengtian and Lin, Zhe and Mech, Radomir and Yumer, Ersin and Ramanan, Deva},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title={{Photo-Sketching}: Inferring Contour Drawings From Images},
  year={2019},
  volume={},
  number={},
  pages={1403-1412},
  keywords={Image edge detection;Training;Task analysis;Visualization;Generators;Generative adversarial networks;Detectors},
  doi={10.1109/WACV.2019.00154}
}
@article{sangkloy2016sketch,
author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
title = {The sketchy database: learning to retrieve badly drawn bunnies},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925954},
doi = {10.1145/2897824.2925954},
abstract = {We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {119},
numpages = {12},
keywords = {deep learning, image synthesis, siamese network, sketch-based image retrieval, triplet network}
}
@article{paul2023instruct,
  author = {Paul, Sayak},
  title = {Instruction-tuning {Stable Diffusion} with {InstructPix2Pix}},
  journal = {Hugging Face Blog},
  year = {2023},
  note = {https://huggingface.co/blog/instruction-tuning-sd},
}
@article{mccoy2023raven,
    title = "How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using {RAVEN}",
    author = "McCoy, R. Thomas  and
      Smolensky, Paul  and
      Linzen, Tal  and
      Gao, Jianfeng  and
      Celikyilmaz, Asli",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.38",
    doi = "10.1162/tacl_a_00567",
    pages = "652--670",
    abstract = "Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure{---}e.g., individual dependencies{---}text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model{'}s test set. For larger-scale structure{---}e.g., overall sentence structure{---}model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that GPT-2{'}s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).",
}
@inproceedings{carlini2023quantifying,
title={Quantifying Memorization Across Neural Language Models},
author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TatRHT_1cK}
}
@inproceedings{raunak2022memo,
    title = "Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks",
    author = "Raunak, Vikas  and
      Menezes, Arul",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.378",
    pages = "5153--5162",
    abstract = "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
}
@inproceedings{meehan2020test,
  title = 	 {A Three Sample Hypothesis Test for Evaluating Generative Models},
  author =       {Meehan, Casey and Chaudhuri, Kamalika and Dasgupta, Sanjoy},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3546--3556},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/meehan20a/meehan20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/meehan20a.html},
  abstract = 	 {Detecting overfitting in generative models is an important challenge in machine learning. In this work, we formalize a form of overfitting that we call {\em{data-copying}} – where the generative model memorizes and outputs training samples or small variations thereof. We provide a three sample test for detecting data-copying that uses the training set, a separate sample from the target distribution, and a generated sample from the model, and study the performance of our test on several canonical models and datasets.}
}
@inproceedings{daras2022discovering,
title={Discovering the Hidden Vocabulary of {DALLE}-2},
author={Giannis Daras and Alex Dimakis},
booktitle={NeurIPS 2022 Workshop on Score-Based Methods},
year={2022},
url={https://openreview.net/forum?id=jxeSZaVzpmg}
}
@article{borji2023deepfakes,
title = {Qualitative failures of image generation models and their application in detecting deepfakes},
journal = {Image and Vision Computing},
volume = {137},
pages = {104771},
year = {2023},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2023.104771},
url = {https://www.sciencedirect.com/science/article/pii/S0262885623001452},
author = {Ali Borji},
keywords = {Generative models, Image and video generation, Qualitative failures, Deepfakes, Image forensics, Object and scene recognition, Neural networks, Deep learning},
abstract = {The remarkable advancement of image and video generation models has led to the creation of exceptionally realistic content, posing challenges in differentiating between genuine and fabricated instances in numerous scenarios. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting generated images and deepfakes. The prevalence of deepfakes in today’s society is a serious concern, and our findings can help mitigate their negative impact. In order to support research in this field, a collection of instances where models have failed is made available at here.}
}
@inproceedings{zhang2023custom,
author = {Zhang, Peiying and Zhao, Nanxuan and Liao, Jing},
title = {Text-Guided Vector Graphics Customization},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610548.3618232},
doi = {10.1145/3610548.3618232},
abstract = {Vector graphics are widely used in digital art and valued by designers for their scalability and layer-wise topological properties. However, the creation and editing of vector graphics necessitate creativity and design expertise, leading to a time-consuming process. In this paper, we propose a novel pipeline that generates high-quality customized vector graphics based on textual prompts while preserving the properties and layer-wise information of a given exemplar SVG. Our method harnesses the capabilities of large pre-trained text-to-image models. By fine-tuning the cross-attention layers of the model, we generate customized raster images guided by textual prompts. To initialize the SVG, we introduce a semantic-based path alignment method that preserves and transforms crucial paths from the exemplar SVG. Additionally, we optimize path parameters using both image-level and vector-level losses, ensuring smooth shape deformation while aligning with the customized raster image. We extensively evaluate our method using multiple metrics from vector-level, image-level, and text-level perspectives. The evaluation results demonstrate the effectiveness of our pipeline in generating diverse customizations of vector graphics with exceptional quality. The project page is https://intchous.github.io/SVGCustomization.},
booktitle = {SIGGRAPH Asia 2023 Conference Papers},
articleno = {54},
numpages = {11},
keywords = {Diffusion Model, Image Editing, Image Vectorization, SVG, Vector Graphics},
location = {, Sydney, NSW, Australia, },
series = {SA '23}
}
@misc{lv2023kosmos25,
      title={Kosmos-2.5: A Multimodal Literate Model}, 
      author={Tengchao Lv and Yupan Huang and Jingye Chen and Lei Cui and Shuming Ma and Yaoyao Chang and Shaohan Huang and Wenhui Wang and Li Dong and Weiyao Luo and Shaoxiang Wu and Guoxin Wang and Cha Zhang and Furu Wei},
      year={2023},
      eprint={2309.11419},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{chaslot2008progressive,
  author={Guillaume M. J-B. Chaslot and Mark H. M. Winands and H. Jaap Van Den Herik and Jos W. H. M. Uiterwijk and Bruno Bouzy},
  title={Progressive Strategies For {Monte-Carlo} Tree Search},
  journal={New Mathematics and Natural Computation (NMNC)},
  year=2008,
  volume={4},
  number={03},
  pages={343-357},
  month={},
  keywords={Monte-Carlo Tree Search; heuristic search; Computer Go},
  doi={10.1142/S1793005708001094},
  abstract={Monte-Carlo Tree Search (MCTS) is a new best-first search guided by the results of Monte-Carlo simulations. In this article, we introduce twoprogressive strategiesfor MCTS, called progressive bias and progressive unpruning. They enable the use of relatively time-expensive heuristic knowledge without speed reduction. Progressive bias directs the search according to heuristic knowledge. Progressive unpruning first reduces the branching factor, and then increases it gradually again. Experiments assess that the two progressive strategies significantly improve the level of our Go programMango. Moreover, we see that the combination of both strategies performs even better on larger board sizes.},
  url={https://ideas.repec.org/a/wsi/nmncxx/v04y2008i03ns1793005708001094.html}
}
@misc{beyer2024paligemma,
      title={{PaliGemma}: A versatile 3B {VLM} for transfer}, 
      author={Lucas Beyer and Andreas Steiner and André Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bošnjak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai},
      year={2024},
      eprint={2407.07726},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.07726}, 
}
@inproceedings{karamcheti2024prismatic,
  title={Prismatic {VLM}s: Investigating the Design Space of Visually-Conditioned Language Models},
  author={Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=6FXtu8clyp}
}
@inproceedings{xu2024contrastive,
title={Contrastive Preference Optimization: Pushing the Boundaries of {LLM} Performance in Machine Translation},
author={Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=51iwkioZpn}
}
