\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adel et~al.(2017)Adel, Valera, Ghahramani, and Weller]{adel2019}
Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller.
\newblock One-network adversarial fairness.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2017.

\bibitem[Agarwal et~al.(2018)Agarwal, Beygelzimer, Dudik, Langford, and
  Wallach]{agarwal_reductions_approach}
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna
  Wallach.
\newblock A reductions approach to fair classification.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Alabdulmohsin and Lucic(2021)]{alabdulmohsin2021near}
Ibrahim~M Alabdulmohsin and Mario Lucic.
\newblock A near-optimal algorithm for debiasing trained machine learning
  models.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Albiero et~al.(2020)Albiero, KS, Vangara, Zhang, King, and
  Bowyer]{albiero2020analysis}
Vitor Albiero, Krishnapriya KS, Kushal Vangara, Kai Zhang, Michael~C King, and
  Kevin~W Bowyer.
\newblock Analysis of gender inequality in face recognition accuracy.
\newblock In \emph{IEEE/CVF Winter Conference on Applications of Computer
  Vision Workshops (WACV)}, 2020.

\bibitem[Alvi et~al.(2019)Alvi, Zisserman, and Nellaker]{alvi2019a}
Mohsan Alvi, Andrew Zisserman, and Christoffer Nellaker.
\newblock Turning a blind eye: Explicit removal of biases and variation from
  deep neural network embeddings.
\newblock In \emph{European Conference on Computer Vision (ECCV) - Workshop},
  2019.

\bibitem[Balakrishnan et~al.(2020)Balakrishnan, Xiong, Xia, and
  Perona]{balakrishnan2021towards}
Guha Balakrishnan, Yuanjun Xiong, Wei Xia, and Pietro Perona.
\newblock Towards causal benchmarking of bias in face analysis algorithms.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Barocas and Selbst(2016)]{barocas2016big}
Solon Barocas and Andrew~D Selbst.
\newblock Big data's disparate impact.
\newblock \emph{Calif. L. Rev.}, 104:\penalty0 671--732, 2016.

\bibitem[Bendekgey and Sudderth(2021)]{bendekgey2021scalable}
Harry Bendekgey and Erik~B. Sudderth.
\newblock Scalable and stable surrogates for flexible classifiers with fairness
  constraints.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Bent(2019)]{bent2019algorithmic}
Jason~R Bent.
\newblock Is algorithmic affirmative action legal?
\newblock \emph{Geo. LJ}, 108:\penalty0 803--853, 2019.

\bibitem[Beutel et~al.(2017)Beutel, Chen, Zhao, and Chi]{Beutel2017DataDA}
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed~H Chi.
\newblock Data decisions and theoretical implications when adversarially
  learning fair representations.
\newblock \emph{FAT/ML 2017 -- Workshop at KDD}, 2017.

\bibitem[Beutel et~al.(2019)Beutel, Chen, Doshi, Qian, Wei, Wu, Heldt, Zhao,
  Hong, Chi, and Goodrow]{beutel2019fairness}
Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li~Wei, Yi~Wu, Lukasz Heldt,
  Zhe Zhao, Lichan Hong, Ed~Huai-hsin Chi, and Cristos Goodrow.
\newblock Fairness in recommendation ranking through pairwise comparisons.
\newblock In \emph{ACM International Conference on Knowledge Discovery and Data
  Mining (KDD)}, 2019.

\bibitem[Buolamwini and Gebru(2018)]{gendershades}
Joy Buolamwini and Timnit Gebru.
\newblock Gender shades: Intersectional accuracy disparities in commercial
  gender classification.
\newblock In \emph{Conference on Fairness, Accountability and Transparency
  (FAccT)}, 2018.

\bibitem[Burns et~al.(2018)Burns, Hendricks, Saenko, Darrell, and
  Rohrbach]{hendricks2018}
Kaylee Burns, Lisa~Anne Hendricks, Kate Saenko, Trevor Darrell, and Anna
  Rohrbach.
\newblock Women also snowboard: Overcoming bias in captioning models.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2018.

\bibitem[Caton and Haas(2020)]{caton2020}
Simon Caton and Christian Haas.
\newblock Fairness in machine learning: A survey.
\newblock \emph{arXiv preprint arXiv:2010.04053}, 2020.

\bibitem[Denton et~al.(2019)Denton, Hutchinson, Mitchell, Gebru, and
  Zaldivar]{denton2019image}
Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, and Andrew
  Zaldivar.
\newblock Image counterfactual sensitivity analysis for detecting unintended
  bias.
\newblock \emph{CVPR Workshop on Fairness Accountability Transparency and
  Ethics in Computer Vision. arXiv preprint arXiv:1906.06439}, 2019.

\bibitem[Donini et~al.(2018)Donini, Oneto, Ben-David, Shawe-Taylor, and
  Pontil]{donini2018}
Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano
  Pontil.
\newblock Empirical risk minimization under fairness constraints.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Du et~al.(2020)Du, Yang, Zou, and Hu]{du2020}
Mengnan Du, Fan Yang, Na~Zou, and Xia Hu.
\newblock Fairness in deep learning: A computational perspective.
\newblock In \emph{IEEE Intelligent Systems}, 2020.

\bibitem[Dwork et~al.(2012)Dwork, Hardt, Pitassi, Reingold, and Zemel]{fta2012}
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel.
\newblock Fairness through awareness.
\newblock In \emph{Innovations in Theoretical Computer Science Conference
  (ITCS)}, 2012.

\bibitem[Dwork et~al.(2018)Dwork, Immorlica, Kalai, and Leiserson]{dwork18a}
Cynthia Dwork, Nicole Immorlica, Adam~Tauman Kalai, and Max Leiserson.
\newblock Decoupled classifiers for group-fair and efficient machine learning.
\newblock In \emph{Conference on Fairness, Accountability and Transparency
  (FAccT)}, 2018.

\bibitem[Edwards and Storkey(2016)]{edwards2016}
Harrison Edwards and Amos~J. Storkey.
\newblock Censoring representations with an adversary.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Fazelpour and Lipton(2020)]{fazelpour2020algorithmic}
Sina Fazelpour and Zachary~C Lipton.
\newblock Algorithmic fairness from a non-ideal perspective.
\newblock In \emph{AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  pages 57--63, 2020.

\bibitem[Feldman and Peake(2021)]{feldman2021}
Tal Feldman and Ashley Peake.
\newblock End-to-end bias mitigation: Removing gender bias in deep learning.
\newblock \emph{arXiv preprint arXiv:2104.02532}, 2021.

\bibitem[Feng et~al.(2019)Feng, Yang, Lyu, Tan, Sun, and Wang]{feng2019}
Rui Feng, Yang Yang, Yuehan Lyu, Chenhao Tan, Yizhou Sun, and Chunping Wang.
\newblock Learning fair representations via an adversarial framework.
\newblock \emph{arXiv preprint arXiv:1904.13341}, 2019.

\bibitem[Goh et~al.(2016)Goh, Cotter, Gupta, and Friedlander]{goh2016}
Gabriel Goh, Andrew Cotter, Maya Gupta, and Michael Friedlander.
\newblock Satisfying real-world goals with dataset constraints.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem[Hardt et~al.(2016)Hardt, Price, and Srebro]{hardt2016equality}
Moritz Hardt, Eric Price, and Nathan Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem[Harned and Wallach(2019)]{harned2019stretching}
Zach Harned and Hanna Wallach.
\newblock Stretching human laws to apply to machines: The dangers of a
  ``colorblind'' computer.
\newblock \emph{Fla. St. UL Rev.}, 47:\penalty0 617--648, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[Hellman(2020)]{hellman2020measuring}
Deborah Hellman.
\newblock Measuring algorithmic fairness.
\newblock \emph{Va. L. Rev.}, 106:\penalty0 811--866, 2020.

\bibitem[Howard et~al.(2019)Howard, Sandler, Chu, Chen, Chen, Tan, Wang, Zhu,
  Pang, Vasudevan, et~al.]{howard2019mobilenet}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In \emph{IEEE/CVF International Conference on Computer Vision
  (CVPR)}, 2019.

\bibitem[Jia et~al.(2018)Jia, Lansdall-Welfare, and Cristianini]{jia2018right}
Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini.
\newblock Right for the right reason: Training agnostic networks.
\newblock In \emph{International Symposium on Intelligent Data Analysis}, 2018.

\bibitem[Jia et~al.(2020)Jia, Meng, Zhao, and Chang]{jia2020}
Shengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei Chang.
\newblock Mitigating gender bias amplification in distribution by posterior
  regularization.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2020.

\bibitem[Joshi(2018)]{joshi2018racial}
Yuvraj Joshi.
\newblock Racial indirection.
\newblock \emph{UC Davis L. Rev.}, 52:\penalty0 2495--2568, 2018.

\bibitem[Kamiran and Calders(2012)]{Kamiran2012}
Faisal Kamiran and Toon Calders.
\newblock Data preprocessing techniques for classification without
  discrimination.
\newblock \emph{Knowledge and Information Systems (KAIS)}, 2012.

\bibitem[Karkkainen and Joo(2021)]{fairface}
Kimmo Karkkainen and Jungseock Joo.
\newblock Fairface: Face attribute dataset for balanced race, gender, and age
  for bias measurement and mitigation.
\newblock In \emph{IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV)}, 2021.

\bibitem[Kendall(1945)]{kendall1945}
Maurice Kendall.
\newblock The treatment of ties in ranking problem.
\newblock \emph{Biometrika}, 1945.

\bibitem[Kim(2022)]{kim2022race}
Pauline Kim.
\newblock Race-aware algorithms: Fairness, nondiscrimination and affirmative
  action.
\newblock \emph{California Law Review, Forthcoming}, 2022.

\bibitem[King and Hemenway(2020)]{king2020blurred}
Allan King and Alexandra Hemenway.
\newblock Blurred lines: Disparate impact and disparate treatment challenges to
  subjective decisions-the case of reductions in force.
\newblock \emph{Wm. \& Mary Bus. L. Rev.}, 2020.

\bibitem[Kingma and Ba(2017)]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2017.

\bibitem[Klare et~al.(2012)Klare, Burge, Klontz, Bruegge, and
  Jain]{klare2012face}
Brendan~F Klare, Mark~J Burge, Joshua~C Klontz, Richard W~Vorder Bruegge, and
  Anil~K Jain.
\newblock Face recognition performance: Role of demographic information.
\newblock \emph{IEEE Transactions on Information Forensics and Security}, 2012.

\bibitem[Kleindessner et~al.(2022)Kleindessner, Samadi, Zafar, Kenthapadi, and
  Russell]{kleindessner2021}
Matth{\"a}us Kleindessner, Samira Samadi, Muhammad~Bilal Zafar, Krishnaram
  Kenthapadi, and Chris Russell.
\newblock Pairwise fairness for ordinal regression.
\newblock In \emph{Artificial Intelligence and Statistics (AISTATS)}, 2022.

\bibitem[Kroll et~al.(2017)Kroll, Huey, Barocas, Felten, Reidenberg, Robinson,
  and Yu]{kroll165accountable}
Joshua~A Kroll, Joanna Huey, Solon Barocas, Edward~W Felten, Joel~R Reidenberg,
  David~G Robinson, and Harlan Yu.
\newblock Accountable algorithms.
\newblock \emph{University of Pennsylvania Law Review}, 165:\penalty0 633--706,
  2017.

\bibitem[Lipton et~al.(2018)Lipton, Chouldechova, and McAuley]{lipton2018}
Zachary~C. Lipton, Alexandra Chouldechova, and Julian McAuley.
\newblock Does mitigating {ML's} impact disparity require treatment disparity?
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, 2015.
\newblock Dataset available at
  \url{https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html}.

\bibitem[Lohaus et~al.(2020)Lohaus, Perrot, and von Luxburg]{lohaus2020}
Michael Lohaus, Michael Perrot, and Ulrike von Luxburg.
\newblock Too relaxed to be fair.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Louizos et~al.(2016)Louizos, Swersky, Li, Welling, and
  Zemel]{louizos2016}
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel.
\newblock The variational fair autoencoder.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Madras et~al.(2018)Madras, Creager, Pitassi, and Zemel]{madras2018}
David Madras, Elliot Creager, Toniann Pitassi, and Richard~S. Zemel.
\newblock Learning adversarially fair and transferable representations.
\newblock In \emph{International Conference on Machine Learning, (ICML)}, 2018.

\bibitem[Manisha and Gujar(2020)]{padala2020fnnc}
Padala Manisha and Sujit Gujar.
\newblock Fnnc: Achieving fairness through neural networks.
\newblock In \emph{International Joint Conferences on Artificial Intelligence
  Organization (IJCAI)}, 2020.

\bibitem[Mehrabi et~al.(2021)Mehrabi, Morstatter, Saxena, Lerman, and
  Galstyan]{mehrabi2021}
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
  Galstyan.
\newblock A survey on bias and fairness in machine learning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 2021.

\bibitem[Menon and Williamson(2018)]{Menon2018}
Aditya~Krishna Menon and Robert~C. Williamson.
\newblock The cost of fairness in binary classification.
\newblock In \emph{Conference on Fairness, Accountability and Transparency
  (FAccT)}, 2018.

\bibitem[Moyer et~al.(2018)Moyer, Gao, Brekelmans, Steeg, and
  Galstyan]{moyer2018invariant}
Daniel Moyer, Shuyang Gao, Rob Brekelmans, Greg~Ver Steeg, and Aram Galstyan.
\newblock Invariant representations without adversarial training.
\newblock \emph{arXiv preprint arXiv:1805.09458}, 2018.

\bibitem[Oneto et~al.(2019)Oneto, Donini, Elders, and Pontil]{oneto2019}
Luca Oneto, Michele Donini, Amon Elders, and Massimiliano Pontil.
\newblock Taking advantage of multitask learning for fair classification.
\newblock In \emph{AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  2019.

\bibitem[Perrone et~al.(2021)Perrone, Donini, Zafar, Schmucker, Kenthapadi, and
  Archambeau]{perrone2021fairbayesian}
Valerio Perrone, Michele Donini, Muhammad~Bilal Zafar, Robin Schmucker,
  Krishnaram Kenthapadi, and C\'{e}dric Archambeau.
\newblock Fair bayesian optimization.
\newblock In \emph{AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  2021.

\bibitem[Prates et~al.(2020)Prates, Avelar, and Lamb]{prates2020}
Marcelo O.~R. Prates, Pedro H.~C. Avelar, and Luis Lamb.
\newblock Assessing gender bias in machine translation: a case study with
  google translate.
\newblock \emph{Neural Computing and Applications}, 32:\penalty0 6363--6381,
  2020.

\bibitem[Raff and Sylvester(2018)]{raff2018gradientreversal}
Edward Raff and Jared Sylvester.
\newblock Gradient reversal against discrimination: A fair neural network
  learning approach.
\newblock In \emph{IEEE International Conference on Data Science and Advanced
  Analytics (DSAA)}, 2018.

\bibitem[Ramaswamy et~al.(2021)Ramaswamy, Kim, and
  Russakovsky]{ramaswamy2020gandebiasing}
Vikram~V. Ramaswamy, Sunnie S.~Y. Kim, and Olga Russakovsky.
\newblock Fair attribute classification through latent space de-biasing.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2021.

\bibitem[Risser et~al.(2022)Risser, Sanz, Vincenot, and
  Loubes]{risser2021tackling}
Laurent Risser, Alberto~Gonzalez Sanz, Quentin Vincenot, and Jean-Michel
  Loubes.
\newblock Tackling algorithmic bias in neural-network classifiers using
  wasserstein-2 regularization.
\newblock \emph{Journal of Mathematical Imaging and Vision}, 2022.

\bibitem[Selmi(2013)]{selmi2013indirect}
Michael Selmi.
\newblock Indirect discrimination and the anti-discrimination mandate.
\newblock \emph{Philosophical foundations of discrimination law}, pages
  250--268, 2013.

\bibitem[Serna et~al.(2020)Serna, Morales, Fierrez, and Obradovich]{serna2020}
Ignacio Serna, Aythami Morales, Julian Fierrez, and Nick Obradovich.
\newblock Sensitiveloss: Improving accuracy and fairness of face
  representations with discrimination-aware deep learning.
\newblock \emph{Artificial Intelligence}, 305:\penalty0 103682, 2020.

\bibitem[Song and Shmatikov(2020)]{song2019overlearning}
Congzheng Song and Vitaly Shmatikov.
\newblock Overlearning reveals sensitive attributes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Statute(1991)]{civilrights_title7}
Federal Statute.
\newblock {Amendment of Civil Rights Act, Pub. L. No. 102-166, sec. 106, 105
  Stat. 1071, 1075 (codified at 42 U.S.C. sec 2000e-2(l))}.
\newblock 1991.

\bibitem[{Supreme Court}(1999)]{1999hunt}
{Supreme Court}.
\newblock {Hunt v. Cromartie}.
\newblock 526 U.S. 541, 1999.

\bibitem[{Supreme Court}(2009)]{ricci}
{Supreme Court}.
\newblock {Ricci v. DeStefano}.
\newblock 557 U.S. 557, 2009.

\bibitem[{United States Court of Appeals for the Seventh
  Circuit}(1994)]{Troupevmay}
{United States Court of Appeals for the Seventh Circuit}.
\newblock {Troupe v. May Dept. Stores}.
\newblock 20 F.3d 734, 736, 1994.

\bibitem[Ustun et~al.(2019)Ustun, Liu, and Parkes]{ustun19a}
Berk Ustun, Yang Liu, and David Parkes.
\newblock Fairness without harm: Decoupled classifiers with preference
  guarantees.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[van~der Maaten and Hinton(2008)]{maaten2008tsne}
Laurens van~der Maaten and Geoffrey~E. Hinton.
\newblock Visualizing high-dimensional data using t-sne.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 9:\penalty0
  2579--2605, 2008.

\bibitem[Wachter et~al.(2020)Wachter, Mittelstadt, and
  Russell]{wachter2020bias}
Sandra Wachter, Brent Mittelstadt, and Chris Russell.
\newblock Bias preservation in machine learning: the legality of fairness
  metrics under eu non-discrimination law.
\newblock \emph{W. Va. L. Rev.}, 123:\penalty0 735--790, 2020.

\bibitem[Wachter et~al.(2021)Wachter, Mittelstadt, and
  Russell]{wachter2021fairness}
Sandra Wachter, Brent Mittelstadt, and Chris Russell.
\newblock Why fairness cannot be automated: Bridging the gap between {EU}
  non-discrimination law and {AI}.
\newblock \emph{Computer Law \& Security Review}, 2021.

\bibitem[Wang and Russakovsky(2021)]{wang2021}
Angelina Wang and Olga Russakovsky.
\newblock Directional bias amplification.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Deng, Hu, Tao, and
  Huang]{wang2019racial}
Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang.
\newblock Racial faces in the wild: Reducing racial bias by information
  maximization adaptation network.
\newblock In \emph{IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Zhao, Yatskar, Chang, and
  Ordonez]{wang2019balanced}
Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez.
\newblock Balanced datasets are not enough: Estimating and mitigating gender
  bias in deep image representations.
\newblock In \emph{IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 2019{\natexlab{b}}.

\bibitem[Wang et~al.(2020)Wang, Qinami, Karakozis, Genova, Nair, Hata, and
  Russakovsky]{wang2020towards}
Zeyu Wang, Klint Qinami, Ioannis~Christos Karakozis, Kyle Genova, Prem Nair,
  Kenji Hata, and Olga Russakovsky.
\newblock Towards fairness in visual recognition: Effective strategies for bias
  mitigation.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2020.

\bibitem[Wick et~al.(2019)Wick, Panda, and Tristan]{wick2019}
Michael Wick, Swetasudha Panda, and Jean-Baptiste Tristan.
\newblock Unlocking fairness: a trade-off revisited.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Woodworth et~al.(2017)Woodworth, Gunasekar, Ohannessian, and
  Srebro]{woodworth2017}
B.~Woodworth, S.~Gunasekar, M.~I. Ohannessian, and N.~Srebro.
\newblock Learning non-discriminatory predictors.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2017.

\bibitem[Wu et~al.(2019)Wu, Zhang, and Wu]{wu2019}
Yongkai Wu, Lu~Zhang, and Xintao Wu.
\newblock On convexity and bounds of fairness-aware classification.
\newblock In \emph{International World Wide Web Conference (WWW)}, 2019.

\bibitem[Xie et~al.(2017)Xie, Dai, Du, Hovy, and Neubig]{xie2017controllable}
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig.
\newblock Controllable invariance through adversarial feature learning.
\newblock \emph{arXiv preprint arXiv:1705.11122}, 2017.

\bibitem[Zafar et~al.(2017{\natexlab{a}})Zafar, Valera, Rodriguez, and
  Gummadi]{zafar2017fairness}
Muhammad~Bilal Zafar, Isabel Valera, Manuel~Gomez Rodriguez, and Krishna~P
  Gummadi.
\newblock Fairness constraints: Mechanisms for fair classification.
\newblock In \emph{Artificial Intelligence and Statistics (AISTATS)},
  2017{\natexlab{a}}.

\bibitem[Zafar et~al.(2017{\natexlab{b}})Zafar, Valera, Rodriguez, and
  Gummadi]{zafar2017www}
Muhammad~Bilal Zafar, Isabel Valera, Manuel~Gomez Rodriguez, and Krishna~P.
  Gummadi.
\newblock Fairness beyond disparate treatment \& disparate impact: Learning
  classification without disparate mistreatment.
\newblock In \emph{International World Wide Web Conference (WWW)},
  2017{\natexlab{b}}.

\bibitem[Zemel et~al.(2013)Zemel, Wu, Swersky, Pitassi, and Dwork]{zemel2013}
Rich Zemel, Yu~Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork.
\newblock Learning fair representations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2013.

\bibitem[Zhao and Gordon(2019)]{zhao2019inherent}
Han Zhao and Geoffrey~J. Gordon.
\newblock Inherent tradeoffs in learning fair representations.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Zhao et~al.(2017)Zhao, Wang, Yatskar, Ordonez, and Chang]{zhao2017men}
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.
\newblock Men also like shopping: Reducing gender bias amplification using
  corpus-level constraints.
\newblock \emph{arXiv preprint arXiv:1707.09457}, 2017.

\end{thebibliography}
