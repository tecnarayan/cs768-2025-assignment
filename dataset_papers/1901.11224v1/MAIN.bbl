\begin{thebibliography}{31}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Agarwal and Bottou(2015)}]{agarwal2015lower}
\textsc{Agarwal, A.} and \textsc{Bottou, L.} (2015).
\newblock A lower bound for the optimization of finite sums.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Agarwal et~al.(2017)Agarwal, Allen-Zhu, Bullins, Hazan and
  Ma}]{agarwal2017finding}
\textsc{Agarwal, N.}, \textsc{Allen-Zhu, Z.}, \textsc{Bullins, B.},
  \textsc{Hazan, E.} and \textsc{Ma, T.} (2017).
\newblock Finding approximate local minima faster than gradient descent.
\newblock In \textit{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}. ACM.

\bibitem[{Agarwal and Hazan(2017)}]{agarwal2017lower}
\textsc{Agarwal, N.} and \textsc{Hazan, E.} (2017).
\newblock Lower bounds for higher-order convex optimization.
\newblock \textit{arXiv preprint arXiv:1710.10329} .

\bibitem[{Allen-Zhu(2017{\natexlab{a}})}]{allen2017katyusha}
\textsc{Allen-Zhu, Z.} (2017{\natexlab{a}}).
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \textit{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}. ACM.

\bibitem[{Allen-Zhu(2017{\natexlab{b}})}]{allen2017natasha}
\textsc{Allen-Zhu, Z.} (2017{\natexlab{b}}).
\newblock Natasha: Faster non-convex stochastic optimization via strongly
  non-convex parameter.
\newblock \textit{arXiv preprint arXiv:1702.00763} .

\bibitem[{Allen-Zhu(2018)}]{allen2018katyushax}
\textsc{Allen-Zhu, Z.} (2018).
\newblock Katyusha x: Practical momentum method for stochastic sum-of-nonconvex
  optimization.
\newblock \textit{arXiv preprint arXiv:1802.03866} .

\bibitem[{Allen-Zhu and Li(2016)}]{allen2016lazysvd}
\textsc{Allen-Zhu, Z.} and \textsc{Li, Y.} (2016).
\newblock Lazysvd: Even faster svd decomposition yet without agonizing pain.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Arjevani and Shamir(2016)}]{arjevani2016dimension}
\textsc{Arjevani, Y.} and \textsc{Shamir, O.} (2016).
\newblock Dimension-free iteration complexity of finite sum optimization
  problems.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Arjevani et~al.()Arjevani, Shamir and Shiff}]{arjevani2017oracle}
\textsc{Arjevani, Y.}, \textsc{Shamir, O.} and \textsc{Shiff, R.} (????).
\newblock Oracle complexity of second-order methods for smooth convex
  optimization.
\newblock \textit{Mathematical Programming}  1--34.

\bibitem[{Bietti and Mairal(2017)}]{bietti2017stochastic}
\textsc{Bietti, A.} and \textsc{Mairal, J.} (2017).
\newblock Stochastic optimization with variance reduction for infinite datasets
  with finite sum structure.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Carmon et~al.(2017{\natexlab{a}})Carmon, Duchi, Hinder and
  Sidford}]{carmon2017lower}
\textsc{Carmon, Y.}, \textsc{Duchi, J.~C.}, \textsc{Hinder, O.} and
  \textsc{Sidford, A.} (2017{\natexlab{a}}).
\newblock Lower bounds for finding stationary points i.
\newblock \textit{arXiv preprint arXiv:1710.11606} .

\bibitem[{Carmon et~al.(2017{\natexlab{b}})Carmon, Duchi, Hinder and
  Sidford}]{carmon2017lower2}
\textsc{Carmon, Y.}, \textsc{Duchi, J.~C.}, \textsc{Hinder, O.} and
  \textsc{Sidford, A.} (2017{\natexlab{b}}).
\newblock Lower bounds for finding stationary points ii: First-order methods.
\newblock \textit{arXiv preprint arXiv:1711.00841} .

\bibitem[{Carmon et~al.(2018)Carmon, Duchi, Hinder and
  Sidford}]{carmon2018accelerated}
\textsc{Carmon, Y.}, \textsc{Duchi, J.~C.}, \textsc{Hinder, O.} and
  \textsc{Sidford, A.} (2018).
\newblock Accelerated methods for nonconvex optimization.
\newblock \textit{SIAM Journal on Optimization} \textbf{28} 1751--1772.

\bibitem[{Chen and Yang(2018)}]{yang2018does}
\textsc{Chen, Z.} and \textsc{Yang, T.} (2018).
\newblock A variance reduction method for non-convex optimization with improved
  convergence under large condition number.
\newblock \textit{arXiv preprint arXiv:1809.06754} .

\bibitem[{Defazio et~al.(2014)Defazio, Bach and
  Lacoste-Julien}]{defazio2014saga}
\textsc{Defazio, A.}, \textsc{Bach, F.} and \textsc{Lacoste-Julien, S.} (2014).
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Fang et~al.(2018)Fang, Li, Lin and Zhang}]{fang2018spider}
\textsc{Fang, C.}, \textsc{Li, C.~J.}, \textsc{Lin, Z.} and \textsc{Zhang, T.}
  (2018).
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Garber et~al.(2016)Garber, Hazan, Jin, Kakade, Musco, Netrapalli and
  Sidford}]{garber2016faster}
\textsc{Garber, D.}, \textsc{Hazan, E.}, \textsc{Jin, C.}, \textsc{Kakade,
  S.~M.}, \textsc{Musco, C.}, \textsc{Netrapalli, P.} and \textsc{Sidford, A.}
  (2016).
\newblock Faster eigenvector computation via shift-and-invert preconditioning.
\newblock In \textit{ICML}.

\bibitem[{Johnson and Zhang(2013)}]{johnson2013accelerating}
\textsc{Johnson, R.} and \textsc{Zhang, T.} (2013).
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Lan and Yang(2018)}]{lan2018accelerated}
\textsc{Lan, G.} and \textsc{Yang, Y.} (2018).
\newblock Accelerated stochastic algorithms for nonconvex finite-sum and
  multi-block optimization.
\newblock \textit{arXiv preprint arXiv:1805.05411} .

\bibitem[{Lan and Zhou()}]{lan2017optimal}
\textsc{Lan, G.} and \textsc{Zhou, Y.} (????).
\newblock An optimal randomized incremental gradient method.
\newblock \textit{Mathematical programming}  1--49.

\bibitem[{Mairal(2015)}]{mairal2015incremental}
\textsc{Mairal, J.} (2015).
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock \textit{SIAM Journal on Optimization} \textbf{25} 829--855.

\bibitem[{Murty and Kabadi(1987)}]{murty1987some}
\textsc{Murty, K.~G.} and \textsc{Kabadi, S.~N.} (1987).
\newblock Some np-complete problems in quadratic and nonlinear programming.
\newblock \textit{Mathematical programming} \textbf{39} 117--129.

\bibitem[{Nesterov(2013)}]{nesterov2013introductory}
\textsc{Nesterov, Y.} (2013).
\newblock \textit{Introductory lectures on convex optimization: A basic
  course}, vol.~87.
\newblock Springer Science \& Business Media.

\bibitem[{Nesterov(1983)}]{Nesterov1983A}
\textsc{Nesterov, Y.~E.} (1983).
\newblock A method for solving the convex programming problem with convergence
  rate $o(1/ksp{2})$.
\newblock \textit{Dokl.akad.nauk Sssr}  543--547.

\bibitem[{Roux et~al.(2012)Roux, Schmidt and Bach}]{roux2012stochastic}
\textsc{Roux, N.~L.}, \textsc{Schmidt, M.} and \textsc{Bach, F.~R.} (2012).
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Shalev-Shwartz(2015)}]{shalev2015sdca}
\textsc{Shalev-Shwartz, S.} (2015).
\newblock Sdca without duality.
\newblock \textit{arXiv preprint arXiv:1502.06177} .

\bibitem[{Shalev-Shwartz(2016)}]{shalev2016sdca}
\textsc{Shalev-Shwartz, S.} (2016).
\newblock Sdca without duality, regularization, and individual convexity.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Woodworth and Srebro(2016)}]{woodworth2016tight}
\textsc{Woodworth, B.~E.} and \textsc{Srebro, N.} (2016).
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Xiao and Zhang(2014)}]{xiao2014proximal}
\textsc{Xiao, L.} and \textsc{Zhang, T.} (2014).
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \textit{SIAM Journal on Optimization} \textbf{24} 2057--2075.

\bibitem[{Yao(1977)}]{yao1977probabilistic}
\textsc{Yao, A. C.-C.} (1977).
\newblock Probabilistic computations: Toward a unified measure of complexity.
\newblock In \textit{Foundations of Computer Science, 1977., 18th Annual
  Symposium on}. IEEE.

\bibitem[{Zhou et~al.(2018)Zhou, Xu and Gu}]{zhou2018stochastic}
\textsc{Zhou, D.}, \textsc{Xu, P.} and \textsc{Gu, Q.} (2018).
\newblock Stochastic nested variance reduced gradient descent for nonconvex
  optimization.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
