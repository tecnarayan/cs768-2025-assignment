\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Buolamwini and Gebru(2018)]{buolamwini2018gender}
Joy Buolamwini and Timnit Gebru.
\newblock Gender shades: Intersectional accuracy disparities in commercial gender classification.
\newblock In \emph{Conference on Fairness, Accountability and Transparency}, 2018.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do {I}magenet classifiers generalize to {I}magenet?
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Jean et~al.(2016)Jean, Burke, Xie, Davis, Lobell, and Ermon]{jean2016combining}
Neal Jean, Marshall Burke, Michael Xie, W~Matthew Davis, David~B Lobell, and Stefano Ermon.
\newblock Combining satellite imagery and machine learning to predict poverty.
\newblock \emph{Science}, 353\penalty0 (6301):\penalty0 790--794, 2016.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and Schmidt]{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu et~al.(2021)Liu, Wang, and Long]{liu2021cycle}
Hong Liu, Jianmin Wang, and Mingsheng Long.
\newblock Cycle self-training for domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Yi et~al.(2023)Yi, Xu, Xu, Li, Pu, Ling, McLeod, and Wang]{yi2023source}
Li~Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, Ian McLeod, and Boyu Wang.
\newblock When source-free domain adaptation meets learning with noisy labels.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Berthelot et~al.(2021)Berthelot, Roelofs, Sohn, Carlini, and Kurakin]{berthelot2021adamatch}
David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin.
\newblock Adamatch: A unified approach to semi-supervised learning and domain adaptation.
\newblock \emph{arXiv preprint arXiv:2106.04732}, 2021.

\bibitem[Lee(2013)]{lee2013pseudo}
Dong-Hyun Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.
\newblock In \emph{Workshop on Challenges in Representation Learning, ICML}, 2013.

\bibitem[Arazo et~al.(2020)Arazo, Ortego, Albert, O’Connor, and McGuinness]{arazo2020pseudo}
Eric Arazo, Diego Ortego, Paul Albert, Noel~E O’Connor, and Kevin McGuinness.
\newblock Pseudo-labeling and confirmation bias in deep semi-supervised learning.
\newblock In \emph{International Joint Conference on Neural Networks}, 2020.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel, Cubuk, Kurakin, and Li]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin~A Raffel, Ekin~Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Wang, Hou, Wu, Wang, Okumura, and Shinozaki]{zhang2021flexmatch}
Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki.
\newblock Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Chen, Heng, Hou, Fan, Wu, Wang, Savvides, Shinozaki, Raj, et~al.]{wang2022freematch}
Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, et~al.
\newblock Freematch: Self-adaptive thresholding for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2205.07246}, 2022.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin, Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Liang et~al.(2020)Liang, Hu, and Feng]{liang2020we}
Jian Liang, Dapeng Hu, and Jiashi Feng.
\newblock Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Yang et~al.(2021{\natexlab{a}})Yang, van~de Weijer, Herranz, Jui, et~al.]{yang2021exploiting}
Shiqi Yang, Joost van~de Weijer, Luis Herranz, Shangling Jui, et~al.
\newblock Exploiting the intrinsic neighborhood structure for source-free domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021{\natexlab{a}}.

\bibitem[Yang et~al.(2021{\natexlab{b}})Yang, Wang, Van De~Weijer, Herranz, and Jui]{yang2021generalized}
Shiqi Yang, Yaxing Wang, Joost Van De~Weijer, Luis Herranz, and Shangling Jui.
\newblock Generalized source-free domain adaptation.
\newblock In \emph{IEEE/CVF International Conference on Computer Vision}, 2021{\natexlab{b}}.

\bibitem[Yeh et~al.(2022)Yeh, Westfechtel, Huang, and Harada]{yeh2022boosting}
Hao-Wei Yeh, Thomas Westfechtel, Jia-Bin Huang, and Tatsuya Harada.
\newblock Boosting source-free domain adaptation via confidence-based subsets feature alignment.
\newblock In \emph{International Conference on Pattern Recognition}, 2022.

\bibitem[Karim et~al.(2023)Karim, Mithun, Rajvanshi, Chiu, Samarasekera, and Rahnavard]{karim2023c}
Nazmul Karim, Niluthpol~Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and Nazanin Rahnavard.
\newblock C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and Fernandez-Granda]{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115, 2021{\natexlab{b}}.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio, Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and Hinton]{pereyra2017regularizing}
Gabriel Pereyra, George Tucker, Jan Chorowski, {\L}ukasz Kaiser, and Geoffrey Hinton.
\newblock Regularizing neural networks by penalizing confident output distributions.
\newblock \emph{arXiv preprint arXiv:1701.06548}, 2017.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Yu et~al.(2022)Yu, Bates, Ma, and Jordan]{yu2022robust}
Yaodong Yu, Stephen Bates, Yi~Ma, and Michael Jordan.
\newblock Robust calibration with multi-domain temperature scaling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Joo and Klabjan(2024)]{joo2023iw}
Taejong Joo and Diego Klabjan.
\newblock {IW-GAE}: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Safaryan et~al.(2023)Safaryan, Peste, and Alistarh]{safaryan2024knowledge}
Mher Safaryan, Alexandra Peste, and Dan Alistarh.
\newblock Knowledge distillation performs partial variance reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Rusak et~al.(2022)Rusak, Schneider, Pachitariu, Eck, Gehler, Bringmann, Brendel, and Bethge]{rusak2022if}
Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter~Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge.
\newblock If your data distribution shifts, use self-learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{li2020gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2020.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and P~Shanti Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2017.

\bibitem[Press et~al.(2023)Press, Schneider, K{\"u}mmerer, and Bethge]{press2024rdumb}
Ori Press, Steffen Schneider, Matthias K{\"u}mmerer, and Matthias Bethge.
\newblock Rdumb: A simple approach that questions our progress in continual test-time adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Shi and Sha(2012)]{shi2012infomax}
Yuan Shi and Fei Sha.
\newblock Information-theoretical learning of discriminative clusters for unsupervised domain adaptation.
\newblock In \emph{International Coference on International Conference on Machine Learning}, 2012.

\bibitem[Tu et~al.(2023)Tu, Deng, Gedeon, and Zheng]{tu2022assessing}
Weijie Tu, Weijian Deng, Tom Gedeon, and Liang Zheng.
\newblock Assessing model out-of-distribution generalization with softmax prediction probability baselines and a correlation method, 2023.
\newblock URL \url{https://openreview.net/forum?id=1maXoEyeqx}.

\bibitem[Hu et~al.(2023)Hu, Liang, Liew, Xue, Bai, and Wang]{hu2024mixed}
Dapeng Hu, Jian Liang, Jun~Hao Liew, Chuhui Xue, Song Bai, and Xinchao Wang.
\newblock Mixed samples as probes for unsupervised model selection in domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Saenko et~al.(2010)Saenko, Kulis, Fritz, and Darrell]{saenko2010adapting}
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.
\newblock Adapting visual category models to new domains.
\newblock In \emph{European Conference on Computer Vision}, 2010.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and Panchanathan]{venkateswara2017deep}
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2017.

\bibitem[Peng et~al.(2017)Peng, Usman, Kaushik, Hoffman, Wang, and Saenko]{peng2017visda}
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko.
\newblock Visda: The visual domain adaptation challenge.
\newblock \emph{arXiv preprint arXiv:1710.06924}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2009.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Krauledat, and M{\"u}ller]{sugiyama2007covariate}
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M{\"u}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (5), 2007.

\bibitem[Morerio et~al.(2017)Morerio, Cavazza, and Murino]{morerio2017minimal}
Pietro Morerio, Jacopo Cavazza, and Vittorio Murino.
\newblock Minimal-entropy correlation alignment for unsupervised deep domain adaptation.
\newblock \emph{arXiv preprint arXiv:1711.10288}, 2017.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man{\'e}.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Lee and See(2004)]{lee2004trust}
John~D Lee and Katrina~A See.
\newblock Trust in automation: Designing for appropriate reliance.
\newblock \emph{Human Factors}, 46\penalty0 (1):\penalty0 50--80, 2004.

\bibitem[Liu and Guo(2020)]{liu2020peer}
Yang Liu and Hongyi Guo.
\newblock Peer loss functions: Learning from noisy labels without knowing noise rates.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Polyak(1963)]{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki}, 3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Charles and Papailiopoulos(2018)]{charles2018stability}
Zachary Charles and Dimitris Papailiopoulos.
\newblock Stability and generalization of learning algorithms that converge to global optima.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Chernoff(1952)]{chernoff1952measure}
Herman Chernoff.
\newblock A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations.
\newblock \emph{The Annals of Mathematical Statistics}, pages 493--507, 1952.

\bibitem[Joo et~al.(2020)Joo, Chung, and Seo]{joo2020being}
Taejong Joo, Uijung Chung, and Min-Gwan Seo.
\newblock Being {B}ayesian about categorical probability.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty in deep learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Long et~al.(2018)Long, Cao, Wang, and Jordan]{long2018conditional}
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael Jordan.
\newblock Conditional adversarial domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Liu, Long, and Jordan]{zhang2019bridging}
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\end{thebibliography}
