\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson \& Morley(1985)Anderson and Morley]{AndersonMorley+1985}
Anderson, W.~N. and Morley, T.~D.
\newblock Eigenvalues of the {L}aplacian of a graph.
\newblock \emph{Linear and Multilinear Algebra}, 18\penalty0 (2):\penalty0
  141--145, 1985.

\bibitem[Azizian \& Lelarge(2021)Azizian and Lelarge]{Azi+2020}
Azizian, W. and Lelarge, M.
\newblock Characterizing the expressive power of invariant and equivariant
  graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Babai(1979)]{Bab1979}
Babai, L.
\newblock Lectures on graph isomorphism.
\newblock University of Toronto, Department of Computer Science. Mimeographed
  lecture notes, 1979.

\bibitem[Beaini et~al.(2024)Beaini, Huang, Cunha, Li, Moisescu{-}Pareja, Dymov,
  Maddrell{-}Mander, McLean, Wenkel, M{\"{u}}ller, Mohamud, Parviz, Craig,
  Koziarski, Lu, Zhu, Gabellini, Klaser, Dean, Wognum, Sypetkowski, Rabusseau,
  Rabbany, Tang, Morris, Koutis, Ravanelli, Wolf, Tossou, Mary, Bois,
  Fitzgibbon, Banaszewski, Martin, and Masters]{Beaini+2023}
Beaini, D., Huang, S., Cunha, J.~A., Li, Z., Moisescu{-}Pareja, G., Dymov, O.,
  Maddrell{-}Mander, S., McLean, C., Wenkel, F., M{\"{u}}ller, L., Mohamud,
  J.~H., Parviz, A., Craig, M., Koziarski, M., Lu, J., Zhu, Z., Gabellini, C.,
  Klaser, K., Dean, J., Wognum, C., Sypetkowski, M., Rabusseau, G., Rabbany,
  R., Tang, J., Morris, C., Koutis, I., Ravanelli, M., Wolf, G., Tossou, P.,
  Mary, H., Bois, T., Fitzgibbon, A.~W., Banaszewski, B., Martin, C., and
  Masters, D.
\newblock Towards foundational models for molecular learning on large-scale
  multi-task datasets.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Bodnar et~al.(2021)Bodnar, Frasca, Otter, Wang, Li{\`{o}},
  Mont{\'{u}}far, and Bronstein]{Bod+2021b}
Bodnar, C., Frasca, F., Otter, N., Wang, Y.~G., Li{\`{o}}, P., Mont{\'{u}}far,
  G., and Bronstein, M.~M.
\newblock Weisfeiler and {L}ehman go cellular: {CW} networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brownGPT3+2020}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Cai et~al.(1992)Cai, F{\"{u}}rer, and Immerman]{Cai+1992}
Cai, J., F{\"{u}}rer, M., and Immerman, N.
\newblock An optimal lower bound on the number of variables for graph
  identifications.
\newblock \emph{Combinatorica}, 12\penalty0 (4):\penalty0 389--410, 1992.

\bibitem[Chen et~al.(2019)Chen, Villar, Chen, and Bruna]{Che+2019}
Chen, Z., Villar, S., Chen, L., and Bruna, J.
\newblock On the equivalence between graph isomorphism testing and function
  approximation with gnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{Choromanski+2021}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A.,
  Sarl{\'{o}}s, T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L.,
  Belanger, D.~B., Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'{e}}]{Dao+2022}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'{e}}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin+2019}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistics}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{Dosovitskiy+2021}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Geerts \& Reutter(2022)Geerts and Reutter]{geerts2022}
Geerts, F. and Reutter, J.~L.
\newblock Expressiveness and approximation properties of graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{Gil+2017}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Glickman \& Yahav(2023)Glickman and Yahav]{Glickman+2023}
Glickman, D. and Yahav, E.
\newblock Diffusing graph attention.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Grohe(2017)]{Gro2017}
Grohe, M.
\newblock \emph{Descriptive Complexity, Canonisation, and Definable Graph
  Structure Theory}.
\newblock Cambridge University Press, 2017.

\bibitem[Grohe(2021)]{Gro+2021}
Grohe, M.
\newblock The logic of graph neural networks.
\newblock In \emph{Symposium on Logic in Computer Science}, 2021.

\bibitem[He et~al.(2023)He, Hooi, Laurent, Perold, LeCun, and Bresson]{He+2022}
He, X., Hooi, B., Laurent, T., Perold, A., LeCun, Y., and Bresson, X.
\newblock A generalization of vit/mlp-mixer to graphs.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{Hendrycks+2016}
Hendrycks, D. and Gimpel, K.
\newblock Bridging nonlinearities and stochastic regularizers with gaussian
  error linear units.
\newblock \emph{ArXiv preprint}, 2016.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{Horn+2012}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix Analysis, 2nd Edition}.
\newblock Cambridge University Press, 2012.

\bibitem[Hu et~al.(2020{\natexlab{a}})Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta,
  and Leskovec]{hu2020ogb}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{a}}.

\bibitem[Hu et~al.(2020{\natexlab{b}})Hu, Liu, Gomes, Zitnik, Liang, Pande, and
  Leskovec]{hu+2020+strategies}
Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V.~S., and Leskovec,
  J.
\newblock Strategies for pre-training graph neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Hu et~al.(2021)Hu, Fey, Ren, Nakata, Dong, and Leskovec]{Hu2021}
Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y., and Leskovec, J.
\newblock {OGB}-{LSC}: A large-scale challenge for machine learning on graphs.
\newblock In \emph{NeurIPS: Datasets and Benchmarks Track}, 2021.

\bibitem[Huang et~al.(2024)Huang, Lu, Robinson, Yang, Zhang, Jegelka, and
  Li]{Huang+2023}
Huang, Y., Lu, W., Robinson, J., Yang, Y., Zhang, M., Jegelka, S., and Li, P.
\newblock On the stability of expressive positional encodings for graphs.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Jaegle et~al.(2021)Jaegle, Gimeno, Brock, Vinyals, Zisserman, and
  Carreira]{Jaegle+2021}
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J.
\newblock Perceiver: General perception with iterative attention.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Kim et~al.(2021)Kim, Oh, and Hong]{Kim+2021}
Kim, J., Oh, S., and Hong, S.
\newblock Transformers generalize deepsets and can be extended to graphs {\&}
  hypergraphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Kim et~al.(2022)Kim, Nguyen, Min, Cho, Lee, Lee, and Hong]{Kim+2022}
Kim, J., Nguyen, D., Min, S., Cho, S., Lee, M., Lee, H., and Hong, S.
\newblock Pure transformers are powerful graph learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{Kip+2017}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'{e}}tourneau, and
  Tossou]{Kre+2021}
Kreuzer, D., Beaini, D., Hamilton, W.~L., L{\'{e}}tourneau, V., and Tossou, P.
\newblock Rethinking graph transformers with spectral attention.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lim et~al.(2023)Lim, Robinson, Zhao, Smidt, Sra, Maron, and
  Jegelka]{Lim+2022}
Lim, D., Robinson, J.~D., Zhao, L., Smidt, T.~E., Sra, S., Maron, H., and
  Jegelka, S.
\newblock Sign and basis invariant networks for spectral graph representation
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Lipman et~al.(2020)Lipman, Puny, and Ben-Hamu]{yaronlipman2020global}
Lipman, Y., Puny, O., and Ben-Hamu, H.
\newblock Global attention improves graph networks generalization.
\newblock \emph{ArXiv preprint}, 2020.

\bibitem[Ma et~al.(2023)Ma, Lin, Lim, Romero-Soriano, Dokania, Coates,
  H.S.~Torr, and Lim]{ma2023GraphInductiveBiases}
Ma, L., Lin, C., Lim, D., Romero-Soriano, A., Dokania, K., Coates, M.,
  H.S.~Torr, P., and Lim, S.-N.
\newblock Graph {Inductive} {Biases} in {Transformers} without {Message}
  {Passing}.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Malkin(2014)]{Mal2014}
Malkin, P.~N.
\newblock Sherali--{A}dams relaxations of graph isomorphism polytopes.
\newblock \emph{Discrete Optimization}, 2014.

\bibitem[Maron et~al.(2019{\natexlab{a}})Maron, Ben{-}Hamu, Serviansky, and
  Lipman]{Mar+2019}
Maron, H., Ben{-}Hamu, H., Serviansky, H., and Lipman, Y.
\newblock Provably powerful graph networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{a}}.

\bibitem[Maron et~al.(2019{\natexlab{b}})Maron, Ben{-}Hamu, Shamir, and
  Lipman]{Mar+2019c}
Maron, H., Ben{-}Hamu, H., Shamir, N., and Lipman, Y.
\newblock Invariant and equivariant graph networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Maron et~al.(2019{\natexlab{c}})Maron, Fetaya, Segol, and
  Lipman]{Mar+2019b}
Maron, H., Fetaya, E., Segol, N., and Lipman, Y.
\newblock On the universality of invariant networks.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{c}}.

\bibitem[M{\'{e}}ndez{-}Lucio et~al.(2022)M{\'{e}}ndez{-}Lucio, Nicolaou, and
  Earnshaw]{Lucio+2022}
M{\'{e}}ndez{-}Lucio, O., Nicolaou, C.~A., and Earnshaw, B.
\newblock Mole: a molecular foundation model for drug discovery.
\newblock \emph{ArXiv preprint}, 2022.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{Mor+2019}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G.,
  and Grohe, M.
\newblock Weisfeiler and {L}eman go neural: Higher-order graph neural networks.
\newblock In \emph{{AAAI} Conference on Artificial Intelligence}, 2019.

\bibitem[Morris et~al.(2020)Morris, Rattan, and Mutzel]{Morris2020b}
Morris, C., Rattan, G., and Mutzel, P.
\newblock {Weisfeiler and Leman} go sparse: Towards higher-order graph
  embeddings.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Morris et~al.(2022)Morris, Rattan, Kiefer, and Ravanbakhsh]{Mor+2022b}
Morris, C., Rattan, G., Kiefer, S., and Ravanbakhsh, S.
\newblock {SpeqNets}: Sparsity-aware permutation-equivariant graph networks.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Morris et~al.(2023)Morris, L., Maron, Rieck, Kriege, Grohe, Fey, and
  Borgwardt]{Mor+2022}
Morris, C., L., Y., Maron, H., Rieck, B., Kriege, N.~M., Grohe, M., Fey, M.,
  and Borgwardt, K.
\newblock {Weisfeiler and Leman} go machine learning: The story so far.
\newblock \emph{Journal of Machine Learning Research}, 2023.

\bibitem[M{\"{u}}ller et~al.(2024)M{\"{u}}ller, Galkin, Morris, and
  Ramp{\'{a}}sek]{Mue+2023}
M{\"{u}}ller, L., Galkin, M., Morris, C., and Ramp{\'{a}}sek, L.
\newblock Attending to graph transformers.
\newblock \emph{Transactions on Machine Learning Research}, 2024.

\bibitem[Puny et~al.(2023)Puny, Lim, Kiani, Maron, and Lipman]{Puny+2023}
Puny, O., Lim, D., Kiani, B.~T., Maron, H., and Lipman, Y.
\newblock Equivariant polynomials for graph neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Ramp{\'a}{\v{s}}ek et~al.(2022)Ramp{\'a}{\v{s}}ek, Galkin, Dwivedi,
  Luu, Wolf, and Beaini]{rampavsek2022recipe}
Ramp{\'a}{\v{s}}ek, L., Galkin, M., Dwivedi, V.~P., Luu, A.~T., Wolf, G., and
  Beaini, D.
\newblock Recipe for a general, powerful, scalable graph transformer.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Scarselli et~al.(2009)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{Sca+2009}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
\newblock The graph neural network model.
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (1):\penalty0 61--80, 2009.

\bibitem[Shchur et~al.(2018)Shchur, Mumme, Bojchevski, and
  G{\"{u}}nnemann]{Shchur+2018+Pitfalls}
Shchur, O., Mumme, M., Bojchevski, A., and G{\"{u}}nnemann, S.
\newblock Pitfalls of graph neural network evaluation.
\newblock \emph{ArXiv preprint}, 2018.

\bibitem[T{\"{o}}nshoff et~al.(2023)T{\"{o}}nshoff, Ritzert, Rosenbluth, and
  Grohe]{Toenshoff+2023+Gap}
T{\"{o}}nshoff, J., Ritzert, M., Rosenbluth, E., and Grohe, M.
\newblock Where did the gap go? reassessing the long-range graph benchmark.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang \& Zhang(2023)Wang and Zhang]{WangZhang2023BREC}
Wang, Y. and Zhang, M.
\newblock Towards better evaluation of {GNN} expressiveness with {BREC}
  dataset.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Weisfeiler \& Leman(1968)Weisfeiler and Leman]{Wei+1968}
Weisfeiler, B. and Leman, A.
\newblock The reduction of a graph to canonical form and the algebra which
  appears therein.
\newblock \emph{Nauchno-Technicheskaya Informatsia}, 2\penalty0 (9):\penalty0
  12--16, 1968.

\bibitem[Xu et~al.(2019{\natexlab{a}})Xu, Hu, Leskovec, and Jegelka]{Xu+2018b}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Xu et~al.(2019{\natexlab{b}})Xu, Wang, Yu, Feng, Song, Wang, and
  Yu]{Xu+2019}
Xu, K., Wang, L., Yu, M., Feng, Y., Song, Y., Wang, Z., and Yu, D.
\newblock Cross-lingual knowledge graph alignment via graph matching neural
  network.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2019{\natexlab{b}}.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{Ying2021}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y.
\newblock Do transformers really perform badly for graph representation?
\newblock In \emph{Advances in Neural Information Processing System}, 2021.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{ZaheerNIPS2017DeepSets}
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.~R., and
  Smola, A.~J.
\newblock Deep sets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Luo, Wang, and He]{Zha+2023}
Zhang, B., Luo, S., Wang, L., and He, D.
\newblock Rethinking the expressive power of gnns via graph biconnectivity.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\end{thebibliography}
