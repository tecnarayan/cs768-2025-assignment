\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L.
\newblock Random forests.
\newblock \emph{Machine learning}, 45\penalty0 (1):\penalty0 5--32, 2001.

\bibitem[Date \& Nagi(2016)Date and Nagi]{date2016gpu}
Date, K. and Nagi, R.
\newblock Gpu-accelerated hungarian algorithms for the linear assignment
  problem.
\newblock \emph{Parallel Computing}, 57:\penalty0 52--72, 2016.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean2012large}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A.,
  Tucker, P., Yang, K., Le, Q.~V., et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1223--1231, 2012.

\bibitem[Dietterich(2000)]{dietterich2000ensemble}
Dietterich, T.~G.
\newblock Ensemble methods in machine learning.
\newblock In \emph{International workshop on multiple classifier systems}, pp.\
   1--15. Springer, 2000.

\bibitem[{E}{U}(2016)]{eu:gdpr}
{E}{U}.
\newblock {Regulation (EU) 2016/679 of the European Parliament and of the
  Council of 27 April 2016 on the protection of natural persons with regard to
  the processing of personal data and on the free movement of such data, and
  repealing Directive 95/46/EC (General Data Protection Regulation)}.
\newblock \emph{Official Journal of the European Union}, L119:\penalty0 1--88,
  may 2016.
\newblock URL
  \url{http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L:2016:119:TOC}.

\bibitem[Ghahramani \& Griffiths(2005)Ghahramani and
  Griffiths]{ghahramani2005infinite}
Ghahramani, Z. and Griffiths, T.~L.
\newblock Infinite latent feature models and the {I}ndian buffet process.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  475--482, 2005.

\bibitem[Griffiths \& Ghahramani(2011)Griffiths and
  Ghahramani]{griffiths2011indian}
Griffiths, T.~L. and Ghahramani, Z.
\newblock The {I}ndian buffet process: {A}n introduction and review.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 1185--1224,
  2011.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Kuhn(1955)]{kuhn1955hungarian}
Kuhn, H.~W.
\newblock The {H}ungarian method for the assignment problem.
\newblock \emph{Naval Research Logistics (NRL)}, 2\penalty0 (1-2):\penalty0
  83--97, 1955.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436, 2015.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Li, M., Andersen, D.~G., Park, J.~W., Smola, A.~J., Ahmed, A., Josifovski, V.,
  Long, J., Shekita, E.~J., and Su, B.-Y.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pp.\  583--598, 2014.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Lian, X., Huang, Y., Li, Y., and Liu, J.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2737--2745, 2015.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017async}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  5330--5340. 2017.

\bibitem[Lloyd(1982)]{lloyd1982least}
Lloyd, S.
\newblock Least squares quantization in {PCM}.
\newblock \emph{Information Theory, IEEE Transactions on}, 28\penalty0
  (2):\penalty0 129--137, Mar 1982.

\bibitem[Ma et~al.(2015)Ma, Smith, Jaggi, Jordan, Richtarik, and Takac]{ma2015}
Ma, C., Smith, V., Jaggi, M., Jordan, M., Richtarik, P., and Takac, M.
\newblock Adding vs. averaging in distributed primal-dual optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pp.\  1973--1982, 2015.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1273--1282,
  2017.

\bibitem[Moritz et~al.(2015)Moritz, Nishihara, Stoica, and
  Jordan]{moritz2015sparknet}
Moritz, P., Nishihara, R., Stoica, I., and Jordan, M.~I.
\newblock Sparknet: Training deep networks in spark.
\newblock \emph{arXiv preprint arXiv:1511.06051}, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Poggio et~al.(2017)Poggio, Mhaskar, Rosasco, Miranda, and
  Liao]{poggio2017and}
Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q.
\newblock Why and when can deep-but not shallow-networks avoid the curse of
  dimensionality: a review.
\newblock \emph{International Journal of Automation and Computing}, 14\penalty0
  (5):\penalty0 503--519, 2017.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{amsgrad}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryQu7f-RZ}.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Shamir, O., Srebro, N., and Zhang, T.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International conference on machine learning}, pp.\
  1000--1008, 2014.

\bibitem[Smith et~al.(2017)Smith, Chiang, Sanjabi, and
  Talwalkar]{smith2017federated}
Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A.~S.
\newblock Federated multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4424--4434, 2017.

\bibitem[Teh et~al.(2007)Teh, Gr{\"u}r, and Ghahramani]{teh2007stick}
Teh, Y.~W., Gr{\"u}r, D., and Ghahramani, Z.
\newblock Stick-breaking construction for the {I}ndian buffet process.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  556--563,
  2007.

\bibitem[Thibaux \& Jordan(2007)Thibaux and Jordan]{thibaux2007hierarchical}
Thibaux, R. and Jordan, M.~I.
\newblock Hierarchical {B}eta processes and the {I}ndian buffet process.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  564--571,
  2007.

\bibitem[Yang(2013)]{yang2013trading}
Yang, T.
\newblock Trading computation for communication: Distributed stochastic dual
  coordinate ascent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  629--637, 2013.

\bibitem[Yurochkin et~al.(2018)Yurochkin, Fan, Guha, Koutris, and
  Nguyen]{yurochkin2018sddm}
Yurochkin, M., Fan, Z., Guha, A., Koutris, P., and Nguyen, X.
\newblock Scalable inference of topic evolution via models for latent geometric
  structures.
\newblock \emph{arXiv preprint arXiv:1809.08738}, 2018.

\bibitem[Zhang \& Lin(2015)Zhang and Lin]{zhang2015disco}
Zhang, Y. and Lin, X.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International conference on machine learning}, pp.\
  362--370, 2015.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, Jordan, and
  Wainwright]{zhang2013information}
Zhang, Y., Duchi, J., Jordan, M.~I., and Wainwright, M.~J.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2328--2336, 2013.

\end{thebibliography}
