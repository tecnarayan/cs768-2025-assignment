\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Auger \& Hansen(2013)Auger and Hansen]{AugerH13}
Auger, A. and Hansen, N.
\newblock {On Proving Linear Convergence of Comparison-based Step-size Adaptive
  Randomized Search on Scaling-Invariant Functions via Stability of Markov
  Chains}.
\newblock \emph{CoRR}, abs/1310.7697, 2013.

\bibitem[Bertsekas(2015)]{BertsekasSurvey}
Bertsekas, D.~P.
\newblock Incremental gradient, subgradient, and proximal methods for convex
  optimization: A survey, 2015.

\bibitem[Bolte et~al.(2017)Bolte, Nguyen, Peypouquet, and Suter]{Bolte2017}
Bolte, J., Nguyen, T.~P., Peypouquet, J., and Suter, B.~W.
\newblock {From error bounds to the complexity of first-order descent methods
  for convex functions}.
\newblock \emph{Mathematical Programming}, 165\penalty0 (2):\penalty0 471--507,
  Oct 2017.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{bottou2016optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv:1606.04838}, 2016.

\bibitem[Csiba \& Richt{\'a}rik(2017)Csiba and Richt{\'a}rik]{csiba2017global}
Csiba, D. and Richt{\'a}rik, P.
\newblock {Global Convergence of Arbitrary-Block Gradient Methods for
  Generalized Polyak-Lojasiewicz Functions}.
\newblock \emph{arXiv preprint arXiv:1709.03014}, 2017.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{NIPS}, pp.\  1646--1654, 2014.

\bibitem[Drusvyatskiy \& Lewis(2018)Drusvyatskiy and
  Lewis]{drusvyatskiy2018error}
Drusvyatskiy, D. and Lewis, A.~S.
\newblock {Error bounds, quadratic growth, and linear convergence of proximal
  methods}.
\newblock \emph{Mathematics of Operations Research}, 43\penalty0 (3):\penalty0
  919--948, 2018.

\bibitem[Gordji et~al.()Gordji, Delavar, and De~La~Sen]{gordji2016phi}
Gordji, M.~E., Delavar, M.~R., and De~La~Sen, M.
\newblock {On $\phi$-convex functions}.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'{a}}rik]{Gower2019}
Gower, R.~M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and
  Richt{\'{a}}rik, P.
\newblock {{SGD:} General Analysis and Improved Rates}.
\newblock \emph{CoRR}, abs/1901.09401, 2019.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and Friedman]{ESL}
Hastie, T., Tibshirani, R., and Friedman, J.
\newblock \emph{The Elements of Statistical Learning: Data Mining, Inference,
  and Prediction}.
\newblock Springer Series in Statistics, 2nd edition, 2009.

\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015beyond}
Hazan, E., Levy, K., and Shalev-Shwartz, S.
\newblock Beyond convexity: Stochastic quasi-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1594--1602, 2015.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{SVRG}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, pp.\  315--323, 2013.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{SAG}
Le~Roux, N., Schmidt, M., and Bach, F.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{NIPS}, pp.\  2663--2671, 2012.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nemirovsky \& Yudin(1983)Nemirovsky and Yudin]{nemirovsky1983problem}
Nemirovsky, A.~S. and Yudin, D.~B.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2004)]{nesterov2004}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization : a basic course}.
\newblock Applied optimization. Kluwer Academic Publ., Boston, Dordrecht,
  London, 2004.
\newblock ISBN 1-4020-7553-7.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richtarik, Scheinberg,
  and Takac]{Nguyen2018}
Nguyen, L., Nguyen, P.~H., van Dijk, M., Richtarik, P., Scheinberg, K., and
  Takac, M.
\newblock {SGD} and {H}ogwild! convergence without the bounded gradients
  assumption.
\newblock In \emph{ICML}, 2018.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{Nguyen2017_sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{ICML}, 2017.

\bibitem[Nocedal \& Wright(2006)Nocedal and Wright]{Nocedal2006NO}
Nocedal, J. and Wright, S.~J.
\newblock \emph{Numerical Optimization}.
\newblock Springer, New York, 2nd edition, 2006.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, P{\'{o}}czos, and
  Smola]{nonconvexSVRG}
Reddi, S.~J., Hefny, A., Sra, S., P{\'{o}}czos, B., and Smola, A.~J.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{ICML}, pp.\  314--323, 2016.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{RM1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 1951.

\bibitem[Schmidt \& Roux(2013)Schmidt and Roux]{schmidt2013fast}
Schmidt, M. and Roux, N.~L.
\newblock {Fast convergence of stochastic gradient descent under a strong
  growth condition}.
\newblock \emph{arXiv preprint arXiv:1308.6370}, 2013.

\bibitem[Schmidt et~al.(2016)Schmidt, Le~Roux, and Bach]{SAGjournal}
Schmidt, M., Le~Roux, N., and Bach, F.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, pp.\  1--30, 2016.

\bibitem[Wright et~al.(2009)Wright, Nowak, and Figueiredo]{WrightSPARSA}
Wright, S.~J., Nowak, R.~D., and Figueiredo, M. A.~T.
\newblock Sparse reconstruction by separable approximation.
\newblock \emph{IEEE Transactions on Signal Processing}, 57\penalty0
  (7):\penalty0 2479--2493, July 2009.

\bibitem[Xu et~al.(2016)Xu, Lin, and Yang]{xu2016accelerated}
Xu, Y., Lin, Q., and Yang, T.
\newblock {Accelerated stochastic subgradient methods under local error bound
  condition}.
\newblock \emph{arXiv preprint arXiv:1607.01027}, 2016.

\end{thebibliography}
