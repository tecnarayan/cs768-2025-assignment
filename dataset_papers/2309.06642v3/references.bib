
@misc{noauthor_3d_nodate,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	url = {https://www.researchgate.net/publication/339908871_3D_Phaseless_Imaging_at_Nano-scale_Challenges_and_Possible_Solutions},
	note = {Publication Title: ResearchGate},
}

@article{jiang2023autodir,
	title={Autodir: Automatic all-in-one image restoration with latent diffusion},
	author={Jiang, Yitong and Zhang, Zhaoyang and Xue, Tianfan and Gu, Jinwei},
	journal={arXiv preprint arXiv:2310.10123},
	year={2023}
}

@article{rout2023beyond,
	title={Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion},
	author={Rout, Litu and Chen, Yujia and Kumar, Abhishek and Caramanis, Constantine and Shakkottai, Sanjay and Chu, Wen-Sheng},
	journal={arXiv preprint arXiv:2312.00852},
	year={2023}
}

@inproceedings{preechakul2022diffusion,
	title={Diffusion autoencoders: Toward a meaningful and decodable representation},
	author={Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={10619--10629},
	year={2022}
}


@article{agarwal_efficient_2020,
	title = {Efficient {Full}-{Matrix} {Adaptive} {Regularization}},
	journal = {arXiv:1806.02958 [cs, math, stat]},
	author = {Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
	year = {2020},
}

@article{chan2016plug,
	title={Plug-and-play ADMM for image restoration: Fixed-point convergence and applications},
	author={Chan, Stanley H and Wang, Xiran and Elgendy, Omar A},
	journal={IEEE Transactions on Computational Imaging},
	volume={3},
	number={1},
	pages={84--98},
	year={2016},
	publisher={IEEE}
}

@article{song2023solving,
	title={Solving inverse problems with latent diffusion models via hard data consistency},
	author={Song, Bowen and Kwon, Soo Min and Zhang, Zecheng and Hu, Xinyu and Qu, Qing and Shen, Liyue},
	journal={arXiv preprint arXiv:2307.08123},
	year={2023}
}

@article{chung2023prompt,
	title={Prompt-tuning latent diffusion models for inverse problems},
	author={Chung, Hyungjin and Ye, Jong Chul and Milanfar, Peyman and Delbracio, Mauricio},
	journal={arXiv preprint arXiv:2310.01110},
	year={2023}
}

@article{yu2015lsun,
	title={Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop},
	author={Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
	journal={arXiv preprint arXiv:1506.03365},
	year={2015}
}

@article{delbracio2023inversion,
	title={Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration},
	author={Delbracio, Mauricio and Milanfar, Peyman},
	journal={arXiv preprint arXiv:2303.11435},
	year={2023}
}


@article{aggarwal_modl_2018,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	number = {2},
	journal = {IEEE transactions on medical imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {394--405},
}

@article{fabian2023diracdiffusion,
	title={DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency},
	author={Fabian, Zalan and Tinaz, Berk and Soltanolkotabi, Mahdi},
	journal={arXiv preprint arXiv:2303.14353},
	year={2023}
}


@article{akiyama_first_2019,
	title = {First {M87} {Event} {Horizon} {Telescope} {Results}. {IV}. {Imaging} the {Central} {Supermassive} {Black} {Hole}},
	journal = {The Astrophysical Journal Letters},
	author = {Akiyama, Kazunori},
	year = {2019},
	pages = {52},
}

@inproceedings{luo2023refusion,
	title={Refusion: Enabling large-size realistic image restoration with latent-space diffusion models},
	author={Luo, Ziwei and Gustafsson, Fredrik K and Zhao, Zheng and Sj{\"o}lund, Jens and Sch{\"o}n, Thomas B},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={1680--1691},
	year={2023}
}

@article{rout2023solving,
	title={Solving linear inverse problems provably via posterior sampling with latent diffusion models},
	author={Rout, Litu and Raoof, Negin and Daras, Giannis and Caramanis, Constantine and Dimakis, Alex and Shakkottai, Sanjay},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@inproceedings{sohl2015deep,
	title={Deep unsupervised learning using nonequilibrium thermodynamics},
	author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	booktitle={International Conference on Machine Learning},
	pages={2256--2265},
	year={2015},
	organization={PMLR}
}

@article{anil_second_nodate,
	title = {Second {Order} {Optimization} {Made} {Practical}},
	author = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
	pages = {19},
}

@article{ardizzone_analyzing_2019,
	title = {Analyzing {Inverse} {Problems} with {Invertible} {Neural} {Networks}},
	journal = {arXiv:1808.04730 [cs, stat]},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	year = {2019},
}

@article{arora_fine-grained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	journal = {arXiv:1901.08584 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	year = {2019},
}

@article{arpit_benefits_2019,
	title = {The {Benefits} of {Over}-{Parameterization} at {Initialization} in {Deep} {ReLU} {Networks}},
	journal = {arXiv:1901.03611 [cs, stat]},
	author = {Arpit, Devansh and Bengio, Yoshua},
	year = {2019},
}

@article{aslan_distributed_2020,
	title = {Distributed {Optimization} with {Tunable} {Learned} {Priors} for {Robust} {Ptycho}-{Tomography}},
	journal = {arXiv:2009.09498 [eess, math]},
	author = {Aslan, Selin and Liu, Zhengchun and Nikitin, Viktor and Bicer, Tekin and Leyffer, Sven and Gursoy, Doga},
	year = {2020},
}

@article{ba_distributed_2016,
	title = {Distributed {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximations}},
	author = {Ba, Jimmy and Grosse, Roger and Martens, James},
	year = {2016},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year = {2016},
}

@article{barbastathis_use_2019,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barbastathis_use_2019-1,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barutcu_limited-angle_2021,
	title = {Limited-{Angle} {Computed} {Tomography} with {Deep} {Image} and {Physics} {Priors}},
	volume = {11},
	number = {1},
	journal = {Scientific Reports},
	author = {Barutcu, Semih and Aslan, Selin and Katsaggelos, Aggelos K. and Gürsoy, Doğa},
	year = {2021},
	pages = {17740},
}

@article{beatty_rapid_2005,
	title = {Rapid {Gridding} {Reconstruction} with a {Minimal} {Oversampling} {Ratio}},
	volume = {24},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Beatty, P.J. and Nishimura, D.G. and Pauly, J.M.},
	year = {2005},
	pages = {799--808},
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-{Depth} {Concurrency} {Analysis}},
	volume = {52},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	year = {2019},
	pages = {1--43},
}

@article{bernstein_distance_2020,
	title = {On the {Distance} between {Two} {Neural} {Networks} and the {Stability} of {Learning}},
	journal = {arXiv:2002.03432 [cs, math, stat]},
	author = {Bernstein, Jeremy and Vahdat, Arash and Yue, Yisong and Liu, Ming-Yu},
	year = {2020},
}

@book{bertero_introduction_1998,
	address = {Bristol, UK ; Philadelphia, Pa},
	title = {Introduction to {Inverse} {Problems} in {Imaging}},
	isbn = {978-0-7503-0439-9 978-0-7503-0435-1},
	publisher = {Institute of Physics Pub},
	author = {Bertero, Mario and Boccacci, Patrizia},
	year = {1998},
	lccn = {TA1637 .B47 1998},
}

@book{bertsekas_introduction_2002,
	address = {Belmont, Mass},
	edition = {2. print},
	series = {Optimization and {Computation} {Series}},
	title = {Introduction to {Probability}},
	isbn = {978-1-886529-40-3},
	number = {1},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	year = {2002},
}

@book{bhatia_matrix_1997,
	address = {New York},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Matrix {Analysis}},
	isbn = {978-0-387-94846-1},
	number = {169},
	publisher = {Springer},
	author = {Bhatia, Rajendra},
	year = {1997},
	lccn = {QA188 .B485 1997},
}

@article{bora_ambientgan_2018,
	title = {{AMBIENTGAN}: {GENERATIVE} {MODELS} {FROM} {LOSSY} {MEASUREMENTS}},
	author = {Bora, Ashish and Dimakis, Alexandros G},
	year = {2018},
	pages = {22},
}

@article{bora_compressed_nodate,
	title = {Compressed {Sensing} {Using} {Generative} {Models}},
	author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alex},
	pages = {10},
}

@article{bostan_deep_2020,
	title = {Deep {Phase} {Decoder}: {Self}-{Calibrating} {Phase} {Microscopy} with an {Untrained} {Deep} {Neural} {Network}},
	journal = {arXiv:2001.09803 [physics]},
	author = {Bostan, Emrah and Heckel, Reinhard and Chen, Michael and Kellman, Michael and Waller, Laura},
	year = {2020},
}

@article{bottou_optimization_2018,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	volume = {60},
	number = {2},
	journal = {SIAM Review},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	year = {2018},
	pages = {223--311},
}

@book{bovik_essential_2009,
	address = {London ; Boston},
	title = {The {Essential} {Guide} to {Image} {Processing}},
	isbn = {978-0-12-374457-9},
	publisher = {Academic Press},
	author = {Bovik, Alan C.},
	year = {2009},
	lccn = {TA1637 .B68 2009},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex {Optimization}},
	isbn = {978-0-521-83378-3},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	lccn = {QA402.5 .B69 2004},
}

@article{boyd_neal_nodate,
	title = {Neal {Parikh} {Department} of {Computer} {Science} {Stanford} {University}},
	author = {Boyd, Stephen},
	pages = {113},
}

@article{brock_high-performance_2021,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	year = {2021},
}

@article{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	year = {2019},
}

@article{bronstein_geometric_2017,
	title = {Geometric {Deep} {Learning}: {Going} beyond {Euclidean} {Data}},
	volume = {34},
	number = {4},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	year = {2017},
	pages = {18--42},
}

@article{burdakov_efficiently_2017,
	title = {On {Efficiently} {Combining} {Limited}-{Memory} and {Trust}-{Region} {Techniques}},
	volume = {9},
	number = {1},
	journal = {Mathematical Programming Computation},
	author = {Burdakov, Oleg and Gong, Lujin and Zikrin, Spartak and Yuan, Ya-xiang},
	year = {2017},
	pages = {101--134},
}

@article{candes_phase_2015,
	title = {Phase {Retrieval} from {Coded} {Diffraction} {Patterns}},
	volume = {39},
	number = {2},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Candès, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {277--299},
}

@article{candes_phase_2015-1,
	title = {Phase {Retrieval} via {Wirtinger} {Flow}: {Theory} and {Algorithms}},
	volume = {61},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Candes, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {1985--2007},
}

@article{candes_stable_2006,
	title = {Stable {signal} {recovery} from {incomplete} and {inaccurate} {measurements}},
	volume = {59},
	number = {8},
	journal = {Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
	author = {Candes, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
	year = {2006},
	pages = {1207--1223},
}

@article{noauthor_chapter_nodate,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@article{noauthor_chapter_nodate-1,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@inproceedings{chartsias_adversarial_2017,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-1,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{rombach2022high,
	title={High-resolution image synthesis with latent diffusion models},
	author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={10684--10695},
	year={2022}
}


@inproceedings{chartsias_adversarial_2017-2,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-3,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year = {2020},
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	year = {2014},
}

@article{cohen_certified_2019,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	journal = {arXiv:1902.02918 [cs, stat]},
	author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
	year = {2019},
}

@book{noauthor_introduction_2009,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to {Algorithms}},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	year = {2009},
	lccn = {QA76.6 .C662 2009},
}

@article{cranmer_lagrangian_2020,
	title = {Lagrangian {Neural} {Networks}},
	journal = {arXiv:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	year = {2020},
}

@inproceedings{cubuk_autoaugment_2019,
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2019},
	pages = {113--123},
}

@article{curtis_trust_2017,
	title = {A {Trust} {Region} {Algorithm} with a {Worst}-{Case} {Iteration} {Complexity} of \$\${\textbackslash}textbackslash mathcal\{{\textbackslash}vphantom\}{O}{\textbackslash}vphantom\{\}({\textbackslash}textbackslash epsilon {\textasciicircum}\{-3/2\})\$\$ {O} ( ϵ - 3 / 2 ) for {Nonconvex} {Optimization}},
	volume = {162},
	number = {1-2},
	journal = {Mathematical Programming},
	author = {Curtis, Frank E. and Robinson, Daniel P. and Samadi, Mohammadreza},
	year = {2017},
	pages = {1--32},
}

@book{daubechies_ten_1992,
	address = {Philadelphia, Pa},
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Ten {Lectures} on {Wavelets}},
	isbn = {978-0-89871-274-2},
	number = {61},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Daubechies, Ingrid},
	year = {1992},
	lccn = {QA403.3 .D38 1992},
}

@article{davison_singular_1981,
	title = {A {Singular} {Value} {Decomposition} for the {Radon} {Transform} in {\textbackslash}emphn -{Dimensional} {Euclidean} {Space}},
	volume = {3},
	number = {3},
	journal = {Numerical Functional Analysis and Optimization},
	author = {Davison, M. E.},
	year = {1981},
	pages = {321--340},
}

@article{dean_large_nodate,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
	pages = {9},
}

@article{defazio_mri_2020,
	title = {{MRI} {Banding} {Removal} via {Adversarial} {Training}},
	journal = {arXiv:2001.08699 [cs, eess, stat]},
	author = {Defazio, Aaron and Murrell, Tullie and Recht, Michael P.},
	year = {2020},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-{Training} of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
}

@article{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	journal = {arXiv preprint arXiv:2105.05233},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	year = {2021},
}

@article{dierolf_ptychographic_nodate,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	author = {Dierolf, Martin},
	pages = {6},
}

@article{dierolf_ptychographic_2010,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	volume = {467},
	number = {7314},
	journal = {Nature},
	author = {Dierolf, Martin and Menzel, Andreas and Thibault, Pierre and Schneider, Philipp and Kewish, Cameron M. and Wepf, Roger and Bunk, Oliver and Pfeiffer, Franz},
	year = {2010},
	pages = {436--439},
}

@article{dinh_nice_2015,
	title = {{NICE}: {Non}-{Linear} {Independent} {Components} {Estimation}},
	journal = {arXiv:1410.8516 [cs]},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	year = {2015},
}

@article{domingos_few_2012,
	title = {A {Few} {Useful} {Things} to {Know} about {Machine} {Learning}},
	volume = {55},
	number = {10},
	journal = {Communications of the ACM},
	author = {Domingos, Pedro},
	year = {2012},
	pages = {78--87},
}

@article{donoho_compressed_2006,
	title = {Compressed {sensing}},
	volume = {52},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Donoho, David L.},
	year = {2006},
	pages = {1289--1306},
}

@article{du_gradient_2019,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1811.03804 [cs, math, stat]},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	year = {2019},
}

@article{dumoulin_guide_nodate,
	title = {A {Guide} to {Convolution} {Arithmetic} for {Deep} {Learning}},
	author = {Dumoulin, Vincent and Visin, Francesco},
	pages = {31},
}

@article{durrett_probability_nodate,
	title = {Probability: {Theory} and {Examples}},
	author = {Durrett, Rick},
	pages = {386},
}

@article{fabian_3d_2020,
	title = {{3D} {Phase} {Retrieval} at {Nano}-{Scale} via {Accelerated} {Wirtinger} {Flow}},
	journal = {arXiv preprint arXiv:2002.11785},
	author = {Fabian, Zalan and Haldar, Justin and Leahy, Richard and Soltanolkotabi, Mahdi},
	year = {2020},
}

@misc{noauthor_fastmri_nodate,
	title = {{FastMRI}},
	url = {https://fastmri.org/leaderboards/},
}

@article{feng_golden-angle_2014,
	title = {Golden-{Angle} {Radial} {Sparse} {Parallel} {MRI}: {Combination} of {Compressed} {Sensing}, {Parallel} {Imaging}, and {Golden}-{Angle} {Radial} {Sampling} for {Fast} and {Flexible} {Dynamic} {Volumetric} {MRI}: {iGRASP}: {Iterative} {Golden}-{Angle} {RAdial} {Sparse} {Parallel} {MRI}},
	volume = {72},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Feng, Li and Grimm, Robert and Block, Kai Tobias and Chandarana, Hersh and Kim, Sungheon and Xu, Jian and Axel, Leon and Sodickson, Daniel K. and Otazo, Ricardo},
	year = {2014},
	pages = {707--717},
}

@article{foret_sharpness-aware_2021,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	journal = {arXiv:2010.01412 [cs, stat]},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	year = {2021},
}

@inproceedings{gatys_image_2016,
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	year = {2016},
	pages = {2414--2423},
}

@article{george_fast_2018,
	title = {Fast {Approximate} {Natural} {Gradient} {Descent} in a {Kronecker}-{Factored} {Eigenbasis}},
	journal = {arXiv:1806.03884 [cs, stat]},
	author = {George, Thomas and Laurent, César and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
	year = {2018},
}

@article{gilton_neumann_2019,
	title = {Neumann {Networks} for {Linear} {Inverse} {Problems} in {Imaging}},
	volume = {6},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gilton, Davis and Ongie, Greg and Willett, Rebecca},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {328--343},
}

@article{glorot_understanding_nodate,
	title = {Understanding the {Difficulty} of {Training} {Deep} {Feedforward} {Neural} {Networks}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
}

@article{goldfarb_practical_2020,
	title = {Practical {Quasi}-{Newton} {Methods} for {Training} {Deep} {Neural} {Networks}},
	journal = {arXiv:2006.08877 [cs, math, stat]},
	author = {Goldfarb, Donald and Ren, Yi and Bahamou, Achraf},
	year = {2020},
}

@article{gomez_reversible_nodate,
	title = {The {Reversible} {Residual} {Network}: {Backpropagation} {Without} {Storing} {Activations}},
	author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
	pages = {11},
}

@article{goyal_accurate_2018,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2018},
}

@article{goyal_self-supervised_2021,
	title = {Self-{Supervised} {Pretraining} of {Visual} {Features} in the {Wild}},
	journal = {arXiv:2103.01988 [cs]},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
	year = {2021},
}

@inproceedings{gregor_learning_2010,
	title = {Learning {Fast} {Approximations} of {Sparse} {Coding}},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	author = {Gregor, Karol and LeCun, Yann},
	year = {2010},
	pages = {399--406},
}

@article{grosse_kronecker-factored_nodate,
	title = {A {Kronecker}-{Factored} {Approximate} {Fisher} {Matrix} for {Convolution} {Layers}},
	author = {Grosse, Roger and Martens, James},
	pages = {10},
}

@article{gubner_probability_nodate,
	title = {{PROBABILITY} {AND} {RANDOM} {PROCESSES} {FOR} {ELECTRICAL} {AND} {COMPUTER} {ENGINEERS}},
	author = {Gubner, John A},
	pages = {642},
}

@article{guizar-sicairos_phase_2011,
	title = {Phase {Tomography} from {X}-{Ray} {Coherent} {Diffractive} {Imaging} {Projections}},
	volume = {19},
	number = {22},
	journal = {Optics Express},
	author = {Guizar-Sicairos, Manuel and Diaz, Ana and Holler, Mirko and Lucas, Miriam S. and Menzel, Andreas and Wepf, Roger A. and Bunk, Oliver},
	year = {2011},
	pages = {21345},
}

@article{guo_direct_2017,
	title = {Direct {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps} from {Highly} {Undersampled} {Brain} {Dynamic} {Contrast} {Enhanced} {MRI}: {Direct} {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps}},
	volume = {78},
	number = {4},
	journal = {Magnetic Resonance in Medicine},
	author = {Guo, Yi and Lingala, Sajan Goud and Zhu, Yinghua and Lebel, R. Marc and Nayak, Krishna S.},
	year = {2017},
	pages = {1566--1578},
}

@inproceedings{gupta_shampoo_2018,
	title = {Shampoo: {Preconditioned} {Stochastic} {Tensor} {Optimization}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gupta, Vineet and Koren, Tomer and Singer, Yoram},
	year = {2018},
	pages = {1842--1850},
}

@article{haghighi_transferable_2021,
	title = {Transferable {Visual} {Words}: {Exploiting} the {Semantics} of {Anatomical} {Patterns} for {Self}-{Supervised} {Learning}},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Haghighi, Fatemeh and Taher, Mohammad Reza Hosseinzadeh and Zhou, Zongwei and Gotway, Michael B. and Liang, Jianming},
	year = {2021},
	note = {Publisher: IEEE},
}

@book{hajek_random_2015,
	edition = {First},
	title = {Random {Processes} for {Engineers}},
	isbn = {978-1-107-10012-1 978-1-316-16460-0},
	publisher = {Cambridge University Press},
	author = {Hajek, Bruce},
	year = {2015},
}

@article{halko_finding_2010,
	title = {Finding {Structure} with {Randomness}: {Probabilistic} {Algorithms} for {Constructing} {Approximate} {Matrix} {Decompositions}},
	journal = {arXiv:0909.4061 [math]},
	author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
	year = {2010},
}

@article{hand_phase_2018,
	title = {Phase {Retrieval} {Under} a {Generative} {Prior}},
	journal = {arXiv:1807.04261 [cs, math]},
	author = {Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
	year = {2018},
}

@article{heckel_deep_nodate,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-{Convolutional} {Networks}},
	author = {Heckel, Reinhard and Hand, Paul},
	pages = {17},
}

@article{heckel_regularizing_2019,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{heckel_regularizing_2019-1,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{he_deep_2015,
	title = {Deep {residual} {learning} for {image} {recognition}},
	journal = {arXiv:1512.03385},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
}

@article{he_distributed_2017,
	title = {Distributed {Hessian}-{Free} {Optimization} for {Deep} {Neural} {Network}},
	journal = {arXiv:1606.00511 [cs, math]},
	author = {He, Xi and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Takáč, Martin},
	year = {2017},
}

@article{ho2022cascaded,
	title={Cascaded Diffusion Models for High Fidelity Image Generation.},
	author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	journal={J. Mach. Learn. Res.},
	volume={23},
	number={47},
	pages={1--33},
	year={2022}
}
@inproceedings{saharia2022palette,
	title={Palette: Image-to-image diffusion models},
	author={Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris and Ho, Jonathan and Salimans, Tim and Fleet, David and Norouzi, Mohammad},
	booktitle={ACM SIGGRAPH 2022 Conference Proceedings},
	pages={1--10},
	year={2022}
}

@article{kong2020diffwave,
	title={Diffwave: A versatile diffusion model for audio synthesis},
	author={Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	journal={arXiv preprint arXiv:2009.09761},
	year={2020}
}

@article{ho2022video,
	title={Video diffusion models},
	author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
	journal={arXiv preprint arXiv:2204.03458},
	year={2022}
}

@article{nichol2021glide,
	title={Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
	author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
	journal={arXiv preprint arXiv:2112.10741},
	year={2021}
}


@article{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2006.11239},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
}

@article{holler_high-resolution_2017,
	title = {High-{Resolution} {Non}-{Destructive} {Three}-{Dimensional} {Imaging} of {Integrated} {Circuits}},
	volume = {543},
	number = {7645},
	journal = {Nature},
	author = {Holler, Mirko and Guizar-Sicairos, Manuel and Tsai, Esther H. R. and Dinapoli, Roberto and Müller, Elisabeth and Bunk, Oliver and Raabe, Jörg and Aeppli, Gabriel},
	year = {2017},
	pages = {402--406},
}

@article{holler_omnytomography_2018,
	title = {{OMNY}—{A} {tOMography} {Nano} {crYo} {Stage}},
	journal = {Rev. Sci. Instrum.},
	author = {Holler, M and Raabe, J and Diaz, A and Guizar-Sicairos, M and Wepf, R and Odstrcil, M and Shaik, F R and Panneels, V and Menzel, A and Sarafimov, B and Maag, S and Wang, X and Thominet, V and Walther, H and Lachat, T and Vitins, M and Bunk, O},
	year = {2018},
	pages = {14},
}

@book{horn_matrix_2012,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Matrix {Analysis}},
	isbn = {978-0-521-83940-2},
	publisher = {Cambridge University Press},
	author = {Horn, Roger A. and Johnson, Charles R.},
	year = {2012},
	lccn = {QA188 .H66 2012},
}

@inproceedings{huang_densely_2017,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {2261--2269},
}

@article{huang_provably_2018,
	title = {A {Provably} {Convergent} {Scheme} for {Compressive} {Sensing} under {Random} {Generative} {Priors}},
	journal = {arXiv:1812.04176 [math]},
	author = {Huang, Wen and Hand, Paul and Heckel, Reinhard and Voroninski, Vladislav},
	year = {2018},
}

@article{hussain_differential_nodate,
	title = {Differential {Data} {Augmentation} {Techniques} for {Medical} {Imaging} {Classification} {Tasks}},
	author = {Hussain, Zeshan and Gimenez, Francisco and Yi, Darvin and Rubin, Daniel},
	pages = {6},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
}

@article{jacobsen_i-revnet_2018,
	title = {I-{RevNet}: {Deep} {Invertible} {Networks}},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	year = {2018},
}

@article{jalal_robust_2021,
	title = {Robust {Compressed} {Sensing} {MRI} with {Deep} {Generative} {Priors}},
	journal = {arXiv:2108.01368 [cs, math, stat]},
	author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G. and Tamir, Jonathan I.},
	year = {2021},
}

@article{jiang_linear_nodate,
	title = {A {Linear} {Speedup} {Analysis} of {Distributed} {Deep} {Learning} with {Sparse} and {Quantized} {Communication}},
	author = {Jiang, Peng and Agrawal, Gagan},
	pages = {12},
}

@article{jin_time-dependent_2019,
	title = {Time-{Dependent} {Deep} {Image} {Prior} for {Dynamic} {MRI}},
	journal = {arXiv:1910.01684 [cs, eess]},
	author = {Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jerome and Stuber, Matthias and Unser, Michael},
	year = {2019},
}

@inproceedings{jnawali_deep_2018,
	title = {Deep {3D} {Convolution} {Neural} {Network} for {CT} {Brain} {Hemorrhage} {Classification}},
	volume = {10575},
	booktitle = {Medical {Imaging} 2018: {Computer}-{Aided} {Diagnosis}},
	publisher = {International Society for Optics and Photonics},
	author = {Jnawali, Kamal and Arbabshirani, Mohammad R. and Rao, Navalgund and Patel, Alpen A.},
	year = {2018},
	pages = {105751C},
}

@inproceedings{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	year = {2016},
	pages = {694--711},
}

@article{kahnt_coupled_nodate,
	title = {Coupled {Ptychography} and {Tomography} {Algorithm} {Improves} {Reconstruction} of {Experimental} {Data}},
	author = {Kahnt, Maik and Becher, Johannes and Brückner, Dennis and Fam, Yakub and Sheppard, Thomas and Weissenberger, Tobias and Wittwer, Felix and Grunwaldt, Jan-Dierk and Schwieger, Wilhelm and Schroer, Christian G},
	pages = {8},
}

@article{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@article{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
}

@article{karras_style-based_nodate,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	pages = {10},
}

@article{karras_training_2020,
	title = {Training {Generative} {Adversarial} {Networks} with {Limited} {Data}},
	journal = {arXiv:2006.06676 [cs, stat]},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@inproceedings{keuper_distributed_2016,
	address = {Salt Lake City, UT, USA},
	title = {Distributed {Training} of {Deep} {Neural} {Networks}: {Theoretical} and {Practical} {Limits} of {Parallel} {Scalability}},
	isbn = {978-1-5090-3882-4},
	booktitle = {2016 2nd {Workshop} on {Machine} {Learning} in {HPC} {Environments} ({MLHPC})},
	publisher = {IEEE},
	author = {Keuper, Janis and Preundt, Franz-Josef},
	year = {2016},
	pages = {19--26},
}

@article{kingma_glow_nodate,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{kingma_glow_nodate-1,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{knoll_assessment_2019,
	title = {Assessment of the {Generalization} of {Learned} {Image} {Reconstruction} and the {Potential} for {Transfer} {Learning}},
	volume = {81},
	number = {1},
	journal = {Magnetic Resonance in Medicine},
	author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
	year = {2019},
	pages = {116--128},
}

@article{kobler_total_2020,
	title = {Total {Deep} {Variation} for {Linear} {Inverse} {Problems}},
	journal = {arXiv:2001.05005 [cs, math]},
	author = {Kobler, Erich and Effland, Alexander and Kunisch, Karl and Pock, Thomas},
	year = {2020},
}

@article{kobyzev_normalizing_2020,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, I. and Prince, S. and Brubaker, M.},
	year = {2020},
	pages = {1--1},
}

@article{korkmaz_unsupervised_2021,
	title = {Unsupervised {MRI} {Reconstruction} via {Zero}-{Shot} {Learned} {Adversarial} {Transformers}},
	journal = {arXiv:2105.08059 [cs, eess]},
	author = {Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and Özbey, Muzaffer and Çukur, Tolga},
	year = {2021},
}

@article{kreutz_complex_nodate,
	title = {The {Complex} {Gradient} {Operator} and the {CR}-{Calculus}},
	author = {Kreutz, Kenneth},
	pages = {74},
}

@article{krishnan_neumann_2017,
	title = {Neumann {Optimizer}: {A} {Practical} {Optimization} {Algorithm} for {Deep} {Neural} {Networks}},
	journal = {arXiv:1712.03298 [cs, stat]},
	author = {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A.},
	year = {2017},
}

@article{kuo_featmatch_2020,
	title = {{FeatMatch}: {Feature}-{Based} {Augmentation} for {Semi}-{Supervised} {Learning}},
	journal = {arXiv preprint arXiv:2007.08505},
	author = {Kuo, Chia-Wen and Ma, Chih-Yao and Huang, Jia-Bin and Kira, Zsolt},
	year = {2020},
}

@article{kwon_asam_2021,
	title = {{ASAM}: {Adaptive} {Sharpness}-{Aware} {Minimization} for {Scale}-{Invariant} {Learning} of {Deep} {Neural} {Networks}},
	journal = {arXiv:2102.11600 [cs, stat]},
	author = {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
	year = {2021},
}

@article{lauro_multiple_2020,
	title = {Multiple {Subglacial} {Water} {Bodies} below the {South} {Pole} of {Mars} {Unveiled} by {New} {MARSIS} {Data}},
	journal = {Nature Astronomy},
	author = {Lauro, Sebastian Emanuel and Pettinelli, Elena and Caprarelli, Graziella and Guallini, Luca and Rossi, Angelo Pio and Mattei, Elisabetta and Cosciotti, Barbara and Cicchetti, Andrea and Soldovieri, Francesco and Cartacci, Marco and Di Paolo, Federico and Noschese, Raffaella and Orosei, Roberto},
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
}

@article{lee_deep_2018,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	journal = {arXiv:1711.00165 [cs, stat]},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	year = {2018},
}

@article{lemley_smart_2017,
	title = {Smart {Augmentation} {Learning} an {Optimal} {Data} {Augmentation} {Strategy}},
	volume = {5},
	journal = {IEEE Access},
	author = {Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
	year = {2017},
	pages = {5858--5869},
}

@article{liang_swinir_2021,
	title = {{SwinIR}: {Image} {restoration} {using} {Swin} {Transformer}},
	journal = {arXiv:2108.10257},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
}

@article{li_communication_nodate,
	title = {Communication {Efficient} {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	author = {Li, Mu and Andersen, David G and Smola, Alexander and Yu, Kai},
	pages = {9},
}

@article{li_fourier_2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	journal = {arXiv:2010.08895 [cs, math]},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
}

@article{li_gan_2020,
	title = {{GAN} {Compression}: {Efficient} {Architectures} for {Interactive} {Conditional} {GANs}},
	journal = {arXiv:2003.08936 [cs, eess]},
	author = {Li, Muyang and Lin, Ji and Ding, Yaoyao and Liu, Zhijian and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{li_sacnn_2020,
	title = {{SACNN}: {Self}-{Attention} {Convolutional} {Neural} {Network} for {Low}-{Dose} {CT} {Denoising} with {Self}-{Supervised} {Perceptual} {Loss} {Network}},
	volume = {39},
	number = {7},
	journal = {IEEE transactions on medical imaging},
	author = {Li, Meng and Hsu, William and Xie, Xiaodong and Cong, Jason and Gao, Wen},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {2289--2301},
}

@article{liu_coupled_nodate,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@article{liu_coupled_nodate-1,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@incollection{liu_image_2018,
	address = {Cham},
	title = {Image {Inpainting} for {Irregular} {Holes} {Using} {Partial} {Convolutions}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
	year = {2018},
	pages = {89--105},
}

@article{liu_spark-based_2016,
	title = {Spark-{Based} {Large}-{Scale} {Matrix} {Inversion} for {Big} {Data} {Processing}},
	volume = {4},
	journal = {IEEE Access},
	author = {Liu, Jun and Liang, Yang and Ansari, Nirwan},
	year = {2016},
	pages = {2166--2176},
}

@article{liu_unsupervised_nodate,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{liu_unsupervised_nodate-1,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{long_deep_nodate,
	title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
	pages = {10},
}

@article{long_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
}

@article{lustig_compressed_2008,
	title = {Compressed {sensing} {MRI}},
	volume = {25},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Lustig, Michael and Donoho, David L. and Santos, Juan M. and Pauly, John M.},
	year = {2008},
	pages = {72--82},
}

@article{lutter_deep_2019,
	title = {Deep {Lagrangian} {Networks}: {Using} {Physics} as {Model} {Prior} for {Deep} {Learning}},
	journal = {arXiv:1907.04490 [cs, eess, stat]},
	author = {Lutter, Michael and Ritter, Christian and Peters, Jan},
	year = {2019},
}

@article{maiden_improved_2009,
	title = {An {Improved} {Ptychographical} {Phase} {Retrieval} {Algorithm} for {Diffractive} {Imaging}},
	volume = {109},
	number = {10},
	journal = {Ultramicroscopy},
	author = {Maiden, Andrew M. and Rodenburg, John M.},
	year = {2009},
	pages = {1256--1262},
}

@article{maiden_ptychographic_nodate,
	title = {Ptychographic {Transmission} {Microscopy} in {Three} {Dimensions} {Using} a {Multi}-{Slice} {Approach}},
	author = {Maiden, A M and Humphry, M J and Rodenburg, J M},
	pages = {9},
}

@article{ma_inefficiency_2019,
	title = {Inefficiency of {K}-{FAC} for {Large} {Batch} {Size} {Training}},
	journal = {arXiv:1903.06237 [cs, stat]},
	author = {Ma, Linjian and Montague, Gabe and Ye, Jiayu and Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2019},
}

@article{mallat_theory_nodate,
	title = {A {Theory} for {Multiresolution} {Signal} {Decomposition}: {The} {Wavelet} {Representation}},
	author = {Mallat, Stephane G},
	pages = {31},
}

@article{mardani_deep_2017,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{mardani_deep_2017-1,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{martens_deep_nodate,
	title = {Deep {Learning} via {Hessian}-{Free} {Optimization}},
	author = {Martens, James},
	pages = {8},
}

@article{martens_new_2014,
	title = {New {Insights} and {Perspectives} on the {Natural} {Gradient} {Method}},
	journal = {arXiv preprint arXiv:1412.1193},
	author = {Martens, James},
	year = {2014},
}

@article{martens_optimizing_nodate,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-{Factored} {Approximate} {Curvature}},
	author = {Martens, James and Grosse, Roger},
	pages = {10},
}

@article{martinsson_randomized_2011,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {47--68},
}

@article{martinsson_randomized_2011-1,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{martinsson_randomized_2011-2,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{menon_pulse_2020,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{menon_pulse_2020-1,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{metzler_prdeep_nodate,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with {Flexible} {Deep} {Neural} {Networks}},
	author = {Metzler, Christopher A and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G},
	pages = {10},
}

@article{metzler_prdeep_2018,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with a {Flexible} {Deep} {Network}},
	journal = {arXiv:1803.00212 [cs, stat]},
	author = {Metzler, Christopher A. and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G.},
	year = {2018},
}

@book{meyer_matrix_2000,
	address = {Philadelphia},
	title = {Matrix {Analysis} and {Applied} {Linear} {Algebra}},
	isbn = {978-0-89871-454-8},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Meyer, C. D.},
	year = {2000},
	lccn = {QA188 .M495 2000},
}

@misc{noauthor_models_nodate,
	title = {Models {Genesis} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1361841520302048?token=B6B4930C1609C04FD49C6498E7935D83275465F07F9155D2DA9C09FBACAA0F60708219F8E62F34E2DB5319E5BC0CA8E0&originRegion=us-east-1&originCreation=20210914005025},
}

@article{moulin_analysis_1999,
	title = {Analysis of {Multiresolution} {Image} {Denoising} {Schemes} {Using} {Generalized} {Gaussian} and {Complexity} {Priors}},
	volume = {45},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Moulin, P. and {Juan Liu}},
	year = {1999},
	pages = {909--919},
}

@article{myagotin_efficient_2013,
	title = {Efficient {Volume} {Reconstruction} for {Parallel}-{Beam} {Computed} {Laminography} by {Filtered} {Backprojection} on {Multi}-{Core} {Clusters}},
	volume = {22},
	number = {12},
	journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
	author = {Myagotin, Anton and Voropaev, Alexey and Helfen, Lukas and Hänschke, Daniel and Baumbach, Tilo},
	year = {2013},
	pages = {14},
}

@article{nagle-mcnaughton_planet_2020,
	title = {{PlaNet}: {A} {Neural} {Network} for {Detecting} {Transverse} {Aeolian} {Ridges} on {Mars}},
	volume = {12},
	number = {21},
	journal = {Remote Sensing},
	author = {Nagle-McNaughton, Timothy and McClanahan, Timothy and Scuderi, Louis},
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {3607},
}

@inproceedings{narayanan_pipedream_2019,
	address = {Huntsville Ontario Canada},
	title = {{PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}},
	isbn = {978-1-4503-6873-5},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	year = {2019},
	pages = {1--15},
}

@article{neyshabur_towards_2018,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{neyshabur_towards_2018-1,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2102.09672},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	year = {2021},
}

@misc{noauthor_nie_nodate,
	title = {Nie: {Medical} {Image} {Synthesis} with {Deep} {Convolutional}... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?title=Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks&author=D.%20Nie&publication_year=2018},
}

@article{nie_medical_2018,
	title = {Medical {Image} {Synthesis} with {Deep} {Convolutional} {Adversarial} {Networks}},
	volume = {65},
	number = {12},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Nie, D. and Trullo, R. and Lian, J. and Wang, L. and Petitjean, C. and Ruan, S. and Wang, Q. and Shen, D.},
	year = {2018},
	pages = {2720--2730},
}

@book{nocedal_numerical_1999,
	address = {New York},
	series = {Springer {Series} in {Operations} {Research}},
	title = {Numerical {Optimization}},
	isbn = {978-0-387-98793-4},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {1999},
	lccn = {QA402.5 .N62 1999},
}

@article{oktay_attention_2018,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	journal = {arXiv preprint arXiv:1804.03999},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard},
	year = {2018},
}

@article{ongie_deep_2020,
	title = {Deep {learning} {techniques} for {inverse} {problems} in {imaging}},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Baraniuk, Christopher A. Metzler Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	year = {2020}
}

@article{ongie_function_2019,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	journal = {arXiv:1910.01635 [cs, stat]},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@inproceedings{osawa_large-scale_2019,
	title = {Large-{Scale} {Distributed} {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximate} {Curvature} for {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
	year = {2019},
	pages = {12359--12367},
}

@article{osullivan_fast_1985,
	title = {A {Fast} {Sinc} {Function} {Gridding} {Algorithm} for {Fourier} {Inversion} in {Computer} {Tomography}},
	volume = {4},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {O'Sullivan, J. D.},
	year = {1985},
	pages = {200--207},
}

@article{oymak_generalization_2019,
	title = {Generalization {Guarantees} for {Neural} {Networks} via {Harnessing} the {Low}-{Rank} {Structure} of the {Jacobian}},
	journal = {arXiv:1906.05392 [cs, math, stat]},
	author = {Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
	year = {2019},
}

@article{oymak_towards_nodate,
	title = {Towards {Moderate} {Overparameterization}: {Global} {Convergence} {Guarantees} for {Training} {Shallow} {Neural} {Networks}},
	author = {Oymak, Samet and Soltanolkotabi, Mahdi},
	pages = {41},
}

@article{pardoe_boosting_nodate,
	title = {Boosting for {Regression} {Transfer}},
	author = {Pardoe, David and Stone, Peter},
	pages = {8},
}

@article{pauloski_convolutional_nodate,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	author = {Pauloski, J Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T},
	pages = {11},
}

@article{pauloski_convolutional_2020,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-1,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-2,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-3,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{petersen__nodate,
	title = {[ http://matrixcookbook.com ]},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	pages = {72},
}

@article{pezzotti_adaptive-cs-net_2019,
	title = {Adaptive-{CS}-{Net}: {FastMRI} with {Adaptive} {Intelligence}},
	journal = {arXiv:1912.12259 [eess]},
	author = {Pezzotti, Nicola and de Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and van Gemert, Jeroen and Schülke, Christophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and van Osch, Matthias J. P. and Staring, Marius},
	year = {2019},
}

@inproceedings{pham_phaseless_2018,
	address = {Washington, DC},
	title = {Phaseless {Diffraction} {Tomography} with {Regularized} {Beam} {Propagation}},
	isbn = {978-1-5386-3636-7},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Pham, Thanh-an and Soubies, Emmanuel and Lim, Joowon and Goy, Alexandre and Soulez, Ferreol and Psaltis, Demetri and Unser, Michael},
	year = {2018},
	pages = {1268--1271},
}

@article{pilanci_newton_2015,
	title = {Newton {Sketch}: {A} {Linear}-{Time} {Optimization} {Algorithm} with {Linear}-{Quadratic} {Convergence}},
	journal = {arXiv:1505.02250 [cs, math, stat]},
	author = {Pilanci, Mert and Wainwright, Martin J.},
	year = {2015},
}

@article{powell_algorithms_1978,
	title = {Algorithms for {Nonlinear} {Constraints} {That} {Use} {Lagrangian} {Functions}},
	volume = {14},
	number = {1},
	journal = {Mathematical Programming},
	author = {Powell, M. J. D.},
	year = {1978},
	pages = {224--248},
}

@inproceedings{putzky_invert_2019,
	title = {Invert to {Learn} to {Invert}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Putzky, Patrick and Welling, Max},
	year = {2019},
	pages = {446--456},
}

@article{putzky_i-rim_2019,
	title = {I-{RIM} {applied} to the {fastMRI} {Challenge}},
	journal = {arXiv:1910.08952},
	author = {Putzky, Patrick and Karkalousos, Dimitrios and Teuwen, Jonas and Miriakov, Nikita and Bakker, Bart and Caan, Matthan and Welling, Max},
	year = {2019},
}

@article{putzky_recurrent_2017,
	title = {Recurrent {Inference} {Machines} for {Solving} {Inverse} {Problems}},
	journal = {arXiv:1706.04008 [cs]},
	author = {Putzky, Patrick and Welling, Max},
	year = {2017},
}

@article{qin_convolutional_2019,
	title = {Convolutional {Recurrent} {Neural} {Networks} for {Dynamic} {MR} {Image} {Reconstruction}},
	volume = {38},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Qin, Chen and Schlemper, Jo and Caballero, Jose and Price, Anthony N. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
	pages = {280--290},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2016},
}

@inproceedings{rafati_improving_2018,
	title = {Improving {L}-{BFGS} {Initialization} for {Trust}-{Region} {Methods} in {Deep} {Learning}},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Rafati, Jacob and Marcia, Roummel F.},
	year = {2018},
	pages = {501--508},
}

@article{ren_benchmarking_2021,
	title = {Benchmarking {Deep} {Inverse} {Models} over {Time}, and the {Neural}-{Adjoint} {Method}},
	journal = {arXiv:2009.12919 [cs, eess, stat]},
	author = {Ren, Simiao and Padilla, Willie and Malof, Jordan},
	year = {2021},
}

@article{noauthor_representations_nodate,
	title = {Representations of {Quasi}-{Newton} {Matrices} and {Their} {Use} in {Limited} {Memory} {Methods}},
	pages = {28},
}

@article{rezende_variational_nodate,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	pages = {10},
}

@article{saharia_image_2021,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	journal = {arXiv:2104.07636 [cs, eess]},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	year = {2021},
}

@article{sajjad_multi-grade_2019,
	title = {Multi-{Grade} {Brain} {Tumor} {Classification} {Using} {Deep} {CNN} with {Extensive} {Data} {Augmentation}},
	volume = {30},
	journal = {Journal of Computational Science},
	author = {Sajjad, Muhammad and Khan, Salman and Muhammad, Khan and Wu, Wanqing and Ullah, Amin and Baik, Sung Wook},
	year = {2019},
	pages = {174--182},
}

@article{salimans_weight_2016,
	title = {Weight {Normalization}: {A} {Simple} {Reparameterization} to {Accelerate} {Training} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1602.07868 [cs]},
	author = {Salimans, Tim and Kingma, Diederik P.},
	year = {2016},
}

@article{salman_provably_2020,
	title = {Provably {Robust} {Deep} {Learning} via {Adversarially} {Trained} {Smoothed} {Classifiers}},
	journal = {arXiv:1906.04584 [cs, stat]},
	author = {Salman, Hadi and Yang, Greg and Li, Jerry and Zhang, Pengchuan and Zhang, Huan and Razenshteyn, Ilya and Bubeck, Sebastien},
	year = {2020},
}

@article{savarese_how_2019,
	title = {How {Do} {Infinite} {Width} {Bounded} {Norm} {Networks} {Look} in {Function} {Space}?},
	journal = {arXiv:1902.05040 [cs, stat]},
	author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@book{sawyer_creation_nodate,
	title = {Creation of {Fully} {Sampled} {MR} {Data} {Repository} for {Compressed} {Sensing} of the {Knee}},
	author = {Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas and Healthcare, Ge},
}

@article{schlemper_dautomap_2019,
	title = {{dAUTOMAP}: {Decomposing} {AUTOMAP} to {Achieve} {Scalability} and {Enhance} {Performance}},
	journal = {arXiv:1909.10995 [cs, eess, stat]},
	author = {Schlemper, Jo and Oksuz, Ilkay and Clough, James R. and Duan, Jinming and King, Andrew P. and Schnabel, Julia A. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
}

@article{schlemper_deep_2017,
	title = {A {Deep} {Cascade} of {Convolutional} {Neural} {Networks} for {MR} {Image} {Reconstruction}},
	journal = {arXiv:1703.00555 [cs]},
	author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony and Rueckert, Daniel},
	year = {2017},
}

@book{shalev-shwartz_understanding_2014,
	address = {Cambridge},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
}

@article{shazeer_mesh-tensorflow_2018,
	title = {Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}},
	journal = {arXiv:1811.02084 [cs, stat]},
	author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
	year = {2018},
}

@article{shechtman_phase_nodate,
	title = {Phase {Retrieval} with {Application} to {Optical} {Imaging}},
	author = {Shechtman, Yoav and Eldar, Yonina C and Cohen, Oren and Chapman, Henry N and Miao, Jianwei and Segev, Mordechai},
	pages = {23},
}

@article{sheikh_image_nodate,
	title = {{IMAGE} {INFORMATION} {AND} {VISUAL} {QUALITY}},
	author = {Sheikh, Hamid R and Bovik, Alan C},
	pages = {4},
}

@article{shi_is_nodate,
	title = {Is the {Deconvolution} {Layer} the {Same} as a {Convolutional} {Layer}?},
	author = {Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Ledig, Christian and Wang, Zehan},
	pages = {7},
}

@inproceedings{shi_real-time_2016,
	address = {Las Vegas, NV, USA},
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	isbn = {978-1-4673-8851-1},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Wenzhe and Caballero, Jose and Huszar, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	year = {2016},
	pages = {1874--1883},
}

@article{shorten_survey_2019,
	title = {A {Survey} on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
	pages = {60},
}

@inproceedings{soltanolkotabi_3d_2019,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	booktitle = {2019 13th {International} {Conference} on {Sampling} {Theory} and {Applications} ({SampTA})},
	publisher = {IEEE},
	author = {Soltanolkotabi, Mahdi},
	year = {2019},
	pages = {1--3},
}

@article{song_generative_2020,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	journal = {arXiv:1907.05600 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{song_improved_2020,
	title = {Improved {Techniques} for {Training} {Score}-{Based} {Generative} {Models}},
	journal = {arXiv:2006.09011 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{steihaug_conjugate_1983,
	title = {The {Conjugate} {Gradient} {Method} and {Trust} {Regions} in {Large} {Scale} {Optimization}},
	volume = {20},
	number = {3},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Steihaug, Trond},
	year = {1983},
	pages = {626--637},
}

@article{stockmar_x-ray_2015,
	title = {X-{Ray} {Nanotomography} {Using} near-{Field} {Ptychography}},
	volume = {23},
	number = {10},
	journal = {Optics Express},
	author = {Stockmar, Marco and Hubert, Maxime and Dierolf, Martin and Enders, Bjoern and Clare, Richard and Allner, Sebastian and Fehringer, Andreas and Zanette, Irene and Villanova, Julie and Laurencin, Jérôme and Cloetens, Peter and Pfeiffer, Franz and Thibault, Pierre},
	year = {2015},
	pages = {12720},
}

@book{strang_linear_2006,
	address = {Belmont, CA},
	edition = {4th ed},
	title = {Linear {Algebra} and {Its} {Applications}},
	isbn = {978-0-03-010567-8},
	publisher = {Thomson, Brooks/Cole},
	author = {Strang, Gilbert},
	year = {2006},
	lccn = {QA184.2 .S77 2006},
}

@article{tan_survey_2018,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	journal = {arXiv:1808.01974 [cs, stat]},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	year = {2015},
}

@inproceedings{tzeng_adversarial_2017,
	address = {Honolulu, HI},
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	year = {2017},
	pages = {2962--2971},
}

@article{uecker_espirit_2014,
	title = {{ESPIRiT}— an {Eigenvalue} {Approach} to {Autocalibrating} {Parallel} {MRI}: {Where} {SENSE} {Meets} {GRAPPA}},
	volume = {71},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Uecker, Martin and Lai, Peng and Murphy, Mark J. and Virtue, Patrick and Elad, Michael and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2014},
	pages = {990--1001},
}

@article{ulyanov_instance_2017,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	year = {2017},
}

@article{unser_family_1993,
	title = {A {Family} of {Polynomial} {Spline} {Wavelet} {Transforms}},
	volume = {30},
	number = {2},
	journal = {Signal Processing},
	author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
	year = {1993},
	pages = {141--162},
}

@article{van_veen_compressed_2019,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{van_veen_compressed_2019-1,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{vershynin_high-dimensional_nodate,
	title = {High-{Dimensional} {Probability}},
	author = {Vershynin, Roman},
	pages = {301},
}

@article{wang_dense_2021,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	journal = {arXiv:2011.09157 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
}

@article{wang_pyramid_2020,
	title = {Pyramid {Convolutional} {RNN} for {MRI} {Reconstruction}},
	journal = {arXiv:1912.00543 [cs, eess, stat]},
	author = {Wang, Puyang and Chen, Eric Z. and Chen, Terrence and Patel, Vishal M. and Sun, Shanhui},
	year = {2020},
}

@inproceedings{wang_residual_2017,
	address = {Honolulu, HI},
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {6450--6458},
}

@inproceedings{wu_group_2018,
	title = {Group {Normalization}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Wu, Yuxin and He, Kaiming},
	year = {2018},
	pages = {3--19},
}

@article{xie_self-training_2020,
	title = {Self-{Training} with {Noisy} {Student} {Improves} {ImageNet} {Classification}},
	journal = {arXiv:1911.04252 [cs, stat]},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2020},
}

@article{xu_accelerated_nodate,
	title = {Accelerated {Wirtinger} {Flow}: {A} {New} {Fast} {Algorithm} for {Phase} {Retrieval}},
	author = {Xu, Rui and Soltanolkotabi, Mahdi and Haldar, Justin P and Zusman, Joshua and Levi, Anthony F J},
	pages = {18},
}

@article{yang_dagan_2018,
	title = {{DAGAN}: {Deep} {De}-{Aliasing} {Generative} {Adversarial} {Networks} for {Fast} {Compressed} {Sensing} {MRI} {Reconstruction}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and Firmin, David},
	year = {2018},
	pages = {1310--1321},
}

@article{yang_deep_nodate,
	title = {Deep {ADMM}-{Net} for {Compressive} {Sensing} {MRI}},
	author = {Yang, Yan and Li, Huibin and Sun, Jian and Xu, Zongben},
	pages = {9},
}

@article{yang_deep_2019,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_deep_2019-1,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_low_2018,
	title = {Low {Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Wang, Ge},
	year = {2018},
	pages = {1348--1357},
}

@article{yang_low-dose_2018,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@article{yang_low-dose_2018-1,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@inproceedings{yao_pyhessian_2020,
	address = {Atlanta, GA, USA},
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	isbn = {978-1-72816-251-5},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
}

@article{yi_generative_2019,
	title = {Generative {Adversarial} {Network} in {Medical} {Imaging}: {A} {Review}},
	volume = {58},
	journal = {Medical Image Analysis},
	author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
	year = {2019},
	pages = {101552},
}

@article{yu_generative_nodate,
	title = {Generative {Image} {Inpainting} with {Contextual} {Attention}},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
	pages = {15},
}

@article{zavriev_heavy-ball_1993,
	title = {Heavy-{Ball} {Method} in {Nonconvex} {Optimization} {Problems}},
	volume = {4},
	number = {4},
	journal = {Computational Mathematics and Modeling},
	author = {Zavriev, S. K. and Kostyuk, F. V.},
	year = {1993},
	pages = {336--341},
}

@article{zbontar_fastmri_2019,
	title = {{fastMRI}: {An} {open} {dataset} and {benchmarks} for {accelerated} {MRI}},
	journal = {arXiv:1811.08839},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	year = {2019},
}

@article{zhang_coil_2013,
	title = {Coil {Compression} for {Accelerated} {Imaging} with {Cartesian} {Sampling}},
	volume = {69},
	number = {2},
	journal = {Magnetic Resonance in Medicine},
	author = {Zhang, Tao and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2013},
	pages = {571--582},
}

@inproceedings{zhang_deep_2018,
	title = {Deep {Imitation} {Learning} for {Complex} {Manipulation} {Tasks} from {Virtual} {Reality} {Teleoperation}},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
	year = {2018},
	pages = {1--8},
}

@article{zhang_beyond_2017,
	title = {Beyond a {Gaussian} {Denoiser}: {Residual} {Learning} of {Deep} {CNN} for {Image} {Denoising}},
	volume = {26},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
	year = {2017},
	pages = {3142--3155},
}

@inproceedings{zhang_ista-net_2018,
	title = {{ISTA}-{Net}: {Interpretable} {optimization}-{inspired} {deep} {network} for {image} {compressive} {sensing}},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Jian and Ghanem, Bernard},
	year = {2018},
	pages = {1828--1837},
}

@article{zhang_understanding_2017,
	title = {Understanding {Deep} {Learning} {Requires} {Rethinking} {Generalization}},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
}

@article{zhao_differentiable_2020,
	title = {Differentiable {Augmentation} for {Data}-{Efficient} {GAN} {Training}},
	journal = {arXiv:2006.10738 [cs]},
	author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{zhao_image_2020,
	title = {Image {Augmentations} for {GAN} {Training}},
	journal = {arXiv:2006.02595 [cs, eess, stat]},
	author = {Zhao, Zhengli and Zhang, Zizhao and Chen, Ting and Singh, Sameer and Zhang, Han},
	year = {2020},
}

@inproceedings{zhu_unpaired_2017,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@inproceedings{zhu_unpaired_2017-1,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@misc{noauthor_3d_nodate-1,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	url = {https://www.researchgate.net/publication/339908871_3D_Phaseless_Imaging_at_Nano-scale_Challenges_and_Possible_Solutions},
	note = {Publication Title: ResearchGate},
}

@article{agarwal_efficient_2020-1,
	title = {Efficient {Full}-{Matrix} {Adaptive} {Regularization}},
	journal = {arXiv:1806.02958 [cs, math, stat]},
	author = {Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
	year = {2020},
}

@article{aggarwal_modl_2018-1,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	number = {2},
	journal = {IEEE transactions on medical imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {394--405},
}

@article{akiyama_first_2019-1,
	title = {First {M87} {Event} {Horizon} {Telescope} {Results}. {IV}. {Imaging} the {Central} {Supermassive} {Black} {Hole}},
	journal = {The Astrophysical Journal Letters},
	author = {Akiyama, Kazunori},
	year = {2019},
	pages = {52},
}

@article{anil_second_nodate-1,
	title = {Second {Order} {Optimization} {Made} {Practical}},
	author = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
	pages = {19},
}

@article{ardizzone_analyzing_2019-1,
	title = {Analyzing {Inverse} {Problems} with {Invertible} {Neural} {Networks}},
	journal = {arXiv:1808.04730 [cs, stat]},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	year = {2019},
}

@article{arora_fine-grained_2019-1,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	journal = {arXiv:1901.08584 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	year = {2019},
}

@article{arpit_benefits_2019-1,
	title = {The {Benefits} of {Over}-{Parameterization} at {Initialization} in {Deep} {ReLU} {Networks}},
	journal = {arXiv:1901.03611 [cs, stat]},
	author = {Arpit, Devansh and Bengio, Yoshua},
	year = {2019},
}

@article{asano_self-labelling_2019,
	title = {Self-{Labelling} via {Simultaneous} {Clustering} and {Representation} {Learning}},
	journal = {arXiv preprint arXiv:1911.05371},
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year = {2019},
}

@article{aslan_distributed_2020-1,
	title = {Distributed {Optimization} with {Tunable} {Learned} {Priors} for {Robust} {Ptycho}-{Tomography}},
	journal = {arXiv:2009.09498 [eess, math]},
	author = {Aslan, Selin and Liu, Zhengchun and Nikitin, Viktor and Bicer, Tekin and Leyffer, Sven and Gursoy, Doga},
	year = {2020},
}

@article{bachman_learning_2019,
	title = {Learning {Representations} by {Maximizing} {Mutual} {Information} {Across} {Views}},
	journal = {arXiv:1906.00910 [cs, stat]},
	author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
	year = {2019},
}

@article{ba_distributed_2016-1,
	title = {Distributed {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximations}},
	author = {Ba, Jimmy and Grosse, Roger and Martens, James},
	year = {2016},
}

@article{ba_layer_2016-1,
	title = {Layer {Normalization}},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year = {2016},
}

@article{barbastathis_use_2019-2,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barbastathis_use_2019-3,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barutcu_limited-angle_2021-1,
	title = {Limited-{Angle} {Computed} {Tomography} with {Deep} {Image} and {Physics} {Priors}},
	volume = {11},
	number = {1},
	journal = {Scientific Reports},
	author = {Barutcu, Semih and Aslan, Selin and Katsaggelos, Aggelos K. and Gürsoy, Doğa},
	year = {2021},
	pages = {17740},
}

@article{beatty_rapid_2005-1,
	title = {Rapid {Gridding} {Reconstruction} with a {Minimal} {Oversampling} {Ratio}},
	volume = {24},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Beatty, P.J. and Nishimura, D.G. and Pauly, J.M.},
	year = {2005},
	pages = {799--808},
}

@article{becker_self-organizing_1992,
	title = {Self-{Organizing} {Neural} {Network} {That} {Discovers} {Surfaces} in {Random}-{Dot} {Stereograms}},
	volume = {355},
	number = {6356},
	journal = {Nature},
	author = {Becker, Suzanna and Hinton, Geoffrey E.},
	year = {1992},
	pages = {161--163},
}

@article{ben-nun_demystifying_2019-1,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-{Depth} {Concurrency} {Analysis}},
	volume = {52},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	year = {2019},
	pages = {1--43},
}

@article{bernstein_distance_2020-1,
	title = {On the {Distance} between {Two} {Neural} {Networks} and the {Stability} of {Learning}},
	journal = {arXiv:2002.03432 [cs, math, stat]},
	author = {Bernstein, Jeremy and Vahdat, Arash and Yue, Yisong and Liu, Ming-Yu},
	year = {2020},
}

@book{bertero_introduction_1998-1,
	address = {Bristol, UK ; Philadelphia, Pa},
	title = {Introduction to {Inverse} {Problems} in {Imaging}},
	isbn = {978-0-7503-0439-9 978-0-7503-0435-1},
	publisher = {Institute of Physics Pub},
	author = {Bertero, Mario and Boccacci, Patrizia},
	year = {1998},
	lccn = {TA1637 .B47 1998},
}

@book{bertsekas_introduction_2002-1,
	address = {Belmont, Mass},
	edition = {2. print},
	series = {Optimization and {Computation} {Series}},
	title = {Introduction to {Probability}},
	isbn = {978-1-886529-40-3},
	number = {1},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	year = {2002},
}

@book{bhatia_matrix_1997-1,
	address = {New York},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Matrix {Analysis}},
	isbn = {978-0-387-94846-1},
	number = {169},
	publisher = {Springer},
	author = {Bhatia, Rajendra},
	year = {1997},
	lccn = {QA188 .B485 1997},
}

@article{bora_ambientgan_2018-1,
	title = {{AMBIENTGAN}: {GENERATIVE} {MODELS} {FROM} {LOSSY} {MEASUREMENTS}},
	author = {Bora, Ashish and Dimakis, Alexandros G},
	year = {2018},
	pages = {22},
}

@article{bora_compressed_nodate-1,
	title = {Compressed {Sensing} {Using} {Generative} {Models}},
	author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alex},
	pages = {10},
}

@article{bostan_deep_2020-1,
	title = {Deep {Phase} {Decoder}: {Self}-{Calibrating} {Phase} {Microscopy} with an {Untrained} {Deep} {Neural} {Network}},
	journal = {arXiv:2001.09803 [physics]},
	author = {Bostan, Emrah and Heckel, Reinhard and Chen, Michael and Kellman, Michael and Waller, Laura},
	year = {2020},
}

@article{bottou_optimization_2018-1,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	volume = {60},
	number = {2},
	journal = {SIAM Review},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	year = {2018},
	pages = {223--311},
}

@book{bovik_essential_2009-1,
	address = {London ; Boston},
	title = {The {Essential} {Guide} to {Image} {Processing}},
	isbn = {978-0-12-374457-9},
	publisher = {Academic Press},
	author = {Bovik, Alan C.},
	year = {2009},
	lccn = {TA1637 .B68 2009},
}

@book{boyd_convex_2004-1,
	address = {Cambridge, UK ; New York},
	title = {Convex {Optimization}},
	isbn = {978-0-521-83378-3},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	lccn = {QA402.5 .B69 2004},
}

@article{boyd_neal_nodate-1,
	title = {Neal {Parikh} {Department} of {Computer} {Science} {Stanford} {University}},
	author = {Boyd, Stephen},
	pages = {113},
}

@article{brock_high-performance_2021-1,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	year = {2021},
}

@article{brock_large_2019-1,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	year = {2019},
}

@article{bronstein_geometric_2017-1,
	title = {Geometric {Deep} {Learning}: {Going} beyond {Euclidean} {Data}},
	volume = {34},
	number = {4},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	year = {2017},
	pages = {18--42},
}

@article{burdakov_efficiently_2017-1,
	title = {On {Efficiently} {Combining} {Limited}-{Memory} and {Trust}-{Region} {Techniques}},
	volume = {9},
	number = {1},
	journal = {Mathematical Programming Computation},
	author = {Burdakov, Oleg and Gong, Lujin and Zikrin, Spartak and Yuan, Ya-xiang},
	year = {2017},
	pages = {101--134},
}

@article{candes_phase_2015-2,
	title = {Phase {Retrieval} from {Coded} {Diffraction} {Patterns}},
	volume = {39},
	number = {2},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Candès, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {277--299},
}

@article{candes_phase_2015-3,
	title = {Phase {Retrieval} via {Wirtinger} {Flow}: {Theory} and {Algorithms}},
	volume = {61},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Candes, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {1985--2007},
}

@article{candes_stable_2006-1,
	title = {Stable {Signal} {Recovery} from {Incomplete} and {Inaccurate} {Measurements}},
	volume = {59},
	number = {8},
	journal = {Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
	author = {Candes, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
	year = {2006},
	note = {Publisher: Wiley Online Library},
	pages = {1207--1223},
}

@inproceedings{caron_deep_2018,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year = {2018},
	pages = {132--149},
}

@article{caron_unsupervised_2021,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	journal = {arXiv:2006.09882 [cs]},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
}

@article{noauthor_chapter_nodate-2,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@article{noauthor_chapter_nodate-3,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@inproceedings{chartsias_adversarial_2017-4,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-5,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-6,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-7,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@article{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	journal = {arXiv:2011.10566 [cs]},
	author = {Chen, Xinlei and He, Kaiming},
	year = {2020},
}

@article{chen_simple_2020-1,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year = {2020},
}

@article{chen_simple_2020-2,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year = {2020},
}

@article{child_generating_2019,
	title = {Generating {Long} {Sequences} with {Sparse} {Transformers}},
	journal = {arXiv:1904.10509 [cs, stat]},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	year = {2019},
}

@article{chung_empirical_2014-1,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	year = {2014},
}

@article{cohen_certified_2019-1,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	journal = {arXiv:1902.02918 [cs, stat]},
	author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
	year = {2019},
}

@book{noauthor_introduction_2009-1,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to {Algorithms}},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	year = {2009},
	lccn = {QA76.6 .C662 2009},
}

@article{cranmer_lagrangian_2020-1,
	title = {Lagrangian {Neural} {Networks}},
	journal = {arXiv:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	year = {2020},
}

@inproceedings{cubuk_autoaugment_2019-1,
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2019},
	pages = {113--123},
}

@article{curtis_trust_2017-1,
	title = {A {Trust} {Region} {Algorithm} with a {Worst}-{Case} {Iteration} {Complexity} of \$\${\textbackslash}textbackslash mathcal\{{\textbackslash}vphantom\}{O}{\textbackslash}vphantom\{\}({\textbackslash}textbackslash epsilon {\textasciicircum}\{-3/2\})\$\$ {O} ( ϵ - 3 / 2 ) for {Nonconvex} {Optimization}},
	volume = {162},
	number = {1-2},
	journal = {Mathematical Programming},
	author = {Curtis, Frank E. and Robinson, Daniel P. and Samadi, Mohammadreza},
	year = {2017},
	pages = {1--32},
}

@book{daubechies_ten_1992-1,
	address = {Philadelphia, Pa},
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Ten {Lectures} on {Wavelets}},
	isbn = {978-0-89871-274-2},
	number = {61},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Daubechies, Ingrid},
	year = {1992},
	lccn = {QA403.3 .D38 1992},
}

@article{davison_singular_1981-1,
	title = {A {Singular} {Value} {Decomposition} for the {Radon} {Transform} in {\textbackslash}emphn -{Dimensional} {Euclidean} {Space}},
	volume = {3},
	number = {3},
	journal = {Numerical Functional Analysis and Optimization},
	author = {Davison, M. E.},
	year = {1981},
	pages = {321--340},
}

@article{dean_large_nodate-1,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
	pages = {9},
}

@article{defazio_mri_2020-1,
	title = {{MRI} {Banding} {Removal} via {Adversarial} {Training}},
	journal = {arXiv:2001.08699 [cs, eess, stat]},
	author = {Defazio, Aaron and Murrell, Tullie and Recht, Michael P.},
	year = {2020},
}

@inproceedings{derezinski_distributed_2019,
	title = {Distributed {Estimation} of the {Inverse} {Hessian} by {Determinantal} {Averaging}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Derezinski, Michal and Mahoney, Michael W.},
	year = {2019},
	pages = {11405--11415},
}

@article{devlin_bert_2018,
	title = {Bert: {Pre}-{Training} of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
}

@article{devlin_bert_2019-1,
	title = {{BERT}: {Pre}-{Training} of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
}

@article{dhariwal_diffusion_2021-1,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	journal = {arXiv preprint arXiv:2105.05233},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	year = {2021},
}

@article{dierolf_ptychographic_nodate-1,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	author = {Dierolf, Martin},
	pages = {6},
}

@article{dierolf_ptychographic_2010-1,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	volume = {467},
	number = {7314},
	journal = {Nature},
	author = {Dierolf, Martin and Menzel, Andreas and Thibault, Pierre and Schneider, Philipp and Kewish, Cameron M. and Wepf, Roger and Bunk, Oliver and Pfeiffer, Franz},
	year = {2010},
	pages = {436--439},
}

@article{dinh_nice_2015-1,
	title = {{NICE}: {Non}-{Linear} {Independent} {Components} {Estimation}},
	journal = {arXiv:1410.8516 [cs]},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	year = {2015},
}

@article{domingos_few_2012-1,
	title = {A {Few} {Useful} {Things} to {Know} about {Machine} {Learning}},
	volume = {55},
	number = {10},
	journal = {Communications of the ACM},
	author = {Domingos, Pedro},
	year = {2012},
	pages = {78--87},
}

@article{donoho_compressed_2006-1,
	title = {Compressed {Sensing}},
	volume = {52},
	number = {4},
	journal = {IEEE Transactions on information theory},
	author = {Donoho, David L.},
	year = {2006},
	note = {Publisher: IEEE},
	pages = {1289--1306},
}

@article{dosovitskiy_image_2020,
	title = {An {image} {is} {worth} 16x16 {words}: {Transformers} for {image} {recognition} at {scale}},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2020},
}

@article{du_gradient_2019-1,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1811.03804 [cs, math, stat]},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	year = {2019},
}

@article{dumoulin_guide_nodate-1,
	title = {A {Guide} to {Convolution} {Arithmetic} for {Deep} {Learning}},
	author = {Dumoulin, Vincent and Visin, Francesco},
	pages = {31},
}

@article{durrett_probability_nodate-1,
	title = {Probability: {Theory} and {Examples}},
	author = {Durrett, Rick},
	pages = {386},
}

@article{el-nouby_xcit_2021,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	journal = {arXiv:2106.09681 [cs]},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Hervé},
	year = {2021},
}

@article{fabian_3d_2020-1,
	title = {{3D} {Phase} {Retrieval} at {Nano}-{Scale} via {Accelerated} {Wirtinger} {Flow}},
	journal = {arXiv preprint arXiv:2002.11785},
	author = {Fabian, Zalan and Haldar, Justin and Leahy, Richard and Soltanolkotabi, Mahdi},
	year = {2020},
}

@misc{noauthor_fastmri_nodate-1,
	title = {{FastMRI}},
	url = {https://fastmri.org/leaderboards/},
}

@article{fedus_switch_2021,
	title = {Switch {Transformers}: {scaling} to {trillion} {parameter} {models} with {simple} and {efficient} {sparsity}},
	journal = {arXiv preprint arXiv:2101.03961},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2021},
}

@article{feng_golden-angle_2014-1,
	title = {Golden-{Angle} {Radial} {Sparse} {Parallel} {MRI}: {Combination} of {Compressed} {Sensing}, {Parallel} {Imaging}, and {Golden}-{Angle} {Radial} {Sampling} for {Fast} and {Flexible} {Dynamic} {Volumetric} {MRI}: {iGRASP}: {Iterative} {Golden}-{Angle} {RAdial} {Sparse} {Parallel} {MRI}},
	volume = {72},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Feng, Li and Grimm, Robert and Block, Kai Tobias and Chandarana, Hersh and Kim, Sungheon and Xu, Jian and Axel, Leon and Sodickson, Daniel K. and Otazo, Ricardo},
	year = {2014},
	pages = {707--717},
}

@article{foret_sharpness-aware_2021-1,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	journal = {arXiv:2010.01412 [cs, stat]},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	year = {2021},
}

@inproceedings{gatys_image_2016-1,
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	year = {2016},
	pages = {2414--2423},
}

@article{george_fast_2018-1,
	title = {Fast {Approximate} {Natural} {Gradient} {Descent} in a {Kronecker}-{Factored} {Eigenbasis}},
	journal = {arXiv:1806.03884 [cs, stat]},
	author = {George, Thomas and Laurent, César and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
	year = {2018},
}

@article{ghorbani_investigation_2019,
	title = {An {Investigation} into {Neural} {Net} {Optimization} via {Hessian} {Eigenvalue} {Density}},
	journal = {arXiv:1901.10159 [cs, stat]},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	year = {2019},
}

@article{gilton_neumann_2019-1,
	title = {Neumann {Networks} for {Linear} {Inverse} {Problems} in {Imaging}},
	volume = {6},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gilton, Davis and Ongie, Greg and Willett, Rebecca},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {328--343},
}

@article{glorot_understanding_nodate-1,
	title = {Understanding the {Difficulty} of {Training} {Deep} {Feedforward} {Neural} {Networks}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
}

@article{goldfarb_practical_2020-1,
	title = {Practical {Quasi}-{Newton} {Methods} for {Training} {Deep} {Neural} {Networks}},
	journal = {arXiv:2006.08877 [cs, math, stat]},
	author = {Goldfarb, Donald and Ren, Yi and Bahamou, Achraf},
	year = {2020},
}

@article{gomez_reversible_nodate-1,
	title = {The {Reversible} {Residual} {Network}: {Backpropagation} {Without} {Storing} {Activations}},
	author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
	pages = {11},
}

@article{goyal_accurate_2018-1,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2018},
}

@inproceedings{gregor_learning_2010-1,
	title = {Learning {Fast} {Approximations} of {Sparse} {Coding}},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	author = {Gregor, Karol and LeCun, Yann},
	year = {2010},
	pages = {399--406},
}

@article{grill_bootstrap_2020,
	title = {Bootstrap {Your} {Own} {Latent}: {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	journal = {arXiv:2006.07733 [cs, stat]},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	year = {2020},
}

@article{grosse_kronecker-factored_nodate-1,
	title = {A {Kronecker}-{Factored} {Approximate} {Fisher} {Matrix} for {Convolution} {Layers}},
	author = {Grosse, Roger and Martens, James},
	pages = {10},
}

@article{gubner_probability_nodate-1,
	title = {{PROBABILITY} {AND} {RANDOM} {PROCESSES} {FOR} {ELECTRICAL} {AND} {COMPUTER} {ENGINEERS}},
	author = {Gubner, John A},
	pages = {642},
}

@article{guizar-sicairos_phase_2011-1,
	title = {Phase {Tomography} from {X}-{Ray} {Coherent} {Diffractive} {Imaging} {Projections}},
	volume = {19},
	number = {22},
	journal = {Optics Express},
	author = {Guizar-Sicairos, Manuel and Diaz, Ana and Holler, Mirko and Lucas, Miriam S. and Menzel, Andreas and Wepf, Roger A. and Bunk, Oliver},
	year = {2011},
	pages = {21345},
}

@article{guo_direct_2017-1,
	title = {Direct {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps} from {Highly} {Undersampled} {Brain} {Dynamic} {Contrast} {Enhanced} {MRI}: {Direct} {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps}},
	volume = {78},
	number = {4},
	journal = {Magnetic Resonance in Medicine},
	author = {Guo, Yi and Lingala, Sajan Goud and Zhu, Yinghua and Lebel, R. Marc and Nayak, Krishna S.},
	year = {2017},
	pages = {1566--1578},
}

@inproceedings{gupta_shampoo_2018-1,
	title = {Shampoo: {Preconditioned} {Stochastic} {Tensor} {Optimization}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gupta, Vineet and Koren, Tomer and Singer, Yoram},
	year = {2018},
	pages = {1842--1850},
}

@inproceedings{gutmann_noise-contrastive_2010,
	title = {Noise-{Contrastive} {Estimation}: {A} {New} {Estimation} {Principle} for {Unnormalized} {Statistical} {Models}},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	year = {2010},
	pages = {297--304},
}

@book{hajek_random_2015-1,
	edition = {First},
	title = {Random {Processes} for {Engineers}},
	isbn = {978-1-107-10012-1 978-1-316-16460-0},
	publisher = {Cambridge University Press},
	author = {Hajek, Bruce},
	year = {2015},
}

@article{halko_finding_2010-1,
	title = {Finding {Structure} with {Randomness}: {Probabilistic} {Algorithms} for {Constructing} {Approximate} {Matrix} {Decompositions}},
	journal = {arXiv:0909.4061 [math]},
	author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
	year = {2010},
}

@article{hand_phase_2018-1,
	title = {Phase {Retrieval} {Under} a {Generative} {Prior}},
	journal = {arXiv:1807.04261 [cs, math]},
	author = {Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
	year = {2018},
}

@article{heckel_deep_nodate-1,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-{Convolutional} {Networks}},
	author = {Heckel, Reinhard and Hand, Paul},
	pages = {17},
}

@article{heckel_regularizing_2019-2,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{heckel_regularizing_2019-3,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{he_deep_2015-1,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
}

@article{he_distributed_2017-1,
	title = {Distributed {Hessian}-{Free} {Optimization} for {Deep} {Neural} {Network}},
	journal = {arXiv:1606.00511 [cs, math]},
	author = {He, Xi and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Takáč, Martin},
	year = {2017},
}

@article{ho_cascaded_nodate-1,
	title = {Cascaded {Diffusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	pages = {28},
}

@article{saharia2022photorealistic,
	title={Photorealistic text-to-image diffusion models with deep language understanding},
	author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
	journal={arXiv preprint arXiv:2205.11487},
	year={2022}
}

@article{ho_denoising_2020-1,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2006.11239},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
}

@article{holler_high-resolution_2017-1,
	title = {High-{Resolution} {Non}-{Destructive} {Three}-{Dimensional} {Imaging} of {Integrated} {Circuits}},
	volume = {543},
	number = {7645},
	journal = {Nature},
	author = {Holler, Mirko and Guizar-Sicairos, Manuel and Tsai, Esther H. R. and Dinapoli, Roberto and Müller, Elisabeth and Bunk, Oliver and Raabe, Jörg and Aeppli, Gabriel},
	year = {2017},
	pages = {402--406},
}

@article{holler_omnytomography_2018-1,
	title = {{OMNY}—{A} {tOMography} {Nano} {crYo} {Stage}},
	journal = {Rev. Sci. Instrum.},
	author = {Holler, M and Raabe, J and Diaz, A and Guizar-Sicairos, M and Wepf, R and Odstrcil, M and Shaik, F R and Panneels, V and Menzel, A and Sarafimov, B and Maag, S and Wang, X and Thominet, V and Walther, H and Lachat, T and Vitins, M and Bunk, O},
	year = {2018},
	pages = {14},
}

@book{horn_matrix_2012-1,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Matrix {Analysis}},
	isbn = {978-0-521-83940-2},
	publisher = {Cambridge University Press},
	author = {Horn, Roger A. and Johnson, Charles R.},
	year = {2012},
	lccn = {QA188 .H66 2012},
}

@inproceedings{huang_densely_2017-1,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {2261--2269},
}

@article{huang_provably_2018-1,
	title = {A {Provably} {Convergent} {Scheme} for {Compressive} {Sensing} under {Random} {Generative} {Priors}},
	journal = {arXiv:1812.04176 [math]},
	author = {Huang, Wen and Hand, Paul and Heckel, Reinhard and Voroninski, Vladislav},
	year = {2018},
}

@article{hussain_differential_nodate-1,
	title = {Differential {Data} {Augmentation} {Techniques} for {Medical} {Imaging} {Classification} {Tasks}},
	author = {Hussain, Zeshan and Gimenez, Francisco and Yi, Darvin and Rubin, Daniel},
	pages = {6},
}

@article{ioffe_batch_2015-1,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
}

@article{jacobsen_i-revnet_2018-1,
	title = {I-{RevNet}: {Deep} {Invertible} {Networks}},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	year = {2018},
}

@article{jaegle_perceiver_2021,
	title = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
	journal = {arXiv:2103.03206 [cs, eess]},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	year = {2021},
}

@article{jaiswal_survey_2021,
	title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
	journal = {arXiv:2011.00362 [cs]},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	year = {2021},
}

@article{jalal_robust_2021-1,
	title = {Robust {Compressed} {Sensing} {MRI} with {Deep} {Generative} {Priors}},
	journal = {arXiv:2108.01368 [cs, math, stat]},
	author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G. and Tamir, Jonathan I.},
	year = {2021},
}

@article{jiang_linear_nodate-1,
	title = {A {Linear} {Speedup} {Analysis} of {Distributed} {Deep} {Learning} with {Sparse} and {Quantized} {Communication}},
	author = {Jiang, Peng and Agrawal, Gagan},
	pages = {12},
}

@article{jiang_transgan_2021,
	title = {{TransGAN}: {Two} {Transformers} {Can} {Make} {One} {Strong} {GAN}},
	journal = {arXiv preprint arXiv:2102.07074},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
}

@article{jin_time-dependent_2019-1,
	title = {Time-{Dependent} {Deep} {Image} {Prior} for {Dynamic} {MRI}},
	journal = {arXiv:1910.01684 [cs, eess]},
	author = {Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jerome and Stuber, Matthias and Unser, Michael},
	year = {2019},
}

@inproceedings{jnawali_deep_2018-1,
	title = {Deep {3D} {Convolution} {Neural} {Network} for {CT} {Brain} {Hemorrhage} {Classification}},
	volume = {10575},
	booktitle = {Medical {Imaging} 2018: {Computer}-{Aided} {Diagnosis}},
	publisher = {International Society for Optics and Photonics},
	author = {Jnawali, Kamal and Arbabshirani, Mohammad R. and Rao, Navalgund and Patel, Alpen A.},
	year = {2018},
	pages = {105751C},
}

@inproceedings{johnson_perceptual_2016-1,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	year = {2016},
	pages = {694--711},
}

@article{kahnt_coupled_nodate-1,
	title = {Coupled {Ptychography} and {Tomography} {Algorithm} {Improves} {Reconstruction} of {Experimental} {Data}},
	author = {Kahnt, Maik and Becher, Johannes and Brückner, Dennis and Fam, Yakub and Sheppard, Thomas and Weissenberger, Tobias and Wittwer, Felix and Grunwaldt, Jan-Dierk and Schwieger, Wilhelm and Schroer, Christian G},
	pages = {8},
}

@article{karras_analyzing_2020-1,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@article{karras_progressive_2018-1,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
}

@article{karras_style-based_nodate-1,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	pages = {10},
}

@article{karras_training_2020-1,
	title = {Training {Generative} {Adversarial} {Networks} with {Limited} {Data}},
	journal = {arXiv:2006.06676 [cs, stat]},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@inproceedings{keuper_distributed_2016-1,
	address = {Salt Lake City, UT, USA},
	title = {Distributed {Training} of {Deep} {Neural} {Networks}: {Theoretical} and {Practical} {Limits} of {Parallel} {Scalability}},
	isbn = {978-1-5090-3882-4},
	booktitle = {2016 2nd {Workshop} on {Machine} {Learning} in {HPC} {Environments} ({MLHPC})},
	publisher = {IEEE},
	author = {Keuper, Janis and Preundt, Franz-Josef},
	year = {2016},
	pages = {19--26},
}

@article{kingma_glow_nodate-2,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{kingma_glow_nodate-3,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{knoll_assessment_2019-1,
	title = {Assessment of the {Generalization} of {Learned} {Image} {Reconstruction} and the {Potential} for {Transfer} {Learning}},
	volume = {81},
	number = {1},
	journal = {Magnetic Resonance in Medicine},
	author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
	year = {2019},
	pages = {116--128},
}

@article{kobler_total_2020-1,
	title = {Total {Deep} {Variation} for {Linear} {Inverse} {Problems}},
	journal = {arXiv:2001.05005 [cs, math]},
	author = {Kobler, Erich and Effland, Alexander and Kunisch, Karl and Pock, Thomas},
	year = {2020},
}

@article{kobyzev_normalizing_2020-1,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, I. and Prince, S. and Brubaker, M.},
	year = {2020},
	pages = {1--1},
}

@inproceedings{kolesnikov_revisiting_2019,
	title = {Revisiting {Self}-{Supervised} {Visual} {Representation} {Learning}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
	year = {2019},
	pages = {1920--1929},
}

@article{korkmaz_unsupervised_2021-1,
	title = {Unsupervised {MRI} {Reconstruction} via {Zero}-{Shot} {Learned} {Adversarial} {Transformers}},
	journal = {arXiv:2105.08059 [cs, eess]},
	author = {Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and Özbey, Muzaffer and Çukur, Tolga},
	year = {2021},
}

@article{kreutz_complex_nodate-1,
	title = {The {Complex} {Gradient} {Operator} and the {CR}-{Calculus}},
	author = {Kreutz, Kenneth},
	pages = {74},
}

@article{krishnan_neumann_2017-1,
	title = {Neumann {Optimizer}: {A} {Practical} {Optimization} {Algorithm} for {Deep} {Neural} {Networks}},
	journal = {arXiv:1712.03298 [cs, stat]},
	author = {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A.},
	year = {2017},
}

@article{kuo_featmatch_2020-1,
	title = {{FeatMatch}: {Feature}-{Based} {Augmentation} for {Semi}-{Supervised} {Learning}},
	journal = {arXiv preprint arXiv:2007.08505},
	author = {Kuo, Chia-Wen and Ma, Chih-Yao and Huang, Jia-Bin and Kira, Zsolt},
	year = {2020},
}

@article{kwon_asam_2021-1,
	title = {{ASAM}: {Adaptive} {Sharpness}-{Aware} {Minimization} for {Scale}-{Invariant} {Learning} of {Deep} {Neural} {Networks}},
	journal = {arXiv:2102.11600 [cs, stat]},
	author = {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
	year = {2021},
}

@article{lauro_multiple_2020-1,
	title = {Multiple {Subglacial} {Water} {Bodies} below the {South} {Pole} of {Mars} {Unveiled} by {New} {MARSIS} {Data}},
	journal = {Nature Astronomy},
	author = {Lauro, Sebastian Emanuel and Pettinelli, Elena and Caprarelli, Graziella and Guallini, Luca and Rossi, Angelo Pio and Mattei, Elisabetta and Cosciotti, Barbara and Cicchetti, Andrea and Soldovieri, Francesco and Cartacci, Marco and Di Paolo, Federico and Noschese, Raffaella and Orosei, Roberto},
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
}

@article{lee_deep_2018-1,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	journal = {arXiv:1711.00165 [cs, stat]},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	year = {2018},
}

@article{lemley_smart_2017-1,
	title = {Smart {Augmentation} {Learning} an {Optimal} {Data} {Augmentation} {Strategy}},
	volume = {5},
	journal = {IEEE Access},
	author = {Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
	year = {2017},
	pages = {5858--5869},
}

@article{liang_swinir_2021-1,
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	journal = {arXiv:2108.10257 [cs, eess]},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
}

@article{li_communication_nodate-1,
	title = {Communication {Efficient} {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	author = {Li, Mu and Andersen, David G and Smola, Alexander and Yu, Kai},
	pages = {9},
}

@article{li_fourier_2020-1,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	journal = {arXiv:2010.08895 [cs, math]},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
}

@article{li_gan_2020-1,
	title = {{GAN} {Compression}: {Efficient} {Architectures} for {Interactive} {Conditional} {GANs}},
	journal = {arXiv:2003.08936 [cs, eess]},
	author = {Li, Muyang and Lin, Ji and Ding, Yaoyao and Liu, Zhijian and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{li_sacnn_2020-1,
	title = {{SACNN}: {Self}-{Attention} {Convolutional} {Neural} {Network} for {Low}-{Dose} {CT} {Denoising} with {Self}-{Supervised} {Perceptual} {Loss} {Network}},
	volume = {39},
	number = {7},
	journal = {IEEE transactions on medical imaging},
	author = {Li, Meng and Hsu, William and Xie, Xiaodong and Cong, Jason and Gao, Wen},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {2289--2301},
}

@article{liu_coupled_nodate-2,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@article{liu_coupled_nodate-3,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@incollection{liu_image_2018-1,
	address = {Cham},
	title = {Image {Inpainting} for {Irregular} {Holes} {Using} {Partial} {Convolutions}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
	year = {2018},
	pages = {89--105},
}

@article{liu_spark-based_2016-1,
	title = {Spark-{Based} {Large}-{Scale} {Matrix} {Inversion} for {Big} {Data} {Processing}},
	volume = {4},
	journal = {IEEE Access},
	author = {Liu, Jun and Liang, Yang and Ansari, Nirwan},
	year = {2016},
	pages = {2166--2176},
}

@article{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} {using} {shifted} {windows}},
	journal = {arXiv:2103.14030},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
}

@article{liu_unsupervised_nodate-2,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{liu_unsupervised_nodate-3,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{long_deep_nodate-1,
	title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
	pages = {10},
}

@article{long_fully_nodate-1,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
}

@article{lu_pretrained_2021,
	title = {Pretrained {Transformers} as {Universal} {Computation} {Engines}},
	journal = {arXiv:2103.05247 [cs]},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	year = {2021},
}

@article{lutter_deep_2019-1,
	title = {Deep {Lagrangian} {Networks}: {Using} {Physics} as {Model} {Prior} for {Deep} {Learning}},
	journal = {arXiv:1907.04490 [cs, eess, stat]},
	author = {Lutter, Michael and Ritter, Christian and Peters, Jan},
	year = {2019},
}

@article{maiden_improved_2009-1,
	title = {An {Improved} {Ptychographical} {Phase} {Retrieval} {Algorithm} for {Diffractive} {Imaging}},
	volume = {109},
	number = {10},
	journal = {Ultramicroscopy},
	author = {Maiden, Andrew M. and Rodenburg, John M.},
	year = {2009},
	pages = {1256--1262},
}

@article{maiden_ptychographic_nodate-1,
	title = {Ptychographic {Transmission} {Microscopy} in {Three} {Dimensions} {Using} a {Multi}-{Slice} {Approach}},
	author = {Maiden, A M and Humphry, M J and Rodenburg, J M},
	pages = {9},
}

@article{ma_inefficiency_2019-1,
	title = {Inefficiency of {K}-{FAC} for {Large} {Batch} {Size} {Training}},
	journal = {arXiv:1903.06237 [cs, stat]},
	author = {Ma, Linjian and Montague, Gabe and Ye, Jiayu and Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2019},
}

@article{mallat_theory_nodate-1,
	title = {A {Theory} for {Multiresolution} {Signal} {Decomposition}: {The} {Wavelet} {Representation}},
	author = {Mallat, Stephane G},
	pages = {31},
}

@article{mardani_deep_2017-2,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{mardani_deep_2017-3,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{martens_deep_nodate-1,
	title = {Deep {Learning} via {Hessian}-{Free} {Optimization}},
	author = {Martens, James},
	pages = {8},
}

@article{martens_new_2014-1,
	title = {New {Insights} and {Perspectives} on the {Natural} {Gradient} {Method}},
	journal = {arXiv preprint arXiv:1412.1193},
	author = {Martens, James},
	year = {2014},
}

@article{martens_optimizing_nodate-1,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-{Factored} {Approximate} {Curvature}},
	author = {Martens, James and Grosse, Roger},
	pages = {10},
}

@article{martinsson_randomized_2011-3,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {47--68},
}

@article{martinsson_randomized_2011-4,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{martinsson_randomized_2011-5,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{menon_pulse_2020-2,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{menon_pulse_2020-3,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{metzler_prdeep_nodate-1,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with {Flexible} {Deep} {Neural} {Networks}},
	author = {Metzler, Christopher A and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G},
	pages = {10},
}

@article{metzler_prdeep_2018-1,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with a {Flexible} {Deep} {Network}},
	journal = {arXiv:1803.00212 [cs, stat]},
	author = {Metzler, Christopher A. and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G.},
	year = {2018},
}

@book{meyer_matrix_2000-1,
	address = {Philadelphia},
	title = {Matrix {Analysis} and {Applied} {Linear} {Algebra}},
	isbn = {978-0-89871-454-8},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Meyer, C. D.},
	year = {2000},
	lccn = {QA188 .M495 2000},
}

@misc{noauthor_models_nodate-1,
	title = {Models {Genesis} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1361841520302048?token=B6B4930C1609C04FD49C6498E7935D83275465F07F9155D2DA9C09FBACAA0F60708219F8E62F34E2DB5319E5BC0CA8E0&originRegion=us-east-1&originCreation=20210914005025},
}

@article{moulin_analysis_1999-1,
	title = {Analysis of {Multiresolution} {Image} {Denoising} {Schemes} {Using} {Generalized} {Gaussian} and {Complexity} {Priors}},
	volume = {45},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Moulin, P. and {Juan Liu}},
	year = {1999},
	pages = {909--919},
}

@article{myagotin_efficient_2013-1,
	title = {Efficient {Volume} {Reconstruction} for {Parallel}-{Beam} {Computed} {Laminography} by {Filtered} {Backprojection} on {Multi}-{Core} {Clusters}},
	volume = {22},
	number = {12},
	journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
	author = {Myagotin, Anton and Voropaev, Alexey and Helfen, Lukas and Hänschke, Daniel and Baumbach, Tilo},
	year = {2013},
	pages = {14},
}

@article{nagle-mcnaughton_planet_2020-1,
	title = {{PlaNet}: {A} {Neural} {Network} for {Detecting} {Transverse} {Aeolian} {Ridges} on {Mars}},
	volume = {12},
	number = {21},
	journal = {Remote Sensing},
	author = {Nagle-McNaughton, Timothy and McClanahan, Timothy and Scuderi, Louis},
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {3607},
}

@inproceedings{narayanan_pipedream_2019-1,
	address = {Huntsville Ontario Canada},
	title = {{PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}},
	isbn = {978-1-4503-6873-5},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	year = {2019},
	pages = {1--15},
}

@article{neyshabur_towards_2018-2,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{neyshabur_towards_2018-3,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{nichol_improved_2021-1,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2102.09672},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	year = {2021},
}

@misc{noauthor_nie_nodate-1,
	title = {Nie: {Medical} {Image} {Synthesis} with {Deep} {Convolutional}... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?title=Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks&author=D.%20Nie&publication_year=2018},
}

@article{nie_medical_2018-1,
	title = {Medical {Image} {Synthesis} with {Deep} {Convolutional} {Adversarial} {Networks}},
	volume = {65},
	number = {12},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Nie, D. and Trullo, R. and Lian, J. and Wang, L. and Petitjean, C. and Ruan, S. and Wang, Q. and Shen, D.},
	year = {2018},
	pages = {2720--2730},
}

@book{nocedal_numerical_1999-1,
	address = {New York},
	series = {Springer {Series} in {Operations} {Research}},
	title = {Numerical {Optimization}},
	isbn = {978-0-387-98793-4},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {1999},
	lccn = {QA402.5 .N62 1999},
}

@article{oktay_attention_2018-1,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	journal = {arXiv preprint arXiv:1804.03999},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard},
	year = {2018},
}

@article{ongie_deep_2020-1,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Baraniuk, Christopher A. Metzler Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	year = {2020},
	note = {Publisher: IEEE},
}

@article{ongie_function_2019-1,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	journal = {arXiv:1910.01635 [cs, stat]},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@article{van_den_oord_representation_2018,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	journal = {arXiv preprint arXiv:1807.03748},
	author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
	year = {2018},
}

@inproceedings{osawa_large-scale_2019-1,
	title = {Large-{Scale} {Distributed} {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximate} {Curvature} for {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
	year = {2019},
	pages = {12359--12367},
}

@article{osullivan_fast_1985-1,
	title = {A {Fast} {Sinc} {Function} {Gridding} {Algorithm} for {Fourier} {Inversion} in {Computer} {Tomography}},
	volume = {4},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {O'Sullivan, J. D.},
	year = {1985},
	pages = {200--207},
}

@article{oymak_generalization_2019-1,
	title = {Generalization {Guarantees} for {Neural} {Networks} via {Harnessing} the {Low}-{Rank} {Structure} of the {Jacobian}},
	journal = {arXiv:1906.05392 [cs, math, stat]},
	author = {Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
	year = {2019},
}

@article{oymak_towards_nodate-1,
	title = {Towards {Moderate} {Overparameterization}: {Global} {Convergence} {Guarantees} for {Training} {Shallow} {Neural} {Networks}},
	author = {Oymak, Samet and Soltanolkotabi, Mahdi},
	pages = {41},
}

@article{papyan_full_2019,
	title = {The {Full} {Spectrum} of {Deepnet} {Hessians} at {Scale}: {Dynamics} with {SGD} {Training} and {Sample} {Size}},
	journal = {arXiv:1811.07062 [cs, stat]},
	author = {Papyan, Vardan},
	year = {2019},
}

@article{pardoe_boosting_nodate-1,
	title = {Boosting for {Regression} {Transfer}},
	author = {Pardoe, David and Stone, Peter},
	pages = {8},
}

@article{pauloski_convolutional_nodate-1,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	author = {Pauloski, J Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T},
	pages = {11},
}

@article{pauloski_convolutional_2020-4,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-5,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-6,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-7,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{petersen__nodate-1,
	title = {[ http://matrixcookbook.com ]},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	pages = {72},
}

@article{pezzotti_adaptive-cs-net_2019-1,
	title = {Adaptive-{CS}-{Net}: {FastMRI} with {Adaptive} {Intelligence}},
	journal = {arXiv:1912.12259 [eess]},
	author = {Pezzotti, Nicola and de Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and van Gemert, Jeroen and Schülke, Christophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and van Osch, Matthias J. P. and Staring, Marius},
	year = {2019},
}

@inproceedings{pham_phaseless_2018-1,
	address = {Washington, DC},
	title = {Phaseless {Diffraction} {Tomography} with {Regularized} {Beam} {Propagation}},
	isbn = {978-1-5386-3636-7},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Pham, Thanh-an and Soubies, Emmanuel and Lim, Joowon and Goy, Alexandre and Soulez, Ferreol and Psaltis, Demetri and Unser, Michael},
	year = {2018},
	pages = {1268--1271},
}

@article{pilanci_newton_2015-1,
	title = {Newton {Sketch}: {A} {Linear}-{Time} {Optimization} {Algorithm} with {Linear}-{Quadratic} {Convergence}},
	journal = {arXiv:1505.02250 [cs, math, stat]},
	author = {Pilanci, Mert and Wainwright, Martin J.},
	year = {2015},
}

@article{powell_algorithms_1978-1,
	title = {Algorithms for {Nonlinear} {Constraints} {That} {Use} {Lagrangian} {Functions}},
	volume = {14},
	number = {1},
	journal = {Mathematical Programming},
	author = {Powell, M. J. D.},
	year = {1978},
	pages = {224--248},
}

@article{putzky_invert_nodate-3,
	title = {Invert to {Learn} to {Invert}},
	author = {Putzky, Patrick and Welling, Max},
	pages = {11},
}

@inproceedings{putzky_invert_2019-1,
	title = {Invert to {Learn} to {Invert}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Putzky, Patrick and Welling, Max},
	year = {2019},
	pages = {446--456},
}

@article{putzky_invert_nodate-4,
	title = {Invert to {Learn} to {Invert}},
	author = {Putzky, Patrick and Welling, Max},
	pages = {11},
}

@article{putzky_invert_nodate-5,
	title = {Invert to {Learn} to {Invert}},
	author = {Putzky, Patrick and Welling, Max},
	pages = {11},
}

@article{putzky_i-rim_2019-1,
	title = {I-{RIM} {Applied} to the {fastMRI} {Challenge}},
	journal = {arXiv:1910.08952 [cs, eess]},
	author = {Putzky, Patrick and Karkalousos, Dimitrios and Teuwen, Jonas and Miriakov, Nikita and Bakker, Bart and Caan, Matthan and Welling, Max},
	year = {2019},
}

@article{putzky_recurrent_2017-1,
	title = {Recurrent {Inference} {Machines} for {Solving} {Inverse} {Problems}},
	journal = {arXiv:1706.04008 [cs]},
	author = {Putzky, Patrick and Welling, Max},
	year = {2017},
}

@article{qin_convolutional_2019-1,
	title = {Convolutional {Recurrent} {Neural} {Networks} for {Dynamic} {MR} {Image} {Reconstruction}},
	volume = {38},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Qin, Chen and Schlemper, Jo and Caballero, Jose and Price, Anthony N. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
	pages = {280--290},
}

@article{radford_unsupervised_2016-1,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2016},
}

@inproceedings{rafati_improving_2018-1,
	title = {Improving {L}-{BFGS} {Initialization} for {Trust}-{Region} {Methods} in {Deep} {Learning}},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Rafati, Jacob and Marcia, Roummel F.},
	year = {2018},
	pages = {501--508},
}

@article{ren_benchmarking_2021-1,
	title = {Benchmarking {Deep} {Inverse} {Models} over {Time}, and the {Neural}-{Adjoint} {Method}},
	journal = {arXiv:2009.12919 [cs, eess, stat]},
	author = {Ren, Simiao and Padilla, Willie and Malof, Jordan},
	year = {2021},
}

@article{noauthor_representations_nodate-1,
	title = {Representations of {Quasi}-{Newton} {Matrices} and {Their} {Use} in {Limited} {Memory} {Methods}},
	pages = {28},
}

@article{rezende_variational_nodate-1,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	pages = {10},
}

@article{sagun_eigenvalues_2017,
	title = {Eigenvalues of the {Hessian} in {Deep} {Learning}: {Singularity} and {Beyond}},
	journal = {arXiv:1611.07476 [cs]},
	author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
	year = {2017},
}

@article{saharia_image_2021-1,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	journal = {arXiv:2104.07636 [cs, eess]},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	year = {2021},
}

@article{sajjad_multi-grade_2019-1,
	title = {Multi-{Grade} {Brain} {Tumor} {Classification} {Using} {Deep} {CNN} with {Extensive} {Data} {Augmentation}},
	volume = {30},
	journal = {Journal of Computational Science},
	author = {Sajjad, Muhammad and Khan, Salman and Muhammad, Khan and Wu, Wanqing and Ullah, Amin and Baik, Sung Wook},
	year = {2019},
	pages = {174--182},
}

@article{salimans_weight_2016-1,
	title = {Weight {Normalization}: {A} {Simple} {Reparameterization} to {Accelerate} {Training} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1602.07868 [cs]},
	author = {Salimans, Tim and Kingma, Diederik P.},
	year = {2016},
}

@article{salman_provably_2020-1,
	title = {Provably {Robust} {Deep} {Learning} via {Adversarially} {Trained} {Smoothed} {Classifiers}},
	journal = {arXiv:1906.04584 [cs, stat]},
	author = {Salman, Hadi and Yang, Greg and Li, Jerry and Zhang, Pengchuan and Zhang, Huan and Razenshteyn, Ilya and Bubeck, Sebastien},
	year = {2020},
}

@article{savarese_how_2019-1,
	title = {How {Do} {Infinite} {Width} {Bounded} {Norm} {Networks} {Look} in {Function} {Space}?},
	journal = {arXiv:1902.05040 [cs, stat]},
	author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@book{sawyer_creation_nodate-1,
	title = {Creation of {Fully} {Sampled} {MR} {Data} {Repository} for {Compressed} {Sensing} of the {Knee}},
	author = {Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas and Healthcare, Ge},
}

@article{schlemper_dautomap_2019-1,
	title = {{dAUTOMAP}: {Decomposing} {AUTOMAP} to {Achieve} {Scalability} and {Enhance} {Performance}},
	journal = {arXiv:1909.10995 [cs, eess, stat]},
	author = {Schlemper, Jo and Oksuz, Ilkay and Clough, James R. and Duan, Jinming and King, Andrew P. and Schnabel, Julia A. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
}

@article{schlemper_deep_2017-1,
	title = {A {Deep} {Cascade} of {Convolutional} {Neural} {Networks} for {MR} {Image} {Reconstruction}},
	journal = {arXiv:1703.00555 [cs]},
	author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony and Rueckert, Daniel},
	year = {2017},
}

@book{shalev-shwartz_understanding_2014-1,
	address = {Cambridge},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
}

@article{shazeer_mesh-tensorflow_2018-1,
	title = {Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}},
	journal = {arXiv:1811.02084 [cs, stat]},
	author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
	year = {2018},
}

@article{shechtman_phase_nodate-1,
	title = {Phase {Retrieval} with {Application} to {Optical} {Imaging}},
	author = {Shechtman, Yoav and Eldar, Yonina C and Cohen, Oren and Chapman, Henry N and Miao, Jianwei and Segev, Mordechai},
	pages = {23},
}

@article{sheikh_image_nodate-1,
	title = {{IMAGE} {INFORMATION} {AND} {VISUAL} {QUALITY}},
	author = {Sheikh, Hamid R and Bovik, Alan C},
	pages = {4},
}

@article{shen_powernorm_2020,
	title = {{PowerNorm}: {Rethinking} {Batch} {Normalization} in {Transformers}},
	journal = {arXiv:2003.07845 [cs]},
	author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
	year = {2020},
}

@article{shi_is_nodate-1,
	title = {Is the {Deconvolution} {Layer} the {Same} as a {Convolutional} {Layer}?},
	author = {Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Ledig, Christian and Wang, Zehan},
	pages = {7},
}

@inproceedings{shi_real-time_2016-1,
	address = {Las Vegas, NV, USA},
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	isbn = {978-1-4673-8851-1},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Wenzhe and Caballero, Jose and Huszar, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	year = {2016},
	pages = {1874--1883},
}

@article{shorten_survey_2019-1,
	title = {A {Survey} on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
	pages = {60},
}

@article{skorski_chain_2019,
	title = {Chain {Rules} for {Hessian} and {Higher} {Derivatives} {Made} {Easy} by {Tensor} {Calculus}},
	journal = {arXiv:1911.13292 [cs]},
	author = {Skorski, Maciej},
	year = {2019},
}

@inproceedings{soltanolkotabi_3d_2019-1,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	booktitle = {2019 13th {International} {Conference} on {Sampling} {Theory} and {Applications} ({SampTA})},
	publisher = {IEEE},
	author = {Soltanolkotabi, Mahdi},
	year = {2019},
	pages = {1--3},
}

@article{song_generative_2020-1,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	journal = {arXiv:1907.05600 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{song_improved_2020-1,
	title = {Improved {Techniques} for {Training} {Score}-{Based} {Generative} {Models}},
	journal = {arXiv:2006.09011 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{steihaug_conjugate_1983-1,
	title = {The {Conjugate} {Gradient} {Method} and {Trust} {Regions} in {Large} {Scale} {Optimization}},
	volume = {20},
	number = {3},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Steihaug, Trond},
	year = {1983},
	pages = {626--637},
}

@article{stockmar_x-ray_2015-1,
	title = {X-{Ray} {Nanotomography} {Using} near-{Field} {Ptychography}},
	volume = {23},
	number = {10},
	journal = {Optics Express},
	author = {Stockmar, Marco and Hubert, Maxime and Dierolf, Martin and Enders, Bjoern and Clare, Richard and Allner, Sebastian and Fehringer, Andreas and Zanette, Irene and Villanova, Julie and Laurencin, Jérôme and Cloetens, Peter and Pfeiffer, Franz and Thibault, Pierre},
	year = {2015},
	pages = {12720},
}

@book{strang_linear_2006-1,
	address = {Belmont, CA},
	edition = {4th ed},
	title = {Linear {Algebra} and {Its} {Applications}},
	isbn = {978-0-03-010567-8},
	publisher = {Thomson, Brooks/Cole},
	author = {Strang, Gilbert},
	year = {2006},
	lccn = {QA184.2 .S77 2006},
}

@article{tan_survey_2018-1,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	journal = {arXiv:1808.01974 [cs, stat]},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
}

@article{tian_understanding_2021,
	title = {Understanding {Self}-{Supervised} {Learning} {Dynamics} without {Contrastive} {Pairs}},
	journal = {arXiv:2102.06810 [cs]},
	author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
	year = {2021},
}

@article{tishby_information_2000,
	title = {The {Information} {Bottleneck} {Method}},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {2000},
}

@article{touvron_training_2020,
	title = {Training {Data}-{Efficient} {Image} {Transformers} \& {Distillation} through {Attention}},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	year = {2020},
}

@inproceedings{tzeng_adversarial_2017-1,
	address = {Honolulu, HI},
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	year = {2017},
	pages = {2962--2971},
}

@article{uecker_espirit_2014-1,
	title = {{ESPIRiT}— an {Eigenvalue} {Approach} to {Autocalibrating} {Parallel} {MRI}: {Where} {SENSE} {Meets} {GRAPPA}},
	volume = {71},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Uecker, Martin and Lai, Peng and Murphy, Mark J. and Virtue, Patrick and Elad, Michael and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2014},
	pages = {990--1001},
}

@article{ulyanov_instance_2017-1,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	year = {2017},
}

@article{unser_family_1993-1,
	title = {A {Family} of {Polynomial} {Spline} {Wavelet} {Transforms}},
	volume = {30},
	number = {2},
	journal = {Signal Processing},
	author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
	year = {1993},
	pages = {141--162},
}

@article{van_veen_compressed_2019-2,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{van_veen_compressed_2019-3,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{vaswani_attention_2017,
	title = {Attention {is} {all} {you} {need}},
	volume = {30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
}

@article{vershynin_high-dimensional_nodate-1,
	title = {High-{Dimensional} {Probability}},
	author = {Vershynin, Roman},
	pages = {301},
}

@article{wang_dense_2021-1,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	journal = {arXiv:2011.09157 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
}

@article{wang_dense_2021-2,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	journal = {arXiv:2011.09157 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
}

@article{wang_pyramid_2020-1,
	title = {Pyramid {Convolutional} {RNN} for {MRI} {Reconstruction}},
	journal = {arXiv:1912.00543 [cs, eess, stat]},
	author = {Wang, Puyang and Chen, Eric Z. and Chen, Terrence and Patel, Vishal M. and Sun, Shanhui},
	year = {2020},
}

@inproceedings{wang_residual_2017-1,
	address = {Honolulu, HI},
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {6450--6458},
}

@inproceedings{wu_group_2018-1,
	title = {Group {Normalization}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Wu, Yuxin and He, Kaiming},
	year = {2018},
	pages = {3--19},
}

@article{xiao_region_2021,
	title = {Region {Similarity} {Representation} {Learning}},
	journal = {arXiv:2103.12902 [cs]},
	author = {Xiao, Tete and Reed, Colorado J. and Wang, Xiaolong and Keutzer, Kurt and Darrell, Trevor},
	year = {2021},
}

@article{xu_accelerated_nodate-1,
	title = {Accelerated {Wirtinger} {Flow}: {A} {New} {Fast} {Algorithm} for {Phase} {Retrieval}},
	author = {Xu, Rui and Soltanolkotabi, Mahdi and Haldar, Justin P and Zusman, Joshua and Levi, Anthony F J},
	pages = {18},
}

@article{yang_dagan_2018-1,
	title = {{DAGAN}: {Deep} {De}-{Aliasing} {Generative} {Adversarial} {Networks} for {Fast} {Compressed} {Sensing} {MRI} {Reconstruction}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and Firmin, David},
	year = {2018},
	pages = {1310--1321},
}

@article{yang_deep_nodate-1,
	title = {Deep {ADMM}-{Net} for {Compressive} {Sensing} {MRI}},
	author = {Yang, Yan and Li, Huibin and Sun, Jian and Xu, Zongben},
	pages = {9},
}

@article{yang_deep_2019-2,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_deep_2019-3,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_low_2018-1,
	title = {Low {Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Wang, Ge},
	year = {2018},
	pages = {1348--1357},
}

@article{yang_low-dose_2018-2,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@article{yang_low-dose_2018-3,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@article{yang_ouroboros_nodate,
	title = {Ouroboros: {On} {Accelerating} {Training} of {Transformer}-{Based} {Language} {Models}},
	author = {Yang, Qian and Huo, Zhouyuan and Wang, Wenlin and Carin, Lawrence},
	pages = {11},
}

@article{yao_adahessian_2020,
	title = {{ADAHESSIAN}: {An} {Adaptive} {Second} {Order} {Optimizer} for {Machine} {Learning}},
	journal = {arXiv:2006.00719 [cs, math, stat]},
	author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
}

@inproceedings{yao_pyhessian_2020-1,
	address = {Atlanta, GA, USA},
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	isbn = {978-1-72816-251-5},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
}

@inproceedings{yao_pyhessian_2020-2,
	address = {Atlanta, GA, USA},
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	isbn = {978-1-72816-251-5},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
}

@article{yi_generative_2019-1,
	title = {Generative {Adversarial} {Network} in {Medical} {Imaging}: {A} {Review}},
	volume = {58},
	journal = {Medical Image Analysis},
	author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
	year = {2019},
	pages = {101552},
}

@article{yuan_volo_2021,
	title = {{VOLO}: {Vision} {Outlooker} for {Visual} {Recognition}},
	journal = {arXiv:2106.13112 [cs]},
	author = {Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
	year = {2021},
}

@article{yu_generative_nodate-1,
	title = {Generative {Image} {Inpainting} with {Contextual} {Attention}},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
	pages = {15},
}

@article{zavriev_heavy-ball_1993-1,
	title = {Heavy-{Ball} {Method} in {Nonconvex} {Optimization} {Problems}},
	volume = {4},
	number = {4},
	journal = {Computational Mathematics and Modeling},
	author = {Zavriev, S. K. and Kostyuk, F. V.},
	year = {1993},
	pages = {336--341},
}

@article{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	year = {2021},
}

@article{zbontar_fastmri_2019-1,
	title = {{fastMRI}: {An} {Open} {Dataset} and {Benchmarks} for {Accelerated} {MRI}},
	journal = {arXiv:1811.08839 [physics, stat]},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	year = {2019},
}

@article{zhang_coil_2013-1,
	title = {Coil {Compression} for {Accelerated} {Imaging} with {Cartesian} {Sampling}},
	volume = {69},
	number = {2},
	journal = {Magnetic Resonance in Medicine},
	author = {Zhang, Tao and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2013},
	pages = {571--582},
}

@inproceedings{zhang_deep_2018-1,
	title = {Deep {Imitation} {Learning} for {Complex} {Manipulation} {Tasks} from {Virtual} {Reality} {Teleoperation}},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
	year = {2018},
	pages = {1--8},
}

@article{zhang_beyond_2017-1,
	title = {Beyond a {Gaussian} {Denoiser}: {Residual} {Learning} of {Deep} {CNN} for {Image} {Denoising}},
	volume = {26},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
	year = {2017},
	pages = {3142--3155},
}

@inproceedings{zhang_ista-net_2018-1,
	address = {Salt Lake City, UT},
	title = {{ISTA}-{Net}: {Interpretable} {Optimization}-{Inspired} {Deep} {Network} for {Image} {Compressive} {Sensing}},
	isbn = {978-1-5386-6420-9},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Jian and Ghanem, Bernard},
	year = {2018},
	pages = {1828--1837},
}

@article{zhang_understanding_2017-1,
	title = {Understanding {Deep} {Learning} {Requires} {Rethinking} {Generalization}},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
}

@article{zhao_differentiable_2020-1,
	title = {Differentiable {Augmentation} for {Data}-{Efficient} {GAN} {Training}},
	journal = {arXiv:2006.10738 [cs]},
	author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{zhao_image_2020-1,
	title = {Image {Augmentations} for {GAN} {Training}},
	journal = {arXiv:2006.02595 [cs, eess, stat]},
	author = {Zhao, Zhengli and Zhang, Zizhao and Chen, Ting and Singh, Sameer and Zhang, Han},
	year = {2020},
}

@inproceedings{zhu_unpaired_2017-2,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@inproceedings{zhu_unpaired_2017-3,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@article{gan_ss-jircs_nodate,
	title = {{SS}-{JIRCS}: {Self}-{Supervised} {Joint} {Image} {Reconstruction} and {Coil} {Sensitivity} {Calibration} in {Parallel} {MRI} {Without} {Ground} {Truth}},
	abstract = {Parallel magnetic resonance imaging (MRI) is a widelyused technique that accelerates data collection by making use of the spatial encoding provided by multiple receiver coils. A key issue in parallel MRI is the estimation of coil sensitivity maps (CSMs) that are used for reconstructing a single high-quality image. This paper addresses this issue by developing SS-JIRCS, a new self-supervised model-based deep-learning (DL) method for image reconstruction that is equipped with automated CSM calibration. Our deep network consists of three types of modules: data-consistency, regularization, and CSM calibration. Unlike traditional supervised DL methods, these modules are directly trained on undersampled and noisy k-space data rather than on fully sampled high-quality ground truth. We present empirical results on simulated data that show the potential of the proposed method for achieving better performance than several baseline methods.},
	language = {en},
	author = {Gan, Weijie and Hu, Yuyang and Eldeniz, Cihat and Liu, Jiaming and Chen, Yasheng and An, Hongyu and Kamilov, Ulugbek S},
	pages = {9},
	file = {Gan et al. - SS-JIRCS Self-Supervised Joint Image Reconstructi.pdf:/Users/zalan/Zotero/storage/A4DQJ6JH/Gan et al. - SS-JIRCS Self-Supervised Joint Image Reconstructi.pdf:application/pdf},
}

@article{desai_noise2recon_2021,
	title = {{Noise2Recon}: {A} {Semi}-{Supervised} {Framework} for {Joint} {MRI} {Reconstruction} and {Denoising}},
	shorttitle = {{Noise2Recon}},
	url = {http://arxiv.org/abs/2110.00075},
	abstract = {Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, standard supervised DL methods depend on extensive amounts of fully-sampled ground-truth data and are sensitive to out-ofdistribution (OOD) shifts, in particular for low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose a semisupervised, consistency-based framework (termed Noise2Recon) for joint MR reconstruction and denoising. Our method enables the usage of a limited number of fully-sampled and a large number of undersampled-only scans. We compare our method to augmentation-based supervised techniques and ﬁne-tuned denoisers. Results demonstrate that even with minimal ground-truth data, Noise2Recon (1) achieves high performance on in-distribution (low-noise) scans and (2) improves generalizability to OOD, noisy scans.},
	language = {en},
	urldate = {2021-10-19},
	journal = {arXiv:2110.00075 [cs, eess]},
	author = {Desai, Arjun D. and Ozturkler, Batu M. and Sandino, Christopher M. and Vasanawala, Shreyas and Hargreaves, Brian A. and Re, Christopher M. and Pauly, John M. and Chaudhari, Akshay S.},
	month = sep,
	year = {2021},
	note = {arXiv: 2110.00075},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Desai et al. - 2021 - Noise2Recon A Semi-Supervised Framework for Joint.pdf:/Users/zalan/Zotero/storage/WUWIKZEL/Desai et al. - 2021 - Noise2Recon A Semi-Supervised Framework for Joint.pdf:application/pdf},
}

@article{zbontar_fastmri_2019-2,
	title = {{fastMRI}: {An} {Open} {Dataset} and {Benchmarks} for {Accelerated} {MRI}},
	shorttitle = {{fastMRI}},
	url = {http://arxiv.org/abs/1811.08839},
	abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
	urldate = {2021-10-26},
	journal = {arXiv:1811.08839 [physics, stat]},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	month = dec,
	year = {2019},
	note = {arXiv: 1811.08839},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Physics - Medical Physics, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/WGRRX9D9/Zbontar et al. - 2019 - fastMRI An Open Dataset and Benchmarks for Accele.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/8R4GPTFM/1811.html:text/html},
}

@article{moritz_linearly-convergent_nodate,
	title = {A {Linearly}-{Convergent} {Stochastic} {L}-{BFGS} {Algorithm}},
	abstract = {We propose a new stochastic L-BFGS algorithm and prove a linear convergence rate for strongly convex and smooth functions. Our algorithm draws heavily from a recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as a recent approach to variance reduction for stochastic gradient descent from Johnson and Zhang (2013). We demonstrate experimentally that our algorithm performs well on large-scale convex and non-convex optimization problems, exhibiting linear convergence and rapidly solving the optimization problems to high levels of precision. Furthermore, we show that our algorithm performs well for a wide-range of step sizes, often diﬀering by several orders of magnitude.},
	language = {en},
	author = {Moritz, Philipp and Nishihara, Robert and Jordan, Michael I},
	pages = {10},
	file = {Moritz et al. - A Linearly-Convergent Stochastic L-BFGS Algorithm.pdf:/Users/zalan/Zotero/storage/2R77EYZS/Moritz et al. - A Linearly-Convergent Stochastic L-BFGS Algorithm.pdf:application/pdf},
}

@inproceedings{gower_stochastic_2016,
	title = {Stochastic {Block} {BFGS}: {Squeezing} {More} {Curvature} out of {Data}},
	shorttitle = {Stochastic {Block} {BFGS}},
	url = {https://proceedings.mlr.press/v48/gower16.html},
	abstract = {We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gower, Robert and Goldfarb, Donald and Richtarik, Peter},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1869--1878},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/4NDW3WMJ/Gower et al. - 2016 - Stochastic Block BFGS Squeezing More Curvature ou.pdf:application/pdf},
}

@inproceedings{chen_simple_2020-3,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2021-11-05},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/NR6PBD2X/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:/Users/zalan/Zotero/storage/7REDCDMN/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@article{jaiswal_survey_2021-1,
	title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2227-7080/9/1/2},
	doi = {10.3390/technologies9010002},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Technologies},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {contrastive learning, discriminative learning, image/video classification, object detection, self-supervised learning, transfer learning, unsupervised learning},
	pages = {2},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/VDFDYQ2Q/Jaiswal et al. - 2021 - A Survey on Contrastive Self-Supervised Learning.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/5PHG2ACH/2.html:text/html},
}

@article{zbontar_barlow_2021-1,
	title = {Barlow twins: {Self}-supervised learning via redundancy reduction},
	shorttitle = {Barlow twins},
	journal = {arXiv preprint arXiv:2103.03230},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/DTDHYIDT/Zbontar et al. - 2021 - Barlow twins Self-supervised learning via redunda.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/RE9ID5UJ/2103.html:text/html},
}

@article{grill_bootstrap_2020-1,
	title = {Bootstrap your own latent: {A} new approach to self-supervised learning},
	shorttitle = {Bootstrap your own latent},
	journal = {arXiv preprint arXiv:2006.07733},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/WFEEL46G/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/G7CUPWUE/2006.html:text/html},
}

@inproceedings{caron_deep_2018-1,
	title = {Deep clustering for unsupervised learning of visual features},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year = {2018},
	pages = {132--149},
	file = {Full Text:/Users/zalan/Zotero/storage/6LGIJQAJ/Caron et al. - 2018 - Deep clustering for unsupervised learning of visua.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/PB6NSRCK/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.html:text/html},
}

@inproceedings{tishby_deep_2015-1,
	title = {Deep learning and the information bottleneck principle},
	doi = {10.1109/ITW.2015.7133169},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	booktitle = {2015 {IEEE} {Information} {Theory} {Workshop} ({ITW})},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = apr,
	year = {2015},
	keywords = {Bifurcation, Complexity theory, Computer architecture, Distortion, Feature extraction, Mutual information, Training},
	pages = {1--5},
	file = {IEEE Xplore Full Text PDF:/Users/zalan/Zotero/storage/JLGBW5X4/Tishby and Zaslavsky - 2015 - Deep learning and the information bottleneck princ.pdf:application/pdf},
}

@inproceedings{wang_dense_2021-3,
	title = {Dense contrastive learning for self-supervised visual pre-training},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
	pages = {3024--3033},
	file = {Full Text:/Users/zalan/Zotero/storage/4M6RFK5N/Wang et al. - 2021 - Dense contrastive learning for self-supervised vis.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/6WCVJXMA/Wang_Dense_Contrastive_Learning_for_Self-Supervised_Visual_Pre-Training_CVPR_2021_paper.html:text/html},
}

@article{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	language = {en},
	urldate = {2021-11-05},
	journal = {arXiv:2104.14294 [cs]},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv: 2104.14294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:/Users/zalan/Zotero/storage/M98T6PGY/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf},
}

@inproceedings{chen_exploring_2021,
	title = {Exploring simple siamese representation learning},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chen, Xinlei and He, Kaiming},
	year = {2021},
	pages = {15750--15758},
	file = {Full Text:/Users/zalan/Zotero/storage/KPFEXVNU/Chen and He - 2021 - Exploring simple siamese representation learning.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/ZC65M73Z/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html:text/html},
}

@article{bachman_learning_2019-1,
	title = {Learning representations by maximizing mutual information across views},
	journal = {arXiv preprint arXiv:1906.00910},
	author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/3JUSBFXZ/Bachman et al. - 2019 - Learning representations by maximizing mutual info.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/AXVYUNLK/1906.html:text/html},
}

@inproceedings{gutmann_noise-contrastive_2010-1,
	title = {Noise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
	shorttitle = {Noise-contrastive estimation},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	year = {2010},
	pages = {297--304},
	file = {Full Text:/Users/zalan/Zotero/storage/MKRRRAMF/Gutmann and Hyvärinen - 2010 - Noise-contrastive estimation A new estimation pri.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/Z86MXL6G/gutmann10a.html:text/html},
}

@article{xiao_region_2021-1,
	title = {Region {Similarity} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2103.12902},
	abstract = {We present Region Similarity Representation Learning (ReSim), a new approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. While existing work has largely focused on solely learning global representations for an entire image, ReSim learns both regional representations for localization as well as semantic image-level representations. ReSim operates by sliding a fixed-sized window across the overlapping area between two views (e.g., image crops), aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps; this allows downstream tasks to leverage these representations for localization. Through object detection, instance segmentation, and dense pose estimation experiments, we illustrate how ReSim learns representations which significantly improve the localization and classification performance compared to a competitive MoCo-v2 baseline: \$+2.7\$ AP\${\textasciicircum}\{{\textbackslash}text\{bb\}\}\_\{75\}\$ VOC, \$+1.1\$ AP\${\textasciicircum}\{{\textbackslash}text\{bb\}\}\_\{75\}\$ COCO, and \$+1.9\$ AP\${\textasciicircum}\{{\textbackslash}text\{mk\}\}\$ Cityscapes. Code and pre-trained models are released at: {\textbackslash}url\{https://github.com/Tete-Xiao/ReSim\}},
	urldate = {2021-11-05},
	journal = {arXiv:2103.12902 [cs]},
	author = {Xiao, Tete and Reed, Colorado J. and Wang, Xiaolong and Keutzer, Kurt and Darrell, Trevor},
	month = aug,
	year = {2021},
	note = {arXiv: 2103.12902},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/5UJKTLYG/Xiao et al. - 2021 - Region Similarity Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/JY7C5UD6/2103.html:text/html},
}

@article{oord_representation_2018,
	title = {Representation learning with contrastive predictive coding},
	journal = {arXiv preprint arXiv:1807.03748},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	year = {2018},
	file = {Full Text:/Users/zalan/Zotero/storage/XT7BKKNW/Oord et al. - 2018 - Representation learning with contrastive predictiv.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/Z25B3W3R/1807.html:text/html},
}

@inproceedings{kolesnikov_revisiting_2019-1,
	title = {Revisiting self-supervised visual representation learning},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
	year = {2019},
	pages = {1920--1929},
	file = {Full Text:/Users/zalan/Zotero/storage/FJ3MIJK6/Kolesnikov et al. - 2019 - Revisiting self-supervised visual representation l.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/IGMJ9BD8/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html:text/html},
}

@article{asano_self-labelling_2019-1,
	title = {Self-labelling via simultaneous clustering and representation learning},
	journal = {arXiv preprint arXiv:1911.05371},
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/NSD5W6UF/Asano et al. - 2019 - Self-labelling via simultaneous clustering and rep.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/DZN5Z5H6/1911.html:text/html},
}

@article{becker_self-organizing_1992-1,
	title = {Self-organizing neural network that discovers surfaces in random-dot stereograms},
	volume = {355},
	number = {6356},
	journal = {Nature},
	author = {Becker, Suzanna and Hinton, Geoffrey E.},
	year = {1992},
	note = {Publisher: Nature Publishing Group},
	pages = {161--163},
	file = {Full Text:/Users/zalan/Zotero/storage/GP6R37D5/Becker and Hinton - 1992 - Self-organizing neural network that discovers surf.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/KJY73HJJ/355161a0.html:text/html},
}

@article{goyal_self-supervised_2021-1,
	title = {Self-supervised pretraining of visual features in the wild},
	journal = {arXiv preprint arXiv:2103.01988},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/PVH2LRMK/Goyal et al. - 2021 - Self-supervised pretraining of visual features in .pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/IFKRSIMK/2103.html:text/html},
}

@inproceedings{xie_self-training_2020-1,
	title = {Self-training with noisy student improves imagenet classification},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2020},
	pages = {10687--10698},
	file = {Full Text:/Users/zalan/Zotero/storage/C4PDTVF2/Xie et al. - 2020 - Self-training with noisy student improves imagenet.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/4CAUAEX9/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.html:text/html},
}

@article{tishby_information_2000-1,
	title = {The information bottleneck method},
	journal = {arXiv preprint physics/0004057},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {2000},
	file = {Full Text:/Users/zalan/Zotero/storage/YBHFMQX7/Tishby et al. - 2000 - The information bottleneck method.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/3TLLVF7N/0004057.html:text/html},
}

@article{tian_understanding_2021-1,
	title = {Understanding self-supervised learning dynamics without contrastive pairs},
	journal = {arXiv preprint arXiv:2102.06810},
	author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/V4FWCX2W/Tian et al. - 2021 - Understanding self-supervised learning dynamics wi.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/SD95MIPL/2102.html:text/html},
}

@article{caron_unsupervised_2020,
	title = {Unsupervised learning of visual features by contrasting cluster assignments},
	journal = {arXiv preprint arXiv:2006.09882},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/77WS45DX/Caron et al. - 2020 - Unsupervised learning of visual features by contra.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/EHPCRU7Y/2006.html:text/html},
}

@article{dosovitskiy_image_2020-1,
	title = {An image is worth 16x16 words: {Transformers} for image recognition at scale},
	shorttitle = {An image is worth 16x16 words},
	journal = {arXiv preprint arXiv:2010.11929},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/N546RUF7/Dosovitskiy et al. - 2020 - An image is worth 16x16 words Transformers for im.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/E7WSU5L2/2010.html:text/html},
}

@inproceedings{vaswani_attention_2017-1,
	title = {Attention is all you need},
	booktitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
	file = {Full Text:/Users/zalan/Zotero/storage/ZSHGG3WQ/Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf},
}

@article{devlin_bert_2018-1,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	file = {Full Text:/Users/zalan/Zotero/storage/C6NQKQRF/Devlin et al. - 2018 - Bert Pre-training of deep bidirectional transform.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/GJT8ZD8C/1810.html:text/html},
}

@article{child_generating_2019-1,
	title = {Generating long sequences with sparse transformers},
	journal = {arXiv preprint arXiv:1904.10509},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/AT2VYMQF/Child et al. - 2019 - Generating long sequences with sparse transformers.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/JSDSA6VN/1904.html:text/html},
}

@article{yang_ouroboros_2019,
	title = {Ouroboros: {On} accelerating training of transformer-based language models},
	shorttitle = {Ouroboros},
	journal = {arXiv preprint arXiv:1909.06695},
	author = {Yang, Qian and Huo, Zhouyuan and Wang, Wenlin and Huang, Heng and Carin, Lawrence},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/YD725VLC/Yang et al. - 2019 - Ouroboros On accelerating training of transformer.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/V67II7TW/1909.html:text/html},
}

@article{jaegle_perceiver_2021-1,
	title = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
	shorttitle = {Perceiver},
	url = {https://arxiv.org/abs/2103.03206v2},
	abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
	language = {en},
	urldate = {2021-11-10},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	month = mar,
	year = {2021},
	file = {Snapshot:/Users/zalan/Zotero/storage/7UHYN6FL/2103.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/RYEI69YW/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf:application/pdf},
}

@inproceedings{shen_powernorm_2020-1,
	title = {Powernorm: {Rethinking} batch normalization in transformers},
	shorttitle = {Powernorm},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael and Keutzer, Kurt},
	year = {2020},
	pages = {8741--8751},
	file = {Full Text:/Users/zalan/Zotero/storage/NN4YXMWI/Shen et al. - 2020 - Powernorm Rethinking batch normalization in trans.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/HVX7BA7Z/shen20e.html:text/html},
}

@article{lu_pretrained_2021-1,
	title = {Pretrained transformers as universal computation engines},
	journal = {arXiv preprint arXiv:2103.05247},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/JH52X7KN/Lu et al. - 2021 - Pretrained transformers as universal computation e.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/BVMDY65U/2103.html:text/html},
}

@article{liu_swin_2021-1,
	title = {Swin transformer: {Hierarchical} vision transformer using shifted windows},
	shorttitle = {Swin transformer},
	journal = {arXiv preprint arXiv:2103.14030},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
}

@article{fedus_switch_2021-1,
	title = {Switch transformers: {Scaling} to trillion parameter models with simple and efficient sparsity},
	shorttitle = {Switch transformers},
	journal = {arXiv preprint arXiv:2101.03961},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/3V7YGV5F/Fedus et al. - 2021 - Switch transformers Scaling to trillion parameter.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/HIZIK8MI/2101.html:text/html},
}

@inproceedings{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	year = {2021},
	pages = {10347--10357},
	file = {Full Text:/Users/zalan/Zotero/storage/ETTBIIAT/Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/7W9YGJNB/touvron21a.html:text/html},
}

@article{jiang_transgan_2021-1,
	title = {Transgan: {Two} transformers can make one strong gan},
	volume = {1},
	shorttitle = {Transgan},
	number = {3},
	journal = {arXiv preprint arXiv:2102.07074},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/ZSJME487/Jiang et al. - 2021 - Transgan Two transformers can make one strong gan.pdf:application/pdf},
}

@article{yuan_volo_2021-1,
	title = {Volo: {Vision} outlooker for visual recognition},
	shorttitle = {Volo},
	journal = {arXiv preprint arXiv:2106.13112},
	author = {Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/SWK8DLNF/Yuan et al. - 2021 - Volo Vision outlooker for visual recognition.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/BM5ZEGF8/2106.html:text/html},
}

@article{el-nouby_xcit_2021-1,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	shorttitle = {{XCiT}},
	journal = {arXiv preprint arXiv:2106.09681},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/S929RZF6/El-Nouby et al. - 2021 - XCiT Cross-Covariance Image Transformers.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/KC6Y5P6A/2106.html:text/html},
}

@article{yao_adahessian_2020-1,
	title = {{ADAHESSIAN}: {An} adaptive second order optimizer for machine learning},
	shorttitle = {{ADAHESSIAN}},
	journal = {arXiv preprint arXiv:2006.00719},
	author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/4EW6E43M/Yao et al. - 2020 - ADAHESSIAN An adaptive second order optimizer for.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/ZFJ3J9UN/2006.html:text/html},
}

@inproceedings{ghorbani_investigation_2019-1,
	title = {An investigation into neural net optimization via hessian eigenvalue density},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	year = {2019},
	pages = {2232--2241},
	file = {Full Text:/Users/zalan/Zotero/storage/8H3RV4HZ/Ghorbani et al. - 2019 - An investigation into neural net optimization via .pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/6AAM4HGE/ghorbani19b.html:text/html},
}

@article{skorski_chain_2019-1,
	title = {Chain rules for hessian and higher derivatives made easy by tensor calculus},
	journal = {arXiv preprint arXiv:1911.13292},
	author = {Skorski, Maciej},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/YLCJY2Y2/Skorski - 2019 - Chain rules for hessian and higher derivatives mad.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/YSF6JIF6/1911.html:text/html},
}

@article{derezinski_distributed_2019-1,
	title = {Distributed estimation of the inverse {Hessian} by determinantal averaging},
	journal = {arXiv preprint arXiv:1905.11546},
	author = {Dereziński, Micha{\textbackslash}l and Mahoney, Michael W.},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/8P6USSX8/Dereziński and Mahoney - 2019 - Distributed estimation of the inverse Hessian by d.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/PX2Y4RC7/1905.html:text/html},
}

@article{sagun_eigenvalues_2016,
	title = {Eigenvalues of the hessian in deep learning: {Singularity} and beyond},
	shorttitle = {Eigenvalues of the hessian in deep learning},
	journal = {arXiv preprint arXiv:1611.07476},
	author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
	year = {2016},
	file = {Full Text:/Users/zalan/Zotero/storage/R9LZGIUR/Sagun et al. - 2016 - Eigenvalues of the hessian in deep learning Singu.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/WTUYKBQF/1611.html:text/html},
}

@inproceedings{yao_pyhessian_2020-3,
	title = {Pyhessian: {Neural} networks through the lens of the hessian},
	shorttitle = {Pyhessian},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
	file = {Snapshot:/Users/zalan/Zotero/storage/5HQWZIKD/9378171.html:text/html},
}

@article{papyan_full_2018,
	title = {The full spectrum of deepnet hessians at scale: {Dynamics} with sgd training and sample size},
	shorttitle = {The full spectrum of deepnet hessians at scale},
	journal = {arXiv preprint arXiv:1811.07062},
	author = {Papyan, Vardan},
	year = {2018},
	file = {Full Text:/Users/zalan/Zotero/storage/A69TXGEV/Papyan - 2018 - The full spectrum of deepnet hessians at scale Dy.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/2UPUIMRL/1811.html:text/html},
}

@inproceedings{oakden-rayner_hidden_2020,
	address = {Toronto Ontario Canada},
	title = {Hidden stratification causes clinically meaningful failures in machine learning for medical imaging},
	isbn = {978-1-4503-7046-2},
	url = {https://dl.acm.org/doi/10.1145/3368555.3384468},
	doi = {10.1145/3368555.3384468},
	abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
	language = {en},
	urldate = {2021-11-10},
	booktitle = {Proceedings of the {ACM} {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {ACM},
	author = {Oakden-Rayner, Luke and Dunnmon, Jared and Carneiro, Gustavo and Re, Christopher},
	month = apr,
	year = {2020},
	pages = {151--159},
	file = {Oakden-Rayner et al. - 2020 - Hidden stratification causes clinically meaningful.pdf:/Users/zalan/Zotero/storage/8ZQTC7DT/Oakden-Rayner et al. - 2020 - Hidden stratification causes clinically meaningful.pdf:application/pdf},
}

@article{zhang_contrastive_2020,
	title = {Contrastive {Learning} of {Medical} {Visual} {Representations} from {Paired} {Images} and {Text}},
	url = {http://arxiv.org/abs/2010.00747},
	abstract = {Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classiﬁcation tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classiﬁcation tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efﬁciency.},
	language = {en},
	urldate = {2021-11-10},
	journal = {arXiv:2010.00747 [cs]},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.00747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Zhang et al. - 2020 - Contrastive Learning of Medical Visual Representat.pdf:/Users/zalan/Zotero/storage/JGRYN5TW/Zhang et al. - 2020 - Contrastive Learning of Medical Visual Representat.pdf:application/pdf},
}

@article{pham_dualnet_2021,
	title = {{DualNet}: {Continual} {Learning}, {Fast} and {Slow}},
	shorttitle = {{DualNet}},
	url = {http://arxiv.org/abs/2110.00175},
	abstract = {According to Complementary Learning Systems (CLS) theory{\textasciitilde}{\textbackslash}citep\{mcclelland1995there\} in neuroscience, humans do effective {\textbackslash}emph\{continual learning\} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named "DualNet", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised representation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complementary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL objectives to validate DualNet's efficacy, robustness, and scalability. Code will be made available upon acceptance.},
	urldate = {2021-11-10},
	journal = {arXiv:2110.00175 [cs]},
	author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
	month = sep,
	year = {2021},
	note = {arXiv: 2110.00175},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/758PELZQ/Pham et al. - 2021 - DualNet Continual Learning, Fast and Slow.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/5BDSN8YP/2110.html:text/html},
}

@article{kadkhodaie2021stochastic,
	title={Stochastic solutions for linear inverse problems using the prior implicit in a denoiser},
	author={Kadkhodaie, Zahra and Simoncelli, Eero},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={13242--13254},
	year={2021}
}

@article{kawar2021snips,
	title={SNIPS: Solving noisy inverse problems stochastically},
	author={Kawar, Bahjat and Vaksman, Gregory and Elad, Michael},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={21757--21769},
	year={2021}
}

@article{jalal2021robust,
	title={Robust compressed sensing mri with deep generative priors},
	author={Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G and Tamir, Jon},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={14938--14954},
	year={2021}
}

@article{chaudhry_tiny_2019,
	title = {On {Tiny} {Episodic} {Memories} in {Continual} {Learning}},
	url = {https://arxiv.org/abs/1902.10486v4},
	abstract = {In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7{\textbackslash}\% and 17{\textbackslash}\% when the memory is populated with a single example per class.},
	language = {en},
	urldate = {2021-11-11},
	author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K. and Torr, Philip H. S. and Ranzato, Marc'Aurelio},
	month = feb,
	year = {2019},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/MUMW485Z/Chaudhry et al. - 2019 - On Tiny Episodic Memories in Continual Learning.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/RHL9KZHU/1902.html:text/html},
}

@article{zhou_detecting_2021,
	title = {Detecting {Hallucinated} {Content} in {Conditional} {Neural} {Sequence} {Generation}},
	url = {http://arxiv.org/abs/2011.02593},
	abstract = {Neural sequence models can generate highly ﬂuent sentences, but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input. These variety of ﬂuent but wrong outputs are particularly problematic, as it will not be possible for users to tell they are being presented incorrect content. To detect these errors, we propose a task to predict whether each token in the output sequence is hallucinated (not contained in the input) and collect new manually annotated evaluation sets for this task. We also introduce a method for learning to detect hallucinations using pretrained language models ﬁne tuned on synthetic data that includes automatically inserted hallucinations Experiments on machine translation (MT) and abstractive summarization demonstrate that our proposed approach consistently outperforms strong baselines on all benchmark datasets. We further demonstrate how to use the token-level hallucination labels to deﬁne a ﬁne-grained loss over the target sequence in low-resource MT and achieve signiﬁcant improvements over strong baseline methods.We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1.},
	language = {en},
	urldate = {2021-11-11},
	journal = {arXiv:2011.02593 [cs]},
	author = {Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzman, Paco and Zettlemoyer, Luke and Ghazvininejad, Marjan},
	month = jun,
	year = {2021},
	note = {arXiv: 2011.02593},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Zhou et al. - 2021 - Detecting Hallucinated Content in Conditional Neur.pdf:/Users/zalan/Zotero/storage/TG5ZFR7J/Zhou et al. - 2021 - Detecting Hallucinated Content in Conditional Neur.pdf:application/pdf},
}

@article{chaudhari_prospective_2021,
	title = {Prospective {Deployment} of {Deep} {Learning} in {MRI}: {A} {Framework} for {Important} {Considerations}, {Challenges}, and {Recommendations} for {Best} {Practices}},
	volume = {54},
	issn = {1522-2586},
	shorttitle = {Prospective {Deployment} of {Deep} {Learning} in {MRI}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.27331},
	doi = {10.1002/jmri.27331},
	abstract = {Artificial intelligence algorithms based on principles of deep learning (DL) have made a large impact on the acquisition, reconstruction, and interpretation of MRI data. Despite the large number of retrospective studies using DL, there are fewer applications of DL in the clinic on a routine basis. To address this large translational gap, we review the recent publications to determine three major use cases that DL can have in MRI, namely, that of model-free image synthesis, model-based image reconstruction, and image or pixel-level classification. For each of these three areas, we provide a framework for important considerations that consist of appropriate model training paradigms, evaluation of model robustness, downstream clinical utility, opportunities for future advances, as well recommendations for best current practices. We draw inspiration for this framework from advances in computer vision in natural imaging as well as additional healthcare fields. We further emphasize the need for reproducibility of research studies through the sharing of datasets and software. Level of Evidence 5 Technical Efficacy Stage 2},
	language = {en},
	number = {2},
	urldate = {2021-11-12},
	journal = {Journal of Magnetic Resonance Imaging},
	author = {Chaudhari, Akshay S. and Sandino, Christopher M. and Cole, Elizabeth K. and Larson, David B. and Gold, Garry E. and Vasanawala, Shreyas S. and Lungren, Matthew P. and Hargreaves, Brian A. and Langlotz, Curtis P.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.27331},
	keywords = {artificial intelligence, classification, convolutional neural networks, deep learning, MRI reconstruction, segmentation},
	pages = {357--371},
	file = {Snapshot:/Users/zalan/Zotero/storage/LUWB4THT/jmri.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/3D4NNJQZ/Chaudhari et al. - 2021 - Prospective Deployment of Deep Learning in MRI A .pdf:application/pdf},
}

@article{knoll_assessment_2019-2,
	title = {Assessment of the generalization of learned image reconstruction and the potential for transfer learning},
	volume = {81},
	issn = {1522-2594},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27355},
	doi = {10.1002/mrm.27355},
	abstract = {Purpose Although deep learning has shown great promise for MR image reconstruction, an open question regarding the success of this approach is the robustness in the case of deviations between training and test data. The goal of this study is to assess the influence of image contrast, SNR, and image content on the generalization of learned image reconstruction, and to demonstrate the potential for transfer learning. Methods Reconstructions were trained from undersampled data using data sets with varying SNR, sampling pattern, image contrast, and synthetic data generated from a public image database. The performance of the trained reconstructions was evaluated on 10 in vivo patient knee MRI acquisitions from 2 different pulse sequences that were not used during training. Transfer learning was evaluated by fine-tuning baseline trainings from synthetic data with a small subset of in vivo MR training data. Results Deviations in SNR between training and testing led to substantial decreases in reconstruction image quality, whereas image contrast was less relevant. Trainings from heterogeneous training data generalized well toward the test data with a range of acquisition parameters. Trainings from synthetic, non-MR image data showed residual aliasing artifacts, which could be removed by transfer learning–inspired fine-tuning. Conclusion This study presents insights into the generalization ability of learned image reconstruction with respect to deviations in the acquisition settings between training and testing. It also provides an outlook for the potential of transfer learning to fine-tune trainings to a particular target application using only a small number of training cases.},
	language = {en},
	number = {1},
	urldate = {2021-11-12},
	journal = {Magnetic Resonance in Medicine},
	author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.27355},
	keywords = {transfer learning, deep learning, accelerated imaging, iterative image reconstruction, machine learning, variational network},
	pages = {116--128},
	file = {Snapshot:/Users/zalan/Zotero/storage/3UGGYGRQ/mrm.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/2D4JQRC4/Knoll et al. - 2019 - Assessment of the generalization of learned image .pdf:application/pdf},
}

@article{ravula_inverse_2021,
	title = {Inverse {Problems} {Leveraging} {Pre}-trained {Contrastive} {Representations}},
	url = {http://arxiv.org/abs/2110.07439},
	abstract = {We study a new family of inverse problems for recovering representations of corrupted data. We assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x), if we are only given a corrupted version A(x), for some known forward operator A. We propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. Using a linear probe on our robust representations, we achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking. We evaluate on a subset of ImageNet and observe that our method is robust to varying levels of distortion. Our method outperforms end-to-end baselines even with a fraction of the labeled data in a wide range of forward operators.},
	language = {en},
	urldate = {2021-11-12},
	journal = {arXiv:2110.07439 [cs]},
	author = {Ravula, Sriram and Smyrnis, Georgios and Jordan, Matt and Dimakis, Alexandros G.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.07439},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Ravula et al. - 2021 - Inverse Problems Leveraging Pre-trained Contrastiv.pdf:/Users/zalan/Zotero/storage/WFX2P669/Ravula et al. - 2021 - Inverse Problems Leveraging Pre-trained Contrastiv.pdf:application/pdf},
}

@article{desai_vortex_2021,
	title = {{VORTEX}: {Physics}-{Driven} {Data} {Augmentations} for {Consistency} {Training} for {Robust} {Accelerated} {MRI} {Reconstruction}},
	shorttitle = {{VORTEX}},
	url = {http://arxiv.org/abs/2111.02549},
	abstract = {Deep neural networks have enabled improved image quality and fast inference times for various inverse problems, including accelerated magnetic resonance imaging (MRI) reconstruction. However, such models require large amounts of fully-sampled ground truth data, which are difficult to curate and are sensitive to distribution drifts. In this work, we propose applying physics-driven data augmentations for consistency training that leverage our domain knowledge of the forward MRI data acquisition process and MRI physics for improved data efficiency and robustness to clinically-relevant distribution drifts. Our approach, termed VORTEX (1) demonstrates strong improvements over supervised baselines with and without augmentation in robustness to signal-to-noise ratio change and motion corruption in data-limited regimes; (2) considerably outperforms state-of-the-art data augmentation techniques that are purely image-based on both in-distribution and out-of-distribution data; and (3) enables composing heterogeneous image-based and physics-driven augmentations.},
	urldate = {2021-11-13},
	journal = {arXiv:2111.02549 [physics]},
	author = {Desai, Arjun D. and Gunel, Beliz and Ozturkler, Batu M. and Beg, Harris and Vasanawala, Shreyas and Hargreaves, Brian A. and Ré, Christopher and Pauly, John M. and Chaudhari, Akshay S.},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.02549},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/CLMFREIY/Desai et al. - 2021 - VORTEX Physics-Driven Data Augmentations for Cons.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/P9NRX9UM/2111.html:text/html},
}

@article{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2021-11-17},
	journal = {arXiv:2111.06377 [cs]},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.06377},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:/Users/zalan/Zotero/storage/3CAIDDNC/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@article{bao_beit_2021,
	title = {{BEiT}: {BERT} {Pre}-{Training} of {Image} {Transformers}},
	shorttitle = {{BEiT}},
	url = {http://arxiv.org/abs/2106.08254},
	abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
	urldate = {2021-11-17},
	journal = {arXiv:2106.08254 [cs]},
	author = {Bao, Hangbo and Dong, Li and Wei, Furu},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/676XS5IJ/Bao et al. - 2021 - BEiT BERT Pre-Training of Image Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/WJ36EZRV/2106.html:text/html},
}

@article{ma_vaem_nodate,
	title = {{VAEM}: a {Deep} {Generative} {Model} for {Heterogeneous} {Mixed} {Type} {Data}},
	abstract = {Deep generative models often perform poorly in real-world applications due to the heterogeneity of natural data sets. Heterogeneity arises from data containing different types of features (categorical, ordinal, continuous, etc.) and features of the same type having different marginal distributions. We propose an extension of variational autoencoders (VAEs) called VAEM to handle such heterogeneous data. VAEM is a deep generative model that is trained in a two stage manner such that the ﬁrst stage provides a more uniform representation of the data to the second stage, thereby sidestepping the problems caused by heterogeneous data. We provide extensions of VAEM to handle partially observed data, and demonstrate its performance in data generation, missing data prediction and sequential feature selection tasks. Our results show that VAEM broadens the range of real-world applications where deep generative models can be successfully deployed.},
	language = {en},
	author = {Ma, Chao and Tschiatschek, Sebastian and Turner, Richard and Hernandez-Lobato, Jose Miguel and Zhang, Cheng},
	pages = {11},
	file = {Ma et al. - VAEM a Deep Generative Model for Heterogeneous Mi.pdf:/Users/zalan/Zotero/storage/33PMK5AS/Ma et al. - VAEM a Deep Generative Model for Heterogeneous Mi.pdf:application/pdf},
}

@incollection{ma_dual-task_2021,
	address = {Cham},
	title = {Dual-{Task} {Mutual} {Learning} for {Semi}-supervised {Medical} {Image} {Segmentation}},
	volume = {13021},
	isbn = {978-3-030-88009-5 978-3-030-88010-1},
	url = {https://link.springer.com/10.1007/978-3-030-88010-1_46},
	abstract = {The success of deep learning methods in medical image segmentation tasks usually requires a large amount of labeled data. However, obtaining reliable annotations is expensive and time-consuming. Semi-supervised learning has attracted much attention in medical image segmentation by taking the advantage of unlabeled data which is much easier to acquire. In this paper, we propose a novel dual-task mutual learning framework for semi-supervised medical image segmentation. Our framework can be formulated as an integration of two individual segmentation networks based on two tasks: learning region-based shape constraint and learning boundary-based surface mismatch. Diﬀerent from the one-way transfer between teacher and student networks, an ensemble of dual-task students can learn collaboratively and implicitly explore useful knowledge from each other during the training process. By jointly learning the segmentation probability maps and signed distance maps of targets, our framework can enforce the geometric shape constraint and learn more reliable information. Experimental results demonstrate that our method achieves performance gains by leveraging unlabeled data and outperforms the state-of-the-art semi-supervised segmentation methods.},
	language = {en},
	urldate = {2021-11-18},
	booktitle = {Pattern {Recognition} and {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Zhang, Yichi and Zhang, Jicong},
	editor = {Ma, Huimin and Wang, Liang and Zhang, Changshui and Wu, Fei and Tan, Tieniu and Wang, Yaonan and Lai, Jianhuang and Zhao, Yao},
	year = {2021},
	doi = {10.1007/978-3-030-88010-1_46},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {548--559},
	file = {Zhang and Zhang - 2021 - Dual-Task Mutual Learning for Semi-supervised Medi.pdf:/Users/zalan/Zotero/storage/VL6Q7SD5/Zhang and Zhang - 2021 - Dual-Task Mutual Learning for Semi-supervised Medi.pdf:application/pdf},
}

@article{hasan_multi-task_nodate,
	title = {A {Multi}-{Task} {Cross}-{Task} {Learning} {Architecture} for {Ad}-hoc {Uncertainty} {Estimation} in {3D} {Cardiac} {MRI} {Image} {Segmentation}},
	abstract = {Medical image segmentation has signiﬁcantly beneﬁtted thanks to deep learning architectures. Furthermore, semi-supervised learning (SSL) has recently been a growing trend for improving a model’s overall performance by leveraging abundant unlabeled data. Moreover, learning multiple tasks within the same model further improves model generalizability. To generate smooth and accurate segmentation masks from 3D cardiac MR images, we present a Multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justiﬁes the effectiveness of our model for the segmentation of the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR) images. With the incorporation of uncertainty estimates to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to ﬂag low-quality segmentation from a given model.},
	language = {en},
	author = {Hasan, S M Kamrul and Linte, Cristian A},
	pages = {4},
	file = {Hasan and Linte - A Multi-Task Cross-Task Learning Architecture for .pdf:/Users/zalan/Zotero/storage/WKPC3N2I/Hasan and Linte - A Multi-Task Cross-Task Learning Architecture for .pdf:application/pdf},
}

@article{wang_vlmo_nodate,
	title = {{VLMO}: {Uniﬁed} {Vision}-{Language} {Pre}-{Training} with {Mixture}-of-{Modality}-{Experts}},
	abstract = {We present a uniﬁed Vision-Language pretrained Model (VLMO) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Speciﬁcally, we introduce Mixture-of-Modality-Experts (MOME) Transformer, where each block contains a pool of modality-speciﬁc experts and a shared selfattention layer. Because of the modeling ﬂexibility of MOME, pretrained VLMO can be ﬁne-tuned as a fusion encoder for vision-language classiﬁcation tasks, or used as a dual encoder for efﬁcient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMO achieves state-of-the-art results on various vision-language tasks, including VQA and NLVR2. The code and pretrained models are available at https://aka.ms/ vlmo.},
	language = {en},
	author = {Wang, Wenhui and Dong, Li and Wei, Furu},
	pages = {15},
	file = {Wang et al. - VLMO Uniﬁed Vision-Language Pre-Training with Mix.pdf:/Users/zalan/Zotero/storage/E5VLKWWN/Wang et al. - VLMO Uniﬁed Vision-Language Pre-Training with Mix.pdf:application/pdf},
}

@inproceedings{liang_swinir_2021-2,
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	shorttitle = {{SwinIR}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2021-11-18},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
	pages = {1833--1844},
	file = {Snapshot:/Users/zalan/Zotero/storage/QR3ZH7T4/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/EWEFCY94/Liang et al. - 2021 - SwinIR Image Restoration Using Swin Transformer.pdf:application/pdf},
}

@article{riquelme_scaling_2021,
	title = {Scaling {Vision} with {Sparse} {Mixture} of {Experts}},
	url = {http://arxiv.org/abs/2106.05974},
	abstract = {Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are "dense", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35\% on ImageNet.},
	urldate = {2021-11-18},
	journal = {arXiv:2106.05974 [cs, stat]},
	author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Pinto, André Susano and Keysers, Daniel and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05974},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/FP262VN8/Riquelme et al. - 2021 - Scaling Vision with Sparse Mixture of Experts.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/RTQPLKH3/2106.html:text/html},
}

@article{yang_condconv_2020,
	title = {{CondConv}: {Conditionally} {Parameterized} {Convolutions} for {Efficient} {Inference}},
	shorttitle = {{CondConv}},
	url = {http://arxiv.org/abs/1904.04971},
	abstract = {Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efﬁcient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classiﬁcation and detection tasks. On ImageNet classiﬁcation, our CondConv approach applied to EfﬁcientNet-B0 achieves state-ofthe-art performance of 78.3\% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorﬂow layer and CondConv-EfﬁcientNet models are available at: https://github.com/tensorflow/tpu/tree/master/ models/official/efficientnet/condconv.},
	language = {en},
	urldate = {2021-11-18},
	journal = {arXiv:1904.04971 [cs]},
	author = {Yang, Brandon and Bender, Gabriel and Le, Quoc V. and Ngiam, Jiquan},
	month = sep,
	year = {2020},
	note = {arXiv: 1904.04971},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Yang et al. - 2020 - CondConv Conditionally Parameterized Convolutions.pdf:/Users/zalan/Zotero/storage/CJM73289/Yang et al. - 2020 - CondConv Conditionally Parameterized Convolutions.pdf:application/pdf},
}

@article{wang_deep_nodate,
	title = {Deep {Mixture} of {Experts} via {Shallow} {Embedding}},
	abstract = {Larger networks generally have greater representational power at the cost of increased computational complexity. Sparsifying such networks has been an active area of research but has been generally limited to static regularization or dynamic approaches using reinforcement learning. We explore a mixture of experts (MoE) approach to deep dynamic routing, which activates certain experts in the network on a per-example basis. Our novel DeepMoE architecture increases the representational power of standard convolutional networks by adaptively sparsifying and recalibrating channel-wise features in each convolutional layer. We employ a multi-headed sparse gating network to determine the selection and scaling of channels for each input, leveraging exponential combinations of experts within a single convolutional network. Our proposed architecture is evaluated on four benchmark datasets and tasks, and we show that DeepMoEs are able to achieve higher accuracy with lower computation than standard convolutional networks.},
	language = {en},
	author = {Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E},
	pages = {11},
	file = {Wang et al. - Deep Mixture of Experts via Shallow Embedding.pdf:/Users/zalan/Zotero/storage/NYQ6KE22/Wang et al. - Deep Mixture of Experts via Shallow Embedding.pdf:application/pdf},
}

@article{song_dynamic_nodate,
	title = {Dynamic {Grained} {Encoder} for {Vision} {Transformers}},
	abstract = {Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Speciﬁcally, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a ﬁne-grained representation in discriminative regions while keeping high efﬁciency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40\%-60\% while maintaining comparable performance on image classiﬁcation. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.},
	language = {en},
	author = {Song, Lin and Zhang, Songyang and Liu, Songtao and Li, Zeming and He, Xuming and Sun, Hongbin and Sun, Jian and Zheng, Nanning},
	pages = {14},
	file = {Song et al. - Dynamic Grained Encoder for Vision Transformers.pdf:/Users/zalan/Zotero/storage/9WYQJ3ZD/Song et al. - Dynamic Grained Encoder for Vision Transformers.pdf:application/pdf},
}

@article{hatamizadeh_unetr_2021,
	title = {{UNETR}: {Transformers} for {3D} {Medical} {Image} {Segmentation}},
	shorttitle = {{UNETR}},
	url = {http://arxiv.org/abs/2103.10504},
	abstract = {Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful “U-shaped” network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the ﬁnal semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multiorgan segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:2103.10504 [cs, eess]},
	author = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger and Xu, Daguang},
	month = oct,
	year = {2021},
	note = {arXiv: 2103.10504},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	file = {Hatamizadeh et al. - 2021 - UNETR Transformers for 3D Medical Image Segmentat.pdf:/Users/zalan/Zotero/storage/2K499ZXM/Hatamizadeh et al. - 2021 - UNETR Transformers for 3D Medical Image Segmentat.pdf:application/pdf},
}

@article{cao_swin-unet_2021,
	title = {Swin-{Unet}: {Unet}-like {pure} {Transformer} for {medical} {image} {segmentation}},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:2105.05537},
	author = {Cao, Hu and Wang, Yueyue and Chen, Joy and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi and Wang, Manning},
	month = may,
	year = {2021},
	note = {arXiv: 2105.05537},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{sriram_end--end_2020,
	title = {End-to-{end} {variational} {networks} for {accelerated} {MRI} {reconstruction}},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention}},
	author = {Sriram, Anuroop and Zbontar, Jure and Murrell, Tullie and Defazio, Aaron and Zitnick, C. Lawrence and Yakubova, Nafissa and Knoll, Florian and Johnson, Patricia},
	year = {2020},
	pages = {64--73},
}

@article{eo_kiki-net_2018,
	title = {{KIKI}-net: cross-domain convolutional neural networks for reconstructing undersampled magnetic resonance images},
	volume = {80},
	issn = {1522-2594},
	shorttitle = {{KIKI}-net},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27201},
	doi = {10.1002/mrm.27201},
	abstract = {Purpose To demonstrate accurate MR image reconstruction from undersampled k-space data using cross-domain convolutional neural networks (CNNs) Methods Cross-domain CNNs consist of 3 components: (1) a deep CNN operating on the k-space (KCNN), (2) a deep CNN operating on an image domain (ICNN), and (3) an interleaved data consistency operations. These components are alternately applied, and each CNN is trained to minimize the loss between the reconstructed and corresponding fully sampled k-spaces. The final reconstructed image is obtained by forward-propagating the undersampled k-space data through the entire network. Results Performances of K-net (KCNN with inverse Fourier transform), I-net (ICNN with interleaved data consistency), and various combinations of the 2 different networks were tested. The test results indicated that K-net and I-net have different advantages/disadvantages in terms of tissue-structure restoration. Consequently, the combination of K-net and I-net is superior to single-domain CNNs. Three MR data sets, the T2 fluid-attenuated inversion recovery (T2 FLAIR) set from the Alzheimer's Disease Neuroimaging Initiative and 2 data sets acquired at our local institute (T2 FLAIR and T1 weighted), were used to evaluate the performance of 7 conventional reconstruction algorithms and the proposed cross-domain CNNs, which hereafter is referred to as KIKI-net. KIKI-net outperforms conventional algorithms with mean improvements of 2.29 dB in peak SNR and 0.031 in structure similarity. Conclusion KIKI-net exhibits superior performance over state-of-the-art conventional algorithms in terms of restoring tissue structures and removing aliasing artifacts. The results demonstrate that KIKI-net is applicable up to a reduction factor of 3 to 4 based on variable-density Cartesian undersampling.},
	language = {en},
	number = {5},
	urldate = {2021-12-07},
	journal = {Magnetic Resonance in Medicine},
	author = {Eo, Taejoon and Jun, Yohan and Kim, Taeseong and Jang, Jinseong and Lee, Ho-Joon and Hwang, Dosik},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.27201},
	keywords = {convolutional neural networks, MRI acceleration, cross-domain deep learning, image reconstruction, k-space completion},
	pages = {2188--2201},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/ELVEBB6P/Eo et al. - 2018 - KIKI-net cross-domain convolutional neural networ.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/EYB42KBT/mrm.html:text/html},
}

@article{lopez-paz_gradient_nodate,
	title = {Gradient {Episodic} {Memory} for {Continual} {Learning}},
	abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneﬁcial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
	language = {en},
	author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
	pages = {10},
	file = {Lopez-Paz and Ranzato - Gradient Episodic Memory for Continual Learning.pdf:/Users/zalan/Zotero/storage/MWHNYY9Y/Lopez-Paz and Ranzato - Gradient Episodic Memory for Continual Learning.pdf:application/pdf},
}

@article{rao_continual_2019,
	title = {Continual {Unsupervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1910.14481},
	abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the ﬁeld has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shufﬂed. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efﬁcacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
	language = {en},
	urldate = {2021-12-08},
	journal = {arXiv:1910.14481 [cs, stat]},
	author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.14481},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:/Users/zalan/Zotero/storage/ECQWJTEB/Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:application/pdf},
}

@inproceedings{yoon_federated_2021,
	title = {Federated {Continual} {Learning} with {Weighted} {Inter}-client {Transfer}},
	url = {https://proceedings.mlr.press/v139/yoon21b.html},
	abstract = {There has been a surge of interest in continual learning and federated learning, both of which are important in deep neural networks in real-world scenarios. Yet little research has been done regarding the scenario where each client learns on a sequence of tasks from a private local data stream. This problem of federated continual learning poses new challenges to continual learning, such as utilizing knowledge from other clients, while preventing interference from irrelevant knowledge. To resolve these issues, we propose a novel federated continual learning framework, Federated Weighted Inter-client Transfer (FedWeIT), which decomposes the network weights into global federated parameters and sparse task-specific parameters, and each client receives selective knowledge from other clients by taking a weighted combination of their task-specific parameters. FedWeIT minimizes interference between incompatible tasks, and also allows positive knowledge transfer across clients during learning. We validate our FedWeIT against existing federated learning and continual learning methods under varying degrees of task similarity across clients, and our model significantly outperforms them with a large reduction in the communication cost.},
	language = {en},
	urldate = {2021-12-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yoon, Jaehong and Jeong, Wonyong and Lee, Giwoong and Yang, Eunho and Hwang, Sung Ju},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12073--12086},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/B3EI4B75/Yoon et al. - 2021 - Federated Continual Learning with Weighted Inter-c.pdf:application/pdf},
}

@article{le_federated_2021,
	title = {Federated {Continuous} {Learning} {With} {Broad} {Network} {Architecture}},
	volume = {51},
	issn = {2168-2267, 2168-2275},
	url = {https://ieeexplore.ieee.org/document/9477571/},
	doi = {10.1109/TCYB.2021.3090260},
	abstract = {Federated learning (FL) is a machine-learning setting, where multiple clients collaboratively train a model under the coordination of a central server. The clients’ raw data are locally stored, and each client only uploads the trained weight to the server, which can mitigate the privacy risks from the centralized machine learning. However, most of the existing FL models focus on one-time learning without consideration for continuous learning. Continuous learning supports learning from streaming data continuously, so it can adapt to environmental changes and provide better real-time performance. In this article, we present a federated continuous learning scheme based on broad learning (FCL-BL) to support efﬁcient and accurate federated continuous learning (FCL). In FCL-BL, we propose a weighted processing strategy to solve the catastrophic forgetting problem, so FCL-BL can handle continuous learning. Then, we develop a local-independent training solution to support fast and accurate training in FCL-BL. The proposed solution enables us to avoid using a time-consuming synchronous approach while addressing the inaccurate-training issue rooted in the previous asynchronous approach. Moreover, we introduce a batch-asynchronous approach and broad learning (BL) technique to guarantee the high efﬁciency of FCL-BL. Speciﬁcally, the batch-asynchronous approach reduces the number of client–server interaction rounds, and the BL technique supports incremental learning without retraining when learning newly produced data. Finally, theoretical analysis and experimental results further illustrate that FCL-BL is superior to the existing FL schemes in terms of efﬁciency and accuracy in FCL.},
	language = {en},
	number = {8},
	urldate = {2021-12-09},
	journal = {IEEE Transactions on Cybernetics},
	author = {Le, Junqing and Lei, Xinyu and Mu, Nankun and Zhang, Hengrun and Zeng, Kai and Liao, Xiaofeng},
	month = aug,
	year = {2021},
	pages = {3874--3888},
	file = {Le et al. - 2021 - Federated Continuous Learning With Broad Network A.pdf:/Users/zalan/Zotero/storage/YQQQGHCA/Le et al. - 2021 - Federated Continuous Learning With Broad Network A.pdf:application/pdf},
}

@article{casado_concept_2021,
	title = {Concept drift detection and adaptation for federated and continual learning},
	issn = {1380-7501, 1573-7721},
	url = {http://arxiv.org/abs/2105.13309},
	doi = {10.1007/s11042-021-11219-x},
	abstract = {Smart devices, such as smartphones, wearables, robots, and others, can collect vast amounts of data from their environment. This data is suitable for training machine learning models, which can signiﬁcantly improve their behavior, and therefore, the user experience. Federated learning is a young and popular framework that allows multiple distributed devices to train deep learning models collaboratively while preserving data privacy. Nevertheless, this approach may not be optimal for scenarios where data distribution is nonidentical among the participants or changes over time, causing what is known as concept drift. Little research has yet been done in this ﬁeld, but this kind of situation is quite frequent in real life and poses new challenges to both continual and federated learning. Therefore, in this work, we present a new method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our proposal is an extension of the most popular federated algorithm, Federated Averaging (FedAvg), enhancing it for continual adaptation under concept drift. We empirically demonstrate the weaknesses of regular FedAvg and prove that CDA-FedAvg outperforms it in this type of scenario.},
	language = {en},
	urldate = {2021-12-09},
	journal = {Multimedia Tools and Applications},
	author = {Casado, Fernando E. and Lema, Dylan and Criado, Marcos F. and Iglesias, Roberto and Regueiro, Carlos V. and Barro, Senén},
	month = jul,
	year = {2021},
	note = {arXiv: 2105.13309},
	keywords = {Computer Science - Machine Learning},
	file = {Casado et al. - 2021 - Concept drift detection and adaptation for federat.pdf:/Users/zalan/Zotero/storage/2NSPEJAS/Casado et al. - 2021 - Concept drift detection and adaptation for federat.pdf:application/pdf},
}

@article{hendryx_federated_2021,
	title = {Federated {Reconnaissance}: {Efficient}, {Distributed}, {Class}-{Incremental} {Learning}},
	shorttitle = {Federated {Reconnaissance}},
	url = {http://arxiv.org/abs/2109.00150},
	abstract = {We describe federated reconnaissance, a class of learning problems in which distributed clients learn new concepts independently and communicate that knowledge efﬁciently. In particular, we propose an evaluation framework and methodological baseline for a system in which each client is expected to learn a growing set of classes and communicate knowledge of those classes efﬁciently with other clients, such that, after knowledge merging, the clients should be able to accurately discriminate between classes in the superset of classes observed by the set of clients. We compare a range of learning algorithms for this problem and ﬁnd that prototypical networks are a strong approach in that they are robust to catastrophic forgetting while incorporating new information efﬁciently. Furthermore, we show that the online averaging of prototype vectors is effective for client model merging and requires only a small amount of communication overhead, memory, and update time per class with no gradient-based learning or hyperparameter tuning. Additionally, to put our results in context, we ﬁnd that a simple, prototypical network with four convolutional layers signiﬁcantly outperforms complex, state of the art continual learning algorithms, increasing the accuracy by over 22\% after learning 600 Omniglot classes and over 33\% after learning 20 mini-ImageNet classes incrementally. These results have important implications for federated reconnaissance and continual learning more generally by demonstrating that communicating feature vectors is an efﬁcient, robust, and effective means for distributed, continual learning.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2109.00150 [cs]},
	author = {Hendryx, Sean M. and KC, Dharma Raj and Walls, Bradley and Morrison, Clayton T.},
	month = aug,
	year = {2021},
	note = {arXiv: 2109.00150},
	keywords = {Computer Science - Machine Learning},
	file = {Hendryx et al. - 2021 - Federated Reconnaissance Efficient, Distributed, .pdf:/Users/zalan/Zotero/storage/5NLN3WTP/Hendryx et al. - 2021 - Federated Reconnaissance Efficient, Distributed, .pdf:application/pdf},
}

@article{usmanova_distillation-based_2021,
	title = {A distillation-based approach integrating continual learning and federated learning for pervasive services},
	url = {http://arxiv.org/abs/2109.04197},
	abstract = {Federated Learning, a new machine learning paradigm enhancing the use of edge devices, is receiving a lot of attention in the pervasive community to support the development of smart services. Nevertheless, this approach still needs to be adapted to the speciﬁcity of the pervasive domain. In particular, issues related to continual learning need to be addressed. In this paper, we present a distillation-based approach dealing with catastrophic forgetting in federated learning scenario. Speciﬁcally, Human Activity Recognition tasks are used as a demonstration domain.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2109.04197 [cs]},
	author = {Usmanova, Anastasiia and Portet, François and Lalanda, Philippe and Vega, German},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.04197},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Usmanova et al. - 2021 - A distillation-based approach integrating continua.pdf:/Users/zalan/Zotero/storage/Y7QWV3W9/Usmanova et al. - 2021 - A distillation-based approach integrating continua.pdf:application/pdf},
}

@article{park_tackling_2021,
	title = {Tackling {Dynamics} in {Federated} {Incremental} {Learning} with {Variational} {Embedding} {Rehearsal}},
	url = {http://arxiv.org/abs/2110.09695},
	abstract = {Federated Learning is a fast growing area of ML where the training datasets are extremely distributed, all while dynamically changing over time. Models need to be trained on clients’ devices without any guarantees for either homogeneity or stationarity of the local private data. The need for continual training has also risen, due to the ever-increasing production of in-task data. However, pursuing both directions at the same time is challenging, since client data privacy is a major constraint, especially for rehearsal methods. Herein, we propose a novel algorithm to address the incremental learning process in an FL scenario, based on realistic client enrollment scenarios where clients can drop in or out dynamically. We ﬁrst propose using deep Variational Embeddings that secure the privacy of the client data. Second, we propose a server-side training method that enables a model to rehearse the previously learnt knowledge. Finally, we investigate the performance of federated incremental learning in dynamic client enrollment scenarios. The proposed method shows parity with ofﬂine training on domain-incremental learning, addressing challenges in both the dynamic enrollment of clients and the domain shifting of client data.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2110.09695 [cs]},
	author = {Park, Tae Jin and Kumatani, Kenichi and Dimitriadis, Dimitrios},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09695},
	keywords = {Computer Science - Machine Learning},
	file = {Park et al. - 2021 - Tackling Dynamics in Federated Incremental Learnin.pdf:/Users/zalan/Zotero/storage/GCFEC6DF/Park et al. - 2021 - Tackling Dynamics in Federated Incremental Learnin.pdf:application/pdf},
}

@article{zamir_restormer_2021,
	title = {Restormer: {Efficient} {Transformer} for {high}-{resolution} {image} {restoration}},
	journal = {arXiv:2111.09881},
	author = {Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
	year = {2021},
}

@article{ghesu_self-supervised_2022,
	title = {Self-supervised {Learning} from 100 {Million} {Medical} {Images}},
	url = {http://arxiv.org/abs/2201.01283},
	abstract = {Building accurate and robust artiﬁcial intelligence systems for medical image assessment requires not only the research and design of advanced deep learning models but also the creation of large and curated sets of annotated training examples. Constructing such datasets, however, is often very costly – due to the complex nature of annotation tasks and the high level of expertise required for the interpretation of medical images (e.g., expert radiologists). To counter this limitation, we propose a method for self-supervised learning of rich image features based on contrastive learning and online feature clustering. For this purpose we leverage large training datasets of over 100,000,000 medical images of various modalities, including radiography, computed tomography (CT), magnetic resonance (MR) imaging and ultrasonography. We propose to use these features to guide model training in supervised and hybrid self-supervised/supervised regime on various downstream tasks. We highlight a number of advantages of this strategy on challenging image assessment problems in radiography, CT and MR: 1) Signiﬁcant increase in accuracy compared to the state-of-theart (e.g., AUC boost of 3-7\% for detection of abnormalities from chest radiography scans and hemorrhage detection on brain CT); 2) Acceleration of model convergence during training by up to 85\% compared to using no pretraining (e.g., 83\% when training a model for detection of brain metastases in MR scans); 3) Increase in robustness to various image augmentations, such as intensity variations, rotations or scaling reﬂective of data variation seen in the ﬁeld.},
	language = {en},
	urldate = {2022-01-07},
	journal = {arXiv:2201.01283 [cs]},
	author = {Ghesu, Florin C. and Georgescu, Bogdan and Mansoor, Awais and Yoo, Youngjin and Neumann, Dominik and Patel, Pragneshkumar and Vishwanath, R. S. and Balter, James M. and Cao, Yue and Grbic, Sasa and Comaniciu, Dorin},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.01283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ghesu et al. - 2022 - Self-supervised Learning from 100 Million Medical .pdf:/Users/zalan/Zotero/storage/8M68SH4M/Ghesu et al. - 2022 - Self-supervised Learning from 100 Million Medical .pdf:application/pdf},
}

@article{ongie_deep_2020-2,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	volume = {1},
	issn = {2641-8770},
	doi = {10.1109/JSAIT.2020.2991563},
	abstract = {Recent work in machine learning shows that deep neural networks can be used to solve a wide variety of inverse problems arising in computational imaging. We explore the central prevailing themes of this emerging area and present a taxonomy that can be used to categorize different problems and reconstruction methods. Our taxonomy is organized along two central axes: (1) whether or not a forward model is known and to what extent it is used in training and testing, and (2) whether or not the learning is supervised or unsupervised, i.e., whether or not the training relies on access to matched ground truth image and measurement pairs. We also discuss the tradeoffs associated with these different reconstruction approaches, caveats and common failure modes, plus open problems and avenues for future work.},
	number = {1},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Journal on Selected Areas in Information Theory},
	keywords = {Training, Deep learning, image reconstruction, computational imaging, Computational modeling, deep neural networks, Image reconstruction, image restoration, Imaging, Interpolation, inverse problems, Inverse problems, Machine learning},
	pages = {39--56},
	file = {IEEE Xplore Full Text PDF:/Users/zalan/Zotero/storage/6DVBF3R7/Ongie et al. - 2020 - Deep Learning Techniques for Inverse Problems in I.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/zalan/Zotero/storage/5NHS22QG/9084378.html:text/html},
}

@article{ramzi2020xpdnet,
	title={{XPDNet} for {MRI} Reconstruction: an application to the 2020 {fastMRI} {Challenge}},
	author={Ramzi, Zaccharie and Ciuciu, Philippe and Starck, Jean-Luc},
	journal={arXiv preprint arXiv:2010.07290},
	year={2020}
}

@article{hammernik2019sigma,
	title={{$\Sigma$}-net: Systematic Evaluation of Iterative Deep Neural Networks for Fast Parallel {MR} Image Reconstruction},
	author={Hammernik, Kerstin and Schlemper, Jo and Qin, Chen and Duan, Jinming and Summers, Ronald M and Rueckert, Daniel},
	journal={arXiv preprint arXiv:1912.09278},
	year={2019}
}

@article{muckley2021results,
	title={Results of the 2020 {fastMRI} challenge for machine learning {MR} image reconstruction},
	author={Muckley, Matthew J and Riemenschneider, Bruno and Radmanesh, Alireza and Kim, Sunwoo and Jeong, Geunu and Ko, Jingyu and Jun, Yohan and Shin, Hyungseob and Hwang, Dosik and Mostapha, Mahmoud and others},
	journal={IEEE transactions on Medical Imaging},
	volume={40},
	number={9},
	pages={2306--2317},
	year={2021}
}

@inproceedings{ronneberger2015u,
	title={U-net: Convolutional networks for biomedical image segmentation},
	author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages={234--241},
	year={2015}
}

@article{hyun2018deep,
	title={Deep learning for undersampled {MRI} reconstruction},
	author={Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
	journal={Physics in Medicine \& Biology},
	volume={63},
	number={13},
	pages={135007},
	year={2018},
}

@article{han2018framing,
	title={Framing {U-Net} via deep convolutional framelets: {Application} to sparse-view {CT}},
	author={Han, Yoseob and Ye, Jong Chul},
	journal={IEEE Transactions on Medical Imaging},
	volume={37},
	number={6},
	pages={1418--1429},
	year={2018}
}

@inproceedings{cciccek20163d,
	title={{3D} {U-Net}: {Learning} dense volumetric segmentation from sparse annotation},
	author={{\c{C}}i{\c{c}}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
	booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages={424--432},
	year={2016},
	organization={Springer}
}

@incollection{zhou2018unet++,
	title={Unet++: A nested {U-net} architecture for medical image segmentation},
	author={Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
	booktitle={Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
	pages={3--11},
	year={2018},
}

@article{sun2016deep,
	title={Deep {ADMM-Net} for compressive sensing {MRI}},
	author={Sun, Jian and Li, Huibin and Xu, Zongben and others},
	journal={Advances in Neural Information Processing Systems},
	volume={29},
	year={2016}
}

@article{hammernik2018learning,
	title={Learning a variational network for reconstruction of accelerated {MRI} data},
	author={Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P and Sodickson, Daniel K and Pock, Thomas and Knoll, Florian},
	journal={Magnetic Resonance in Medicine},
	volume={79},
	number={6},
	pages={3055--3071},
	year={2018},
}

@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}

@article{liu2019roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}

@article{radford2018improving,
	title={Improving language understanding by generative pre-training},
	author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year={2018},
	note={Technical paper}
}

@inproceedings{carion2020end,
	title={End-to-end object detection with {Transformers}},
	author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	booktitle={European Conference on Computer Vision},
	pages={213--229},
	year={2020},
	organization={Springer}
}

@inproceedings{wang2021pyramid,
	title={Pyramid {Vision} {Transformer}: {A} versatile backbone for dense prediction without convolutions},
	author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={568--578},
	year={2021}
}

@article{cao2021video,
	title={Video super-resolution {Transformer}},
	author={Cao, Jiezhang and Li, Yawei and Zhang, Kai and Van Gool, Luc},
	journal={arXiv preprint arXiv:2106.06847},
	year={2021}
}

@inproceedings{chen2021pre,
	title={Pre-trained image processing {Transformer}},
	author={Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={12299--12310},
	year={2021}
}

@inproceedings{heo2021rethinking,
	title={Rethinking spatial dimensions of {Vision} {Transformers}},
	author={Heo, Byeongho and Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Choe, Junsuk and Oh, Seong Joon},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={11936--11945},
	year={2021}
}

@article{wang2021uformer,
	title={Uformer: A general {U-shaped} {Transformer} for image restoration},
	author={Wang, Zhendong and Cun, Xiaodong and Bao, Jianmin and Liu, Jianzhuang},
	journal={arXiv preprint arXiv:2106.03106},
	year={2021}
}

@article{zhou2021nnformer,
	title={{nnFormer}: {Interleaved} {Transformer} for Volumetric Segmentation},
	author={Zhou, Hong-Yu and Guo, Jiansen and Zhang, Yinghao and Yu, Lequan and Wang, Liansheng and Yu, Yizhou},
	journal={arXiv preprint arXiv:2109.03201},
	year={2021}
}

@article{huang2021missformer,
	title={MISSFormer: An effective medical image segmentation {Transformer}},
	author={Huang, Xiaohong and Deng, Zhifang and Li, Dandan and Yuan, Xueguang},
	journal={arXiv preprint arXiv:2109.07162},
	year={2021}
}

@article{wu2022d,
	title={{D-Former}: {A} {U-shaped} Dilated {Transformer} for {3D} Medical Image Segmentation},
	author={Wu, Yixuan and Liao, Kuanlun and Chen, Jintai and Chen, Danny Z and Wang, Jinhong and Gao, Honghao and Wu, Jian},
	journal={arXiv preprint arXiv:2201.00462},
	year={2022}
}

@inproceedings{wang2021ted,
	title={{TED-net}: {Convolution-free} {T2T} {Vision}{Transformer-based} Encoder-decoder Dilation network for Low-dose {CT} Denoising},
	author={Wang, Dayang and Wu, Zhan and Yu, Hengyong},
	booktitle={International Workshop on Machine Learning in Medical Imaging},
	pages={416--425},
	year={2021}
}

@article{luthra2021eformer,
	title={Eformer: Edge Enhancement based {Transformer} for Medical Image Denoising},
	author={Luthra, Achleshwar and Sulakhe, Harsh and Mittal, Tanish and Iyer, Abhishek and Yadav, Santosh},
	journal={arXiv preprint arXiv:2109.08044},
	year={2021}
}

@article{zhang2021spatial,
	title={Spatial adaptive and {Transformer} fusion network {(STFNet)} for low-count {PET} blind denoising with {MRI}},
	author={Zhang, Lipei and Xiao, Zizheng and Zhou, Chao and Yuan, Jianmin and He, Qiang and Yang, Yongfeng and Liu, Xin and Liang, Dong and Zheng, Hairong and Fan, Wei and others},
	journal={Medical Physics},
	year={2021}
}

@article{huang2022swin,
	title={Swin {Transformer} for Fast {MRI}},
	author={Huang, Jiahao and Fang, Yingying and Wu, Yinzhe and Wu, Huanjun and Gao, Zhifan and Li, Yang and Del Ser, Javier and Xia, Jun and Yang, Guang},
	journal={arXiv preprint arXiv:2201.03230},
	year={2022}
}

@inproceedings{feng2021task,
	title={Task {T}ransformer network for joint {MRI} reconstruction and super-resolution},
	author={Feng, Chun-Mei and Yan, Yunlu and Fu, Huazhu and Chen, Li and Xu, Yong},
	booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages={307--317},
	year={2021}
}

@article{uecker2014espirit,
	title={{ESPIRiT}—an eigenvalue approach to autocalibrating parallel {MRI}: where {SENSE} meets {GRAPPA}},
	author={Uecker, Martin and Lai, Peng and Murphy, Mark J and Virtue, Patrick and Elad, Michael and Pauly, John M and Vasanawala, Shreyas S and Lustig, Michael},
	journal={Magnetic Resonance in Medicine},
	volume={71},
	number={3},
	pages={990--1001},
	year={2014},
}

@misc{fastMRIleaderboard,
	author={URL},
	title = {fastMRI Public Leaderboard},
	howpublished = {\url{https://fastmri.org/leaderboards}},
	note = {Accessed: 02/24/2022}
}

@misc{website:Stanford2D,
	title = {{{Stanford 2D FSE}}},
	author={Cheng, Joseph Y.},
	howpublished = {http://mridata.org/list?project=Stanford 2D FSE},
}

@article{mridata:sawyer2013creation,
	title={Creation of fully sampled {MR} data repository for compressed sensing of the knee},
	author={Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas},
	year={2013},
	publisher={Citeseer}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{bai2019deep,
	title={Deep equilibrium models},
	author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}

@article{gilton2021deep,
	title={Deep equilibrium architectures for inverse problems in imaging},
	author={Gilton, Davis and Ongie, Gregory and Willett, Rebecca},
	journal={IEEE Transactions on Computational Imaging},
	volume={7},
	pages={1123--1133},
	year={2021},
	publisher={IEEE}
}

@inproceedings{
	lin2022vision,
	title={Vision {T}ransformers Enable Fast and Robust Accelerated {MRI}},
	author={Kang Lin and Reinhard Heckel},
	booktitle={Medical Imaging with Deep Learning},
	year={2022}
}

@article{korkmaz2022unsupervised,
	title={Unsupervised {MRI} reconstruction via zero-shot learned adversarial {T}ransformers},
	author={Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and {\"O}zbey, Muzaffer and Cukur, Tolga},
	journal={IEEE Transactions on Medical Imaging},
	year={2022},
	publisher={IEEE}
}

@inproceedings{wang2003multiscale,
	title={Multiscale structural similarity for image quality assessment},
	author={Wang, Zhou and Simoncelli, Eero P and Bovik, Alan C},
	booktitle={The Thrity-Seventh Asilomar Conference on Signals, Systems \& Computers, 2003},
	volume={2},
	pages={1398--1402},
	year={2003},
	organization={Ieee}
}

@article{klug2022scaling,
	title={Scaling Laws For Deep Learning Based Image Reconstruction},
	author={Klug, Tobit and Heckel, Reinhard},
	journal={arXiv preprint arXiv:2209.13435},
	year={2022}
}

@article{daras2022soft,
	title={Soft diffusion: Score matching for general corruptions},
	author={Daras, Giannis and Delbracio, Mauricio and Talebi, Hossein and Dimakis, Alexandros G and Milanfar, Peyman},
	journal={arXiv preprint arXiv:2209.05442},
	year={2022}
}

@article{vincent2011connection,
	title={A connection between score matching and denoising autoencoders},
	author={Vincent, Pascal},
	journal={Neural computation},
	volume={23},
	number={7},
	pages={1661--1674},
	year={2011},
	publisher={MIT Press}
}

@misc{noauthor_3d_nodate,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	url = {https://www.researchgate.net/publication/339908871_3D_Phaseless_Imaging_at_Nano-scale_Challenges_and_Possible_Solutions},
	note = {Publication Title: ResearchGate},
}

@article{agarwal_efficient_2020,
	title = {Efficient {Full}-{Matrix} {Adaptive} {Regularization}},
	journal = {arXiv:1806.02958 [cs, math, stat]},
	author = {Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
	year = {2020},
}

@article{kawar2022denoising,
	title={Denoising diffusion restoration models},
	author={Kawar, Bahjat and Elad, Michael and Ermon, Stefano and Song, Jiaming},
	journal={arXiv preprint arXiv:2201.11793},
	year={2022}
}

@inproceedings{song2021denoising,
	title={Denoising Diffusion Implicit Models},
	author={Jiaming Song and Chenlin Meng and Stefano Ermon},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=St1giarCHLP}
}


@article{aggarwal_modl_2018,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	number = {2},
	journal = {IEEE transactions on medical imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {394--405},
}

@article{akiyama_first_2019,
	title = {First {M87} {Event} {Horizon} {Telescope} {Results}. {IV}. {Imaging} the {Central} {Supermassive} {Black} {Hole}},
	journal = {The Astrophysical Journal Letters},
	author = {Akiyama, Kazunori},
	year = {2019},
	pages = {52},
}

@article{anil_second_nodate,
	title = {Second {Order} {Optimization} {Made} {Practical}},
	author = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
	pages = {19},
}

@article{ardizzone_analyzing_2019,
	title = {Analyzing {Inverse} {Problems} with {Invertible} {Neural} {Networks}},
	journal = {arXiv:1808.04730 [cs, stat]},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	year = {2019},
}

@article{arora_fine-grained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	journal = {arXiv:1901.08584 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	year = {2019},
}

@article{arpit_benefits_2019,
	title = {The {Benefits} of {Over}-{Parameterization} at {Initialization} in {Deep} {ReLU} {Networks}},
	journal = {arXiv:1901.03611 [cs, stat]},
	author = {Arpit, Devansh and Bengio, Yoshua},
	year = {2019},
}

@article{aslan_distributed_2020,
	title = {Distributed {Optimization} with {Tunable} {Learned} {Priors} for {Robust} {Ptycho}-{Tomography}},
	journal = {arXiv:2009.09498 [eess, math]},
	author = {Aslan, Selin and Liu, Zhengchun and Nikitin, Viktor and Bicer, Tekin and Leyffer, Sven and Gursoy, Doga},
	year = {2020},
}

@article{ba_distributed_2016,
	title = {Distributed {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximations}},
	author = {Ba, Jimmy and Grosse, Roger and Martens, James},
	year = {2016},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year = {2016},
}

@article{barbastathis_use_2019,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barbastathis_use_2019-1,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barutcu_limited-angle_2021,
	title = {Limited-{Angle} {Computed} {Tomography} with {Deep} {Image} and {Physics} {Priors}},
	volume = {11},
	number = {1},
	journal = {Scientific Reports},
	author = {Barutcu, Semih and Aslan, Selin and Katsaggelos, Aggelos K. and Gürsoy, Doğa},
	year = {2021},
	pages = {17740},
}

@article{beatty_rapid_2005,
	title = {Rapid {Gridding} {Reconstruction} with a {Minimal} {Oversampling} {Ratio}},
	volume = {24},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Beatty, P.J. and Nishimura, D.G. and Pauly, J.M.},
	year = {2005},
	pages = {799--808},
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-{Depth} {Concurrency} {Analysis}},
	volume = {52},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	year = {2019},
	pages = {1--43},
}

@article{bernstein_distance_2020,
	title = {On the {Distance} between {Two} {Neural} {Networks} and the {Stability} of {Learning}},
	journal = {arXiv:2002.03432 [cs, math, stat]},
	author = {Bernstein, Jeremy and Vahdat, Arash and Yue, Yisong and Liu, Ming-Yu},
	year = {2020},
}

@book{bertero_introduction_1998,
	address = {Bristol, UK ; Philadelphia, Pa},
	title = {Introduction to {Inverse} {Problems} in {Imaging}},
	isbn = {978-0-7503-0439-9 978-0-7503-0435-1},
	publisher = {Institute of Physics Pub},
	author = {Bertero, Mario and Boccacci, Patrizia},
	year = {1998},
	lccn = {TA1637 .B47 1998},
}

@book{bertsekas_introduction_2002,
	address = {Belmont, Mass},
	edition = {2. print},
	series = {Optimization and {Computation} {Series}},
	title = {Introduction to {Probability}},
	isbn = {978-1-886529-40-3},
	number = {1},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	year = {2002},
}

@book{bhatia_matrix_1997,
	address = {New York},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Matrix {Analysis}},
	isbn = {978-0-387-94846-1},
	number = {169},
	publisher = {Springer},
	author = {Bhatia, Rajendra},
	year = {1997},
	lccn = {QA188 .B485 1997},
}

@article{bora_ambientgan_2018,
	title = {{AMBIENTGAN}: {GENERATIVE} {MODELS} {FROM} {LOSSY} {MEASUREMENTS}},
	author = {Bora, Ashish and Dimakis, Alexandros G},
	year = {2018},
	pages = {22},
}

@article{bora_compressed_nodate,
	title = {Compressed {Sensing} {Using} {Generative} {Models}},
	author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alex},
	pages = {10},
}

@article{bostan_deep_2020,
	title = {Deep {Phase} {Decoder}: {Self}-{Calibrating} {Phase} {Microscopy} with an {Untrained} {Deep} {Neural} {Network}},
	journal = {arXiv:2001.09803 [physics]},
	author = {Bostan, Emrah and Heckel, Reinhard and Chen, Michael and Kellman, Michael and Waller, Laura},
	year = {2020},
}

@article{bottou_optimization_2018,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	volume = {60},
	number = {2},
	journal = {SIAM Review},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	year = {2018},
	pages = {223--311},
}

@book{bovik_essential_2009,
	address = {London ; Boston},
	title = {The {Essential} {Guide} to {Image} {Processing}},
	isbn = {978-0-12-374457-9},
	publisher = {Academic Press},
	author = {Bovik, Alan C.},
	year = {2009},
	lccn = {TA1637 .B68 2009},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex {Optimization}},
	isbn = {978-0-521-83378-3},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	lccn = {QA402.5 .B69 2004},
}

@article{boyd_neal_nodate,
	title = {Neal {Parikh} {Department} of {Computer} {Science} {Stanford} {University}},
	author = {Boyd, Stephen},
	pages = {113},
}

@article{brock_high-performance_2021,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	year = {2021},
}

@article{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	year = {2019},
}

@article{bronstein_geometric_2017,
	title = {Geometric {Deep} {Learning}: {Going} beyond {Euclidean} {Data}},
	volume = {34},
	number = {4},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	year = {2017},
	pages = {18--42},
}

@article{burdakov_efficiently_2017,
	title = {On {Efficiently} {Combining} {Limited}-{Memory} and {Trust}-{Region} {Techniques}},
	volume = {9},
	number = {1},
	journal = {Mathematical Programming Computation},
	author = {Burdakov, Oleg and Gong, Lujin and Zikrin, Spartak and Yuan, Ya-xiang},
	year = {2017},
	pages = {101--134},
}

@article{candes_phase_2015,
	title = {Phase {Retrieval} from {Coded} {Diffraction} {Patterns}},
	volume = {39},
	number = {2},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Candès, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {277--299},
}

@article{candes_phase_2015-1,
	title = {Phase {Retrieval} via {Wirtinger} {Flow}: {Theory} and {Algorithms}},
	volume = {61},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Candes, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {1985--2007},
}

@article{candes_stable_2006,
	title = {Stable {signal} {recovery} from {incomplete} and {inaccurate} {measurements}},
	volume = {59},
	number = {8},
	journal = {Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
	author = {Candes, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
	year = {2006},
	pages = {1207--1223},
}

@article{noauthor_chapter_nodate,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@article{noauthor_chapter_nodate-1,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@inproceedings{chartsias_adversarial_2017,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-1,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-2,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-3,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year = {2020},
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	year = {2014},
}

@article{cohen_certified_2019,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	journal = {arXiv:1902.02918 [cs, stat]},
	author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
	year = {2019},
}

@book{noauthor_introduction_2009,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to {Algorithms}},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	year = {2009},
	lccn = {QA76.6 .C662 2009},
}

@article{cranmer_lagrangian_2020,
	title = {Lagrangian {Neural} {Networks}},
	journal = {arXiv:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	year = {2020},
}

@inproceedings{cubuk_autoaugment_2019,
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2019},
	pages = {113--123},
}

@article{curtis_trust_2017,
	title = {A {Trust} {Region} {Algorithm} with a {Worst}-{Case} {Iteration} {Complexity} of \$\${\textbackslash}textbackslash mathcal\{{\textbackslash}vphantom\}{O}{\textbackslash}vphantom\{\}({\textbackslash}textbackslash epsilon {\textasciicircum}\{-3/2\})\$\$ {O} ( ϵ - 3 / 2 ) for {Nonconvex} {Optimization}},
	volume = {162},
	number = {1-2},
	journal = {Mathematical Programming},
	author = {Curtis, Frank E. and Robinson, Daniel P. and Samadi, Mohammadreza},
	year = {2017},
	pages = {1--32},
}

@book{daubechies_ten_1992,
	address = {Philadelphia, Pa},
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Ten {Lectures} on {Wavelets}},
	isbn = {978-0-89871-274-2},
	number = {61},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Daubechies, Ingrid},
	year = {1992},
	lccn = {QA403.3 .D38 1992},
}

@article{davison_singular_1981,
	title = {A {Singular} {Value} {Decomposition} for the {Radon} {Transform} in {\textbackslash}emphn -{Dimensional} {Euclidean} {Space}},
	volume = {3},
	number = {3},
	journal = {Numerical Functional Analysis and Optimization},
	author = {Davison, M. E.},
	year = {1981},
	pages = {321--340},
}

@article{dean_large_nodate,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
	pages = {9},
}

@article{defazio_mri_2020,
	title = {{MRI} {Banding} {Removal} via {Adversarial} {Training}},
	journal = {arXiv:2001.08699 [cs, eess, stat]},
	author = {Defazio, Aaron and Murrell, Tullie and Recht, Michael P.},
	year = {2020},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-{Training} of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
}

@article{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	journal = {arXiv preprint arXiv:2105.05233},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	year = {2021},
}

@article{dierolf_ptychographic_nodate,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	author = {Dierolf, Martin},
	pages = {6},
}

@article{dierolf_ptychographic_2010,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	volume = {467},
	number = {7314},
	journal = {Nature},
	author = {Dierolf, Martin and Menzel, Andreas and Thibault, Pierre and Schneider, Philipp and Kewish, Cameron M. and Wepf, Roger and Bunk, Oliver and Pfeiffer, Franz},
	year = {2010},
	pages = {436--439},
}

@article{dinh_nice_2015,
	title = {{NICE}: {Non}-{Linear} {Independent} {Components} {Estimation}},
	journal = {arXiv:1410.8516 [cs]},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	year = {2015},
}

@article{domingos_few_2012,
	title = {A {Few} {Useful} {Things} to {Know} about {Machine} {Learning}},
	volume = {55},
	number = {10},
	journal = {Communications of the ACM},
	author = {Domingos, Pedro},
	year = {2012},
	pages = {78--87},
}

@article{donoho_compressed_2006,
	title = {Compressed {sensing}},
	volume = {52},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Donoho, David L.},
	year = {2006},
	pages = {1289--1306},
}

@article{du_gradient_2019,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1811.03804 [cs, math, stat]},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	year = {2019},
}

@article{dumoulin_guide_nodate,
	title = {A {Guide} to {Convolution} {Arithmetic} for {Deep} {Learning}},
	author = {Dumoulin, Vincent and Visin, Francesco},
	pages = {31},
}

@article{durrett_probability_nodate,
	title = {Probability: {Theory} and {Examples}},
	author = {Durrett, Rick},
	pages = {386},
}

@article{fabian_3d_2020,
	title = {{3D} {Phase} {Retrieval} at {Nano}-{Scale} via {Accelerated} {Wirtinger} {Flow}},
	journal = {arXiv preprint arXiv:2002.11785},
	author = {Fabian, Zalan and Haldar, Justin and Leahy, Richard and Soltanolkotabi, Mahdi},
	year = {2020},
}

@misc{noauthor_fastmri_nodate,
	title = {{FastMRI}},
	url = {https://fastmri.org/leaderboards/},
}

@article{feng_golden-angle_2014,
	title = {Golden-{Angle} {Radial} {Sparse} {Parallel} {MRI}: {Combination} of {Compressed} {Sensing}, {Parallel} {Imaging}, and {Golden}-{Angle} {Radial} {Sampling} for {Fast} and {Flexible} {Dynamic} {Volumetric} {MRI}: {iGRASP}: {Iterative} {Golden}-{Angle} {RAdial} {Sparse} {Parallel} {MRI}},
	volume = {72},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Feng, Li and Grimm, Robert and Block, Kai Tobias and Chandarana, Hersh and Kim, Sungheon and Xu, Jian and Axel, Leon and Sodickson, Daniel K. and Otazo, Ricardo},
	year = {2014},
	pages = {707--717},
}

@article{foret_sharpness-aware_2021,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	journal = {arXiv:2010.01412 [cs, stat]},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	year = {2021},
}

@inproceedings{gatys_image_2016,
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	year = {2016},
	pages = {2414--2423},
}

@article{george_fast_2018,
	title = {Fast {Approximate} {Natural} {Gradient} {Descent} in a {Kronecker}-{Factored} {Eigenbasis}},
	journal = {arXiv:1806.03884 [cs, stat]},
	author = {George, Thomas and Laurent, César and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
	year = {2018},
}

@article{gilton_neumann_2019,
	title = {Neumann {Networks} for {Linear} {Inverse} {Problems} in {Imaging}},
	volume = {6},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gilton, Davis and Ongie, Greg and Willett, Rebecca},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {328--343},
}

@article{glorot_understanding_nodate,
	title = {Understanding the {Difficulty} of {Training} {Deep} {Feedforward} {Neural} {Networks}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
}

@article{goldfarb_practical_2020,
	title = {Practical {Quasi}-{Newton} {Methods} for {Training} {Deep} {Neural} {Networks}},
	journal = {arXiv:2006.08877 [cs, math, stat]},
	author = {Goldfarb, Donald and Ren, Yi and Bahamou, Achraf},
	year = {2020},
}

@article{gomez_reversible_nodate,
	title = {The {Reversible} {Residual} {Network}: {Backpropagation} {Without} {Storing} {Activations}},
	author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
	pages = {11},
}

@article{goyal_accurate_2018,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2018},
}

@article{goyal_self-supervised_2021,
	title = {Self-{Supervised} {Pretraining} of {Visual} {Features} in the {Wild}},
	journal = {arXiv:2103.01988 [cs]},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
	year = {2021},
}

@inproceedings{gregor_learning_2010,
	title = {Learning {Fast} {Approximations} of {Sparse} {Coding}},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	author = {Gregor, Karol and LeCun, Yann},
	year = {2010},
	pages = {399--406},
}

@article{grosse_kronecker-factored_nodate,
	title = {A {Kronecker}-{Factored} {Approximate} {Fisher} {Matrix} for {Convolution} {Layers}},
	author = {Grosse, Roger and Martens, James},
	pages = {10},
}

@article{gubner_probability_nodate,
	title = {{PROBABILITY} {AND} {RANDOM} {PROCESSES} {FOR} {ELECTRICAL} {AND} {COMPUTER} {ENGINEERS}},
	author = {Gubner, John A},
	pages = {642},
}

@article{guizar-sicairos_phase_2011,
	title = {Phase {Tomography} from {X}-{Ray} {Coherent} {Diffractive} {Imaging} {Projections}},
	volume = {19},
	number = {22},
	journal = {Optics Express},
	author = {Guizar-Sicairos, Manuel and Diaz, Ana and Holler, Mirko and Lucas, Miriam S. and Menzel, Andreas and Wepf, Roger A. and Bunk, Oliver},
	year = {2011},
	pages = {21345},
}

@article{guo_direct_2017,
	title = {Direct {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps} from {Highly} {Undersampled} {Brain} {Dynamic} {Contrast} {Enhanced} {MRI}: {Direct} {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps}},
	volume = {78},
	number = {4},
	journal = {Magnetic Resonance in Medicine},
	author = {Guo, Yi and Lingala, Sajan Goud and Zhu, Yinghua and Lebel, R. Marc and Nayak, Krishna S.},
	year = {2017},
	pages = {1566--1578},
}

@inproceedings{gupta_shampoo_2018,
	title = {Shampoo: {Preconditioned} {Stochastic} {Tensor} {Optimization}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gupta, Vineet and Koren, Tomer and Singer, Yoram},
	year = {2018},
	pages = {1842--1850},
}

@article{haghighi_transferable_2021,
	title = {Transferable {Visual} {Words}: {Exploiting} the {Semantics} of {Anatomical} {Patterns} for {Self}-{Supervised} {Learning}},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Haghighi, Fatemeh and Taher, Mohammad Reza Hosseinzadeh and Zhou, Zongwei and Gotway, Michael B. and Liang, Jianming},
	year = {2021},
	note = {Publisher: IEEE},
}

@book{hajek_random_2015,
	edition = {First},
	title = {Random {Processes} for {Engineers}},
	isbn = {978-1-107-10012-1 978-1-316-16460-0},
	publisher = {Cambridge University Press},
	author = {Hajek, Bruce},
	year = {2015},
}

@article{halko_finding_2010,
	title = {Finding {Structure} with {Randomness}: {Probabilistic} {Algorithms} for {Constructing} {Approximate} {Matrix} {Decompositions}},
	journal = {arXiv:0909.4061 [math]},
	author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
	year = {2010},
}

@article{hand_phase_2018,
	title = {Phase {Retrieval} {Under} a {Generative} {Prior}},
	journal = {arXiv:1807.04261 [cs, math]},
	author = {Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
	year = {2018},
}

@article{heckel_deep_nodate,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-{Convolutional} {Networks}},
	author = {Heckel, Reinhard and Hand, Paul},
	pages = {17},
}

@article{heckel_regularizing_2019,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{heckel_regularizing_2019-1,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{he_deep_2015,
	title = {Deep {residual} {learning} for {image} {recognition}},
	journal = {arXiv:1512.03385},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
}

@article{he_distributed_2017,
	title = {Distributed {Hessian}-{Free} {Optimization} for {Deep} {Neural} {Network}},
	journal = {arXiv:1606.00511 [cs, math]},
	author = {He, Xi and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Takáč, Martin},
	year = {2017},
}

@article{ho_cascaded_nodate,
	title = {Cascaded {Diffusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	pages = {28},
}

@article{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2006.11239},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
}

@article{holler_high-resolution_2017,
	title = {High-{Resolution} {Non}-{Destructive} {Three}-{Dimensional} {Imaging} of {Integrated} {Circuits}},
	volume = {543},
	number = {7645},
	journal = {Nature},
	author = {Holler, Mirko and Guizar-Sicairos, Manuel and Tsai, Esther H. R. and Dinapoli, Roberto and Müller, Elisabeth and Bunk, Oliver and Raabe, Jörg and Aeppli, Gabriel},
	year = {2017},
	pages = {402--406},
}

@article{holler_omnytomography_2018,
	title = {{OMNY}—{A} {tOMography} {Nano} {crYo} {Stage}},
	journal = {Rev. Sci. Instrum.},
	author = {Holler, M and Raabe, J and Diaz, A and Guizar-Sicairos, M and Wepf, R and Odstrcil, M and Shaik, F R and Panneels, V and Menzel, A and Sarafimov, B and Maag, S and Wang, X and Thominet, V and Walther, H and Lachat, T and Vitins, M and Bunk, O},
	year = {2018},
	pages = {14},
}

@book{horn_matrix_2012,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Matrix {Analysis}},
	isbn = {978-0-521-83940-2},
	publisher = {Cambridge University Press},
	author = {Horn, Roger A. and Johnson, Charles R.},
	year = {2012},
	lccn = {QA188 .H66 2012},
}

@inproceedings{huang_densely_2017,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {2261--2269},
}

@article{huang_provably_2018,
	title = {A {Provably} {Convergent} {Scheme} for {Compressive} {Sensing} under {Random} {Generative} {Priors}},
	journal = {arXiv:1812.04176 [math]},
	author = {Huang, Wen and Hand, Paul and Heckel, Reinhard and Voroninski, Vladislav},
	year = {2018},
}

@article{hussain_differential_nodate,
	title = {Differential {Data} {Augmentation} {Techniques} for {Medical} {Imaging} {Classification} {Tasks}},
	author = {Hussain, Zeshan and Gimenez, Francisco and Yi, Darvin and Rubin, Daniel},
	pages = {6},
}

@article{song2020score,
	title={Score-based generative modeling through stochastic differential equations},
	author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	journal={arXiv preprint arXiv:2011.13456},
	year={2020}
}

@article{anderson1982reverse,
	title={Reverse-time diffusion equation models},
	author={Anderson, Brian DO},
	journal={Stochastic Processes and their Applications},
	volume={12},
	number={3},
	pages={313--326},
	year={1982},
	publisher={Elsevier}
}


@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
}

@article{jacobsen_i-revnet_2018,
	title = {I-{RevNet}: {Deep} {Invertible} {Networks}},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	year = {2018},
}

@article{jalal_robust_2021,
	title = {Robust {Compressed} {Sensing} {MRI} with {Deep} {Generative} {Priors}},
	journal = {arXiv:2108.01368 [cs, math, stat]},
	author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G. and Tamir, Jonathan I.},
	year = {2021},
}

@article{jiang_linear_nodate,
	title = {A {Linear} {Speedup} {Analysis} of {Distributed} {Deep} {Learning} with {Sparse} and {Quantized} {Communication}},
	author = {Jiang, Peng and Agrawal, Gagan},
	pages = {12},
}

@article{jin_time-dependent_2019,
	title = {Time-{Dependent} {Deep} {Image} {Prior} for {Dynamic} {MRI}},
	journal = {arXiv:1910.01684 [cs, eess]},
	author = {Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jerome and Stuber, Matthias and Unser, Michael},
	year = {2019},
}

@inproceedings{jnawali_deep_2018,
	title = {Deep {3D} {Convolution} {Neural} {Network} for {CT} {Brain} {Hemorrhage} {Classification}},
	volume = {10575},
	booktitle = {Medical {Imaging} 2018: {Computer}-{Aided} {Diagnosis}},
	publisher = {International Society for Optics and Photonics},
	author = {Jnawali, Kamal and Arbabshirani, Mohammad R. and Rao, Navalgund and Patel, Alpen A.},
	year = {2018},
	pages = {105751C},
}

@article{chung2022score,
	title={Score-based diffusion models for accelerated MRI},
	author={Chung, Hyungjin and Ye, Jong Chul},
	journal={Medical Image Analysis},
	volume={80},
	pages={102479},
	year={2022},
	publisher={Elsevier}
}

@inproceedings{chung2022come,
	title={Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction},
	author={Chung, Hyungjin and Sim, Byeongsu and Ye, Jong Chul},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={12413--12422},
	year={2022}
}

@article{song2021solving,
	title={Solving inverse problems in medical imaging with score-based generative models},
	author={Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
	journal={arXiv preprint arXiv:2111.08005},
	year={2021}
}

@article{chung2022improving,
	title={Improving diffusion models for inverse problems using manifold constraints},
	author={Chung, Hyungjin and Sim, Byeongsu and Ryu, Dohoon and Ye, Jong Chul},
	journal={arXiv preprint arXiv:2206.00941},
	year={2022}
}

@article{kawar2022jpeg,
	title={Jpeg artifact correction using denoising diffusion restoration models},
	author={Kawar, Bahjat and Song, Jiaming and Ermon, Stefano and Elad, Michael},
	journal={arXiv preprint arXiv:2209.11888},
	year={2022}
}

@inproceedings{ledig2017photo,
	title={Photo-realistic single image super-resolution using a generative adversarial network},
	author={Ledig, Christian and Theis, Lucas and Husz{\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4681--4690},
	year={2017}
}

@inproceedings{kupyn2018deblurgan,
	title={Deblurgan: Blind motion deblurring using conditional adversarial networks},
	author={Kupyn, Orest and Budzan, Volodymyr and Mykhailych, Mykola and Mishkin, Dmytro and Matas, Ji{\v{r}}{\'\i}},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={8183--8192},
	year={2018}
}

@article{wang2018image,
	title={Image inpainting via generative multi-column convolutional neural networks},
	author={Wang, Yi and Tao, Xin and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

@article{fabian2023diracdiffusion,
	title={DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency},
	author={Fabian, Zalan and Tinaz, Berk and Soltanolkotabi, Mahdi},
	journal={arXiv preprint arXiv:2303.14353},
	year={2023}
}

@article{kingma2013auto,
	title={Auto-encoding variational bayes},
	author={Kingma, Diederik P and Welling, Max},
	journal={arXiv preprint arXiv:1312.6114},
	year={2013}
}

@article{razavi2019generating,
	title={Generating diverse high-fidelity images with vq-vae-2},
	author={Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

@article{bengio2013representation,
	title={Representation learning: A review and new perspectives},
	author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={35},
	number={8},
	pages={1798--1828},
	year={2013},
	publisher={IEEE}
}


@article{ardila2019end,
	title={End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography},
	author={Ardila, Diego and Kiraly, Atilla P and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and others},
	journal={Nature medicine},
	volume={25},
	number={6},
	pages={954--961},
	year={2019},
	publisher={Nature Publishing Group US New York}
}

@article{hand2018phase,
	title={Phase retrieval under a generative prior},
	author={Hand, Paul and Leong, Oscar and Voroninski, Vlad},
	journal={Advances in Neural Information Processing Systems},
	volume={31},
	year={2018}
}

@article{feng2023score,
	title={Score-Based Diffusion Models as Principled Priors for Inverse Imaging},
	author={Feng, Berthy T and Smith, Jamie and Rubinstein, Michael and Chang, Huiwen and Bouman, Katherine L and Freeman, William T},
	journal={arXiv preprint arXiv:2304.11751},
	year={2023}
}

@article{bai2019deep,
	title={Deep equilibrium models},
	author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}

@inproceedings{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	year = {2016},
	pages = {694--711},
}

@article{kahnt_coupled_nodate,
	title = {Coupled {Ptychography} and {Tomography} {Algorithm} {Improves} {Reconstruction} of {Experimental} {Data}},
	author = {Kahnt, Maik and Becher, Johannes and Brückner, Dennis and Fam, Yakub and Sheppard, Thomas and Weissenberger, Tobias and Wittwer, Felix and Grunwaldt, Jan-Dierk and Schwieger, Wilhelm and Schroer, Christian G},
	pages = {8},
}

@article{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@article{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
}

@inproceedings{karras2019style,
	title={A style-based generator architecture for generative adversarial networks},
	author={Karras, Tero and Laine, Samuli and Aila, Timo},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={4401--4410},
	year={2019}
}

@article{heusel2017gans,
	title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
	author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@inproceedings{zhang2018unreasonable,
	title={The unreasonable effectiveness of deep features as a perceptual metric},
	author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={586--595},
	year={2018}
}


@article{karras_training_2020,
	title = {Training {Generative} {Adversarial} {Networks} with {Limited} {Data}},
	journal = {arXiv:2006.06676 [cs, stat]},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@inproceedings{keuper_distributed_2016,
	address = {Salt Lake City, UT, USA},
	title = {Distributed {Training} of {Deep} {Neural} {Networks}: {Theoretical} and {Practical} {Limits} of {Parallel} {Scalability}},
	isbn = {978-1-5090-3882-4},
	booktitle = {2016 2nd {Workshop} on {Machine} {Learning} in {HPC} {Environments} ({MLHPC})},
	publisher = {IEEE},
	author = {Keuper, Janis and Preundt, Franz-Josef},
	year = {2016},
	pages = {19--26},
}

@article{kingma_glow_nodate,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{kingma_glow_nodate-1,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{knoll_assessment_2019,
	title = {Assessment of the {Generalization} of {Learned} {Image} {Reconstruction} and the {Potential} for {Transfer} {Learning}},
	volume = {81},
	number = {1},
	journal = {Magnetic Resonance in Medicine},
	author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
	year = {2019},
	pages = {116--128},
}

@article{kobler_total_2020,
	title = {Total {Deep} {Variation} for {Linear} {Inverse} {Problems}},
	journal = {arXiv:2001.05005 [cs, math]},
	author = {Kobler, Erich and Effland, Alexander and Kunisch, Karl and Pock, Thomas},
	year = {2020},
}

@article{kobyzev_normalizing_2020,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, I. and Prince, S. and Brubaker, M.},
	year = {2020},
	pages = {1--1},
}

@article{korkmaz_unsupervised_2021,
	title = {Unsupervised {MRI} {Reconstruction} via {Zero}-{Shot} {Learned} {Adversarial} {Transformers}},
	journal = {arXiv:2105.08059 [cs, eess]},
	author = {Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and Özbey, Muzaffer and Çukur, Tolga},
	year = {2021},
}

@article{kreutz_complex_nodate,
	title = {The {Complex} {Gradient} {Operator} and the {CR}-{Calculus}},
	author = {Kreutz, Kenneth},
	pages = {74},
}

@article{krishnan_neumann_2017,
	title = {Neumann {Optimizer}: {A} {Practical} {Optimization} {Algorithm} for {Deep} {Neural} {Networks}},
	journal = {arXiv:1712.03298 [cs, stat]},
	author = {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A.},
	year = {2017},
}

@article{kuo_featmatch_2020,
	title = {{FeatMatch}: {Feature}-{Based} {Augmentation} for {Semi}-{Supervised} {Learning}},
	journal = {arXiv preprint arXiv:2007.08505},
	author = {Kuo, Chia-Wen and Ma, Chih-Yao and Huang, Jia-Bin and Kira, Zsolt},
	year = {2020},
}

@article{kwon_asam_2021,
	title = {{ASAM}: {Adaptive} {Sharpness}-{Aware} {Minimization} for {Scale}-{Invariant} {Learning} of {Deep} {Neural} {Networks}},
	journal = {arXiv:2102.11600 [cs, stat]},
	author = {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
	year = {2021},
}

@article{lauro_multiple_2020,
	title = {Multiple {Subglacial} {Water} {Bodies} below the {South} {Pole} of {Mars} {Unveiled} by {New} {MARSIS} {Data}},
	journal = {Nature Astronomy},
	author = {Lauro, Sebastian Emanuel and Pettinelli, Elena and Caprarelli, Graziella and Guallini, Luca and Rossi, Angelo Pio and Mattei, Elisabetta and Cosciotti, Barbara and Cicchetti, Andrea and Soldovieri, Francesco and Cartacci, Marco and Di Paolo, Federico and Noschese, Raffaella and Orosei, Roberto},
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
}

@article{lee_deep_2018,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	journal = {arXiv:1711.00165 [cs, stat]},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	year = {2018},
}

@article{lemley_smart_2017,
	title = {Smart {Augmentation} {Learning} an {Optimal} {Data} {Augmentation} {Strategy}},
	volume = {5},
	journal = {IEEE Access},
	author = {Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
	year = {2017},
	pages = {5858--5869},
}

@article{liang_swinir_2021,
	title = {{SwinIR}: {Image} {restoration} {using} {Swin} {Transformer}},
	journal = {arXiv:2108.10257},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
}

@article{li_communication_nodate,
	title = {Communication {Efficient} {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	author = {Li, Mu and Andersen, David G and Smola, Alexander and Yu, Kai},
	pages = {9},
}

@article{li_fourier_2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	journal = {arXiv:2010.08895 [cs, math]},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
}

@article{li_gan_2020,
	title = {{GAN} {Compression}: {Efficient} {Architectures} for {Interactive} {Conditional} {GANs}},
	journal = {arXiv:2003.08936 [cs, eess]},
	author = {Li, Muyang and Lin, Ji and Ding, Yaoyao and Liu, Zhijian and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{li_sacnn_2020,
	title = {{SACNN}: {Self}-{Attention} {Convolutional} {Neural} {Network} for {Low}-{Dose} {CT} {Denoising} with {Self}-{Supervised} {Perceptual} {Loss} {Network}},
	volume = {39},
	number = {7},
	journal = {IEEE transactions on medical imaging},
	author = {Li, Meng and Hsu, William and Xie, Xiaodong and Cong, Jason and Gao, Wen},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {2289--2301},
}

@article{liu_coupled_nodate,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@article{liu_coupled_nodate-1,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@incollection{liu_image_2018,
	address = {Cham},
	title = {Image {Inpainting} for {Irregular} {Holes} {Using} {Partial} {Convolutions}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
	year = {2018},
	pages = {89--105},
}

@article{liu_spark-based_2016,
	title = {Spark-{Based} {Large}-{Scale} {Matrix} {Inversion} for {Big} {Data} {Processing}},
	volume = {4},
	journal = {IEEE Access},
	author = {Liu, Jun and Liang, Yang and Ansari, Nirwan},
	year = {2016},
	pages = {2166--2176},
}

@article{liu_unsupervised_nodate,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{liu_unsupervised_nodate-1,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{long_deep_nodate,
	title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
	pages = {10},
}

@article{long_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
}

@article{lustig_compressed_2008,
	title = {Compressed {sensing} {MRI}},
	volume = {25},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Lustig, Michael and Donoho, David L. and Santos, Juan M. and Pauly, John M.},
	year = {2008},
	pages = {72--82},
}

@article{lutter_deep_2019,
	title = {Deep {Lagrangian} {Networks}: {Using} {Physics} as {Model} {Prior} for {Deep} {Learning}},
	journal = {arXiv:1907.04490 [cs, eess, stat]},
	author = {Lutter, Michael and Ritter, Christian and Peters, Jan},
	year = {2019},
}

@article{maiden_improved_2009,
	title = {An {Improved} {Ptychographical} {Phase} {Retrieval} {Algorithm} for {Diffractive} {Imaging}},
	volume = {109},
	number = {10},
	journal = {Ultramicroscopy},
	author = {Maiden, Andrew M. and Rodenburg, John M.},
	year = {2009},
	pages = {1256--1262},
}

@article{maiden_ptychographic_nodate,
	title = {Ptychographic {Transmission} {Microscopy} in {Three} {Dimensions} {Using} a {Multi}-{Slice} {Approach}},
	author = {Maiden, A M and Humphry, M J and Rodenburg, J M},
	pages = {9},
}

@article{ma_inefficiency_2019,
	title = {Inefficiency of {K}-{FAC} for {Large} {Batch} {Size} {Training}},
	journal = {arXiv:1903.06237 [cs, stat]},
	author = {Ma, Linjian and Montague, Gabe and Ye, Jiayu and Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2019},
}

@article{mallat_theory_nodate,
	title = {A {Theory} for {Multiresolution} {Signal} {Decomposition}: {The} {Wavelet} {Representation}},
	author = {Mallat, Stephane G},
	pages = {31},
}

@article{mardani_deep_2017,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{mardani_deep_2017-1,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{martens_deep_nodate,
	title = {Deep {Learning} via {Hessian}-{Free} {Optimization}},
	author = {Martens, James},
	pages = {8},
}

@article{martens_new_2014,
	title = {New {Insights} and {Perspectives} on the {Natural} {Gradient} {Method}},
	journal = {arXiv preprint arXiv:1412.1193},
	author = {Martens, James},
	year = {2014},
}

@article{martens_optimizing_nodate,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-{Factored} {Approximate} {Curvature}},
	author = {Martens, James and Grosse, Roger},
	pages = {10},
}

@article{martinsson_randomized_2011,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {47--68},
}

@article{martinsson_randomized_2011-1,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{martinsson_randomized_2011-2,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{menon_pulse_2020,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{menon_pulse_2020-1,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{metzler_prdeep_nodate,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with {Flexible} {Deep} {Neural} {Networks}},
	author = {Metzler, Christopher A and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G},
	pages = {10},
}

@article{metzler_prdeep_2018,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with a {Flexible} {Deep} {Network}},
	journal = {arXiv:1803.00212 [cs, stat]},
	author = {Metzler, Christopher A. and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G.},
	year = {2018},
}

@book{meyer_matrix_2000,
	address = {Philadelphia},
	title = {Matrix {Analysis} and {Applied} {Linear} {Algebra}},
	isbn = {978-0-89871-454-8},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Meyer, C. D.},
	year = {2000},
	lccn = {QA188 .M495 2000},
}

@misc{noauthor_models_nodate,
	title = {Models {Genesis} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1361841520302048?token=B6B4930C1609C04FD49C6498E7935D83275465F07F9155D2DA9C09FBACAA0F60708219F8E62F34E2DB5319E5BC0CA8E0&originRegion=us-east-1&originCreation=20210914005025},
}

@article{moulin_analysis_1999,
	title = {Analysis of {Multiresolution} {Image} {Denoising} {Schemes} {Using} {Generalized} {Gaussian} and {Complexity} {Priors}},
	volume = {45},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Moulin, P. and {Juan Liu}},
	year = {1999},
	pages = {909--919},
}

@article{myagotin_efficient_2013,
	title = {Efficient {Volume} {Reconstruction} for {Parallel}-{Beam} {Computed} {Laminography} by {Filtered} {Backprojection} on {Multi}-{Core} {Clusters}},
	volume = {22},
	number = {12},
	journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
	author = {Myagotin, Anton and Voropaev, Alexey and Helfen, Lukas and Hänschke, Daniel and Baumbach, Tilo},
	year = {2013},
	pages = {14},
}

@article{nagle-mcnaughton_planet_2020,
	title = {{PlaNet}: {A} {Neural} {Network} for {Detecting} {Transverse} {Aeolian} {Ridges} on {Mars}},
	volume = {12},
	number = {21},
	journal = {Remote Sensing},
	author = {Nagle-McNaughton, Timothy and McClanahan, Timothy and Scuderi, Louis},
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {3607},
}

@inproceedings{narayanan_pipedream_2019,
	address = {Huntsville Ontario Canada},
	title = {{PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}},
	isbn = {978-1-4503-6873-5},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	year = {2019},
	pages = {1--15},
}

@article{neyshabur_towards_2018,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{neyshabur_towards_2018-1,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2102.09672},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	year = {2021},
}

@misc{noauthor_nie_nodate,
	title = {Nie: {Medical} {Image} {Synthesis} with {Deep} {Convolutional}... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?title=Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks&author=D.%20Nie&publication_year=2018},
}

@article{nie_medical_2018,
	title = {Medical {Image} {Synthesis} with {Deep} {Convolutional} {Adversarial} {Networks}},
	volume = {65},
	number = {12},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Nie, D. and Trullo, R. and Lian, J. and Wang, L. and Petitjean, C. and Ruan, S. and Wang, Q. and Shen, D.},
	year = {2018},
	pages = {2720--2730},
}

@book{nocedal_numerical_1999,
	address = {New York},
	series = {Springer {Series} in {Operations} {Research}},
	title = {Numerical {Optimization}},
	isbn = {978-0-387-98793-4},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {1999},
	lccn = {QA402.5 .N62 1999},
}

@article{oktay_attention_2018,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	journal = {arXiv preprint arXiv:1804.03999},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard},
	year = {2018},
}

@article{ongie_deep_2020,
	title = {Deep {learning} {techniques} for {inverse} {problems} in {imaging}},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Baraniuk, Christopher A. Metzler Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	year = {2020}
}

@article{ongie_function_2019,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	journal = {arXiv:1910.01635 [cs, stat]},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@inproceedings{osawa_large-scale_2019,
	title = {Large-{Scale} {Distributed} {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximate} {Curvature} for {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
	year = {2019},
	pages = {12359--12367},
}

@article{osullivan_fast_1985,
	title = {A {Fast} {Sinc} {Function} {Gridding} {Algorithm} for {Fourier} {Inversion} in {Computer} {Tomography}},
	volume = {4},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {O'Sullivan, J. D.},
	year = {1985},
	pages = {200--207},
}

@article{oymak_generalization_2019,
	title = {Generalization {Guarantees} for {Neural} {Networks} via {Harnessing} the {Low}-{Rank} {Structure} of the {Jacobian}},
	journal = {arXiv:1906.05392 [cs, math, stat]},
	author = {Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
	year = {2019},
}

@article{oymak_towards_nodate,
	title = {Towards {Moderate} {Overparameterization}: {Global} {Convergence} {Guarantees} for {Training} {Shallow} {Neural} {Networks}},
	author = {Oymak, Samet and Soltanolkotabi, Mahdi},
	pages = {41},
}

@article{pardoe_boosting_nodate,
	title = {Boosting for {Regression} {Transfer}},
	author = {Pardoe, David and Stone, Peter},
	pages = {8},
}

@article{pauloski_convolutional_nodate,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	author = {Pauloski, J Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T},
	pages = {11},
}

@article{pauloski_convolutional_2020,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-1,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-2,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-3,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{petersen__nodate,
	title = {[ http://matrixcookbook.com ]},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	pages = {72},
}

@article{pezzotti_adaptive-cs-net_2019,
	title = {Adaptive-{CS}-{Net}: {FastMRI} with {Adaptive} {Intelligence}},
	journal = {arXiv:1912.12259 [eess]},
	author = {Pezzotti, Nicola and de Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and van Gemert, Jeroen and Schülke, Christophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and van Osch, Matthias J. P. and Staring, Marius},
	year = {2019},
}

@inproceedings{pham_phaseless_2018,
	address = {Washington, DC},
	title = {Phaseless {Diffraction} {Tomography} with {Regularized} {Beam} {Propagation}},
	isbn = {978-1-5386-3636-7},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Pham, Thanh-an and Soubies, Emmanuel and Lim, Joowon and Goy, Alexandre and Soulez, Ferreol and Psaltis, Demetri and Unser, Michael},
	year = {2018},
	pages = {1268--1271},
}

@article{pilanci_newton_2015,
	title = {Newton {Sketch}: {A} {Linear}-{Time} {Optimization} {Algorithm} with {Linear}-{Quadratic} {Convergence}},
	journal = {arXiv:1505.02250 [cs, math, stat]},
	author = {Pilanci, Mert and Wainwright, Martin J.},
	year = {2015},
}

@article{powell_algorithms_1978,
	title = {Algorithms for {Nonlinear} {Constraints} {That} {Use} {Lagrangian} {Functions}},
	volume = {14},
	number = {1},
	journal = {Mathematical Programming},
	author = {Powell, M. J. D.},
	year = {1978},
	pages = {224--248},
}

@inproceedings{putzky_invert_2019,
	title = {Invert to {Learn} to {Invert}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Putzky, Patrick and Welling, Max},
	year = {2019},
	pages = {446--456},
}

@article{putzky_i-rim_2019,
	title = {I-{RIM} {applied} to the {fastMRI} {Challenge}},
	journal = {arXiv:1910.08952},
	author = {Putzky, Patrick and Karkalousos, Dimitrios and Teuwen, Jonas and Miriakov, Nikita and Bakker, Bart and Caan, Matthan and Welling, Max},
	year = {2019},
}

@article{putzky_recurrent_2017,
	title = {Recurrent {Inference} {Machines} for {Solving} {Inverse} {Problems}},
	journal = {arXiv:1706.04008 [cs]},
	author = {Putzky, Patrick and Welling, Max},
	year = {2017},
}

@article{qin_convolutional_2019,
	title = {Convolutional {Recurrent} {Neural} {Networks} for {Dynamic} {MR} {Image} {Reconstruction}},
	volume = {38},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Qin, Chen and Schlemper, Jo and Caballero, Jose and Price, Anthony N. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
	pages = {280--290},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2016},
}

@inproceedings{rafati_improving_2018,
	title = {Improving {L}-{BFGS} {Initialization} for {Trust}-{Region} {Methods} in {Deep} {Learning}},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Rafati, Jacob and Marcia, Roummel F.},
	year = {2018},
	pages = {501--508},
}

@article{ren_benchmarking_2021,
	title = {Benchmarking {Deep} {Inverse} {Models} over {Time}, and the {Neural}-{Adjoint} {Method}},
	journal = {arXiv:2009.12919 [cs, eess, stat]},
	author = {Ren, Simiao and Padilla, Willie and Malof, Jordan},
	year = {2021},
}

@article{noauthor_representations_nodate,
	title = {Representations of {Quasi}-{Newton} {Matrices} and {Their} {Use} in {Limited} {Memory} {Methods}},
	pages = {28},
}

@article{rezende_variational_nodate,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	pages = {10},
}

@inproceedings{venkatakrishnan2013plug,
	title={Plug-and-play priors for model based reconstruction},
	author={Venkatakrishnan, Singanallur V and Bouman, Charles A and Wohlberg, Brendt},
	booktitle={2013 IEEE Global Conference on Signal and Information Processing},
	pages={945--948},
	year={2013},
	organization={IEEE}
}

@article{saharia_image_2021,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	journal = {arXiv:2104.07636 [cs, eess]},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	year = {2021},
}

@article{sajjad_multi-grade_2019,
	title = {Multi-{Grade} {Brain} {Tumor} {Classification} {Using} {Deep} {CNN} with {Extensive} {Data} {Augmentation}},
	volume = {30},
	journal = {Journal of Computational Science},
	author = {Sajjad, Muhammad and Khan, Salman and Muhammad, Khan and Wu, Wanqing and Ullah, Amin and Baik, Sung Wook},
	year = {2019},
	pages = {174--182},
}

@article{salimans_weight_2016,
	title = {Weight {Normalization}: {A} {Simple} {Reparameterization} to {Accelerate} {Training} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1602.07868 [cs]},
	author = {Salimans, Tim and Kingma, Diederik P.},
	year = {2016},
}

@article{salman_provably_2020,
	title = {Provably {Robust} {Deep} {Learning} via {Adversarially} {Trained} {Smoothed} {Classifiers}},
	journal = {arXiv:1906.04584 [cs, stat]},
	author = {Salman, Hadi and Yang, Greg and Li, Jerry and Zhang, Pengchuan and Zhang, Huan and Razenshteyn, Ilya and Bubeck, Sebastien},
	year = {2020},
}

@article{savarese_how_2019,
	title = {How {Do} {Infinite} {Width} {Bounded} {Norm} {Networks} {Look} in {Function} {Space}?},
	journal = {arXiv:1902.05040 [cs, stat]},
	author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}
@article{bansal2022cold,
	title={Cold diffusion: Inverting arbitrary image transforms without noise},
	author={Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	journal={arXiv preprint arXiv:2208.09392},
	year={2022}
}
@article{lee2022progressive,
	title={Progressive deblurring of diffusion models for coarse-to-fine image synthesis},
	author={Lee, Sangyun and Chung, Hyungjin and Kim, Jaehyeon and Ye, Jong Chul},
	journal={arXiv preprint arXiv:2207.11192},
	year={2022}
}

@article{nachmani2021denoising,
	title={Denoising diffusion gamma models},
	author={Nachmani, Eliya and Roman, Robin San and Wolf, Lior},
	journal={arXiv preprint arXiv:2110.05948},
	year={2021}
}

@article{welker2022driftrec,
	title={DriftRec: Adapting diffusion models to blind image restoration tasks},
	author={Welker, Simon and Chapman, Henry N and Gerkmann, Timo},
	journal={arXiv preprint arXiv:2211.06757},
	year={2022}
}

@article{ren2022image,
	title={Image Deblurring with Domain Generalizable Diffusion Models},
	author={Ren, Mengwei and Delbracio, Mauricio and Talebi, Hossein and Gerig, Guido and Milanfar, Peyman},
	journal={arXiv preprint arXiv:2212.01789},
	year={2022}
}

@article{okhotin2023star,
	title={Star-Shaped Denoising Diffusion Probabilistic Models},
	author={Okhotin, Andrey and Molchanov, Dmitry and Arkhipkin, Vladimir and Bartosh, Grigory and Alanov, Aibek and Vetrov, Dmitry},
	journal={arXiv preprint arXiv:2302.05259},
	year={2023}
}

@inproceedings{songpseudoinverse,
	title={Pseudoinverse-Guided Diffusion Models for Inverse Problems},
	author={Song, Jiaming and Vahdat, Arash and Mardani, Morteza and Kautz, Jan},
	booktitle={International Conference on Learning Representations}
}
@article{hoogeboom2022blurring,
	title={Blurring diffusion models},
	author={Hoogeboom, Emiel and Salimans, Tim},
	journal={arXiv preprint arXiv:2209.05557},
	year={2022}
}

@article{rissanen2022generative,
	title={Generative modelling with inverse heat dissipation},
	author={Rissanen, Severi and Heinonen, Markus and Solin, Arno},
	journal={arXiv preprint arXiv:2206.13397},
	year={2022}
}


@article{deasy2021heavy,
	title={Heavy-tailed denoising score matching},
	author={Deasy, Jacob and Simidjievski, Nikola and Li{\`o}, Pietro},
	journal={arXiv preprint arXiv:2112.09788},
	year={2021}
}

@article{deasy2021heavy,
	title={Heavy-tailed denoising score matching},
	author={Deasy, Jacob and Simidjievski, Nikola and Li{\`o}, Pietro},
	journal={arXiv preprint arXiv:2112.09788},
	year={2021}
}

@book{sawyer_creation_nodate,
	title = {Creation of {Fully} {Sampled} {MR} {Data} {Repository} for {Compressed} {Sensing} of the {Knee}},
	author = {Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas and Healthcare, Ge},
}

@article{schlemper_dautomap_2019,
	title = {{dAUTOMAP}: {Decomposing} {AUTOMAP} to {Achieve} {Scalability} and {Enhance} {Performance}},
	journal = {arXiv:1909.10995 [cs, eess, stat]},
	author = {Schlemper, Jo and Oksuz, Ilkay and Clough, James R. and Duan, Jinming and King, Andrew P. and Schnabel, Julia A. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
}

@article{schlemper_deep_2017,
	title = {A {Deep} {Cascade} of {Convolutional} {Neural} {Networks} for {MR} {Image} {Reconstruction}},
	journal = {arXiv:1703.00555 [cs]},
	author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony and Rueckert, Daniel},
	year = {2017},
}

@book{shalev-shwartz_understanding_2014,
	address = {Cambridge},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
}

@article{shazeer_mesh-tensorflow_2018,
	title = {Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}},
	journal = {arXiv:1811.02084 [cs, stat]},
	author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
	year = {2018},
}

@article{shechtman_phase_nodate,
	title = {Phase {Retrieval} with {Application} to {Optical} {Imaging}},
	author = {Shechtman, Yoav and Eldar, Yonina C and Cohen, Oren and Chapman, Henry N and Miao, Jianwei and Segev, Mordechai},
	pages = {23},
}

@article{sheikh_image_nodate,
	title = {{IMAGE} {INFORMATION} {AND} {VISUAL} {QUALITY}},
	author = {Sheikh, Hamid R and Bovik, Alan C},
	pages = {4},
}

@article{shi_is_nodate,
	title = {Is the {Deconvolution} {Layer} the {Same} as a {Convolutional} {Layer}?},
	author = {Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Ledig, Christian and Wang, Zehan},
	pages = {7},
}

@inproceedings{shi_real-time_2016,
	address = {Las Vegas, NV, USA},
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	isbn = {978-1-4673-8851-1},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Wenzhe and Caballero, Jose and Huszar, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	year = {2016},
	pages = {1874--1883},
}

@article{shorten_survey_2019,
	title = {A {Survey} on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
	pages = {60},
}

@inproceedings{soltanolkotabi_3d_2019,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	booktitle = {2019 13th {International} {Conference} on {Sampling} {Theory} and {Applications} ({SampTA})},
	publisher = {IEEE},
	author = {Soltanolkotabi, Mahdi},
	year = {2019},
	pages = {1--3},
}

@article{song_generative_2020,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	journal = {arXiv:1907.05600 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{song_improved_2020,
	title = {Improved {Techniques} for {Training} {Score}-{Based} {Generative} {Models}},
	journal = {arXiv:2006.09011 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{steihaug_conjugate_1983,
	title = {The {Conjugate} {Gradient} {Method} and {Trust} {Regions} in {Large} {Scale} {Optimization}},
	volume = {20},
	number = {3},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Steihaug, Trond},
	year = {1983},
	pages = {626--637},
}

@article{stockmar_x-ray_2015,
	title = {X-{Ray} {Nanotomography} {Using} near-{Field} {Ptychography}},
	volume = {23},
	number = {10},
	journal = {Optics Express},
	author = {Stockmar, Marco and Hubert, Maxime and Dierolf, Martin and Enders, Bjoern and Clare, Richard and Allner, Sebastian and Fehringer, Andreas and Zanette, Irene and Villanova, Julie and Laurencin, Jérôme and Cloetens, Peter and Pfeiffer, Franz and Thibault, Pierre},
	year = {2015},
	pages = {12720},
}

@book{strang_linear_2006,
	address = {Belmont, CA},
	edition = {4th ed},
	title = {Linear {Algebra} and {Its} {Applications}},
	isbn = {978-0-03-010567-8},
	publisher = {Thomson, Brooks/Cole},
	author = {Strang, Gilbert},
	year = {2006},
	lccn = {QA184.2 .S77 2006},
}

@article{tan_survey_2018,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	journal = {arXiv:1808.01974 [cs, stat]},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	year = {2015},
}

@inproceedings{tzeng_adversarial_2017,
	address = {Honolulu, HI},
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	year = {2017},
	pages = {2962--2971},
}

@article{uecker_espirit_2014,
	title = {{ESPIRiT}— an {Eigenvalue} {Approach} to {Autocalibrating} {Parallel} {MRI}: {Where} {SENSE} {Meets} {GRAPPA}},
	volume = {71},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Uecker, Martin and Lai, Peng and Murphy, Mark J. and Virtue, Patrick and Elad, Michael and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2014},
	pages = {990--1001},
}

@article{ulyanov_instance_2017,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	year = {2017},
}

@article{unser_family_1993,
	title = {A {Family} of {Polynomial} {Spline} {Wavelet} {Transforms}},
	volume = {30},
	number = {2},
	journal = {Signal Processing},
	author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
	year = {1993},
	pages = {141--162},
}

@article{van_veen_compressed_2019,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{van_veen_compressed_2019-1,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{vershynin_high-dimensional_nodate,
	title = {High-{Dimensional} {Probability}},
	author = {Vershynin, Roman},
	pages = {301},
}

@article{wang_dense_2021,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	journal = {arXiv:2011.09157 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
}

@article{wang_pyramid_2020,
	title = {Pyramid {Convolutional} {RNN} for {MRI} {Reconstruction}},
	journal = {arXiv:1912.00543 [cs, eess, stat]},
	author = {Wang, Puyang and Chen, Eric Z. and Chen, Terrence and Patel, Vishal M. and Sun, Shanhui},
	year = {2020},
}

@inproceedings{wang_residual_2017,
	address = {Honolulu, HI},
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {6450--6458},
}

@inproceedings{wu_group_2018,
	title = {Group {Normalization}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Wu, Yuxin and He, Kaiming},
	year = {2018},
	pages = {3--19},
}

@article{xie_self-training_2020,
	title = {Self-{Training} with {Noisy} {Student} {Improves} {ImageNet} {Classification}},
	journal = {arXiv:1911.04252 [cs, stat]},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2020},
}

@article{xu_accelerated_nodate,
	title = {Accelerated {Wirtinger} {Flow}: {A} {New} {Fast} {Algorithm} for {Phase} {Retrieval}},
	author = {Xu, Rui and Soltanolkotabi, Mahdi and Haldar, Justin P and Zusman, Joshua and Levi, Anthony F J},
	pages = {18},
}

@article{yang_dagan_2018,
	title = {{DAGAN}: {Deep} {De}-{Aliasing} {Generative} {Adversarial} {Networks} for {Fast} {Compressed} {Sensing} {MRI} {Reconstruction}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and Firmin, David},
	year = {2018},
	pages = {1310--1321},
}

@article{yang_deep_nodate,
	title = {Deep {ADMM}-{Net} for {Compressive} {Sensing} {MRI}},
	author = {Yang, Yan and Li, Huibin and Sun, Jian and Xu, Zongben},
	pages = {9},
}

@article{yang_deep_2019,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_deep_2019-1,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_low_2018,
	title = {Low {Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Wang, Ge},
	year = {2018},
	pages = {1348--1357},
}

@article{yang_low-dose_2018,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@article{yang_low-dose_2018-1,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@inproceedings{yao_pyhessian_2020,
	address = {Atlanta, GA, USA},
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	isbn = {978-1-72816-251-5},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
}

@article{yi_generative_2019,
	title = {Generative {Adversarial} {Network} in {Medical} {Imaging}: {A} {Review}},
	volume = {58},
	journal = {Medical Image Analysis},
	author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
	year = {2019},
	pages = {101552},
}

@article{yu_generative_nodate,
	title = {Generative {Image} {Inpainting} with {Contextual} {Attention}},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
	pages = {15},
}

@article{zavriev_heavy-ball_1993,
	title = {Heavy-{Ball} {Method} in {Nonconvex} {Optimization} {Problems}},
	volume = {4},
	number = {4},
	journal = {Computational Mathematics and Modeling},
	author = {Zavriev, S. K. and Kostyuk, F. V.},
	year = {1993},
	pages = {336--341},
}

@article{zbontar_fastmri_2019,
	title = {{fastMRI}: {An} {open} {dataset} and {benchmarks} for {accelerated} {MRI}},
	journal = {arXiv:1811.08839},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	year = {2019},
}

@article{zhang_coil_2013,
	title = {Coil {Compression} for {Accelerated} {Imaging} with {Cartesian} {Sampling}},
	volume = {69},
	number = {2},
	journal = {Magnetic Resonance in Medicine},
	author = {Zhang, Tao and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2013},
	pages = {571--582},
}

@inproceedings{zhang_deep_2018,
	title = {Deep {Imitation} {Learning} for {Complex} {Manipulation} {Tasks} from {Virtual} {Reality} {Teleoperation}},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
	year = {2018},
	pages = {1--8},
}

@article{zhang_beyond_2017,
	title = {Beyond a {Gaussian} {Denoiser}: {Residual} {Learning} of {Deep} {CNN} for {Image} {Denoising}},
	volume = {26},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
	year = {2017},
	pages = {3142--3155},
}

@inproceedings{zhang_ista-net_2018,
	title = {{ISTA}-{Net}: {Interpretable} {optimization}-{inspired} {deep} {network} for {image} {compressive} {sensing}},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Jian and Ghanem, Bernard},
	year = {2018},
	pages = {1828--1837},
}

@article{zhang_understanding_2017,
	title = {Understanding {Deep} {Learning} {Requires} {Rethinking} {Generalization}},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
}

@article{zhao_differentiable_2020,
	title = {Differentiable {Augmentation} for {Data}-{Efficient} {GAN} {Training}},
	journal = {arXiv:2006.10738 [cs]},
	author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{zhao_image_2020,
	title = {Image {Augmentations} for {GAN} {Training}},
	journal = {arXiv:2006.02595 [cs, eess, stat]},
	author = {Zhao, Zhengli and Zhang, Zizhao and Chen, Ting and Singh, Sameer and Zhang, Han},
	year = {2020},
}

@inproceedings{zhu_unpaired_2017,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@inproceedings{zhu_unpaired_2017-1,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@misc{noauthor_3d_nodate-1,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	url = {https://www.researchgate.net/publication/339908871_3D_Phaseless_Imaging_at_Nano-scale_Challenges_and_Possible_Solutions},
	note = {Publication Title: ResearchGate},
}

@article{agarwal_efficient_2020-1,
	title = {Efficient {Full}-{Matrix} {Adaptive} {Regularization}},
	journal = {arXiv:1806.02958 [cs, math, stat]},
	author = {Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
	year = {2020},
}

@article{aggarwal_modl_2018-1,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	number = {2},
	journal = {IEEE transactions on medical imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {394--405},
}

@article{akiyama_first_2019-1,
	title = {First {M87} {Event} {Horizon} {Telescope} {Results}. {IV}. {Imaging} the {Central} {Supermassive} {Black} {Hole}},
	journal = {The Astrophysical Journal Letters},
	author = {Akiyama, Kazunori},
	year = {2019},
	pages = {52},
}

@article{anil_second_nodate-1,
	title = {Second {Order} {Optimization} {Made} {Practical}},
	author = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
	pages = {19},
}

@article{ardizzone_analyzing_2019-1,
	title = {Analyzing {Inverse} {Problems} with {Invertible} {Neural} {Networks}},
	journal = {arXiv:1808.04730 [cs, stat]},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	year = {2019},
}

@article{arora_fine-grained_2019-1,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	journal = {arXiv:1901.08584 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	year = {2019},
}

@article{arpit_benefits_2019-1,
	title = {The {Benefits} of {Over}-{Parameterization} at {Initialization} in {Deep} {ReLU} {Networks}},
	journal = {arXiv:1901.03611 [cs, stat]},
	author = {Arpit, Devansh and Bengio, Yoshua},
	year = {2019},
}

@article{asano_self-labelling_2019,
	title = {Self-{Labelling} via {Simultaneous} {Clustering} and {Representation} {Learning}},
	journal = {arXiv preprint arXiv:1911.05371},
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year = {2019},
}

@article{aslan_distributed_2020-1,
	title = {Distributed {Optimization} with {Tunable} {Learned} {Priors} for {Robust} {Ptycho}-{Tomography}},
	journal = {arXiv:2009.09498 [eess, math]},
	author = {Aslan, Selin and Liu, Zhengchun and Nikitin, Viktor and Bicer, Tekin and Leyffer, Sven and Gursoy, Doga},
	year = {2020},
}

@article{bachman_learning_2019,
	title = {Learning {Representations} by {Maximizing} {Mutual} {Information} {Across} {Views}},
	journal = {arXiv:1906.00910 [cs, stat]},
	author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
	year = {2019},
}

@article{ba_distributed_2016-1,
	title = {Distributed {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximations}},
	author = {Ba, Jimmy and Grosse, Roger and Martens, James},
	year = {2016},
}

@article{ba_layer_2016-1,
	title = {Layer {Normalization}},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year = {2016},
}

@article{barbastathis_use_2019-2,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barbastathis_use_2019-3,
	title = {On the {Use} of {Deep} {Learning} for {Computational} {Imaging}},
	volume = {6},
	number = {8},
	journal = {Optica},
	author = {Barbastathis, George and Ozcan, Aydogan and Situ, Guohai},
	year = {2019},
	pages = {921},
}

@article{barutcu_limited-angle_2021-1,
	title = {Limited-{Angle} {Computed} {Tomography} with {Deep} {Image} and {Physics} {Priors}},
	volume = {11},
	number = {1},
	journal = {Scientific Reports},
	author = {Barutcu, Semih and Aslan, Selin and Katsaggelos, Aggelos K. and Gürsoy, Doğa},
	year = {2021},
	pages = {17740},
}

@article{beatty_rapid_2005-1,
	title = {Rapid {Gridding} {Reconstruction} with a {Minimal} {Oversampling} {Ratio}},
	volume = {24},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Beatty, P.J. and Nishimura, D.G. and Pauly, J.M.},
	year = {2005},
	pages = {799--808},
}

@article{becker_self-organizing_1992,
	title = {Self-{Organizing} {Neural} {Network} {That} {Discovers} {Surfaces} in {Random}-{Dot} {Stereograms}},
	volume = {355},
	number = {6356},
	journal = {Nature},
	author = {Becker, Suzanna and Hinton, Geoffrey E.},
	year = {1992},
	pages = {161--163},
}

@article{ben-nun_demystifying_2019-1,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-{Depth} {Concurrency} {Analysis}},
	volume = {52},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	year = {2019},
	pages = {1--43},
}

@article{bernstein_distance_2020-1,
	title = {On the {Distance} between {Two} {Neural} {Networks} and the {Stability} of {Learning}},
	journal = {arXiv:2002.03432 [cs, math, stat]},
	author = {Bernstein, Jeremy and Vahdat, Arash and Yue, Yisong and Liu, Ming-Yu},
	year = {2020},
}

@book{bertero_introduction_1998-1,
	address = {Bristol, UK ; Philadelphia, Pa},
	title = {Introduction to {Inverse} {Problems} in {Imaging}},
	isbn = {978-0-7503-0439-9 978-0-7503-0435-1},
	publisher = {Institute of Physics Pub},
	author = {Bertero, Mario and Boccacci, Patrizia},
	year = {1998},
	lccn = {TA1637 .B47 1998},
}

@book{bertsekas_introduction_2002-1,
	address = {Belmont, Mass},
	edition = {2. print},
	series = {Optimization and {Computation} {Series}},
	title = {Introduction to {Probability}},
	isbn = {978-1-886529-40-3},
	number = {1},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	year = {2002},
}

@book{bhatia_matrix_1997-1,
	address = {New York},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Matrix {Analysis}},
	isbn = {978-0-387-94846-1},
	number = {169},
	publisher = {Springer},
	author = {Bhatia, Rajendra},
	year = {1997},
	lccn = {QA188 .B485 1997},
}

@article{bora_ambientgan_2018-1,
	title = {{AMBIENTGAN}: {GENERATIVE} {MODELS} {FROM} {LOSSY} {MEASUREMENTS}},
	author = {Bora, Ashish and Dimakis, Alexandros G},
	year = {2018},
	pages = {22},
}

@article{bora_compressed_nodate-1,
	title = {Compressed {Sensing} {Using} {Generative} {Models}},
	author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alex},
	pages = {10},
}

@article{bostan_deep_2020-1,
	title = {Deep {Phase} {Decoder}: {Self}-{Calibrating} {Phase} {Microscopy} with an {Untrained} {Deep} {Neural} {Network}},
	journal = {arXiv:2001.09803 [physics]},
	author = {Bostan, Emrah and Heckel, Reinhard and Chen, Michael and Kellman, Michael and Waller, Laura},
	year = {2020},
}

@article{bottou_optimization_2018-1,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	volume = {60},
	number = {2},
	journal = {SIAM Review},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	year = {2018},
	pages = {223--311},
}

@book{bovik_essential_2009-1,
	address = {London ; Boston},
	title = {The {Essential} {Guide} to {Image} {Processing}},
	isbn = {978-0-12-374457-9},
	publisher = {Academic Press},
	author = {Bovik, Alan C.},
	year = {2009},
	lccn = {TA1637 .B68 2009},
}

@book{boyd_convex_2004-1,
	address = {Cambridge, UK ; New York},
	title = {Convex {Optimization}},
	isbn = {978-0-521-83378-3},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	lccn = {QA402.5 .B69 2004},
}

@article{boyd_neal_nodate-1,
	title = {Neal {Parikh} {Department} of {Computer} {Science} {Stanford} {University}},
	author = {Boyd, Stephen},
	pages = {113},
}

@article{brock_high-performance_2021-1,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	year = {2021},
}

@article{brock_large_2019-1,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	year = {2019},
}

@article{bronstein_geometric_2017-1,
	title = {Geometric {Deep} {Learning}: {Going} beyond {Euclidean} {Data}},
	volume = {34},
	number = {4},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	year = {2017},
	pages = {18--42},
}

@article{burdakov_efficiently_2017-1,
	title = {On {Efficiently} {Combining} {Limited}-{Memory} and {Trust}-{Region} {Techniques}},
	volume = {9},
	number = {1},
	journal = {Mathematical Programming Computation},
	author = {Burdakov, Oleg and Gong, Lujin and Zikrin, Spartak and Yuan, Ya-xiang},
	year = {2017},
	pages = {101--134},
}

@article{candes_phase_2015-2,
	title = {Phase {Retrieval} from {Coded} {Diffraction} {Patterns}},
	volume = {39},
	number = {2},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Candès, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {277--299},
}

@article{candes_phase_2015-3,
	title = {Phase {Retrieval} via {Wirtinger} {Flow}: {Theory} and {Algorithms}},
	volume = {61},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Candes, Emmanuel J. and Li, Xiaodong and Soltanolkotabi, Mahdi},
	year = {2015},
	pages = {1985--2007},
}

@article{candes_stable_2006-1,
	title = {Stable {Signal} {Recovery} from {Incomplete} and {Inaccurate} {Measurements}},
	volume = {59},
	number = {8},
	journal = {Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
	author = {Candes, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
	year = {2006},
	note = {Publisher: Wiley Online Library},
	pages = {1207--1223},
}

@inproceedings{caron_deep_2018,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year = {2018},
	pages = {132--149},
}

@article{caron_unsupervised_2021,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	journal = {arXiv:2006.09882 [cs]},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
}

@article{noauthor_chapter_nodate-2,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@article{noauthor_chapter_nodate-3,
	title = {Chapter 6 - {Multiscale} {Image} {Decompositions} and {Wavelets}},
	pages = {20},
}

@inproceedings{chartsias_adversarial_2017-4,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-5,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-6,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@inproceedings{chartsias_adversarial_2017-7,
	title = {Adversarial {Image} {Synthesis} for {Unpaired} {Multi}-{Modal} {Cardiac} {Data}},
	booktitle = {International {Workshop} on {Simulation} and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer},
	author = {Chartsias, Agisilaos and Joyce, Thomas and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
	year = {2017},
	pages = {3--13},
}

@article{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	journal = {arXiv:2011.10566 [cs]},
	author = {Chen, Xinlei and He, Kaiming},
	year = {2020},
}

@article{chen_simple_2020-1,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year = {2020},
}

@article{chen_simple_2020-2,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year = {2020},
}

@article{child_generating_2019,
	title = {Generating {Long} {Sequences} with {Sparse} {Transformers}},
	journal = {arXiv:1904.10509 [cs, stat]},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	year = {2019},
}

@article{chung_empirical_2014-1,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	year = {2014},
}

@article{cohen_certified_2019-1,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	journal = {arXiv:1902.02918 [cs, stat]},
	author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
	year = {2019},
}

@book{noauthor_introduction_2009-1,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to {Algorithms}},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	year = {2009},
	lccn = {QA76.6 .C662 2009},
}

@article{cranmer_lagrangian_2020-1,
	title = {Lagrangian {Neural} {Networks}},
	journal = {arXiv:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	year = {2020},
}

@inproceedings{cubuk_autoaugment_2019-1,
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2019},
	pages = {113--123},
}

@article{curtis_trust_2017-1,
	title = {A {Trust} {Region} {Algorithm} with a {Worst}-{Case} {Iteration} {Complexity} of \$\${\textbackslash}textbackslash mathcal\{{\textbackslash}vphantom\}{O}{\textbackslash}vphantom\{\}({\textbackslash}textbackslash epsilon {\textasciicircum}\{-3/2\})\$\$ {O} ( ϵ - 3 / 2 ) for {Nonconvex} {Optimization}},
	volume = {162},
	number = {1-2},
	journal = {Mathematical Programming},
	author = {Curtis, Frank E. and Robinson, Daniel P. and Samadi, Mohammadreza},
	year = {2017},
	pages = {1--32},
}

@book{daubechies_ten_1992-1,
	address = {Philadelphia, Pa},
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Ten {Lectures} on {Wavelets}},
	isbn = {978-0-89871-274-2},
	number = {61},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Daubechies, Ingrid},
	year = {1992},
	lccn = {QA403.3 .D38 1992},
}

@article{davison_singular_1981-1,
	title = {A {Singular} {Value} {Decomposition} for the {Radon} {Transform} in {\textbackslash}emphn -{Dimensional} {Euclidean} {Space}},
	volume = {3},
	number = {3},
	journal = {Numerical Functional Analysis and Optimization},
	author = {Davison, M. E.},
	year = {1981},
	pages = {321--340},
}

@article{dean_large_nodate-1,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
	pages = {9},
}

@article{defazio_mri_2020-1,
	title = {{MRI} {Banding} {Removal} via {Adversarial} {Training}},
	journal = {arXiv:2001.08699 [cs, eess, stat]},
	author = {Defazio, Aaron and Murrell, Tullie and Recht, Michael P.},
	year = {2020},
}

@inproceedings{derezinski_distributed_2019,
	title = {Distributed {Estimation} of the {Inverse} {Hessian} by {Determinantal} {Averaging}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Derezinski, Michal and Mahoney, Michael W.},
	year = {2019},
	pages = {11405--11415},
}

@article{devlin_bert_2018,
	title = {Bert: {Pre}-{Training} of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
}

@article{devlin_bert_2019-1,
	title = {{BERT}: {Pre}-{Training} of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
}

@article{dhariwal_diffusion_2021-1,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	journal = {arXiv preprint arXiv:2105.05233},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	year = {2021},
}

@article{dierolf_ptychographic_nodate-1,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	author = {Dierolf, Martin},
	pages = {6},
}

@article{dierolf_ptychographic_2010-1,
	title = {Ptychographic {X}-{Ray} {Computed} {Tomography} at the {Nanoscale}},
	volume = {467},
	number = {7314},
	journal = {Nature},
	author = {Dierolf, Martin and Menzel, Andreas and Thibault, Pierre and Schneider, Philipp and Kewish, Cameron M. and Wepf, Roger and Bunk, Oliver and Pfeiffer, Franz},
	year = {2010},
	pages = {436--439},
}

@article{dinh_nice_2015-1,
	title = {{NICE}: {Non}-{Linear} {Independent} {Components} {Estimation}},
	journal = {arXiv:1410.8516 [cs]},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	year = {2015},
}

@article{domingos_few_2012-1,
	title = {A {Few} {Useful} {Things} to {Know} about {Machine} {Learning}},
	volume = {55},
	number = {10},
	journal = {Communications of the ACM},
	author = {Domingos, Pedro},
	year = {2012},
	pages = {78--87},
}

@article{donoho_compressed_2006-1,
	title = {Compressed {Sensing}},
	volume = {52},
	number = {4},
	journal = {IEEE Transactions on information theory},
	author = {Donoho, David L.},
	year = {2006},
	note = {Publisher: IEEE},
	pages = {1289--1306},
}

@article{dosovitskiy_image_2020,
	title = {An {image} {is} {worth} 16x16 {words}: {Transformers} for {image} {recognition} at {scale}},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2020},
}

@article{du_gradient_2019-1,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1811.03804 [cs, math, stat]},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	year = {2019},
}

@article{dumoulin_guide_nodate-1,
	title = {A {Guide} to {Convolution} {Arithmetic} for {Deep} {Learning}},
	author = {Dumoulin, Vincent and Visin, Francesco},
	pages = {31},
}

@article{durrett_probability_nodate-1,
	title = {Probability: {Theory} and {Examples}},
	author = {Durrett, Rick},
	pages = {386},
}

@article{el-nouby_xcit_2021,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	journal = {arXiv:2106.09681 [cs]},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Hervé},
	year = {2021},
}

@article{fabian_3d_2020-1,
	title = {{3D} {Phase} {Retrieval} at {Nano}-{Scale} via {Accelerated} {Wirtinger} {Flow}},
	journal = {arXiv preprint arXiv:2002.11785},
	author = {Fabian, Zalan and Haldar, Justin and Leahy, Richard and Soltanolkotabi, Mahdi},
	year = {2020},
}

@misc{noauthor_fastmri_nodate-1,
	title = {{FastMRI}},
	url = {https://fastmri.org/leaderboards/},
}

@article{fedus_switch_2021,
	title = {Switch {Transformers}: {scaling} to {trillion} {parameter} {models} with {simple} and {efficient} {sparsity}},
	journal = {arXiv preprint arXiv:2101.03961},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2021},
}

@article{feng_golden-angle_2014-1,
	title = {Golden-{Angle} {Radial} {Sparse} {Parallel} {MRI}: {Combination} of {Compressed} {Sensing}, {Parallel} {Imaging}, and {Golden}-{Angle} {Radial} {Sampling} for {Fast} and {Flexible} {Dynamic} {Volumetric} {MRI}: {iGRASP}: {Iterative} {Golden}-{Angle} {RAdial} {Sparse} {Parallel} {MRI}},
	volume = {72},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Feng, Li and Grimm, Robert and Block, Kai Tobias and Chandarana, Hersh and Kim, Sungheon and Xu, Jian and Axel, Leon and Sodickson, Daniel K. and Otazo, Ricardo},
	year = {2014},
	pages = {707--717},
}

@article{foret_sharpness-aware_2021-1,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	journal = {arXiv:2010.01412 [cs, stat]},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	year = {2021},
}

@inproceedings{gatys_image_2016-1,
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	year = {2016},
	pages = {2414--2423},
}

@article{george_fast_2018-1,
	title = {Fast {Approximate} {Natural} {Gradient} {Descent} in a {Kronecker}-{Factored} {Eigenbasis}},
	journal = {arXiv:1806.03884 [cs, stat]},
	author = {George, Thomas and Laurent, César and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
	year = {2018},
}

@article{ghorbani_investigation_2019,
	title = {An {Investigation} into {Neural} {Net} {Optimization} via {Hessian} {Eigenvalue} {Density}},
	journal = {arXiv:1901.10159 [cs, stat]},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	year = {2019},
}

@article{gilton_neumann_2019-1,
	title = {Neumann {Networks} for {Linear} {Inverse} {Problems} in {Imaging}},
	volume = {6},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gilton, Davis and Ongie, Greg and Willett, Rebecca},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {328--343},
}

@article{glorot_understanding_nodate-1,
	title = {Understanding the {Difficulty} of {Training} {Deep} {Feedforward} {Neural} {Networks}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
}

@article{goldfarb_practical_2020-1,
	title = {Practical {Quasi}-{Newton} {Methods} for {Training} {Deep} {Neural} {Networks}},
	journal = {arXiv:2006.08877 [cs, math, stat]},
	author = {Goldfarb, Donald and Ren, Yi and Bahamou, Achraf},
	year = {2020},
}

@article{gomez_reversible_nodate-1,
	title = {The {Reversible} {Residual} {Network}: {Backpropagation} {Without} {Storing} {Activations}},
	author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
	pages = {11},
}

@article{goyal_accurate_2018-1,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2018},
}

@inproceedings{gregor_learning_2010-1,
	title = {Learning {Fast} {Approximations} of {Sparse} {Coding}},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	author = {Gregor, Karol and LeCun, Yann},
	year = {2010},
	pages = {399--406},
}

@article{grill_bootstrap_2020,
	title = {Bootstrap {Your} {Own} {Latent}: {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	journal = {arXiv:2006.07733 [cs, stat]},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	year = {2020},
}

@article{grosse_kronecker-factored_nodate-1,
	title = {A {Kronecker}-{Factored} {Approximate} {Fisher} {Matrix} for {Convolution} {Layers}},
	author = {Grosse, Roger and Martens, James},
	pages = {10},
}

@article{gubner_probability_nodate-1,
	title = {{PROBABILITY} {AND} {RANDOM} {PROCESSES} {FOR} {ELECTRICAL} {AND} {COMPUTER} {ENGINEERS}},
	author = {Gubner, John A},
	pages = {642},
}

@article{guizar-sicairos_phase_2011-1,
	title = {Phase {Tomography} from {X}-{Ray} {Coherent} {Diffractive} {Imaging} {Projections}},
	volume = {19},
	number = {22},
	journal = {Optics Express},
	author = {Guizar-Sicairos, Manuel and Diaz, Ana and Holler, Mirko and Lucas, Miriam S. and Menzel, Andreas and Wepf, Roger A. and Bunk, Oliver},
	year = {2011},
	pages = {21345},
}

@article{guo_direct_2017-1,
	title = {Direct {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps} from {Highly} {Undersampled} {Brain} {Dynamic} {Contrast} {Enhanced} {MRI}: {Direct} {Estimation} of {Tracer}-{Kinetic} {Parameter} {Maps}},
	volume = {78},
	number = {4},
	journal = {Magnetic Resonance in Medicine},
	author = {Guo, Yi and Lingala, Sajan Goud and Zhu, Yinghua and Lebel, R. Marc and Nayak, Krishna S.},
	year = {2017},
	pages = {1566--1578},
}

@inproceedings{gupta_shampoo_2018-1,
	title = {Shampoo: {Preconditioned} {Stochastic} {Tensor} {Optimization}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gupta, Vineet and Koren, Tomer and Singer, Yoram},
	year = {2018},
	pages = {1842--1850},
}

@inproceedings{gutmann_noise-contrastive_2010,
	title = {Noise-{Contrastive} {Estimation}: {A} {New} {Estimation} {Principle} for {Unnormalized} {Statistical} {Models}},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	year = {2010},
	pages = {297--304},
}

@book{hajek_random_2015-1,
	edition = {First},
	title = {Random {Processes} for {Engineers}},
	isbn = {978-1-107-10012-1 978-1-316-16460-0},
	publisher = {Cambridge University Press},
	author = {Hajek, Bruce},
	year = {2015},
}

@article{halko_finding_2010-1,
	title = {Finding {Structure} with {Randomness}: {Probabilistic} {Algorithms} for {Constructing} {Approximate} {Matrix} {Decompositions}},
	journal = {arXiv:0909.4061 [math]},
	author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
	year = {2010},
}

@article{hand_phase_2018-1,
	title = {Phase {Retrieval} {Under} a {Generative} {Prior}},
	journal = {arXiv:1807.04261 [cs, math]},
	author = {Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
	year = {2018},
}

@article{heckel_deep_nodate-1,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-{Convolutional} {Networks}},
	author = {Heckel, Reinhard and Hand, Paul},
	pages = {17},
}

@article{heckel_regularizing_2019-2,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{heckel_regularizing_2019-3,
	title = {Regularizing {Linear} {Inverse} {Problems} with {Convolutional} {Neural} {Networks}},
	journal = {arXiv:1907.03100 [cs, math, stat]},
	author = {Heckel, Reinhard},
	year = {2019},
}

@article{he_deep_2015-1,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
}

@article{he_distributed_2017-1,
	title = {Distributed {Hessian}-{Free} {Optimization} for {Deep} {Neural} {Network}},
	journal = {arXiv:1606.00511 [cs, math]},
	author = {He, Xi and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Takáč, Martin},
	year = {2017},
}

@article{ho_cascaded_nodate-1,
	title = {Cascaded {Diffusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	pages = {28},
}

@article{ho_denoising_2020-1,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2006.11239},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
}

@article{holler_high-resolution_2017-1,
	title = {High-{Resolution} {Non}-{Destructive} {Three}-{Dimensional} {Imaging} of {Integrated} {Circuits}},
	volume = {543},
	number = {7645},
	journal = {Nature},
	author = {Holler, Mirko and Guizar-Sicairos, Manuel and Tsai, Esther H. R. and Dinapoli, Roberto and Müller, Elisabeth and Bunk, Oliver and Raabe, Jörg and Aeppli, Gabriel},
	year = {2017},
	pages = {402--406},
}

@article{holler_omnytomography_2018-1,
	title = {{OMNY}—{A} {tOMography} {Nano} {crYo} {Stage}},
	journal = {Rev. Sci. Instrum.},
	author = {Holler, M and Raabe, J and Diaz, A and Guizar-Sicairos, M and Wepf, R and Odstrcil, M and Shaik, F R and Panneels, V and Menzel, A and Sarafimov, B and Maag, S and Wang, X and Thominet, V and Walther, H and Lachat, T and Vitins, M and Bunk, O},
	year = {2018},
	pages = {14},
}

@book{horn_matrix_2012-1,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Matrix {Analysis}},
	isbn = {978-0-521-83940-2},
	publisher = {Cambridge University Press},
	author = {Horn, Roger A. and Johnson, Charles R.},
	year = {2012},
	lccn = {QA188 .H66 2012},
}

@inproceedings{huang_densely_2017-1,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {2261--2269},
}

@article{huang_provably_2018-1,
	title = {A {Provably} {Convergent} {Scheme} for {Compressive} {Sensing} under {Random} {Generative} {Priors}},
	journal = {arXiv:1812.04176 [math]},
	author = {Huang, Wen and Hand, Paul and Heckel, Reinhard and Voroninski, Vladislav},
	year = {2018},
}

@article{hussain_differential_nodate-1,
	title = {Differential {Data} {Augmentation} {Techniques} for {Medical} {Imaging} {Classification} {Tasks}},
	author = {Hussain, Zeshan and Gimenez, Francisco and Yi, Darvin and Rubin, Daniel},
	pages = {6},
}

@article{ioffe_batch_2015-1,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
}

@article{jacobsen_i-revnet_2018-1,
	title = {I-{RevNet}: {Deep} {Invertible} {Networks}},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	year = {2018},
}

@article{jaegle_perceiver_2021,
	title = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
	journal = {arXiv:2103.03206 [cs, eess]},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	year = {2021},
}

@article{jaiswal_survey_2021,
	title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
	journal = {arXiv:2011.00362 [cs]},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	year = {2021},
}

@article{jalal_robust_2021-1,
	title = {Robust {Compressed} {Sensing} {MRI} with {Deep} {Generative} {Priors}},
	journal = {arXiv:2108.01368 [cs, math, stat]},
	author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G. and Tamir, Jonathan I.},
	year = {2021},
}

@article{jiang_linear_nodate-1,
	title = {A {Linear} {Speedup} {Analysis} of {Distributed} {Deep} {Learning} with {Sparse} and {Quantized} {Communication}},
	author = {Jiang, Peng and Agrawal, Gagan},
	pages = {12},
}

@article{jiang_transgan_2021,
	title = {{TransGAN}: {Two} {Transformers} {Can} {Make} {One} {Strong} {GAN}},
	journal = {arXiv preprint arXiv:2102.07074},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
}

@article{jin_time-dependent_2019-1,
	title = {Time-{Dependent} {Deep} {Image} {Prior} for {Dynamic} {MRI}},
	journal = {arXiv:1910.01684 [cs, eess]},
	author = {Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jerome and Stuber, Matthias and Unser, Michael},
	year = {2019},
}

@inproceedings{jnawali_deep_2018-1,
	title = {Deep {3D} {Convolution} {Neural} {Network} for {CT} {Brain} {Hemorrhage} {Classification}},
	volume = {10575},
	booktitle = {Medical {Imaging} 2018: {Computer}-{Aided} {Diagnosis}},
	publisher = {International Society for Optics and Photonics},
	author = {Jnawali, Kamal and Arbabshirani, Mohammad R. and Rao, Navalgund and Patel, Alpen A.},
	year = {2018},
	pages = {105751C},
}

@inproceedings{johnson_perceptual_2016-1,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	year = {2016},
	pages = {694--711},
}

@article{kahnt_coupled_nodate-1,
	title = {Coupled {Ptychography} and {Tomography} {Algorithm} {Improves} {Reconstruction} of {Experimental} {Data}},
	author = {Kahnt, Maik and Becher, Johannes and Brückner, Dennis and Fam, Yakub and Sheppard, Thomas and Weissenberger, Tobias and Wittwer, Felix and Grunwaldt, Jan-Dierk and Schwieger, Wilhelm and Schroer, Christian G},
	pages = {8},
}

@article{karras_analyzing_2020-1,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@article{karras_progressive_2018-1,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
}

@article{karras_style-based_nodate-1,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	pages = {10},
}

@article{karras_training_2020-1,
	title = {Training {Generative} {Adversarial} {Networks} with {Limited} {Data}},
	journal = {arXiv:2006.06676 [cs, stat]},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
}

@inproceedings{keuper_distributed_2016-1,
	address = {Salt Lake City, UT, USA},
	title = {Distributed {Training} of {Deep} {Neural} {Networks}: {Theoretical} and {Practical} {Limits} of {Parallel} {Scalability}},
	isbn = {978-1-5090-3882-4},
	booktitle = {2016 2nd {Workshop} on {Machine} {Learning} in {HPC} {Environments} ({MLHPC})},
	publisher = {IEEE},
	author = {Keuper, Janis and Preundt, Franz-Josef},
	year = {2016},
	pages = {19--26},
}

@article{kingma_glow_nodate-2,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{kingma_glow_nodate-3,
	title = {Glow: {Generative} {Flow} with {Invertible} 1× 1 {Convolutions}},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	pages = {10},
}

@article{knoll_assessment_2019-1,
	title = {Assessment of the {Generalization} of {Learned} {Image} {Reconstruction} and the {Potential} for {Transfer} {Learning}},
	volume = {81},
	number = {1},
	journal = {Magnetic Resonance in Medicine},
	author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
	year = {2019},
	pages = {116--128},
}

@article{kobler_total_2020-1,
	title = {Total {Deep} {Variation} for {Linear} {Inverse} {Problems}},
	journal = {arXiv:2001.05005 [cs, math]},
	author = {Kobler, Erich and Effland, Alexander and Kunisch, Karl and Pock, Thomas},
	year = {2020},
}

@article{kobyzev_normalizing_2020-1,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, I. and Prince, S. and Brubaker, M.},
	year = {2020},
	pages = {1--1},
}

@inproceedings{kolesnikov_revisiting_2019,
	title = {Revisiting {Self}-{Supervised} {Visual} {Representation} {Learning}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
	year = {2019},
	pages = {1920--1929},
}

@article{korkmaz_unsupervised_2021-1,
	title = {Unsupervised {MRI} {Reconstruction} via {Zero}-{Shot} {Learned} {Adversarial} {Transformers}},
	journal = {arXiv:2105.08059 [cs, eess]},
	author = {Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and Özbey, Muzaffer and Çukur, Tolga},
	year = {2021},
}

@article{kreutz_complex_nodate-1,
	title = {The {Complex} {Gradient} {Operator} and the {CR}-{Calculus}},
	author = {Kreutz, Kenneth},
	pages = {74},
}

@article{krishnan_neumann_2017-1,
	title = {Neumann {Optimizer}: {A} {Practical} {Optimization} {Algorithm} for {Deep} {Neural} {Networks}},
	journal = {arXiv:1712.03298 [cs, stat]},
	author = {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A.},
	year = {2017},
}

@article{kuo_featmatch_2020-1,
	title = {{FeatMatch}: {Feature}-{Based} {Augmentation} for {Semi}-{Supervised} {Learning}},
	journal = {arXiv preprint arXiv:2007.08505},
	author = {Kuo, Chia-Wen and Ma, Chih-Yao and Huang, Jia-Bin and Kira, Zsolt},
	year = {2020},
}

@article{kwon_asam_2021-1,
	title = {{ASAM}: {Adaptive} {Sharpness}-{Aware} {Minimization} for {Scale}-{Invariant} {Learning} of {Deep} {Neural} {Networks}},
	journal = {arXiv:2102.11600 [cs, stat]},
	author = {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
	year = {2021},
}

@article{lauro_multiple_2020-1,
	title = {Multiple {Subglacial} {Water} {Bodies} below the {South} {Pole} of {Mars} {Unveiled} by {New} {MARSIS} {Data}},
	journal = {Nature Astronomy},
	author = {Lauro, Sebastian Emanuel and Pettinelli, Elena and Caprarelli, Graziella and Guallini, Luca and Rossi, Angelo Pio and Mattei, Elisabetta and Cosciotti, Barbara and Cicchetti, Andrea and Soldovieri, Francesco and Cartacci, Marco and Di Paolo, Federico and Noschese, Raffaella and Orosei, Roberto},
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
}

@article{lee_deep_2018-1,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	journal = {arXiv:1711.00165 [cs, stat]},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	year = {2018},
}

@article{lemley_smart_2017-1,
	title = {Smart {Augmentation} {Learning} an {Optimal} {Data} {Augmentation} {Strategy}},
	volume = {5},
	journal = {IEEE Access},
	author = {Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
	year = {2017},
	pages = {5858--5869},
}

@article{liang_swinir_2021-1,
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	journal = {arXiv:2108.10257 [cs, eess]},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
}

@article{li_communication_nodate-1,
	title = {Communication {Efficient} {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	author = {Li, Mu and Andersen, David G and Smola, Alexander and Yu, Kai},
	pages = {9},
}

@article{li_fourier_2020-1,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	journal = {arXiv:2010.08895 [cs, math]},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
}

@article{li_gan_2020-1,
	title = {{GAN} {Compression}: {Efficient} {Architectures} for {Interactive} {Conditional} {GANs}},
	journal = {arXiv:2003.08936 [cs, eess]},
	author = {Li, Muyang and Lin, Ji and Ding, Yaoyao and Liu, Zhijian and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{li_sacnn_2020-1,
	title = {{SACNN}: {Self}-{Attention} {Convolutional} {Neural} {Network} for {Low}-{Dose} {CT} {Denoising} with {Self}-{Supervised} {Perceptual} {Loss} {Network}},
	volume = {39},
	number = {7},
	journal = {IEEE transactions on medical imaging},
	author = {Li, Meng and Hsu, William and Xie, Xiaodong and Cong, Jason and Gao, Wen},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {2289--2301},
}

@article{liu_coupled_nodate-2,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@article{liu_coupled_nodate-3,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	pages = {9},
}

@incollection{liu_image_2018-1,
	address = {Cham},
	title = {Image {Inpainting} for {Irregular} {Holes} {Using} {Partial} {Convolutions}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
	year = {2018},
	pages = {89--105},
}

@article{liu_spark-based_2016-1,
	title = {Spark-{Based} {Large}-{Scale} {Matrix} {Inversion} for {Big} {Data} {Processing}},
	volume = {4},
	journal = {IEEE Access},
	author = {Liu, Jun and Liang, Yang and Ansari, Nirwan},
	year = {2016},
	pages = {2166--2176},
}

@article{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} {using} {shifted} {windows}},
	journal = {arXiv:2103.14030},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
}

@article{liu_unsupervised_nodate-2,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{liu_unsupervised_nodate-3,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	pages = {9},
}

@article{long_deep_nodate-1,
	title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
	pages = {10},
}

@article{long_fully_nodate-1,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
}

@article{lu_pretrained_2021,
	title = {Pretrained {Transformers} as {Universal} {Computation} {Engines}},
	journal = {arXiv:2103.05247 [cs]},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	year = {2021},
}

@article{lutter_deep_2019-1,
	title = {Deep {Lagrangian} {Networks}: {Using} {Physics} as {Model} {Prior} for {Deep} {Learning}},
	journal = {arXiv:1907.04490 [cs, eess, stat]},
	author = {Lutter, Michael and Ritter, Christian and Peters, Jan},
	year = {2019},
}

@article{maiden_improved_2009-1,
	title = {An {Improved} {Ptychographical} {Phase} {Retrieval} {Algorithm} for {Diffractive} {Imaging}},
	volume = {109},
	number = {10},
	journal = {Ultramicroscopy},
	author = {Maiden, Andrew M. and Rodenburg, John M.},
	year = {2009},
	pages = {1256--1262},
}

@article{maiden_ptychographic_nodate-1,
	title = {Ptychographic {Transmission} {Microscopy} in {Three} {Dimensions} {Using} a {Multi}-{Slice} {Approach}},
	author = {Maiden, A M and Humphry, M J and Rodenburg, J M},
	pages = {9},
}

@article{ma_inefficiency_2019-1,
	title = {Inefficiency of {K}-{FAC} for {Large} {Batch} {Size} {Training}},
	journal = {arXiv:1903.06237 [cs, stat]},
	author = {Ma, Linjian and Montague, Gabe and Ye, Jiayu and Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2019},
}

@article{mallat_theory_nodate-1,
	title = {A {Theory} for {Multiresolution} {Signal} {Decomposition}: {The} {Wavelet} {Representation}},
	author = {Mallat, Stephane G},
	pages = {31},
}

@article{mardani_deep_2017-2,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{mardani_deep_2017-3,
	title = {Deep {Generative} {Adversarial} {Networks} for {Compressed} {Sensing} {Automates} {MRI}},
	journal = {arXiv:1706.00051 [cs, stat]},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas and Zaharchuk, Greg and Alley, Marcus and Thakur, Neil and Han, Song and Dally, William and Pauly, John M. and Xing, Lei},
	year = {2017},
}

@article{martens_deep_nodate-1,
	title = {Deep {Learning} via {Hessian}-{Free} {Optimization}},
	author = {Martens, James},
	pages = {8},
}

@article{martens_new_2014-1,
	title = {New {Insights} and {Perspectives} on the {Natural} {Gradient} {Method}},
	journal = {arXiv preprint arXiv:1412.1193},
	author = {Martens, James},
	year = {2014},
}

@article{martens_optimizing_nodate-1,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-{Factored} {Approximate} {Curvature}},
	author = {Martens, James and Grosse, Roger},
	pages = {10},
}

@article{martinsson_randomized_2011-3,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {47--68},
}

@article{martinsson_randomized_2011-4,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{martinsson_randomized_2011-5,
	title = {A {Randomized} {Algorithm} for the {Decomposition} of {Matrices}},
	volume = {30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	year = {2011},
	pages = {47--68},
}

@article{menon_pulse_2020-2,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{menon_pulse_2020-3,
	title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
	journal = {arXiv:2003.03808 [cs, eess]},
	author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
	year = {2020},
}

@article{metzler_prdeep_nodate-1,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with {Flexible} {Deep} {Neural} {Networks}},
	author = {Metzler, Christopher A and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G},
	pages = {10},
}

@article{metzler_prdeep_2018-1,
	title = {{prDeep}: {Robust} {Phase} {Retrieval} with a {Flexible} {Deep} {Network}},
	journal = {arXiv:1803.00212 [cs, stat]},
	author = {Metzler, Christopher A. and Schniter, Philip and Veeraraghavan, Ashok and Baraniuk, Richard G.},
	year = {2018},
}

@book{meyer_matrix_2000-1,
	address = {Philadelphia},
	title = {Matrix {Analysis} and {Applied} {Linear} {Algebra}},
	isbn = {978-0-89871-454-8},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Meyer, C. D.},
	year = {2000},
	lccn = {QA188 .M495 2000},
}

@misc{noauthor_models_nodate-1,
	title = {Models {Genesis} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1361841520302048?token=B6B4930C1609C04FD49C6498E7935D83275465F07F9155D2DA9C09FBACAA0F60708219F8E62F34E2DB5319E5BC0CA8E0&originRegion=us-east-1&originCreation=20210914005025},
}

@article{moulin_analysis_1999-1,
	title = {Analysis of {Multiresolution} {Image} {Denoising} {Schemes} {Using} {Generalized} {Gaussian} and {Complexity} {Priors}},
	volume = {45},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Moulin, P. and {Juan Liu}},
	year = {1999},
	pages = {909--919},
}

@article{myagotin_efficient_2013-1,
	title = {Efficient {Volume} {Reconstruction} for {Parallel}-{Beam} {Computed} {Laminography} by {Filtered} {Backprojection} on {Multi}-{Core} {Clusters}},
	volume = {22},
	number = {12},
	journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
	author = {Myagotin, Anton and Voropaev, Alexey and Helfen, Lukas and Hänschke, Daniel and Baumbach, Tilo},
	year = {2013},
	pages = {14},
}

@article{nagle-mcnaughton_planet_2020-1,
	title = {{PlaNet}: {A} {Neural} {Network} for {Detecting} {Transverse} {Aeolian} {Ridges} on {Mars}},
	volume = {12},
	number = {21},
	journal = {Remote Sensing},
	author = {Nagle-McNaughton, Timothy and McClanahan, Timothy and Scuderi, Louis},
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {3607},
}

@inproceedings{narayanan_pipedream_2019-1,
	address = {Huntsville Ontario Canada},
	title = {{PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}},
	isbn = {978-1-4503-6873-5},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	year = {2019},
	pages = {1--15},
}

@article{neyshabur_towards_2018-2,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{neyshabur_towards_2018-3,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	year = {2018},
}

@article{nichol_improved_2021-1,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	journal = {arXiv preprint arXiv:2102.09672},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	year = {2021},
}

@misc{noauthor_nie_nodate-1,
	title = {Nie: {Medical} {Image} {Synthesis} with {Deep} {Convolutional}... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?title=Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks&author=D.%20Nie&publication_year=2018},
}

@article{nie_medical_2018-1,
	title = {Medical {Image} {Synthesis} with {Deep} {Convolutional} {Adversarial} {Networks}},
	volume = {65},
	number = {12},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Nie, D. and Trullo, R. and Lian, J. and Wang, L. and Petitjean, C. and Ruan, S. and Wang, Q. and Shen, D.},
	year = {2018},
	pages = {2720--2730},
}

@book{nocedal_numerical_1999-1,
	address = {New York},
	series = {Springer {Series} in {Operations} {Research}},
	title = {Numerical {Optimization}},
	isbn = {978-0-387-98793-4},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {1999},
	lccn = {QA402.5 .N62 1999},
}

@article{oktay_attention_2018-1,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	journal = {arXiv preprint arXiv:1804.03999},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard},
	year = {2018},
}

@article{ongie_deep_2020-1,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Baraniuk, Christopher A. Metzler Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	year = {2020},
	note = {Publisher: IEEE},
}

@article{ongie_function_2019-1,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	journal = {arXiv:1910.01635 [cs, stat]},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@article{van_den_oord_representation_2018,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	journal = {arXiv preprint arXiv:1807.03748},
	author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
	year = {2018},
}

@inproceedings{osawa_large-scale_2019-1,
	title = {Large-{Scale} {Distributed} {Second}-{Order} {Optimization} {Using} {Kronecker}-{Factored} {Approximate} {Curvature} for {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
	year = {2019},
	pages = {12359--12367},
}

@article{osullivan_fast_1985-1,
	title = {A {Fast} {Sinc} {Function} {Gridding} {Algorithm} for {Fourier} {Inversion} in {Computer} {Tomography}},
	volume = {4},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {O'Sullivan, J. D.},
	year = {1985},
	pages = {200--207},
}

@article{oymak_generalization_2019-1,
	title = {Generalization {Guarantees} for {Neural} {Networks} via {Harnessing} the {Low}-{Rank} {Structure} of the {Jacobian}},
	journal = {arXiv:1906.05392 [cs, math, stat]},
	author = {Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
	year = {2019},
}

@article{oymak_towards_nodate-1,
	title = {Towards {Moderate} {Overparameterization}: {Global} {Convergence} {Guarantees} for {Training} {Shallow} {Neural} {Networks}},
	author = {Oymak, Samet and Soltanolkotabi, Mahdi},
	pages = {41},
}

@article{papyan_full_2019,
	title = {The {Full} {Spectrum} of {Deepnet} {Hessians} at {Scale}: {Dynamics} with {SGD} {Training} and {Sample} {Size}},
	journal = {arXiv:1811.07062 [cs, stat]},
	author = {Papyan, Vardan},
	year = {2019},
}

@article{pardoe_boosting_nodate-1,
	title = {Boosting for {Regression} {Transfer}},
	author = {Pardoe, David and Stone, Peter},
	pages = {8},
}

@article{pauloski_convolutional_nodate-1,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	author = {Pauloski, J Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T},
	pages = {11},
}

@article{pauloski_convolutional_2020-4,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-5,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-6,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv preprint arXiv:2007.00784},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{pauloski_convolutional_2020-7,
	title = {Convolutional {Neural} {Network} {Training} with {Distributed} {K}-{FAC}},
	journal = {arXiv:2007.00784 [cs, stat]},
	author = {Pauloski, J. Gregory and Zhang, Zhao and Huang, Lei and Xu, Weijia and Foster, Ian T.},
	year = {2020},
}

@article{petersen__nodate-1,
	title = {[ http://matrixcookbook.com ]},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	pages = {72},
}

@article{pezzotti_adaptive-cs-net_2019-1,
	title = {Adaptive-{CS}-{Net}: {FastMRI} with {Adaptive} {Intelligence}},
	journal = {arXiv:1912.12259 [eess]},
	author = {Pezzotti, Nicola and de Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and van Gemert, Jeroen and Schülke, Christophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and van Osch, Matthias J. P. and Staring, Marius},
	year = {2019},
}

@inproceedings{pham_phaseless_2018-1,
	address = {Washington, DC},
	title = {Phaseless {Diffraction} {Tomography} with {Regularized} {Beam} {Propagation}},
	isbn = {978-1-5386-3636-7},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Pham, Thanh-an and Soubies, Emmanuel and Lim, Joowon and Goy, Alexandre and Soulez, Ferreol and Psaltis, Demetri and Unser, Michael},
	year = {2018},
	pages = {1268--1271},
}

@article{pilanci_newton_2015-1,
	title = {Newton {Sketch}: {A} {Linear}-{Time} {Optimization} {Algorithm} with {Linear}-{Quadratic} {Convergence}},
	journal = {arXiv:1505.02250 [cs, math, stat]},
	author = {Pilanci, Mert and Wainwright, Martin J.},
	year = {2015},
}

@article{powell_algorithms_1978-1,
	title = {Algorithms for {Nonlinear} {Constraints} {That} {Use} {Lagrangian} {Functions}},
	volume = {14},
	number = {1},
	journal = {Mathematical Programming},
	author = {Powell, M. J. D.},
	year = {1978},
	pages = {224--248},
}

@article{putzky_invert_nodate-3,
	title = {Invert to {Learn} to {Invert}},
	author = {Putzky, Patrick and Welling, Max},
	pages = {11},
}

@inproceedings{putzky_invert_2019-1,
	title = {Invert to {Learn} to {Invert}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Putzky, Patrick and Welling, Max},
	year = {2019},
	pages = {446--456},
}

@article{putzky_invert_nodate-4,
	title = {Invert to {Learn} to {Invert}},
	author = {Putzky, Patrick and Welling, Max},
	pages = {11},
}

@article{putzky_invert_nodate-5,
	title = {Invert to {Learn} to {Invert}},
	author = {Putzky, Patrick and Welling, Max},
	pages = {11},
}

@article{putzky_i-rim_2019-1,
	title = {I-{RIM} {Applied} to the {fastMRI} {Challenge}},
	journal = {arXiv:1910.08952 [cs, eess]},
	author = {Putzky, Patrick and Karkalousos, Dimitrios and Teuwen, Jonas and Miriakov, Nikita and Bakker, Bart and Caan, Matthan and Welling, Max},
	year = {2019},
}

@article{putzky_recurrent_2017-1,
	title = {Recurrent {Inference} {Machines} for {Solving} {Inverse} {Problems}},
	journal = {arXiv:1706.04008 [cs]},
	author = {Putzky, Patrick and Welling, Max},
	year = {2017},
}

@article{qin_convolutional_2019-1,
	title = {Convolutional {Recurrent} {Neural} {Networks} for {Dynamic} {MR} {Image} {Reconstruction}},
	volume = {38},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Qin, Chen and Schlemper, Jo and Caballero, Jose and Price, Anthony N. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
	pages = {280--290},
}

@article{radford_unsupervised_2016-1,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2016},
}

@inproceedings{rafati_improving_2018-1,
	title = {Improving {L}-{BFGS} {Initialization} for {Trust}-{Region} {Methods} in {Deep} {Learning}},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Rafati, Jacob and Marcia, Roummel F.},
	year = {2018},
	pages = {501--508},
}

@article{ren_benchmarking_2021-1,
	title = {Benchmarking {Deep} {Inverse} {Models} over {Time}, and the {Neural}-{Adjoint} {Method}},
	journal = {arXiv:2009.12919 [cs, eess, stat]},
	author = {Ren, Simiao and Padilla, Willie and Malof, Jordan},
	year = {2021},
}

@article{noauthor_representations_nodate-1,
	title = {Representations of {Quasi}-{Newton} {Matrices} and {Their} {Use} in {Limited} {Memory} {Methods}},
	pages = {28},
}

@article{rezende_variational_nodate-1,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	pages = {10},
}

@article{sagun_eigenvalues_2017,
	title = {Eigenvalues of the {Hessian} in {Deep} {Learning}: {Singularity} and {Beyond}},
	journal = {arXiv:1611.07476 [cs]},
	author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
	year = {2017},
}

@article{saharia_image_2021-1,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	journal = {arXiv:2104.07636 [cs, eess]},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	year = {2021},
}

@article{sajjad_multi-grade_2019-1,
	title = {Multi-{Grade} {Brain} {Tumor} {Classification} {Using} {Deep} {CNN} with {Extensive} {Data} {Augmentation}},
	volume = {30},
	journal = {Journal of Computational Science},
	author = {Sajjad, Muhammad and Khan, Salman and Muhammad, Khan and Wu, Wanqing and Ullah, Amin and Baik, Sung Wook},
	year = {2019},
	pages = {174--182},
}

@article{salimans_weight_2016-1,
	title = {Weight {Normalization}: {A} {Simple} {Reparameterization} to {Accelerate} {Training} of {Deep} {Neural} {Networks}},
	journal = {arXiv:1602.07868 [cs]},
	author = {Salimans, Tim and Kingma, Diederik P.},
	year = {2016},
}

@article{salman_provably_2020-1,
	title = {Provably {Robust} {Deep} {Learning} via {Adversarially} {Trained} {Smoothed} {Classifiers}},
	journal = {arXiv:1906.04584 [cs, stat]},
	author = {Salman, Hadi and Yang, Greg and Li, Jerry and Zhang, Pengchuan and Zhang, Huan and Razenshteyn, Ilya and Bubeck, Sebastien},
	year = {2020},
}

@article{savarese_how_2019-1,
	title = {How {Do} {Infinite} {Width} {Bounded} {Norm} {Networks} {Look} in {Function} {Space}?},
	journal = {arXiv:1902.05040 [cs, stat]},
	author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@book{sawyer_creation_nodate-1,
	title = {Creation of {Fully} {Sampled} {MR} {Data} {Repository} for {Compressed} {Sensing} of the {Knee}},
	author = {Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas and Healthcare, Ge},
}

@article{schlemper_dautomap_2019-1,
	title = {{dAUTOMAP}: {Decomposing} {AUTOMAP} to {Achieve} {Scalability} and {Enhance} {Performance}},
	journal = {arXiv:1909.10995 [cs, eess, stat]},
	author = {Schlemper, Jo and Oksuz, Ilkay and Clough, James R. and Duan, Jinming and King, Andrew P. and Schnabel, Julia A. and Hajnal, Joseph V. and Rueckert, Daniel},
	year = {2019},
}

@article{schlemper_deep_2017-1,
	title = {A {Deep} {Cascade} of {Convolutional} {Neural} {Networks} for {MR} {Image} {Reconstruction}},
	journal = {arXiv:1703.00555 [cs]},
	author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony and Rueckert, Daniel},
	year = {2017},
}

@book{shalev-shwartz_understanding_2014-1,
	address = {Cambridge},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
}

@article{shazeer_mesh-tensorflow_2018-1,
	title = {Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}},
	journal = {arXiv:1811.02084 [cs, stat]},
	author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
	year = {2018},
}

@article{shechtman_phase_nodate-1,
	title = {Phase {Retrieval} with {Application} to {Optical} {Imaging}},
	author = {Shechtman, Yoav and Eldar, Yonina C and Cohen, Oren and Chapman, Henry N and Miao, Jianwei and Segev, Mordechai},
	pages = {23},
}

@article{sheikh_image_nodate-1,
	title = {{IMAGE} {INFORMATION} {AND} {VISUAL} {QUALITY}},
	author = {Sheikh, Hamid R and Bovik, Alan C},
	pages = {4},
}

@article{shen_powernorm_2020,
	title = {{PowerNorm}: {Rethinking} {Batch} {Normalization} in {Transformers}},
	journal = {arXiv:2003.07845 [cs]},
	author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
	year = {2020},
}

@article{shi_is_nodate-1,
	title = {Is the {Deconvolution} {Layer} the {Same} as a {Convolutional} {Layer}?},
	author = {Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Ledig, Christian and Wang, Zehan},
	pages = {7},
}

@inproceedings{shi_real-time_2016-1,
	address = {Las Vegas, NV, USA},
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	isbn = {978-1-4673-8851-1},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Wenzhe and Caballero, Jose and Huszar, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	year = {2016},
	pages = {1874--1883},
}

@article{shorten_survey_2019-1,
	title = {A {Survey} on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
	pages = {60},
}

@article{skorski_chain_2019,
	title = {Chain {Rules} for {Hessian} and {Higher} {Derivatives} {Made} {Easy} by {Tensor} {Calculus}},
	journal = {arXiv:1911.13292 [cs]},
	author = {Skorski, Maciej},
	year = {2019},
}

@inproceedings{soltanolkotabi_3d_2019-1,
	title = {{3D} {Phaseless} {Imaging} at {Nano}-{Scale}: {Challenges} and {Possible} {Solutions}},
	booktitle = {2019 13th {International} {Conference} on {Sampling} {Theory} and {Applications} ({SampTA})},
	publisher = {IEEE},
	author = {Soltanolkotabi, Mahdi},
	year = {2019},
	pages = {1--3},
}

@article{song_generative_2020-1,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	journal = {arXiv:1907.05600 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{song_improved_2020-1,
	title = {Improved {Techniques} for {Training} {Score}-{Based} {Generative} {Models}},
	journal = {arXiv:2006.09011 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	year = {2020},
}

@article{steihaug_conjugate_1983-1,
	title = {The {Conjugate} {Gradient} {Method} and {Trust} {Regions} in {Large} {Scale} {Optimization}},
	volume = {20},
	number = {3},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Steihaug, Trond},
	year = {1983},
	pages = {626--637},
}

@article{stockmar_x-ray_2015-1,
	title = {X-{Ray} {Nanotomography} {Using} near-{Field} {Ptychography}},
	volume = {23},
	number = {10},
	journal = {Optics Express},
	author = {Stockmar, Marco and Hubert, Maxime and Dierolf, Martin and Enders, Bjoern and Clare, Richard and Allner, Sebastian and Fehringer, Andreas and Zanette, Irene and Villanova, Julie and Laurencin, Jérôme and Cloetens, Peter and Pfeiffer, Franz and Thibault, Pierre},
	year = {2015},
	pages = {12720},
}

@book{strang_linear_2006-1,
	address = {Belmont, CA},
	edition = {4th ed},
	title = {Linear {Algebra} and {Its} {Applications}},
	isbn = {978-0-03-010567-8},
	publisher = {Thomson, Brooks/Cole},
	author = {Strang, Gilbert},
	year = {2006},
	lccn = {QA184.2 .S77 2006},
}

@article{tan_survey_2018-1,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	journal = {arXiv:1808.01974 [cs, stat]},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
}

@article{tian_understanding_2021,
	title = {Understanding {Self}-{Supervised} {Learning} {Dynamics} without {Contrastive} {Pairs}},
	journal = {arXiv:2102.06810 [cs]},
	author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
	year = {2021},
}

@article{tishby_information_2000,
	title = {The {Information} {Bottleneck} {Method}},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {2000},
}

@article{touvron_training_2020,
	title = {Training {Data}-{Efficient} {Image} {Transformers} \& {Distillation} through {Attention}},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	year = {2020},
}

@inproceedings{tzeng_adversarial_2017-1,
	address = {Honolulu, HI},
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	year = {2017},
	pages = {2962--2971},
}

@article{uecker_espirit_2014-1,
	title = {{ESPIRiT}— an {Eigenvalue} {Approach} to {Autocalibrating} {Parallel} {MRI}: {Where} {SENSE} {Meets} {GRAPPA}},
	volume = {71},
	number = {3},
	journal = {Magnetic Resonance in Medicine},
	author = {Uecker, Martin and Lai, Peng and Murphy, Mark J. and Virtue, Patrick and Elad, Michael and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2014},
	pages = {990--1001},
}

@article{ulyanov_instance_2017-1,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	year = {2017},
}

@article{unser_family_1993-1,
	title = {A {Family} of {Polynomial} {Spline} {Wavelet} {Transforms}},
	volume = {30},
	number = {2},
	journal = {Signal Processing},
	author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
	year = {1993},
	pages = {141--162},
}

@article{van_veen_compressed_2019-2,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{van_veen_compressed_2019-3,
	title = {Compressed {Sensing} with {Deep} {Image} {Prior} and {Learned} {Regularization}},
	journal = {arXiv:1806.06438 [cs, math, stat]},
	author = {Van Veen, Dave and Jalal, Ajil and Soltanolkotabi, Mahdi and Price, Eric and Vishwanath, Sriram and Dimakis, Alexandros G.},
	year = {2019},
}

@article{vaswani_attention_2017,
	title = {Attention {is} {all} {you} {need}},
	volume = {30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
}

@article{vershynin_high-dimensional_nodate-1,
	title = {High-{Dimensional} {Probability}},
	author = {Vershynin, Roman},
	pages = {301},
}

@article{wang_dense_2021-1,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	journal = {arXiv:2011.09157 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
}

@article{wang_dense_2021-2,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	journal = {arXiv:2011.09157 [cs]},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
}

@article{wang_pyramid_2020-1,
	title = {Pyramid {Convolutional} {RNN} for {MRI} {Reconstruction}},
	journal = {arXiv:1912.00543 [cs, eess, stat]},
	author = {Wang, Puyang and Chen, Eric Z. and Chen, Terrence and Patel, Vishal M. and Sun, Shanhui},
	year = {2020},
}

@inproceedings{wang_residual_2017-1,
	address = {Honolulu, HI},
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	isbn = {978-1-5386-0457-1},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {6450--6458},
}

@inproceedings{wu_group_2018-1,
	title = {Group {Normalization}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Wu, Yuxin and He, Kaiming},
	year = {2018},
	pages = {3--19},
}

@article{xiao_region_2021,
	title = {Region {Similarity} {Representation} {Learning}},
	journal = {arXiv:2103.12902 [cs]},
	author = {Xiao, Tete and Reed, Colorado J. and Wang, Xiaolong and Keutzer, Kurt and Darrell, Trevor},
	year = {2021},
}

@article{xu_accelerated_nodate-1,
	title = {Accelerated {Wirtinger} {Flow}: {A} {New} {Fast} {Algorithm} for {Phase} {Retrieval}},
	author = {Xu, Rui and Soltanolkotabi, Mahdi and Haldar, Justin P and Zusman, Joshua and Levi, Anthony F J},
	pages = {18},
}

@article{yang_dagan_2018-1,
	title = {{DAGAN}: {Deep} {De}-{Aliasing} {Generative} {Adversarial} {Networks} for {Fast} {Compressed} {Sensing} {MRI} {Reconstruction}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and Firmin, David},
	year = {2018},
	pages = {1310--1321},
}

@article{yang_deep_nodate-1,
	title = {Deep {ADMM}-{Net} for {Compressive} {Sensing} {MRI}},
	author = {Yang, Yan and Li, Huibin and Sun, Jian and Xu, Zongben},
	pages = {9},
}

@article{yang_deep_2019-2,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_deep_2019-3,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	year = {2019},
	pages = {3106--3121},
}

@article{yang_low_2018-1,
	title = {Low {Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Wang, Ge},
	year = {2018},
	pages = {1348--1357},
}

@article{yang_low-dose_2018-2,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@article{yang_low-dose_2018-3,
	title = {Low-{Dose} {CT} {Image} {Denoising} {Using} a {Generative} {Adversarial} {Network} with {Wasserstein} {Distance} and {Perceptual} {Loss}},
	volume = {37},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1348--1357},
}

@article{yang_ouroboros_nodate,
	title = {Ouroboros: {On} {Accelerating} {Training} of {Transformer}-{Based} {Language} {Models}},
	author = {Yang, Qian and Huo, Zhouyuan and Wang, Wenlin and Carin, Lawrence},
	pages = {11},
}

@article{yao_adahessian_2020,
	title = {{ADAHESSIAN}: {An} {Adaptive} {Second} {Order} {Optimizer} for {Machine} {Learning}},
	journal = {arXiv:2006.00719 [cs, math, stat]},
	author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
}

@inproceedings{yao_pyhessian_2020-1,
	address = {Atlanta, GA, USA},
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	isbn = {978-1-72816-251-5},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
}

@inproceedings{yao_pyhessian_2020-2,
	address = {Atlanta, GA, USA},
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	isbn = {978-1-72816-251-5},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
}

@article{yi_generative_2019-1,
	title = {Generative {Adversarial} {Network} in {Medical} {Imaging}: {A} {Review}},
	volume = {58},
	journal = {Medical Image Analysis},
	author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
	year = {2019},
	pages = {101552},
}

@article{yuan_volo_2021,
	title = {{VOLO}: {Vision} {Outlooker} for {Visual} {Recognition}},
	journal = {arXiv:2106.13112 [cs]},
	author = {Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
	year = {2021},
}

@article{yu_generative_nodate-1,
	title = {Generative {Image} {Inpainting} with {Contextual} {Attention}},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
	pages = {15},
}

@article{zavriev_heavy-ball_1993-1,
	title = {Heavy-{Ball} {Method} in {Nonconvex} {Optimization} {Problems}},
	volume = {4},
	number = {4},
	journal = {Computational Mathematics and Modeling},
	author = {Zavriev, S. K. and Kostyuk, F. V.},
	year = {1993},
	pages = {336--341},
}

@article{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	year = {2021},
}

@article{zbontar_fastmri_2019-1,
	title = {{fastMRI}: {An} {Open} {Dataset} and {Benchmarks} for {Accelerated} {MRI}},
	journal = {arXiv:1811.08839 [physics, stat]},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	year = {2019},
}

@article{zhang_coil_2013-1,
	title = {Coil {Compression} for {Accelerated} {Imaging} with {Cartesian} {Sampling}},
	volume = {69},
	number = {2},
	journal = {Magnetic Resonance in Medicine},
	author = {Zhang, Tao and Pauly, John M. and Vasanawala, Shreyas S. and Lustig, Michael},
	year = {2013},
	pages = {571--582},
}

@inproceedings{zhang_deep_2018-1,
	title = {Deep {Imitation} {Learning} for {Complex} {Manipulation} {Tasks} from {Virtual} {Reality} {Teleoperation}},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
	year = {2018},
	pages = {1--8},
}

@article{zhang_beyond_2017-1,
	title = {Beyond a {Gaussian} {Denoiser}: {Residual} {Learning} of {Deep} {CNN} for {Image} {Denoising}},
	volume = {26},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
	year = {2017},
	pages = {3142--3155},
}

@inproceedings{zhang_ista-net_2018-1,
	address = {Salt Lake City, UT},
	title = {{ISTA}-{Net}: {Interpretable} {Optimization}-{Inspired} {Deep} {Network} for {Image} {Compressive} {Sensing}},
	isbn = {978-1-5386-6420-9},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Jian and Ghanem, Bernard},
	year = {2018},
	pages = {1828--1837},
}

@article{zhang_understanding_2017-1,
	title = {Understanding {Deep} {Learning} {Requires} {Rethinking} {Generalization}},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
}

@article{zhao_differentiable_2020-1,
	title = {Differentiable {Augmentation} for {Data}-{Efficient} {GAN} {Training}},
	journal = {arXiv:2006.10738 [cs]},
	author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
	year = {2020},
}

@article{zhao_image_2020-1,
	title = {Image {Augmentations} for {GAN} {Training}},
	journal = {arXiv:2006.02595 [cs, eess, stat]},
	author = {Zhao, Zhengli and Zhang, Zizhao and Chen, Ting and Singh, Sameer and Zhang, Han},
	year = {2020},
}

@inproceedings{zhu_unpaired_2017-2,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@inproceedings{zhu_unpaired_2017-3,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
}

@article{gan_ss-jircs_nodate,
	title = {{SS}-{JIRCS}: {Self}-{Supervised} {Joint} {Image} {Reconstruction} and {Coil} {Sensitivity} {Calibration} in {Parallel} {MRI} {Without} {Ground} {Truth}},
	abstract = {Parallel magnetic resonance imaging (MRI) is a widelyused technique that accelerates data collection by making use of the spatial encoding provided by multiple receiver coils. A key issue in parallel MRI is the estimation of coil sensitivity maps (CSMs) that are used for reconstructing a single high-quality image. This paper addresses this issue by developing SS-JIRCS, a new self-supervised model-based deep-learning (DL) method for image reconstruction that is equipped with automated CSM calibration. Our deep network consists of three types of modules: data-consistency, regularization, and CSM calibration. Unlike traditional supervised DL methods, these modules are directly trained on undersampled and noisy k-space data rather than on fully sampled high-quality ground truth. We present empirical results on simulated data that show the potential of the proposed method for achieving better performance than several baseline methods.},
	language = {en},
	author = {Gan, Weijie and Hu, Yuyang and Eldeniz, Cihat and Liu, Jiaming and Chen, Yasheng and An, Hongyu and Kamilov, Ulugbek S},
	pages = {9},
	file = {Gan et al. - SS-JIRCS Self-Supervised Joint Image Reconstructi.pdf:/Users/zalan/Zotero/storage/A4DQJ6JH/Gan et al. - SS-JIRCS Self-Supervised Joint Image Reconstructi.pdf:application/pdf},
}

@article{desai_noise2recon_2021,
	title = {{Noise2Recon}: {A} {Semi}-{Supervised} {Framework} for {Joint} {MRI} {Reconstruction} and {Denoising}},
	shorttitle = {{Noise2Recon}},
	url = {http://arxiv.org/abs/2110.00075},
	abstract = {Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, standard supervised DL methods depend on extensive amounts of fully-sampled ground-truth data and are sensitive to out-ofdistribution (OOD) shifts, in particular for low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose a semisupervised, consistency-based framework (termed Noise2Recon) for joint MR reconstruction and denoising. Our method enables the usage of a limited number of fully-sampled and a large number of undersampled-only scans. We compare our method to augmentation-based supervised techniques and ﬁne-tuned denoisers. Results demonstrate that even with minimal ground-truth data, Noise2Recon (1) achieves high performance on in-distribution (low-noise) scans and (2) improves generalizability to OOD, noisy scans.},
	language = {en},
	urldate = {2021-10-19},
	journal = {arXiv:2110.00075 [cs, eess]},
	author = {Desai, Arjun D. and Ozturkler, Batu M. and Sandino, Christopher M. and Vasanawala, Shreyas and Hargreaves, Brian A. and Re, Christopher M. and Pauly, John M. and Chaudhari, Akshay S.},
	month = sep,
	year = {2021},
	note = {arXiv: 2110.00075},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Desai et al. - 2021 - Noise2Recon A Semi-Supervised Framework for Joint.pdf:/Users/zalan/Zotero/storage/WUWIKZEL/Desai et al. - 2021 - Noise2Recon A Semi-Supervised Framework for Joint.pdf:application/pdf},
}

@article{zbontar_fastmri_2019-2,
	title = {{fastMRI}: {An} {Open} {Dataset} and {Benchmarks} for {Accelerated} {MRI}},
	shorttitle = {{fastMRI}},
	url = {http://arxiv.org/abs/1811.08839},
	abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
	urldate = {2021-10-26},
	journal = {arXiv:1811.08839 [physics, stat]},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	month = dec,
	year = {2019},
	note = {arXiv: 1811.08839},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Physics - Medical Physics, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/WGRRX9D9/Zbontar et al. - 2019 - fastMRI An Open Dataset and Benchmarks for Accele.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/8R4GPTFM/1811.html:text/html},
}

@article{moritz_linearly-convergent_nodate,
	title = {A {Linearly}-{Convergent} {Stochastic} {L}-{BFGS} {Algorithm}},
	abstract = {We propose a new stochastic L-BFGS algorithm and prove a linear convergence rate for strongly convex and smooth functions. Our algorithm draws heavily from a recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as a recent approach to variance reduction for stochastic gradient descent from Johnson and Zhang (2013). We demonstrate experimentally that our algorithm performs well on large-scale convex and non-convex optimization problems, exhibiting linear convergence and rapidly solving the optimization problems to high levels of precision. Furthermore, we show that our algorithm performs well for a wide-range of step sizes, often diﬀering by several orders of magnitude.},
	language = {en},
	author = {Moritz, Philipp and Nishihara, Robert and Jordan, Michael I},
	pages = {10},
	file = {Moritz et al. - A Linearly-Convergent Stochastic L-BFGS Algorithm.pdf:/Users/zalan/Zotero/storage/2R77EYZS/Moritz et al. - A Linearly-Convergent Stochastic L-BFGS Algorithm.pdf:application/pdf},
}

@inproceedings{gower_stochastic_2016,
	title = {Stochastic {Block} {BFGS}: {Squeezing} {More} {Curvature} out of {Data}},
	shorttitle = {Stochastic {Block} {BFGS}},
	url = {https://proceedings.mlr.press/v48/gower16.html},
	abstract = {We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gower, Robert and Goldfarb, Donald and Richtarik, Peter},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1869--1878},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/4NDW3WMJ/Gower et al. - 2016 - Stochastic Block BFGS Squeezing More Curvature ou.pdf:application/pdf},
}

@inproceedings{chen_simple_2020-3,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2021-11-05},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/NR6PBD2X/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:/Users/zalan/Zotero/storage/7REDCDMN/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@article{ramesh2022hierarchical,
	title={Hierarchical text-conditional image generation with clip latents},
	author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	journal={arXiv preprint arXiv:2204.06125},
	year={2022}
}


@article{jaiswal_survey_2021-1,
	title = {A {Survey} on {Contrastive} {Self}-{Supervised} {Learning}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2227-7080/9/1/2},
	doi = {10.3390/technologies9010002},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Technologies},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = mar,
	year = {2021},
	note = {Number: 1
	Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {contrastive learning, discriminative learning, image/video classification, object detection, self-supervised learning, transfer learning, unsupervised learning},
	pages = {2},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/VDFDYQ2Q/Jaiswal et al. - 2021 - A Survey on Contrastive Self-Supervised Learning.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/5PHG2ACH/2.html:text/html},
}

@article{zbontar_barlow_2021-1,
	title = {Barlow twins: {Self}-supervised learning via redundancy reduction},
	shorttitle = {Barlow twins},
	journal = {arXiv preprint arXiv:2103.03230},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/DTDHYIDT/Zbontar et al. - 2021 - Barlow twins Self-supervised learning via redunda.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/RE9ID5UJ/2103.html:text/html},
}

@article{grill_bootstrap_2020-1,
	title = {Bootstrap your own latent: {A} new approach to self-supervised learning},
	shorttitle = {Bootstrap your own latent},
	journal = {arXiv preprint arXiv:2006.07733},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/WFEEL46G/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/G7CUPWUE/2006.html:text/html},
}

@inproceedings{caron_deep_2018-1,
	title = {Deep clustering for unsupervised learning of visual features},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year = {2018},
	pages = {132--149},
	file = {Full Text:/Users/zalan/Zotero/storage/6LGIJQAJ/Caron et al. - 2018 - Deep clustering for unsupervised learning of visua.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/PB6NSRCK/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.html:text/html},
}

@inproceedings{tishby_deep_2015-1,
	title = {Deep learning and the information bottleneck principle},
	doi = {10.1109/ITW.2015.7133169},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	booktitle = {2015 {IEEE} {Information} {Theory} {Workshop} ({ITW})},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = apr,
	year = {2015},
	keywords = {Bifurcation, Complexity theory, Computer architecture, Distortion, Feature extraction, Mutual information, Training},
	pages = {1--5},
	file = {IEEE Xplore Full Text PDF:/Users/zalan/Zotero/storage/JLGBW5X4/Tishby and Zaslavsky - 2015 - Deep learning and the information bottleneck princ.pdf:application/pdf},
}

@inproceedings{wang_dense_2021-3,
	title = {Dense contrastive learning for self-supervised visual pre-training},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
	pages = {3024--3033},
	file = {Full Text:/Users/zalan/Zotero/storage/4M6RFK5N/Wang et al. - 2021 - Dense contrastive learning for self-supervised vis.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/6WCVJXMA/Wang_Dense_Contrastive_Learning_for_Self-Supervised_Visual_Pre-Training_CVPR_2021_paper.html:text/html},
}

@article{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	language = {en},
	urldate = {2021-11-05},
	journal = {arXiv:2104.14294 [cs]},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv: 2104.14294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:/Users/zalan/Zotero/storage/M98T6PGY/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf},
}

@inproceedings{chen_exploring_2021,
	title = {Exploring simple siamese representation learning},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chen, Xinlei and He, Kaiming},
	year = {2021},
	pages = {15750--15758},
	file = {Full Text:/Users/zalan/Zotero/storage/KPFEXVNU/Chen and He - 2021 - Exploring simple siamese representation learning.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/ZC65M73Z/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html:text/html},
}

@article{bachman_learning_2019-1,
	title = {Learning representations by maximizing mutual information across views},
	journal = {arXiv preprint arXiv:1906.00910},
	author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/3JUSBFXZ/Bachman et al. - 2019 - Learning representations by maximizing mutual info.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/AXVYUNLK/1906.html:text/html},
}

@inproceedings{gutmann_noise-contrastive_2010-1,
	title = {Noise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
	shorttitle = {Noise-contrastive estimation},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	year = {2010},
	pages = {297--304},
	file = {Full Text:/Users/zalan/Zotero/storage/MKRRRAMF/Gutmann and Hyvärinen - 2010 - Noise-contrastive estimation A new estimation pri.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/Z86MXL6G/gutmann10a.html:text/html},
}

@article{xiao_region_2021-1,
	title = {Region {Similarity} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2103.12902},
	abstract = {We present Region Similarity Representation Learning (ReSim), a new approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. While existing work has largely focused on solely learning global representations for an entire image, ReSim learns both regional representations for localization as well as semantic image-level representations. ReSim operates by sliding a fixed-sized window across the overlapping area between two views (e.g., image crops), aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps; this allows downstream tasks to leverage these representations for localization. Through object detection, instance segmentation, and dense pose estimation experiments, we illustrate how ReSim learns representations which significantly improve the localization and classification performance compared to a competitive MoCo-v2 baseline: \$+2.7\$ AP\${\textasciicircum}\{{\textbackslash}text\{bb\}\}\_\{75\}\$ VOC, \$+1.1\$ AP\${\textasciicircum}\{{\textbackslash}text\{bb\}\}\_\{75\}\$ COCO, and \$+1.9\$ AP\${\textasciicircum}\{{\textbackslash}text\{mk\}\}\$ Cityscapes. Code and pre-trained models are released at: {\textbackslash}url\{https://github.com/Tete-Xiao/ReSim\}},
	urldate = {2021-11-05},
	journal = {arXiv:2103.12902 [cs]},
	author = {Xiao, Tete and Reed, Colorado J. and Wang, Xiaolong and Keutzer, Kurt and Darrell, Trevor},
	month = aug,
	year = {2021},
	note = {arXiv: 2103.12902},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/5UJKTLYG/Xiao et al. - 2021 - Region Similarity Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/JY7C5UD6/2103.html:text/html},
}

@article{oord_representation_2018,
	title = {Representation learning with contrastive predictive coding},
	journal = {arXiv preprint arXiv:1807.03748},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	year = {2018},
	file = {Full Text:/Users/zalan/Zotero/storage/XT7BKKNW/Oord et al. - 2018 - Representation learning with contrastive predictiv.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/Z25B3W3R/1807.html:text/html},
}

@inproceedings{kolesnikov_revisiting_2019-1,
	title = {Revisiting self-supervised visual representation learning},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
	year = {2019},
	pages = {1920--1929},
	file = {Full Text:/Users/zalan/Zotero/storage/FJ3MIJK6/Kolesnikov et al. - 2019 - Revisiting self-supervised visual representation l.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/IGMJ9BD8/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html:text/html},
}

@article{asano_self-labelling_2019-1,
	title = {Self-labelling via simultaneous clustering and representation learning},
	journal = {arXiv preprint arXiv:1911.05371},
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/NSD5W6UF/Asano et al. - 2019 - Self-labelling via simultaneous clustering and rep.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/DZN5Z5H6/1911.html:text/html},
}

@article{becker_self-organizing_1992-1,
	title = {Self-organizing neural network that discovers surfaces in random-dot stereograms},
	volume = {355},
	number = {6356},
	journal = {Nature},
	author = {Becker, Suzanna and Hinton, Geoffrey E.},
	year = {1992},
	note = {Publisher: Nature Publishing Group},
	pages = {161--163},
	file = {Full Text:/Users/zalan/Zotero/storage/GP6R37D5/Becker and Hinton - 1992 - Self-organizing neural network that discovers surf.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/KJY73HJJ/355161a0.html:text/html},
}

@article{goyal_self-supervised_2021-1,
	title = {Self-supervised pretraining of visual features in the wild},
	journal = {arXiv preprint arXiv:2103.01988},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/PVH2LRMK/Goyal et al. - 2021 - Self-supervised pretraining of visual features in .pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/IFKRSIMK/2103.html:text/html},
}

@inproceedings{xie_self-training_2020-1,
	title = {Self-training with noisy student improves imagenet classification},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2020},
	pages = {10687--10698},
	file = {Full Text:/Users/zalan/Zotero/storage/C4PDTVF2/Xie et al. - 2020 - Self-training with noisy student improves imagenet.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/4CAUAEX9/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.html:text/html},
}

@article{tishby_information_2000-1,
	title = {The information bottleneck method},
	journal = {arXiv preprint physics/0004057},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {2000},
	file = {Full Text:/Users/zalan/Zotero/storage/YBHFMQX7/Tishby et al. - 2000 - The information bottleneck method.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/3TLLVF7N/0004057.html:text/html},
}

@article{tian_understanding_2021-1,
	title = {Understanding self-supervised learning dynamics without contrastive pairs},
	journal = {arXiv preprint arXiv:2102.06810},
	author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/V4FWCX2W/Tian et al. - 2021 - Understanding self-supervised learning dynamics wi.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/SD95MIPL/2102.html:text/html},
}

@article{caron_unsupervised_2020,
	title = {Unsupervised learning of visual features by contrasting cluster assignments},
	journal = {arXiv preprint arXiv:2006.09882},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/77WS45DX/Caron et al. - 2020 - Unsupervised learning of visual features by contra.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/EHPCRU7Y/2006.html:text/html},
}

@article{dosovitskiy_image_2020-1,
	title = {An image is worth 16x16 words: {Transformers} for image recognition at scale},
	shorttitle = {An image is worth 16x16 words},
	journal = {arXiv preprint arXiv:2010.11929},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/N546RUF7/Dosovitskiy et al. - 2020 - An image is worth 16x16 words Transformers for im.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/E7WSU5L2/2010.html:text/html},
}

@inproceedings{vaswani_attention_2017-1,
	title = {Attention is all you need},
	booktitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
	file = {Full Text:/Users/zalan/Zotero/storage/ZSHGG3WQ/Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf},
}

@article{devlin_bert_2018-1,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	file = {Full Text:/Users/zalan/Zotero/storage/C6NQKQRF/Devlin et al. - 2018 - Bert Pre-training of deep bidirectional transform.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/GJT8ZD8C/1810.html:text/html},
}

@article{child_generating_2019-1,
	title = {Generating long sequences with sparse transformers},
	journal = {arXiv preprint arXiv:1904.10509},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/AT2VYMQF/Child et al. - 2019 - Generating long sequences with sparse transformers.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/JSDSA6VN/1904.html:text/html},
}

@article{yang_ouroboros_2019,
	title = {Ouroboros: {On} accelerating training of transformer-based language models},
	shorttitle = {Ouroboros},
	journal = {arXiv preprint arXiv:1909.06695},
	author = {Yang, Qian and Huo, Zhouyuan and Wang, Wenlin and Huang, Heng and Carin, Lawrence},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/YD725VLC/Yang et al. - 2019 - Ouroboros On accelerating training of transformer.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/V67II7TW/1909.html:text/html},
}

@article{jaegle_perceiver_2021-1,
	title = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
	shorttitle = {Perceiver},
	url = {https://arxiv.org/abs/2103.03206v2},
	abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
	language = {en},
	urldate = {2021-11-10},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	month = mar,
	year = {2021},
	file = {Snapshot:/Users/zalan/Zotero/storage/7UHYN6FL/2103.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/RYEI69YW/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf:application/pdf},
}

@inproceedings{shen_powernorm_2020-1,
	title = {Powernorm: {Rethinking} batch normalization in transformers},
	shorttitle = {Powernorm},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael and Keutzer, Kurt},
	year = {2020},
	pages = {8741--8751},
	file = {Full Text:/Users/zalan/Zotero/storage/NN4YXMWI/Shen et al. - 2020 - Powernorm Rethinking batch normalization in trans.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/HVX7BA7Z/shen20e.html:text/html},
}

@article{lu_pretrained_2021-1,
	title = {Pretrained transformers as universal computation engines},
	journal = {arXiv preprint arXiv:2103.05247},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/JH52X7KN/Lu et al. - 2021 - Pretrained transformers as universal computation e.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/BVMDY65U/2103.html:text/html},
}

@article{liu_swin_2021-1,
	title = {Swin transformer: {Hierarchical} vision transformer using shifted windows},
	shorttitle = {Swin transformer},
	journal = {arXiv preprint arXiv:2103.14030},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
}

@article{fedus_switch_2021-1,
	title = {Switch transformers: {Scaling} to trillion parameter models with simple and efficient sparsity},
	shorttitle = {Switch transformers},
	journal = {arXiv preprint arXiv:2101.03961},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/3V7YGV5F/Fedus et al. - 2021 - Switch transformers Scaling to trillion parameter.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/HIZIK8MI/2101.html:text/html},
}

@inproceedings{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	year = {2021},
	pages = {10347--10357},
	file = {Full Text:/Users/zalan/Zotero/storage/ETTBIIAT/Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/7W9YGJNB/touvron21a.html:text/html},
}

@article{jiang_transgan_2021-1,
	title = {Transgan: {Two} transformers can make one strong gan},
	volume = {1},
	shorttitle = {Transgan},
	number = {3},
	journal = {arXiv preprint arXiv:2102.07074},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/ZSJME487/Jiang et al. - 2021 - Transgan Two transformers can make one strong gan.pdf:application/pdf},
}

@article{yuan_volo_2021-1,
	title = {Volo: {Vision} outlooker for visual recognition},
	shorttitle = {Volo},
	journal = {arXiv preprint arXiv:2106.13112},
	author = {Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/SWK8DLNF/Yuan et al. - 2021 - Volo Vision outlooker for visual recognition.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/BM5ZEGF8/2106.html:text/html},
}

@article{el-nouby_xcit_2021-1,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	shorttitle = {{XCiT}},
	journal = {arXiv preprint arXiv:2106.09681},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob},
	year = {2021},
	file = {Full Text:/Users/zalan/Zotero/storage/S929RZF6/El-Nouby et al. - 2021 - XCiT Cross-Covariance Image Transformers.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/KC6Y5P6A/2106.html:text/html},
}

@article{yao_adahessian_2020-1,
	title = {{ADAHESSIAN}: {An} adaptive second order optimizer for machine learning},
	shorttitle = {{ADAHESSIAN}},
	journal = {arXiv preprint arXiv:2006.00719},
	author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	file = {Full Text:/Users/zalan/Zotero/storage/4EW6E43M/Yao et al. - 2020 - ADAHESSIAN An adaptive second order optimizer for.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/ZFJ3J9UN/2006.html:text/html},
}

@inproceedings{ghorbani_investigation_2019-1,
	title = {An investigation into neural net optimization via hessian eigenvalue density},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	year = {2019},
	pages = {2232--2241},
	file = {Full Text:/Users/zalan/Zotero/storage/8H3RV4HZ/Ghorbani et al. - 2019 - An investigation into neural net optimization via .pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/6AAM4HGE/ghorbani19b.html:text/html},
}

@article{skorski_chain_2019-1,
	title = {Chain rules for hessian and higher derivatives made easy by tensor calculus},
	journal = {arXiv preprint arXiv:1911.13292},
	author = {Skorski, Maciej},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/YLCJY2Y2/Skorski - 2019 - Chain rules for hessian and higher derivatives mad.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/YSF6JIF6/1911.html:text/html},
}

@article{derezinski_distributed_2019-1,
	title = {Distributed estimation of the inverse {Hessian} by determinantal averaging},
	journal = {arXiv preprint arXiv:1905.11546},
	author = {Dereziński, Micha{\textbackslash}l and Mahoney, Michael W.},
	year = {2019},
	file = {Full Text:/Users/zalan/Zotero/storage/8P6USSX8/Dereziński and Mahoney - 2019 - Distributed estimation of the inverse Hessian by d.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/PX2Y4RC7/1905.html:text/html},
}

@article{sagun_eigenvalues_2016,
	title = {Eigenvalues of the hessian in deep learning: {Singularity} and beyond},
	shorttitle = {Eigenvalues of the hessian in deep learning},
	journal = {arXiv preprint arXiv:1611.07476},
	author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
	year = {2016},
	file = {Full Text:/Users/zalan/Zotero/storage/R9LZGIUR/Sagun et al. - 2016 - Eigenvalues of the hessian in deep learning Singu.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/WTUYKBQF/1611.html:text/html},
}

@inproceedings{yao_pyhessian_2020-3,
	title = {Pyhessian: {Neural} networks through the lens of the hessian},
	shorttitle = {Pyhessian},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2020},
	pages = {581--590},
	file = {Snapshot:/Users/zalan/Zotero/storage/5HQWZIKD/9378171.html:text/html},
}

@article{papyan_full_2018,
	title = {The full spectrum of deepnet hessians at scale: {Dynamics} with sgd training and sample size},
	shorttitle = {The full spectrum of deepnet hessians at scale},
	journal = {arXiv preprint arXiv:1811.07062},
	author = {Papyan, Vardan},
	year = {2018},
	file = {Full Text:/Users/zalan/Zotero/storage/A69TXGEV/Papyan - 2018 - The full spectrum of deepnet hessians at scale Dy.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/2UPUIMRL/1811.html:text/html},
}

@inproceedings{oakden-rayner_hidden_2020,
	address = {Toronto Ontario Canada},
	title = {Hidden stratification causes clinically meaningful failures in machine learning for medical imaging},
	isbn = {978-1-4503-7046-2},
	url = {https://dl.acm.org/doi/10.1145/3368555.3384468},
	doi = {10.1145/3368555.3384468},
	abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
	language = {en},
	urldate = {2021-11-10},
	booktitle = {Proceedings of the {ACM} {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {ACM},
	author = {Oakden-Rayner, Luke and Dunnmon, Jared and Carneiro, Gustavo and Re, Christopher},
	month = apr,
	year = {2020},
	pages = {151--159},
	file = {Oakden-Rayner et al. - 2020 - Hidden stratification causes clinically meaningful.pdf:/Users/zalan/Zotero/storage/8ZQTC7DT/Oakden-Rayner et al. - 2020 - Hidden stratification causes clinically meaningful.pdf:application/pdf},
}

@article{zhang_contrastive_2020,
	title = {Contrastive {Learning} of {Medical} {Visual} {Representations} from {Paired} {Images} and {Text}},
	url = {http://arxiv.org/abs/2010.00747},
	abstract = {Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classiﬁcation tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classiﬁcation tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efﬁciency.},
	language = {en},
	urldate = {2021-11-10},
	journal = {arXiv:2010.00747 [cs]},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.00747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Zhang et al. - 2020 - Contrastive Learning of Medical Visual Representat.pdf:/Users/zalan/Zotero/storage/JGRYN5TW/Zhang et al. - 2020 - Contrastive Learning of Medical Visual Representat.pdf:application/pdf},
}

@article{pham_dualnet_2021,
	title = {{DualNet}: {Continual} {Learning}, {Fast} and {Slow}},
	shorttitle = {{DualNet}},
	url = {http://arxiv.org/abs/2110.00175},
	abstract = {According to Complementary Learning Systems (CLS) theory{\textasciitilde}{\textbackslash}citep\{mcclelland1995there\} in neuroscience, humans do effective {\textbackslash}emph\{continual learning\} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named "DualNet", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised representation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complementary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL objectives to validate DualNet's efficacy, robustness, and scalability. Code will be made available upon acceptance.},
	urldate = {2021-11-10},
	journal = {arXiv:2110.00175 [cs]},
	author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
	month = sep,
	year = {2021},
	note = {arXiv: 2110.00175},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/758PELZQ/Pham et al. - 2021 - DualNet Continual Learning, Fast and Slow.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/5BDSN8YP/2110.html:text/html},
}

@article{chaudhry_tiny_2019,
	title = {On {Tiny} {Episodic} {Memories} in {Continual} {Learning}},
	url = {https://arxiv.org/abs/1902.10486v4},
	abstract = {In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7{\textbackslash}\% and 17{\textbackslash}\% when the memory is populated with a single example per class.},
	language = {en},
	urldate = {2021-11-11},
	author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K. and Torr, Philip H. S. and Ranzato, Marc'Aurelio},
	month = feb,
	year = {2019},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/MUMW485Z/Chaudhry et al. - 2019 - On Tiny Episodic Memories in Continual Learning.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/RHL9KZHU/1902.html:text/html},
}

@article{zhou_detecting_2021,
	title = {Detecting {Hallucinated} {Content} in {Conditional} {Neural} {Sequence} {Generation}},
	url = {http://arxiv.org/abs/2011.02593},
	abstract = {Neural sequence models can generate highly ﬂuent sentences, but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input. These variety of ﬂuent but wrong outputs are particularly problematic, as it will not be possible for users to tell they are being presented incorrect content. To detect these errors, we propose a task to predict whether each token in the output sequence is hallucinated (not contained in the input) and collect new manually annotated evaluation sets for this task. We also introduce a method for learning to detect hallucinations using pretrained language models ﬁne tuned on synthetic data that includes automatically inserted hallucinations Experiments on machine translation (MT) and abstractive summarization demonstrate that our proposed approach consistently outperforms strong baselines on all benchmark datasets. We further demonstrate how to use the token-level hallucination labels to deﬁne a ﬁne-grained loss over the target sequence in low-resource MT and achieve signiﬁcant improvements over strong baseline methods.We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1.},
	language = {en},
	urldate = {2021-11-11},
	journal = {arXiv:2011.02593 [cs]},
	author = {Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzman, Paco and Zettlemoyer, Luke and Ghazvininejad, Marjan},
	month = jun,
	year = {2021},
	note = {arXiv: 2011.02593},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Zhou et al. - 2021 - Detecting Hallucinated Content in Conditional Neur.pdf:/Users/zalan/Zotero/storage/TG5ZFR7J/Zhou et al. - 2021 - Detecting Hallucinated Content in Conditional Neur.pdf:application/pdf},
}

@article{chaudhari_prospective_2021,
	title = {Prospective {Deployment} of {Deep} {Learning} in {MRI}: {A} {Framework} for {Important} {Considerations}, {Challenges}, and {Recommendations} for {Best} {Practices}},
	volume = {54},
	issn = {1522-2586},
	shorttitle = {Prospective {Deployment} of {Deep} {Learning} in {MRI}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.27331},
	doi = {10.1002/jmri.27331},
	abstract = {Artificial intelligence algorithms based on principles of deep learning (DL) have made a large impact on the acquisition, reconstruction, and interpretation of MRI data. Despite the large number of retrospective studies using DL, there are fewer applications of DL in the clinic on a routine basis. To address this large translational gap, we review the recent publications to determine three major use cases that DL can have in MRI, namely, that of model-free image synthesis, model-based image reconstruction, and image or pixel-level classification. For each of these three areas, we provide a framework for important considerations that consist of appropriate model training paradigms, evaluation of model robustness, downstream clinical utility, opportunities for future advances, as well recommendations for best current practices. We draw inspiration for this framework from advances in computer vision in natural imaging as well as additional healthcare fields. We further emphasize the need for reproducibility of research studies through the sharing of datasets and software. Level of Evidence 5 Technical Efficacy Stage 2},
	language = {en},
	number = {2},
	urldate = {2021-11-12},
	journal = {Journal of Magnetic Resonance Imaging},
	author = {Chaudhari, Akshay S. and Sandino, Christopher M. and Cole, Elizabeth K. and Larson, David B. and Gold, Garry E. and Vasanawala, Shreyas S. and Lungren, Matthew P. and Hargreaves, Brian A. and Langlotz, Curtis P.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.27331},
	keywords = {artificial intelligence, classification, convolutional neural networks, deep learning, MRI reconstruction, segmentation},
	pages = {357--371},
	file = {Snapshot:/Users/zalan/Zotero/storage/LUWB4THT/jmri.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/3D4NNJQZ/Chaudhari et al. - 2021 - Prospective Deployment of Deep Learning in MRI A .pdf:application/pdf},
}

@article{knoll_assessment_2019-2,
	title = {Assessment of the generalization of learned image reconstruction and the potential for transfer learning},
	volume = {81},
	issn = {1522-2594},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27355},
	doi = {10.1002/mrm.27355},
	abstract = {Purpose Although deep learning has shown great promise for MR image reconstruction, an open question regarding the success of this approach is the robustness in the case of deviations between training and test data. The goal of this study is to assess the influence of image contrast, SNR, and image content on the generalization of learned image reconstruction, and to demonstrate the potential for transfer learning. Methods Reconstructions were trained from undersampled data using data sets with varying SNR, sampling pattern, image contrast, and synthetic data generated from a public image database. The performance of the trained reconstructions was evaluated on 10 in vivo patient knee MRI acquisitions from 2 different pulse sequences that were not used during training. Transfer learning was evaluated by fine-tuning baseline trainings from synthetic data with a small subset of in vivo MR training data. Results Deviations in SNR between training and testing led to substantial decreases in reconstruction image quality, whereas image contrast was less relevant. Trainings from heterogeneous training data generalized well toward the test data with a range of acquisition parameters. Trainings from synthetic, non-MR image data showed residual aliasing artifacts, which could be removed by transfer learning–inspired fine-tuning. Conclusion This study presents insights into the generalization ability of learned image reconstruction with respect to deviations in the acquisition settings between training and testing. It also provides an outlook for the potential of transfer learning to fine-tune trainings to a particular target application using only a small number of training cases.},
	language = {en},
	number = {1},
	urldate = {2021-11-12},
	journal = {Magnetic Resonance in Medicine},
	author = {Knoll, Florian and Hammernik, Kerstin and Kobler, Erich and Pock, Thomas and Recht, Michael P and Sodickson, Daniel K},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.27355},
	keywords = {transfer learning, deep learning, accelerated imaging, iterative image reconstruction, machine learning, variational network},
	pages = {116--128},
	file = {Snapshot:/Users/zalan/Zotero/storage/3UGGYGRQ/mrm.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/2D4JQRC4/Knoll et al. - 2019 - Assessment of the generalization of learned image .pdf:application/pdf},
}

@article{ravula_inverse_2021,
	title = {Inverse {Problems} {Leveraging} {Pre}-trained {Contrastive} {Representations}},
	url = {http://arxiv.org/abs/2110.07439},
	abstract = {We study a new family of inverse problems for recovering representations of corrupted data. We assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x), if we are only given a corrupted version A(x), for some known forward operator A. We propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. Using a linear probe on our robust representations, we achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking. We evaluate on a subset of ImageNet and observe that our method is robust to varying levels of distortion. Our method outperforms end-to-end baselines even with a fraction of the labeled data in a wide range of forward operators.},
	language = {en},
	urldate = {2021-11-12},
	journal = {arXiv:2110.07439 [cs]},
	author = {Ravula, Sriram and Smyrnis, Georgios and Jordan, Matt and Dimakis, Alexandros G.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.07439},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Ravula et al. - 2021 - Inverse Problems Leveraging Pre-trained Contrastiv.pdf:/Users/zalan/Zotero/storage/WFX2P669/Ravula et al. - 2021 - Inverse Problems Leveraging Pre-trained Contrastiv.pdf:application/pdf},
}

@article{desai_vortex_2021,
	title = {{VORTEX}: {Physics}-{Driven} {Data} {Augmentations} for {Consistency} {Training} for {Robust} {Accelerated} {MRI} {Reconstruction}},
	shorttitle = {{VORTEX}},
	url = {http://arxiv.org/abs/2111.02549},
	abstract = {Deep neural networks have enabled improved image quality and fast inference times for various inverse problems, including accelerated magnetic resonance imaging (MRI) reconstruction. However, such models require large amounts of fully-sampled ground truth data, which are difficult to curate and are sensitive to distribution drifts. In this work, we propose applying physics-driven data augmentations for consistency training that leverage our domain knowledge of the forward MRI data acquisition process and MRI physics for improved data efficiency and robustness to clinically-relevant distribution drifts. Our approach, termed VORTEX (1) demonstrates strong improvements over supervised baselines with and without augmentation in robustness to signal-to-noise ratio change and motion corruption in data-limited regimes; (2) considerably outperforms state-of-the-art data augmentation techniques that are purely image-based on both in-distribution and out-of-distribution data; and (3) enables composing heterogeneous image-based and physics-driven augmentations.},
	urldate = {2021-11-13},
	journal = {arXiv:2111.02549 [physics]},
	author = {Desai, Arjun D. and Gunel, Beliz and Ozturkler, Batu M. and Beg, Harris and Vasanawala, Shreyas and Hargreaves, Brian A. and Ré, Christopher and Pauly, John M. and Chaudhari, Akshay S.},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.02549},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/CLMFREIY/Desai et al. - 2021 - VORTEX Physics-Driven Data Augmentations for Cons.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/P9NRX9UM/2111.html:text/html},
}

@article{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2021-11-17},
	journal = {arXiv:2111.06377 [cs]},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.06377},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:/Users/zalan/Zotero/storage/3CAIDDNC/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@article{bao_beit_2021,
	title = {{BEiT}: {BERT} {Pre}-{Training} of {Image} {Transformers}},
	shorttitle = {{BEiT}},
	url = {http://arxiv.org/abs/2106.08254},
	abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
	urldate = {2021-11-17},
	journal = {arXiv:2106.08254 [cs]},
	author = {Bao, Hangbo and Dong, Li and Wei, Furu},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/676XS5IJ/Bao et al. - 2021 - BEiT BERT Pre-Training of Image Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/WJ36EZRV/2106.html:text/html},
}

@article{ma_vaem_nodate,
	title = {{VAEM}: a {Deep} {Generative} {Model} for {Heterogeneous} {Mixed} {Type} {Data}},
	abstract = {Deep generative models often perform poorly in real-world applications due to the heterogeneity of natural data sets. Heterogeneity arises from data containing different types of features (categorical, ordinal, continuous, etc.) and features of the same type having different marginal distributions. We propose an extension of variational autoencoders (VAEs) called VAEM to handle such heterogeneous data. VAEM is a deep generative model that is trained in a two stage manner such that the ﬁrst stage provides a more uniform representation of the data to the second stage, thereby sidestepping the problems caused by heterogeneous data. We provide extensions of VAEM to handle partially observed data, and demonstrate its performance in data generation, missing data prediction and sequential feature selection tasks. Our results show that VAEM broadens the range of real-world applications where deep generative models can be successfully deployed.},
	language = {en},
	author = {Ma, Chao and Tschiatschek, Sebastian and Turner, Richard and Hernandez-Lobato, Jose Miguel and Zhang, Cheng},
	pages = {11},
	file = {Ma et al. - VAEM a Deep Generative Model for Heterogeneous Mi.pdf:/Users/zalan/Zotero/storage/33PMK5AS/Ma et al. - VAEM a Deep Generative Model for Heterogeneous Mi.pdf:application/pdf},
}

@incollection{ma_dual-task_2021,
	address = {Cham},
	title = {Dual-{Task} {Mutual} {Learning} for {Semi}-supervised {Medical} {Image} {Segmentation}},
	volume = {13021},
	isbn = {978-3-030-88009-5 978-3-030-88010-1},
	url = {https://link.springer.com/10.1007/978-3-030-88010-1_46},
	abstract = {The success of deep learning methods in medical image segmentation tasks usually requires a large amount of labeled data. However, obtaining reliable annotations is expensive and time-consuming. Semi-supervised learning has attracted much attention in medical image segmentation by taking the advantage of unlabeled data which is much easier to acquire. In this paper, we propose a novel dual-task mutual learning framework for semi-supervised medical image segmentation. Our framework can be formulated as an integration of two individual segmentation networks based on two tasks: learning region-based shape constraint and learning boundary-based surface mismatch. Diﬀerent from the one-way transfer between teacher and student networks, an ensemble of dual-task students can learn collaboratively and implicitly explore useful knowledge from each other during the training process. By jointly learning the segmentation probability maps and signed distance maps of targets, our framework can enforce the geometric shape constraint and learn more reliable information. Experimental results demonstrate that our method achieves performance gains by leveraging unlabeled data and outperforms the state-of-the-art semi-supervised segmentation methods.},
	language = {en},
	urldate = {2021-11-18},
	booktitle = {Pattern {Recognition} and {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Zhang, Yichi and Zhang, Jicong},
	editor = {Ma, Huimin and Wang, Liang and Zhang, Changshui and Wu, Fei and Tan, Tieniu and Wang, Yaonan and Lai, Jianhuang and Zhao, Yao},
	year = {2021},
	doi = {10.1007/978-3-030-88010-1_46},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {548--559},
	file = {Zhang and Zhang - 2021 - Dual-Task Mutual Learning for Semi-supervised Medi.pdf:/Users/zalan/Zotero/storage/VL6Q7SD5/Zhang and Zhang - 2021 - Dual-Task Mutual Learning for Semi-supervised Medi.pdf:application/pdf},
}

@article{hasan_multi-task_nodate,
	title = {A {Multi}-{Task} {Cross}-{Task} {Learning} {Architecture} for {Ad}-hoc {Uncertainty} {Estimation} in {3D} {Cardiac} {MRI} {Image} {Segmentation}},
	abstract = {Medical image segmentation has signiﬁcantly beneﬁtted thanks to deep learning architectures. Furthermore, semi-supervised learning (SSL) has recently been a growing trend for improving a model’s overall performance by leveraging abundant unlabeled data. Moreover, learning multiple tasks within the same model further improves model generalizability. To generate smooth and accurate segmentation masks from 3D cardiac MR images, we present a Multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justiﬁes the effectiveness of our model for the segmentation of the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR) images. With the incorporation of uncertainty estimates to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to ﬂag low-quality segmentation from a given model.},
	language = {en},
	author = {Hasan, S M Kamrul and Linte, Cristian A},
	pages = {4},
	file = {Hasan and Linte - A Multi-Task Cross-Task Learning Architecture for .pdf:/Users/zalan/Zotero/storage/WKPC3N2I/Hasan and Linte - A Multi-Task Cross-Task Learning Architecture for .pdf:application/pdf},
}

@article{wang_vlmo_nodate,
	title = {{VLMO}: {Uniﬁed} {Vision}-{Language} {Pre}-{Training} with {Mixture}-of-{Modality}-{Experts}},
	abstract = {We present a uniﬁed Vision-Language pretrained Model (VLMO) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Speciﬁcally, we introduce Mixture-of-Modality-Experts (MOME) Transformer, where each block contains a pool of modality-speciﬁc experts and a shared selfattention layer. Because of the modeling ﬂexibility of MOME, pretrained VLMO can be ﬁne-tuned as a fusion encoder for vision-language classiﬁcation tasks, or used as a dual encoder for efﬁcient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMO achieves state-of-the-art results on various vision-language tasks, including VQA and NLVR2. The code and pretrained models are available at https://aka.ms/ vlmo.},
	language = {en},
	author = {Wang, Wenhui and Dong, Li and Wei, Furu},
	pages = {15},
	file = {Wang et al. - VLMO Uniﬁed Vision-Language Pre-Training with Mix.pdf:/Users/zalan/Zotero/storage/E5VLKWWN/Wang et al. - VLMO Uniﬁed Vision-Language Pre-Training with Mix.pdf:application/pdf},
}

@inproceedings{liang_swinir_2021-2,
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	shorttitle = {{SwinIR}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2021-11-18},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
	pages = {1833--1844},
	file = {Snapshot:/Users/zalan/Zotero/storage/QR3ZH7T4/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html:text/html;Full Text PDF:/Users/zalan/Zotero/storage/EWEFCY94/Liang et al. - 2021 - SwinIR Image Restoration Using Swin Transformer.pdf:application/pdf},
}

@article{riquelme_scaling_2021,
	title = {Scaling {Vision} with {Sparse} {Mixture} of {Experts}},
	url = {http://arxiv.org/abs/2106.05974},
	abstract = {Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are "dense", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35\% on ImageNet.},
	urldate = {2021-11-18},
	journal = {arXiv:2106.05974 [cs, stat]},
	author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Pinto, André Susano and Keysers, Daniel and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05974},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zalan/Zotero/storage/FP262VN8/Riquelme et al. - 2021 - Scaling Vision with Sparse Mixture of Experts.pdf:application/pdf;arXiv.org Snapshot:/Users/zalan/Zotero/storage/RTQPLKH3/2106.html:text/html},
}

@article{yang_condconv_2020,
	title = {{CondConv}: {Conditionally} {Parameterized} {Convolutions} for {Efficient} {Inference}},
	shorttitle = {{CondConv}},
	url = {http://arxiv.org/abs/1904.04971},
	abstract = {Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efﬁcient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classiﬁcation and detection tasks. On ImageNet classiﬁcation, our CondConv approach applied to EfﬁcientNet-B0 achieves state-ofthe-art performance of 78.3\% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorﬂow layer and CondConv-EfﬁcientNet models are available at: https://github.com/tensorflow/tpu/tree/master/ models/official/efficientnet/condconv.},
	language = {en},
	urldate = {2021-11-18},
	journal = {arXiv:1904.04971 [cs]},
	author = {Yang, Brandon and Bender, Gabriel and Le, Quoc V. and Ngiam, Jiquan},
	month = sep,
	year = {2020},
	note = {arXiv: 1904.04971},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Yang et al. - 2020 - CondConv Conditionally Parameterized Convolutions.pdf:/Users/zalan/Zotero/storage/CJM73289/Yang et al. - 2020 - CondConv Conditionally Parameterized Convolutions.pdf:application/pdf},
}

@article{wang_deep_nodate,
	title = {Deep {Mixture} of {Experts} via {Shallow} {Embedding}},
	abstract = {Larger networks generally have greater representational power at the cost of increased computational complexity. Sparsifying such networks has been an active area of research but has been generally limited to static regularization or dynamic approaches using reinforcement learning. We explore a mixture of experts (MoE) approach to deep dynamic routing, which activates certain experts in the network on a per-example basis. Our novel DeepMoE architecture increases the representational power of standard convolutional networks by adaptively sparsifying and recalibrating channel-wise features in each convolutional layer. We employ a multi-headed sparse gating network to determine the selection and scaling of channels for each input, leveraging exponential combinations of experts within a single convolutional network. Our proposed architecture is evaluated on four benchmark datasets and tasks, and we show that DeepMoEs are able to achieve higher accuracy with lower computation than standard convolutional networks.},
	language = {en},
	author = {Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E},
	pages = {11},
	file = {Wang et al. - Deep Mixture of Experts via Shallow Embedding.pdf:/Users/zalan/Zotero/storage/NYQ6KE22/Wang et al. - Deep Mixture of Experts via Shallow Embedding.pdf:application/pdf},
}

@article{song_dynamic_nodate,
	title = {Dynamic {Grained} {Encoder} for {Vision} {Transformers}},
	abstract = {Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Speciﬁcally, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a ﬁne-grained representation in discriminative regions while keeping high efﬁciency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40\%-60\% while maintaining comparable performance on image classiﬁcation. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.},
	language = {en},
	author = {Song, Lin and Zhang, Songyang and Liu, Songtao and Li, Zeming and He, Xuming and Sun, Hongbin and Sun, Jian and Zheng, Nanning},
	pages = {14},
	file = {Song et al. - Dynamic Grained Encoder for Vision Transformers.pdf:/Users/zalan/Zotero/storage/9WYQJ3ZD/Song et al. - Dynamic Grained Encoder for Vision Transformers.pdf:application/pdf},
}

@article{hatamizadeh_unetr_2021,
	title = {{UNETR}: {Transformers} for {3D} {Medical} {Image} {Segmentation}},
	shorttitle = {{UNETR}},
	url = {http://arxiv.org/abs/2103.10504},
	abstract = {Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful “U-shaped” network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the ﬁnal semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multiorgan segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:2103.10504 [cs, eess]},
	author = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger and Xu, Daguang},
	month = oct,
	year = {2021},
	note = {arXiv: 2103.10504},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	file = {Hatamizadeh et al. - 2021 - UNETR Transformers for 3D Medical Image Segmentat.pdf:/Users/zalan/Zotero/storage/2K499ZXM/Hatamizadeh et al. - 2021 - UNETR Transformers for 3D Medical Image Segmentat.pdf:application/pdf},
}

@article{cao_swin-unet_2021,
	title = {Swin-{Unet}: {Unet}-like {pure} {Transformer} for {medical} {image} {segmentation}},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:2105.05537},
	author = {Cao, Hu and Wang, Yueyue and Chen, Joy and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi and Wang, Manning},
	month = may,
	year = {2021},
	note = {arXiv: 2105.05537},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{sriram_end--end_2020,
	title = {End-to-{end} {variational} {networks} for {accelerated} {MRI} {reconstruction}},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention}},
	author = {Sriram, Anuroop and Zbontar, Jure and Murrell, Tullie and Defazio, Aaron and Zitnick, C. Lawrence and Yakubova, Nafissa and Knoll, Florian and Johnson, Patricia},
	year = {2020},
	pages = {64--73},
}

@article{eo_kiki-net_2018,
	title = {{KIKI}-net: cross-domain convolutional neural networks for reconstructing undersampled magnetic resonance images},
	volume = {80},
	issn = {1522-2594},
	shorttitle = {{KIKI}-net},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27201},
	doi = {10.1002/mrm.27201},
	abstract = {Purpose To demonstrate accurate MR image reconstruction from undersampled k-space data using cross-domain convolutional neural networks (CNNs) Methods Cross-domain CNNs consist of 3 components: (1) a deep CNN operating on the k-space (KCNN), (2) a deep CNN operating on an image domain (ICNN), and (3) an interleaved data consistency operations. These components are alternately applied, and each CNN is trained to minimize the loss between the reconstructed and corresponding fully sampled k-spaces. The final reconstructed image is obtained by forward-propagating the undersampled k-space data through the entire network. Results Performances of K-net (KCNN with inverse Fourier transform), I-net (ICNN with interleaved data consistency), and various combinations of the 2 different networks were tested. The test results indicated that K-net and I-net have different advantages/disadvantages in terms of tissue-structure restoration. Consequently, the combination of K-net and I-net is superior to single-domain CNNs. Three MR data sets, the T2 fluid-attenuated inversion recovery (T2 FLAIR) set from the Alzheimer's Disease Neuroimaging Initiative and 2 data sets acquired at our local institute (T2 FLAIR and T1 weighted), were used to evaluate the performance of 7 conventional reconstruction algorithms and the proposed cross-domain CNNs, which hereafter is referred to as KIKI-net. KIKI-net outperforms conventional algorithms with mean improvements of 2.29 dB in peak SNR and 0.031 in structure similarity. Conclusion KIKI-net exhibits superior performance over state-of-the-art conventional algorithms in terms of restoring tissue structures and removing aliasing artifacts. The results demonstrate that KIKI-net is applicable up to a reduction factor of 3 to 4 based on variable-density Cartesian undersampling.},
	language = {en},
	number = {5},
	urldate = {2021-12-07},
	journal = {Magnetic Resonance in Medicine},
	author = {Eo, Taejoon and Jun, Yohan and Kim, Taeseong and Jang, Jinseong and Lee, Ho-Joon and Hwang, Dosik},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.27201},
	keywords = {convolutional neural networks, MRI acceleration, cross-domain deep learning, image reconstruction, k-space completion},
	pages = {2188--2201},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/ELVEBB6P/Eo et al. - 2018 - KIKI-net cross-domain convolutional neural networ.pdf:application/pdf;Snapshot:/Users/zalan/Zotero/storage/EYB42KBT/mrm.html:text/html},
}

@article{lopez-paz_gradient_nodate,
	title = {Gradient {Episodic} {Memory} for {Continual} {Learning}},
	abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneﬁcial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
	language = {en},
	author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
	pages = {10},
	file = {Lopez-Paz and Ranzato - Gradient Episodic Memory for Continual Learning.pdf:/Users/zalan/Zotero/storage/MWHNYY9Y/Lopez-Paz and Ranzato - Gradient Episodic Memory for Continual Learning.pdf:application/pdf},
}

@article{rao_continual_2019,
	title = {Continual {Unsupervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1910.14481},
	abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the ﬁeld has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shufﬂed. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efﬁcacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
	language = {en},
	urldate = {2021-12-08},
	journal = {arXiv:1910.14481 [cs, stat]},
	author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.14481},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:/Users/zalan/Zotero/storage/ECQWJTEB/Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:application/pdf},
}

@inproceedings{yoon_federated_2021,
	title = {Federated {Continual} {Learning} with {Weighted} {Inter}-client {Transfer}},
	url = {https://proceedings.mlr.press/v139/yoon21b.html},
	abstract = {There has been a surge of interest in continual learning and federated learning, both of which are important in deep neural networks in real-world scenarios. Yet little research has been done regarding the scenario where each client learns on a sequence of tasks from a private local data stream. This problem of federated continual learning poses new challenges to continual learning, such as utilizing knowledge from other clients, while preventing interference from irrelevant knowledge. To resolve these issues, we propose a novel federated continual learning framework, Federated Weighted Inter-client Transfer (FedWeIT), which decomposes the network weights into global federated parameters and sparse task-specific parameters, and each client receives selective knowledge from other clients by taking a weighted combination of their task-specific parameters. FedWeIT minimizes interference between incompatible tasks, and also allows positive knowledge transfer across clients during learning. We validate our FedWeIT against existing federated learning and continual learning methods under varying degrees of task similarity across clients, and our model significantly outperforms them with a large reduction in the communication cost.},
	language = {en},
	urldate = {2021-12-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yoon, Jaehong and Jeong, Wonyong and Lee, Giwoong and Yang, Eunho and Hwang, Sung Ju},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12073--12086},
	file = {Full Text PDF:/Users/zalan/Zotero/storage/B3EI4B75/Yoon et al. - 2021 - Federated Continual Learning with Weighted Inter-c.pdf:application/pdf},
}

@article{le_federated_2021,
	title = {Federated {Continuous} {Learning} {With} {Broad} {Network} {Architecture}},
	volume = {51},
	issn = {2168-2267, 2168-2275},
	url = {https://ieeexplore.ieee.org/document/9477571/},
	doi = {10.1109/TCYB.2021.3090260},
	abstract = {Federated learning (FL) is a machine-learning setting, where multiple clients collaboratively train a model under the coordination of a central server. The clients’ raw data are locally stored, and each client only uploads the trained weight to the server, which can mitigate the privacy risks from the centralized machine learning. However, most of the existing FL models focus on one-time learning without consideration for continuous learning. Continuous learning supports learning from streaming data continuously, so it can adapt to environmental changes and provide better real-time performance. In this article, we present a federated continuous learning scheme based on broad learning (FCL-BL) to support efﬁcient and accurate federated continuous learning (FCL). In FCL-BL, we propose a weighted processing strategy to solve the catastrophic forgetting problem, so FCL-BL can handle continuous learning. Then, we develop a local-independent training solution to support fast and accurate training in FCL-BL. The proposed solution enables us to avoid using a time-consuming synchronous approach while addressing the inaccurate-training issue rooted in the previous asynchronous approach. Moreover, we introduce a batch-asynchronous approach and broad learning (BL) technique to guarantee the high efﬁciency of FCL-BL. Speciﬁcally, the batch-asynchronous approach reduces the number of client–server interaction rounds, and the BL technique supports incremental learning without retraining when learning newly produced data. Finally, theoretical analysis and experimental results further illustrate that FCL-BL is superior to the existing FL schemes in terms of efﬁciency and accuracy in FCL.},
	language = {en},
	number = {8},
	urldate = {2021-12-09},
	journal = {IEEE Transactions on Cybernetics},
	author = {Le, Junqing and Lei, Xinyu and Mu, Nankun and Zhang, Hengrun and Zeng, Kai and Liao, Xiaofeng},
	month = aug,
	year = {2021},
	pages = {3874--3888},
	file = {Le et al. - 2021 - Federated Continuous Learning With Broad Network A.pdf:/Users/zalan/Zotero/storage/YQQQGHCA/Le et al. - 2021 - Federated Continuous Learning With Broad Network A.pdf:application/pdf},
}

@article{casado_concept_2021,
	title = {Concept drift detection and adaptation for federated and continual learning},
	issn = {1380-7501, 1573-7721},
	url = {http://arxiv.org/abs/2105.13309},
	doi = {10.1007/s11042-021-11219-x},
	abstract = {Smart devices, such as smartphones, wearables, robots, and others, can collect vast amounts of data from their environment. This data is suitable for training machine learning models, which can signiﬁcantly improve their behavior, and therefore, the user experience. Federated learning is a young and popular framework that allows multiple distributed devices to train deep learning models collaboratively while preserving data privacy. Nevertheless, this approach may not be optimal for scenarios where data distribution is nonidentical among the participants or changes over time, causing what is known as concept drift. Little research has yet been done in this ﬁeld, but this kind of situation is quite frequent in real life and poses new challenges to both continual and federated learning. Therefore, in this work, we present a new method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our proposal is an extension of the most popular federated algorithm, Federated Averaging (FedAvg), enhancing it for continual adaptation under concept drift. We empirically demonstrate the weaknesses of regular FedAvg and prove that CDA-FedAvg outperforms it in this type of scenario.},
	language = {en},
	urldate = {2021-12-09},
	journal = {Multimedia Tools and Applications},
	author = {Casado, Fernando E. and Lema, Dylan and Criado, Marcos F. and Iglesias, Roberto and Regueiro, Carlos V. and Barro, Senén},
	month = jul,
	year = {2021},
	note = {arXiv: 2105.13309},
	keywords = {Computer Science - Machine Learning},
	file = {Casado et al. - 2021 - Concept drift detection and adaptation for federat.pdf:/Users/zalan/Zotero/storage/2NSPEJAS/Casado et al. - 2021 - Concept drift detection and adaptation for federat.pdf:application/pdf},
}

@article{hendryx_federated_2021,
	title = {Federated {Reconnaissance}: {Efficient}, {Distributed}, {Class}-{Incremental} {Learning}},
	shorttitle = {Federated {Reconnaissance}},
	url = {http://arxiv.org/abs/2109.00150},
	abstract = {We describe federated reconnaissance, a class of learning problems in which distributed clients learn new concepts independently and communicate that knowledge efﬁciently. In particular, we propose an evaluation framework and methodological baseline for a system in which each client is expected to learn a growing set of classes and communicate knowledge of those classes efﬁciently with other clients, such that, after knowledge merging, the clients should be able to accurately discriminate between classes in the superset of classes observed by the set of clients. We compare a range of learning algorithms for this problem and ﬁnd that prototypical networks are a strong approach in that they are robust to catastrophic forgetting while incorporating new information efﬁciently. Furthermore, we show that the online averaging of prototype vectors is effective for client model merging and requires only a small amount of communication overhead, memory, and update time per class with no gradient-based learning or hyperparameter tuning. Additionally, to put our results in context, we ﬁnd that a simple, prototypical network with four convolutional layers signiﬁcantly outperforms complex, state of the art continual learning algorithms, increasing the accuracy by over 22\% after learning 600 Omniglot classes and over 33\% after learning 20 mini-ImageNet classes incrementally. These results have important implications for federated reconnaissance and continual learning more generally by demonstrating that communicating feature vectors is an efﬁcient, robust, and effective means for distributed, continual learning.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2109.00150 [cs]},
	author = {Hendryx, Sean M. and KC, Dharma Raj and Walls, Bradley and Morrison, Clayton T.},
	month = aug,
	year = {2021},
	note = {arXiv: 2109.00150},
	keywords = {Computer Science - Machine Learning},
	file = {Hendryx et al. - 2021 - Federated Reconnaissance Efficient, Distributed, .pdf:/Users/zalan/Zotero/storage/5NLN3WTP/Hendryx et al. - 2021 - Federated Reconnaissance Efficient, Distributed, .pdf:application/pdf},
}

@article{usmanova_distillation-based_2021,
	title = {A distillation-based approach integrating continual learning and federated learning for pervasive services},
	url = {http://arxiv.org/abs/2109.04197},
	abstract = {Federated Learning, a new machine learning paradigm enhancing the use of edge devices, is receiving a lot of attention in the pervasive community to support the development of smart services. Nevertheless, this approach still needs to be adapted to the speciﬁcity of the pervasive domain. In particular, issues related to continual learning need to be addressed. In this paper, we present a distillation-based approach dealing with catastrophic forgetting in federated learning scenario. Speciﬁcally, Human Activity Recognition tasks are used as a demonstration domain.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2109.04197 [cs]},
	author = {Usmanova, Anastasiia and Portet, François and Lalanda, Philippe and Vega, German},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.04197},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Usmanova et al. - 2021 - A distillation-based approach integrating continua.pdf:/Users/zalan/Zotero/storage/Y7QWV3W9/Usmanova et al. - 2021 - A distillation-based approach integrating continua.pdf:application/pdf},
}

@article{park_tackling_2021,
	title = {Tackling {Dynamics} in {Federated} {Incremental} {Learning} with {Variational} {Embedding} {Rehearsal}},
	url = {http://arxiv.org/abs/2110.09695},
	abstract = {Federated Learning is a fast growing area of ML where the training datasets are extremely distributed, all while dynamically changing over time. Models need to be trained on clients’ devices without any guarantees for either homogeneity or stationarity of the local private data. The need for continual training has also risen, due to the ever-increasing production of in-task data. However, pursuing both directions at the same time is challenging, since client data privacy is a major constraint, especially for rehearsal methods. Herein, we propose a novel algorithm to address the incremental learning process in an FL scenario, based on realistic client enrollment scenarios where clients can drop in or out dynamically. We ﬁrst propose using deep Variational Embeddings that secure the privacy of the client data. Second, we propose a server-side training method that enables a model to rehearse the previously learnt knowledge. Finally, we investigate the performance of federated incremental learning in dynamic client enrollment scenarios. The proposed method shows parity with ofﬂine training on domain-incremental learning, addressing challenges in both the dynamic enrollment of clients and the domain shifting of client data.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2110.09695 [cs]},
	author = {Park, Tae Jin and Kumatani, Kenichi and Dimitriadis, Dimitrios},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09695},
	keywords = {Computer Science - Machine Learning},
	file = {Park et al. - 2021 - Tackling Dynamics in Federated Incremental Learnin.pdf:/Users/zalan/Zotero/storage/GCFEC6DF/Park et al. - 2021 - Tackling Dynamics in Federated Incremental Learnin.pdf:application/pdf},
}

@article{zamir_restormer_2021,
	title = {Restormer: {Efficient} {Transformer} for {high}-{resolution} {image} {restoration}},
	journal = {arXiv:2111.09881},
	author = {Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
	year = {2021},
}

@article{ghesu_self-supervised_2022,
	title = {Self-supervised {Learning} from 100 {Million} {Medical} {Images}},
	url = {http://arxiv.org/abs/2201.01283},
	abstract = {Building accurate and robust artiﬁcial intelligence systems for medical image assessment requires not only the research and design of advanced deep learning models but also the creation of large and curated sets of annotated training examples. Constructing such datasets, however, is often very costly – due to the complex nature of annotation tasks and the high level of expertise required for the interpretation of medical images (e.g., expert radiologists). To counter this limitation, we propose a method for self-supervised learning of rich image features based on contrastive learning and online feature clustering. For this purpose we leverage large training datasets of over 100,000,000 medical images of various modalities, including radiography, computed tomography (CT), magnetic resonance (MR) imaging and ultrasonography. We propose to use these features to guide model training in supervised and hybrid self-supervised/supervised regime on various downstream tasks. We highlight a number of advantages of this strategy on challenging image assessment problems in radiography, CT and MR: 1) Signiﬁcant increase in accuracy compared to the state-of-theart (e.g., AUC boost of 3-7\% for detection of abnormalities from chest radiography scans and hemorrhage detection on brain CT); 2) Acceleration of model convergence during training by up to 85\% compared to using no pretraining (e.g., 83\% when training a model for detection of brain metastases in MR scans); 3) Increase in robustness to various image augmentations, such as intensity variations, rotations or scaling reﬂective of data variation seen in the ﬁeld.},
	language = {en},
	urldate = {2022-01-07},
	journal = {arXiv:2201.01283 [cs]},
	author = {Ghesu, Florin C. and Georgescu, Bogdan and Mansoor, Awais and Yoo, Youngjin and Neumann, Dominik and Patel, Pragneshkumar and Vishwanath, R. S. and Balter, James M. and Cao, Yue and Grbic, Sasa and Comaniciu, Dorin},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.01283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ghesu et al. - 2022 - Self-supervised Learning from 100 Million Medical .pdf:/Users/zalan/Zotero/storage/8M68SH4M/Ghesu et al. - 2022 - Self-supervised Learning from 100 Million Medical .pdf:application/pdf},
}

@article{ongie_deep_2020-2,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	volume = {1},
	issn = {2641-8770},
	doi = {10.1109/JSAIT.2020.2991563},
	abstract = {Recent work in machine learning shows that deep neural networks can be used to solve a wide variety of inverse problems arising in computational imaging. We explore the central prevailing themes of this emerging area and present a taxonomy that can be used to categorize different problems and reconstruction methods. Our taxonomy is organized along two central axes: (1) whether or not a forward model is known and to what extent it is used in training and testing, and (2) whether or not the learning is supervised or unsupervised, i.e., whether or not the training relies on access to matched ground truth image and measurement pairs. We also discuss the tradeoffs associated with these different reconstruction approaches, caveats and common failure modes, plus open problems and avenues for future work.},
	number = {1},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Journal on Selected Areas in Information Theory},
	keywords = {Training, Deep learning, image reconstruction, computational imaging, Computational modeling, deep neural networks, Image reconstruction, image restoration, Imaging, Interpolation, inverse problems, Inverse problems, Machine learning},
	pages = {39--56},
	file = {IEEE Xplore Full Text PDF:/Users/zalan/Zotero/storage/6DVBF3R7/Ongie et al. - 2020 - Deep Learning Techniques for Inverse Problems in I.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/zalan/Zotero/storage/5NHS22QG/9084378.html:text/html},
}

@article{ramzi2020xpdnet,
	title={{XPDNet} for {MRI} Reconstruction: an application to the 2020 {fastMRI} {Challenge}},
	author={Ramzi, Zaccharie and Ciuciu, Philippe and Starck, Jean-Luc},
	journal={arXiv preprint arXiv:2010.07290},
	year={2020}
}

@article{hammernik2019sigma,
	title={{$\Sigma$}-net: Systematic Evaluation of Iterative Deep Neural Networks for Fast Parallel {MR} Image Reconstruction},
	author={Hammernik, Kerstin and Schlemper, Jo and Qin, Chen and Duan, Jinming and Summers, Ronald M and Rueckert, Daniel},
	journal={arXiv preprint arXiv:1912.09278},
	year={2019}
}

@article{muckley2021results,
	title={Results of the 2020 {fastMRI} challenge for machine learning {MR} image reconstruction},
	author={Muckley, Matthew J and Riemenschneider, Bruno and Radmanesh, Alireza and Kim, Sunwoo and Jeong, Geunu and Ko, Jingyu and Jun, Yohan and Shin, Hyungseob and Hwang, Dosik and Mostapha, Mahmoud and others},
	journal={IEEE transactions on Medical Imaging},
	volume={40},
	number={9},
	pages={2306--2317},
	year={2021}
}

@inproceedings{ronneberger2015u,
	title={U-net: Convolutional networks for biomedical image segmentation},
	author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages={234--241},
	year={2015}
}

@article{hyun2018deep,
	title={Deep learning for undersampled {MRI} reconstruction},
	author={Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
	journal={Physics in Medicine \& Biology},
	volume={63},
	number={13},
	pages={135007},
	year={2018},
}

@article{han2018framing,
	title={Framing {U-Net} via deep convolutional framelets: {Application} to sparse-view {CT}},
	author={Han, Yoseob and Ye, Jong Chul},
	journal={IEEE Transactions on Medical Imaging},
	volume={37},
	number={6},
	pages={1418--1429},
	year={2018}
}

@inproceedings{cciccek20163d,
	title={{3D} {U-Net}: {Learning} dense volumetric segmentation from sparse annotation},
	author={{\c{C}}i{\c{c}}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
	booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages={424--432},
	year={2016},
	organization={Springer}
}

@incollection{zhou2018unet++,
	title={Unet++: A nested {U-net} architecture for medical image segmentation},
	author={Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
	booktitle={Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
	pages={3--11},
	year={2018},
}

@article{sun2016deep,
	title={Deep {ADMM-Net} for compressive sensing {MRI}},
	author={Sun, Jian and Li, Huibin and Xu, Zongben and others},
	journal={Advances in Neural Information Processing Systems},
	volume={29},
	year={2016}
}

@article{hammernik2018learning,
	title={Learning a variational network for reconstruction of accelerated {MRI} data},
	author={Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P and Sodickson, Daniel K and Pock, Thomas and Knoll, Florian},
	journal={Magnetic Resonance in Medicine},
	volume={79},
	number={6},
	pages={3055--3071},
	year={2018},
}

@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}

@article{liu2019roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}

@article{radford2018improving,
	title={Improving language understanding by generative pre-training},
	author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year={2018},
	note={Technical paper}
}

@inproceedings{carion2020end,
	title={End-to-end object detection with {Transformers}},
	author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	booktitle={European Conference on Computer Vision},
	pages={213--229},
	year={2020},
	organization={Springer}
}

@inproceedings{wang2021pyramid,
	title={Pyramid {Vision} {Transformer}: {A} versatile backbone for dense prediction without convolutions},
	author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={568--578},
	year={2021}
}

@article{cao2021video,
	title={Video super-resolution {Transformer}},
	author={Cao, Jiezhang and Li, Yawei and Zhang, Kai and Van Gool, Luc},
	journal={arXiv preprint arXiv:2106.06847},
	year={2021}
}

@inproceedings{chen2021pre,
	title={Pre-trained image processing {Transformer}},
	author={Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={12299--12310},
	year={2021}
}

@inproceedings{heo2021rethinking,
	title={Rethinking spatial dimensions of {Vision} {Transformers}},
	author={Heo, Byeongho and Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Choe, Junsuk and Oh, Seong Joon},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={11936--11945},
	year={2021}
}

@article{wang2021uformer,
	title={Uformer: A general {U-shaped} {Transformer} for image restoration},
	author={Wang, Zhendong and Cun, Xiaodong and Bao, Jianmin and Liu, Jianzhuang},
	journal={arXiv preprint arXiv:2106.03106},
	year={2021}
}

@article{zhou2021nnformer,
	title={{nnFormer}: {Interleaved} {Transformer} for Volumetric Segmentation},
	author={Zhou, Hong-Yu and Guo, Jiansen and Zhang, Yinghao and Yu, Lequan and Wang, Liansheng and Yu, Yizhou},
	journal={arXiv preprint arXiv:2109.03201},
	year={2021}
}

@article{huang2021missformer,
	title={MISSFormer: An effective medical image segmentation {Transformer}},
	author={Huang, Xiaohong and Deng, Zhifang and Li, Dandan and Yuan, Xueguang},
	journal={arXiv preprint arXiv:2109.07162},
	year={2021}
}

@inproceedings{deng2009imagenet,
	title={Imagenet: A large-scale hierarchical image database},
	author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	booktitle={2009 IEEE conference on computer vision and pattern recognition},
	pages={248--255},
	year={2009},
	organization={Ieee}
}
  
@article{wu2022d,
	title={{D-Former}: {A} {U-shaped} Dilated {Transformer} for {3D} Medical Image Segmentation},
	author={Wu, Yixuan and Liao, Kuanlun and Chen, Jintai and Chen, Danny Z and Wang, Jinhong and Gao, Honghao and Wu, Jian},
	journal={arXiv preprint arXiv:2201.00462},
	year={2022}
}

@inproceedings{wang2021ted,
	title={{TED-net}: {Convolution-free} {T2T} {Vision}{Transformer-based} Encoder-decoder Dilation network for Low-dose {CT} Denoising},
	author={Wang, Dayang and Wu, Zhan and Yu, Hengyong},
	booktitle={International Workshop on Machine Learning in Medical Imaging},
	pages={416--425},
	year={2021}
}

@article{luthra2021eformer,
	title={Eformer: Edge Enhancement based {Transformer} for Medical Image Denoising},
	author={Luthra, Achleshwar and Sulakhe, Harsh and Mittal, Tanish and Iyer, Abhishek and Yadav, Santosh},
	journal={arXiv preprint arXiv:2109.08044},
	year={2021}
}

@article{zhang2021spatial,
	title={Spatial adaptive and {Transformer} fusion network {(STFNet)} for low-count {PET} blind denoising with {MRI}},
	author={Zhang, Lipei and Xiao, Zizheng and Zhou, Chao and Yuan, Jianmin and He, Qiang and Yang, Yongfeng and Liu, Xin and Liang, Dong and Zheng, Hairong and Fan, Wei and others},
	journal={Medical Physics},
	year={2021}
}

@article{huang2022swin,
	title={Swin {Transformer} for Fast {MRI}},
	author={Huang, Jiahao and Fang, Yingying and Wu, Yinzhe and Wu, Huanjun and Gao, Zhifan and Li, Yang and Del Ser, Javier and Xia, Jun and Yang, Guang},
	journal={arXiv preprint arXiv:2201.03230},
	year={2022}
}

@inproceedings{feng2021task,
	title={Task {T}ransformer network for joint {MRI} reconstruction and super-resolution},
	author={Feng, Chun-Mei and Yan, Yunlu and Fu, Huazhu and Chen, Li and Xu, Yong},
	booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages={307--317},
	year={2021}
}

@article{uecker2014espirit,
	title={{ESPIRiT}—an eigenvalue approach to autocalibrating parallel {MRI}: where {SENSE} meets {GRAPPA}},
	author={Uecker, Martin and Lai, Peng and Murphy, Mark J and Virtue, Patrick and Elad, Michael and Pauly, John M and Vasanawala, Shreyas S and Lustig, Michael},
	journal={Magnetic Resonance in Medicine},
	volume={71},
	number={3},
	pages={990--1001},
	year={2014},
}

@misc{fastMRIleaderboard,
	author={URL},
	title = {fastMRI Public Leaderboard},
	howpublished = {\url{https://fastmri.org/leaderboards}},
	note = {Accessed: 02/24/2022}
}

@misc{website:Stanford2D,
	title = {{{Stanford 2D FSE}}},
	author={Cheng, Joseph Y.},
	howpublished = {http://mridata.org/list?project=Stanford 2D FSE},
}

@article{mridata:sawyer2013creation,
	title={Creation of fully sampled {MR} data repository for compressed sensing of the knee},
	author={Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas},
	year={2013},
	publisher={Citeseer}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{bai2019deep,
	title={Deep equilibrium models},
	author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}

@article{gilton2021deep,
	title={Deep equilibrium architectures for inverse problems in imaging},
	author={Gilton, Davis and Ongie, Gregory and Willett, Rebecca},
	journal={IEEE Transactions on Computational Imaging},
	volume={7},
	pages={1123--1133},
	year={2021},
	publisher={IEEE}
}

@inproceedings{
	lin2022vision,
	title={Vision {T}ransformers Enable Fast and Robust Accelerated {MRI}},
	author={Kang Lin and Reinhard Heckel},
	booktitle={Medical Imaging with Deep Learning},
	year={2022}
}

@article{korkmaz2022unsupervised,
	title={Unsupervised {MRI} reconstruction via zero-shot learned adversarial {T}ransformers},
	author={Korkmaz, Yilmaz and Dar, Salman UH and Yurt, Mahmut and {\"O}zbey, Muzaffer and Cukur, Tolga},
	journal={IEEE Transactions on Medical Imaging},
	year={2022},
	publisher={IEEE}
}

@inproceedings{wang2003multiscale,
	title={Multiscale structural similarity for image quality assessment},
	author={Wang, Zhou and Simoncelli, Eero P and Bovik, Alan C},
	booktitle={The Thrity-Seventh Asilomar Conference on Signals, Systems \& Computers, 2003},
	volume={2},
	pages={1398--1402},
	year={2003},
	organization={Ieee}
}

@article{klug2022scaling,
	title={Scaling Laws For Deep Learning Based Image Reconstruction},
	author={Klug, Tobit and Heckel, Reinhard},
	journal={arXiv preprint arXiv:2209.13435},
	year={2022}
}

@inproceedings{blau2018perception,
	title={The perception-distortion tradeoff},
	author={Blau, Yochai and Michaeli, Tomer},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={6228--6237},
	year={2018}
}

@article{chung2022diffusion,
	title={Diffusion posterior sampling for general noisy inverse problems},
	author={Chung, Hyungjin and Kim, Jeongsol and Mccann, Michael T and Klasky, Marc L and Ye, Jong Chul},
	journal={arXiv preprint arXiv:2209.14687},
	year={2022}
}

@inproceedings{tran2021explore,
	title={Explore image deblurring via encoded blur kernel space},
	author={Tran, Phong and Tran, Anh Tuan and Phung, Quynh and Hoai, Minh},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={11956--11965},
	year={2021}
}
