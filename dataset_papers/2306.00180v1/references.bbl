\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Yu et~al.(2021)Yu, Ye, Tancik, and Kanazawa]{yu2020pixelnerf}
Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
\newblock {pixelNeRF}: {N}eural radiance fields from one or few images.
\newblock In \emph{Proc. CVPR}, 2021.

\bibitem[Niemeyer et~al.(2020)Niemeyer, Mescheder, Oechsle, and
  Geiger]{niemeyer2020dvr}
Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger.
\newblock Differentiable volumetric rendering: Learning implicit 3d
  representations without 3d supervision.
\newblock In \emph{Proc. CVPR}, 2020.

\bibitem[Du et~al.(2023)Du, Smith, Tewari, and Sitzmann]{du2023cross}
Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann.
\newblock Learning to render novel views from wide-baseline stereo pairs.
\newblock In \emph{Proc. CVPR}, 2023.

\bibitem[Trevithick and Yang(2021)]{trevithick2021grf}
Alex Trevithick and Bo~Yang.
\newblock Grf: Learning a general radiance field for 3d representation and
  rendering.
\newblock In \emph{Proc. ICCV}, pages 15182--15192, 2021.

\bibitem[Suhail et~al.(2022)Suhail, Esteves, Sigal, and
  Makadia]{suhail2022generalizable}
Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia.
\newblock Generalizable patch-based neural rendering.
\newblock In \emph{European Conference on Computer Vision}. Springer, 2022.

\bibitem[Chibane et~al.(2021)Chibane, Bansal, Lazova, and
  Pons-Moll]{chibane2021stereo}
Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll.
\newblock Stereo radiance fields (srf): Learning view synthesis for sparse
  views of novel scenes.
\newblock In \emph{Proc. CVPR}, pages 7911--7920, 2021.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Wang, Genova, Srinivasan, Zhou,
  Barron, Martin-Brualla, Snavely, and Funkhouser]{wang2021ibrnet}
Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou,
  Jonathan~T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas
  Funkhouser.
\newblock Ibrnet: Learning multi-view image-based rendering.
\newblock In \emph{CVPR}, 2021{\natexlab{a}}.

\bibitem[Chan et~al.(2023)Chan, Nagano, Chan, Bergman, Park, Levy, Aittala,
  De~Mello, Karras, and Wetzstein]{chan2023generative}
Eric~R Chan, Koki Nagano, Matthew~A Chan, Alexander~W Bergman, Jeong~Joon Park,
  Axel Levy, Miika Aittala, Shalini De~Mello, Tero Karras, and Gordon
  Wetzstein.
\newblock Generative novel view synthesis with 3d-aware diffusion models.
\newblock \emph{arXiv preprint arXiv:2304.02602}, 2023.

\bibitem[Zhou and Tulsiani(2022)]{zhou2022sparsefusion}
Zhizhuo Zhou and Shubham Tulsiani.
\newblock Sparsefusion: Distilling view-conditioned diffusion for 3d
  reconstruction.
\newblock \emph{Proc. CVPR}, 2022.

\bibitem[Godard et~al.(2017)Godard, Mac~Aodha, and
  Brostow]{godard2017unsupervised}
Cl{\'e}ment Godard, Oisin Mac~Aodha, and Gabriel~J Brostow.
\newblock Unsupervised monocular depth estimation with left-right consistency.
\newblock In \emph{Proc. CVPR}, pages 270--279, 2017.

\bibitem[Godard et~al.(2019{\natexlab{a}})Godard, Mac~Aodha, Firman, and
  Brostow]{godard2019digging}
Cl{\'e}ment Godard, Oisin Mac~Aodha, Michael Firman, and Gabriel~J Brostow.
\newblock Digging into self-supervised monocular depth estimation.
\newblock In \emph{CVPR}, 2019{\natexlab{a}}.

\bibitem[Campos et~al.(2021{\natexlab{a}})Campos, Elvira, Rodr{\'\i}guez,
  Montiel, and Tard{\'o}s]{campos2021orb}
Carlos Campos, Richard Elvira, Juan J~G{\'o}mez Rodr{\'\i}guez, Jos{\'e}~MM
  Montiel, and Juan~D Tard{\'o}s.
\newblock Orb-slam3: An accurate open-source library for visual,
  visual--inertial, and multimap slam.
\newblock \emph{IEEE Transactions on Robotics}, 37\penalty0 (6):\penalty0
  1874--1890, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2022)Xie, Takikawa, Saito, Litany, Yan, Khan, Tombari,
  Tompkin, Sitzmann, and Sridhar]{xie2021neuralfield}
Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or~Litany, Shiqin Yan, Numair
  Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar.
\newblock Neural fields in visual computing and beyond.
\newblock In \emph{Proc. EUROGRAPHICS STAR}, 2022.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein]{sitzmann2019siren}
Vincent Sitzmann, Julien~N.P. Martel, Alexander~W. Bergman, David~B. Lindell,
  and Gordon Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock In \emph{Proc. NeurIPS}, 2020.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin
  Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{Proc. NeurIPS}, 2020.

\bibitem[Nguyen-Phuoc et~al.(2018)Nguyen-Phuoc, Li, Balaban, and
  Yang]{nguyen2018rendernet}
Thu~H Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang.
\newblock Rendernet: A deep convolutional network for differentiable rendering
  from 3d shapes.
\newblock In \emph{Proc. NeurIPS}, volume~31, 2018.

\bibitem[Sitzmann et~al.(2019{\natexlab{a}})Sitzmann, Thies, Heide,
  Nie{\ss}ner, Wetzstein, and Zollh{\"o}fer]{sitzmann2019deepvoxels}
Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie{\ss}ner, Gordon
  Wetzstein, and Michael Zollh{\"o}fer.
\newblock Deepvoxels: Learning persistent 3d feature embeddings.
\newblock In \emph{Proc. CVPR}, 2019{\natexlab{a}}.

\bibitem[Sitzmann et~al.(2019{\natexlab{b}})Sitzmann, Zollh{\"o}fer, and
  Wetzstein]{sitzmann2019srns}
Vincent Sitzmann, Michael Zollh{\"o}fer, and Gordon Wetzstein.
\newblock Scene representation networks: Continuous 3d-structure-aware neural
  scene representations.
\newblock In \emph{Proc. NeurIPS}, volume~32, 2019{\natexlab{b}}.

\bibitem[Mildenhall et~al.(2020)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng]{mildenhall2020nerf}
Ben Mildenhall, Pratul~P Srinivasan, Matthew Tancik, Jonathan~T Barron, Ravi
  Ramamoorthi, and Ren Ng.
\newblock {NeRF}: {R}epresenting scenes as neural radiance fields for view
  synthesis.
\newblock In \emph{Proc. ECCV}, pages 405--421, 2020.

\bibitem[Tewari et~al.(2020)Tewari, Fried, Thies, Sitzmann, Lombardi,
  Sunkavalli, Martin-Brualla, Simon, Saragih, Nie{\ss}ner,
  et~al.]{tewari2020state}
Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi,
  Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih,
  Matthias Nie{\ss}ner, et~al.
\newblock State of the art on neural rendering.
\newblock In \emph{Computer Graphics Forum}, volume~39, pages 701--727. Wiley
  Online Library, 2020.

\bibitem[Sharma et~al.()Sharma, Tewari, Du, Zakharov, Ambrus, Gaidon, Freeman,
  Durand, Tenenbaum, and Sitzmann]{sharma2022neural}
Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov, Rares~Andrei Ambrus,
  Adrien Gaidon, William~T Freeman, Fredo Durand, Joshua~B Tenenbaum, and
  Vincent Sitzmann.
\newblock Neural groundplans: Persistent neural scene representations from a
  single image.
\newblock In \emph{Proc. ICLR}.

\bibitem[Sajjadi et~al.(2021)Sajjadi, Meyer, Pot, Bergmann, Greff, Radwan,
  Vora, Lucic, Duckworth, Dosovitskiy, et~al.]{sajjadi2021scene}
Mehdi~SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha
  Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy,
  et~al.
\newblock Scene representation transformer: {G}eometry-free novel view
  synthesis through set-latent scene representations.
\newblock \emph{arXiv preprint arXiv:2111.13152}, 2021.

\bibitem[Sajjadi et~al.(2023)Sajjadi, Mahendran, Kipf, Pot, Duckworth,
  Lu{\v{c}}i{\'c}, and Greff]{sajjadi2022rust}
Mehdi S.~M. Sajjadi, Aravindh Mahendran, Thomas Kipf, Etienne Pot, Daniel
  Duckworth, Mario Lu{\v{c}}i{\'c}, and Klaus Greff.
\newblock {RUST: Latent Neural Scene Representations from Unposed Imagery}.
\newblock \emph{CVPR}, 2023.

\bibitem[Hartley and Zisserman(2003)]{hartley2003multiple}
Richard Hartley and Andrew Zisserman.
\newblock \emph{Multiple view geometry in computer vision}.
\newblock Cambridge university press, 2003.

\bibitem[Schonberger and Frahm(2016)]{schonberger2016structure}
Johannes~L Schonberger and Jan-Michael Frahm.
\newblock Structure-from-motion revisited.
\newblock In \emph{Proc. CVPR}, 2016.

\bibitem[Agarwal et~al.(2011)Agarwal, Furukawa, Snavely, Simon, Curless, Seitz,
  and Szeliski]{agarwal2011building}
Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless,
  Steven~M Seitz, and Richard Szeliski.
\newblock Building rome in a day.
\newblock \emph{Communications of the ACM}, 54\penalty0 (10):\penalty0
  105--112, 2011.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Wu, Xie, Chen, and
  Prisacariu]{wang2021nerf}
Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor~Adrian Prisacariu.
\newblock Nerf--: Neural radiance fields without known camera parameters.
\newblock \emph{arXiv preprint arXiv:2102.07064}, 2021{\natexlab{b}}.

\bibitem[Yen-Chen et~al.(2021)Yen-Chen, Florence, Barron, Rodriguez, Isola, and
  Lin]{yen2021inerf}
Lin Yen-Chen, Pete Florence, Jonathan~T Barron, Alberto Rodriguez, Phillip
  Isola, and Tsung-Yi Lin.
\newblock inerf: Inverting neural radiance fields for pose estimation.
\newblock In \emph{Proc. IROS}, pages 1323--1330. IEEE, 2021.

\bibitem[Lin et~al.(2021)Lin, Ma, Torralba, and Lucey]{lin2021barf}
Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey.
\newblock Barf: Bundle-adjusting neural radiance fields.
\newblock In \emph{Proc. CVPR}, 2021.

\bibitem[Levy et~al.(2023)Levy, Matthews, Sela, Wetzstein, and
  Lagun]{levy2023melon}
Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, and Dmitry Lagun.
\newblock Melon: Nerf with unposed images using equivalence class estimation.
\newblock \emph{arXiv preprint arXiv:2303.08096}, 2023.

\bibitem[Mur-Artal et~al.(2015)Mur-Artal, Montiel, and Tardos]{mur2015orb}
Raul Mur-Artal, Jose Maria~Martinez Montiel, and Juan~D Tardos.
\newblock Orb-slam: a versatile and accurate monocular slam system.
\newblock 2015.

\bibitem[Engel et~al.(2014)Engel, Sch{\"o}ps, and Cremers]{engel2014lsd}
Jakob Engel, Thomas Sch{\"o}ps, and Daniel Cremers.
\newblock Lsd-slam: Large-scale direct monocular slam.
\newblock In \emph{Proc. ECCV}, 2014.

\bibitem[Teed and Deng(2021)]{teed2021droid}
Zachary Teed and Jia Deng.
\newblock Droid-slam: Deep visual slam for monocular, stereo, and rgb-d
  cameras.
\newblock 2021.

\bibitem[Jatavallabhula et~al.(2020)Jatavallabhula, Iyer, and
  Paull]{jatavallabhula2020slam}
Krishna~Murthy Jatavallabhula, Ganesh Iyer, and Liam Paull.
\newblock Grad-slam: Dense slam meets automatic differentiation.
\newblock In \emph{Proc. ICRA}. IEEE, 2020.

\bibitem[Zhu et~al.(2022)Zhu, Peng, Larsson, Xu, Bao, Cui, Oswald, and
  Pollefeys]{zhu2022nice}
Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui,
  Martin~R Oswald, and Marc Pollefeys.
\newblock Nice-slam: Neural implicit scalable encoding for slam.
\newblock In \emph{Proc. CVPR}, pages 12786--12796, 2022.

\bibitem[Rosinol et~al.(2022)Rosinol, Leonard, and Carlone]{rosinol2022nerf}
Antoni Rosinol, John~J Leonard, and Luca Carlone.
\newblock Nerf-slam: Real-time dense monocular slam with neural radiance
  fields.
\newblock \emph{arXiv preprint arXiv:2210.13641}, 2022.

\bibitem[Chung et~al.(2022)Chung, Tseng, Hsu, Shi, Hua, Yeh, Chen, Chen, and
  Hsu]{chung2022orbeez}
Chi-Ming Chung, Yang-Che Tseng, Ya-Ching Hsu, Xiang-Qian Shi, Yun-Hung Hua,
  Jia-Fong Yeh, Wen-Chin Chen, Yi-Ting Chen, and Winston~H Hsu.
\newblock Orbeez-slam: A real-time monocular visual slam with orb features and
  nerf-realized mapping.
\newblock \emph{arXiv preprint arXiv:2209.13274}, 2022.

\bibitem[Sucar et~al.(2021)Sucar, Liu, Ortiz, and Davison]{sucar2021imap}
Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew~J Davison.
\newblock imap: Implicit mapping and positioning in real-time.
\newblock In \emph{Proc. ICCV}, pages 6229--6238, 2021.

\bibitem[Zhou et~al.(2017)Zhou, Brown, Snavely, and Lowe]{zhou2017unsupervised}
Tinghui Zhou, Matthew Brown, Noah Snavely, and David~G Lowe.
\newblock Unsupervised learning of depth and ego-motion from video.
\newblock In \emph{Proc. CVPR}, 2017.

\bibitem[Yin and Shi(2018)]{yin2018geonet}
Zhichao Yin and Jianping Shi.
\newblock Geonet: Unsupervised learning of dense depth, optical flow and camera
  pose.
\newblock In \emph{Proc. CVPR}, 2018.

\bibitem[Guizilini et~al.(2022)Guizilini, Lee, Ambru{\c{s}}, and
  Gaidon]{guizilini2022learning}
Vitor Guizilini, Kuan-Hui Lee, Rare{\c{s}} Ambru{\c{s}}, and Adrien Gaidon.
\newblock Learning optical flow, depth, and scene flow without real-world
  labels.
\newblock \emph{Proc. ICRA}, 2022.

\bibitem[Fu et~al.(2022)Fu, Misra, and Wang]{fu2022multiplane}
Yang Fu, Ishan Misra, and Xiaolong Wang.
\newblock Multiplane nerf-supervised disentanglement of depth and camera pose
  from videos.
\newblock \emph{arXiv preprint arXiv:2210.07181}, 2022.

\bibitem[Lai et~al.(2021)Lai, Liu, Efros, and Wang]{lai2021video}
Zihang Lai, Sifei Liu, Alexei~A Efros, and Xiaolong Wang.
\newblock Video autoencoder: self-supervised disentanglement of static 3d
  structure and motion.
\newblock In \emph{Proc. ICCV}, 2021.

\bibitem[Tung et~al.(2019)Tung, Cheng, and Fragkiadaki]{tung2019learning}
Hsiao-Yu~Fish Tung, Ricson Cheng, and Katerina Fragkiadaki.
\newblock Learning spatial common sense with geometry-aware recurrent networks.
\newblock In \emph{Proc. CVPR}, 2019.

\bibitem[Chen and Lee(2023)]{chen2023dbarf}
Yu~Chen and Gim~Hee Lee.
\newblock Dbarf: Deep bundle-adjusting generalizable neural radiance fields.
\newblock \emph{arXiv preprint arXiv:2303.14478}, 2023.

\bibitem[Teed and Deng(2020{\natexlab{a}})]{teed2020raft}
Zachary Teed and Jia Deng.
\newblock Raft: Recurrent all-pairs field transforms for optical flow.
\newblock In \emph{Proc. CVPR}, pages 402--419. Springer, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2023)Jin, Zhang, Hold-Geoffroy, Wang, Matzen, Sticha, and
  Fouhey]{jin2022PerspectiveFields}
Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Matzen,
  Matthew Sticha, and David~F. Fouhey.
\newblock Perspective fields for single image camera calibration.
\newblock \emph{Proc. CVPR}, 2023.

\bibitem[Choy et~al.(2020)Choy, Dong, and Koltun]{choy2020deep}
Christopher Choy, Wei Dong, and Vladlen Koltun.
\newblock Deep global registration.
\newblock In \emph{Proc. CVPR}, 2020.

\bibitem[Teed and Deng(2020{\natexlab{b}})]{raft}
Zachary Teed and Jia Deng.
\newblock {RAFT}: {R}ecurrent all-pairs field transforms for optical flow.
\newblock In \emph{Proc. ECCV}, 2020{\natexlab{b}}.

\bibitem[Kopf et~al.(2021)Kopf, Rong, and Huang]{kopf2021robust}
Johannes Kopf, Xuejian Rong, and Jia-Bin Huang.
\newblock Robust consistent video depth estimation.
\newblock In \emph{Proc. CVPR}, pages 1611--1621, 2021.

\bibitem[Knapitsch et~al.(2017)Knapitsch, Park, Zhou, and
  Koltun]{Knapitsch2017}
Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun.
\newblock Tanks and temples: Benchmarking large-scale scene reconstruction.
\newblock \emph{Proc. TOG}, 36\penalty0 (4), 2017.

\bibitem[Zhou et~al.(2018)Zhou, Tucker, Flynn, Fyffe, and
  Snavely]{zhou2018stereo}
Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely.
\newblock Stereo magnification: Learning view synthesis using multiplane
  images.
\newblock \emph{Proc. TOG}, 2018.

\bibitem[Reizenstein et~al.(2021)Reizenstein, Shapovalov, Henzler, Sbordone,
  Labatut, and Novotny]{reizenstein2021common}
Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick
  Labatut, and David Novotny.
\newblock Common objects in 3d: Large-scale learning and evaluation of
  real-life 3d category reconstruction.
\newblock In \emph{Proc. ICCV}, pages 10901--10911, 2021.

\bibitem[Geiger et~al.(2012)Geiger, Lenz, and Urtasun]{geiger2012we}
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
\newblock Are we ready for autonomous driving? {T}he {KITTI} vision benchmark
  suite.
\newblock In \emph{Proc. CVPR}, 2012.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari,
  Girdhar, Hamburger, Jiang, Liu, Liu, et~al.]{grauman2022ego4d}
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino
  Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,
  et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In \emph{Proc. CVPR}, pages 18995--19012, 2022.

\bibitem[Campos et~al.(2021{\natexlab{b}})Campos, Elvira, Rodriguez, Montiel,
  and Tardos]{Campos_2021}
Carlos Campos, Richard Elvira, Juan J.~Gomez Rodriguez, Jose M.~M. Montiel, and
  Juan~D. Tardos.
\newblock {ORB}-{SLAM}3: An accurate open-source library for visual,
  visual{\textendash}inertial, and multimap {SLAM}.
\newblock \emph{{IEEE} Transactions on Robotics}, pages 1874--1890, dec
  2021{\natexlab{b}}.

\bibitem[Godard et~al.(2019{\natexlab{b}})Godard, {Mac Aodha}, Firman, and
  Brostow]{monodepth2}
Cl{\'{e}}ment Godard, Oisin {Mac Aodha}, Michael Firman, and Gabriel~J.
  Brostow.
\newblock Digging into self-supervised monocular depth prediction.
\newblock In \emph{Proc. ICCV}, 2019{\natexlab{b}}.

\bibitem[Liu et~al.(2023)Liu, Gao, Meuleman, Tseng, Saraf, Kim, Chuang, Kopf,
  and Huang]{liu2023robust}
Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil
  Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang.
\newblock Robust dynamic radiance fields.
\newblock In \emph{Proc. CVPR}, 2023.

\bibitem[Li et~al.(2021)Li, Niklaus, Snavely, and Wang]{li2020neural}
Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
\newblock Neural scene flow fields for space-time view synthesis of dynamic
  scenes.
\newblock In \emph{Proc. CVPR}, 2021.

\bibitem[Park et~al.(2021)Park, Sinha, Barron, Bouaziz, Goldman, Seitz, and
  Martin-Brualla]{park2021nerfies}
Keunhong Park, Utkarsh Sinha, Jonathan~T. Barron, Sofien Bouaziz, Dan~B
  Goldman, Steven~M. Seitz, and Ricardo Martin-Brualla.
\newblock Nerfies: Deformable neural radiance fields.
\newblock \emph{Proc. ICCV}, 2021.

\end{thebibliography}
