\begin{thebibliography}{10}

\bibitem{agarwal2014taming}
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert
  Schapire.
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  1638--1646. PMLR, 2014.

\bibitem{ando2005framework}
Rie~Kubota Ando and Tong Zhang.
\newblock A framework for learning predictive structures from multiple tasks
  and unlabeled data.
\newblock {\em Journal of Machine Learning Research}, 6(Nov):1817--1853, 2005.

\bibitem{arora2020provable}
Sanjeev Arora, Simon~S Du, Sham Kakade, Yuping Luo, and Nikunj Saunshi.
\newblock Provable representation learning for imitation learning via bi-level
  optimization.
\newblock {\em arXiv preprint arXiv:2002.10544}, 2020.

\bibitem{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock {\em Journal of artificial intelligence research}, 12:149--198, 2000.

\bibitem{ben2003exploiting}
Shai Ben-David and Reba Schuller.
\newblock Exploiting task relatedness for multiple task learning.
\newblock In {\em Learning Theory and Kernel Machines}, pages 567--580.
  Springer, 2003.

\bibitem{berner2019dota}
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw
  Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris
  Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1912.06680}, 2019.

\bibitem{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock {\em Machine learning}, 28(1):41--75, 1997.

\bibitem{cavallanti2010linear}
Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile.
\newblock Linear algorithms for online multitask classification.
\newblock {\em Journal of Machine Learning Research}, 11(Oct):2901--2934, 2010.

\bibitem{cheng2022provable}
Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang.
\newblock Provable benefit of multitask representation learning in
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2206.05900}, 2022.

\bibitem{deng2012mnist}
Li~Deng.
\newblock The mnist database of handwritten digit images for machine learning
  research [best of the web].
\newblock {\em IEEE Signal Processing Magazine}, 29(6):141--142, 2012.

\bibitem{d'eramo2020sharing}
Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan
  Peters.
\newblock Sharing knowledge in multi-task deep reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{dong2021provable}
Kefan Dong, Jiaqi Yang, and Tengyu Ma.
\newblock Provable model-based nonlinear bandit and reinforcement learning:
  Shelve optimism, embrace virtual curvature.
\newblock {\em arXiv preprint arXiv:2102.04168}, 2021.

\bibitem{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International conference on machine learning}, pages
  1675--1685. PMLR, 2019.

\bibitem{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock {\em arXiv preprint arXiv:2002.09434}, 2020.

\bibitem{du2017hypothesis}
Simon~S Du, Jayanth Koushik, Aarti Singh, and Barnab{\'a}s P{\'o}czos.
\newblock Hypothesis transfer learning via transformation functions.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 574--584, 2017.

\bibitem{hessel2019multi}
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,
  and Hado van Hasselt.
\newblock Multi-task deep reinforcement learning with popart.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3796--3803, 2019.

\bibitem{hu2021near}
Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang.
\newblock Near-optimal representation learning for linear bandits and linear
  rl.
\newblock In {\em International Conference on Machine Learning}, pages
  4349--4358. PMLR, 2021.

\bibitem{hu2013fast}
Yao Hu, Debing Zhang, Jieping Ye, Xuelong Li, and Xiaofei He.
\newblock Fast and accurate matrix completion via truncated nuclear norm
  regularization.
\newblock {\em Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, 35(9):2117--2130, 2013.

\bibitem{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  {PAC}-learnable.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1704--1713. JMLR. org, 2017.

\bibitem{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em Advances in neural information processing systems},
  34:13406--13418, 2021.

\bibitem{jin2019provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:1907.05388}, 2019.

\bibitem{lazaric2011transfer}
Alessandro Lazaric and Marcello Restelli.
\newblock Transfer from multiple mdps.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1746--1754, 2011.

\bibitem{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em The Journal of Machine Learning Research}, 17(1):1334--1373,
  2016.

\bibitem{li2010contextual}
Lihong Li, Wei Chu, John Langford, and Robert~E Schapire.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In {\em Proceedings of the 19th international conference on World
  wide web}, pages 661--670, 2010.

\bibitem{liu2016decoding}
Lydia~T Liu, Urun Dogan, and Katja Hofmann.
\newblock Decoding multitask dqn in the world of minecraft.
\newblock In {\em The 13th European Workshop on Reinforcement Learning (EWRL)
  2016}, 2016.

\bibitem{lu2021power}
Rui Lu, Gao Huang, and Simon~S Du.
\newblock On the power of multitask representation learning in linear mdp.
\newblock {\em arXiv preprint arXiv:2106.08053}, 2021.

\bibitem{maurer2006bounds}
Andreas Maurer.
\newblock Bounds for linear multi-task learning.
\newblock {\em Journal of Machine Learning Research}, 7(Jan):117--139, 2006.

\bibitem{maurer2016benefit}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock {\em The Journal of Machine Learning Research}, 17(1):2853--2884,
  2016.

\bibitem{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{papini2021reinforcement}
Matteo Papini, Andrea Tirinzoni, Aldo Pacchiano, Marcello Restelli, Alessandro
  Lazaric, and Matteo Pirotta.
\newblock Reinforcement learning in linear mdps: Constant regret and
  representation selection.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16371--16383, 2021.

\bibitem{parisotto2015actor}
Emilio Parisotto, Jimmy~Lei Ba, and Ruslan Salakhutdinov.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock {\em arXiv preprint arXiv:1511.06342}, 2015.

\bibitem{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em NIPS}, pages 2256--2264. Citeseer, 2013.

\bibitem{rusu2015policy}
Andrei~A Rusu, Sergio~Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,
  James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and
  Raia Hadsell.
\newblock Policy distillation.
\newblock {\em arXiv preprint arXiv:1511.06295}, 2015.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}, 550(7676):354--359, 2017.

\bibitem{sun2018model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based reinforcement learning in contextual decision processes.
\newblock {\em arXiv preprint arXiv:1811.08540}, 2018.

\bibitem{taylor2009transfer}
Matthew~E Taylor and Peter Stone.
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock {\em Journal of Machine Learning Research}, 10(Jul):1633--1685, 2009.

\bibitem{teh2017distral}
Yee Teh, Victor Bapst, Wojciech~M Czarnecki, John Quan, James Kirkpatrick, Raia
  Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4496--4506, 2017.

\bibitem{tripuraneni2020provable}
Nilesh Tripuraneni, Chi Jin, and Michael~I Jordan.
\newblock Provable meta-learning of linear representations.
\newblock {\em arXiv preprint arXiv:2002.11684}, 2020.

\bibitem{wang2020reinforcement}
Ruosong Wang, Ruslan Salakhutdinov, and Lin~F Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock {\em arXiv preprint arXiv:2005.10804}, 2020.

\bibitem{yang2021impact}
Jiaqi Yang, Wei Hu, Jason~D. Lee, and Simon~Shaolei Du.
\newblock Impact of representation learning in linear bandits.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{yang2022nearly}
Jiaqi Yang, Qi~Lei, Jason~D Lee, and Simon~S Du.
\newblock Nearly minimax algorithms for linear bandits with shared
  representation.
\newblock {\em arXiv preprint arXiv:2203.15664}, 2022.

\bibitem{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In {\em International Conference on Machine Learning}, pages
  6995--7004. PMLR, 2019.

\bibitem{yang2019learning}
Lin~F Yang, Chengzhuo Ni, and Mengdi Wang.
\newblock Learning to control in metric space with optimal regret.
\newblock {\em arXiv preprint arXiv:1905.01576}, 2019.

\bibitem{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020.

\end{thebibliography}
