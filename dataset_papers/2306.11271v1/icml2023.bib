@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement Learning: A Survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@article{liang2016deep,
  title={Why Deep Neural Networks for Function Approximation?},
  author={Liang, Shiyu and Srikant, Rayadurgam},
  journal={arXiv preprint arXiv:1610.04161},
  year={2016}
  }

@inproceedings{liang2017deep,
  title={Why Deep Neural Networks for Function Approximation?},
  author={Liang, Shiyu and Srikant, R},
  booktitle={5th International Conference on Learning Representations, ICLR 2017},
  year={2017}
}

@article{schrittwieser2020mastering,
  title={Mastering Atari, Go, Chess And Shogi By Planning With A Learned Model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering Chess And Shogi by Self-play With A General Reinforcement Learning Algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{wu2017flow,
  title={Flow: Architecture and Benchmarking for Reinforcement Learning In Traffic Control},
  author={Wu, Cathy and Kreidieh, Aboudy and Parvate, Kanaad and Vinitsky, Eugene and Bayen, Alexandre M},
  journal={arXiv preprint arXiv:1710.05465},
  volume={10},
  year={2017}
}

@article{choi2009reinforcement,
  title={Reinforcement Learning and Savings Behavior},
  author={Choi, James J and Laibson, David and Madrian, Brigitte C and Metrick, Andrew},
  journal={The Journal of finance},
  volume={64},
  number={6},
  pages={2515--2534},
  year={2009},
  publisher={Wiley Online Library}
}

@article{jiang2017deep,
  title={A Deep Reinforcement Learning Framework for The Financial Portfolio Management Problem},
  author={Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
  journal={arXiv preprint arXiv:1706.10059},
  year={2017}
}

@article{shalev2016safe,
  title={Safe, Multi-agent, Reinforcement Learning For Autonomous Driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{nair2020accelerating,
  title={Accelerating Online Reinforcement Learning with Offline Datasets},
  author={Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@inproceedings{gelly2007combining,
  title={Combining Online and Offline Knowledge in {UCT}},
  author={Gelly, Sylvain and Silver, David},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={273--280},
  year={2007}
}

@article{uchendu2022jump,
  title={Jump-Start Reinforcement Learning},
  author={Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Jos{\'e}phine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and others},
  journal={arXiv preprint arXiv:2204.02372},
  year={2022}
}

@article{kumar2020discor,
  title={Discor: Corrective Feedback in Reinforcement Learning via Distribution Correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18560--18572},
  year={2020}
}

@article{xie2021policy,
  title={Policy Finetuning: Bridging Sample-efficient Offline and Online Reinforcement Learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@inproceedings{kalashnikov2018scalable,
  title={Scalable Deep Reinforcement Learning for Vision-based Robotic Manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  booktitle={Conference on Robot Learning},
  pages={651--673},
  year={2018},
  organization={PMLR}
}

@article{silver2018general,
  title={A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{tesauro1994td,
  title={Td-gammon, A Self-teaching Backgammon Program, Achieves Master-level Play},
  author={Tesauro, Gerald},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={215--219},
  year={1994},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{konda1999actor,
  title={Actor-critic Algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{peters2008natural,
  title={Natural Actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@article{lagoudakis2003least,
  title={Least-squares Policy Iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={The Journal of Machine Learning Research},
  volume={4},
  pages={1107--1149},
  year={2003},
  publisher={JMLR. org}
}

@article{tamar2016value,
  title={Value Iteration Networks},
  author={Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{grondman2012survey,
  title={A Survey Of Actor-critic Reinforcement Learning: Standard and Natural Policy Gradients},
  author={Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel AD and Babuska, Robert},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={42},
  number={6},
  pages={1291--1307},
  year={2012},
  publisher={IEEE}
}

@article{grand2021convex,
  title={From Convex Optimization to {MDPs}: A Review Of First-order, Second-order And Quasi-newton Methods for {MDPs}},
  author={Grand-Cl{\'e}ment, Julien},
  journal={arXiv preprint arXiv:2104.10677},
  year={2021}
}

@article{santos2004convergence,
  title={Convergence Properties of Policy Iteration},
  author={Santos, Manuel S and Rust, John},
  journal={SIAM Journal on Control and Optimization},
  volume={42},
  number={6},
  pages={2094--2115},
  year={2004},
  publisher={SIAM}
}

@article{puterman1979convergence,
  title={On The Convergence of Policy Iteration in Stationary Dynamic Programming},
  author={Puterman, Martin L and Brumelle, Shelby L},
  journal={Mathematics of Operations Research},
  volume={4},
  number={1},
  pages={60--69},
  year={1979},
  publisher={INFORMS}
}

@book{boyd2004convex,
  title={Convex Optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{uehara2021finite,
  title={Finite Sample Analysis of Minimax Offline Reinforcement Learning: Completeness, Fast Rates and First-order Efficiency},
  author={Uehara, Masatoshi and Imaizumi, Masaaki and Jiang, Nan and Kallus, Nathan and Sun, Wen and Xie, Tengyang},
  journal={arXiv preprint arXiv:2102.02981},
  year={2021}
}

@inproceedings{dalal2020tale,
  title={A Tale of Two-timescale Reinforcement Learning with The Tightest Finite-time Bound},
  author={Dalal, Gal and Szorenyi, Balazs and Thoppe, Gugan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3701--3708},
  year={2020}
}

@inproceedings{doan2019finite,
  title={Finite-time Analysis of Distributed {TD (0)} With Linear Function Approximation on Multi-agent Reinforcement Learning},
  author={Doan, Thinh and Maguluri, Siva and Romberg, Justin},
  booktitle={International Conference on Machine Learning},
  pages={1626--1635},
  year={2019},
  organization={PMLR}
}

@article{wu2020finite,
  title={A Finite-time Analysis of Two Time-scale Actor-critic Methods},
  author={Wu, Yue Frank and Zhang, Weitong and Xu, Pan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17617--17628},
  year={2020}
}

@article{schrittwieser2021online,
  title={Online And Offline Reinforcement Learning by Planning With A Learned Model},
  author={Schrittwieser, Julian and Hubert, Thomas and Mandhane, Amol and Barekatain, Mohammadamin and Antonoglou, Ioannis and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{gupta2020relay,
  title={Relay Policy Learning: Solving Long-horizon Tasks via Imitation and Reinforcement Learning},
  author={Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  booktitle={Conference on Robot Learning},
  pages={1025--1037},
  year={2020},
  organization={PMLR}
}

@article{ijspeert2002learning,
  title={Learning Attractor Landscapes for Learning Motor Primitives},
  author={Ijspeert, Auke and Nakanishi, Jun and Schaal, Stefan},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}

@article{kim2013learning,
  title={Learning from Limited Demonstrations},
  author={Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@inproceedings{lee2022offline,
  title={Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble},
  author={Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo},
  booktitle={Conference on Robot Learning},
  pages={1702--1712},
  year={2022},
  organization={PMLR}
}

@article{sutton1999policy,
  title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@book{bertsekas2022lessons,
  title={Lessons from Alphazero for Optimal, Model Predictive, and Adaptive Control},
  author={Bertsekas, Dimitri},
  year={2022},
  publisher={Athena Scientific}
}

@article{gargiani2022dynamic,
  title={Dynamic Programming Through the Lens of Semismooth Newton-Type Methods},
  author={Gargiani, Matilde and Zanelli, Andrea and Liao-McPherson, Dominic and Summers, TH and Lygeros, John},
  journal={IEEE Control Systems Letters},
  year={2022},
  publisher={IEEE}
}

@inproceedings{wang2020warm,
  title={Warm-start Alphazero Self-play Search Enhancements},
  author={Wang, Hui and Preuss, Mike and Plaat, Aske},
  booktitle={International Conference on Parallel Problem Solving from Nature},
  pages={528--542},
  year={2020},
  organization={Springer}
}

@inproceedings{yang2018finite,
  title={A Finite Sample Analysis of the Actor-critic Algorithm},
  author={Yang, Zhuoran and Zhang, Kaiqing and Hong, Mingyi and Ba{\c{s}}ar, Tamer},
  booktitle={2018 IEEE conference on decision and control (CDC)},
  pages={2759--2764},
  year={2018},
  organization={IEEE}
}

@article{yang2019provably,
  title={Provably Global Convergence of Actor-critic: A Case For Linear Quadratic Regulator with Ergodic Cost},
  author={Yang, Zhuoran and Chen, Yongxin and Hong, Mingyi and Wang, Zhaoran},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{
wang2019neural,
title={Neural Policy Gradient Methods: Global Optimality and Rates of Convergence},
author={Lingxiao Wang and Qi Cai and Zhuoran Yang and Zhaoran Wang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJgQfkSYDS}
}

@article{kumar2023sample,
  title={On the sample complexity of actor-critic method for reinforcement learning with function approximation},
  author={Kumar, Harshat and Koppel, Alec and Ribeiro, Alejandro},
  journal={Machine Learning},
  pages={1--35},
  year={2023},
  publisher={Springer}
}

@inproceedings{fu2020single,
  title={Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy},
  author={Fu, Zuyue and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@book{bertsekas2022abstract,
  title={Abstract Dynamic Programming},
  author={Bertsekas, Dimitri},
  year={2022},
  publisher={Athena Scientific}
}

@inproceedings{thrun1993issues,
  title={Issues in Using Function Approximation for Reinforcement Learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
  volume={6},
  year={1993}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing Function Approximation Error in Actor-critic Methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@article{xu2020non,
  title={Non-asymptotic Convergence Analysis of Two Time-scale (Natural) Actor-critic Algorithms},
  author={Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
  journal={arXiv preprint arXiv:2005.03557},
  year={2020}
}

@inproceedings{bhandari2018finite,
  title={A Finite Time Analysis of Temporal Difference Learning with Linear Function Approximation},
  author={Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},
  booktitle={Conference on learning theory},
  pages={1691--1692},
  year={2018},
  organization={PMLR}
}

@article{jiang2018bernstein,
  title={Bernstein's Inequality for General Markov Chains},
  author={Jiang, Bai and Sun, Qiang and Fan, Jianqing},
  journal={arXiv preprint arXiv:1805.10721},
  year={2018}
}

@article{ortner2020regret,
  title={Regret Bounds for Reinforcement Learning via Markov Chain Concentration},
  author={Ortner, Ronald},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={115--128},
  year={2020}
}

@inproceedings{amit2020discount,
  title={Discount Factor as A Regularizer in Reinforcement Learning},
  author={Amit, Ron and Meir, Ron and Ciosek, Kamil},
  booktitle={International conference on machine learning},
  pages={269--278},
  year={2020},
  organization={PMLR}
}

@inproceedings{silver2014deterministic,
  title={Deterministic Policy Gradient Algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}

@article{ajalloeian2020convergence,
  title={On the Convergence of {SGD} with Biased Gradients},
  author={Ajalloeian, Ahmad and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2008.00051},
  year={2020}
}

@inproceedings{xu2020improving,
author = {Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
title = {Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms},
year = {2020},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
}


@article{zou2019finite,
  title={Finite-sample Analysis for {SARSA} with Linear Function Approximation},
  author={Zou, Shaofeng and Xu, Tengyu and Liang, Yingbin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@article{li2017deep,
  title={Deep Reinforcement Learning: An Overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@article{garcia2015comprehensive,
  title={A Comprehensive Survey on Safe Reinforcement Learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@article{fan2021hoeffding,
  title={Hoeffding's Inequality for General Markov Chains and Its Applications to Statistical Learning},
  author={Fan, Jianqing and Jiang, Bai and Sun, Qiang},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={139},
  pages={1--35},
  year={2021},
  publisher={Microtome Publishing}
}

@inproceedings{dalal2018finite,
  title={Finite Sample Analysis of Two-timescale Stochastic Approximation with Applications to Reinforcement Learning},
  author={Dalal, Gal and Thoppe, Gugan and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Mannor, Shie},
  booktitle={Conference On Learning Theory},
  pages={1199--1233},
  year={2018},
  organization={PMLR}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{van2016deep,
  title={Deep Reinforcement Learning with Double Q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}


@article{wang2021adaptive,
  title={Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback},
  author={Wang, Hang and Lin, Sen and Zhang, Junshan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@book{nesterov2003introductory,
  title={Introductory Lectures on Convex Optimization: A Basic Course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@book{levin2017markov,
  title={Markov Chains and Mixing Times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}


@article{lezaud1998chernoff,
  title={Chernoff-type Bound for Finite Markov Chains},
  author={Lezaud, Pascal},
  journal={Annals of Applied Probability},
  pages={849--867},
  year={1998},
  publisher={JSTOR}
}

@article{agarwal2021theory,
  title={On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@article{khodadadian2022finite,
  title={Finite sample analysis of two-time-scale natural actor-critic algorithm},
  author={Khodadadian, Sajad and Doan, Thinh T and Romberg, Justin and Maguluri, Siva Theja},
  journal={IEEE Transactions on Automatic Control},
  year={2022},
  publisher={IEEE}
}


@inproceedings{munos2003error,
  title={Error bounds for approximate policy iteration},
  author={Munos, R{\'e}mi},
  booktitle={ICML},
  volume={3},
  pages={560--567},
  year={2003},
  organization={Citeseer}
}

@inproceedings{lazaric2016analysis,
title={Analysis of a classification-based policy iteration algorithm},
  author={Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'e}mi},
  booktitle={27th International Conference on Machine Learning},
  pages={607--614},
  year={2010},
  organization={Omnipress}
}

@article{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}

@article{bagnell2003policy,
  title={Policy search by dynamic programming},
  author={Bagnell, James and Kakade, Sham M and Schneider, Jeff and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}

@article{song2022hybrid,
  title={Hybrid RL: Using both offline and online data can make RL efficient},
  author={Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Bagnell, J Andrew and Krishnamurthy, Akshay and Sun, Wen},
  journal={arXiv preprint arXiv:2210.06718},
  year={2022}
}

@article{wagenmaker2022leveraging,
  title={Leveraging Offline Data in Online Reinforcement Learning},
  author={Wagenmaker, Andrew and Pacchiano, Aldo},
  journal={arXiv preprint arXiv:2211.04974},
  year={2022}
}