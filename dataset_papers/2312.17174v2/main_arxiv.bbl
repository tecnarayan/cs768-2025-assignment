\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adebayo et~al.(2018)Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and
  Kim]{sanitycheck}
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian~J. Goodfellow, Moritz Hardt,
  and Been Kim.
\newblock Sanity checks for saliency maps.
\newblock \emph{CoRR}, abs/1810.03292, 2018.

\bibitem[Alemi et~al.(2017)Alemi, Fischer, Dillon, and Murphy]{alemi2017deep}
Alexander~A. Alemi, Ian Fischer, Joshua~V. Dillon, and Kevin Murphy.
\newblock Deep variational information bottleneck.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Boecking et~al.(2022)Boecking, Usuyama, Bannur, Castro, Schwaighofer,
  Hyland, Wetscherek, Naumann, Nori, Alvarez-Valle, Poon, and Oktay]{MSCXR}
Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel~C. Castro, Anton
  Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya
  Nori, Javier Alvarez-Valle, Hoifung Poon, and Ozan Oktay.
\newblock Making the most of text semantics to improve biomedical
  vision--language processing.
\newblock In Shai Avidan, Gabriel Brostow, Moustapha Ciss{\'e}, Giovanni~Maria
  Farinella, and Tal Hassner, editors, \emph{Computer Vision -- ECCV 2022},
  pages 1--21, Cham, 2022. Springer Nature Switzerland.
\newblock ISBN 978-3-031-20059-5.

\bibitem[Chattopadhay et~al.(2018)Chattopadhay, Sarkar, Howlader, and
  Balasubramanian]{gradcam++}
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth~N
  Balasubramanian.
\newblock Grad-{CAM}++: Generalized gradient-based visual explanations for deep
  convolutional networks.
\newblock In \emph{2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 839--847, 2018.
\newblock \doi{10.1109/WACV.2018.00097}.

\bibitem[Chefer et~al.(2020)Chefer, Gur, and Wolf]{beyondattention}
Hila Chefer, Shir Gur, and Lior Wolf.
\newblock Transformer interpretability beyond attention visualization.
\newblock \emph{CoRR}, abs/2012.09838, 2020.

\bibitem[Chefer et~al.(2021)Chefer, Gur, and
  Wolf]{Chefer_2021_ICCV_attention_attr}
Hila Chefer, Shir Gur, and Lior Wolf.
\newblock Generic attention-model explainability for interpreting bi-modal and
  encoder-decoder transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 397--406, October 2021.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an_vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Endo et~al.(2021)Endo, Krishnan, Krishna, Ng, and
  Rajpurkar]{pmlr-v158-endo21a-CXR-RePaiR}
Mark Endo, Rayan Krishnan, Viswesh Krishna, Andrew~Y. Ng, and Pranav Rajpurkar.
\newblock Retrieval-based chest x-ray report generation using a pre-trained
  contrastive language-image model.
\newblock In \emph{Proceedings of Machine Learning for Health}, volume 158 of
  \emph{Proceedings of Machine Learning Research}, pages 209--219, 2021.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and
  Misra]{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev
  Alwala, Armand Joulin, and Ishan Misra.
\newblock {ImageBind}: One embedding space to bind them all, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hooker et~al.(2019)Hooker, Erhan, Kindermans, and
  Kim]{NEURIPS2019_roar}
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim.
\newblock A benchmark for interpretability methods in deep neural networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Jiang et~al.(2020)Jiang, Tang, Xin, and
  Lin]{jiang-etal-2020-inserting}
Zhiying Jiang, Raphael Tang, Ji~Xin, and Jimmy Lin.
\newblock Inserting information bottlenecks for attribution in transformers.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3850--3857, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.343}.

\bibitem[Johnson et~al.(2019)Johnson, Pollard, Berkowitz, Greenbaum, Lungren,
  Deng, Mark, and Horng]{mimic-cxr}
Alistair E.~W. Johnson, Tom~J. Pollard, Seth~J. Berkowitz, Nathaniel~R.
  Greenbaum, Matthew~P. Lungren, Chih{-}ying Deng, Roger~G. Mark, and Steven
  Horng.
\newblock {MIMIC-CXR:} {A} large publicly available database of labeled chest
  radiographs.
\newblock \emph{CoRR}, abs/1901.07042, 2019.

\bibitem[Liu and Deng(2015)]{VGG}
Shuying Liu and Weihong Deng.
\newblock Very deep convolutional neural network based image classification
  using small training sample size.
\newblock In \emph{2015 3rd IAPR Asian Conference on Pattern Recognition
  (ACPR)}, pages 730--734, 2015.
\newblock \doi{10.1109/ACPR.2015.7486599}.

\bibitem[Long et~al.(2022)Long, Cao, Han, and Yang]{VLPMsurvey}
Siqu Long, Feiqi Cao, {Soyeon Caren} Han, and Haiqin Yang.
\newblock Vision-and-language pretrained models: A survey.
\newblock In Luc {De Raedt} and Luc {De Raedt}, editors, \emph{Proceedings of
  the 31st International Joint Conference on Artificial Intelligence, IJCAI
  2022}, IJCAI International Joint Conference on Artificial Intelligence, pages
  5530--5537. International Joint Conferences on Artificial Intelligence, 2022.
\newblock \doi{10.24963/IJCAI.2022/773}.
\newblock 31st International Joint Conference on Artificial Intelligence, IJCAI
  2022 ; Conference date: 23-07-2022 Through 29-07-2022.

\bibitem[Lu et~al.(2017)Lu, Wang, Zheng, and Li]{lu2017RSICD}
Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, and Xuelong Li.
\newblock Exploring models and data for remote sensing image caption
  generation.
\newblock \emph{IEEE Transactions on Geoscience and Remote Sensing},
  56\penalty0 (4):\penalty0 2183--2195, 2017.
\newblock \doi{10.1109/TGRS.2017.2776321}.

\bibitem[Lundberg and Lee(2017)]{NIPS2017_8a20a862_SHAP}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Mu et~al.(2021)Mu, Kirillov, Wagner, and Xie]{SLIP}
Norman Mu, Alexander Kirillov, David~A. Wagner, and Saining Xie.
\newblock {SLIP:} self-supervision meets language-image pre-training.
\newblock \emph{CoRR}, abs/2112.12750, 2021.

\bibitem[Pelka et~al.(2018)Pelka, Koitka, R{\"u}ckert, Nensa, and
  Friedrich]{Pelka2018RadiologyOI_roco}
Obioma Pelka, Sven Koitka, Johannes R{\"u}ckert, Felix Nensa, and C.~Friedrich.
\newblock {R}adiology {O}bjects in c{O}ntext ({ROCO}): A multimodal image
  dataset.
\newblock In \emph{CVII-STENT/LABELS@MICCAI}, 2018.

\bibitem[Petsiuk et~al.(2018)Petsiuk, Das, and Saenko]{Petsiuk2018rise}
Vitali Petsiuk, Abir Das, and Kate Saenko.
\newblock {RISE}: Randomized input sampling for explanation of black-box
  models.
\newblock In \emph{Proceedings of the British Machine Vision Conference
  (BMVC)}, 2018.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{CoRR}, abs/2103.00020, 2021.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{lime}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock "{W}hy should {I} trust you?": Explaining the predictions of any
  classifier.
\newblock In \emph{Proceedings of the 22nd {ACM} {SIGKDD} International
  Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA,
  August 13-17, 2016}, pages 1135--1144, 2016.

\bibitem[Rong et~al.(2022)Rong, Leemann, Borisov, Kasneci, and
  Kasneci]{rong22consistent_road}
Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda
  Kasneci.
\newblock A consistent and efficient evaluation strategy for attribution
  methods.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, pages 18770--18795. PMLR, 2022.

\bibitem[Schulz et~al.(2020)Schulz, Sixt, Tombari, and
  Landgraf]{Schulz2020Restricting}
Karl Schulz, Leon Sixt, Federico Tombari, and Tim Landgraf.
\newblock Restricting the flow: Information bottlenecks for attribution.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Selvaraju et~al.(2016)Selvaraju, Das, Vedantam, Cogswell, Parikh, and
  Batra]{gradcam}
Ramprasaath~R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-{CAM}: Why did you say that? visual explanations from deep
  networks via gradient-based localization.
\newblock \emph{CoRR}, abs/1610.02391, 2016.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{conceptualcaptions}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-1238}.

\bibitem[Shen et~al.(2022)Shen, Li, Tan, Bansal, Rohrbach, Chang, Yao, and
  Keutzer]{CLIPdownstream}
Sheng Shen, Liunian~Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei
  Chang, Zhewei Yao, and Kurt Keutzer.
\newblock How much can {CLIP} benefit vision-and-language tasks?
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Simonyan et~al.(2014)Simonyan, Vedaldi, and
  Zisserman]{Simonyan14a_saliency}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock In \emph{Workshop at International Conference on Learning
  Representations}, 2014.

\bibitem[{Springenberg} et~al.(2014){Springenberg}, {Dosovitskiy}, {Brox}, and
  {Riedmiller}]{guidedbackprop}
Jost~Tobias {Springenberg}, Alexey {Dosovitskiy}, Thomas {Brox}, and Martin
  {Riedmiller}.
\newblock {Striving for Simplicity: The All Convolutional Net}.
\newblock \emph{arXiv e-prints}, art. arXiv:1412.6806, December 2014.
\newblock \doi{10.48550/arXiv.1412.6806}.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{pmlr-v70-sundararajan17a_ig}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 3319--3328. PMLR,
  06--11 Aug 2017.

\bibitem[Tishby and Zaslavsky(2015)]{IB}
Naftali Tishby and Noga Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{2015 ieee information theory workshop (itw)}, pages 1--5.
  IEEE, 2015.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{IBM}
Naftali Tishby, Fernando~C. Pereira, and William Bialek.
\newblock The information bottleneck method, 2000.

\bibitem[Wang et~al.(2020)Wang, Wang, Du, Yang, Zhang, Ding, Mardziel, and
  Hu]{wang2020scorecam}
Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr
  Mardziel, and Xia Hu.
\newblock Score-{CAM}: Score-weighted visual explanations for convolutional
  neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, pages 24--25, 2020.

\end{thebibliography}
