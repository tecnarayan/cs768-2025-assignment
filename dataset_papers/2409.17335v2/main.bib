


@article{dryer1991svo,
  title={SVO languages and the OV: VO typology1},
  author={Dryer, Matthew S},
  journal={Journal of linguistics},
  volume={27},
  number={2},
  pages={443--482},
  year={1991},
  publisher={Cambridge University Press}
}

@article{yannakakis1982complexity,
  title={The complexity of the partial order dimension problem},
  author={Yannakakis, Mihalis},
  journal={SIAM Journal on Algebraic Discrete Methods},
  volume={3},
  number={3},
  pages={351--358},
  year={1982},
  publisher={SIAM}
}

@article{vardi2023implicit,
  title={On the implicit bias in deep-learning algorithms},
  author={Vardi, Gal},
  journal={Communications of the ACM},
  volume={66},
  number={6},
  pages={86--93},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{kou2024implicit,
  title={Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data},
  author={Kou, Yiwen and Chen, Zixiang and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{frei2022implicit,
  title={Implicit bias in leaky relu networks trained on high-dimensional data},
  author={Frei, Spencer and Vardi, Gal and Bartlett, Peter L and Srebro, Nathan and Hu, Wei},
  journal={arXiv preprint arXiv:2210.07082},
  year={2022}
}

@inproceedings{phuong2020inductive,
  title={The inductive bias of ReLU networks on orthogonally separable data},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{ji2021fast,
  title={Fast margin maximization via dual acceleration},
  author={Ji, Ziwei and Srebro, Nathan and Telgarsky, Matus},
  booktitle={International Conference on Machine Learning},
  pages={4860--4869},
  year={2021},
  organization={PMLR}
}


@inproceedings{ji2021characterizing,
  title={Characterizing the implicit bias via a primal-dual analysis},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Algorithmic Learning Theory},
  pages={772--804},
  year={2021},
  organization={PMLR}
}
@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}


@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}


@article{ahn2024transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={11398--11442},
  year={2023},
  organization={PMLR}
}

@article{fu2023transformers,
  title={Transformers learn higher-order optimization methods for in-context learning: A study with linear models},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2310.17086},
  year={2023}
}


@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{tian2023joma,
  title={Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention},
  author={Tian, Yuandong and Wang, Yiping and Zhang, Zhenyu and Chen, Beidi and Du, Simon},
  journal={arXiv preprint arXiv:2310.00535},
  year={2023}
}
@article{belkin2024necessity,
  title={The necessity of machine learning theory in mitigating AI risk.},
  author={Belkin, Mikhail},
  journal={ACM/JMS Journal of Data Science},
  year={2024},
  publisher={ACM New York, NY}
}


@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the conference. Association for computational linguistics. Meeting},
  volume={2019},
  pages={6558},
  year={2019},
  organization={NIH Public Access}
}



@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}



@inproceedings{li2024mechanics,
  title={Mechanics of next token prediction with self-attention},
  author={Li, Yingcong and Huang, Yixiao and Ildiz, Muhammed E and Rawat, Ankit Singh and Oymak, Samet},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={685--693},
  year={2024},
  organization={PMLR}
}


@inproceedings{nacson2019convergence,
  title={Convergence of gradient descent on separable data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019},
  organization={PMLR}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  pages={1--57},
  year={2018}
}



@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}

@inproceedings{tarzanagh2023max,
  title={Max-margin token selection in attention mechanism},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Zhang, Xuechen and Oymak, Samet},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{deora2023optimization,
  title={On the optimization and generalization of multi-head attention},
  author={Deora, Puneesh and Ghaderi, Rouzbeh and Taheri, Hossein and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2310.12680},
  year={2023}
}

 
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{tian2023scan,
  title={Scan and snap: Understanding training dynamics and token composition in 1-layer transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={71911--71947},
  year={2023}
}

@article{nichani2024transformers,
  title={How Transformers Learn Causal Structure with Gradient Descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}


@article{boix2023can,
  title={When can transformers reason with abstract symbols?},
  author={Boix-Adsera, Enric and Saremi, Omid and Abbe, Emmanuel and Bengio, Samy and Littwin, Etai and Susskind, Joshua},
  journal={arXiv preprint arXiv:2310.09753},
  year={2023}
}

@article{chen2024training,
  title={Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
  author={Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2402.19442},
  year={2024}
}



@misc{thrampoulidis2024implicit,
      title={Implicit Bias of Next-Token Prediction}, 
      author={Christos Thrampoulidis},
      year={2024},
      eprint={2402.18551},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{sason2015reverse,
  title={On reverse Pinsker inequalities},
  author={Sason, Igal},
  journal={arXiv preprint arXiv:1503.07118},
  year={2015}
}


@article{xing2024benefits,
  title={Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data},
  author={Xing, Yue and Lin, Xiaofeng and Suh, Namjoon and Song, Qifan and Cheng, Guang},
  journal={arXiv preprint arXiv:2402.00743},
  year={2024}
}

@article{cao2022benign,
  title={Benign overfitting in two-layer convolutional neural networks},
  author={Cao, Yuan and Chen, Zixiang and Belkin, Misha and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={25237--25250},
  year={2022}
}


@inproceedings{frei2022benign,
  title={Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter},
  booktitle={Conference on Learning Theory},
  pages={2668--2703},
  year={2022},
  organization={PMLR}
}

@article{vasudeva2024implicit,
  title={Implicit Bias and Fast Convergence Rates for Self-attention},
  author={Vasudeva, Bhavya and Deora, Puneesh and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2402.05738},
  year={2024}
}

@article{li2023theoretical,
  title={A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity},
  author={Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2302.06015},
  year={2023}
}



@article{kim2024transformers,
  title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2402.01258},
  year={2024}
}

@article{fu2023can,
  title={What can a single attention layer learn? a study through the random features lens},
  author={Fu, Hengyu and Guo, Tianyu and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2307.11353},
  year={2023}
}


@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@inproceedings{damian2022neural,
  title={Neural networks can learn representations with gradient descent},
  author={Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle={Conference on Learning Theory},
  pages={5413--5452},
  year={2022},
  organization={PMLR}
}


@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}


@article{lin2023transformers,
  title={Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining},
  author={Lin, Licong and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2310.08566},
  year={2023}
}

@article{bai2023transformers,
  title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={arXiv preprint arXiv:2306.04637},
  year={2023}
}


@article{cheng2023transformers,
  title={Transformers implement functional gradient descent to learn non-linear functions in context},
  author={Cheng, Xiang and Chen, Yuxin and Sra, Suvrit},
  journal={arXiv preprint arXiv:2312.06528},
  year={2023}
}

@article{cui2024superiority,
  title={Superiority of Multi-Head Attention in In-Context Linear Regression},
  author={Cui, Yingqian and Ren, Jie and He, Pengfei and Tang, Jiliang and Xing, Yue},
  journal={arXiv preprint arXiv:2401.17426},
  year={2024}
}


@article{chen2024provably,
  title={Provably learning a multi-head attention layer},
  author={Chen, Sitan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2402.04084},
  link = {https://arxiv.org/pdf/2402.04084.pdf},
  year={2024}
}