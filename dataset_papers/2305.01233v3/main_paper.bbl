\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2018)Agrawal, Batra, Parikh, and
  Kembhavi]{agrawal2018don}
Agrawal, A., Batra, D., Parikh, D., and Kembhavi, A.
\newblock Don't just assume; look and answer: Overcoming priors for visual
  question answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4971--4980, 2018.

\bibitem[Allen-Zhu \& Li(2020)Allen-Zhu and Li]{allen2020towards}
Allen-Zhu, Z. and Li, Y.
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020.

\bibitem[Amini et~al.(2009)Amini, Usunier, and Goutte]{amini2009learning}
Amini, M.~R., Usunier, N., and Goutte, C.
\newblock Learning from multiple partially observed views-an application to
  multilingual text categorization.
\newblock \emph{Advances in neural information processing systems},
  22:\penalty0 28--36, 2009.

\bibitem[Arora et~al.(2016)Arora, Mianjy, and Marinov]{arora2016stochastic}
Arora, R., Mianjy, P., and Marinov, T.
\newblock Stochastic optimization for multiview representation learning using
  partial least squares.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1786--1794. PMLR, 2016.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil]{bucilu2006model}
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  535--541, 2006.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Jain, Schissler, Gari, Al-Halah,
  Ithapu, Robinson, and Grauman]{chen2020soundspaces}
Chen, C., Jain, U., Schissler, C., Gari, S. V.~A., Al-Halah, Z., Ithapu, V.~K.,
  Robinson, P., and Grauman, K.
\newblock Soundspaces: Audio-visual navigation in 3d environments.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Xie, Vedaldi, and
  Zisserman]{chen2020vggsound}
Chen, H., Xie, W., Vedaldi, A., and Zisserman, A.
\newblock Vggsound: A large-scale audio-visual dataset.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  721--725. IEEE,
  2020{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020{\natexlab{c}}.

\bibitem[Cheuk et~al.(2020)Cheuk, Anderson, Agres, and
  Herremans]{cheuk2020nnaudio}
Cheuk, K.~W., Anderson, H., Agres, K., and Herremans, D.
\newblock nnaudio: An on-the-fly gpu audio to spectrogram conversion toolbox
  using 1d convolutional neural networks.
\newblock \emph{IEEE Access}, 8:\penalty0 161981--162003, 2020.

\bibitem[Fayek \& Kumar(2020)Fayek and Kumar]{fayek2020large}
Fayek, H.~M. and Kumar, A.
\newblock Large scale audiovisual learning of sounds with weakly labeled data.
\newblock \emph{arXiv preprint arXiv:2006.01595}, 2020.

\bibitem[Feichtenhofer et~al.(2016)Feichtenhofer, Pinz, and
  Zisserman]{feichtenhofer2016convolutional}
Feichtenhofer, C., Pinz, A., and Zisserman, A.
\newblock Convolutional two-stream network fusion for video action recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1933--1941, 2016.

\bibitem[Garcia et~al.(2018)Garcia, Morerio, and Murino]{garcia2018modality}
Garcia, N.~C., Morerio, P., and Murino, V.
\newblock Modality distillation with multiple stream networks for action
  recognition.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  103--118, 2018.

\bibitem[Gou et~al.(2021)Gou, Yu, Maybank, and Tao]{gou2021knowledge}
Gou, J., Yu, B., Maybank, S.~J., and Tao, D.
\newblock Knowledge distillation: A survey.
\newblock \emph{International Journal of Computer Vision}, 129\penalty0
  (6):\penalty0 1789--1819, 2021.

\bibitem[Gupta et~al.(2016)Gupta, Hoffman, and Malik]{gupta2016cross}
Gupta, S., Hoffman, J., and Malik, J.
\newblock Cross modal distillation for supervision transfer.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2827--2836, 2016.

\bibitem[Hessel \& Lee(2020)Hessel and Lee]{hessel2020does}
Hessel, J. and Lee, L.
\newblock Does my multimodal model learn cross-modal interactions? it's harder
  to tell than you might think!
\newblock \emph{arXiv preprint arXiv:2010.06572}, 2020.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2019)Hu, Yang, Fei, and Wang]{hu2019acnet}
Hu, X., Yang, K., Fei, L., and Wang, K.
\newblock Acnet: Attention based network to exploit complementary features for
  rgbd semantic segmentation.
\newblock In \emph{2019 IEEE International Conference on Image Processing
  (ICIP)}, pp.\  1440--1444. IEEE, 2019.

\bibitem[Huang et~al.(2021)Huang, Du, Xue, Chen, Zhao, and
  Huang]{huang2021makes}
Huang, Y., Du, C., Xue, Z., Chen, X., Zhao, H., and Huang, L.
\newblock What makes multimodal learning better than single (provably).
\newblock \emph{arXiv preprint arXiv:2106.04538}, 2021.

\bibitem[Huang et~al.(2022)Huang, Lin, Zhou, Yang, and
  Huang]{huang2022modality}
Huang, Y., Lin, J., Zhou, C., Yang, H., and Huang, L.
\newblock Modality competition: What makes joint training of multi-modal
  network fail in deep learning?(provably).
\newblock \emph{arXiv preprint arXiv:2203.12221}, 2022.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{kay2017kinetics}
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
  S., Viola, F., Green, T., Back, T., Natsev, P., et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Liang et~al.(2021)Liang, Lyu, Fan, Wu, Cheng, Wu, Chen, Wu, Lee, Zhu,
  et~al.]{liang2021multibench}
Liang, P.~P., Lyu, Y., Fan, X., Wu, Z., Cheng, Y., Wu, J., Chen, L., Wu, P.,
  Lee, M.~A., Zhu, Y., et~al.
\newblock Multibench: Multiscale benchmarks for multimodal representation
  learning.
\newblock \emph{arXiv preprint arXiv:2107.07502}, 2021.

\bibitem[Liang et~al.(2022)Liang, Lyu, Chhablani, Jain, Deng, Wang, Morency,
  and Salakhutdinov]{liang2022multiviz}
Liang, P.~P., Lyu, Y., Chhablani, G., Jain, N., Deng, Z., Wang, X., Morency,
  L.-P., and Salakhutdinov, R.
\newblock Multiviz: An analysis benchmark for visualizing and understanding
  multimodal models.
\newblock \emph{arXiv preprint arXiv:2207.00056}, 2022.

\bibitem[Luo et~al.(2018)Luo, Hsieh, Jiang, Niebles, and Fei-Fei]{luo2018graph}
Luo, Z., Hsieh, J.-T., Jiang, L., Niebles, J.~C., and Fei-Fei, L.
\newblock Graph distillation for action detection with privileged modalities.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  166--183, 2018.

\bibitem[Nagrani et~al.(2021)Nagrani, Yang, Arnab, Jansen, Schmid, and
  Sun]{nagrani2021attention}
Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., and Sun, C.
\newblock Attention bottlenecks for multimodal fusion.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Neverova et~al.(2015)Neverova, Wolf, Taylor, and
  Nebout]{neverova2015moddrop}
Neverova, N., Wolf, C., Taylor, G., and Nebout, F.
\newblock Moddrop: adaptive multi-modal gesture recognition.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 38\penalty0 (8):\penalty0 1692--1706, 2015.

\bibitem[Ngiam et~al.(2011)Ngiam, Khosla, Kim, Nam, Lee, and
  Ng]{ngiam2011multimodal}
Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A.~Y.
\newblock Multimodal deep learning.
\newblock In \emph{ICML}, 2011.

\bibitem[Panda et~al.(2021)Panda, Chen, Fan, Sun, Saenko, Oliva, and
  Feris]{panda2021adamml}
Panda, R., Chen, C.-F., Fan, Q., Sun, X., Saenko, K., Oliva, A., and Feris, R.
\newblock Adamml: Adaptive multi-modal learning for efficient video
  recognition.
\newblock \emph{arXiv preprint arXiv:2105.05165}, 2021.

\bibitem[Park et~al.(2017)Park, Hong, and Lee]{park2017rdfnet}
Park, S.-J., Hong, K.-S., and Lee, S.
\newblock Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic
  segmentation.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  4980--4989, 2017.

\bibitem[Peng et~al.(2022)Peng, Wei, Deng, Wang, and Hu]{peng2022balanced}
Peng, X., Wei, Y., Deng, A., Wang, D., and Hu, D.
\newblock Balanced multimodal learning via on-the-fly gradient modulation.
\newblock \emph{arXiv preprint arXiv:2203.15332}, 2022.

\bibitem[Pezeshki et~al.(2020)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2020gradient}
Pezeshki, M., Kaba, S.-O., Bengio, Y., Courville, A., Precup, D., and Lajoie,
  G.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{arXiv preprint arXiv:2011.09468}, 2020.

\bibitem[Pham et~al.(2019)Pham, Liang, Manzini, Morency, and
  P{\'o}czos]{pham2019found}
Pham, H., Liang, P.~P., Manzini, T., Morency, L.-P., and P{\'o}czos, B.
\newblock Found in translation: Learning robust joint representations by cyclic
  translations between modalities.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  6892--6899, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Romero et~al.(2014)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
Romero, A., Ballas, N., Kahou, S.~E., Chassang, A., Gatta, C., and Bengio, Y.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{arXiv preprint arXiv:1412.6550}, 2014.

\bibitem[Seichter et~al.(2020)Seichter, K{\"o}hler, Lewandowski, Wengefeld, and
  Gross]{esanet2020}
Seichter, D., K{\"o}hler, M., Lewandowski, B., Wengefeld, T., and Gross, H.-M.
\newblock Efficient rgb-d semantic segmentation for indoor scene analysis.
\newblock \emph{arXiv preprint arXiv:2011.06961}, 2020.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock \emph{arXiv preprint arXiv:2006.07710}, 2020.

\bibitem[Silberman et~al.(2012)Silberman, Hoiem, Kohli, and
  Fergus]{silberman2012indoor}
Silberman, N., Hoiem, D., Kohli, P., and Fergus, R.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In \emph{European conference on computer vision}, pp.\  746--760.
  Springer, 2012.

\bibitem[Smith \& Gasser(2005)Smith and Gasser]{smith2005development}
Smith, L. and Gasser, M.
\newblock The development of embodied cognition: Six lessons from babies.
\newblock \emph{Artificial life}, 11\penalty0 (1-2):\penalty0 13--29, 2005.

\bibitem[Soomro et~al.(2012)Soomro, Zamir, and Shah]{soomro2012ucf101}
Soomro, K., Zamir, A.~R., and Shah, M.
\newblock Ucf101: A dataset of 101 human actions classes from videos in the
  wild.
\newblock \emph{arXiv preprint arXiv:1212.0402}, 2012.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Tan \& Bansal(2020)Tan and Bansal]{tan2020vokenization}
Tan, H. and Bansal, M.
\newblock Vokenization: Improving language understanding with contextualized,
  visual-grounded supervision.
\newblock \emph{arXiv preprint arXiv:2010.06775}, 2020.

\bibitem[Tian et~al.(2019)Tian, Krishnan, and Isola]{tian2019contrastive}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive representation distillation.
\newblock \emph{arXiv preprint arXiv:1910.10699}, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Zhan, Thompson, and
  Zhou]{wang2020multimodal}
Wang, Q., Zhan, L., Thompson, P., and Zhou, J.
\newblock Multimodal learning with incomplete modalities by knowledge
  distillation.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  1828--1838, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Tran, and Feiszli]{wang2020makes}
Wang, W., Tran, D., and Feiszli, M.
\newblock What makes training multi-modal classification networks hard?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12695--12705, 2020{\natexlab{b}}.

\bibitem[Wu et~al.(2022)Wu, Jastrzebski, Cho, and Geras]{wu2022characterizing}
Wu, N., Jastrzebski, S., Cho, K., and Geras, K.~J.
\newblock Characterizing and overcoming the greedy nature of learning in
  multi-modal deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  24043--24055. PMLR, 2022.

\bibitem[Xiao et~al.(2020)Xiao, Lee, Grauman, Malik, and
  Feichtenhofer]{xiao2020audiovisual}
Xiao, F., Lee, Y.~J., Grauman, K., Malik, J., and Feichtenhofer, C.
\newblock Audiovisual slowfast networks for video recognition.
\newblock \emph{arXiv preprint arXiv:2001.08740}, 2020.

\bibitem[Xu et~al.(2013)Xu, Tao, and Xu]{xu2013survey}
Xu, C., Tao, D., and Xu, C.
\newblock A survey on multi-view learning.
\newblock \emph{arXiv preprint arXiv:1304.5634}, 2013.

\end{thebibliography}
