\begin{thebibliography}{10}

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{settles2009active}
Burr Settles.
\newblock Active learning literature survey.
\newblock Technical report, University of Wisconsin--Madison, 2009.

\bibitem{hanneke2014theory}
Steve Hanneke et~al.
\newblock Theory of disagreement-based active learning.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  7(2-3):131--309, 2014.

\bibitem{cohn1994improving}
David Cohn, Les Atlas, and Richard Ladner.
\newblock Improving generalization with active learning.
\newblock {\em Machine learning}, 15(2):201--221, 1994.

\bibitem{balcan2009agnostic}
Maria-Florina Balcan, Alina Beygelzimer, and John Langford.
\newblock Agnostic active learning.
\newblock {\em Journal of Computer and System Sciences}, 75(1):78--89, 2009.

\bibitem{dasgupta2008hierarchical}
Sanjoy Dasgupta and Daniel Hsu.
\newblock Hierarchical sampling for active learning.
\newblock In {\em International conference on Machine learning}, pages
  208--215, 2008.

\bibitem{balcan2007margin}
Maria{-}Florina Balcan, Andrei~Z. Broder, and Tong Zhang.
\newblock Margin based active learning.
\newblock In {\em Conference on Learning Theory}, volume 4539, pages 35--50,
  2007.

\bibitem{balcan2013active}
Maria{-}Florina Balcan and Philip~M. Long.
\newblock Active and passive learning of linear separators under log-concave
  distributions.
\newblock In {\em Conference on Learning Theory}, volume~30, pages 288--316,
  2013.

\bibitem{karzand2019active}
Mina Karzand and Robert~D. Nowak.
\newblock Active learning in the overparameterized and interpolating regime.
\newblock {\em CoRR}, abs/1905.12782, 2019.

\bibitem{kirsch2019batchbald}
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal.
\newblock Batchbald: Efficient and diverse batch acquisition for deep bayesian
  active learning.
\newblock In {\em Neural Information Processing Systems}, pages 7024--7035,
  2019.

\bibitem{ash2019deep}
Jordan~T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh
  Agarwal.
\newblock Deep batch active learning by diverse, uncertain gradient lower
  bounds.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{jacot2018NTK}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In {\em Neural Information Processing Systems}, pages 8580--8589,
  2018.

\bibitem{arora2019fineGrained}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, volume~97,
  pages 322--332, 2019.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S. Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl{-}Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Neural Information Processing Systems}, pages 8570--8581,
  2019.

\bibitem{hanin2019finiteNTK}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  32:10836--10846, 2019.

\bibitem{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em International Conference on Machine Learning}, volume~48,
  pages 1225--1234, 2016.

\bibitem{liu2017algorithmic}
Tongliang Liu, G{\'{a}}bor Lugosi, Gergely Neu, and Dacheng Tao.
\newblock Algorithmic stability and hypothesis complexity.
\newblock In {\em International Conference on Machine Learning}, volume~70,
  pages 2159--2167, 2017.

\bibitem{lyle2020bayesian}
Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van~der Wilk.
\newblock A bayesian perspective on training speed and model selection.
\newblock In {\em Neural Information Processing Systems}, 2020.

\bibitem{ru2020revisiting}
Binxin Ru, Clare Lyle, Lisa Schut, Mark van~der Wilk, and Yarin Gal.
\newblock Revisiting the train loss: an efficient performance estimator for
  neural architecture search.
\newblock {\em arXiv preprint arXiv:2006.04492}, 2020.

\bibitem{xu2021optimization}
Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi.
\newblock Optimization of graph neural networks: Implicit acceleration by skip
  connections and more depth.
\newblock In {\em International Conference on Machine Learning}, volume 139,
  pages 11592--11602, 2021.

\bibitem{borgwardt2006integrating}
Karsten~M Borgwardt, Arthur Gretton, Malte~J Rasch, Hans-Peter Kriegel,
  Bernhard Sch{\"o}lkopf, and Alex~J Smola.
\newblock Integrating structured biological data by kernel maximum mean
  discrepancy.
\newblock {\em Bioinformatics}, 22(14):e49--e57, 2006.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Ro~Bissacco, Bo~Wu, and Andrew~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning,
  2011.

\bibitem{fei2004learning}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In {\em 2004 conference on computer vision and pattern recognition
  workshop}, pages 178--178. IEEE, 2004.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 770--778, 2016.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{mu2020gradients}
Fangzhou Mu, Yingyu Liang, and Yin Li.
\newblock Gradients as features for deep representation learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{liu2021influence}
Zhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He.
\newblock Influence selection for active learning.
\newblock {\em arXiv preprint arXiv:2108.09331}, 2021.

\bibitem{huang2016active}
Jiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh, and Adam
  Coates.
\newblock Active learning for speech recognition: the power of gradients.
\newblock {\em arXiv preprint arXiv:1612.03226}, 2016.

\bibitem{shukla2021egl++}
Megh Shukla.
\newblock Egl++: Extending expected gradient length to active learning for
  human pose estimation.
\newblock {\em arXiv preprint arXiv:2104.09493}, 2021.

\bibitem{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em International Conference on Machine Learning}, volume~70,
  pages 1885--1894, 2017.

\bibitem{arora2019exact}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Neural Information Processing Systems}, pages 8139--8148,
  2019.

\bibitem{wang2015querying}
Zheng Wang and Jieping Ye.
\newblock Querying discriminative and representative samples for batch mode
  active learning.
\newblock {\em ACM Transactions on Knowledge Discovery from Data}, 9(3):1--23,
  2015.

\bibitem{jia2021efficient}
Sheng Jia, Ehsan Nezhadarya, Yuhuai Wu, and Jimmy Ba.
\newblock Efficient statistical tests: {A} neural tangent kernel approach.
\newblock In {\em International Conference on Machine Learning}, volume 139,
  pages 4893--4903, 2021.

\bibitem{kendall1938new}
Maurice~G Kendall.
\newblock A new measure of rank correlation.
\newblock {\em Biometrika}, 30(1/2):81--93, 1938.

\bibitem{du2019GNTK}
Simon~S Du, Kangcheng Hou, Barnab{\'a}s P{\'o}czos, Ruslan Salakhutdinov,
  Ruosong Wang, and Keyulu Xu.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock {\em arXiv preprint arXiv:1905.13192}, 2019.

\bibitem{alemohammad2020recurrent}
Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk.
\newblock The recurrent neural tangent kernel.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{huang2020neural}
Wei Huang, Weitao Du, and Richard~Yi Da~Xu.
\newblock On the neural tangent kernel of deep networks with orthogonal
  initialization.
\newblock {\em arXiv preprint arXiv:2004.05867}, 2020.

\bibitem{huang2022demystify}
Wei Huang, Chunrui Liu, Yilan Chen, Tianyu Liu, and Richard~Yi Da~Xu.
\newblock Demystify optimization and generalization of over-parameterized
  pac-bayesian learning.
\newblock {\em arXiv preprint arXiv:2202.01958}, 2022.

\bibitem{huang2021towards}
Wei Huang, Yayong Li, Weitao Du, Richard~Yi Da~Xu, Jie Yin, Ling Chen, and Miao
  Zhang.
\newblock Towards deepening graph neural networks: A gntk-based optimization
  perspective.
\newblock {\em arXiv preprint arXiv:2103.03113}, 2021.

\bibitem{du2015exploring}
Bo~Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang, Wei Liu, Jialie Shen, and
  Dacheng Tao.
\newblock Exploring representativeness and informativeness for active learning.
\newblock {\em IEEE transactions on cybernetics}, 47(1):14--26, 2015.

\bibitem{volpi2018generalizing}
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John~C. Duchi, Vittorio Murino,
  and Silvio Savarese.
\newblock Generalizing to unseen domains via adversarial data augmentation.
\newblock In {\em Neural Information Processing Systems}, pages 5339--5349,
  2018.

\bibitem{zhdanov2019diverse}
Fedor Zhdanov.
\newblock Diverse mini-batch active learning.
\newblock {\em arXiv preprint arXiv:1901.05954}, 2019.

\bibitem{roy2001toward}
Nicholas Roy and Andrew McCallum.
\newblock Toward optimal active learning through sampling estimation of error
  reduction.
\newblock In {\em International Conference on Machine Learning}, pages
  441--448, 2001.

\bibitem{zhu2012uncertainty}
Jingbo Zhu and Matthew Ma.
\newblock Uncertainty-based active learning with instability estimation for
  text classification.
\newblock {\em ACM Transactions on Speech and Language Processing (TSLP)},
  8(4):1--21, 2012.

\bibitem{yang2016active}
Yazhou Yang and Marco Loog.
\newblock Active learning using uncertainty information.
\newblock In {\em International Conference on Pattern Recognition}, pages
  2646--2651, 2016.

\bibitem{yoo2019learning}
Donggeun Yoo and In~So Kweon.
\newblock Learning loss for active learning.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 93--102, 2019.

\bibitem{settles2007multiple}
Burr Settles, Mark Craven, and Soumya Ray.
\newblock Multiple-instance active learning.
\newblock In {\em Neural Information Processing Systems}, pages 1289--1296,
  2007.

\bibitem{awasthi2021neural}
Pranjal Awasthi, Christoph Dann, Claudio Gentile, Ayush Sekhari, and Zhilei
  Wang.
\newblock Neural active learning with performance guarantees.
\newblock {\em arXiv preprint arXiv:2106.03243}, 2021.

\bibitem{Ban2022ImprovedAF}
Yikun Ban, Yuheng Zhang, Hanghang Tong, Arindam Banerjee, and Jingrui He.
\newblock Improved algorithms for neural active learning.
\newblock 2022.

\bibitem{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em The Journal of Machine Learning Research}, 13(1):723--773, 2012.

\bibitem{dangel2020backpack}
Felix Dangel, Frederik Kunstner, and Philipp Hennig.
\newblock Back{PACK}: Packing more into backprop.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{rame2022fishr}
Alexandre Rame, Corentin Dancette, and Matthieu Cord.
\newblock Fishr: Invariant gradient variances for out-of-distribution
  generalization.
\newblock In {\em International Conference on Machine Learning}, pages
  18347--18377. PMLR, 2022.

\bibitem{subramani2021enabling}
Pranav Subramani, Nicholas Vadivelu, and Gautam Kamath.
\newblock Enabling fast differentially private sgd via just-in-time compilation
  and vectorization.
\newblock {\em Advances in Neural Information Processing Systems},
  34:26409--26421, 2021.

\bibitem{zandieh2021scaling}
Amir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin.
\newblock Scaling neural tangent kernels via sketching and random features.
\newblock {\em arXiv preprint arXiv:2106.07880}, 2021.

\bibitem{botsch2011chapter}
R~Botsch.
\newblock Chapter 12: Significance and measures of association.
\newblock {\em Scopes and Methods of Political Science}, 2011.

\bibitem{sener2017active}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: {A} core-set
  approach.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wang2014new}
Dan Wang and Yi~Shang.
\newblock A new active labeling method for deep learning.
\newblock In {\em International Joint Conference on Neural Networks}, pages
  112--119, 2014.

\bibitem{roth2006margin}
Dan Roth and Kevin Small.
\newblock Margin-based active learning for structured output spaces.
\newblock In {\em Machine Learning: {ECML} 2006, 17th European Conference on
  Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings},
  volume 4212, pages 413--424, 2006.

\bibitem{hsu2015active}
Wei{-}Ning Hsu and Hsuan{-}Tien Lin.
\newblock Active learning by learning.
\newblock In {\em AAAI Conference on Artificial Intelligence}, pages
  2659--2665, 2015.

\bibitem{lee2020finite}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,
  Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15156--15172, 2020.

\bibitem{fort2020deep}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
  Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5850--5861, 2020.

\bibitem{park2020towards}
Daniel~S Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein.
\newblock Towards nngp-guided neural architecture search.
\newblock {\em arXiv preprint arXiv:2011.06006}, 2020.

\bibitem{chen2021neural}
Wuyang Chen, Xinyu Gong, and Zhangyang Wang.
\newblock Neural architecture search on imagenet in four gpu hours: A
  theoretically inspired perspective.
\newblock {\em arXiv preprint arXiv:2102.11535}, 2021.

\bibitem{chen2021vision}
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong.
\newblock When vision transformers outperform resnets without pre-training or
  strong data augmentations.
\newblock {\em arXiv preprint arXiv:2106.01548}, 2021.

\bibitem{deshpande2021linearized}
Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca
  Zancato, Charless Fowlkes, Rahul Bhotika, Stefano Soatto, and Pietro Perona.
\newblock A linearized framework and a new benchmark for model selection for
  fine-tuning.
\newblock {\em arXiv preprint arXiv:2102.00084}, 2021.

\end{thebibliography}
