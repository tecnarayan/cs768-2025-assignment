========================================================================================================
=========================================== BOOK     ===================================================
========================================================================================================


============================================= AAA ======================================================

@book{agre1996computational,
	title={Computational theories of interaction and agency},
	author={Agre, Philip and Rosenschein, Stanley J},
	year={1996},
	publisher={Mit Press}
}
============================================= BBB ======================================================
============================================= CCC ======================================================
@book{chinchuluun2008pareto,
	title={Pareto optimality, game theory and equilibria},
	author={Chinchuluun, Altannar and Pardalos, Panos M and Migdalas, Athanasios and Pitsoulis, Leonidas},
	year={2008},
	publisher={Springer}
}

@book{baldassarre2013intrinsically,
	title={Intrinsically motivated learning in natural and artificial systems},
	author={Baldassarre, Gianluca and Mirolli, Marco},
	year={2013},
	publisher={Springer}
}

@book{Cover:book,
	title={Elements of Information Theory },
	author={Cover, T. M. and Thomas, J. A.},
	year={2006},
	publisher={Wiley}
}

============================================= DDD ======================================================
============================================= EEE ======================================================
============================================= FFF ======================================================
============================================= GGG ======================================================
============================================= HHH ======================================================


============================================= III ======================================================
============================================= JJJ ======================================================
============================================= KKK ======================================================
============================================= LLL ======================================================
@techreport{lin1993reinforcement,
  title={Reinforcement learning for robots using neural networks},
  author={Lin, Long-Ji},
  year={1993},
  institution={Carnegie-Mellon Univ Pittsburgh PA School of Computer Science}
}
============================================= MMM ======================================================
============================================= NNN ======================================================
============================================= OOO ======================================================
============================================= PPP ======================================================
============================================= QQQ ======================================================
============================================= RRR ======================================================
============================================= SSS ======================================================
@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={R. S. Sutton and A. G. Barto},
  publisher =    {The MIT Press},
  year =     {1998},
  address =      {Cambridge, MA}
}

@book{sugiyama2012density,
  title={Density ratio estimation in machine learning},
  author={Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
  year={2012},
  publisher={Cambridge University Press}
}
============================================= TTT ======================================================
============================================= UUU ======================================================
============================================= VVV ======================================================
============================================= WWW ======================================================
============================================= XXX ======================================================
============================================= YYY ======================================================
============================================= ZZZ ======================================================


========================================================================================================
=========================================== ARTICLE  ===================================================
========================================================================================================


============================================= AAA ======================================================
@article{AmariNatural,
  title={Natural gradient works efficiently in learning},
  author={S. Amari},
  journal={Neural Computation},
  vol={10},
  pages={251 - 276},
  year={1998}
}

@inproceedings{ahmed2019understanding,
  title={Understanding the impact of entropy on policy optimization},
  author={Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={151--160},
  year={2019},
  organization={PMLR}
}
============================================= BBB ======================================================
============================================= CCC ======================================================
============================================= DDD ======================================================
============================================= EEE ======================================================
============================================= FFF ======================================================
============================================= GGG ======================================================
============================================= HHH ======================================================
@article{Hunter&Lange:TAS04,
  title={A tutorial on {MM} algorithms},
  author={D. R. Hunter and K. Lange},
  journal={The American Statistician},
  vol={58},
  no={1},
  pages={30 - 37},
  month={Feb.},
  year={2004}
}
============================================= III ======================================================
============================================= JJJ ======================================================
============================================= KKK ======================================================
============================================= LLL ======================================================
============================================= MMM ======================================================
============================================= NNN ======================================================
============================================= OOO ======================================================
============================================= PPP ======================================================
@article{puterman1979convergence,
  title={On the convergence of policy iteration in stationary dynamic programming},
  author={Puterman, Martin L and Brumelle, Shelby L},
  journal={Mathematics of Operations Research},
  volume={4},
  number={1},
  pages={60--69},
  year={1979},
  publisher={INFORMS}
}
============================================= QQQ ======================================================
============================================= RRR ======================================================

============================================= SSS ======================================================
@article{santos2004convergence,
  title={Convergence properties of policy iteration},
  author={Santos, Manuel S and Rust, John},
  journal={SIAM Journal on Control and Optimization},
  volume={42},
  number={6},
  pages={2094--2115},
  year={2004},
  publisher={SIAM}
}
============================================= TTT ======================================================
============================================= UUU ======================================================
============================================= VVV ======================================================
============================================= WWW ======================================================
============================================= XXX ======================================================
============================================= YYY ======================================================
============================================= ZZZ ======================================================




========================================================================================================
=========================================== CONFERENCE==================================================
========================================================================================================


============================================= AAA ======================================================

@article{achiam2017surprise,
	title={Surprise-based intrinsic motivation for deep reinforcement learning},
	author={Achiam, Joshua and Sastry, Shankar},
	journal={arXiv preprint arXiv:1703.01732},
	year={2017}
}

============================================= BBB ======================================================
@inproceedings{baird1994reinforcement,
  title={Reinforcement learning in continuous time: Advantage updating},
  author={Baird, Leemon C},
  booktitle={Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on},
  volume={4},
  pages={2448--2453},
  year={1994},
  organization={IEEE}
}
@article{bradtke1996linear,
	title={Linear least-squares algorithms for temporal difference learning},
	author={Bradtke, Steven J and Barto, Andrew G},
	journal={Machine learning},
	volume={22},
	number={1-3},
	pages={33--57},
	year={1996},
	publisher={Springer}
}
@incollection{barto2013intrinsic,
	title={Intrinsic motivation and reinforcement learning},
	author={Barto, Andrew G},
	booktitle={Intrinsically motivated learning in natural and artificial systems},
	pages={17--47},
	year={2013},
	publisher={Springer}
}
@article{brockman2016openai,
  title={OpenAI gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}
@inproceedings{bellemare2016unifying,
title={Unifying count-based exploration and intrinsic motivation},
author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
booktitle={Advances in Neural Information Processing Systems},
pages={1471--1479},
year={2016}
}
@article{burda2018exploration,
	title={Exploration by random network distillation},
	author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
	journal={arXiv preprint arXiv:1810.12894},
	year={2018}
}


============================================= CCC ======================================================
@inproceedings{chentanez2005intrinsically,
	title={Intrinsically motivated reinforcement learning},
	author={Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},
	booktitle={Advances in neural information processing systems},
	pages={1281--1288},
	year={2005}
}

============================================= DDD ======================================================
@article{degris2012off,
	title={Off-policy actor-critic},
	author={Degris, Thomas and White, Martha and Sutton, Richard S},
	journal={arXiv preprint arXiv:1205.4839},
	year={2012}
}


============================================= EEE ======================================================
@article{espeholt2018impala,
	title={IMPALA: Scalable distributed Deep-RL with importance weighted actor-learner architectures},
	author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
	journal={arXiv preprint arXiv:1802.01561},
	year={2018}
}

@article{eysenbach2018diversity,
  title={Diversity is all you need: Learning skills without a reward function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}
============================================= FFF ======================================================
@article{fox2015taming,

	title={Taming the noise in reinforcement learning via soft updates},

	author={Fox, Roy and Pakman, Ari and Tishby, Naftali},

	journal={arXiv preprint arXiv:1512.08562},

	year={2015}

}
@article{fujimoto2018addressing,
	title={Addressing Function Approximation Error in Actor-Critic Methods},
	author={Fujimoto, Scott and van Hoof, Herke and Meger, Dave},
	journal={arXiv preprint arXiv:1802.09477},
	year={2018}
}
============================================= GGG ======================================================

@inproceedings{goodfellow2014generative,
	title={Generative adversarial nets},
	author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle={Advances in neural information processing systems},
	pages={2672--2680},
	year={2014}
}
@article{gu2016q,
	title={Q-prop: Sample-efficient policy gradient with an off-policy critic},
	author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
	journal={arXiv preprint arXiv:1611.02247},
	year={2016}
}
@inproceedings{gu2017interpolated,
	title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
	author={Gu, Shixiang Shane and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3846--3855},
	year={2017}
}
@article{gangwani2018learning,
	title={Learning self-imitating diverse policies},
	author={Gangwani, Tanmay and Liu, Qiang and Peng, Jian},
	journal={arXiv preprint arXiv:1805.10309},
	year={2018}
}

@article{guo2018generative,
  title={Generative adversarial self-imitation learning},
  author={Guo, Yijie and Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  journal={arXiv preprint arXiv:1812.00950},
  year={2018}
}
============================================= HHH ======================================================
@inproceedings{hazan2019provably,
  title={Provably efficient maximum entropy exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019},
  organization={PMLR}
}
@article{heess2017emergence,
	title={Emergence of locomotion behaviours in rich environments},
	author={Heess, Nicolas and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, Ali and Riedmiller, Martin and others},
	journal={arXiv preprint arXiv:1707.02286},
	year={2017}
}

@article{han2019dimension,
	title={Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning},
	author={Han, Seungyul and Sung, Youngchul},
	journal={International Conference on Machine Learning},
	year={2019}
}
@article{haarnoja2017rein,
	title={Reinforcement learning with deep energy-based policies},
	author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	journal={arXiv preprint arXiv:1702.08165},
	year={2017}
}
@article{haarnoja2018soft2,
	title={Soft actor-critic algorithms and applications},
	author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
	journal={arXiv preprint arXiv:1812.05905},
	year={2018}
}

@inproceedings{houthooft2016vime,
	title={Vime: Variational information maximizing exploration},
	author={Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1109--1117},
	year={2016}
}
@inproceedings{hong2018diversity,
	title={Diversity-driven exploration strategy for deep reinforcement learning},
	author={Hong, Zhang-Wei and Shann, Tzu-Yun and Su, Shih-Yang and Chang, Yi-Hsiang and Fu, Tsu-Jui and Lee, Chun-Yi},
	booktitle={Advances in Neural Information Processing Systems},
	pages={10489--10500},
	year={2018}
}

@article{haarnoja2018soft,
	title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
	author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	journal={arXiv preprint arXiv:1801.01290},
	year={2018}
}
============================================= III ======================================================
============================================= JJJ

@inproceedings{jie2010connection,
	title={On a connection between importance sampling and the likelihood ratio policy gradient},
	author={Jie, Tang and Abbeel, Pieter},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1000--1008},
	year={2010}
}
======================================================
============================================= KKK ======================================================
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}
@inproceedings{kakade2002natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1531--1538},
  year={2002}
}

@article{kingma2013auto,
	title={Auto-encoding variational bayes},
	author={Kingma, Diederik P and Welling, Max},
	journal={arXiv preprint arXiv:1312.6114},
	year={2013}
}

============================================= LLL ======================================================
@inproceedings{lopes2012exploration,
	title={Exploration in model-based reinforcement learning by empirically estimating learning progress},
	author={Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-Yves},
	booktitle={Advances in neural information processing systems},
	pages={206--214},
	year={2012}
}

@article{liu2017effects,
	title={The effects of memory replay in reinforcement learning},
	author={Liu, Ruishan and Zou, James},
	journal={arXiv preprint arXiv:1710.06574},
	year={2017}
}
@inproceedings{levine2013guided,
	title={Guided policy search},
	author={Levine, Sergey and Koltun, Vladlen},
	booktitle={International Conference on Machine Learning},
	pages={1--9},
	year={2013}
}

@article{lillicrap2015continuous,
	title={Continuous control with deep reinforcement learning},
	author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	journal={arXiv preprint arXiv:1509.02971},
	year={2015}
}
@article{liu2017sample,
	title={Sample-efficient Policy Optimization with Stein Control Variate},
	author={Liu, Hao and Feng, Yihao and Mao, Yi and Zhou, Dengyong and Peng, Jian and Liu, Qiang},
	journal={arXiv preprint arXiv:1710.11198},
	year={2017}
}
@article{lee2019tsallis,
	
	title={Tsallis Reinforcement Learning: A Unified Framework for Maximum Entropy Reinforcement Learning},
	
	author={Lee, Kyungjae and Kim, Sungyub and Lim, Sungbin and Choi, Sungjoon and Oh, Songhwai},
	
	journal={arXiv preprint arXiv:1902.00137},
	
	year={2019}
	
}
============================================= MMM ======================================================
@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}
@article{meuleau2000off,
	title={Off-policy policy search},
	author={Meuleau, Nicolas and Peshkin, Leonid and Kaelbling, Leslie P and Kim, Kee-Eung}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year={2016}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Research}
}
@inproceedings{munos2016safe,
	title={Safe and efficient off-policy reinforcement learning},
	author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1054--1062},
	year={2016}
}
@article{martin2017count,
	title={Count-based exploration in feature space for reinforcement learning},
	author={Martin, Jarryd and Sasikumar, Suraj Narayanan and Everitt, Tom and Hutter, Marcus},
	journal={arXiv preprint arXiv:1706.08090},
	year={2017}
}

@article{mazoure2019leveraging,
  title={Leveraging exploration in off-policy algorithms via normalizing flows},
  author={Mazoure, Bogdan and Doan, Thang and Durand, Audrey and Hjelm, R Devon and Pineau, Joelle},
  journal={arXiv preprint arXiv:1905.06893},
  year={2019}
}

============================================= NNN ======================================================
@article{nachum2017trust,
	title={Trust-pcl: An off-policy trust region method for continuous control},
	author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
	journal={arXiv preprint arXiv:1707.01891},
	year={2017}
}
@inproceedings{nachum2017bridging,

	title={Bridging the gap between value and policy based reinforcement learning},

	author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},

	booktitle={Advances in Neural Information Processing Systems},

	pages={2775--2785},

	year={2017}

}

@article{nielsen2019jensen,
	title={On the Jensen--Shannon Symmetrization of Distances Relying on Abstract Means},
	author={Nielsen, Frank},
	journal={Entropy},
	volume={21},
	number={5},
	pages={485},
	year={2019},
	publisher={Multidisciplinary Digital Publishing Institute}
}

============================================= OOO ======================================================

@article{o2016combining,

	title={Combining policy gradient and Q-learning},

	author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},

	journal={arXiv preprint arXiv:1611.01626},

	year={2016}

}


============================================= PPP ======================================================
@article{peshkin2002learning,
	title={Learning from scarce experience},
	author={Peshkin, Leonid and Shelton, Christian R},
	journal={arXiv preprint cs/0204043},
	year={2002}
}
@article{precup2000eligibility,
	title={Eligibility traces for off-policy policy evaluation},
	author={Precup, Doina},
	journal={Computer Science Department Faculty Publication Series},
	pages={80},
	year={2000}
}
@article{plappert2017parameter,
  title={Parameter space noise for exploration},
  author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  journal={arXiv preprint arXiv:1706.01905},
  year={2017}
}

@inproceedings{peters2005natural,
	title={Natural actor-critic},
	author={Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
	booktitle={European Conference on Machine Learning},
	pages={280--291},
	year={2005},
	organization={Springer}
}
@inproceedings{pathak2017curiosity,
	title={Curiosity-driven exploration by self-supervised prediction},
	author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
	pages={16--17},
	year={2017}
}



============================================= QQQ ======================================================
============================================= RRR ======================================================
@inproceedings{rawlik2013stochastic,

	title={On stochastic optimal control and reinforcement learning by approximate inference},

	author={Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},

	booktitle={Twenty-Third International Joint Conference on Artificial Intelligence},

	year={2013}

}

============================================= SSS ======================================================


@article{shelton2001importance,
	title={Importance sampling for reinforcement learning with multiple objectives},
	author={Shelton, Christian Robert},
	year={2001}
}
@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1057--1063},
  year={2000}
}

@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018}
}

@article{sutton1988learning,
	title={Learning to predict by the methods of temporal differences},
	author={Sutton, Richard S},
	journal={Machine learning},
	volume={3},
	number={1},
	pages={9--44},
	year={1988},
	publisher={Springer}
}
@article{schulman2015high,
	title={High-dimensional continuous control using generalized advantage estimation},
	author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1506.02438},
	year={2015}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  pages={1889--1897},
  year={2015}
}
@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@inproceedings{silver2014deterministic,
	title={Deterministic policy gradient algorithms},
	author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	booktitle={ICML},
	year={2014}
}
@article{schulman2017equivalence,

	title={Equivalence between policy gradients and soft q-learning},

	author={Schulman, John and Chen, Xi and Abbeel, Pieter},

	journal={arXiv preprint arXiv:1704.06440},

	year={2017}

}
@article{strehl2008analysis,
	title={An analysis of model-based interval estimation for Markov decision processes},
	author={Strehl, Alexander L and Littman, Michael L},
	journal={Journal of Computer and System Sciences},
	volume={74},
	number={8},
	pages={1309--1331},
	year={2008},
	publisher={Elsevier}
}

============================================= TTT ======================================================
@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}
@inproceedings{todorov2008general,

	title={General duality between optimal control and estimation},

	author={Todorov, Emanuel},

	booktitle={2008 47th IEEE Conference on Decision and Control},

	pages={4286--4292},

	year={2008},

	organization={IEEE}

}
@inproceedings{tang2017exploration,
	title={\# Exploration: A study of count-based exploration for deep reinforcement learning},
	author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, OpenAI Xi and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
	booktitle={Advances in neural information processing systems},
	pages={2753--2762},
	year={2017}
}

@inproceedings{toussaint2009robot,

	title={Robot trajectory optimization using approximate inference},

	author={Toussaint, Marc},

	booktitle={Proceedings of the 26th annual international conference on machine learning},

	pages={1049--1056},

	year={2009},

	organization={ACM}

}


============================================= UUU ======================================================
============================================= VVV ======================================================
============================================= WWW ======================================================
@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine Learning},
  volume={8},
  number={3},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
@inproceedings{wu2017scalable,
	title={Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
	author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
	booktitle={Advances in neural information processing systems},
	pages={5279--5288},
	year={2017}
}
@article{wawrzynski2009real,
	title={Real-time reinforcement learning by sequential actor--critics and experience replay},
	author={Wawrzy{\'n}ski, Pawe{\l}},
	journal={Neural Networks},
	volume={22},
	number={10},
	pages={1484--1497},
	year={2009},
	publisher={Elsevier}
}

============================================= XXX ======================================================
============================================= YYY ======================================================
============================================= ZZZ ======================================================
@article{ziebart2008maximum,

	title={Maximum entropy inverse reinforcement learning},

	author={Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},

	year={2008},

	publisher={figshare}

}

@phdthesis{ziebart2010modeling,
	title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
	author={Ziebart, Brian D},
	year={2010},
	school={figshare}
}
@inproceedings{zheng2018learning,
	title={On learning intrinsic rewards for policy gradient methods},
	author={Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4644--4654},
	year={2018}
}
@misc{catto2011box2d,
  title={Box2d: A 2d physics engine for games},
  author={Catto, Erin},
  year={2011}
}

@misc{baselines,
	author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
	title = {OpenAI Baselines},
	year = {2017},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/openai/baselines}},
}
