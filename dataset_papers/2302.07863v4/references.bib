
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}
@inproceedings{zoph2018learning,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}

@article{williams2009roofline,
  title={Roofline: an insightful visual performance model for multicore architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{jiang2023llm,
  title={LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2306.02561},
  year={2023}
}

@article{ng2019facebook,
  title={Facebook FAIR's WMT19 news translation task submission},
  author={Ng, Nathan and Yee, Kyra and Baevski, Alexei and Ott, Myle and Auli, Michael and Edunov, Sergey},
  journal={arXiv preprint arXiv:1907.06616},
  year={2019}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Chaumond, Julien and Debut, Lysandre and Sanh, Victor and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and others},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020}
}

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q and Tay, Yi and Metzler, Donald},
  journal={arXiv preprint arXiv:2207.07061},
  year={2022}
}

@article{gu2017non,
  title={Non-autoregressive neural machine translation},
  author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
  journal={arXiv preprint arXiv:1711.02281},
  year={2017}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}


@article{schuster2021consistent,
  title={Consistent accelerated inference via confident adaptive transformers},
  author={Schuster, Tal and Fisch, Adam and Jaakkola, Tommi and Barzilay, Regina},
  journal={arXiv preprint arXiv:2104.08803},
  year={2021}
}

@article{xin2020deebert,
  title={DeeBERT: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}



@article{so2021primer,
  title={Primer: Searching for efficient transformers for language modeling},
  author={So, David R and Ma{\'n}ke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.08668},
  year={2021}
}

@article{yin2021autotinybert,
  title={Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models},
  author={Yin, Yichun and Chen, Cheng and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  journal={arXiv preprint arXiv:2107.13686},
  year={2021}
}

@article{chen2020adabert,
  title={Adabert: Task-adaptive bert compression with differentiable neural architecture search},
  author={Chen, Daoyuan and Li, Yaliang and Qiu, Minghui and Wang, Zhen and Li, Bofang and Ding, Bolin and Deng, Hongbo and Huang, Jun and Lin, Wei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2001.04246},
  year={2020}
}
@inproceedings{dettmersgpt3,
  title={GPT3. int8 (): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{wu2022extreme,
  title={Extreme Compression for Pre-trained Transformers Made Simple and Efficient},
  author={Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01859},
  year={2022}
}

@article{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2101.01321},
  year={2021}
}

@article{yao2022zeroquant,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01861},
  year={2022}
}

@inproceedings{kitaev2019reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{xue2020mt5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}



@article{DBLP:journals/corr/SeeLM17,
  author    = {Abigail See and
               Peter J. Liu and
               Christopher D. Manning},
  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
  journal   = {CoRR},
  volume    = {abs/1704.04368},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04368},
  archivePrefix = {arXiv},
  eprint    = {1704.04368},
  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in neural information processing systems},
  pages={1693--1701},
  year={2015}
}

@InProceedings{wmt14,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@inproceedings{cettolo-etal-2017-overview,
    title = "Overview of the {IWSLT} 2017 Evaluation Campaign",
    author = {Cettolo, Mauro  and
      Federico, Marcello  and
      Bentivogli, Luisa  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Sudoh, Katsuhito  and
      Yoshino, Koichiro  and
      Federmann, Christian},
    booktitle = "Proceedings of the 14th International Conference on Spoken Language Translation",
    month = dec # " 14-15",
    year = "2017",
    address = "Tokyo, Japan",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2017.iwslt-1.1",
    pages = "2--14",
}

@inproceedings{tan2019mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2820--2828},
  year={2019}
}

@article{HLK+20RiseElevate,
author = {Hagedorn, Bastian and Lenfers, Johannes and Kundefinedhler, Thomas and Qin, Xueying and Gorlatch, Sergei and Steuwer, Michel},
title = {Achieving High-Performance the Functional Way: A Functional Pearl on Expressing High-Performance Optimizations as Rewrite Strategies},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {ICFP},
url = {https://doi.org/10.1145/3408974},
doi = {10.1145/3408974},
abstract = {Optimizing programs to run efficiently on modern parallel hardware is hard but crucial for many applications. The predominantly used imperative languages - like C or OpenCL - force the programmer to intertwine the code describing functionality and optimizations. This results in a portability nightmare that is particularly problematic given the accelerating trend towards specialized hardware devices to further increase efficiency. Many emerging DSLs used in performance demanding domains such as deep learning or high-performance image processing attempt to simplify or even fully automate the optimization process. Using a high-level - often functional - language, programmers focus on describing functionality in a declarative way. In some systems such as Halide or TVM, a separate schedule specifies how the program should be optimized. Unfortunately, these schedules are not written in well-defined programming languages. Instead, they are implemented as a set of ad-hoc predefined APIs that the compiler writers have exposed. In this functional pearl, we show how to employ functional programming techniques to solve this challenge with elegance. We present two functional languages that work together - each addressing a separate concern. RISE is a functional language for expressing computations using well known functional data-parallel patterns. ELEVATE is a functional language for describing optimization strategies. A high-level RISE program is transformed into a low-level form using optimization strategies written in ELEVATE . From the rewritten low-level program high-performance parallel code is automatically generated. In contrast to existing high-performance domain-specific systems with scheduling APIs, in our approach programmers are not restricted to a set of built-in operations and optimizations but freely define their own computational patterns in RISE and optimization strategies in ELEVATE in a composable and reusable way. We show how our holistic functional approach achieves competitive performance with the state-of-the-art imperative systems Halide and TVM.},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {92},
numpages = {29},
keywords = {Rewrite Rules, Strategy Languages, ELEVATE, Optimization Strategies}
}

@article{liu2018darts,
  title={Darts: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv preprint arXiv:1806.09055},
  year={2018}
}

@inproceedings{YBR+22Exo,
author = {Ikarashi, Yuka and Bernstein, Gilbert Louis and Reinking, Alex and Genc, Hasan and Ragan-Kelley, Jonathan},
title = {Exocompilation for Productive Programming of Hardware Accelerators},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523446},
doi = {10.1145/3519939.3523446},
abstract = {High-performance kernel libraries are critical to exploiting accelerators and specialized instructions in many applications. Because compilers are difficult to extend to support diverse and rapidly-evolving hardware targets, and automatic optimization is often insufficient to guarantee state-of-the-art performance, these libraries are commonly still coded and optimized by hand, at great expense, in low-level C and assembly. To better support development of high-performance libraries for specialized hardware, we propose a new programming language, Exo, based on the principle of exocompilation: externalizing target-specific code generation support and optimization policies to user-level code. Exo allows custom hardware instructions, specialized memories, and accelerator configuration state to be defined in user libraries. It builds on the idea of user scheduling to externalize hardware mapping and optimization decisions. Schedules are defined as composable rewrites within the language, and we develop a set of effect analyses which guarantee program equivalence and memory safety through these transformations. We show that Exo enables rapid development of state-of-the-art matrix-matrix multiply and convolutional neural network kernels, for both an embedded neural accelerator and x86 with AVX-512 extensions, in a few dozen lines of code each.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {703–718},
numpages = {16},
keywords = {program optimization, instruction abstraction, user-schedulable languages, user-extensible backend &amp; scheduling, hardware accelerators, scheduling},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@article{li21deepLearningCompilerSurvey,
  author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={The Deep Learning Compiler: A Comprehensive Survey}, 
  year={2021},
  volume={32},
  number={3},
  pages={708-727},
  doi={10.1109/TPDS.2020.3030548}}


@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}

@inproceedings{yu2020bignas,
  title={Bignas: Scaling up neural architecture search with big single-stage models},
  author={Yu, Jiahui and Jin, Pengchong and Liu, Hanxiao and Bender, Gabriel and Kindermans, Pieter-Jan and Tan, Mingxing and Huang, Thomas and Song, Xiaodan and Pang, Ruoming and Le, Quoc},
  booktitle={European Conference on Computer Vision},
  pages={702--717},
  year={2020},
  organization={Springer}
}
@article{cai2019once,
  title={Once-for-all: Train one network and specialize it for efficient deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  journal={arXiv preprint arXiv:1908.09791},
  year={2019}
}

@inproceedings{wu2019fbnet,
  title={Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search},
  author={Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10734--10742},
  year={2019}
}

@inproceedings{wan2020fbnetv2,
  title={Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions},
  author={Wan, Alvin and Dai, Xiaoliang and Zhang, Peizhao and He, Zijian and Tian, Yuandong and Xie, Saining and Wu, Bichen and Yu, Matthew and Xu, Tao and Chen, Kan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12965--12974},
  year={2020}
}

@inproceedings{so2019evolved,
  title={The evolved transformer},
  author={So, David and Le, Quoc and Liang, Chen},
  booktitle={International Conference on Machine Learning},
  pages={5877--5886},
  year={2019},
  organization={PMLR}
}

@inproceedings{guo2020single,
  title={Single path one-shot neural architecture search with uniform sampling},
  author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={544--560},
  year={2020},
  organization={Springer}
}

@inproceedings{xu2021bert,
  title={Nas-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search},
  author={Xu, Jin and Tan, Xu and Luo, Renqian and Song, Kaitao and Li, Jian and Qin, Tao and Liu, Tie-Yan},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1933--1943},
  year={2021}
}

@inproceedings{gong2021nasvit,
  title={NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training},
  author={Gong, Chengyue and Wang, Dilin and Li, Meng and Chen, Xinlei and Yan, Zhicheng and Tian, Yuandong and Chandra, Vikas and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{wang2020hat,
  title={Hat: Hardware-aware transformers for efficient natural language processing},
  author={Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2005.14187},
  year={2020}
}

@inproceedings{bojar2014findings,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}

@article{Amid2020chipyard,
  author={Amid, Alon and Biancolin, David and Gonzalez, Abraham and Grubb, Daniel and Karandikar, Sagar and Liew, Harrison and Magyar, Albert and Mao, Howard and Ou, Albert and Pemberton, Nathan and Rigge, Paul and Schmidt, Colin and Wright, John and Zhao, Jerry and Shao, Yakun Sophia and Asanović, Krste and Nikolić, Borivoje},
  journal={IEEE Micro}, 
  title={Chipyard: Integrated Design, Simulation, and Implementation Framework for Custom SoCs}, 
  year={2020},
  volume={40},
  number={4},
  pages={10-21},
  doi={10.1109/MM.2020.2996616}}
  
  @INPROCEEDINGS{karandikar2018firesim,
  author={S. {Karandikar} and H. {Mao} and D. {Kim} and D. {Biancolin} and A. {Amid} and D. {Lee} and N. {Pemberton} and E. {Amaro} and C. {Schmidt} and A. {Chopra} and Q. {Huang} and K. {Kovacs} and B. {Nikolic} and R. {Katz} and J. {Bachrach} and K. {Asanovic}},
  booktitle={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  title={FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud},
  year={2018},
  volume={},
  number={},
  pages={29-42},
  doi={10.1109/ISCA.2018.00014}}
  
  @techreport{asanovic2016rocket,
    Author = {Asanović, Krste and Avizienis, Rimas and Bachrach, Jonathan and Beamer, Scott and Biancolin, David and Celio, Christopher and Cook, Henry and Dabbelt, Daniel and Hauser, John and Izraelevitz, Adam and Karandikar, Sagar and Keller, Ben and Kim, Donggyu and Koenig, John and Lee, Yunsup and Love, Eric and Maas, Martin and Magyar, Albert and Mao, Howard and Moreto, Miquel and Ou, Albert and Patterson, David A. and Richards, Brian and Schmidt, Colin and Twigg, Stephen and Vo, Huy and Waterman, Andrew},
    Title = {The Rocket Chip Generator},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2016},
    Month = {Apr},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html},
    Number = {UCB/EECS-2016-17},
    Abstract = {Rocket Chip is an open-source Sysem-on-Chip design generator that emits synthesizable RTL. It leverages the Chisel hardware construction language to compose a library of sophisticated generators for cores, caches, and interconnects into an integrated SoC. Rocket Chip generates general-purpose processor cores that use the open RISC-V ISA, and provides both an in-order core generator (Rocket) and an out-of-order core generator (BOOM). For SoC designers interested in utilizing heterogeneous specialization for added efficiency gains, Rocket Chip supports the integration of custom accelerators in the form of instruction set extensions, coprocessors, or fully independent novel cores. Rocket Chip has been taped out (manufactured) eleven times, and yielded functional silicon prototypes capable of booting Linux.}
}



@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{hauser2001approximating,
  title={Approximating functions for embedded and ASIC applications},
  author={Hauser, James W and Purdy, Carla N},
  booktitle={Proceedings of the 44th IEEE 2001 Midwest Symposium on Circuits and Systems. MWSCAS 2001 (Cat. No. 01CH37257)},
  volume={1},
  pages={478--481},
  year={2001},
  organization={IEEE}
}

@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}


@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}


@article{tang2019distilling,
  title={Distilling task-specific knowledge from bert into simple neural networks},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019}
}


@article{turc2019well,
  title={Well-read students learn better: On the importance of pre-training compact models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962},
  year={2019}
}


@article{sun2020mobilebert,
  title={Mobilebert: a compact task-agnostic bert for resource-limited devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  journal={arXiv preprint arXiv:2004.02984},
  year={2020}
}


@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.10957},
  year={2020}
}


@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}


@article{raganato2020fixed,
  title={Fixed encoder self-attention patterns in transformer-based machine translation},
  author={Raganato, Alessandro and Scherrer, Yves and Tiedemann, J{\"o}rg},
  journal={arXiv preprint arXiv:2002.10260},
  year={2020}
}


@article{mao2020ladabert,
  title={Ladabert: Lightweight adaptation of bert through hybrid model compression},
  author={Mao, Yihuan and Wang, Yujing and Wu, Chufan and Zhang, Chen and Wang, Yang and Yang, Yaming and Zhang, Quanlu and Tong, Yunhai and Bai, Jing},
  journal={arXiv preprint arXiv:2004.04124},
  year={2020}
}

@article{gordon2020compressing,
  title={Compressing bert: Studying the effects of weight pruning on transfer learning},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}


@article{ganesh2020compressing,
  title={Compressing large-scale transformer-based models: A case study on bert},
  author={Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
  journal={arXiv preprint arXiv:2002.11985},
  year={2020}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}





@article{bai2020binarybert,
  title={BinaryBERT: Pushing the Limit of BERT Quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{jin2021kdlsq,
  title={KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization},
  author={Jin, Jing and Liang, Cai and Wu, Tiancheng and Zou, Liqin and Gan, Zhiliang},
  journal={arXiv preprint arXiv:2101.05938},
  year={2021}
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}

@inproceedings{zadeh2020gobo,
  title={Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference},
  author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={811--824},
  year={2020},
  organization={IEEE}
}

@article{fan2020training,
  title={Training with quantization noise for extreme fixed-point compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, Remi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}

@article{waring1779vii,
  title={Vii. problems concerning interpolations},
  author={Waring, Edward},
  journal={Philosophical transactions of the royal society of London},
  year={1779},
  publisher={The Royal Society London}
}

@book{stewart1996afternotes,
  title={Afternotes on numerical analysis},
  author={Stewart, Gilbert W},
  year={1996},
  publisher={SIAM}
}

@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005}
}

@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@article{iyer2017first,
  title={First Quora Dataset Release: Question Pairs.(2017)},
  author={Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornl},
  journal={URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs},
  year={2017}
}

@article{warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  publisher={MIT Press}
}

@article{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{dagan2005pascal,
  title={The PASCAL recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012},
  organization={Citeseer}
}

@article{rosset2019turing,
  title={Turing-{NLG}: A 17-billion-parameter language model by microsoft},
  author={Rosset, C},
  journal={Microsoft Blog},
  year={2019}
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using gpu model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{lai2018cmsis,
  title={{CMSIS-NN}: Efficient neural network kernels for arm cortex-m cpus},
  author={Lai, Liangzhen and Suda, Naveen and Chandra, Vikas},
  journal={arXiv preprint arXiv:1801.06601},
  year={2018}
}

@book{crandall2006prime,
  title={Prime numbers: a computational perspective},
  author={Crandall, Richard and Pomerance, Carl B},
  volume={182},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{conti2020technical,
  title={Technical Report: NEMO DNN Quantization for Deployment Model},
  author={Conti, Francesco},
  journal={arXiv preprint arXiv:2004.05930},
  year={2020}
}

@inproceedings{kwon2018co,
  title={Co-design of deep neural nets and neural net accelerators for embedded vision applications},
  author={Kwon, Kiseok and Amid, Alon and Gholami, Amir and Wu, Bichen and Asanovic, Krste and Keutzer, Kurt},
  booktitle={2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@article{li2016ternary,
  title={Ternary weight networks},
  author={Li, Fengfu and Zhang, Bo and Liu, Bin},
  journal={arXiv preprint arXiv:1605.04711},
  year={2016}
}

@article{courbariaux2016binarized,
  title={Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1},
  author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1602.02830},
  year={2016}
}

@inproceedings{howard2019searching,
  title={Searching for {MobilenetV3}},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1314--1324},
  year={2019}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units ({GELU}s)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{dodge2020fine,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}

@article{liu2019roberta,
  title={{RoBERTa}: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

@article{wu2020integer,
  title={Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

@inproceedings{dong2019hawq,
  title={{HAWQ}: Hessian aware quantization of neural networks with mixed-precision},
  author={Dong, Zhen and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={293--302},
  year={2019}
}

@inproceedings{shen2020q,
  title={{Q-BERT}: Hessian Based Ultra Low Precision Quantization of BERT.},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={AAAI},
  pages={8815--8821},
  year={2020}
}

@article{zafrir2019q8bert,
  title={{Q8BERT}: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:1910.06188},
  year={2019}
}

@article{bhandare2019efficient,
  title={Efficient 8-bit quantization of transformer neural machine language translation model},
  author={Bhandare, Aishwarya and Sripathi, Vamsi and Karkada, Deepthi and Menon, Vivek and Choi, Sun and Datta, Kushal and Saletore, Vikram},
  journal={arXiv preprint arXiv:1906.00532},
  year={2019}
}

@article{wu2018mixed,
  title={Mixed precision quantization of convnets via differentiable neural architecture search},
  author={Wu, Bichen and Wang, Yanghan and Zhang, Peizhao and Tian, Yuandong and Vajda, Peter and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1812.00090},
  year={2018}
}

@inproceedings{zhang2018lq,
  title={{LQ-Nets}: Learned quantization for highly accurate and compact deep neural networks},
  author={Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={365--382},
  year={2018}
}

@inproceedings{wu2016quantized,
  title={Quantized convolutional neural networks for mobile devices},
  author={Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4820--4828},
  year={2016}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}


@article{wang2018glue,
  title={{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@BOOK{Mah-mat-rev_BOOK,
  author =       {Michael W Mahoney},
  title =        {Randomized algorithms for matrices and data},
  publisher =    {NOW Publishers},
  year =         {2011},
  address =      {Boston},
  series =       {Foundations and Trends in Machine Learning},
}

@INCOLLECTION{RandNLA_PCMIchapter_chapter,
  author =       {Petros Drineas and Michael W Mahoney},
  title =        {Lectures on Randomized Numerical Linear Algebra},
  booktitle =    {The Mathematics of Data},
  year =         {2018},
  pages =        {1--48},
  series =       {IAS/Park City Mathematics Series},
  publisher =    {AMS/IAS/SIAM},
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@inproceedings{wang2019learning,
  title={Learning Deep Transformer Models for Machine Translation},
  author={Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1810--1822},
  year={2019}
}

@article{mahoney2011large,
  title={Large text compression benchmark},
  author={Mahoney, Matt}
}

@article{younesshapes,
  title={Shapes and Diffeomorphisms},
  author={Younes, Laurent},
  journal={Springer Science \& Business Media}
}

@article{turing1990chemical,
  title={The chemical basis of morphogenesis},
  author={Turing, Alan Mathison},
  journal={Bulletin of mathematical biology},
  volume={52},
  number={1-2},
  pages={153--197},
  year={1952},
  publisher={Springer}
}

@BOOK{lions72,
  title = {Some Aspects of the Optimal Control of Distributed Parameter Systems},
  publisher = {SIAM},
  year = {1972},
  author = {Jacques-Louis Lions}
}

@book{rosenblatt1957perceptron,
  title={The perceptron, a perceiving and recognizing automaton Project Para},
  author={Rosenblatt, Frank},
  year={1957},
  publisher={Cornell Aeronautical Laboratory}
}

@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}

@article{minsky1969perceptron,
  title={Perceptron: an introduction to computational geometry},
  author={Minsky, Marvin and Papert, Seymour},
  journal={The MIT Press, Cambridge, expanded edition},
  volume={19},
  number={88},
  pages={2},
  year={1969}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{rissanen1978modeling,
  title={Modeling by shortest data description},
  author={Rissanen, Jorma},
  journal={Automatica},
  volume={14},
  number={5},
  pages={465--471},
  year={1978},
  publisher={Elsevier}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@InProceedings{simonyan2014very,
  author       = "Simonyan, K. and Zisserman, A.",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}



@inproceedings{szegedy2016rethinking,
  title={Rethinking the {Inception} architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@inproceedings{ma2018shufflenet,
  title={Shufflenet {V2}: Practical guidelines for efficient cnn architecture design},
  author={Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={116--131},
  year={2018}
}

@inproceedings{sandler2018mobilenetv2,
  title={{MobilenetV2}: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4510--4520},
  year={2018}
}


@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{zhou2017incremental,
  title={Incremental network quantization: Towards lossless cnns with low-precision weights},
  author={Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{choi2018pact,
  title={{PACT}: Parameterized clipping activation for quantized neural networks},
  author={Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash},
  journal={arXiv preprint arXiv:1805.06085},
  year={2018}
}





@article{polino2018model,
  title={Model compression via distillation and quantization},
  author={Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  journal={arXiv preprint arXiv:1802.05668},
  year={2018}
}


@inproceedings{zhang2018shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6848--6856},
  year={2018}
}

@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}
@article{iandola2016squeezenet,
  title={{SqueezeNet}: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}
@inproceedings{rastegari2016xnor,
  title={{XNOR-Net}: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European Conference on Computer Vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@inproceedings{courbariaux2015binaryconnect,
  title={Binary{C}onnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  booktitle={Advances in neural information processing systems},
  pages={3123--3131},
  year={2015}
}

@InProceedings{Zhang_2018_ECCV,
author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
title = {{LQ-Nets}: Learned Quantization for Highly Accurate and Compact Deep Neural Networks},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

@article{romero2014fitnets,
  title={{FitNet}s: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@inproceedings{grosse2016kronecker,
  title={A Kronecker-factored approximate Fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016}
}

@inproceedings{redmon2017yolo9000,
  title={YOLO9000: better, faster, stronger},
  author={Redmon, Joseph and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7263--7271},
  year={2017}
}


@inproceedings{sharma2018bit,
  title={Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural networks},
  author={Sharma, Hardik and Park, Jongse and Suda, Naveen and Lai, Liangzhen and Chau, Benson and Chandra, Vikas and Esmaeilzadeh, Hadi},
  booktitle={Proceedings of the 45th Annual International Symposium on Computer Architecture},
  pages={764--775},
  year={2018},
  organization={IEEE Press}
}

@inproceedings{bismo,
author = {Umuroglu, Yaman and Rasnayake, Lahiru and Sjalander, Magnus},
title = {BISMO: A Scalable Bit-Serial Matrix Multiplication Overlay for Reconfigurable Computing},
booktitle = {Field Programmable Logic and Applications (FPL), 2018 28th International Conference on},
series = {FPL '18},
year = {2018}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={Workshop paper in NIPS},
  year={2014}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@article{shallue2018measuring,
  title={Measuring the effects of data parallelism on neural network training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joe and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={arXiv preprint arXiv:1811.03600},
  year={2018}
}

@article{osawa2018second,
  title={Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs},
  author={Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  journal={arXiv preprint arXiv:1811.12019},
  year={2018}
}

@article{ba2016distributed,
  title={Distributed second-order optimization using Kronecker-factored approximations},
  author={Ba, Jimmy and Grosse, Roger and Martens, James},
  year={2016}
}

@article{KFAC-G15,
  author    = {James Martens and
               Roger B. Grosse},
  title     = {Optimizing Neural Networks with Kronecker-factored Approximate Curvature},
  journal   = {CoRR},
  volume    = {abs/1503.05671},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.05671},
  archivePrefix = {arXiv},
  eprint    = {1503.05671},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MartensG15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
wd-kfac,
title={Three Mechanisms of Weight Decay Regularization},
author={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2019},
}

@article{FACEBOOK-IMAGENET-1H,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02677},
  archivePrefix = {arXiv},
  eprint    = {1706.02677},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GoyalDGNWKTJH17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{GOOG-2M-IMAGENET,
  author    = {Yang You and
               Zhao Zhang and
               Cho{-}Jui Hsieh and
               James Demmel},
  title     = {100-epoch ImageNet Training with AlexNet in 24 Minutes},
  journal   = {CoRR},
  volume    = {abs/1709.05011},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.05011},
  archivePrefix = {arXiv},
  eprint    = {1709.05011},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-05011},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ma2017power,
  title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  journal={arXiv preprint arXiv:1712.06559},
  year={2017}
}

@article{OpenAI-EMP-LBS,
  author    = {Sam McCandlish and
               Jared Kaplan and
               Dario Amodei and
               OpenAI Dota Team},
  title     = {An Empirical Model of Large-Batch Training},
  journal   = {CoRR},
  volume    = {abs/1812.06162},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06162},
  archivePrefix = {arXiv},
  eprint    = {1812.06162},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-06162},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Noah-EMP-CRIT-BS,
  author    = {Noah Golmant and
               Nikita Vemuri and
               Zhewei Yao and
               Vladimir Feinberg and
               Amir Gholami and
               Kai Rothauge and
               Michael W. Mahoney and
               Joseph Gonzalez},
  title     = {On the Computational Inefficiency of Large Batch Sizes for Stochastic
               Gradient Descent},
  journal   = {CoRR},
  volume    = {abs/1811.12941},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12941},
  archivePrefix = {arXiv},
  eprint    = {1811.12941},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-12941},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{GOOG-LB-KFAC-HYPO,
  author    = {Christopher J. Shallue and
               Jaehoon Lee and
               Joseph M. Antognini and
               Jascha Sohl{-}Dickstein and
               Roy Frostig and
               George E. Dahl},
  title     = {Measuring the Effects of Data Parallelism on Neural Network Training},
  journal   = {CoRR},
  volume    = {abs/1811.03600},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03600},
  archivePrefix = {arXiv},
  eprint    = {1811.03600},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-03600},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{gholami2017integrated,
  title={Integrated Model, Batch and Domain Parallelism in Training Neural Networks},
  author={Gholami, Amir and Azad, Ariful and Jin, Peter and Keutzer, Kurt and Buluc, Aydin},
  journal={ACM Symposium on Parallelism in Algorithms and Architectures(SPAA'18)},
note={\href{https://arxiv.org/pdf/1712.04432.pdf}{[PDF]}},
  year={2018}
}

@inproceedings{zhang2015deep,
  title={Deep learning with elastic averaging SGD},
  author={Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  pages={685--693},
  year={2015}
}

@article{jia2018highly,
  title={Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes},
  author={Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and others},
  journal={arXiv preprint arXiv:1807.11205},
  year={2018}
}


@article{bertsimas2011theory,
  title={Theory and applications of robust optimization},
  author={Bertsimas, Dimitris and Brown, David B and Caramanis, Constantine},
  journal={SIAM review},
  volume={53},
  number={3},
  pages={464--501},
  year={2011},
  publisher={SIAM}
}

@article{maleki2017parallel,
  title={Parallel Stochastic Gradient Descent with Sound Combiners},
  author={Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd},
  journal={arXiv preprint arXiv:1705.08030},
  year={2017}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{smith2018bayesian,
  title={A bayesian perspective on generalization and stochastic gradient descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.06451},
  year={2018}
}

@article{smith2017don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch SGD: training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{yao2018hessian,
  title={Hessian-based Analysis of Large Batch Training and Robustness to Adversaries},
  author={Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.04933},
  year={2017}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get M for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1704.00109},
  year={2017}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  institution={Citeseer}
}

@article{lin2013network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1312.4400},
  year={2013}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  booktitle={NIPS workshop on deep learning and unsupervised feature learning},
  volume={2011},
  pages={5},
  year={2011}
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Springer}
}

@inproceedings{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2933--2941},
  year={2014}
}

@article{devarakonda2017adabatch,
  title={AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks},
  author={Devarakonda, Aditya and Naumov, Maxim and Garland, Michael},
  journal={arXiv preprint arXiv:1712.02029},
  year={2017}
}


@article{zhu2018anisotropic,
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@article{friedlander2012hybrid,
  title={Hybrid deterministic-stochastic methods for data fitting},
  author={Friedlander, Michael P and Schmidt, Mark},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={3},
  pages={A1380--A1405},
  year={2012},
  publisher={SIAM}
}

@article{balles2016coupling,
  title={Coupling adaptive batch sizes with learning rates},
  author={Balles, Lukas and Romero, Javier and Hennig, Philipp},
  journal={arXiv preprint arXiv:1612.05086},
  year={2016}
}

@inproceedings{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1731--1741},
  year={2017}
}

@article{you2017scaling,
  title={Scaling sgd batch size to 32k for imagenet training},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{dong2017learning,
  title={Learning accurate low-bit deep neural networks with stochastic quantization},
  author={Dong, Yinpeng and Ni, Renkun and Li, Jianguo and Chen, Yurong and Zhu, Jun and Su, Hang},
  journal={British Machine Vision Conference},
  year={2017}
}

@article{puri2018large,
  title={Large Scale Language Modeling: Converging on 40GB of Text in Four Hours},
  author={Puri, Raul and Kirby, Robert and Yakovenko, Nikolai and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1808.01371},
  year={2018}
}

@article{goodfellow6572explaining,
  title={Explaining and harnessing adversarial examples (2014)},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}


@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{gholami2018squeezenext,
  title={{SqueezeNext}: Hardware-Aware Neural Network Design},
  author={Gholami, Amir and Kwon, Kiseok and Wu, Bichen and Tai, Zizheng and Yue, Xiangyu and Jin, Peter and Zhao, Sicheng and Keutzer, Kurt},
  journal={Workshop paper in CVPR},
  year={2018}
}

@article{thakur2005optimization,
  title={Optimization of collective communication operations in MPICH},
  author={Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
  journal={The International Journal of High Performance Computing Applications},
  volume={19},
  number={1},
  pages={49--66},
  year={2005},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{zheng2016asynchronous,
  title={Asynchronous stochastic gradient descent with delay compensation},
  author={Zheng, Shuxin and Meng, Qi and Wang, Taifeng and Chen, Wei and Yu, Nenghai and Ma, Zhi-Ming and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1609.08326},
  year={2016}
}

@inproceedings{agarwal2011distributed,
  title={Distributed delayed stochastic optimization},
  author={Agarwal, Alekh and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={873--881},
  year={2011}
}

@article{el1997robust,
  title={Robust solutions to least-squares problems with uncertain data},
  author={El Ghaoui, Laurent and Lebret, Herv{\'e}},
  journal={SIAM Journal on matrix analysis and applications},
  volume={18},
  number={4},
  pages={1035--1064},
  year={1997},
  publisher={SIAM}
}

@inproceedings{xu2009robust,
  title={Robust regression and lasso},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1801--1808},
  year={2009}
}

@article{xu2017second,
  title={Second-order optimization for non-convex machine learning: An empirical study},
  author={Xu, Peng and Roosta-Khorasan, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1708.07827},
  year={2017}
}

@article{chen2018comparison,
  title={A comparison of second-order methods for deep convolutional neural networks},
  author={Chen, Patrick H and Hsieh, Cho-jui},
  journal={openreview under ICLR 2018},
  year={2018}
}

@inproceedings{martens2010deep,
  title={Deep learning via {H}essian-free optimization.},
  author={Martens, James},
  booktitle={ICML},
  volume={27},
  pages={735--742},
  year={2010}
}

@inproceedings{moosavi2016deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2574--2582},
  year={2016}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={39--57},
  year={2017},
  organization={IEEE}
}

@article{kurakin2016adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1607.02533},
  year={2016}
}

@article{xu2017newton,
  title={Newton-type methods for non-convex optimization under inexact {H}essian information},
  author={Xu, Peng and Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1708.07164},
  year={2017}
}

@inproceedings{yang2019synetgy,
  title={Synetgy: Algorithm-hardware co-design for {C}onv{N}et accelerators on embedded {FPGA}s},
  author={Yang, Yifan and Huang, Qijing and Wu, Bichen and Zhang, Tianjun and Ma, Liang and Gambardella, Giulio and Blott, Michaela and Lavagno, Luciano and Vissers, Kees and Wawrzynek, John and  Keutzer, Kurt},
  booktitle={Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={23--32},
  year={2019},
  organization={ACM}
}

@article{ward2018adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={arXiv preprint arXiv:1806.01811},
  year={2018}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@article{shaham2015understanding,
  title={Understanding adversarial training: Increasing local stability of neural nets through robust optimization},
  author={Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
  journal={arXiv preprint arXiv:1511.05432},
  year={2015}
}

@inproceedings{shrivastava2017learning,
  title={Learning from Simulated and Unsupervised Images through Adversarial Training.},
  author={Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Joshua and Wang, Wenda and Webb, Russell},
  booktitle={CVPR},
  volume={2},
  pages={5},
  year={2017}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{yao2018large,
  title={Large batch size training of neural networks with adversarial training and second-order information},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1810.01021},
  year={2018}
}

@misc{ginsburg2017tensor,
  title={Tensor processing using low precision format},
  author={Ginsburg, Boris and Nikolaev, Sergei and Kiswani, Ahmad and Wu, Hao and Gholaminejad, Amir and Kierat, Slawomir and Houston, Michael and Fit-Florea, Alex},
  year={2017},
  month=dec # "~28",
  publisher={Google Patents},
  note={US Patent App. 15/624,577}
}


@article{zhou2016dorefa,
  title={{DoReFa-Net}: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}
@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017},
  publisher={JMLR. org}
}

@article{miyashita2016convolutional,
  title={Convolutional neural networks using logarithmic data representation},
  author={Miyashita, Daisuke and Lee, Edward H and Murmann, Boris},
  journal={arXiv preprint arXiv:1603.01025},
  year={2016}
}

@inproceedings{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  booktitle={Advances in neural information processing systems},
  pages={1269--1277},
  year={2014}
}




@inproceedings{hubara2016binarized,
  title={Binarized neural networks},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={4107--4115},
  year={2016}
}
@book{asanovic1991experimental,
  title={Experimental determination of precision requirements for back-propagation training of artificial neural networks},
  author={Asanovic, Krste and Morgan, Nelson},
  year={1991},
  publisher={International Computer Science Institute}
}


@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}


@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{dai2021fbnetv3,
  title={FBNetV3: Joint architecture-recipe search using predictor pretraining},
  author={Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Wu, Bichen and He, Zijian and Wei, Zhen and Chen, Kan and Tian, Yuandong and Yu, Matthew and Vajda, Peter and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16276--16285},
  year={2021}
}

@article{lin2020mcunet,
  title={Mcunet: Tiny deep learning on iot devices},
  author={Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Gan, Chuang and Han, Song and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11711--11722},
  year={2020}
}

@inproceedings{yang2018netadapt,
  title={Netadapt: Platform-aware neural network adaptation for mobile applications},
  author={Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={285--300},
  year={2018}
}

@article{wang2018haq,
  title={{HAQ}: Hardware-Aware Automated Quantization},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  journal={In Proceedings of  the IEEE  conference  on  computer  vision  and  pattern  recognition},
  year={2019}
}

@article{mao2017exploring,
  title={Exploring the regularity of sparse structure in convolutional neural networks},
  author={Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J},
  journal={Workshop paper in CVPR},
  year={2017}
}


@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}


@inproceedings{park2018value,
  title={Value-aware quantization for training and inference of neural networks},
  author={Park, Eunhyeok and Yoo, Sungjoo and Vajda, Peter},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={580--595},
  year={2018}
}

@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6571--6583},
  year={2018}
}

@article{gholami2019anode,
  title={ANODE: Unconditionally Accurate Memory-Efficient Gradients for Neural ODEs},
  author={Gholami, Amir and Keutzer, Kurt and Biros, George},
  journal={arXiv preprint arXiv:1902.10298},
  year={2019}
}


@techreport{rosenblatt1961principles,
  title={Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, Frank},
  year={1961},
  institution={Cornell Aeronautical Lab Inc Buffalo NY}
}
@book{yan2009linear,
  title={Linear regression analysis: theory and computing},
  author={Yan, Xin and Su, Xiaogang},
  year={2009},
  publisher={World Scientific}
}



@book{gauss1809theoria,
  title={Theoria motus corporum coelestium in sectionibus conicis solem ambientium},
  author={Gauss, Carl Friedrich},
  volume={7},
  year={1809},
  publisher={Perthes et Besser}
}

@book{legendre1805nouvelles,
  title={Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des com{\`e}tes},
  author={Legendre, Adrien Marie},
  year={1805},
  publisher={F. Didot}
}


@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@book{rosenblatt1957perceptron,
  title={The perceptron, a perceiving and recognizing automaton Project Para},
  author={Rosenblatt, Frank},
  year={1957},
  publisher={Cornell Aeronautical Laboratory}
}



@article{weinan2017proposal,
  title={A proposal on machine learning via dynamical systems},
  author={Weinan, E},
  journal={Communications in Mathematics and Statistics},
  volume={5},
  number={1},
  pages={1--11},
  year={2017},
  publisher={Springer}
}
@article{haber2017stable,
  title={Stable architectures for deep neural networks},
  author={Haber, Eldad and Ruthotto, Lars},
  journal={Inverse Problems},
  volume={34},
  number={1},
  pages={014004},
  year={2017},
  publisher={IOP Publishing}
}
@article{ruthotto2018deep,
  title={Deep Neural Networks motivated by Partial Differential Equations},
  author={Ruthotto, Lars and Haber, Eldad},
  journal={arXiv preprint arXiv:1804.04272},
  year={2018}
}
@article{lu2017beyond,
  title={Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations},
  author={Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  journal={arXiv preprint arXiv:1710.10121},
  year={2017}
}
@article{ciccone2018nais,
  title={NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations},
  author={Ciccone, Marco and Gallieri, Marco and Masci, Jonathan and Osendorfer, Christian and Gomez, Faustino},
  journal={arXiv preprint arXiv:1804.07209},
  year={2018}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018}
}

@inproceedings{tsuchida2018invariance,
  title={Invariance of Weight Distributions in Rectified MLPs},
  author={Tsuchida, Russell and Roosta-Khorasani, Farbod and Gallagher, Marcus},
  booktitle={International Conference on Machine Learning},
  pages={5002--5011},
  year={2018}
}




@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

@article{stanley2009hypercube,
  title={A hypercube-based encoding for evolving large-scale neural networks},
  author={Stanley, Kenneth O and D'Ambrosio, David B and Gauci, Jason},
  journal={Artificial life},
  volume={15},
  number={2},
  pages={185--212},
  year={2009},
  publisher={MIT Press}
}

@inproceedings{stanley2006exploiting,
  title={Exploiting regularity without development},
  author={Stanley, Kenneth O},
  booktitle={Proceedings of the AAAI Fall Symposium on Developmental Systems},
  pages={37},
  year={2006},
  organization={AAAI Press Menlo Park, CA}
}
@article{stanley2007compositional,
  title={Compositional pattern producing networks: A novel abstraction of development},
  author={Stanley, Kenneth O},
  journal={Genetic programming and evolvable machines},
  volume={8},
  number={2},
  pages={131--162},
  year={2007},
  publisher={Springer}
}




@inproceedings{angeline1995morphogenic,
  title={Morphogenic Evolutionary Computations: Introduction, Issues and Example.},
  author={Angeline, Peter J},
  booktitle={Evolutionary Programming},
  pages={387--401},
  year={1995}
}

% -------


@article{lindenmayer1968mathematical,
  title={Mathematical models for cellular interactions in development I. Filaments with one-sided inputs},
  author={Lindenmayer, Aristid},
  journal={Journal of theoretical biology},
  volume={18},
  number={3},
  pages={280--299},
  year={1968},
  publisher={Elsevier}
}


@inproceedings{belew1993evolving,
  title={Evolving Aesthetic Sorting Networks Using Developmental Grammars.},
  author={Belew, Richard K and Kammeyer, Thomas E},
  booktitle={ICGA},
  pages={629},
  year={1993},
  organization={Citeseer}
}

@inproceedings{bentley1999three,
  title={Three ways to grow designs: A comparison of embryogenies for an evolutionary design problem},
  author={Bentley, Peter and Kumar, Sanjeev},
  booktitle={Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1},
  pages={35--43},
  year={1999},
  organization={Morgan Kaufmann Publishers Inc.}
}

@inproceedings{dellaert1996developmental,
  title={A developmental model for the evolution of complete autonomous agents},
  author={Dellaert, Frank and Beer, Randall D},
  booktitle={Proceedings of the fourth international conference on simulation of adaptive behavior},
  pages={393--401},
  year={1996},
  organization={MIT Press Cambridge, MA}
}

@inproceedings{eggenberger1997evolving,
  title={Evolving morphologies of simulated 3D organisms based on differential gene expression},
  author={Eggenberger, Peter},
  booktitle={Proceedings of the fourth european conference on Artificial Life},
  pages={205--213},
  year={1997}
}


@article{hornby2002creating,
  title={Creating high-level components with a generative representation for body-brain evolution},
  author={Hornby, Gregory S and Pollack, Jordan B},
  journal={Artificial life},
  volume={8},
  number={3},
  pages={223--246},
  year={2002},
  publisher={MIT Press}
}

@inproceedings{koutnik2010evolving,
  title={Evolving neural networks in compressed weight space},
  author={Koutnik, Jan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 12th annual conference on Genetic and evolutionary computation},
  pages={619--626},
  year={2010},
  organization={ACM}
}



@inproceedings{fernando2016convolution,
  title={Convolution by evolution: Differentiable pattern producing networks},
  author={Fernando, Chrisantha and Banarse, Dylan and Reynolds, Malcolm and Besse, Frederic and Pfau, David and Jaderberg, Max and Lanctot, Marc and Wierstra, Daan},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference 2016},
  pages={109--116},
  year={2016},
  organization={ACM}
}


@article{moczulski2015acdc,
  title={Acdc: A structured efficient linear layer},
  author={Moczulski, Marcin and Denil, Misha and Appleyard, Jeremy and de Freitas, Nando},
  journal={arXiv preprint arXiv:1511.05946},
  year={2015}
}

@inproceedings{shao2020minimizing,
  title={Minimizing the bag-of-ngrams difference for non-autoregressive neural machine translation},
  author={Shao, Chenze and Zhang, Jinchao and Feng, Yang and Meng, Fandong and Zhou, Jie},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={01},
  pages={198--205},
  year={2020}
}

@article{wei2019imitation,
  title={Imitation learning for non-autoregressive neural machine translation},
  author={Wei, Bingzhen and Wang, Mingxuan and Zhou, Hao and Lin, Junyang and Xie, Jun and Sun, Xu},
  journal={arXiv preprint arXiv:1906.02041},
  year={2019}
}

@article{guo2020incorporating,
  title={Incorporating bert into parallel sequence decoding with adapters},
  author={Guo, Junliang and Zhang, Zhirui and Xu, Linli and Wei, Hao-Ran and Chen, Boxing and Chen, Enhong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10843--10854},
  year={2020}
}

@article{sun2019fast,
  title={Fast structured decoding for sequence models},
  author={Sun, Zhiqing and Li, Zhuohan and Wang, Haoqing and He, Di and Lin, Zi and Deng, Zhihong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{guo2020jointly,
  title={Jointly masked sequence-to-sequence model for non-autoregressive neural machine translation},
  author={Guo, Junliang and Xu, Linli and Chen, Enhong},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={376--385},
  year={2020}
}

@inproceedings{welleck2019non,
  title={Non-monotonic sequential text generation},
  author={Welleck, Sean and Brantley, Kiant{\'e} and Iii, Hal Daum{\'e} and Cho, Kyunghyun},
  booktitle={International Conference on Machine Learning},
  pages={6716--6726},
  year={2019},
  organization={PMLR}
}

@article{li2019hint,
  title={Hint-based training for non-autoregressive machine translation},
  author={Li, Zhuohan and Lin, Zi and He, Di and Tian, Fei and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1909.06708},
  year={2019}
}

@article{hokamp2020dyne,
  title={Dyne: Dynamic ensemble decoding for multi-document summarization},
  author={Hokamp, Chris and Ghalandari, Demian Gholipour and Pham, Nghia The and Glover, John},
  journal={arXiv preprint arXiv:2006.08748},
  year={2020}
}

@inproceedings{pai2020qiaoning,
  title={QiaoNing at SemEval-2020 Task 4: Commonsense Validation and Explanation system based on ensemble of language model},
  author={Pai, Liu},
  booktitle={Proceedings of the Fourteenth Workshop on Semantic Evaluation},
  pages={415--421},
  year={2020}
}

@article{matsubara2022ensemble,
  title={Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems},
  author={Matsubara, Yoshitomo and Soldaini, Luca and Lind, Eric and Moschitti, Alessandro},
  journal={arXiv preprint arXiv:2201.05767},
  year={2022}
}

@article{xu2020improving,
  title={Improving bert fine-tuning via self-ensemble and self-distillation},
  author={Xu, Yige and Qiu, Xipeng and Zhou, Ligao and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2002.10345},
  year={2020}
}

@article{huy2022autoencoding,
  title={Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation},
  author={Huy, Ngo Quang and Phuong, Tu Minh and Bach, Ngo Xuan},
  journal={arXiv preprint arXiv:2204.03324},
  year={2022}
}

@article{zhou2019understanding,
  title={Understanding knowledge distillation in non-autoregressive machine translation},
  author={Zhou, Chunting and Neubig, Graham and Gu, Jiatao},
  journal={arXiv preprint arXiv:1911.02727},
  year={2019}
}
@article{gu2019levenshtein,
  title={Levenshtein transformer},
  author={Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{lee2018deterministic,
  title={Deterministic non-autoregressive neural sequence modeling by iterative refinement},
  author={Lee, Jason and Mansimov, Elman and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1802.06901},
  year={2018}
}


@inproceedings{stern2019insertion,
  title={Insertion transformer: Flexible sequence generation via insertion operations},
  author={Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={5976--5985},
  year={2019},
  organization={PMLR}
}

@article{ghazvininejad2019mask,
  title={Mask-predict: Parallel decoding of conditional masked language models},
  author={Ghazvininejad, Marjan and Levy, Omer and Liu, Yinhan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1904.09324},
  year={2019}
}

@article{schmidhuber1992learning,
  title={Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={4},
  number={1},
  pages={131--139},
  year={1992},
  publisher={MIT Press}
}

@inproceedings{schmidhuber1993self,
  title={A ‘self-referential’weight matrix},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Artificial Neural Networks},
  pages={446--450},
  year={1993},
  organization={Springer}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}


@article{gholami2016inverse,
  title={An inverse problem formulation for parameter estimation of a reaction--diffusion model of low grade gliomas},
  author={Gholami, Amir and Mang, Andreas and Biros, George},
  journal={Journal of mathematical biology},
  volume={72},
  number={1-2},
  pages={409--433},
  year={2016},
  publisher={Springer}
}
@ONLINE{ibertcode,
  title = {https://github.com/kssteven418/I-BERT},
  year = 2021
}

@article{avron2011randomized,
  title={Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
  author={Avron, Haim and Toledo, Sivan},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={2},
  pages={8},
  year={2011},
  publisher={ACM}
}

@article{bai1996some,
  title={Some large-scale matrix computation problems},
  author={Bai, Zhaojun and Fahey, Gark and Golub, Gene},
  journal={Journal of Computational and Applied Mathematics},
  volume={74},
  number={1-2},
  pages={71--89},
  year={1996},
  publisher={Elsevier}
}

@article{ubaru2017fast,
  title={Fast Estimation of tr(f(A)) via Stochastic {L}anczos Quadrature},
  author={Ubaru, Shashanka and Chen, Jie and Saad, Yousef},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={38},
  number={4},
  pages={1075--1099},
  year={2017},
  publisher={SIAM}
}

@article{lin2016approximating,
  title={Approximating spectral densities of large matrices},
  author={Lin, Lin and Saad, Yousef and Yang, Chao},
  journal={SIAM review},
  volume={58},
  number={1},
  pages={34--65},
  year={2016},
  publisher={SIAM}
}

@book{golub2009matrices,
  title={Matrices, moments and quadrature with applications},
  author={Golub, Gene H and Meurant, G{\'e}rard},
  year={2009},
  publisher={Princeton University Press}
}

@article{golub1969calculation,
  title={Calculation of {G}auss quadrature rules},
  author={Golub, Gene H and Welsch, John H},
  journal={Mathematics of computation},
  volume={23},
  number={106},
  pages={221--230},
  year={1969}
}

fmemory
@inproceedings{yang2019xlnet,
  title={{XLNet}: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5753--5763},
  year={2019}
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@article{ghorbani2019investigation,
  title={An investigation into neural net optimization via {H}essian eigenvalue density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  journal={arXiv preprint arXiv:1901.10159},
  year={2019}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@article{de2022fido,
  title={FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference},
  author={de Jong, Michiel and Zemlyanskiy, Yury and Ainslie, Joshua and FitzGerald, Nicholas and Sanghai, Sumit and Sha, Fei and Cohen, William},
  journal={arXiv preprint arXiv:2212.08153},
  year={2022}
}

@article{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{moon2020confidence,
  title={Confidence-aware learning for deep neural networks},
  author={Moon, Jooyoung and Kim, Jihyo and Shin, Younghak and Hwang, Sangheum},
  booktitle={international conference on machine learning},
  pages={7034--7044},
  year={2020},
  organization={PMLR}
}

@article{sagun2016eigenvalues,
  title={Eigenvalues of the {H}essian in deep learning: Singularity and beyond},
  author={Sagun, Levent and Bottou, Leon and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2016}
}

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@ONLINE{pyhessian,
  title = {https://github.com/amirgholami/PyHessian.git},
  month = Sep,
  year = 2019
}

@ONLINE{biglittle_code,
  title = {https://github.com/kssteven418/BigLittleDecoder},
  month = Feb,
  year = 2023
}

@ONLINE{biglittlecore,
  title = {https://www.arm.com/technologies/big-little}
}

@article{kasai2020deep,
  title={Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation},
  author={Kasai, Jungo and Pappas, Nikolaos and Peng, Hao and Cross, James and Smith, Noah A},
  journal={arXiv preprint arXiv:2006.10369},
  year={2020}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@article{lagunas2021block,
  title={Block pruning for faster transformers},
  author={Lagunas, Fran{\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M},
  journal={arXiv preprint arXiv:2109.04838},
  year={2021}
}

@article{kurtic2022optimal,
  title={The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models},
  author={Kurtic, Eldar and Campos, Daniel and Nguyen, Tuan and Frantar, Elias and Kurtz, Mark and Fineran, Benjamin and Goin, Michael and Alistarh, Dan},
  journal={arXiv preprint arXiv:2203.07259},
  year={2022}
}

@article{chen2023accelerating,
  title={Accelerating Large Language Model Decoding with Speculative Sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}

@article{xia2022structured,
  title={Structured pruning learns compact and accurate models},
  author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2204.00408},
  year={2022}
}

@inproceedings{flamand2018gap,
  title={GAP-8: A RISC-V SoC for AI at the Edge of the IoT},
  author={Flamand, Eric and Rossi, Davide and Conti, Francesco and Loi, Igor and Pullini, Antonio and Rotenberg, Florent and Benini, Luca},
  booktitle={2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
  pages={1--4},
  year={2018},
  organization={IEEE}
}

@ONLINE{armcortexm,
  author={{ARM}},
  title = {{Cortex-M}, https://developer.arm.com/ip-products/processors/cortex-m},
  year = 2020
}


@ONLINE{distilgpt2,
  author={{Huggingface}},
  title = {https://huggingface.co/distilgpt2}
}

@ONLINE{tensorrtbert,
  author={ Mukherjee, Purnendu and Weill, Eddie and Taneja, Rohit and Onofrio, Davide and Ko, Young-Jun and Sharma, Siddharth},
  title = {Real-Time Natural Language Understanding with BERT Using TensorRT, hhttps://developer.nvidia.com/blog/nlu-with-tensorrt-bert/},
  year = 2019
}

@ONLINE{tensorrt,
  author={{NVIDIA}},
  title = {Tensor{RT}: https://developer.nvidia.com/tensorrt},
  year={2018}
}

@inproceedings{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2483--2493},
  year={2018}
}



@article{martin2019traditional,
  title={Traditional and heavy-tailed self regularization in neural network models},
  author={Martin, Charles H and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1901.08276},
  year={2019}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}


@article{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}

@inproceedings{klambauer2017self,
  title={Self-normalizing neural networks},
  author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  booktitle={Advances in neural information processing systems},
  pages={971--980},
  year={2017}
}

@article{mahoney2009cur,
  title={{CUR} matrix decompositions for improved data analysis},
  author={Mahoney, Michael W and Drineas, Petros},
  journal={Proceedings of the National Academy of Sciences},
  volume={106},
  number={3},
  pages={697--702},
  year={2009},
  publisher={National Acad Sciences}
}

@misc{kao2022demystifyingNpu,
  doi = {10.48550/ARXIV.2210.03731},
  
  url = {https://arxiv.org/abs/2210.03731},
  
  author = {Kao, Sheng-Chun and Parashar, Angshuman and Tsai, Po-An and Krishna, Tushar},
  
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Demystifying Map Space Exploration for NPUs},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{drineas2006fast,
  title={Fast Monte Carlo algorithms for matrices {II}: Computing a low-rank approximation to a matrix},
  author={Drineas, Petros and Kannan, Ravi and Mahoney, Michael W},
  journal={SIAM Journal on computing},
  volume={36},
  number={1},
  pages={158--183},
  year={2006},
  publisher={SIAM}
}

@inproceedings{becker1988improving,
  title={Improving the convergence of back-propagation learning with second order methods},
  author={Becker, Sue and Le Cun, Yann},
  booktitle={Proceedings of the 1988 connectionist models summer school},
  pages={29--37},
  year={1988}
}

@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}



@article{papyan2018full,
  title={The full spectrum of deep net hessians at scale: Dynamics with sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}


@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@inproceedings{pennington2017geometry,
  title={Geometry of neural network loss surfaces via random matrix theory},
  author={Pennington, Jeffrey and Bahri, Yasaman},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2798--2806},
  year={2017},
  organization={JMLR. org}
}



@inproceedings{yao2019trust,
  title={Trust region based adversarial attack on neural networks},
  author={Yao, Zhewei and Gholami, Amir and Xu, Peng and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={11350--11359},
  year={2019}
}

@inproceedings{xu2016sub,
  title={Sub-sampled {N}ewton methods with non-uniform sampling},
  author={Xu, Peng and Yang, Jiyan and Roosta-Khorasani, Farbod and R{\'e}, Christopher and Mahoney, Michael W},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3000--3008},
  year={2016}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{byrd2011use,
  title={On the use of stochastic {H}essian information in optimization methods for machine learning},
  author={Byrd, Richard H and Chin, Gillian M and Neveitt, Will and Nocedal, Jorge},
  journal={SIAM Journal on Optimization},
  volume={21},
  number={3},
  pages={977--995},
  year={2011},
  publisher={SIAM}
}

@article{erdogdu2015convergence,
  title={Convergence rates of sub-sampled {N}ewton methods},
  author={Erdogdu, Murat A and Montanari, Andrea},
  journal={arXiv preprint arXiv:1508.02810},
  year={2015}
}

@article{roosta2016sub,
  title={Sub-sampled {N}ewton methods {I}: globally convergent algorithms},
  author={Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1601.04737},
  year={2016}
}

@book{conn2000trust,
	title={Trust region methods},
	author={Conn, Andrew R and Gould, Nicholas IM and Toint, Philippe L},
	series={Series on Optimization},
	publisher={SIAM},
	year={2000},
}

@article{nesterov2006cubic,
	title={{Cubic regularization of {N}ewton method and its global performance}},
	author={Nesterov, Yurii and Polyak, Boris T},
	journal={Mathematical Programming},
	volume={108},
	number={1},
	pages={177--205},
	year={2006},
	publisher={Springer}
}

@article{cartis2011adaptiveI,
	title={{Adaptive cubic regularisation methods for unconstrained optimization. Part I: motivation, convergence and numerical results}},
	author={Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
	journal={Mathematical Programming},
	volume={127},
	number={2},
	pages={245--295},
	year={2011},
	publisher={Springer}
}

@article{cartis2011adaptiveII,
	title={{Adaptive cubic regularisation methods for unconstrained optimization. Part II: worst-case function-and derivative-evaluation complexity}},
	author={Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
	journal={Mathematical programming},
	volume={130},
	number={2},
	pages={295--319},
	year={2011},
	publisher={Springer}
}

@article{yao2018inexact,
  title={Inexact non-convex {N}ewton-type methods},
  author={Yao, Zhewei and Xu, Peng and Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1802.06925},
  year={2018}
}

@article{xuNonconvexTheoretical2017,
	title={{Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information}},
	author={Peng Xu and Farbod Roosta-Khorasani and Michael W. Mahoney},
	journal={arXiv preprint arXiv:1708.07164},
	year={2017}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with {K}ronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015}
}

@article{krishnan2017neumann,
  title={Neumann optimizer: A practical optimization algorithm for deep neural networks},
  author={Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  journal={arXiv preprint arXiv:1712.03298},
  year={2017}
}

@article{xuNonconvexEmpirical2017,
	title={{Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study}},
	author={Peng Xu and Farbod Roosta-Khorasani and Michael W. Mahoney},
	journal={arXiv preprint arXiv:1708.07827},
	year={2017}
}

@inproceedings{vatanen2013pushing,
  title={Pushing stochastic gradient towards second-order methods--backpropagation learning with transformations in nonlinearities},
  author={Vatanen, Tommi and Raiko, Tapani and Valpola, Harri and LeCun, Yann},
  booktitle={International Conference on Neural Information Processing},
  pages={442--449},
  year={2013},
  organization={Springer}
}

@inproceedings{lecun1991second,
  title={Second order properties of error surfaces: Learning time and generalization},
  author={LeCun, Yann and Kanter, Ido and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={918--924},
  year={1991}
}

@article{sagun2016singularity,
  title={Singularity of the hessian in deep learning},
  author={Sagun, Levent and Bottou, L{\'e}on and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2016}
}

@article{dong2019hawqv2,
  title={{HAWQ-V2}: {H}essian {A}ware trace-{W}eighted {Q}uantization of Neural Networks},
  author={Dong, Zhen and Yao, Zhewei and Arfeen, Daiyaan and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  journal={NeurIPS’19 workshop on Beyond First-Order Optimization Methods in Machine Learning.},
  year={2019}
}


@article{agarwal2016second,
  title={Second-order stochastic optimization in linear time},
  author={Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  journal={Journal of Machine Learning Research},
  volume={1050},
  pages={15},
  year={2016}
}

@article{dembo1982inexact,
  title={Inexact {N}ewton methods},
  author={Dembo, Ron S and Eisenstat, Stanley C and Steihaug, Trond},
  journal={SIAM Journal on Numerical analysis},
  volume={19},
  number={2},
  pages={400--408},
  year={1982},
  publisher={SIAM}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the {H}essian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@article{pilanci2017newton,
  title={Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence},
  author={Pilanci, Mert and Wainwright, Martin J},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={1},
  pages={205--245},
  year={2017},
  publisher={SIAM}
}

@article{pratt1998gauss,
  title={Gauss--{N}ewton and full {N}ewton methods in frequency--space seismic waveform inversion},
  author={Pratt, R Gerhard and Shin, Changsoo and Hick, GJ},
  journal={Geophysical Journal International},
  volume={133},
  number={2},
  pages={341--362},
  year={1998},
  publisher={Blackwell Publishing Ltd Oxford, UK}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}

@article{bollapragada2018progressive,
  title={A progressive batching {L-BFGS} method for machine learning},
  author={Bollapragada, Raghu and Mudigere, Dheevatsa and Nocedal, Jorge and Shi, Hao-Jun Michael and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1802.05374},
  year={2018}
}

@article{liu1989limited,
  title={On the limited memory {BFGS} method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1-3},
  pages={503--528},
  year={1989},
  publisher={Springer}
}

@article{luo2018neural,
  title={Neural architecture optimization},
  author={Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@inproceedings{yang2022searching,
  title={Searching for BurgerFormer with Micro-Meso-Macro Space Design},
  author={Yang, Longxing and Hu, Yu and Lu, Shun and Sun, Zihao and Mei, Jilin and Han, Yinhe and Li, Xiaowei},
  booktitle={International Conference on Machine Learning},
  pages={25055--25069},
  year={2022},
  organization={PMLR}
}

@article{liao2021searching,
  title={Searching for Efficient Multi-Stage Vision Transformers},
  author={Liao, Yi-Lun and Karaman, Sertac and Sze, Vivienne},
  journal={arXiv preprint arXiv:2109.00642},
  year={2021}
}

@inproceedings{chen2021glit,
  title={Glit: Neural architecture search for global and local image transformer},
  author={Chen, Boyu and Li, Peixia and Li, Chuming and Li, Baopu and Bai, Lei and Lin, Chen and Sun, Ming and Yan, Junjie and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12--21},
  year={2021}
}

@inproceedings{liu2018progressive,
  title={Progressive neural architecture search},
  author={Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={19--34},
  year={2018}
}

@inproceedings{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  booktitle={Advances in neural information processing systems},
  pages={164--171},
  year={1993}
}

@inproceedings{ma2019inefficiency,
  title={Inefficiency of {K-FAC} for Large Batch Size Training},
  author={Ma, Linjian and Montague, Gabe and Ye, Jiayu and Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={Thirty-Fourth AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}



@article{baker2016designing,
  title={Designing neural network architectures using reinforcement learning},
  author={Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  journal={arXiv preprint arXiv:1611.02167},
  year={2016}
}


@article{bordes2009sgd,
  title={SGD-QN: Careful quasi-{N}ewton stochastic gradient descent},
  author={Bordes, Antoine and Bottou, L{\'e}on and Gallinari, Patrick},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Jul},
  pages={1737--1754},
  year={2009}
}



@article{dennis1974characterization,
  title={A characterization of superlinear convergence and its application to quasi-{N}ewton methods},
  author={Dennis, John E and Mor{\'e}, Jorge J},
  journal={Mathematics of computation},
  volume={28},
  number={126},
  pages={549--560},
  year={1974}
}


@article{nocedal1980updating,
  title={Updating quasi-{N}ewton matrices with limited storage},
  author={Nocedal, Jorge},
  journal={Mathematics of computation},
  volume={35},
  number={151},
  pages={773--782},
  year={1980}
}



@inproceedings{schraudolph2007stochastic,
  title={A stochastic quasi-{N}ewton method for online convex optimization},
  author={Schraudolph, Nicol N and Yu, Jin and G{\"u}nter, Simon},
  booktitle={Artificial intelligence and statistics},
  pages={436--443},
  year={2007}
}



@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{yao2019pyhessian,
  title={PyHessian: Neural Networks Through the Lens of the Hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael},
  journal={arXiv preprint arXiv:1912.07145},
  year={2019}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@inproceedings{
loshchilov2017decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@inproceedings{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@inproceedings{
loshchilov2016sgdr,
title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018}
}

@inproceedings{singh2015layer,
  title={Layer-specific adaptive learning rates for deep networks},
  author={Singh, Bharat and De, Soham and Zhang, Yangmuzi and Goldstein, Thomas and Taylor, Gavin},
  booktitle={2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)},
  pages={364--368},
  year={2015},
  organization={IEEE}
}



@inproceedings{wang2018giant,
  title={GIANT: Globally improved approximate Newton method for distributed optimization},
  author={Wang, Shusen and Roosta, Fred and Xu, Peng and Mahoney, Michael W},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2332--2342},
  year={2018}
}

@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International Conference on Machine Learning},
  pages={343--351},
  year={2013}
}

@inproceedings{liu2020On,
title={On the Variance of the Adaptive Learning Rate and Beyond},
author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4148--4158},
  year={2017}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{bengio2015rmsprop,
  title={Rmsprop and equilibrated adaptive learning rates for nonconvex optimization},
  author={Bengio, Yoshua},
  journal={corr abs/1502.04390},
  year={2015}
}

@article{bekas2007estimator,
  title={An estimator for the diagonal of a matrix},
  author={Bekas, Costas and Kokiopoulou, Effrosyni and Saad, Yousef},
  journal={Applied numerical mathematics},
  volume={57},
  number={11-12},
  pages={1214--1229},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{ott2018scaling,
  title={Scaling Neural Machine Translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={1--9},
  year={2018}
}

@inproceedings{ott2019fairseq,
  title = {{FairSeq}: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{zhang2019improving,
  title={Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention},
  author={Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={897--908},
  year={2019}
}

@article{zhang2019fixup,
  title={Fixup initialization: Residual learning without normalization},
  author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal={arXiv preprint arXiv:1901.09321},
  year={2019}
}


@article{zhang2019adam,
  title={Why ADAM Beats SGD for Attention Models},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  journal={arXiv preprint arXiv:1912.03194},
  year={2019}
}

@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@inproceedings{mikolov2011empirical,
  title={Empirical evaluation and combination of advanced language modeling techniques},
  author={Mikolov, Tom{\'a}{\v{s}} and Deoras, Anoop and Kombrink, Stefan and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan},
  booktitle={Twelfth Annual Conference of the International Speech Communication Association},
  year={2011}
}



@inproceedings{ma2019tensorized,
  title={A tensorized transformer for language modeling},
  author={Ma, Xindian and Zhang, Peng and Zhang, Shuai and Duan, Nan and Hou, Yuexian and Zhou, Ming and Song, Dawei},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2229--2239},
  year={2019}
}

@ONLINE{adahessian,
  title = {https://github.com/amirgholami/ADAHESSIAN.git},
  month = May,
  year = 2020
}

@article{phang2018sentence,
  title={Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks},
  author={Phang, Jason and F{\'e}vry, Thibault and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1811.01088},
  year={2018}
}

@article{martens2014new,
  title={New insights and perspectives on the natural gradient method},
  author={Martens, James},
  journal={arXiv preprint arXiv:1412.1193},
  year={2014}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}


@inproceedings{martens2011learning,
  title={Learning recurrent neural networks with hessian-free optimization},
  author={Martens, James and Sutskever, Ilya},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={1033--1040},
  year={2011},
  organization={Citeseer}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{mcmahan2010adaptive,
  title={Adaptive bound optimization for online convex optimization},
  author={McMahan, H Brendan and Streeter, Matthew},
  journal={arXiv preprint arXiv:1002.4908},
  year={2010}
}


@article{bollapragada2019exact,
  title={Exact and inexact subsampled Newton methods for optimization},
  author={Bollapragada, Raghu and Byrd, Richard H and Nocedal, Jorge},
  journal={IMA Journal of Numerical Analysis},
  volume={39},
  number={2},
  pages={545--578},
  year={2019},
  publisher={Oxford University Press}
}


@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}

@inproceedings{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1724--1732},
  year={2017},
  organization={JMLR. org}
}

@article{levy2016power,
  title={The power of normalization: Faster evasion of saddle points},
  author={Levy, Kfir Y},
  journal={arXiv preprint arXiv:1611.04831},
  year={2016}
}



@article{agarwal2016finding,
  title={Finding approximate local minima for nonconvex optimization in linear time},
  author={Agarwal, Naman and Allen-Zhu, Zeyuan and Bullins, Brian and Hazan, Elad and Ma, Tengyu},
  journal={arXiv preprint arXiv:1611.01146},
  year={2016}
}

@article{carmon2018accelerated,
  title={Accelerated methods for nonconvex optimization},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={2},
  pages={1751--1772},
  year={2018},
  publisher={SIAM}
}
@article{reddi2017generic,
  title={A generic approach for escaping saddle points},
  author={Reddi, Sashank J and Zaheer, Manzil and Sra, Suvrit and Poczos, Barnabas and Bach, Francis and Salakhutdinov, Ruslan and Smola, Alexander J},
  journal={arXiv preprint arXiv:1709.01434},
  year={2017}
}


@article{byrd1995limited,
  title={A limited memory algorithm for bound constrained optimization},
  author={Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  journal={SIAM Journal on scientific computing},
  volume={16},
  number={5},
  pages={1190--1208},
  year={1995},
  publisher={SIAM}
}

@article{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  journal={arXiv preprint arXiv:1802.09568},
  year={2018}
}

@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k\^{} 2)},
  author={Nesterov, Yurii},
  booktitle={Doklady an ussr},
  volume={269},
  pages={543--547},
  year={1983}
}

@incollection{wang2017deep,
  title={Deep \& cross network for ad click predictions},
  author={Wang, Ruoxi and Fu, Bin and Fu, Gang and Wang, Mingliang},
  booktitle={Proceedings of the ADKDD'17},
  pages={1--7},
  year={2017},
}

@book{martens2016second,
  title={Second-order optimization for neural networks},
  author={Martens, James},
  year={2016},
  publisher={University of Toronto (Canada)}
}

@article{mishra2017apprentice,
  title={Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy},
  author={Mishra, Asit and Marr, Debbie},
  journal={arXiv preprint arXiv:1711.05852},
  year={2017}
}

@article{chen2019fast,
  title={Fast Evaluation and Approximation of the Gauss-Newton Hessian Matrix for the Multilayer Perceptron},
  author={Chen, Chao and Reiz, Severin and Yu, Chenhan and Bungartz, Hans-Joachim and Biros, George},
  journal={arXiv preprint arXiv:1910.12184},
  year={2019}
}
@article{tan2019efficientnet,
  title={Efficient{N}et: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:1905.11946},
  year={2019}
}
@article{han2017efficient,
  title={Efficient methods and hardware for deep learning},
  author={Han, Song and Dally, B},
  journal={University Lecture},
  year={2017}
}


@inproceedings{yin2020dreaming,
  title={Dreaming to distill: Data-free knowledge transfer via DeepInversion},
  author={Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8715--8724},
  year={2020}
}
@inproceedings{yang2017designing,
  title={Designing energy-efficient convolutional neural networks using energy-aware pruning},
  author={Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5687--5695},
  year={2017}
}


@article{yao2020hawqv3,
  title={{HAWQV3}: Dyadic Neural Network Quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2011.10680},
  year={2020}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated end-to-end optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={578--594},
  year={2018}
}


@article{micikevicius2022fp8,
  title={FP8 Formats for Deep Learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}



@article{liu2022loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={85--116},
  year={2022},
  publisher={Elsevier}
}

@article{kuzmin2022fp8,
  title={FP8 Quantization: The Power of the Exponent},
  author={Kuzmin, Andrey and Van Baalen, Mart and Ren, Yuwei and Nagel, Markus and Peters, Jorn and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2208.09225},
  year={2022}
}
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}
@article{schraudolph1999fast,
  title={A fast, compact approximation of the exponential function},
  author={Schraudolph, Nicol N},
  journal={Neural Computation},
  volume={11},
  number={4},
  pages={853--862},
  year={1999},
  publisher={MIT Press}
}

@inproceedings{detrey2005parameterized,
  title={A parameterized floating-point exponential function for FPGAs},
  author={Detrey, J{\'e}r{\'e}mie and de Dinechin, Florent},
  booktitle={Proceedings. 2005 IEEE International Conference on Field-Programmable Technology, 2005.},
  pages={27--34},
  year={2005},
  organization={IEEE}
}

@techreport{thomas2004libm,
  title={The libm library and floatingpoint arithmetic in HP-UX for Itanium-based systems},
  author={Thomas, James W and Okada, John P and Markstein, Peter and Li, Ren-Chang},
  year={2004},
  institution={Technical report, Hewlett-Packard Company, Palo Alto, CA, USA}
}

@article{kwon2022fast,
  title={A Fast Post-Training Pruning Framework for Transformers},
  author={Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2204.09656},
  year={2022}
}


@article{hou2020dynabert,
  title={Dynabert: Dynamic bert with adaptive width and depth},
  author={Hou, Lu and Huang, Zhiqi and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9782--9793},
  year={2020}
}

@article{sajjad2020poor,
  title={Poor man’s bert: Smaller and faster transformer models},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={arXiv preprint arXiv:2004.03844},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{marin2021token,
  title={Token pooling in vision transformers},
  author={Marin, Dmitrii and Chang, Jen-Hao Rick and Ranjan, Anurag and Prabhu, Anish and Rastegari, Mohammad and Tuzel, Oncel},
  journal={arXiv preprint arXiv:2110.03860},
  year={2021}
}

@article{kim2020length,
  title={Length-adaptive transformer: Train once with length drop, use anytime with search},
  author={Kim, Gyuwan and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2010.07003},
  year={2020}
}
@inproceedings{wang2021spatten,
  title={Spatten: Efficient sparse attention architecture with cascade token and head pruning},
  author={Wang, Hanrui and Zhang, Zhekai and Han, Song},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={97--110},
  year={2021},
  organization={IEEE}
}

@article{kim2021learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2107.00910},
  year={2021}
}

@inproceedings{goyal2020power,
  title={PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination},
  author={Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
  booktitle={International Conference on Machine Learning},
  pages={3690--3699},
  year={2020},
  organization={PMLR}
}


@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{liu2021ebert,
  title={EBERT: Efficient BERT Inference with Dynamic Structured Pruning},
  author={Liu, Zejian and Li, Fanrong and Li, Gang and Cheng, Jian},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4814--4823},
  year={2021}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{parashar2019timeloop,
  title={Timeloop: A systematic approach to dnn accelerator evaluation},
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W and Emer, Joel},
  booktitle={2019 IEEE international symposium on performance analysis of systems and software (ISPASS)},
  pages={304--315},
  year={2019},
  organization={IEEE}
}


@inproceedings{zhang2022full,
  title={A full-stack search technique for domain optimized deep learning accelerators},
  author={Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={27--42},
  year={2022}
}

@inproceedings{yang2020interstellar,
  title={Interstellar: Using halide's scheduling language to analyze dnn accelerators},
  author={Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff and Pu, Jing and Nayak, Ankita and Bell, Steven and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and others},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={369--383},
  year={2020}
}

@article{kwon2020flexion,
  title={Flexion: A quantitative metric for flexibility in DNN accelerators},
  author={Kwon, Hyoukjun and Pellauer, Michael and Parashar, Angshuman and Krishna, Tushar},
  journal={IEEE Computer Architecture Letters},
  volume={20},
  number={1},
  pages={1--4},
  year={2020},
  publisher={IEEE}
}

@inproceedings{huang2021cosa,
  title={CoSA: Scheduling by Constrained Optimization for Spatial Accelerators},
  author={Huang, Qijing and Kang, Minwoo and Dinh, Grace and Norell, Thomas and Kalaiah, Aravind and  Demmel, James and Wawrzynek, John  and Shao, Yakun Sophia},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={554--566},
  year={2021},
  organization={IEEE}
}
@inproceedings{wu2019accelergy,
  title={Accelergy: An architecture-level energy estimation methodology for accelerator designs},
  author={Wu, Yannan Nellie and Emer, Joel S and Sze, Vivienne},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@article{sekanina2021neural,
  title={Neural Architecture Search and Hardware Accelerator Co-Search: A Survey},
  author={Sekanina, Lukas},
  journal={IEEE Access},
  volume={9},
  pages={151337--151362},
  year={2021},
  publisher={IEEE}
}


@techreport{ZJS+20Ansor,
	abstract = {High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy. We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA GPU by up to \$3.8{\textbackslash}times\$, \$2.6{\textbackslash}times\$, and \$1.7{\textbackslash}times\$, respectively.},
	annotation = {Comment: Published in OSDI 2020},
	author = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
	date = {2020-11},
	doi = {10.48550/arXiv.2006.06762},
	file = {:ZJS+20 - Ansor _ Generating High Performance Tensor Programs for Deep Learning.pdf:PDF},
	institution = {arXiv},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Performance, Computer Science - Programming Languages, Statistics - Machine Learning},
	note = {arXiv:2006.06762 [cs, stat] type: article},
	shorttitle = {Ansor},
	title = {Ansor: {Generating} {High}-{Performance} {Tensor} {Programs} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2006.06762},
	urldate = {2022-05-13},
	bdsk-url-1 = {http://arxiv.org/abs/2006.06762},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2006.06762}}


@article{wu2020lite,
  title={Lite transformer with long-short range attention},
  author={Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  journal={arXiv preprint arXiv:2004.11886},
  year={2020}
}

@article{iandola2020squeezebert,
  title={SqueezeBERT: What can computer vision teach NLP about efficient neural networks?},
  author={Iandola, Forrest N and Shaw, Albert E and Krishna, Ravi and Keutzer, Kurt W},
  journal={arXiv preprint arXiv:2006.11316},
  year={2020}
}


@article{mehta2021mobilevit,
  title={Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2110.02178},
  year={2021}
}
@inproceedings{chen2022mobile,
  title={Mobile-former: Bridging mobilenet and transformer},
  author={Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Dong, Xiaoyi and Yuan, Lu and Liu, Zicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5270--5279},
  year={2022}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}


@article{li2022efficientformer,
  title={EfficientFormer: Vision Transformers at MobileNet Speed},
  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Eric and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  journal={arXiv preprint arXiv:2206.01191},
  year={2022}
}

@article{cai2022efficientvit,
  title={EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition},
  author={Cai, Han and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2205.14756},
  year={2022}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@inproceedings{burchi2021efficient,
  title={Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition},
  author={Burchi, Maxime and Vielzeuf, Valentin},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={8--15},
  year={2021},
  organization={IEEE}
}

@article{kim2022squeezeformer,
  title={Squeezeformer: An Efficient Transformer for Automatic Speech Recognition},
  author={Kim, Sehoon and Gholami, Amir and Shaw, Albert and Lee, Nicholas and Mangalam, Karttikeya and Malik, Jitendra and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2206.00888},
  year={2022}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@article{benmeziane2021comprehensive,
  title={A comprehensive survey on hardware-aware neural architecture search},
  author={Benmeziane, Hadjer and Maghraoui, Kaoutar El and Ouarnoughi, Hamza and Niar, Smail and Wistuba, Martin and Wang, Naigang},
  journal={arXiv preprint arXiv:2101.09336},
  year={2021}
}

@article{ren2021comprehensive,
  title={A comprehensive survey of neural architecture search: Challenges and solutions},
  author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={4},
  pages={1--34},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@inproceedings{zhong2018practical,
  title={Practical block-wise neural network architecture generation},
  author={Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2423--2432},
  year={2018}
}

@inproceedings{dong2018dpp,
  title={Dpp-net: Device-aware progressive search for pareto-optimal neural architectures},
  author={Dong, Jin-Dong and Cheng, An-Chieh and Juan, Da-Cheng and Wei, Wei and Sun, Min},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={517--531},
  year={2018}
}

@article{liu2017hierarchical,
  title={Hierarchical representations for efficient architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1711.00436},
  year={2017}
}

@inproceedings{wang2021alphanet,
  title={AlphaNet: improved training of supernets with alpha-divergence},
  author={Wang, Dilin and Gong, Chengyue and Li, Meng and Liu, Qiang and Chandra, Vikas},
  booktitle={International Conference on Machine Learning},
  pages={10760--10771},
  year={2021},
  organization={PMLR}
}

@inproceedings{real2019regularized,
  title={Regularized evolution for image classifier architecture search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4780--4789},
  year={2019}
}
@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{cai2018proxylessnas,
  title={Proxylessnas: Direct neural architecture search on target task and hardware},
  author={Cai, Han and Zhu, Ligeng and Han, Song},
  journal={arXiv preprint arXiv:1812.00332},
  year={2018}
}

@article{so2021searching,
  title={Searching for Efficient Transformers for Language Modeling},
  author={So, David and Ma{\'n}ke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6010--6022},
  year={2021}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={International Conference on Machine Learning},
  pages={2902--2911},
  year={2017},
  organization={PMLR}
}

@inproceedings{bender2018understanding,
  title={Understanding and simplifying one-shot architecture search},
  author={Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={550--559},
  year={2018},
  organization={PMLR}
}



%%%% Start hardware refs - Coleman


@misc{pruning,
  doi = {10.48550/ARXIV.1506.02626},
  
  url = {https://arxiv.org/abs/1506.02626},
  
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning both Weights and Connections for Efficient Neural Networks},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{dsagen,
author = {Weng, Jian and Liu, Sihao and Dadu, Vidushi and Wang, Zhengrong and Shah, Preyas and Nowatzki, Tony},
title = {DSAGEN: Synthesizing Programmable Spatial Accelerators},
year = {2020},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
series = {ISCA '20}
}

@ARTICLE{dta-trans,  author={Yang, Tao and Ma, Hui and Li, Xiaoling and Liu, Fangxin and Zhao, Yilong and He, Zhezhi and Jiang, Li},  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},   title={DTATrans: Leveraging Dynamic Token-based Quantization with Accuracy Compensation Mechanism for Efficient Transformer Architecture},   year={2022},  volume={},  number={},  pages={1-1},  doi={10.1109/TCAD.2022.3181541}}

@misc{flashattention,
  doi = {10.48550/ARXIV.2205.14135},
  
  url = {https://arxiv.org/abs/2205.14135},
  
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{gobo,
  title={GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference},
  author={Ali Hadi Zadeh and A. Moshovos},
  booktitle={53rd IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2020}
}

@inproceedings{scnn,
author = {Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W. and Dally, William J.},
title = {SCNN: An Accelerator for Compressed-Sparse Convolutional Neural Networks},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {27–40},
numpages = {14},
keywords = {accelerator architecture, Convolutional neural networks},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@misc{onlinenormalizer,
  doi = {10.48550/ARXIV.1805.02867},
  
  url = {https://arxiv.org/abs/1805.02867},
  
  author = {Milakov, Maxim and Gimelshein, Natalia},
  
  keywords = {Performance (cs.PF), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Online normalizer calculation for softmax},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{compressionandacceleration,
  author={Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal={Proceedings of the IEEE}, 
  title={Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey}, 
  year={2020},
  volume={108},
  number={4},
  pages={485-532},
  doi={10.1109/JPROC.2020.2976475}}

@ARTICLE{sparseunstructured,
  author={Zhang, Jie-Fang and Lee, Ching-En and Liu, Chester and Shao, Yakun Sophia and Keckler, Stephen W. and Zhang, Zhengya},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={{SNAP: An Efficient Sparse Neural Acceleration Processor for Unstructured Sparse Deep Neural Network Inference}}, 
  year={2021},
  volume={56},
  number={2},
  pages={636-647},
  doi={10.1109/JSSC.2020.3043870}}

@ARTICLE{sparseandirregular,
  author={Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
  journal={Proceedings of the IEEE}, 
  title={Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights}, 
  year={2021},
  volume={109},
  number={10},
  pages={1706-1752},
  doi={10.1109/JPROC.2021.3098483}}
  
  
  
@misc{quantizationmethods,
  doi = {10.48550/ARXIV.2103.13630},
  
  url = {https://arxiv.org/abs/2103.13630},
  
  author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Survey of Quantization Methods for Efficient Neural Network Inference},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{approximatecomputing,
author = {Armeniakos, Giorgos and Zervakis, Georgios and Soudris, Dimitrios and Henkel, J\"{o}rg},
title = {Hardware Approximate Techniques for Deep Neural Network Accelerators: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527156},
doi = {10.1145/3527156},
abstract = {Deep Neural Networks (DNNs) are very popular because of their high performance in various cognitive tasks in Machine Learning (ML). Recent advancements in DNNs have brought beyond human accuracy in many tasks, but at the cost of high computational complexity. To enable efficient execution of DNN inference, more and more research works, therefore, exploit the inherent error resilience of DNNs and employ Approximate Computing (AC) principles to address the elevated energy demands of DNN accelerators. This article provides a comprehensive survey and analysis of hardware approximation techniques for DNN accelerators. First, we analyze the state of the art and by identifying approximation families, we cluster the respective works with respect to the approximation type. Next, we analyze the complexity of the performed evaluations (with respect to the dataset and DNN size) to assess the efficiency, the potential, and limitations of approximate DNN accelerators. Moreover, a broad discussion is provided, regarding error metrics that are more suitable for designing approximate units for DNN accelerators as well as accuracy recovery approaches that are tailored to DNN inference. Finally, we present how Approximate Computing for DNN accelerators can go beyond energy efficiency and address reliability and security issues, as well.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {mar},
keywords = {Approximate Computing, Error Metrics, Hardware Approximation, Arithmetic Circuits, Deep Neural Networks}
}

@article{hardwaresoftwaresurvey,
	doi = {10.1109/access.2020.3039858},
  
	url = {https://doi.org/10.1109%2Faccess.2020.3039858},
  
	year = 2020,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {8},
  
	pages = {225134--225180},
  
	author = {Maurizio Capra and Beatrice Bussolino and Alberto Marchisio and Guido Masera and Maurizio Martina and Muhammad Shafique},
  
	title = {Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead},
  
	journal = {{IEEE} Access}
}

@INPROCEEDINGS{energyproblem,  author={Horowitz, Mark},  booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},   title={1.1 Computing's energy problem (and what we can do about it)},   year={2014},  volume={},  number={},  pages={10-14},  doi={10.1109/ISSCC.2014.6757323}}

@ARTICLE{dataflows,  author={Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},  journal={IEEE Micro},   title={Using Dataflow to Optimize Energy Efficiency of Deep Neural Network Accelerators},   year={2017},  volume={37},  number={3},  pages={12-21},  doi={10.1109/MM.2017.54}}

@ARTICLE{gpu,  author={Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},  journal={IEEE Micro},   title={NVIDIA A100 Tensor Core GPU: Performance and Innovation},   year={2021},  volume={41},  number={2},  pages={29-35},  doi={10.1109/MM.2021.3061394}}

@misc{efficientprocessing,
  doi = {10.48550/ARXIV.1703.09039},
  
  url = {https://arxiv.org/abs/1703.09039},
  
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{Jouppi2017, 
author={N. P. {Jouppi} and C. {Young} and N. {Patil} and D. {Patterson} and G. {Agrawal} and R. {Bajwa} and S. {Bates} and S. {Bhatia} and N. {Boden} and A. {Borchers} and R. {Boyle} and P. {Cantin} and C. {Chao} and C. {Clark} and J. {Coriell} and M. {Daley} and M. {Dau} and J. {Dean} and B. {Gelb} and T. V. {Ghaemmaghami} and R. {Gottipati} and W. {Gulland} and R. {Hagmann} and C. R. {Ho} and D. {Hogberg} and J. {Hu} and R. {Hundt} and D. {Hurt} and J. {Ibarz} and A. {Jaffey} and A. {Jaworski} and A. {Kaplan} and H. {Khaitan} and D. {Killebrew} and A. {Koch} and N. {Kumar} and S. {Lacy} and J. {Laudon} and J. {Law} and D. {Le} and C. {Leary} and Z. {Liu} and K. {Lucke} and A. {Lundin} and G. {MacKean} and A. {Maggiore} and M. {Mahony} and K. {Miller} and R. {Nagarajan} and R. {Narayanaswami} and R. {Ni} and K. {Nix} and T. {Norrie} and M. {Omernick} and N. {Penukonda} and A. {Phelps} and J. {Ross} and M. {Ross} and A. {Salek} and E. {Samadiani} and C. {Severn} and G. {Sizikov} and M. {Snelham} and J. {Souter} and D. {Steinberg} and A. {Swing} and M. {Tan} and G. {Thorson} and B. {Tian} and H. {Toma} and E. {Tuttle} and V. {Vasudevan} and R. {Walter} and W. {Wang} and E. {Wilcox} and D. H. {Yoon}}, 
booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 
title={In-datacenter performance analysis of a tensor processing unit}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-12}, 
keywords={application specific integrated circuits;circuit optimisation;computer centres;graphics processing units;integrated circuit modelling;low-power electronics;memory architecture;microprocessor chips;neural nets;random-access storage;tensor processing unit;cost-energy-performance;domain-specific hardware;MAC matrix multiply unit;in-datacenter performance analysis;TeraOps/second;software-managed on-chip memory;TPU deterministic execution model;GDDR5 memory;Nvidia K80 GPU;neural networks;TOPS/Watt;production NN applications;server-class Intel Haswell CPU;time-varying optimizations;Transmission line matrix methods;Graphics processing units;Artificial neural networks;Central Processing Unit;Tensile stress;Training;Hardware;DNN;MLP;CNN;RNN;LSTM;neural network;deep learning;domain-specific architecture;accelerator;TensorFlow;TPU;GPU}, 
doi={10.1145/3079856.3080246}, 
ISSN={}, 
month={June},}

@inproceedings{snapea,
author = {Akhlaghi, Vahideh and Yazdanbakhsh, Amir and Samadi, Kambiz and Gupta, Rajesh K. and Esmaeilzadeh, Hadi},
title = {SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks},
year = {2018},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {662–673},
numpages = {12},
}

@misc{vsquant,
  doi = {10.48550/ARXIV.2102.04503},
  
  url = {https://arxiv.org/abs/2102.04503},
  
  author = {Dai, Steve and Venkatesan, Rangharajan and Ren, Haoxing and Zimmer, Brian and Dally, William J. and Khailany, Brucek},
  
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{adaptivebutterfly,
  doi = {10.48550/ARXIV.2209.09570},
  
  url = {https://arxiv.org/abs/2209.09570},
  
  author = {Fan, Hongxiang and Chau, Thomas and Venieris, Stylianos I. and Lee, Royson and Kouris, Alexandros and Luk, Wayne and Lane, Nicholas D. and Abdelfattah, Mohamed S.},
  
  keywords = {Hardware Architecture (cs.AR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

  @INPROCEEDINGS{pervectorquant,  author={Keller, Ben and Venkatesan, Rangharajan and Dai, Steve and Tell, Stephen G. and Zimmer, Brian and Dally, William J. and Thomas Gray, C. and Khailany, Brucek},  booktitle={2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)},   title={A 17–95.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm},   year={2022},  volume={},  number={},  pages={16-17},  doi={10.1109/VLSITechnologyandCir46769.2022.9830277}}

@misc{fused-dataflow,
  doi = {10.48550/ARXIV.2107.06419},
  
  url = {https://arxiv.org/abs/2107.06419},
  
  author = {Kao, Sheng-Chun and Subramanian, Suvinay and Agrawal, Gaurav and Yazdanbakhsh, Amir and Krishna, Tushar},
  
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@INPROCEEDINGS{sparsetransformer,  author={Wang, Yang and Qin, Yubin and Deng, Dazheng and Wei, Jingchuan and Zhou, Yang and Fan, Yuanqi and Chen, Tianbao and Sun, Hao and Liu, Leibo and Wei, Shaojun and Yin, Shouyi},  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)},   title={A 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing},   year={2022},  volume={65},  number={},  pages={1-3},  doi={10.1109/ISSCC42614.2022.9731686}}

@article{divit,
title = {DiVIT: Algorithm and architecture co-design of differential attention in vision transformer},
journal = {Journal of Systems Architecture},
volume = {128},
pages = {102520},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102520},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122000868},
author = {Yangfan Li and Yikun Hu and Fan Wu and Kenli Li},
keywords = {Vision transformer, Accelerators, Patch locality, Neural networks, ASIC},
abstract = {Recently, stransformer has achieve remarkable performance on various computer vision tasks. Unfortunately, vision transformer suffers from high computational cost to calculate the pair-wise relations of patches for images, and the computations grows quadratically with the number of patches in the images, which blocks their deployment on resource-limited devices such as mobile phones and various IoT devices. In this paper, we find that there are large amount of redundant computations and communications in the self-attention operation of vision transformers because of the high degree patch locality, and present a hardware–software co-designed solution for vision transformer exploiting the patch locality, termed as DiVIT. DiVIT substantially reduces redundant computations and communications in vision transformers and improves the performance and energy efficiency. The experiments demonstrates that DiVIT achieves an average of 8.2× and 41.3× speedup and over three orders of magnitude improvements energy-efficiency over CPUs and GPUs on real world datasets.}
}

@misc{vitcod,
  doi = {10.48550/ARXIV.2210.09573},
  
  url = {https://arxiv.org/abs/2210.09573},
  
  author = {You, Haoran and Sun, Zhanyi and Shi, Huihong and Yu, Zhongzhi and Zhao, Yang and Zhang, Yongan and Li, Chaojian and Li, Baopu and Lin, Yingyan},
  
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@INPROCEEDINGS{softermax,  author={Stevens, Jacob R. and Venkatesan, Rangharajan and Dai, Steve and Khailany, Brucek and Raghunathan, Anand},  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)},   title={Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers},   year={2021},  volume={},  number={},  pages={469-474},  doi={10.1109/DAC18074.2021.9586134}}

@inproceedings{nnlut,
author = {Yu, Joonsang and Park, Junki and Park, Seongmin and Kim, Minsoo and Lee, Sihwa and Lee, Dong Hyun and Choi, Jungwook},
title = {NN-LUT: Neural Approximation of Non-Linear Operations for Efficient Transformer Inference},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530505},
doi = {10.1145/3489517.3530505},
abstract = {Non-linear operations such as GELU, Layer normalization, and Soft-max are essential yet costly building blocks of Transformer models. Several prior works simplified these operations with look-up tables or integer computations, but such approximations suffer inferior accuracy or considerable hardware cost with long latency. This paper proposes an accurate and hardware-friendly approximation framework for efficient Transformer inference. Our framework employs a simple neural network as a universal approximator with its structure equivalently transformed into a Look-up table(LUT). The proposed framework called Neural network generated LUT(NN-LUT) can accurately replace all the non-linear operations in popular BERT models with significant reductions in area, power consumption, and latency.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {577–582},
numpages = {6},
keywords = {non-linear function, transformer, neural network, look-up table},
location = {San Francisco, California},
series = {DAC '22}
}

@online{max_trick,
  author = {McCaffrey, James}, 
  title = {The Max Trick when Computing Softmax},
  year = 2016,
  url = {https://jamesmccaffrey.wordpress.com/2016/03/04/the-max-trick-when-computing-softmax/},
}

@online{logsumexp,
  author = {Eisele, Robert}, 
  title = {The log-sum-exp trick in Machine Learning},
  year = 2016,
  url = {https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/},
}

@ARTICLE{energon,  author={Zhou, Zhe and Liu, Junlin and Gu, Zhenyu and Sun, Guangyu},  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},   title={Energon: Towards Efficient Acceleration of Transformers Using Dynamic Sparse Attention},   year={2022},  volume={},  number={},  pages={1-1},  doi={10.1109/TCAD.2022.3170848}}

@INPROCEEDINGS{columnbalance,  author={Peng, Hongwu and Huang, Shaoyi and Geng, Tong and Li, Ang and Jiang, Weiwen and Liu, Hang and Wang, Shusen and Ding, Caiwen},  booktitle={2021 22nd International Symposium on Quality Electronic Design (ISQED)},   title={Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning},   year={2021},  volume={},  number={},  pages={142-148},  doi={10.1109/ISQED51717.2021.9424344}}

@INPROCEEDINGS{fullyquantized,  author={Liu, Zejian and Li, Gang and Cheng, Jian},  booktitle={2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)},   title={Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing},   year={2021},  volume={},  number={},  pages={513-516},  doi={10.23919/DATE51398.2021.9474043}}

@INPROCEEDINGS{speformer,  author={Sun, Yang and Hu, Wei and Liu, Fang and Jiang, Min and Huang, FeiHu and Xu, Dian},  booktitle={2022 IEEE 9th International Conference on Cyber Security and Cloud Computing (CSCloud)/2022 IEEE 8th International Conference on Edge Computing and Scalable Cloud (EdgeCom)},   title={Speformer: An Efficient Hardware-Software Cooperative Solution for Sparse Spectral Transformer},   year={2022},  volume={},  number={},  pages={180-185},  doi={10.1109/CSCloud-EdgeCom54986.2022.00039}}

@inproceedings{ftrans,
author = {Li, Bingbing and Pandey, Santosh and Fang, Haowen and Lyv, Yanjun and Li, Ji and Chen, Jieyang and Xie, Mimi and Wan, Lipeng and Liu, Hang and Ding, Caiwen},
title = {FTRANS: Energy-Efficient Acceleration of Transformers Using FPGA},
year = {2020},
isbn = {9781450370530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3370748.3406567},
doi = {10.1145/3370748.3406567},
abstract = {In natural language processing (NLP), the "Transformer" architecture was proposed as the first transduction model replying entirely on self-attention mechanisms without using sequence-aligned recurrent neural networks (RNNs) or convolution, and it achieved significant improvements for sequence to sequence tasks. The introduced intensive computation and storage of these pre-trained language representations has impeded their popularity into computation and memory constrained devices. The field-programmable gate array (FPGA) is widely used to accelerate deep learning algorithms for its high parallelism and low latency. However, the trained models are still too large to accommodate to an FPGA fabric. In this paper, we propose an efficient acceleration framework, Ftrans, for transformer-based large scale language representations. Our framework includes enhanced block-circulant matrix (BCM)-based weight representation to enable model compression on large-scale language representations at the algorithm level with few accuracy degradation, and an acceleration design at the architecture level. Experimental results show that our proposed framework significantly reduce the model size of NLP models by up to 16 times. Our FPGA design achieves 27.07\texttimes{} and 81 \texttimes{} improvement in performance and energy efficiency compared to CPU, and up to 8.80\texttimes{} improvement in energy efficiency compared to GPU.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
pages = {175–180},
numpages = {6},
location = {Boston, Massachusetts},
series = {ISLPED '20}
}

@INPROCEEDINGS{efficientsoftmax,
  author={Yuan, Bo},
  booktitle={2016 29th IEEE International System-on-Chip Conference (SOCC)}, 
  title={Efficient hardware architecture of softmax layer in deep neural network}, 
  year={2016},
  volume={},
  number={},
  pages={323-326},
  doi={10.1109/SOCC.2016.7905501}}

@INPROCEEDINGS{highspeedsoftmax,
  author={Wang, Meiqi and Lu, Siyuan and Zhu, Danyang and Lin, Jun and Wang, Zhongfeng},
  booktitle={2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)}, 
  title={A High-Speed and Low-Complexity Architecture for Softmax Function in Deep Learning}, 
  year={2018},
  volume={},
  number={},
  pages={223-226},
  doi={10.1109/APCCAS.2018.8605654}}

@INPROCEEDINGS{dsasoftmax,
  author={Wei, Zhigang and Arora, Aman and Patel, Pragenesh and John, Lizy},
  booktitle={2020 IEEE 31st International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 
  title={Design Space Exploration for Softmax Implementations}, 
  year={2020},
  volume={},
  number={},
  pages={45-52},
  doi={10.1109/ASAP49362.2020.00017}}

@inproceedings{sanger,
author = {Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
title = {Sanger: A Co-Design Framework for Enabling Sparse Attention Using Reconfigurable Architecture},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480125},
doi = {10.1145/3466752.3480125},
abstract = {In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving. This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {977–991},
numpages = {15},
keywords = {systolic array, reconfigurable architecture, Transformer, attention, sparse, hardware-software co-design},
location = {Virtual Event, Greece},
series = {MICRO '21}
}


@inproceedings{ibert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}


@article{a3,
 title={A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation},
  author={Tae Jun Ham and S. J. Jung and Seonghak Kim and Young H. Oh and Yeonhong Park and Yongchan Song and Junghun Park and Sang-Hee Lee and K. Park and J. Lee and Deog-Kyoon Jeong},
  journal={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year={2020},
  pages={328-341}
}

@INPROCEEDINGS{taylorseries,
  author={Nilsson, Peter and Shaik, Ateeq Ur Rahman and Gangarajaiah, Rakesh and Hertz, Erik},
  booktitle={2014 NORCHIP}, 
  title={Hardware implementation of the exponential function using Taylor series}, 
  year={2014},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/NORCHIP.2014.7004740}}


@incollection{optimus,
 author = {Park, Junki and Yoon, Hyunsung and Ahn, Daehyun and Choi, Jungwook and Kim, Jae-Joon},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {363--378},
 title = {OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neural network accelerator},
 volume = {2},
 year = {2020}
}

@INPROCEEDINGS{shidianno,  author={Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},  booktitle={2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)},   title={ShiDianNao: Shifting vision processing closer to the sensor},   year={2015},  volume={},  number={},  pages={92-104},  doi={10.1145/2749469.2750389}}


@inproceedings{diannao,
 author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
 title = {DianNao: A Small-footprint High-throughput Accelerator for Ubiquitous Machine-learning},
 booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '14},
 year = {2014},
 location = {Salt Lake City, Utah, USA},
 pages = {269--284},
 numpages = {16},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerator, memory, neural networks},
} 

@INPROCEEDINGS{eyeriss,
author={Y. {Chen} and J. {Emer} and V. {Sze}},
booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
title={Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
year={2016},
pages={367-379},
month={June},}

  @article{pudiannao,
author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
title = {PuDianNao: A Polyvalent Machine Learning Accelerator},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694358},
doi = {10.1145/2775054.2694358},
journal = {SIGPLAN Not.},
month = mar,
pages = {369–381},
numpages = {13},
keywords = {machine learning, computer architecture, accelerator}
}


@inproceedings{tangram,
author = {Gao, Mingyu and Yang, Xuan and Pu, Jing and Horowitz, Mark and Kozyrakis, Christos},
title = {TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304014},
doi = {10.1145/3297858.3304014},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {807–820},
numpages = {14},
keywords = {dataflow, neural networks, parallelism},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@article{eie,
 author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
 title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 2016},
 volume = {44},
 number = {3},
 month = jun,
 year = {2016},
 address = {New York, NY, USA},
 keywords = {ASIC, algorithm-hardware co-design, deep learning, hardware acceleration, model compression},
} 

@misc{i-vit,
  doi = {10.48550/ARXIV.2207.01405},
  
  url = {https://arxiv.org/abs/2207.01405},
  
  author = {Li, Zhikai and Gu, Qingyi},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lu2020hardware,
  title={{Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer}},
  author={Lu, Siyuan and Wang, Meiqi and Liang, Shuang and Lin, Jun and Wang, Zhongfeng},
  journal={arXiv preprint arXiv:2009.08605},
  year={2020}
}

@article{park2020optimus,
  title={{OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neural network accelerator}},
  author={Park, Junki and Yoon, Hyunsung and Ahn, Daehyun and Choi, Jungwook and Kim, Jae-Joon},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={363--378},
  year={2020}
}


@ARTICLE{edgebert,
author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
title = {{EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference}},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480095},
abstract = { Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 \texttimes{}, 2.5 \texttimes{}, and 53 \texttimes{} lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {830–844},
numpages = {15}
}



@inproceedings{UCNN,
author = {Hegde, Kartik and Yu, Jiyong and Agrawal, Rohit and Yan, Mengjia and Pellauer, Michael and Fletcher, Christopher W.},
title = {UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition},
year = {2018},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {674–687},
numpages = {14},
}

@inproceedings{cnvlutin,
author = {Albericio, Jorge and Judd, Patrick and Hetherington, Tayler and Aamodt, Tor and Jerger, Natalie Enright and Moshovos, Andreas},
title = {Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
year = {2016},
}

@INPROCEEDINGS{cambriconx,
  author={Zhang, Shijin and Du, Zidong and Zhang, Lei and Lan, Huiying and Liu, Shaoli and Li, Ling and Guo, Qi and Chen, Tianshi and Chen, Yunji},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Cambricon-X: An accelerator for sparse neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/MICRO.2016.7783723}}

@INPROCEEDINGS{cambricons,  author={Zhou, Xuda and Du, Zidong and Guo, Qi and Liu, Shaoli and Liu, Chengsi and Wang, Chao and Zhou, Xuehai and Li, Ling and Chen, Tianshi and Chen, Yunji},  booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},   title={Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach},   year={2018},  volume={},  number={},  pages={15-28},  doi={10.1109/MICRO.2018.00011}}

@INPROCEEDINGS{elsa,
  author={Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W.},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={692-705},
  doi={10.1109/ISCA52012.2021.00060}}

@INPROCEEDINGS{adaptivfloat_dac,
  author={T. {Tambe} and E. -Y. {Yang} and Z. {Wan} and Y. {Deng} and V. {Janapa Reddi} and A. {Rush} and D. {Brooks} and G. -Y. {Wei}},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)}, 
  title={Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference}, 
  year={2020},
}

@INPROCEEDINGS{240gops,  author={Gokhale, Vinayak and Jin, Jonghoon and Dundar, Aysegul and Martini, Berin and Culurciello, Eugenio},  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},   title={A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks},   year={2014},  volume={},  number={},  pages={696-701},  doi={10.1109/CVPRW.2014.106}}

@inproceedings{salo,
author = {Shen, Guan and Zhao, Jieru and Chen, Quan and Leng, Jingwen and Li, Chao and Guo, Minyi},
title = {SALO: An Efficient Spatial Accelerator Enabling Hybrid Sparse Attention Mechanisms for Long Sequences},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530504},
doi = {10.1145/3489517.3530504},
abstract = {The attention mechanisms of transformers effectively extract pertinent information from the input sequence. However, the quadratic complexity of self-attention w.r.t the sequence length incurs heavy computational and memory burdens, especially for tasks with long sequences. Existing accelerators face performance degradation in these tasks. To this end, we propose SALO to enable hybrid sparse attention mechanisms for long sequences. SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation. We show that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads, i.e., Longformer and ViL.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {571–576},
numpages = {6},
location = {San Francisco, California},
series = {DAC '22}
}

@INPROCEEDINGS{transpim,
  author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer}, 
  year={2022},
  volume={},
  number={},
  pages={1071-1085},
  doi={10.1109/HPCA53966.2022.00082}}

@inproceedings{retransformer, author = {Yang, Xiaoxuan and Yan, Bonan and Li, Hai and Chen, Yiran}, title = {ReTransformer: ReRAM-Based Processing-in-Memory Architecture for Transformer Acceleration}, year = {2020}, isbn = {9781450380263}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3400302.3415640}, doi = {10.1145/3400302.3415640}, abstract = {Transformer has emerged as a popular deep neural network (DNN) model for Neural Language Processing (NLP) applications and demonstrated excellent performance in neural machine translation, entity recognition, etc. However, its scaled dot-product attention mechanism in auto-regressive decoder brings a performance bottleneck during inference. Transformer is also computationally and memory intensive and demands for a hardware acceleration solution. Although researchers have successfully applied ReRAM-based Processing-in-Memory (PIM) to accelerate convolutional neural networks (CNNs) and recurrent neural networks (RNNs), the unique computation process of the scaled dot-product attention in Transformer makes it difficult to directly apply these designs. Besides, how to handle intermediate results in Matrix-matrix Multiplication (MatMul) and how to design a pipeline at a finer granularity of Transformer remain unsolved. In this work, we propose ReTransformer - a ReRAM-based PIM architecture for Transformer acceleration. ReTransformer can not only accelerate the scaled dot-product attention of Transformer using ReRAM-based PIM but also eliminate some data dependency by avoiding writing the intermediate results using the proposed matrix decomposition technique. Moreover, we propose a new sub-matrix pipeline design for multi-head self-attention. Experimental results show that compared to GPU and Pipelayer, ReTransformer improves computing efficiency by 23.21\texttimes{} and 3.25\texttimes{}, respectively. The corresponding overall power is reduced by 1086\texttimes{} and 2.82\texttimes{}, respectively.}, booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design}, articleno = {92}, numpages = {9}, keywords = {transformer, processing-in-memory, ReRAM}, location = {Virtual Event, USA}, series = {ICCAD '20} }

@inproceedings{npe,
	doi = {10.1145/3431920.3439477},
  
	url = {https://doi.org/10.1145\%2F3431920.3439477},
  
	year = 2021,
	month = {feb},
  
	publisher = {{ACM}
},
  
	author = {Hamza Khan and Asma Khan and Zainab Khan and Lun Bin Huang and Kun Wang and Lei He},
  
	title = {{NPE}: An {FPGA}-based Overlay Processor for Natural Language Processing},
  
	booktitle = {The 2021 {ACM}/{SIGDA} International Symposium on Field-Programmable Gate Arrays}
}

@misc{reformer,
  doi = {10.48550/ARXIV.2001.04451},
  
  url = {https://arxiv.org/abs/2001.04451},
  
  author = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Reformer: The Efficient Transformer},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{gradientpruning,
author = {Li, Zheng and Ghodrati, Soroush and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi and Kang, Mingu},
title = {Accelerating Attention through Gradient-Based Learned Runtime Pruning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527423},
doi = {10.1145/3470496.3527423},
abstract = {Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. This attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention, which is only determined at runtime. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. The main challenge is finding the threshold for the scores below which subsequent computation will be inconsequential. Although such a threshold is discrete, this paper formulates its search through a soft differentiable regularizer integrated into the loss function of the training. This formulation piggy backs on the back-propagation training to analytically co-optimize the threshold and the weights simultaneously, striking a formally optimal balance between accuracy and computation pruning. To best utilize this mathematical innovation, we devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism. We evaluate our design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision transformer models. Post-layout results show that, on average, LeOPArd yields 1.9\texttimes{}and 3.9\texttimes{}speedup and energy reduction, respectively, while keeping the average accuracy virtually intact (< 0.2% degradation).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {902–915},
numpages = {14},
keywords = {accelerators, gradient-based optimization, attention mechanism, neural processing units, self-attention, learned pruning, deep learning, transformer},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{mokey,
author = {Zadeh, Ali Hadi and Mahmoud, Mostafa and Abdelhadi, Ameer and Moshovos, Andreas},
title = {Mokey: Enabling Narrow Fixed-Point Inference for out-of-the-Box Floating-Point Transformer Models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527438},
doi = {10.1145/3470496.3527438},
abstract = {Increasingly larger and better Transformer models keep advancing state-of-the-art accuracy and capability for Natural Language Processing applications. These models demand more computational power, storage, and energy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit floating-point transformer models by quantizing all values to 4-bit indexes into dictionaries of representative 16-bit fixed-point centroids. Mokey does not need fine-tuning, an essential feature as often the training resources or datasets are not available to many. Exploiting the range of values that naturally occur in transformer models, Mokey selects centroid values to also fit an exponential curve. This unique feature enables Mokey to replace the bulk of the original multiply-accumulate operations with narrow 3b fixed-point additions resulting in an area- and energy-efficient hardware accelerator design. Over a set of state-of-the-art transformer models, the Mokey accelerator delivers an order of magnitude improvements in energy efficiency over a Tensor Cores-based accelerator while improving performance by at least 4\texttimes{} and as much as 15\texttimes{} depending on the model and on-chip buffering capacity. Optionally, Mokey can be used as memory compression assist for any other accelerator transparently stashing wide floating-point or fixed-point activations or weights into narrow 4-bit indexes. Mokey proves superior to prior state-of-the-art quantization methods for Transformers.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {888–901},
numpages = {14},
keywords = {transformer models, natural language processing, quantization},
location = {New York, New York},
series = {ISCA '22}
}

@misc{inmemorypruning,
  doi = {10.48550/ARXIV.2209.00606},
  
  url = {https://arxiv.org/abs/2209.00606},
  
  author = {Yazdanbakhsh, Amir and Moradifirouzabadi, Ashkan and Li, Zheng and Kang, Mingu},
  
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{dota,
author = {Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan},
title = {DOTA: Detect and Omit Weak Attentions for Scalable Transformer Acceleration},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507738},
doi = {10.1145/3503222.3507738},
abstract = {Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-attention. In this paper, we present DOTA, an algorithm-architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-to-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {14–26},
numpages = {13},
keywords = {Transformer Acceleration, Sparse Architecture, SW-HW Co-design},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}


@article{kim2023full,
  title={Full Stack Optimization of Transformer Inference: a Survey},
  author={Kim, Sehoon and Hooper, Coleman and Wattanawong, Thanakul and Kang, Minwoo and Yan, Ruohan and Genc, Hasan and Dinh, Grace and Huang, Qijing and Keutzer, Kurt and Mahoney, Michael W and others},
  journal={arXiv preprint arXiv:2302.14017},
  year={2023}
}

@ARTICLE{dynamicsparseattn,
  author={Liu, Liu and Qu, Zheng and Chen, Zhaodong and Tu, Fengbin and Ding, Yufei and Xie, Yuan},
  journal={IEEE Transactions on Computers}, 
  title={Dynamic Sparse Attention for Scalable Transformer Acceleration}, 
  year={2022},
  volume={},
  number={},
  pages={1-14},
  doi={10.1109/TC.2022.3208206}}

@article{narayan2018don,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@article{elbayad2019depth,
  title={Depth-adaptive transformer},
  author={Elbayad, Maha and Gu, Jiatao and Grave, Edouard and Auli, Michael},
  journal={arXiv preprint arXiv:1910.10073},
  year={2019}
}

@ARTICLE{cooptimizedframework,
  author={Fang, Chao and Zhou, Aojun and Wang, Zhongfeng},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
  title={An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers}, 
  year={2022},
  volume={},
  number={},
  pages={1-14},
  doi={10.1109/TVLSI.2022.3197282}}

@INPROCEEDINGS{highspeedlowcomplexity,
  author={Wang, Meiqi and Lu, Siyuan and Zhu, Danyang and Lin, Jun and Wang, Zhongfeng},
  booktitle={2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)}, 
  title={A High-Speed and Low-Complexity Architecture for Softmax Function in Deep Learning}, 
  year={2018},
  volume={},
  number={},
  pages={223-226},
  doi={10.1109/APCCAS.2018.8605654}}

@article{logsumexp-and-softmax,
    author = {Blanchard, Pierre and Higham, Desmond J and Higham, Nicholas J},
    title = "{Accurately computing the log-sum-exp and softmax functions}",
    journal = {IMA Journal of Numerical Analysis},
    volume = {41},
    number = {4},
    pages = {2311-2330},
    year = {2020},
    month = {08},
    abstract = "{Evaluating the log-sum-exp function or the softmax function is a key step in many modern data science algorithms, notably in inference and classification. Because of the exponentials that these functions contain, the evaluation is prone to overflow and underflow, especially in low-precision arithmetic. Software implementations commonly use alternative formulas that avoid overflow and reduce the chance of harmful underflow, employing a shift or another rewriting. Although mathematically equivalent, these variants behave differently in floating-point arithmetic and shifting can introduce subtractive cancellation. We give rounding error analyses of different evaluation algorithms and interpret the error bounds using condition numbers for the functions. We conclude, based on the analysis and numerical experiments, that the shifted formulas are of similar accuracy to the unshifted ones, so can safely be used, but that a division-free variant of softmax can suffer from loss of accuracy.}",
    issn = {0272-4979},
    doi = {10.1093/imanum/draa038},
    url = {https://doi.org/10.1093/imanum/draa038},
    eprint = {https://academic.oup.com/imajna/article-pdf/41/4/2311/40758053/draa038.pdf},
}

@INPROCEEDINGS{adaptivetiling,
  author={Kung, H. T. and McDanel, Bradley and Zhang, Sai Qian},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
  title={Adaptive Tiling: Applying Fixed-size Systolic Arrays To Sparse Convolutional Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={1006-1011},
  doi={10.1109/ICPR.2018.8545462}}





%%%% end hardware refs


@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{scao2022bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@inproceedings{wang2019non,
  title={Non-autoregressive machine translation with auxiliary regularization},
  author={Wang, Yiren and Tian, Fei and He, Di and Qin, Tao and Zhai, ChengXiang and Liu, Tie-Yan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5377--5384},
  year={2019}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@misc{openai2019chatgpt,
  title="ChatGPT: Optimizing Language Models for Dialogue",
  year={2022},
  howpublished = {\url{https://openai.com/blog/chatgpt/}},
}
