\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and
  Marc Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 29304--29320, 2021.

\bibitem[Ajay et~al.(2022)Ajay, Du, Gupta, Tenenbaum, Jaakkola, and
  Agrawal]{ajay2022conditional}
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit
  Agrawal.
\newblock Is conditional generative modeling all you need for decision-making?
\newblock \emph{arXiv preprint arXiv:2211.15657}, 2022.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter~Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Arulkumaran et~al.(2022)Arulkumaran, Ashley, Schmidhuber, and
  Srivastava]{arulkumaran2022all}
Kai Arulkumaran, Dylan~R Ashley, J{\"u}rgen Schmidhuber, and Rupesh~K
  Srivastava.
\newblock All you need is supervised learning: From imitation learning to
  meta-rl with upside down rl.
\newblock \emph{arXiv preprint arXiv:2202.11960}, 2022.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{baker2022video}
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24639--24654, 2022.

\bibitem[Belghazi et~al.(2018)Belghazi, Baratin, Rajeswar, Ozair, Bengio,
  Courville, and Hjelm]{belghazi2018mine}
Mohamed~Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua
  Bengio, Aaron Courville, and R~Devon Hjelm.
\newblock Mine: mutual information neural estimation.
\newblock \emph{arXiv preprint arXiv:1801.04062}, 2018.

\bibitem[Boborzi et~al.(2022)Boborzi, Straehle, Buchner, and
  Mikelsons]{boborzi2022imitation}
Damian Boborzi, Christoph-Nikolas Straehle, Jens~S Buchner, and Lars Mikelsons.
\newblock Imitation learning by state-only distribution matching.
\newblock \emph{arXiv preprint arXiv:2202.04332}, 2022.

\bibitem[Brandfonbrener et~al.(2022)Brandfonbrener, Bietti, Buckman, Laroche,
  and Bruna]{brandfonbrener2022does}
David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan
  Bruna.
\newblock When does return-conditioned supervised learning work for offline
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2206.01079}, 2022.

\bibitem[Brown et~al.(2020)Brown, Goo, and Niekum]{brown2020better}
Daniel~S Brown, Wonjoon Goo, and Scott Niekum.
\newblock Better-than-demonstrator imitation learning via automatically-ranked
  demonstrations.
\newblock In \emph{Conference on robot learning}, pages 330--359. PMLR, 2020.

\bibitem[Carroll et~al.(2022)Carroll, Paradise, Lin, Georgescu, Sun, Bignell,
  Milani, Hofmann, Hausknecht, Dragan, et~al.]{carroll2022unimask}
Micah Carroll, Orr Paradise, Jessy Lin, Raluca Georgescu, Mingfei Sun, David
  Bignell, Stephanie Milani, Katja Hofmann, Matthew Hausknecht, Anca Dragan,
  et~al.
\newblock Unimask: Unified inference in sequential decision problems.
\newblock \emph{arXiv preprint arXiv:2211.10869}, 2022.

\bibitem[Chang et~al.(2021)Chang, Uehara, Sreenivas, Kidambi, and
  Sun]{chang2021mitigating}
Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun.
\newblock Mitigating covariate shift in imitation learning via offline data
  with partial coverage.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 965--979, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Dadashi et~al.(2020)Dadashi, Hussenot, Geist, and
  Pietquin]{dadashi2020primal}
Robert Dadashi, L{\'e}onard Hussenot, Matthieu Geist, and Olivier Pietquin.
\newblock Primal wasserstein imitation learning.
\newblock \emph{arXiv preprint arXiv:2006.04678}, 2020.

\bibitem[Dance et~al.(2021)Dance, Perez, and Cachet]{dance2021conditioned}
Christopher~R Dance, Julien Perez, and Th{\'e}o Cachet.
\newblock Conditioned reinforcement learning for few-shot imitation.
\newblock In \emph{International Conference on Machine Learning}, pages
  2376--2387. PMLR, 2021.

\bibitem[DeMoss et~al.(2023)DeMoss, Duckworth, Hawes, and
  Posner]{demoss2023ditto}
Branton DeMoss, Paul Duckworth, Nick Hawes, and Ingmar Posner.
\newblock Ditto: Offline imitation learning with world models.
\newblock \emph{arXiv preprint arXiv:2302.03086}, 2023.

\bibitem[Duan et~al.(2017)Duan, Andrychowicz, Stadie, Jonathan~Ho, Schneider,
  Sutskever, Abbeel, and Zaremba]{duan2017one}
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan~Ho, Jonas
  Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba.
\newblock One-shot imitation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Emmons et~al.(2021)Emmons, Eysenbach, Kostrikov, and
  Levine]{emmons2021rvs}
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine.
\newblock Rvs: What is essential for offline rl via supervised learning?
\newblock \emph{arXiv preprint arXiv:2112.10751}, 2021.

\bibitem[Fickinger et~al.(2021)Fickinger, Cohen, Russell, and
  Amos]{fickinger2021cross}
Arnaud Fickinger, Samuel Cohen, Stuart Russell, and Brandon Amos.
\newblock Cross-domain imitation learning via optimal transport.
\newblock \emph{arXiv preprint arXiv:2110.03684}, 2021.

\bibitem[Florence et~al.(2022)Florence, Lynch, Zeng, Ramirez, Wahid, Downs,
  Wong, Lee, Mordatch, and Tompson]{florence2022implicit}
Pete Florence, Corey Lynch, Andy Zeng, Oscar~A Ramirez, Ayzaan Wahid, Laura
  Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson.
\newblock Implicit behavioral cloning.
\newblock In \emph{Conference on Robot Learning}, pages 158--168. PMLR, 2022.

\bibitem[Franzmeyer et~al.(2022)Franzmeyer, Torr, and
  Henriques]{franzmeyer2022learn}
Tim Franzmeyer, Philip~HS Torr, and Jo{\~a}o~F Henriques.
\newblock Learn what matters: cross-domain imitation learning with
  task-relevant embeddings.
\newblock \emph{arXiv preprint arXiv:2209.12093}, 2022.

\bibitem[Fu et~al.(2017)Fu, Luo, and Levine]{fu2017learning}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1710.11248}, 2017.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Furuta et~al.(2021)Furuta, Matsuo, and Gu]{furuta2021generalized}
Hiroki Furuta, Yutaka Matsuo, and Shixiang~Shane Gu.
\newblock Generalized decision transformer for offline hindsight information
  matching.
\newblock \emph{arXiv preprint arXiv:2111.10364}, 2021.

\bibitem[Gangwani and Peng(2020)]{gangwani2020state}
Tanmay Gangwani and Jian Peng.
\newblock State-only imitation with transition dynamics mismatch.
\newblock \emph{arXiv preprint arXiv:2002.11879}, 2020.

\bibitem[Gangwani et~al.(2022)Gangwani, Zhou, and Peng]{gangwani2022imitation}
Tanmay Gangwani, Yuan Zhou, and Jian Peng.
\newblock Imitation learning from observations under transition model
  disparity.
\newblock \emph{arXiv preprint arXiv:2204.11446}, 2022.

\bibitem[Gao et~al.(2021)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and
  Qiao]{gao2021clip}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang,
  Hongsheng Li, and Yu~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{arXiv preprint arXiv:2110.04544}, 2021.

\bibitem[Garg et~al.(2021)Garg, Chakraborty, Cundy, Song, and
  Ermon]{garg2021iq}
Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano
  Ermon.
\newblock Iq-learn: Inverse soft-q learning for imitation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4028--4039, 2021.

\bibitem[Gleave et~al.(2022)Gleave, Taufeeque, Rocamonde, Jenner, Wang, Toyer,
  Ernestus, Belrose, Emmons, and Russell]{gleave2022imitation}
Adam Gleave, Mohammad Taufeeque, Juan Rocamonde, Erik Jenner, Steven~H. Wang,
  Sam Toyer, Maximilian Ernestus, Nora Belrose, Scott Emmons, and Stuart
  Russell.
\newblock imitation: Clean imitation learning implementations, 2022.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 1273--1286, 2021.

\bibitem[Jarboui and Perchet(2021)]{jarboui2021offline}
Firas Jarboui and Vianney Perchet.
\newblock Offline inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.05068}, 2021.

\bibitem[Jarrett et~al.(2020)Jarrett, Bica, and van~der
  Schaar]{jarrett2020strictly}
Daniel Jarrett, Ioana Bica, and Mihaela van~der Schaar.
\newblock Strictly batch imitation learning by energy-based distribution
  matching.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7354--7365, 2020.

\bibitem[Jiang et~al.(2020)Jiang, Pang, and Yu]{jiang2020offline}
Shengyi Jiang, Jingcheng Pang, and Yang Yu.
\newblock Offline imitation learning with a misspecified simulator.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 8510--8520, 2020.

\bibitem[Judah et~al.(2014)Judah, Fern, Tadepalli, and
  Goetschalckx]{judah2014imitation}
Kshitij Judah, Alan Fern, Prasad Tadepalli, and Robby Goetschalckx.
\newblock Imitation learning with demonstrations and shaping rewards.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28, 2014.

\bibitem[Kang et~al.(2023)Kang, Shi, Liu, He, and Wang]{kang2023beyond}
Yachen Kang, Diyuan Shi, Jinxin Liu, Li~He, and Donglin Wang.
\newblock Beyond reward: Offline preference-guided policy optimization.
\newblock \emph{arXiv preprint arXiv:2305.16217}, 2023.

\bibitem[Ke et~al.(2020)Ke, Choudhury, Barnes, Sun, Lee, and
  Srinivasa]{ke2020imitation}
Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and
  Siddhartha Srinivasa.
\newblock Imitation learning as f-divergence minimization.
\newblock In \emph{International Workshop on the Algorithmic Foundations of
  Robotics}, pages 313--329. Springer, 2020.

\bibitem[Ke et~al.(2021)Ke, Choudhury, Barnes, Sun, Lee, and
  Srinivasa]{ke2021imitation}
Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and
  Siddhartha Srinivasa.
\newblock Imitation learning as f-divergence minimization.
\newblock In \emph{Algorithmic Foundations of Robotics XIV: Proceedings of the
  Fourteenth Workshop on the Algorithmic Foundations of Robotics 14}, pages
  313--329. Springer International Publishing, 2021.

\bibitem[Kim et~al.(2022)Kim, Seo, Lee, Jeon, Hwang, Yang, and
  Kim]{kim2022demodice}
Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang,
  Hongseok Yang, and Kee-Eung Kim.
\newblock Demodice: Offline imitation learning with supplementary imperfect
  demonstrations.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kim et~al.(2020)Kim, Gu, Song, Zhao, and Ermon]{kim2020domain}
Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon.
\newblock Domain adaptive imitation learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5286--5295. PMLR, 2020.

\bibitem[Kostrikov et~al.(2019)Kostrikov, Nachum, and
  Tompson]{kostrikov2019imitation}
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson.
\newblock Imitation learning via off-policy distribution matching.
\newblock \emph{arXiv preprint arXiv:1912.05032}, 2019.

\bibitem[Kumar et~al.(2019)Kumar, Peng, and Levine]{kumar2019reward}
Aviral Kumar, Xue~Bin Peng, and Sergey Levine.
\newblock Reward-conditioned policies.
\newblock \emph{arXiv preprint arXiv:1912.13465}, 2019.

\bibitem[Lai et~al.(2023)Lai, Liu, Tang, Wang, Jianye, and
  Luo]{lai2023chipformer}
Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, HAO Jianye, and Ping Luo.
\newblock Chipformer: Transferable chip placement via offline decision
  transformer.
\newblock \emph{ICML}, 2023.
\newblock URL \url{https://openreview.net/pdf?id=j0miEWtw87}.

\bibitem[Lee et~al.(2021)Lee, Szot, Sun, and Lim]{lee2021generalizable}
Youngwoon Lee, Andrew Szot, Shao-Hua Sun, and Joseph~J Lim.
\newblock Generalizable imitation learning from observation via inferring goal
  proximity.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 16118--16130, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2017)Li, Song, and Ermon]{li2017infogail}
Yunzhu Li, Jiaming Song, and Stefano Ermon.
\newblock Infogail: Interpretable imitation learning from visual
  demonstrations.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Liu et~al.(2019)Liu, Ling, Mu, and Su]{liu2019state}
Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su.
\newblock State alignment-based imitation learning.
\newblock \emph{arXiv preprint arXiv:1911.10947}, 2019.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Wang, Tian, and Chen]{liu2022learn}
Jinxin Liu, Donglin Wang, Qiangxing Tian, and Zhengyu Chen.
\newblock Learn goal-conditioned policy with intrinsic motivation for deep
  reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 7558--7566, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Zhang, and Wang]{liu2022dara}
Jinxin Liu, Hongyin Zhang, and Donglin Wang.
\newblock Dara: Dynamics-aware reward augmentation in offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2203.06662}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2023)Liu, Zhang, Wei, Zhuang, Kang, Gai, and
  Wang]{liu2023beyond}
Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang, Sibo Gai, and
  Donglin Wang.
\newblock Beyond ood state actions: Supported cross-domain offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2306.12755}, 2023.

\bibitem[Liu et~al.(2020)Liu, He, Xu, and Zhang]{liu2020energy}
Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang.
\newblock Energy-based imitation learning.
\newblock \emph{arXiv preprint arXiv:2004.09395}, 2020.

\bibitem[Liu et~al.(2021)Liu, Zhao, Yang, Shen, Zhang, Zhao, and
  Liu]{liu2021curriculum}
Minghuan Liu, Hanye Zhao, Zhengyu Yang, Jian Shen, Weinan Zhang, Li~Zhao, and
  Tie-Yan Liu.
\newblock Curriculum offline imitating learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6266--6277, 2021.

\bibitem[Liu et~al.(2018)Liu, Gupta, Abbeel, and Levine]{liu2018imitation}
YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine.
\newblock Imitation from observation: Learning to imitate behaviors from raw
  video via context translation.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 1118--1125. IEEE, 2018.

\bibitem[Luo et~al.(2023)Luo, Jiang, Cohen, Grefenstette, and
  Deisenroth]{luo2023optimal}
Yicheng Luo, Zhengyao Jiang, Samuel Cohen, Edward Grefenstette, and Marc~Peter
  Deisenroth.
\newblock Optimal transport for offline imitation learning.
\newblock \emph{arXiv preprint arXiv:2303.13971}, 2023.

\bibitem[Ma et~al.(2022)Ma, Shen, Jayaraman, and Bastani]{ma2022smodice}
Yecheng~Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani.
\newblock Smodice: Versatile offline imitation learning via state occupancy
  matching.
\newblock \emph{arXiv e-prints}, pages arXiv--2202, 2022.

\bibitem[Ni et~al.(2021)Ni, Sikchi, Wang, Gupta, Lee, and Eysenbach]{ni2021f}
Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Ben
  Eysenbach.
\newblock f-irl: Inverse reinforcement learning via state marginal matching.
\newblock In \emph{Conference on Robot Learning}, pages 529--551. PMLR, 2021.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Dean~A Pomerleau.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 88--97, 1991.

\bibitem[Qiu et~al.(2023)Qiu, Wu, Cao, and Long]{qiu2023out}
Yiwen Qiu, Jialong Wu, Zhangjie Cao, and Mingsheng Long.
\newblock Out-of-dynamics imitation learning from multimodal demonstrations.
\newblock In \emph{Conference on Robot Learning}, pages 1071--1080. PMLR, 2023.

\bibitem[Raychaudhuri et~al.(2021)Raychaudhuri, Paul, Vanbaar, and
  Roy-Chowdhury]{raychaudhuri2021cross}
Dripta~S Raychaudhuri, Sujoy Paul, Jeroen Vanbaar, and Amit~K Roy-Chowdhury.
\newblock Cross-domain imitation from observations.
\newblock In \emph{International Conference on Machine Learning}, pages
  8902--8912. PMLR, 2021.

\bibitem[Reddy et~al.(2019)Reddy, Dragan, and Levine]{reddy2019sqil}
Siddharth Reddy, Anca~D Dragan, and Sergey Levine.
\newblock Sqil: Imitation learning via reinforcement learning with sparse
  rewards.
\newblock \emph{arXiv preprint arXiv:1905.11108}, 2019.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Schaal(1996)]{schaal1996learning}
Stefan Schaal.
\newblock Learning from demonstration.
\newblock \emph{Advances in neural information processing systems}, 9, 1996.

\bibitem[Srivastava et~al.(2019)Srivastava, Shyam, Mutz, Ja{\'s}kowski, and
  Schmidhuber]{srivastava2019training}
Rupesh~Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja{\'s}kowski, and
  J{\"u}rgen Schmidhuber.
\newblock Training agents using upside-down reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.02877}, 2019.

\bibitem[Torabi et~al.(2018{\natexlab{a}})Torabi, Warnell, and
  Stone]{torabi2018behavioral}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Behavioral cloning from observation.
\newblock \emph{arXiv preprint arXiv:1805.01954}, 2018{\natexlab{a}}.

\bibitem[Torabi et~al.(2018{\natexlab{b}})Torabi, Warnell, and
  Stone]{torabi2018generative}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Generative adversarial imitation from observation.
\newblock \emph{arXiv preprint arXiv:1807.06158}, 2018{\natexlab{b}}.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Viano et~al.(2022)Viano, Huang, Kamalaruban, Innes, Ramamoorthy, and
  Weller]{viano2022robust}
Luca Viano, Yu-Ting Huang, Parameswaran Kamalaruban, Craig Innes, Subramanian
  Ramamoorthy, and Adrian Weller.
\newblock Robust learning from observation with model misspecification.
\newblock \emph{arXiv preprint arXiv:2202.06003}, 2022.

\bibitem[Wang et~al.(2022)Wang, Karnwal, and Atanasov]{wang2022latent}
Tianyu Wang, Nikhil Karnwal, and Nikolay Atanasov.
\newblock Latent policies for adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:2206.11299}, 2022.

\bibitem[Xu et~al.(2022{\natexlab{a}})Xu, Zhan, Yin, and
  Qin]{xu2022discriminator}
Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin.
\newblock Discriminator-weighted offline imitation learning from suboptimal
  demonstrations.
\newblock In \emph{International Conference on Machine Learning}, pages
  24725--24742. PMLR, 2022{\natexlab{a}}.

\bibitem[Xu et~al.(2022{\natexlab{b}})Xu, Shen, Zhang, Lu, Zhao, Tenenbaum, and
  Gan]{xu2022prompting}
Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and
  Chuang Gan.
\newblock Prompting decision transformer for few-shot policy generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  24631--24645. PMLR, 2022{\natexlab{b}}.

\bibitem[Yue et~al.(2023)Yue, Wang, Shao, Zhang, Lin, Ren, and
  Zhang]{yue2023clare}
Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju~Ren, and Junshan
  Zhang.
\newblock Clare: Conservative model-based reward learning for offline inverse
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2302.04782}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Xu, Niu, Cheng, Li, Zhang, Zhou, and
  Zhan]{zhang2023discriminator}
Wenjia Zhang, Haoran Xu, Haoyi Niu, Peng Cheng, Ming Li, Heming Zhang, Guyue
  Zhou, and Xianyuan Zhan.
\newblock Discriminator-guided model-based offline imitation learning.
\newblock In \emph{Conference on Robot Learning}, pages 1266--1276. PMLR, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Yang, Loy, and Liu]{zhou2022learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0
  (9):\penalty0 2337--2348, 2022.

\bibitem[Zhu et~al.(2020)Zhu, Lin, Dai, and Zhou]{zhu2020off}
Zhuangdi Zhu, Kaixiang Lin, Bo~Dai, and Jiayu Zhou.
\newblock Off-policy imitation learning from observations.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12402--12413, 2020.

\bibitem[Zhuang et~al.(2023)Zhuang, Lei, Liu, Wang, and
  Guo]{zhuang2023behavior}
Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang Guo.
\newblock Behavior proximal policy optimization.
\newblock \emph{arXiv preprint arXiv:2302.11312}, 2023.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\bibitem[Zolna et~al.(2020)Zolna, Novikov, Konyushkova, Gulcehre, Wang, Aytar,
  Denil, de~Freitas, and Reed]{zolna2020offline}
Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu
  Wang, Yusuf Aytar, Misha Denil, Nando de~Freitas, and Scott Reed.
\newblock Offline learning from demonstrations and unlabeled experience.
\newblock \emph{arXiv preprint arXiv:2011.13885}, 2020.

\bibitem[Zolna et~al.(2021)Zolna, Reed, Novikov, Colmenarejo, Budden, Cabi,
  Denil, de~Freitas, and Wang]{zolna2021task}
Konrad Zolna, Scott Reed, Alexander Novikov, Sergio~Gomez Colmenarejo, David
  Budden, Serkan Cabi, Misha Denil, Nando de~Freitas, and Ziyu Wang.
\newblock Task-relevant adversarial imitation learning.
\newblock In \emph{Conference on Robot Learning}, pages 247--263. PMLR, 2021.

\end{thebibliography}
