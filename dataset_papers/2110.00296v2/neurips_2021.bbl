\begin{thebibliography}{10}

\bibitem{hermann2015teaching}
Karl~Moritz Hermann, Tom{\'a}{\v{s}} Ko{\v{c}}isk{\`y}, Edward Grefenstette,
  Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock {\em arXiv preprint arXiv:1506.03340}, 2015.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}, 550(7676):354--359, 2017.

\bibitem{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock {\em arXiv preprint arXiv:1912.02292}, 2019.

\bibitem{kalchbrenner2018efficient}
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande,
  Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray
  Kavukcuoglu.
\newblock Efficient neural audio synthesis.
\newblock In {\em International Conference on Machine Learning}, pages
  2410--2419. PMLR, 2018.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature communications}, 9(1):1--12, 2018.

\bibitem{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning}, pages
  2943--2952. PMLR, 2020.

\bibitem{jayakumar2020top}
Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen.
\newblock Top-kast: Top-k always sparse training.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20744--20754, 2020.

\bibitem{robins1995catastrophic}
Anthony Robins.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock {\em Connection Science}, 7(2):123--146, 1995.

\bibitem{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in cognitive sciences}, 3(4):128--135, 1999.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, page
  201611835, 2017.

\bibitem{louizos2017learning}
Christos Louizos, Max Welling, and Diederik~P Kingma.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock {\em arXiv preprint arXiv:1712.01312}, 2017.

\bibitem{Vaskevicius2019}
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini.
\newblock Implicit regularization for optimal sparse recovery.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem{zhao2019}
Peng Zhao, Yun Yang, and Qiao-Chu He.
\newblock Implicit regularization via hadamard product over-parametrization in
  high-dimensional linear regression, 2019.

\bibitem{Gunasekar2017}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{Li2018}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory}, pages
  2--47, 2018.

\bibitem{Beck2003}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock {\em Oper. Res. Lett.}, 31(3):167â€“175, May 2003.

\bibitem{Desjardins2015}
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and koray kavukcuoglu.
\newblock Natural neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~28, 2015.

\bibitem{Flennerhag2020}
Sebastian Flennerhag, Andrei~A. Rusu, Razvan Pascanu, Francesco Visin, Hujun
  Yin, and Raia Hadsell.
\newblock Meta-learning with warped gradient descent.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{lecun2012efficient}
Yann~A LeCun, L{\'e}on Bottou, Genevieve~B Orr, and Klaus-Robert M{\"u}ller.
\newblock Efficient backprop.
\newblock In {\em Neural networks: Tricks of the trade}, pages 9--48. Springer,
  2012.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem{hinton2012neural}
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
\newblock Neural networks for machine learning lecture 6a overview of
  mini-batch gradient descent.
\newblock {\em Cited on}, 14(8), 2012.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{Lecun86}
Yann LeCun.
\newblock Learning process in an asymmetric threshold network.
\newblock In {\em Disordered Systems and Biological Organization}, pages
  233--240. Springer Berlin Heidelberg, 1986.

\bibitem{Lecun87}
Yann Lecun.
\newblock {\em PhD thesis: Modeles connexionnistes de l'apprentissage
  (connectionist learning models)}.
\newblock Universite P. et M. Curie (Paris 6), June 1987.

\bibitem{perpinan14}
Miguel Carreira-Perpinan and Weiran Wang.
\newblock {Distributed optimization of deeply nested systems}.
\newblock In {\em Proceedings of the Seventeenth International Conference on
  Artificial Intelligence and Statistics}, pages 10--19, 2014.

\bibitem{LeeZBB14}
Dong{-}Hyun Lee, Saizheng Zhang, Antoine Biard, and Yoshua Bengio.
\newblock Target propagation.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Workshop Track Proceedings}, 2015.

\bibitem{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  4646--4655. PMLR, 2019.

\bibitem{lecun1989backpropagation}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard,
  Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551, 1989.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  2, 2010.

\bibitem{evci2019difficulty}
Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen.
\newblock The difficulty of training sparse neural networks.
\newblock {\em arXiv preprint arXiv:1906.10732}, 2019.

\bibitem{frankle2019lottery}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock The lottery ticket hypothesis at scale.
\newblock {\em arXiv preprint arXiv:1903.01611}, 8, 2019.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock {\em arXiv preprint arXiv:1703.04200}, 2017.

\bibitem{rolnick2018experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy~P Lillicrap, and Greg
  Wayne.
\newblock Experience replay for continual learning.
\newblock {\em arXiv preprint arXiv:1811.11682}, 2018.

\bibitem{nguyen2017variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock {\em arXiv preprint arXiv:1710.10628}, 2017.

\bibitem{titsias2019functional}
Michalis~K Titsias, Jonathan Schwarz, Alexander G de~G Matthews, Razvan
  Pascanu, and Yee~Whye Teh.
\newblock Functional regularisation for continual learning with gaussian
  processes.
\newblock {\em arXiv preprint arXiv:1901.11356}, 2019.

\bibitem{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7765--7773, 2018.

\bibitem{sokar2021spacenet}
Ghada Sokar, Decebal~Constantin Mocanu, and Mykola Pechenizkiy.
\newblock Spacenet: Make free space for continual learning.
\newblock {\em Neurocomputing}, 439:1--11, 2021.

\bibitem{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem{hsu2018re}
Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira.
\newblock Re-evaluating continual learning scenarios: A categorization and case
  for strong baselines.
\newblock {\em arXiv preprint arXiv:1810.12488}, 2018.

\bibitem{van2019three}
Gido~M Van~de Ven and Andreas~S Tolias.
\newblock Three scenarios for continual learning.
\newblock {\em arXiv preprint arXiv:1904.07734}, 2019.

\bibitem{wortsman2020supermasks}
Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad
  Rastegari, Jason Yosinski, and Ali Farhadi.
\newblock Supermasks in superposition.
\newblock {\em arXiv preprint arXiv:2006.14769}, 2020.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{lakshminarayanan2016simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock {\em arXiv preprint arXiv:1612.01474}, 2016.

\bibitem{rumelhart1985learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for
  Cognitive Science, 1985.

\bibitem{mozer1989skeletonization}
Michael~C Mozer and Paul Smolensky.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In {\em Advances in neural information processing systems}, pages
  107--115, 1989.

\bibitem{hassibi1993second}
Babak Hassibi and David~G Stork.
\newblock {\em Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem{thimm1995evaluating}
Georg Thimm and Emile Fiesler.
\newblock Evaluating pruning methods.
\newblock In {\em Proceedings of the International Symposium on Artificial
  neural networks}, pages 20--25. Citeseer, 1995.

\bibitem{strom1997sparse}
Nikko Str{\"o}m.
\newblock Sparse connection and pruning in large dynamic artificial neural
  networks.
\newblock In {\em Fifth European Conference on Speech Communication and
  Technology}, 1997.

\bibitem{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2498--2507. PMLR, 2017.

\bibitem{kusupati2020soft}
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek
  Jain, Sham Kakade, and Ali Farhadi.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In {\em International Conference on Machine Learning}, pages
  5544--5555. PMLR, 2020.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock {\em arXiv preprint arXiv:1810.02340}, 2018.

\bibitem{bellec2017deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock {\em arXiv preprint arXiv:1711.05136}, 2017.

\bibitem{amid2020reparameterizing}
Ehsan Amid and Manfred~K Warmuth.
\newblock Reparameterizing mirror descent as gradient descent.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.
\newblock Version 1 of the article (\url{https://arxiv.org/abs/2002.10487v1})
  contains experiments with the last layer of a neural network.

\bibitem{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock {\em arXiv preprint arXiv:1705.08690}, 2017.

\bibitem{chaudhry2018efficient}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock {\em arXiv preprint arXiv:1812.00420}, 2018.

\bibitem{kaplanis2020continual}
Christos Kaplanis, Claudia Clopath, and Murray Shanahan.
\newblock Continual reinforcement learning with multi-timescale replay.
\newblock {\em arXiv preprint arXiv:2004.07530}, 2020.

\bibitem{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{schwarz2018progress}
Jonathan Schwarz, Jelena Luketina, Wojciech~M Czarnecki, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock {\em arXiv preprint arXiv:1805.06370}, 2018.

\bibitem{fernando2017pathnet}
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha,
  Andrei~A Rusu, Alexander Pritzel, and Daan Wierstra.
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock {\em arXiv preprint arXiv:1701.08734}, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{eskin2004laplace}
Eleazar Eskin, Alex~J Smola, and SVN Vishwanathan.
\newblock Laplace propagation.
\newblock In {\em Advances in neural information processing systems}, pages
  441--448, 2004.

\bibitem{goodfellow2013empirical}
Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock {\em arXiv preprint arXiv:1312.6211}, 2013.

\bibitem{ritter2018online}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock Online structured laplace approximations for overcoming catastrophic
  forgetting.
\newblock {\em arXiv preprint arXiv:1805.07810}, 2018.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{wolczyk2021world}
Maciej Wolczyk, Michal Zajac, Razvan Pascanu, Lukasz Kucinski, and Piotr Milos.
\newblock Continual world: A robotic benchmark for continual reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2105.10919}, 2021.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem{milan2016forget}
Kieran Milan, Joel Veness, James Kirkpatrick, Michael Bowling, Anna Koop, and
  Demis Hassabis.
\newblock The forget-me-not process.
\newblock {\em Advances in Neural Information Processing Systems},
  29:3702--3710, 2016.

\bibitem{adams2007bayesian}
Ryan~Prescott Adams and David~JC MacKay.
\newblock Bayesian online changepoint detection.
\newblock {\em arXiv preprint arXiv:0710.3742}, 2007.

\bibitem{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154, 2018.

\bibitem{saha2021gradient}
Gobinda Saha, Isha Garg, and Kaushik Roy.
\newblock Gradient projection memory for continual learning.
\newblock {\em arXiv preprint arXiv:2103.09762}, 2021.

\bibitem{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2017.

\bibitem{lopez2017gradient}
David Lopez-Paz et~al.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6470--6479, 2017.

\bibitem{van2018generative}
Gido~M Van~de Ven and Andreas~S Tolias.
\newblock Generative replay with feedback connections as a general strategy for
  continual learning.
\newblock {\em arXiv preprint arXiv:1809.10635}, 2018.

\bibitem{zeng2019continual}
Guanxiong Zeng, Yang Chen, Bo~Cui, and Shan Yu.
\newblock Continual learning of context-dependent processing in neural
  networks.
\newblock {\em Nature Machine Intelligence}, 1(8):364--372, 2019.

\bibitem{serra2018overcoming}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In {\em International Conference on Machine Learning}, pages
  4548--4557. PMLR, 2018.

\bibitem{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{yu2020meta}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In {\em Conference on Robot Learning}, pages 1094--1100. PMLR, 2020.

\end{thebibliography}
