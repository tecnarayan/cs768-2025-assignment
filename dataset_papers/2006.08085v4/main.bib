@article{carmon2019lower,
  title={Lower bounds for finding stationary points i},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  pages={1--50},
  year={2019},
  publisher={Springer}
}

@article{arjevani2019lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={arXiv preprint arXiv:1912.02365},
  year={2019}
}

@inproceedings{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, C{\'e}dric},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5973--5983},
  year={2018}
}

@inproceedings{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1299--1309},
  year={2018}
}

@inproceedings{colin2019theoretical,
  title={Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning},
  author={Colin, Igor and Dos Santos, Ludovic and Scaman, Kevin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12350--12359},
  year={2019}
}

@article{carmon2017lower,
  title={Lower Bounds for Finding Stationary Points II: First-Order Methods},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={arXiv preprint arXiv:1711.00841},
  year={2017}
}

@article{zhou2019lower,
  title={Lower bounds for smooth nonconvex finite-sum optimization},
  author={Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1901.11224},
  year={2019}
}

@article{yu2018distributed,
  title={Distributed learning over unreliable networks},
  author={Yu, Chen and Tang, Hanlin and Renggli, Cedric and Kassing, Simon and Singla, Ankit and Alistarh, Dan and Zhang, Ce and Liu, Ji},
  journal={arXiv preprint arXiv:1810.07766},
  year={2018}
}

@inproceedings{wang2018atomo,
  title={Atomo: Communication-efficient learning via atomic sparsification},
  author={Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9850--9861},
  year={2018}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}

@inproceedings{wen2017terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in neural information processing systems},
  pages={1509--1519},
  year={2017}
}

@article{hendrickx2014graph,
  title={Graph diameter, eigenvalues, and minimum-time consensus},
  author={Hendrickx, Julien M and Jungers, Rapha{\"e}l M and Olshevsky, Alexander and Vankeerberghen, Guillaume},
  journal={Automatica},
  volume={50},
  number={2},
  pages={635--640},
  year={2014},
  publisher={Elsevier}
}

@techreport{georgopoulos2011definitive,
  title={Definitive consensus for distributed data inference},
  author={Georgopoulos, Leonidas},
  year={2011},
  institution={EPFL}
}

@inproceedings{scaman2018optimal,
  title={Optimal algorithms for non-smooth distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2740--2749},
  year={2018}
}

@inproceedings{seaman2017optimal,
  title={Optimal algorithms for smooth and strongly convex distributed optimization in networks},
  author={Seaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3027--3036},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5330--5340},
  year={2017}
}

@article{tang2018d,
  title={D2: Decentralized Training over Decentralized Data},
  author={Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
  journal={arXiv preprint arXiv:1803.07068},
  year={2018}
}

@inproceedings{tang2018communication,
  title={Communication compression for decentralized training},
  author={Tang, Hanlin and Gan, Shaoduo and Zhang, Ce and Zhang, Tong and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7652--7662},
  year={2018}
}

@article{lian2017asynchronous,
  title={Asynchronous decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
  journal={arXiv preprint arXiv:1710.06952},
  year={2017}
}

@article{lu2020mixml,
  title={MixML: A Unified Analysis of Weakly Consistent Parallel Learning},
  author={Lu, Yucheng and Nash, Jack and De Sa, Christopher},
  journal={arXiv preprint arXiv:2005.06706},
  year={2020}
}

@inproceedings{alistarh2018brief,
  title={A Brief Tutorial on Distributed and Concurrent Machine Learning},
  author={Alistarh, Dan},
  booktitle={Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing},
  pages={487--488},
  year={2018}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@book{gropp1999using,
  title={Using MPI-2: Advanced features of the message passing interface},
  author={Gropp, William and Thakur, Rajeev and Lusk, Ewing},
  year={1999},
  publisher={MIT press}
}

@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}

@inproceedings{li2014communication,
  title={Communication efficient distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={19--27},
  year={2014}
}

@inproceedings{ho2013more,
  title={More effective distributed ml via a stale synchronous parallel parameter server},
  author={Ho, Qirong and Cipar, James and Cui, Henggang and Lee, Seunghak and Kim, Jin Kyu and Gibbons, Phillip B and Gibson, Garth A and Ganger, Greg and Xing, Eric P},
  booktitle={Advances in neural information processing systems},
  pages={1223--1231},
  year={2013}
}

@article{alistarh2020elastic,
  title={Elastic Consistency: A General Consistency Model for Distributed Stochastic Gradient Descent},
  author={Alistarh, Dan and Chatterjee, Bapi and Kungurtsev, Vyacheslav},
  journal={arXiv preprint arXiv:2001.05918},
  year={2020}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

@article{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1901.09847},
  year={2019}
}

@inproceedings{recht2011hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in neural information processing systems},
  pages={693--701},
  year={2011}
}

@inproceedings{de2015taming,
  title={Taming the wild: A unified analysis of hogwild-style algorithms},
  author={De Sa, Christopher M and Zhang, Ce and Olukotun, Kunle and R{\'e}, Christopher},
  booktitle={Advances in neural information processing systems},
  pages={2674--2682},
  year={2015}
}

@inproceedings{lian2015asynchronous,
  title={Asynchronous parallel stochastic gradient for nonconvex optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2737--2745},
  year={2015}
}

@article{patarasuk2009bandwidth,
  title={Bandwidth optimal all-reduce algorithms for clusters of workstations},
  author={Patarasuk, Pitch and Yuan, Xin},
  journal={Journal of Parallel and Distributed Computing},
  volume={69},
  number={2},
  pages={117--124},
  year={2009},
  publisher={Elsevier}
}

@article{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Lian, Xiangru and Yu, Chen and Zhang, Tong and Liu, Ji},
  journal={arXiv preprint arXiv:1905.05957},
  year={2019}
}

@article{koloskova2019decentralized,
  title={Decentralized stochastic optimization and gossip algorithms with compressed communication},
  author={Koloskova, Anastasia and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1902.00340},
  year={2019}
}

@article{koloskova2019decentralized2,
  title={Decentralized deep learning with arbitrary communication compression},
  author={Koloskova, Anastasia and Lin, Tao and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1907.09356},
  year={2019}
}

@article{lu2020moniqua,
  title={Moniqua: Modulo Quantized Communication in Decentralized SGD},
  author={Lu, Yucheng and De Sa, Christopher},
  journal={arXiv preprint arXiv:2002.11787},
  year={2020}
}

@article{tang2019texttt,
  title={DeepSqueeze: Parallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression},
  author={Tang, Hanlin and Lian, Xiangru and Qiu, Shuang and Yuan, Lei and Zhang, Ce and Zhang, Tong and Liu, Ji},
  journal={arXiv preprint arXiv:1907.07346},
  year={2019}
}

@article{koloskova2020unified,
  title={A Unified Theory of Decentralized SGD with Changing Topology and Local Updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2003.10422},
  year={2020}
}

@article{mcmahan2016communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and others},
  journal={arXiv preprint arXiv:1602.05629},
  year={2016}
}

@inproceedings{shanthamallu2017brief,
  title={A brief survey of machine learning methods and their sensor and IoT applications},
  author={Shanthamallu, Uday Shankar and Spanias, Andreas and Tepedelenlioglu, Cihan and Stanley, Mike},
  booktitle={2017 8th International Conference on Information, Intelligence, Systems \& Applications (IISA)},
  pages={1--8},
  year={2017},
  organization={IEEE}
}

@inproceedings{kanawaday2017machine,
  title={Machine learning for predictive maintenance of industrial machines using IoT sensor data},
  author={Kanawaday, Ameeth and Sane, Aditya},
  booktitle={2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)},
  pages={87--90},
  year={2017},
  organization={IEEE}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ward2018adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={arXiv preprint arXiv:1806.01811},
  year={2018}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k\^{} 2)},
  author={Nesterov, Yurii},
  booktitle={Doklady an ussr},
  volume={269},
  pages={543--547},
  year={1983}
}

@article{boyd2006randomized,
  title={Randomized gossip algorithms},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  journal={IEEE transactions on information theory},
  volume={52},
  number={6},
  pages={2508--2530},
  year={2006},
  publisher={IEEE}
}

@inproceedings{boyd2005gossip,
  title={Gossip algorithms: Design, analysis and applications},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  booktitle={Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.},
  volume={3},
  pages={1653--1664},
  year={2005},
  organization={IEEE}
}

@article{nazari2019dadam,
  title={Dadam: A consensus-based distributed adaptive gradient method for online optimization},
  author={Nazari, Parvin and Tarzanagh, Davoud Ataee and Michailidis, George},
  journal={arXiv preprint arXiv:1901.09109},
  year={2019}
}

@article{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  journal={arXiv preprint arXiv:1905.03817},
  year={2019}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@phdthesis{ko2010matrix,
  title={On matrix factorization and scheduling for finite-time average-consensus},
  author={Ko, Chih-Kai},
  year={2010},
  school={California Institute of Technology}
}

@inproceedings{he2018cola,
  title={Cola: Decentralized linear learning},
  author={He, Lie and Bian, An and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4536--4546},
  year={2018}
}

@article{assran2018stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Michael},
  journal={arXiv preprint arXiv:1811.10792},
  year={2018}
}

@article{zhang2019asynchronous,
  title={Asynchronous decentralized optimization in directed networks},
  author={Zhang, Jiaqi and You, Keyou},
  journal={arXiv preprint arXiv:1901.08215},
  year={2019}
}

@article{wang2018cooperative,
  title={Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={arXiv preprint arXiv:1808.07576},
  year={2018}
}

@article{wang2019matcha,
  title={MATCHA: Speeding Up Decentralized SGD via Matching Decomposition Sampling},
  author={Wang, Jianyu and Sahu, Anit Kumar and Yang, Zhouyi and Joshi, Gauri and Kar, Soummya},
  journal={arXiv preprint arXiv:1905.09435},
  year={2019}
}

@article{agarwal2014lower,
  title={A lower bound for the optimization of finite sums},
  author={Agarwal, Alekh and Bottou, Leon},
  journal={arXiv preprint arXiv:1410.0723},
  year={2014}
}

@inproceedings{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  booktitle={Advances in neural information processing systems},
  pages={1756--1764},
  year={2015}
}

@article{lan2018optimal,
  title={An optimal randomized incremental gradient method},
  author={Lan, Guanghui and Zhou, Yi},
  journal={Mathematical programming},
  volume={171},
  number={1-2},
  pages={167--215},
  year={2018},
  publisher={Springer}
}

@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={689--699},
  year={2018}
}

@inproceedings{woodworth2018graph,
  title={Graph oracle models, lower bounds, and gaps for parallel stochastic optimization},
  author={Woodworth, Blake E and Wang, Jialei and Smith, Adam and McMahan, Brendan and Srebro, Nati},
  booktitle={Advances in neural information processing systems},
  pages={8496--8506},
  year={2018}
}

@inproceedings{allen2018make,
  title={How to make the gradients small stochastically: Even faster convex and nonconvex sgd},
  author={Allen-Zhu, Zeyuan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1157--1167},
  year={2018}
}

@inproceedings{arjevani2017oracle,
  title={Oracle complexity of second-order methods for finite-sum problems},
  author={Arjevani, Yossi and Shamir, Ohad},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={205--213},
  year={2017},
  organization={JMLR. org}
}

@article{foster2019complexity,
  title={The complexity of making the gradient small in stochastic convex optimization},
  author={Foster, Dylan and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  journal={arXiv preprint arXiv:1902.04686},
  year={2019}
}

@article{balkanski2018parallelization,
  title={Parallelization does not accelerate convex optimization: Adaptivity lower bounds for non-smooth convex minimization},
  author={Balkanski, Eric and Singer, Yaron},
  journal={arXiv preprint arXiv:1808.03880},
  year={2018}
}

@article{tran2019adaptive,
  title={An adaptive primal-dual framework for nonsmooth convex minimization},
  author={Tran-Dinh, Quoc and Alacaoglu, Ahmet and Fercoq, Olivier and Cevher, Volkan},
  journal={Mathematical Programming Computation},
  pages={1--41},
  year={2019},
  publisher={Springer}
}

@article{diakonikolas2018lower,
  title={Lower bounds for parallel and randomized convex optimization},
  author={Diakonikolas, Jelena and Guzm{\'a}n, Crist{\'o}bal},
  journal={arXiv preprint arXiv:1811.01903},
  year={2018}
}

@article{dvinskikh2019decentralized,
  title={Decentralized and parallelized primal and dual accelerated methods for stochastic convex programming problems},
  author={Dvinskikh, Darina and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:1904.09015},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{sun2019distributed,
  title={Distributed non-convex first-order optimization and information processing: Lower complexity bounds and rate optimal algorithms},
  author={Sun, Haoran and Hong, Mingyi},
  journal={IEEE Transactions on Signal processing},
  volume={67},
  number={22},
  pages={5912--5928},
  year={2019},
  publisher={IEEE}
}

@inproceedings{huang2016deep,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European conference on computer vision},
  pages={646--661},
  year={2016},
  organization={Springer}
}

@book{brooks2011handbook,
  title={Handbook of markov chain monte carlo},
  author={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year={2011},
  publisher={CRC press}
}

@article{gerencser2011markov,
  title={Markov chain mixing time on cycles},
  author={Gerencs{\'e}r, Bal{\'a}zs},
  journal={Stochastic processes and their applications},
  volume={121},
  number={11},
  pages={2553--2570},
  year={2011},
  publisher={Elsevier}
}

@article{zhang2019decentralized,
  title={Decentralized stochastic gradient tracking for empirical risk minimization},
  author={Zhang, Jiaqi and You, Keyou},
  journal={arXiv preprint arXiv:1909.02712},
  year={2019}
}

@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}

@inproceedings{wan2020rat,
  title={RAT-Resilient Allreduce Tree for Distributed Machine Learning},
  author={Wan, Xinchen and Zhang, Hong and Wang, Hao and Hu, Shuihai and Zhang, Junxue and Chen, Kai},
  booktitle={4th Asia-Pacific Workshop on Networking},
  pages={52--57},
  year={2020}
}

@article{berthier2020accelerated,
  title={Accelerated Gossip in Networks of Given Dimension using Jacobi Polynomial Iterations},
  author={Berthier, Rapha{\"e}l and Bach, Francis and Gaillard, Pierre},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={1},
  pages={24--47},
  year={2020},
  publisher={SIAM}
}

@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}

@article{liu2011accelerated,
  title={Accelerated linear iterations for distributed averaging},
  author={Liu, Ji and Morse, A Stephen},
  journal={Annual Reviews in Control},
  volume={35},
  number={2},
  pages={160--165},
  year={2011},
  publisher={Elsevier}
}

@article{shi2015extra,
  title={Extra: An exact first-order algorithm for decentralized consensus optimization},
  author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={2},
  pages={944--966},
  year={2015},
  publisher={SIAM}
}

@article{xin2019variance,
  title={Variance-reduced decentralized stochastic optimization with gradient tracking},
  author={Xin, Ran and Khan, Usman A and Kar, Soummya},
  journal={arXiv preprint arXiv:1909.11774},
  year={2019}
}

@article{lin2018don,
  title={Don't Use Large Mini-Batches, Use Local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}

@article{bonawitz2019towards,
  title={Towards federated learning at scale: System design},
  author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Kone{\v{c}}n{\`y}, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and others},
  journal={arXiv preprint arXiv:1902.01046},
  year={2019}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{yang2019federated,
  title={Federated learning},
  author={Yang, Qiang and Liu, Yang and Cheng, Yong and Kang, Yan and Chen, Tianjian and Yu, Han},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={13},
  number={3},
  pages={1--207},
  year={2019},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{tran2019federated,
  title={Federated learning over wireless networks: Optimization model design and analysis},
  author={Tran, Nguyen H and Bao, Wei and Zomaya, Albert and Nguyen, Minh NH and Hong, Choong Seon},
  booktitle={IEEE INFOCOM 2019-IEEE Conference on Computer Communications},
  pages={1387--1395},
  year={2019},
  organization={IEEE}
}

@article{ye2020multi,
  title={Multi-consensus decentralized accelerated gradient descent},
  author={Ye, Haishan and Luo, Luo and Zhou, Ziang and Zhang, Tong},
  journal={arXiv preprint arXiv:2005.00797},
  year={2020}
}

@article{xin2021improved,
  title={An improved convergence analysis for decentralized online stochastic non-convex optimization},
  author={Xin, Ran and Khan, Usman A and Kar, Soummya},
  journal={IEEE Transactions on Signal Processing},
  volume={69},
  pages={1842--1858},
  year={2021},
  publisher={IEEE}
}

@inproceedings{assran2019stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  booktitle={International Conference on Machine Learning},
  pages={344--353},
  year={2019},
  organization={PMLR}
}

@inproceedings{scaman2017optimal,
  title={Optimal algorithms for smooth and strongly convex distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={international conference on machine learning},
  pages={3027--3036},
  year={2017},
  organization={PMLR}
}

@article{liu2021distributed,
  title={Distributed learning systems with first-order methods},
  author={Liu, Ji and Zhang, Ce},
  journal={arXiv preprint arXiv:2104.05245},
  year={2021}
}

@article{tian2020achieving,
  title={Achieving Linear Convergence in Distributed Asynchronous Multiagent Optimization},
  author={Tian, Ye and Sun, Ying and Scutari, Gesualdo},
  journal={IEEE Transactions on Automatic Control},
  volume={65},
  number={12},
  pages={5264--5279},
  year={2020},
  publisher={IEEE}
}

@article{zhang2019asyspa,
  title={AsySPA: An exact asynchronous algorithm for convex optimization over digraphs},
  author={Zhang, Jiaqi and You, Keyou},
  journal={IEEE Transactions on Automatic Control},
  volume={65},
  number={6},
  pages={2494--2509},
  year={2019},
  publisher={IEEE}
}

@article{hendrikx2019asynchronous,
  title={Asynchronous accelerated proximal stochastic gradient for strongly convex distributed finite sums},
  author={Hendrikx, Hadrien and Bach, Francis and Massouli{\'e}, Laurent},
  journal={arXiv preprint arXiv:1901.09865},
  year={2019}
}

@article{nedic2009distributed,
  title={Distributed subgradient methods for multi-agent optimization},
  author={Nedic, Angelia and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  volume={54},
  number={1},
  pages={48--61},
  year={2009},
  publisher={IEEE}
}

@inproceedings{duchi2010distributed,
  title={Distributed Dual Averaging In Networks.},
  author={Duchi, John C and Agarwal, Alekh and Wainwright, Martin J},
  booktitle={NIPS},
  pages={550--558},
  year={2010},
  organization={Citeseer}
}

@article{xin2021hybrid,
  title={A hybrid variance-reduced method for decentralized stochastic non-convex optimization},
  author={Xin, Ran and Khan, Usman A and Kar, Soummya},
  journal={arXiv preprint arXiv:2102.06752},
  year={2021}
}