\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ashukha et~al.(2020)Ashukha, Lyzhov, Molchanov, and
  Vetrov]{ashukha2020pitfalls}
Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D.~P.
\newblock Pitfalls of in-domain uncertainty estimation and ensembling in deep
  learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[D'Angelo \& Fortuin(2021)D'Angelo and Fortuin]{d2021repulsive}
D'Angelo, F. and Fortuin, V.
\newblock Repulsive deep ensembles are bayesian.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)}, 2021.

\bibitem[Du et~al.(2020)Du, You, Li, Wu, Wang, Qian, and Zhang]{du2020agree}
Du, S., You, S., Li, X., Wu, J., Wang, F., Qian, C., and Zhang, C.
\newblock Agree to disagree: Adaptive ensemble knowledge distillation in
  gradient space.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)}, 2020.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of The 34th International Conference on Machine
  Learning (ICML 2017)}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G.~E., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{Deep Learning and Representation Learning Workshop, NIPS
  2014}, 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Proceedings of the 34th Conference on Uncertainty in
  Artificial Intelligence (UAI 2018)}, 2018.

\bibitem[Jin et~al.(2019)Jin, Peng, Wu, Liu, Liu, Liang, Yan, and
  Hu]{jin2019knowledge}
Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J., and Hu, X.
\newblock Knowledge distillation via route constrained optimization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 2019.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock In \emph{Citeseer}, 2009.

\bibitem[Krogh \& Hertz(1991)Krogh and Hertz]{krogh1991simple}
Krogh, A. and Hertz, J.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Advances in Neural Information Processing Systems 4 (NIPS
  1991)}, 1991.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems 30 (NIPS
  2017)}, 2017.

\bibitem[Malinin et~al.(2020)Malinin, Mlodozeniec, and
  Gales]{malinin2019ensemble}
Malinin, A., Mlodozeniec, B., and Gales, M.
\newblock Ensemble distribution distillation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Mariet et~al.(2021)Mariet, Jenatton, Wenzel, and
  Tran]{mariet2021distilling}
Mariet, Z.~E., Jenatton, R., Wenzel, F., and Tran, D.
\newblock Distilling ensembles improves uncertainty estimates.
\newblock In \emph{Third Symposium on Advances in Approximate Bayesian
  Inference}, 2021.

\bibitem[Mirzadeh et~al.(2020)Mirzadeh, Farajtabar, Li, Levine, Matsukawa, and
  Ghasemzadeh]{mirzadeh2020improved}
Mirzadeh, S.~I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., and
  Ghasemzadeh, H.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem[Nam et~al.(2021)Nam, Yoon, Lee, and Lee]{nam2021diversity}
Nam, G., Yoon, J., Lee, Y., and Lee, J.
\newblock Diversity matters when learning from ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)}, 2021.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019trust}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J.~V., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS 2019)}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS 2019)}, 2019.

\bibitem[Rame \& Cord(2021)Rame and Cord]{rame2021dice}
Rame, A. and Cord, M.
\newblock Dice: Diversity in deep ensembles via conditional redundancy
  adversarial estimation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 2015.

\bibitem[Ryabinin et~al.(2021)Ryabinin, Malinin, and
  Gales]{ryabinin2021scaling}
Ryabinin, M., Malinin, A., and Gales, M.
\newblock Scaling ensemble distribution distillation to many classes with proxy
  targets.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)}, 2021.

\bibitem[Srinivas \& Fleuret(2018)Srinivas and Fleuret]{srinivas2018jacobian}
Srinivas, S. and Fleuret, F.
\newblock Knowledge transfer with jacobian matching.
\newblock In \emph{Proceedings of The 35th International Conference on Machine
  Learning (ICML 2018)}, 2018.

\bibitem[Tashiro et~al.(2020)Tashiro, Song, and Ermon]{tashiro2020diversity}
Tashiro, Y., Song, Y., and Ermon, S.
\newblock Diversity can be transferred: Output diversification for white- and
  black-box attacks.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)}, 2020.

\bibitem[Tran et~al.(2020)Tran, Veeling, Roth, Swiatkowski, Dillon, Snoek,
  Mandt, Salimans, Nowozin, and Jenatton]{tran2020hydra}
Tran, L., Veeling, B.~S., Roth, K., Swiatkowski, J., Dillon, J.~V., Snoek, J.,
  Mandt, S., Salimans, T., Nowozin, S., and Jenatton, R.
\newblock Hydra: Preserving ensemble diversity for model distillation.
\newblock \emph{arXiv:2001.04694}, 2020.

\bibitem[Wen et~al.(2020)Wen, Tran, and Ba]{wen2019batchensemble}
Wen, Y., Tran, D., and Ba, J.
\newblock Batchensemble: an alternative approach to efficient ensemble and
  lifelong learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Wenzel et~al.(2020)Wenzel, Snoek, Tran, and
  Jenatton]{wenzel2020hyperparameter}
Wenzel, F., Snoek, J., Tran, D., and Jenatton, R.
\newblock Hyperparameter ensembles for robustness and uncertainty
  quantification.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)}, 2020.

\bibitem[Wortsman et~al.(2021)Wortsman, Horton, Guestrin, Farhadi, and
  Rastegari]{wortsman2021learning}
Wortsman, M., Horton, M., Guestrin, C., Farhadi, A., and Rastegari, M.
\newblock Learning neural network subspaces.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)}, 2021.

\bibitem[Yu et~al.(2015)Yu, Seff, Zhang, Song, Funkhouser, and
  Xiao]{yu2015lsun}
Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J.
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop.
\newblock \emph{arXiv preprint arXiv:1506.03365}, 2015.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{zagoruyko2016wrn}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{Proceedings of the British Machine Vision Conference
  (BMVC)}, 2016.

\bibitem[Zaidi et~al.(2021)Zaidi, Zela, Elsken, Holmes, Hutter, and
  Teh]{zaidi2021neural}
Zaidi, S., Zela, A., Elsken, T., Holmes, C.~C., Hutter, F., and Teh, Y.
\newblock Neural ensemble search for uncertainty estimation and dataset shift.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)}, 2021.

\end{thebibliography}
