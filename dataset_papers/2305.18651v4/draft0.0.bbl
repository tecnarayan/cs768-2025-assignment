\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[A.~Saha(2020)]{Hidden-trigger}
A.~Saha, A.~Subramanya, H.~P.
\newblock Hidden trigger backdoor attacks.
\newblock In \emph{{AAAI Conference on Artificial Intelligence (AAAI)}}, 2020.

\bibitem[Borgatti \& Everett(2000)Borgatti and Everett]{core_periphery}
Borgatti, S.~P. and Everett, M.~G.
\newblock Models of core/periphery structures.
\newblock \emph{Social Networks}, 2000.

\bibitem[Chen et~al.(2018)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{AC}
Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T.,
  Molloy, I., and Srivastava, B.
\newblock {Detecting backdoor attacks on deep neural networks by activation
  clustering}.
\newblock http://arxiv.org/abs/1811.03728, Nov 2018.

\bibitem[Chen et~al.(2019)Chen, Fu, Zhao, and Koushanfar]{DeepInspect}
Chen, H., Fu, C., Zhao, J., and Koushanfar, F.
\newblock Deepinspect: A black-box trojan detection and mitigation framework
  for deep neural networks.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, pp.\  4658--4664, 7 2019.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{Targeted}
Chen, X., Liu, C., Li, B., Lu, K., and Song, D.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock https://arxiv.org/abs/1712.05526v1, 2017.

\bibitem[Chen et~al.(2021)Chen, Salem, Chen, Backes, Ma, Shen, Wu, and
  Zhang]{badnl}
Chen, X., Salem, A., Chen, D., Backes, M., Ma, S., Shen, Q., Wu, Z., and Zhang,
  Y.
\newblock Badnl: Backdoor attacks against nlp models with semantic-preserving
  improvements.
\newblock In \emph{Annual Computer Security Applications Conference (ACSAC)},
  pp.\  554--569, 2021.

\bibitem[Chou et~al.(2020)Chou, Tram{\`{e}}r, Pellegrino, and Boneh]{SentiNet}
Chou, E., Tram{\`{e}}r, F., Pellegrino, G., and Boneh, D.
\newblock Sentinet: Detecting localized universal attacks against deep learning
  systems.
\newblock In \emph{2020 IEEE Security and Privacy Workshops (SPW)}, pp.\
  48--54. IEEE, 2020.

\bibitem[D.~P.~Kingma(2015)]{Adam}
D.~P.~Kingma, J.~B.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Li]{ImageNet}
Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  248--255, 2009.

\bibitem[Devroye et~al.(1996)Devroye, Györfi, and Lugosi]{bayes}
Devroye, L., Györfi, L., and Lugosi, G.
\newblock \emph{A Probabilistic Theory of Pattern Recognition}.
\newblock Springer, 1996.

\bibitem[Doan et~al.(2020)Doan, Abbasnejad, and C.Ranasinghe]{Februus}
Doan, B.~G., Abbasnejad, E., and C.Ranasinghe, D.
\newblock Februus: Input purification defense against trojan attacks on deep
  neural network systems.
\newblock In \emph{Annual Computer Security Applications Conference (ACSAC)},
  pp.\  897–912, 2020.

\bibitem[Dong et~al.(2021)Dong, Yang, Deng, Pang, Xiao, Su, and
  Zhu]{NC_blackbox}
Dong, Y., Yang, X., Deng, Z., Pang, T., Xiao, Z., Su, H., and Zhu, J.
\newblock Black-box detection of backdoor attacks with limited information and
  data.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, 2021.

\bibitem[Du et~al.(2020)Du, Jia, and Song]{Differential_Privacy}
Du, M., Jia, R., and Song, D.
\newblock Robust anomaly detection and backdoor attack detection via
  differential privacy.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2020.

\bibitem[Gao et~al.(2019)Gao, Xu, Wang, Chen, Ranasinghe, and Nepal]{STRIP}
Gao, Y., Xu, C., Wang, D., Chen, S., Ranasinghe, D.~C., and Nepal, S.
\newblock {STRIP:} {A} defence against trojan attacks on deep neural networks.
\newblock In \emph{{Annual Computer Security Applications Conference (ACSAC)}},
  2019.

\bibitem[{Gu} et~al.(2019){Gu}, {Liu}, {Dolan-Gavitt}, and {Garg}]{BadNet}
{Gu}, T., {Liu}, K., {Dolan-Gavitt}, B., and {Garg}, S.
\newblock Badnets: Evaluating backdooring attacks on deep neural networks.
\newblock \emph{IEEE Access}, 7:\penalty0 47230--47244, 2019.

\bibitem[Guan et~al.(2022)Guan, Tu, He, and Tao]{ShapPruning}
Guan, J., Tu, Z., He, R., and Tao, D.
\newblock Few-shot backdoor defense using shapley estimation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Guo et~al.(2019)Guo, Wang, Xing, Du, and Song]{Tabor}
Guo, W., Wang, L., Xing, X., Du, M., and Song, D.
\newblock {TABOR: A highly accurate approach to inspecting and restoring Trojan
  backdoors in AI systems}.
\newblock https://arxiv.org/abs/1908.01763, 2019.

\bibitem[Hampel(1974)]{MAD}
Hampel, F.~R.
\newblock The influence curve and its role in robust estimation.
\newblock \emph{Journal of the American Statistical Association}, 69, 1974.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{ResNet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[Hu et~al.(2022)Hu, Lin, Cogswell, Yao, Jha, and Chen]{hu2022trigger}
Hu, X., Lin, X., Cogswell, M., Yao, Y., Jha, S., and Chen, C.
\newblock Trigger hunting with a topological prior for trojan detection.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Huang et~al.(2022)Huang, Li, Wu, Qin, and Ren]{huang2022backdoor}
Huang, K., Li, Y., Wu, B., Qin, Z., and Ren, K.
\newblock Backdoor defense via decoupling the training process.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Huster \& Ekwedike(2021)Huster and Ekwedike]{top}
Huster, T. and Ekwedike, E.
\newblock {TOP:} backdoor detection in neural networks via transferability of
  perturbation, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.10274}.

\bibitem[ICLR(2022)]{TRC}
ICLR.
\newblock {IEEE Trojan Removal Competition}.
\newblock \url{https://www.trojan-removal.com/}, 2022.

\bibitem[Jia et~al.(2022)Jia, Liu, and Gong]{jia2022badencoder}
Jia, J., Liu, Y., and Gong, N.~Z.
\newblock {BadEncoder}: Backdoor attacks to pre-trained encoders in
  self-supervised learning.
\newblock In \emph{IEEE Symposium on Security and Privacy (SP)}, 2022.

\bibitem[Kolouri et~al.(2020)Kolouri, Saha, Pirsiavash, and Hoffmann]{meta_sup}
Kolouri, S., Saha, A., Pirsiavash, H., and Hoffmann, H.
\newblock {Universal litmus patterns: Revealing backdoor attacks in cnns}.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  298--307, 2020.

\bibitem[Krizhevsky(2012)]{CIFAR10}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{University of Toronto}, 05 2012.

\bibitem[Leaderboard(2018)]{GTSRB_Leaderboard}
Leaderboard.
\newblock {GTSRB Leaderboard}.
\newblock \url{https://www.kaggle.com/c/nyu-cv-fall-2018/leaderboard}, 2018.

\bibitem[{Lecun} et~al.(1998){Lecun}, {Bottou}, {Bengio}, and {Haffner}]{MNIST}
{Lecun}, Y., {Bottou}, L., {Bengio}, Y., and {Haffner}, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Liu, Dong, Zhao, Xue, Zhu, and
  Lu]{language_backdoor}
Li, S., Liu, H., Dong, T., Zhao, B.~Z., Xue, M., Zhu, H., and Lu, J.
\newblock Hidden backdoors in human-centric language models.
\newblock In \emph{ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}, pp.\  3123--3140, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Li, Wu, Li, He, and
  Lyu]{li_ISSBA_2021}
Li, Y., Li, Y., Wu, B., Li, L., He, R., and Lyu, S.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In \emph{IEEE International Conference on Computer Vision (ICCV)},
  2021{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Lyu, Koren, Lyu, Li, and Ma]{NAD}
Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., and Ma, X.
\newblock {Neural Attention Distillation: Erasing Backdoor Triggers from Deep
  Neural Networks}.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021{\natexlab{c}}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Jiang, Li, and Xia]{BAsurvey}
Li, Y., Jiang, Y., Li, Z., and Xia, S.-T.
\newblock Backdoor learning: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  pp.\  1--18, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Zhong, Ma, Jiang, and Xia]{li2022few}
Li, Y., Zhong, H., Ma, X., Jiang, Y., and Xia, S.-T.
\newblock Few-shot backdoor attacks on visual object tracking.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2018)Liu, Doan-Gavitt, and Garg]{FP}
Liu, K., Doan-Gavitt, B., and Garg, S.
\newblock {Fine-pruning: Defending against backdoor attacks on deep neural
  networks}.
\newblock In \emph{International Symposium on Research in Attacks, Intrusions,
  and Defenses (RAID)}, 2018.

\bibitem[Liu et~al.(2019)Liu, Lee, Tao, Ma, Aafer, and Zhang]{ABS}
Liu, Y., Lee, W., Tao, G., Ma, S., Aafer, Y., and Zhang, X.
\newblock {ABS: Scanning neural networks for back-doors by artificial brain
  stimulation}.
\newblock In \emph{ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}, pp.\  1265–1282, 2019.

\bibitem[Liu et~al.(2020)Liu, Ma, Bailey, and Lu]{Liu2020Refool}
Liu, Y., Ma, X., Bailey, J., and Lu, F.
\newblock {Reflection Backdoor: A Natural Backdoor Attack on Deep Neural
  Networks}.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Miller et~al.(2020)Miller, Xiang, and Kesidis]{Review}
Miller, D.~J., Xiang, Z., and Kesidis, G.
\newblock Adversarial learning in statistical classification: {A} comprehensive
  review of defenses against attacks.
\newblock \emph{Proceedings of the IEEE}, 108:\penalty0 402--433, 2020.

\bibitem[Moosavi{-}Dezfooli et~al.(2017)Moosavi{-}Dezfooli, Fawzi, and
  Frossard]{DeepFool_Univ}
Moosavi{-}Dezfooli, S.-M., Fawzi, A., and Frossard, P.
\newblock Universal adversarial perturbations.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[NeurIPS(2022)]{Competition}
NeurIPS.
\newblock {Trojan Detection Challenge NeurIPS 2022}.
\newblock \url{https://trojandetection.ai/}, 2022.

\bibitem[Nguyen \& Tran(2020)Nguyen and Tran]{nguyen2020inputaware}
Nguyen, A. and Tran, A.
\newblock Input-aware dynamic backdoor attack.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem[Nguyen \& Tran(2021)Nguyen and Tran]{nguyen2021wanet}
Nguyen, A. and Tran, A.
\newblock Wanet - imperceptible warping-based backdoor attack.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=eEn8KTtJOx}.

\bibitem[Peng et~al.(2022)Peng, Xiong, Sun, and Li]{label_smoothed}
Peng, M., Xiong, Z., Sun, M., and Li, P.
\newblock {Label-Smoothed Backdoor Attack}.
\newblock \emph{arXiv preprint arXiv:2202.11203}, 2022.

\bibitem[Rousseeuw \& Croux(1993)Rousseeuw and Croux]{rousseeuw-qn-1993}
Rousseeuw, P.~J. and Croux, C.
\newblock Alternatives to the median absolute deviation.
\newblock \emph{Journal of the American Statistical Association}, 1993.

\bibitem[Shen et~al.(2021)Shen, Liu, Tao, An, Xu, Cheng, Ma, and
  Zhang]{Shen2021BackdoorSF}
Shen, G., Liu, Y., Tao, G., An, S., Xu, Q., Cheng, S., Ma, S., and Zhang, X.
\newblock {Backdoor Scanning for Deep Neural Networks through K-Arm
  Optimization}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Stallkamp et~al.(2012)Stallkamp, Schlipsing, Salmen, and Igel]{GTSRB}
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C.
\newblock {Man vs. computer: Benchmarking machine learning algorithms for
  traffic sign recognition}.
\newblock \emph{Neural Networks}, 32:\penalty0 323--332, 2012.

\bibitem[Tao et~al.(2022)Tao, Shen, Liu, An, Xu, Ma, Li, and
  Zhang]{taog2022better}
Tao, G., Shen, G., Liu, Y., An, S., Xu, Q., Ma, S., Li, P., and Zhang, X.
\newblock Better trigger inversion optimization in backdoor scanning.
\newblock In \emph{2022 Conference on Computer Vision and Pattern Recognition
  (CVPR 2022)}, 2022.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{SS}
Tran, B., Li, J., and Madry, A.
\newblock Spectral signatures in backdoor attacks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{Clean_Label_BA}
Turner, A., Tsipras, D., and Madry, A.
\newblock Clean-label backdoor attacks.
\newblock https://people.csail.mit.edu/madry/lab/cleanlabel.pdf, 2019.

\bibitem[Wang et~al.(2019)Wang, Yao, Shan, Li, Viswanath, Zheng, and Zhao]{NC}
Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., and Zhao, B.
\newblock {Neural cleanse: Identifying and mitigating backdoor attacks in
  neural networks}.
\newblock In \emph{IEEE Symposium on Security and Privacy (SP)}, 2019.

\bibitem[Wang et~al.(2020)Wang, Zhang, Liu, Chen, Xiong, and Wang]{DataLimited}
Wang, R., Zhang, G., Liu, S., Chen, P.-Y., Xiong, J., and Wang, M.
\newblock Practical detection of trojan neural networks: Data-limited and
  data-free cases.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Wang et~al.(2022)Wang, Zhai, and Ma]{Wang_2022_CVPR}
Wang, Z., Zhai, J., and Ma, S.
\newblock Bppattack: Stealthy and efficient trojan attacks against deep neural
  networks via image quantization and contrastive adversarial learning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2022.

\bibitem[Wang et~al.(2023)Wang, Mei, Zhai, and Ma]{wang2023unicorn}
Wang, Z., Mei, K., Zhai, J., and Ma, S.
\newblock {UNICORN}: A unified backdoor trigger inversion framework.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Mj7K4lglGyj}.

\bibitem[Wu \& Wang(2021)Wu and Wang]{ANP}
Wu, D. and Wang, Y.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Xiang et~al.(2019)Xiang, Miller, and Kesidis]{CI}
Xiang, Z., Miller, D., and Kesidis, G.
\newblock {A benchmark study of backdoor data poisoning defenses for deep
  neural network classifiers and a novel defense}.
\newblock In \emph{IEEE MLSP}, Pittsburgh, 2019.

\bibitem[Xiang et~al.(2020)Xiang, Miller, and Kesidis]{Post-TNNLS}
Xiang, Z., Miller, D.~J., and Kesidis, G.
\newblock Detection of backdoors in trained classifiers without access to the
  training set.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  pp.\  1--15, 2020.

\bibitem[Xiang et~al.(2021)Xiang, Miller, and Kesidis]{L-RED}
Xiang, Z., Miller, D.~J., and Kesidis, G.
\newblock {L-RED: Efficient post-training detection of imperceptible backdoor
  attacks without access to the training set}.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  3745--3749, 2021.

\bibitem[Xiang et~al.(2022{\natexlab{a}})Xiang, Miller, and Kesidis]{TwoClass}
Xiang, Z., Miller, D., and Kesidis, G.
\newblock Post-training detection of backdoor attacks for two-class and
  multi-attack scenarios.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022{\natexlab{a}}.

\bibitem[Xiang et~al.(2022{\natexlab{b}})Xiang, Miller, Chen, Li, and
  Kesidis]{PCBD}
Xiang, Z., Miller, D.~J., Chen, S., Li, X., and Kesidis, G.
\newblock Detecting backdoor attacks against point cloud classifiers.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, 2022{\natexlab{b}}.

\bibitem[Xie et~al.(2020)Xie, Huang, Chen, and Li]{Xie2020DBA}
Xie, C., Huang, K., Chen, P., and Li, B.
\newblock Dba: Distributed backdoor attacks against federated learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Xu \& Raginsky(2022)Xu and Raginsky]{GDPI}
Xu, A. and Raginsky, M.
\newblock Minimum excess risk in bayesian learning.
\newblock \emph{{IEEE} Trans. Inf. Theory}, pp.\  7935--7955, 2022.

\bibitem[Xu et~al.(2021)Xu, Wang, Li, Borisov, Gunter, and Li]{META}
Xu, X., Wang, Q., Li, H., Borisov, N., Gunter, C., and Li, B.
\newblock {Detecting AI Trojans using meta neural analysis}.
\newblock In \emph{IEEE Symposium on Security and Privacy (SP)}, 2021.

\bibitem[Xue et~al.(2022{\natexlab{a}})Xue, He, Wang, and Liu]{xue2022O2NN2O}
Xue, M., He, C., Wang, J., and Liu, W.
\newblock {One-to-N \& N-to-One: Two Advanced Backdoor Attacks Against Deep
  Learning Models}.
\newblock \emph{IEEE Transactions on Dependable and Secure Computing},
  19\penalty0 (3):\penalty0 1562--1578, 2022{\natexlab{a}}.

\bibitem[Xue et~al.(2022{\natexlab{b}})Xue, Ni, Wu, Zhang, Wang, and
  Liu]{xue2022imperceptible}
Xue, M., Ni, S., Wu, Y., Zhang, Y., Wang, J., and Liu, W.
\newblock Imperceptible and multi-channel backdoor attack against deep neural
  networks, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2019)Yao, Li, Zheng, and Zhao]{Yao_2019_CCS}
Yao, Y., Li, H., Zheng, H., and Zhao, B.~Y.
\newblock Latent backdoor attacks on deep neural networks.
\newblock In \emph{ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}, 2019.

\bibitem[Zeng et~al.(2022)Zeng, Chen, Park, Mao, Jin, and
  Jia]{zeng2022adversarial}
Zeng, Y., Chen, S., Park, W., Mao, Z., Jin, M., and Jia, R.
\newblock Adversarial unlearning of backdoors via implicit hypergradient.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.
\newblock URL \url{https://openreview.net/forum?id=MeeQkFYVbzW}.

\bibitem[Zhao et~al.(2022)Zhao, Chen, Xuan, Dong, Wang, and
  Liang]{Zhao_2022_CVPR}
Zhao, Z., Chen, X., Xuan, Y., Dong, Y., Wang, D., and Liang, K.
\newblock Defeat: Deep hidden feature backdoor attacks by imperceptible
  perturbation and latent representation constraints.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2022.

\bibitem[Zheng et~al.(2022)Zheng, Tang, Li, and Liu]{CLP}
Zheng, R., Tang, R., Li, J., and Liu, L.
\newblock Data-free backdoor removal based on channel lipschitzness.
\newblock In \emph{ECCV}, 2022.

\bibitem[Zhong et~al.(2020)Zhong, Liao, Squicciarini, Zhu, and Miller]{Haoti}
Zhong, H., Liao, C., Squicciarini, A., Zhu, S., and Miller, D.
\newblock Backdoor embedding in convolutional neural network models via
  invisible perturbation.
\newblock In \emph{CODASPY}, 2020.

\end{thebibliography}
