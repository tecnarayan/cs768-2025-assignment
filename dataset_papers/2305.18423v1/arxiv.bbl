\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Can sgd learn recurrent neural networks with provable generalization?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Anthony et~al.(1999)Anthony, Bartlett, Bartlett,
  et~al.]{anthony1999neural}
Martin Anthony, Peter~L Bartlett, Peter~L Bartlett, et~al.
\newblock \emph{Neural network learning: Theoretical foundations}, volume~9.
\newblock cambridge university press Cambridge, 1999.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  254--263. PMLR, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bartlett(1996)]{bartlett1996valid}
Peter Bartlett.
\newblock For valid generalization the size of the weights is more important
  than the size of the network.
\newblock \emph{Advances in neural information processing systems}, 9, 1996.

\bibitem[Bartlett et~al.(1998)Bartlett, Maiorov, and Meir]{bartlett1998almost}
Peter Bartlett, Vitaly Maiorov, and Ron Meir.
\newblock Almost linear vc dimension bounds for piecewise polynomial networks.
\newblock \emph{Advances in neural information processing systems}, 11, 1998.

\bibitem[Bartlett and Maass(2003)]{bartlett2003vapnik}
Peter~L Bartlett and Wolfgang Maass.
\newblock Vapnik-chervonenkis dimension of neural nets.
\newblock \emph{The handbook of brain theory and neural networks}, pages
  1188--1192, 2003.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Peter~L Bartlett, Michael~I Jordan, and Jon~D McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
Peter~L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2285--2301, 2019.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
Peter~L Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Baum and Haussler(1988)]{baum1988size}
Eric Baum and David Haussler.
\newblock What size net gives valid generalization?
\newblock \emph{Advances in neural information processing systems}, 1, 1988.

\bibitem[Belkin et~al.(2018)Belkin, Hsu, and Mitra]{belkin2018overfitting}
Mikhail Belkin, Daniel~J Hsu, and Partha Mitra.
\newblock Overfitting or perfect fitting? risk bounds for classification and
  regression rules that interpolate.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Belkin et~al.(2019)Belkin, Rakhlin, and Tsybakov]{belkin2019does}
Mikhail Belkin, Alexander Rakhlin, and Alexandre~B Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1611--1619. PMLR, 2019.

\bibitem[Boucheron et~al.(2005)Boucheron, Bousquet, and
  Lugosi]{boucheron2005theory}
St{\'e}phane Boucheron, Olivier Bousquet, and G{\'a}bor Lugosi.
\newblock Theory of classification: A survey of some recent advances.
\newblock \emph{ESAIM: probability and statistics}, 9:\penalty0 323--375, 2005.

\bibitem[Chen et~al.(2020)Chen, Li, and Zhao]{chen2020generalization}
Minshuo Chen, Xingguo Li, and Tuo Zhao.
\newblock On generalization bounds of a family of recurrent neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1233--1243. PMLR, 2020.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Dudley(2010)]{dudley2010universal}
Richard~M Dudley.
\newblock Universal donsker classes and metric entropy.
\newblock In \emph{Selected Works of RM Dudley}, pages 345--365. Springer,
  2010.

\bibitem[Dziugaite and Roy(2017)]{DR17}
Gintare~Karolina Dziugaite and Daniel~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proceedings of the 33rd Annual Conference on Uncertainty in
  Artificial Intelligence (UAI)}, 2017.

\bibitem[Fathollah~Pour and Ashtiani(2022)]{pour2022benefits}
Alireza Fathollah~Pour and Hassan Ashtiani.
\newblock Benefits of additive noise in composing classes with bounded
  capacity.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 32709--32722, 2022.

\bibitem[Gao and Zhou(2016)]{gao2016dropout}
Wei Gao and Zhi-Hua Zhou.
\newblock Dropout rademacher complexity of deep neural networks.
\newblock \emph{Science China Information Sciences}, 59\penalty0 (7):\penalty0
  1--12, 2016.

\bibitem[Goldberg and Jerrum(1995)]{goldberg1995bounding}
Paul~W Goldberg and Mark~R Jerrum.
\newblock Bounding the vapnik-chervonenkis dimension of concept classes
  parameterized by real numbers.
\newblock \emph{Machine Learning}, 18\penalty0 (2):\penalty0 131--148, 1995.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference On Learning Theory}, pages 297--299. PMLR, 2018.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 6645--6649. Ieee, 2013.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Haghifam et~al.(2020)Haghifam, Negrea, Khisti, Roy, and
  Dziugaite]{haghifam2020sharpened}
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel~M Roy, and
  Gintare~Karolina Dziugaite.
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9925--9935, 2020.

\bibitem[Hardt et~al.(2018)Hardt, Ma, and Recht]{hardt2016gradient}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{Journal of Machine Learning Research}, 19:\penalty0 1--44,
  2018.

\bibitem[Ji and Telgarsky(2021)]{ji2021characterizing}
Ziwei Ji and Matus Telgarsky.
\newblock Characterizing the implicit bias via a primal-dual analysis.
\newblock In \emph{Algorithmic Learning Theory}, pages 772--804. PMLR, 2021.

\bibitem[Ji et~al.(2020)Ji, Dud{\'\i}k, Schapire, and
  Telgarsky]{ji2020gradient}
Ziwei Ji, Miroslav Dud{\'\i}k, Robert~E Schapire, and Matus Telgarsky.
\newblock Gradient descent follows the regularization path for general losses.
\newblock In \emph{Conference on Learning Theory}, pages 2109--2136. PMLR,
  2020.

\bibitem[Jim et~al.(1996)Jim, Giles, and Horne]{jim1996analysis}
Kam-Chuen Jim, C~Lee Giles, and Bill~G Horne.
\newblock An analysis of noise in recurrent neural networks: convergence and
  generalization.
\newblock \emph{IEEE Transactions on neural networks}, 7\penalty0 (6):\penalty0
  1424--1438, 1996.

\bibitem[Karpathy and Fei-Fei(2015)]{karpathy2015deep}
Andrej Karpathy and Li~Fei-Fei.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3128--3137, 2015.

\bibitem[Koiran and Sontag(1998)]{koiran1998vapnik}
Pascal Koiran and Eduardo~D Sontag.
\newblock Vapnik-chervonenkis dimension of recurrent neural networks.
\newblock \emph{Discrete Applied Mathematics}, 86\penalty0 (1):\penalty0
  63--79, 1998.

\bibitem[Lim et~al.(2021)Lim, Erichson, Hodgkinson, and Mahoney]{lim2021noisy}
Soon~Hoe Lim, N~Benjamin Erichson, Liam Hodgkinson, and Michael~W Mahoney.
\newblock Noisy recurrent neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Long and Sedghi(2020)]{long2020size}
Philip~M Long and Hanie Sedghi.
\newblock Size-free generalization bounds for convolutional neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Maass(1994)]{maass1994neural}
Wolfgang Maass.
\newblock Neural nets with superlinear vc-dimension.
\newblock \emph{Neural Computation}, 6\penalty0 (5):\penalty0 877--884, 1994.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Nagarajan and Kolter(2019)]{nagarajan2019uniform}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Nagarajan and Kolter(2018)]{nagarajan2018deterministic}
Vaishnavh Nagarajan and Zico Kolter.
\newblock Deterministic pac-bayesian generalization bounds for deep networks
  via generalizing noise-resilience.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Negrea et~al.(2019)Negrea, Haghifam, Dziugaite, Khisti, and
  Roy]{negrea2019information}
Jeffrey Negrea, Mahdi Haghifam, Gintare~Karolina Dziugaite, Ashish Khisti, and
  Daniel~M Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Neu et~al.(2021)Neu, Dziugaite, Haghifam, and Roy]{neu2021information}
Gergely Neu, Gintare~Karolina Dziugaite, Mahdi Haghifam, and Daniel~M Roy.
\newblock Information-theoretic generalization bounds for stochastic gradient
  descent.
\newblock In \emph{Conference on Learning Theory}, pages 3526--3545. PMLR,
  2021.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{pmlr-v40-Neyshabur15}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401. PMLR,
  2015.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Qin et~al.(2017)Qin, Song, Cheng, Cheng, Jiang, and
  Cottrell]{qin2017dual}
Yao Qin, Dongjin Song, Haifeng Cheng, Wei Cheng, Guofei Jiang, and Garrison~W
  Cottrell.
\newblock A dual-stage attention-based recurrent neural network for time series
  prediction.
\newblock In \emph{Proceedings of the 26th International Joint Conference on
  Artificial Intelligence}, pages 2627--2633, 2017.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{raginsky2017non}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703. PMLR,
  2017.

\bibitem[Russo and Zou(2016)]{russo2016controlling}
Daniel Russo and James Zou.
\newblock Controlling bias in adaptive data analysis using information theory.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1232--1240.
  PMLR, 2016.

\bibitem[Russo and Zou(2019)]{russo2019much}
Daniel Russo and James Zou.
\newblock How much does your data exploration overfit? controlling bias via
  information usage.
\newblock \emph{IEEE Transactions on Information Theory}, 66\penalty0
  (1):\penalty0 302--323, 2019.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Sontag et~al.(1998)]{sontag1998vc}
Eduardo~D Sontag et~al.
\newblock Vc dimension of neural networks.
\newblock \emph{NATO ASI Series F Computer and Systems Sciences}, 168:\penalty0
  69--96, 1998.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{JMLR:v15:srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Steinke and Zakynthinou(2020)]{steinke2020reasoning}
Thomas Steinke and Lydia Zakynthinou.
\newblock Reasoning about generalization via conditional mutual information.
\newblock In \emph{Conference on Learning Theory}, pages 3437--3452. PMLR,
  2020.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Tu et~al.(2020)Tu, He, and Tao]{Tu2020Understanding}
Zhuozhuo Tu, Fengxiang He, and Dacheng Tao.
\newblock Understanding generalization in recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Vidyasagar(1997)]{vidyasagar1997theory}
Mathukumalli Vidyasagar.
\newblock \emph{A theory of learning and generalization: with applications to
  neural networks and control systems}.
\newblock Springer-Verlag, 1997.

\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol]{vincent2008extracting}
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1096--1103, 2008.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and
  Fergus]{wan2013regularization}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann Le~Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{International conference on machine learning}, pages
  1058--1066. PMLR, 2013.

\bibitem[Wang et~al.(2019)Wang, Yang, Zhao, Luo, Wang, and Tang]{WANG2019177}
Haotian Wang, Wenjing Yang, Zhenyu Zhao, Tingjin Luo, Ji~Wang, and Yuhua Tang.
\newblock Rademacher dropout: An adaptive dropout for deep neural network via
  optimizing generalization gap.
\newblock \emph{Neurocomputing}, 357:\penalty0 177--187, 2019.

\bibitem[Xu and Raginsky(2017)]{xu2017information}
Aolin Xu and Maxim Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang et~al.(2018)Zhang, Lei, and Dhillon]{zhang2018stabilizing}
Jiong Zhang, Qi~Lei, and Inderjit Dhillon.
\newblock Stabilizing gradients for deep neural networks via efficient svd
  parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  5806--5814. PMLR, 2018.

\bibitem[Zhang(2002)]{zhang2002covering}
Tong Zhang.
\newblock Covering number bounds of certain regularized linear function
  classes.
\newblock \emph{Journal of Machine Learning Research}, 2\penalty0
  (Mar):\penalty0 527--550, 2002.

\bibitem[Zhao et~al.(2020)Zhao, Huang, Lv, Duan, Qin, Li, and
  Tian]{zhao2020rnn}
Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li, and
  Guangjian Tian.
\newblock Do rnn and lstm have long memory?
\newblock In \emph{International Conference on Machine Learning}, pages
  11365--11375. PMLR, 2020.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2019non}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P Adams, and Peter Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\end{thebibliography}
