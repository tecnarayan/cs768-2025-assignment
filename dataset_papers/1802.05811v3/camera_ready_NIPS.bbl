\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2014)Agarwal, Chapelle, Dud{\'\i}k, and
  Langford]{agarwal2014reliable}
Alekh Agarwal, Olivier Chapelle, Miroslav Dud{\'\i}k, and John Langford.
\newblock A reliable effective terascale linear learning system.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1111--1133, 2014.

\bibitem[Beygelzimer et~al.(2018)Beygelzimer, Busa-Fekete, Halawi, and
  Orabona]{anona}
Alina Beygelzimer, Robert Busa-Fekete, Guy Halawi, and Francesco Orabona.
\newblock Algorithmic notifications for mail: Debiasing mail data to predict
  user actions.
\newblock In \emph{under review}, 2018.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Cheng and Cant{\'{u}}{-}Paz(2010)]{ChengC10}
Haibin Cheng and Erick Cant{\'{u}}{-}Paz.
\newblock Personalized click prediction in sponsored search.
\newblock In \emph{Proceedings of the Third International Conference on Web
  Search and Web Data Mining, {WSDM} 2010, New York, NY, USA, February 4-6,
  2010}, pages 351--360, 2010.
\newblock \doi{10.1145/1718487.1718531}.
\newblock URL \url{http://doi.acm.org/10.1145/1718487.1718531}.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and Sridharan]{CotterSSS11}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems 24: 25th
  Annual Conference on Neural Information Processing Systems 2011. Proceedings
  of a meeting held 12-14 December 2011, Granada, Spain.}, pages 1647--1655,
  2011.

\bibitem[Cutkosky and Boahen(2017)]{cutkosky2017online}
Ashok Cutkosky and Kwabena Boahen.
\newblock Online learning without prior information.
\newblock In Satyen Kale and Ohad Shamir, editors, \emph{Proceedings of the
  2017 Conference on Learning Theory}, volume~65 of \emph{Proceedings of
  Machine Learning Research}, pages 643--677, Amsterdam, Netherlands, 07--10
  Jul 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v65/cutkosky17a.html}.

\bibitem[Dekel et~al.(2012)Dekel, Gilad{-}Bachrach, Shamir, and
  Xiao]{DekelGSX12}
Ofer Dekel, Ran Gilad{-}Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 165--202,
  2012.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and Sidford]{FrostigGKS15}
Roy Frostig, Rong Ge, Sham~M. Kakade, and Aaron Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory,
  {COLT} 2015, Paris, France, July 3-6, 2015}, pages 728--763, 2015.

\bibitem[Harikandeh et~al.(2015)Harikandeh, Ahmed, Virani, Schmidt,
  Kone{\v{c}}n{\`y}, and Sallinen]{harikandeh2015stopwasting}
Reza Harikandeh, Mohamed~Osama Ahmed, Alim Virani, Mark Schmidt, Jakub
  Kone{\v{c}}n{\`y}, and Scott Sallinen.
\newblock Stopwasting my gradients: Practical svrg.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2251--2259, 2015.

\bibitem[Johnson and Zhang(2013)]{Johnson013}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.},
  pages 315--323, 2013.

\bibitem[Lei and Jordan(2016)]{lei2016less}
Lihua Lei and Michael~I Jordan.
\newblock Less than a single pass: Stochastically controlled stochastic
  gradient method.
\newblock \emph{arXiv preprint arXiv:1609.03261}, 2016.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2345--2355, 2017.

\bibitem[Orabona(2014)]{orabona2014simultaneous}
Francesco Orabona.
\newblock Simultaneous model selection and optimization through parameter-free
  stochastic learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1116--1124, 2014.

\bibitem[Orabona and P{\'a}l(2016)]{orabona2016scale}
Francesco Orabona and D{\'a}vid P{\'a}l.
\newblock Scale-free online learning.
\newblock \emph{arXiv preprint arXiv:1601.01974}, 2016.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Sashank~J Reddi, Jakub Kone{\v{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Ross et~al.(2013)Ross, Mineiro, and Langford]{ross2013normalized}
St{\'e}phane Ross, Paul Mineiro, and John Langford.
\newblock Normalized online learning.
\newblock \emph{arXiv preprint arXiv:1305.6646}, 2013.

\bibitem[Shah et~al.(2016)Shah, Asteris, Kyrillidis, and
  Sanghavi]{shah2016trading}
Vatsal Shah, Megasthenis Asteris, Anastasios Kyrillidis, and Sujay Sanghavi.
\newblock Trading-off variance and complexity in stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1603.06861}, 2016.

\bibitem[Shalev-Shwartz(2012)]{shalev2012online}
Shai Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (2):\penalty0 107--194, 2012.

\bibitem[Shamir et~al.(2013)Shamir, Srebro, and Zhang]{ShamirS013}
Ohad Shamir, Nathan Srebro, and Tong Zhang.
\newblock Communication efficient distributed optimization using an approximate
  newton-type method.
\newblock \emph{CoRR}, abs/1312.7853, 2013.
\newblock URL \url{http://arxiv.org/abs/1312.7853}.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{WangWS17}
Jialei Wang, Weiran Wang, and Nathan Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch prox.
\newblock \emph{CoRR}, abs/1702.06269, 2017.
\newblock URL \url{http://arxiv.org/abs/1702.06269}.

\bibitem[Zhang and Xiao(2015)]{ZhangX15a}
Yuchen Zhang and Lin Xiao.
\newblock Communication-efficient distributed optimization of self-concordant
  empirical loss.
\newblock \emph{CoRR}, abs/1501.00263, 2015.
\newblock URL \url{http://arxiv.org/abs/1501.00263}.

\end{thebibliography}
