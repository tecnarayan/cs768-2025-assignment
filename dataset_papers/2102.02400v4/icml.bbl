\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, pp.\  233--242, 2017.

\bibitem[Berthon et~al.(2021)Berthon, Han, Niu, Liu, and
  Sugiyama]{berthon2020confidence}
Berthon, A., Han, B., Niu, G., Liu, T., and Sugiyama, M.
\newblock Confidence scores make instance-dependent label-noise learning
  possible.
\newblock In \emph{ICML}, 2021.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Chou et~al.(2020)Chou, Niu, Lin, and Sugiyama]{chou2020unbiased}
Chou, Y.-T., Niu, G., Lin, H.-T., and Sugiyama, M.
\newblock Unbiased risk estimators can mislead: A case study of learning with
  complementary labels.
\newblock In \emph{ICML}, pp.\  1929--1938. PMLR, 2020.

\bibitem[Daniely \& Granot(2019)Daniely and Granot]{daniely2019generalization}
Daniely, A. and Granot, E.
\newblock Generalization bounds for neural networks via approximate description
  length.
\newblock In \emph{NeurIPS}, pp.\  13008--13016, 2019.

\bibitem[Fazel et~al.(2003)Fazel, Hindi, and Boyd]{fazel2003log}
Fazel, M., Hindi, H., and Boyd, S.~P.
\newblock Log-det heuristic for matrix rank minimization with applications to
  hankel and euclidean distance matrices.
\newblock In \emph{ACC}, pp.\  2156--2162, 2003.

\bibitem[Feng et~al.(2020)Feng, Kaneko, Han, Niu, An, and
  Sugiyama]{feng2020learning}
Feng, L., Kaneko, T., Han, B., Niu, G., An, B., and Sugiyama, M.
\newblock Learning with multiple complementary labels.
\newblock In \emph{ICML}, pp.\  3072--3081. PMLR, 2020.

\bibitem[Fu et~al.(2015)Fu, Ma, Huang, and Sidiropoulos]{fu2015blind}
Fu, X., Ma, W.-K., Huang, K., and Sidiropoulos, N.~D.
\newblock Blind separation of quasi-stationary sources: Exploiting convex
  geometry in covariance domain.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (9):\penalty0 2306--2320, 2015.

\bibitem[Fu et~al.(2016)Fu, Huang, Yang, Ma, and Sidiropoulos]{fu2016robust}
Fu, X., Huang, K., Yang, B., Ma, W.-K., and Sidiropoulos, N.~D.
\newblock Robust volume minimization-based matrix factorization for remote
  sensing and document clustering.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (23):\penalty0 6254--6268, 2016.

\bibitem[Fu et~al.(2018)Fu, Huang, and Sidiropoulos]{fu2018identifiability}
Fu, X., Huang, K., and Sidiropoulos, N.~D.
\newblock On identifiability of nonnegative matrix factorization.
\newblock \emph{IEEE Signal Processing Letters}, 25\penalty0 (3):\penalty0
  328--332, 2018.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2016training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
Han, B., Yao, J., Niu, G., Zhou, M., Tsang, I., Zhang, Y., and Sugiyama, M.
\newblock Masking: A new perspective of noisy supervision.
\newblock In \emph{NeurIPS}, pp.\  5836--5846, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, pp.\  8527--8537, 2018{\natexlab{b}}.

\bibitem[Han et~al.(2020{\natexlab{a}})Han, Niu, Yu, Yao, Xu, Tsang, and
  Sugiyama]{han2020sigua}
Han, B., Niu, G., Yu, X., Yao, Q., Xu, M., Tsang, I., and Sugiyama, M.
\newblock Sigua: Forgetting may make learning with noisy labels more robust.
\newblock In \emph{ICML}, pp.\  4006--4016. PMLR, 2020{\natexlab{a}}.

\bibitem[Han et~al.(2020{\natexlab{b}})Han, Yao, Liu, Niu, Tsang, Kwok, and
  Sugiyama]{han2020survey}
Han, B., Yao, Q., Liu, T., Niu, G., Tsang, I.~W., Kwok, J.~T., and Sugiyama, M.
\newblock A survey of label-noise representation learning: Past, present and
  future, 2020{\natexlab{b}}.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock {MentorNet}: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, pp.\  2309--2318, 2018.

\bibitem[Karush(1939)]{karush1939minima}
Karush, W.
\newblock Minima of functions of several variables with inequalities as side
  constraints.
\newblock \emph{M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago},
  1939.

\bibitem[Kremer et~al.(2018)Kremer, Sha, and Igel]{kremer2018robust}
Kremer, J., Sha, F., and Igel, C.
\newblock Robust active label correction.
\newblock In \emph{AISTATS}, pp.\  308--316, 2018.

\bibitem[Kuhn \& Tucker(2014)Kuhn and Tucker]{kuhn2014nonlinear}
Kuhn, H.~W. and Tucker, A.~W.
\newblock Nonlinear programming.
\newblock In \emph{Traces and emergence of nonlinear programming}, pp.\
  247--258. Springer, 2014.

\bibitem[Li \& Bioucas-Dias(2008)Li and Bioucas-Dias]{li2008minimum}
Li, J. and Bioucas-Dias, J.~M.
\newblock Minimum volume simplex analysis: A fast algorithm to unmix
  hyperspectral data.
\newblock In \emph{IGARSS}, pp.\  III--250, 2008.

\bibitem[Li et~al.(2019)Li, Socher, and Hoi]{li2019dividemix}
Li, J., Socher, R., and Hoi, S.~C.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{ICLR}, 2019.

\bibitem[Li et~al.(2017)Li, Wang, Li, Agustsson, and Van~Gool]{li2017webvision}
Li, W., Wang, L., Li, W., Agustsson, E., and Van~Gool, L.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{arXiv preprint arXiv:1708.02862}, 2017.

\bibitem[Liu et~al.(2012)Liu, Lin, Yan, Sun, Yu, and Ma]{liu2012robust}
Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., and Ma, Y.
\newblock Robust recovery of subspace structures by low-rank representation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 35\penalty0 (1):\penalty0 171--184, 2012.

\bibitem[Liu \& Tao(2016)Liu and Tao]{liu2016classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2016.

\bibitem[Liu \& Guo(2020)Liu and Guo]{liu2020peer}
Liu, Y. and Guo, H.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock In \emph{ICML}, pp.\  6226--6236. PMLR, 2020.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Ma, X., Wang, Y., Houle, M.~E., Zhou, S., Erfani, S.~M., Xia, S.-T.,
  Wijewickrema, S., and Bailey, J.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{ICML}, pp.\  3361--3370, 2018.

\bibitem[Malach \& Shalev-Shwartz(2017)Malach and
  Shalev-Shwartz]{malach2017decoupling}
Malach, E. and Shalev-Shwartz, S.
\newblock Decoupling" when to update" from" how to update".
\newblock In \emph{NeurIPS}, pp.\  960--970, 2017.

\bibitem[Miao \& Qi(2007)Miao and Qi]{miao2007endmember}
Miao, L. and Qi, H.
\newblock Endmember extraction from highly mixed data using minimum volume
  constrained nonnegative matrix factorization.
\newblock \emph{IEEE Transactions on Geoscience and Remote Sensing},
  45\penalty0 (3):\penalty0 765--777, 2007.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{NeurIPS}, pp.\  1196--1204, 2013.

\bibitem[Nguyen et~al.(2019)Nguyen, Mummadi, Ngo, Nguyen, Beggel, and
  Brox]{nguyen2019self}
Nguyen, D.~T., Mummadi, C.~K., Ngo, T. P.~N., Nguyen, T. H.~P., Beggel, L., and
  Brox, T.
\newblock Self: Learning to filter noisy labels with self-ensembling.
\newblock In \emph{ICLR}, 2019.

\bibitem[Northcutt et~al.(2017)Northcutt, Wu, and Chuang]{northcuttlearning}
Northcutt, C.~G., Wu, T., and Chuang, I.~L.
\newblock Learning with confident examples: Rank pruning for robust
  classification with noisy labels.
\newblock In \emph{UAI}, 2017.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{CVPR}, pp.\  1944--1952, 2017.

\bibitem[Reed et~al.(2015)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S.~E., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock In \emph{ICLR, Workshop Track Proceedings}, 2015.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Ren, M., Zeng, W., Yang, B., and Urtasun, R.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, pp.\  4331--4340, 2018.

\bibitem[Scott(2015)]{scott2015rate}
Scott, C.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{AISTATS}, pp.\  838--846, 2015.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Estrach, Paluri, Bourdev, and
  Fergus]{sukhbaatar2015training}
Sukhbaatar, S., Estrach, J.~B., Paluri, M., Bourdev, L., and Fergus, R.
\newblock Training convolutional networks with noisy labels.
\newblock In \emph{ICLR}, 2015.

\bibitem[Tanaka et~al.(2018)Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018joint}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{CVPR}, pp.\  5552--5560, 2018.

\bibitem[Thekumparampil et~al.(2018)Thekumparampil, Khetan, Lin, and
  Oh]{thekumparampil2018robustness}
Thekumparampil, K.~K., Khetan, A., Lin, Z., and Oh, S.
\newblock Robustness of conditional gans to noisy labels.
\newblock In \emph{NeurIPS}, pp.\  10271--10282, 2018.

\bibitem[Wu et~al.(2021)Wu, Xia, Liu, Han, Gong, Wang, Liu, and
  Niu]{wu2020class2simi}
Wu, S., Xia, X., Liu, T., Han, B., Gong, M., Wang, N., Liu, H., and Niu, G.
\newblock Class2simi: A new perspective on learning with label noise.
\newblock In \emph{ICML}. PMLR, 2021.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{NeurIPS}, pp.\  6835--6846, 2019.

\bibitem[Xia et~al.(2020)Xia, Liu, Han, Wang, Gong, Liu, Niu, Tao, and
  Sugiyama]{xia2020part}
Xia, X., Liu, T., Han, B., Wang, N., Gong, M., Liu, H., Niu, G., Tao, D., and
  Sugiyama, M.
\newblock Part-dependent label noise: Towards instance-dependent label noise.
\newblock \emph{NerurIPS}, 2020.

\bibitem[Xia et~al.(2021)Xia, Liu, Han, Gong, Wang, Ge, and
  Chang]{xia2021robust}
Xia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., and Chang, Y.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In \emph{ICLR}, 2021.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{CVPR}, pp.\  2691--2699, 2015.

\bibitem[Xu et~al.(2019)Xu, Cao, Kong, and Wang]{xu2019l_dmi}
Xu, Y., Cao, P., Kong, Y., and Wang, Y.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock In \emph{NeurIPS}, pp.\  6222--6233, 2019.

\bibitem[Yao et~al.(2020{\natexlab{a}})Yao, Yang, Han, Niu, and
  Kwok]{yao2020searching}
Yao, Q., Yang, H., Han, B., Niu, G., and Kwok, J. T.-Y.
\newblock Searching to exploit memorization effect in learning with noisy
  labels.
\newblock In \emph{ICML}, pp.\  10789--10798. PMLR, 2020{\natexlab{a}}.

\bibitem[Yao et~al.(2020{\natexlab{b}})Yao, Liu, Han, Gong, Deng, Niu, and
  Sugiyama]{yao2020dual}
Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., and Sugiyama, M.
\newblock Dual t: Reducing estimation error for transition matrix in
  label-noise learning.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Yu et~al.(2018)Yu, Liu, Gong, and Tao]{yu2018learning}
Yu, X., Liu, T., Gong, M., and Tao, D.
\newblock Learning with biased complementary labels.
\newblock In \emph{ECCV}, pp.\  68--83, 2018.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{ICML}, pp.\  7164--7173, 2019.

\bibitem[Yu et~al.(2020)Yu, Liu, Gong, Zhang, Batmanghelich, and
  Tao]{yu2020label}
Yu, X., Liu, T., Gong, M., Zhang, K., Batmanghelich, K., and Tao, D.
\newblock Label-noise robust domain adaptation.
\newblock In \emph{ICML}, pp.\  10913--10924. PMLR, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Niu, and Sugiyama]{zhang2021confidence}
Zhang, Y., Niu, G., and Sugiyama, M.
\newblock Learning noise transition matrix from only noisy labels via total
  variation regularization.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{NeurIPS}, pp.\  8778--8788, 2018.

\bibitem[Zhu et~al.(2021{\natexlab{a}})Zhu, Liu, and Liu]{zhu2020second}
Zhu, Z., Liu, T., and Liu, Y.
\newblock A second-order approach to learning with instance-dependent label
  noise.
\newblock In \emph{CVPR}, 2021{\natexlab{a}}.

\bibitem[Zhu et~al.(2021{\natexlab{b}})Zhu, Song, and
  Liu]{zhu2021clusterability}
Zhu, Z., Song, Y., and Liu, Y.
\newblock Clusterability as an alternative to anchor points when learning with
  noisy labels.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\end{thebibliography}
