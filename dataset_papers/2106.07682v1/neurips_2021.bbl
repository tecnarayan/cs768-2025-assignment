\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain and Bengio(2016)]{alain2016understanding}
Guillaume Alain and Yoshua Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock \emph{arXiv preprint arXiv:1610.01644}, 2016.

\bibitem[Bansal et~al.(2021)Bansal, Kaplun, and Barak]{bansal2021for}
Yamini Bansal, Gal Kaplun, and Boaz Barak.
\newblock For self-supervised learning, rationality implies generalization,
  provably.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Srmggo3b3X6}.

\bibitem[Caron et~al.(2021{\natexlab{a}})Caron, Misra, Mairal, Goyal,
  Bojanowski, and Joulin]{swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments, 2021{\natexlab{a}}.

\bibitem[Caron et~al.(2021{\natexlab{b}})Caron, Touvron, Misra, Jégou, Mairal,
  Bojanowski, and Joulin]{dino}
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr
  Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers,
  2021{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Radford, Child, Wu, Jun,
  Dhariwal, Luan, and Sutskever]{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal,
  David Luan, and Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations, 2020{\natexlab{b}}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale, 2020.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International conference on machine learning}, pages
  1309--1318. PMLR, 2018.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020.

\bibitem[Freeman and Bruna(2017)]{freeman2017topology}
C.~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of half-rectified network optimization, 2017.

\bibitem[Garipov et~al.()Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipovloss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns, 2018.

\bibitem[Geirhos et~al.(2020)Geirhos, Narayanappa, Mitzkus, Bethge, Wichmann,
  and Brendel]{DBLP:journals/corr/abs-2010-08377}
Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Matthias Bethge,
  Felix~A. Wichmann, and Wieland Brendel.
\newblock On the surprising similarities between supervised and self-supervised
  models.
\newblock \emph{CoRR}, abs/2010.08377, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.08377}.

\bibitem[Goh et~al.(2021)Goh, †, †, Carter, Petrov, Schubert, Radford, and
  Olah]{goh2021multimodal}
Gabriel Goh, Nick~Cammarata †, Chelsea~Voss †, Shan Carter, Michael Petrov,
  Ludwig Schubert, Alec Radford, and Chris Olah.
\newblock Multimodal neurons in artificial neural networks.
\newblock \emph{Distill}, 2021.
\newblock \doi{10.23915/distill.00030}.
\newblock https://distill.pub/2021/multimodal-neurons.

\bibitem[Goyal et~al.(2021)Goyal, Duval, Reizenstein, Leavitt, Xu, Lefaudeux,
  Singh, Reis, Caron, Bojanowski, Joulin, and Misra]{goyal2021vissl}
Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu,
  Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr
  Bojanowski, Armand Joulin, and Ishan Misra.
\newblock Vissl.
\newblock \url{https://github.com/facebookresearch/vissl}, 2021.

\bibitem[Hardoon et~al.(2004)Hardoon, Szedmak, and
  Shawe-Taylor]{hardoon2004canonical}
David~R Hardoon, Sandor Szedmak, and John Shawe-Taylor.
\newblock Canonical correlation analysis: An overview with application to
  learning methods.
\newblock \emph{Neural computation}, 16\penalty0 (12):\penalty0 2639--2664,
  2004.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition, 2015.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Komarechka(2021)]{mclean21}
Don Komarechka.
\newblock 2021.
\newblock Images on
  \url{https://www.macleans.ca/culture/arts/american-canadian-snowflake-photography/}.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{Kornblith0LH19}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey~E. Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning,
  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pages 3519--3529. {PMLR},
  2019.
\newblock URL \url{http://proceedings.mlr.press/v97/kornblith19a.html}.

\bibitem[Kriegeskorte et~al.(2008)Kriegeskorte, Mur, and
  Bandettini]{Kriegeskorte2008}
Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini.
\newblock Representational similarity analysis - connecting the branches of
  systems neuroscience.
\newblock \emph{Frontiers in Systems Neuroscience}, 2:\penalty0 4, 2008.
\newblock ISSN 1662-5137.
\newblock \doi{10.3389/neuro.06.004.2008}.
\newblock URL
  \url{https://www.frontiersin.org/article/10.3389/neuro.06.004.2008}.

\bibitem[Lenc and Vedaldi(2015)]{lenc2015understanding}
Karel Lenc and Andrea Vedaldi.
\newblock Understanding image representations by measuring their equivariance
  and equivalence.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 991--999, 2015.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 6391--6401, 2018.

\bibitem[Li et~al.(2016)Li, Yosinski, Clune, Lipson, and
  Hopcroft]{li2016convergent}
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same
  representations?, 2016.

\bibitem[Liu et~al.(2020)Liu, Papailiopoulos, and Achlioptas]{liu2020bad}
Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas.
\newblock Bad global minima exist and sgd can reach them.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Morcos et~al.(2018)Morcos, Raghu, and Bengio]{morcos2018insights}
Ari~S Morcos, Maithra Raghu, and Samy Bengio.
\newblock Insights on representational similarity in neural networks with
  canonical correlation.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Nakkiran and Bansal(2020)]{dg}
Preetum Nakkiran and Yamini Bansal.
\newblock Distributional generalization: A new kind of generalization.
\newblock \emph{arXiv preprint arXiv:2009.08092}, 2020.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Neyshabur, and
  Sedghi]{nakkiran2021deep}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap framework: Good online learners are good offline
  generalizers, 2021.

\bibitem[Nguyen et~al.(2021)Nguyen, Raghu, and Kornblith]{nguyen2021wide}
Thao Nguyen, Maithra Raghu, and Simon Kornblith.
\newblock Do wide and deep networks learn the same things? uncovering how
  neural network representations vary with width and depth.
\newblock \emph{ICLR}, 2021.

\bibitem[Olah et~al.(2017)Olah, Mordvintsev, and Schubert]{olah2017feature}
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.
\newblock Feature visualization.
\newblock \emph{Distill}, 2017.
\newblock \doi{10.23915/distill.00007}.
\newblock https://distill.pub/2017/feature-visualization.

\bibitem[Olah et~al.(2020)Olah, Cammarata, Schubert, Goh, Petrov, and
  Carter]{olah2020an}
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and
  Shan Carter.
\newblock An overview of early vision in inceptionv1.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00024.002}.
\newblock https://distill.pub/2020/circuits/early-vision.

\bibitem[Page(2018)]{mcnn}
David Page.
\newblock How to train your resnet.
\newblock \url{https://myrtle.ai/how-to-train-your-resnet-4-architecture/},
  2018.

\bibitem[Raghu et~al.(2017)Raghu, Gilmer, Yosinski, and
  Sohl{-}Dickstein]{RaghuGYS17}
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl{-}Dickstein.
\newblock {SVCCA:} singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pages 6076--6085, 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html}.

\bibitem[Rumelhart et~al.(1985)Rumelhart, Hinton, and
  Williams]{rumelhart1985learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for
  Cognitive Science, 1985.

\bibitem[Wang et~al.(2018)Wang, Hu, Gu, Wu, Hu, He, and
  Hopcroft]{wang2018understanding}
Liwei Wang, Lunjia Hu, Jiayuan Gu, Yue Wu, Zhiqiang Hu, Kun He, and John
  Hopcroft.
\newblock Towards understanding learning representations: To what extent do
  different neural networks learn the same representation, 2018.

\bibitem[Zhou et~al.(2014)Zhou, Lapedriza, Xiao, Torralba, and
  Oliva]{zhou2014learning}
Bolei Zhou, {\`A}gata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude
  Oliva.
\newblock Learning deep features for scene recognition using places database.
\newblock In \emph{Neural Information Processing Systems}, 2014.

\end{thebibliography}
