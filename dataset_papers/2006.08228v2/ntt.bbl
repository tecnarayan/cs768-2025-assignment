\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{Arora2019On}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\'{e} Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  8139--8148. Curran Associates, Inc., 2019.

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and
  Legenstein]{Bellec2018deep}
Bellec, G., Kappel, D., Maass, W., and Legenstein, R.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{Blalock2020what}
Blalock, D., Ortiz, J. J.~G., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?, 2020.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{Jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., and Wanderman-Milne, S.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[{Carreira-Perpinan} \& {Idelbayev}(2018){Carreira-Perpinan} and
  {Idelbayev}]{CarreiraPerpinan2018}
{Carreira-Perpinan}, M.~A. and {Idelbayev}, Y.
\newblock "learning-compression" algorithms for neural net pruning.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  8532--8541, June 2018.

\bibitem[Chauvin(1989)]{Chauvin1989}
Chauvin, Y.
\newblock A back-propagation algorithm with optimal use of hidden units.
\newblock In Touretzky, D.~S. (ed.), \emph{Advances in Neural Information
  Processing Systems 1}, pp.\  519--526. Morgan-Kaufmann, 1989.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{Chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\'{e} Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  2933--2943. Curran Associates, Inc., 2019.

\bibitem[Collins \& Kohli(2014)Collins and Kohli]{Collins2014MemoryBD}
Collins, M.~D. and Kohli, P.
\newblock Memory bounded deep convolutional networks.
\newblock \emph{ArXiv}, abs/1412.1442, 2014.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{Evci2020rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  471--481. PMLR, 2020.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{Frankle2019}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{Glorot10}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In Teh, Y.~W. and Titterington, M. (eds.), \emph{Proceedings of the
  Thirteenth International Conference on Artificial Intelligence and
  Statistics}, volume~9 of \emph{Proceedings of Machine Learning Research},
  pp.\  249--256, Chia Laguna Resort, Sardinia, Italy, 13--15 May 2010.

\bibitem[Gong et~al.(2014)Gong, Liu, Yang, and Bourdev]{gong_compressing_2014}
Gong, Y., Liu, L., Yang, M., and Bourdev, L.
\newblock Compressing {Deep} {Convolutional} {Networks} using {Vector}
  {Quantization}.
\newblock \emph{arXiv:1412.6115 [cs]}, December 2014.
\newblock arXiv: 1412.6115.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{Guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  1379--1387. Curran Associates, Inc., 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In Cortes, C., Lawrence, N.~D., Lee, D.~D., Sugiyama, M., and
  Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems
  28}, pp.\  1135--1143. Curran Associates, Inc., 2015.

\bibitem[Hanson \& Pratt(1989)Hanson and Pratt]{Jose1988comparing}
Hanson, S.~J. and Pratt, L.~Y.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock In Touretzky, D.~S. (ed.), \emph{Advances in Neural Information
  Processing Systems 1}, pp.\  177--185. Morgan-Kaufmann, 1989.

\bibitem[Hassibi et~al.(1994)Hassibi, Stork, and Wolff]{Hassibi1994optimal}
Hassibi, B., Stork, D.~G., and Wolff, G.
\newblock Optimal brain surgeon: Extensions and performance comparisons.
\newblock In Cowan, J.~D., Tesauro, G., and Alspector, J. (eds.),
  \emph{Advances in Neural Information Processing Systems 6}, pp.\  263--270.
  Morgan-Kaufmann, 1994.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2014delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock \emph{IEEE International Conference on Computer Vision (ICCV 2015)},
  1502, 02 2015.

\bibitem[{He} et~al.(2019){He}, {Liu}, {Hadaeghi}, and
  {Jaeger}]{He2019Reservoir}
{He}, X., {Liu}, T., {Hadaeghi}, F., and {Jaeger}, H.
\newblock Reservoir transfer on analog neuromorphic hardware.
\newblock In \emph{9th International {IEEE/EMBS} Conference on Neural
  Engineering (NER)}, pp.\  1234--1238, March 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  8571--8580. Curran Associates,
  Inc., 2018.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg_speeding_2014}
Jaderberg, M., Vedaldi, A., and Zisserman, A.
\newblock Speeding up {Convolutional} {Neural} {Networks} with {Low} {Rank}
  {Expansions}.
\newblock \emph{arXiv:1405.3866 [cs]}, May 2014.
\newblock arXiv: 1405.3866.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma2015adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{Krizhevsky09}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lebedev_speeding-up_2015}
Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I., and Lempitsky, V.
\newblock Speeding-up {Convolutional} {Neural} {Networks} {Using} {Fine}-tuned
  {CP}-{Decomposition}.
\newblock \emph{arXiv:1412.6553 [cs]}, April 2015.
\newblock arXiv: 1412.6553.

\bibitem[LeCun(2019)]{lecun_deep_2019}
LeCun, Y.
\newblock Deep {Learning} {Hardware}: {Past}, {Present}, and {Future}.
\newblock In \emph{2019 {IEEE} {International} {Solid}- {State} {Circuits}
  {Conference}}, February 2019.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{LeCun1990}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In Touretzky, D.~S. (ed.), \emph{Advances in Neural Information
  Processing Systems 2}, pp.\  598--605. Morgan-Kaufmann, 1990.

\bibitem[{LeCun} et~al.(1998){LeCun}, {Bottou}, {Bengio}, and
  {Haffner}]{Lecun1998}
{LeCun}, Y., {Bottou}, L., {Bengio}, Y., and {Haffner}, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, Nov 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{LeCun2015}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521:\penalty0 436--444, May 2015.

\bibitem[Lee et~al.(2019{\natexlab{a}})Lee, Xiao, Schoenholz, Bahri, Novak,
  Sohl-Dickstein, and Pennington]{Lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\'{e} Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  8570--8581. Curran Associates, Inc.,
  2019{\natexlab{a}}.

\bibitem[Lee et~al.(2019{\natexlab{b}})Lee, Ajanthan, and Torr]{Lee2018snip}
Lee, N., Ajanthan, T., and Torr, P. H.~S.
\newblock {SNIP}: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{ICLR}, 2019{\natexlab{b}}.

\bibitem[Lee et~al.(2020)Lee, Ajanthan, Gould, and Torr]{Lee2020signal}
Lee, N., Ajanthan, T., Gould, S., and Torr, P. H.~S.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{Li2017Pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{Li2019enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
\newblock Enhanced convolutional neural tangent kernels, 2019.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{Liu2019rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{Louizos2018learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through l0 regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{Mocanu2018}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature Communications}, 9\penalty0 (1):\penalty0 2383, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{Molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem[Morcos et~al.(2019)Morcos, Yu, Paganini, and Tian]{Morcos2019one}
Morcos, A., Yu, H., Paganini, M., and Tian, Y.
\newblock One ticket to win them all: generalizing lottery ticket
  initializations across datasets and optimizers.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'~Alch\'{e}-Buc,
  F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  4933--4943. Curran Associates, Inc., 2019.

\bibitem[Neftci(2018)]{Neftci2018data}
Neftci, E.~O.
\newblock Data and {Power} {Efficient} {Intelligence} with {Neuromorphic}
  {Learning} {Machines}.
\newblock \emph{iScience}, 5:\penalty0 52--68, July 2018.

\bibitem[Neftci et~al.(2019)Neftci, Mostafa, and Zenke]{Neftci2019}
Neftci, E.~O., Mostafa, H., and Zenke, F.
\newblock Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}:
  {Bringing} the {Power} of {Gradient}-based optimization to spiking neural
  networks, November 2019.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{Netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A.~A., Sohl-Dickstein, J., and
  Schoenholz, S.~S.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Richards et~al.(2019)Richards, Lillicrap, Beaudoin, Bengio, Bogacz,
  Christensen, Clopath, Costa, de~Berker, Ganguli, Gillon, Hafner, Kepecs,
  Kriegeskorte, Latham, Lindsay, Miller, Naud, Pack, Poirazi, Roelfsema,
  Sacramento, Saxe, Scellier, Schapiro, Senn, Wayne, Yamins, Zenke, Zylberberg,
  Therien, and Kording]{richards_deep_2019}
Richards, B.~A., Lillicrap, T.~P., Beaudoin, P., Bengio, Y., Bogacz, R.,
  Christensen, A., Clopath, C., Costa, R.~P., de~Berker, A., Ganguli, S.,
  Gillon, C.~J., Hafner, D., Kepecs, A., Kriegeskorte, N., Latham, P., Lindsay,
  G.~W., Miller, K.~D., Naud, R., Pack, C.~C., Poirazi, P., Roelfsema, P.,
  Sacramento, J., Saxe, A., Scellier, B., Schapiro, A.~C., Senn, W., Wayne, G.,
  Yamins, D., Zenke, F., Zylberberg, J., Therien, D., and Kording, K.~P.
\newblock A deep learning framework for neuroscience.
\newblock \emph{Nat Neurosci}, 22\penalty0 (11):\penalty0 1761--1770, November
  2019.

\bibitem[Rigollet \& Weed(2019)Rigollet and Weed]{Rigollet2019uncoupled}
Rigollet, P. and Weed, J.
\newblock {Uncoupled isotonic regression via minimum Wasserstein
  deconvolution}.
\newblock \emph{Information and Inference: A Journal of the IMA}, 8\penalty0
  (4):\penalty0 691--717, 04 2019.

\bibitem[Roy et~al.(2019)Roy, Jaiswal, and Panda]{Roy2019towards}
Roy, K., Jaiswal, A., and Panda, P.
\newblock Towards spike-based machine intelligence with neuromorphic computing.
\newblock \emph{Nature}, 575\penalty0 (7784):\penalty0 607--617, November 2019.

\bibitem[Schmidhuber(2015)]{Schmidhuber2015}
Schmidhuber, J.
\newblock Deep learning in neural networks: An overview.
\newblock \emph{Neural Networks}, 61:\penalty0 85--117, 2015.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, Driessche,
  Graepel, and Hassabis]{silver_mastering_2017}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui,
  F., Sifre, L., Driessche, G. v.~d., Graepel, T., and Hassabis, D.
\newblock Mastering the game of {Go} without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, October 2017.

\bibitem[Sterling \& Laughlin(2017)Sterling and
  Laughlin]{Sterling2017principles}
Sterling, P. and Laughlin, S.
\newblock \emph{Principles of {Neural} {Design}}.
\newblock The MIT Press, reprint edition edition, June 2017.
\newblock ISBN 978-0-262-53468-0.

\bibitem[Str\"om(1997)]{Stroem1997sparse}
Str\"om, N.
\newblock Sparse connection and pruning in large dynamic artificial neural
  networks, 1997.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{Wang2020picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Xu et~al.(2019)Xu, Honda, Niu, and Sugiyama]{Xu2019uncoupled}
Xu, L., Honda, J., Niu, G., and Sugiyama, M.
\newblock Uncoupled regression from pairwise comparison data.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'~Alch\'{e}-Buc,
  F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  3994--4004. Curran Associates, Inc., 2019.

\bibitem[{Yan} et~al.(2019){Yan}, {Kappel}, {Neumärker}, {Partzsch},
  {Vogginger}, {Höppner}, {Furber}, {Maass}, {Legenstein}, and
  {Mayr}]{Yan2019efficient}
{Yan}, Y., {Kappel}, D., {Neumärker}, F., {Partzsch}, J., {Vogginger}, B.,
  {Höppner}, S., {Furber}, S., {Maass}, W., {Legenstein}, R., and {Mayr}, C.
\newblock Efficient reward-based structural plasticity on a spinnaker 2
  prototype.
\newblock \emph{IEEE Transactions on Biomedical Circuits and Systems},
  13\penalty0 (3):\penalty0 579--591, June 2019.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{Yang2017designing}
Yang, T., Chen, Y., and Sze, V.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pp.\
  6071--6079. {IEEE} Computer Society, 2017.

\bibitem[Zador(2019)]{Zador2019}
Zador, A.~M.
\newblock A critique of pure learning and what artificial neural networks can
  learn from animal brains.
\newblock \emph{Nature Communications}, 10\penalty0 (1):\penalty0 3770, 2019.

\bibitem[Zhu \& Gupta(2018)Zhu and Gupta]{Zhu2018to}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018}, 2018.

\end{thebibliography}
