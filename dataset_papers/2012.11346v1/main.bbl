\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{tensorflow}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ln}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt-3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners, 2020.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{neuralode}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31, pp.\  6571--6583. Curran
  Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf}.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{gckpt}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{CoRR}, abs/1604.06174, 2016.
\newblock URL \url{http://arxiv.org/abs/1604.06174}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{sparsetr}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{CoRR}, abs/1904.10509, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.10509}.

\bibitem[Cho et~al.(2014)Cho, van Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{gru}
Cho, K., van Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
  Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using {RNN} encoder{--}decoder for
  statistical machine translation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pp.\  1724--1734, Doha, Qatar,
  October 2014. Association for Computational Linguistics.
\newblock \doi{10.3115/v1/D14-1179}.
\newblock URL \url{https://www.aclweb.org/anthology/D14-1179}.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{performer}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
  Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L.,
  and Weller, A.
\newblock Rethinking attention with {P}erformers.
\newblock \emph{CoRR}, arXiv:2009.14794, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.14794}.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Cohen, Carbonell, Le, and
  Salakhutdinov]{transformerxl}
Dai, Z., Yang, Z., Yang, Y., Cohen, W.~W., Carbonell, J., Le, Q.~V., and
  Salakhutdinov, R.
\newblock Transformer-{XL}: Language modeling with longer-term dependency,
  2019.
\newblock URL \url{https://openreview.net/forum?id=HJePno0cYm}.

\bibitem[Griewank(1992)]{logckpt}
Griewank, A.
\newblock Achieving logarithmic growth of temporal and spatial complexity in
  reverse automatic differentiation.
\newblock \emph{Optimization Methods and Software}, 1\penalty0 (1):\penalty0
  35--54, 1992.
\newblock \doi{10.1080/10556789208805505}.
\newblock URL \url{https://doi.org/10.1080/10556789208805505}.

\bibitem[Griewank \& Walther(2008)Griewank and Walther]{autograd}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating Derivatives: Principles and Techniques of
  Algorithmic Differentiation, Second Edition}.
\newblock Other Titles in Applied Mathematics. Society for Industrial and
  Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA
  19104), 2008.
\newblock ISBN 9780898717761.
\newblock URL \url{https://books.google.co.uk/books?id=xoiiLaRxcbEC}.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{gelu}
Hendrycks, D. and Gimpel, K.
\newblock Bridging nonlinearities and stochastic regularizers with gaussian
  error linear units.
\newblock \emph{CoRR}, abs/1606.08415, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.08415}.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Comput.}, 9\penalty0 (8):\penalty0 1735--1780, November
  1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.8.1735}.
\newblock URL \url{http://dx.doi.org/10.1162/neco.1997.9.8.1735}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{transfrnn}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock \emph{arXiv preprint arXiv:2006.16236}, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Ladner \& Fischer(1980)Ladner and Fischer]{cumsum}
Ladner, R.~E. and Fischer, M.~J.
\newblock Parallel prefix computation.
\newblock \emph{J. ACM}, 27\penalty0 (4):\penalty0 831â€“838, October 1980.
\newblock ISSN 0004-5411.
\newblock \doi{10.1145/322217.322232}.
\newblock URL \url{https://doi.org/10.1145/322217.322232}.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{albert}
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1eA7AEtvS}.

\bibitem[Li et~al.(2020)Li, Duan, and Zheng]{linattseg}
Li, R., Duan, C., and Zheng, S.
\newblock Linear attention mechanism: An efficient attention for semantic
  segmentation.
\newblock \emph{arXiv preprint arXiv:2007.14902}, 2020.

\bibitem[Mahoney(2009)]{enwik8}
Mahoney, M.
\newblock Large text compression benchmark, 2009.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{ptb}
Marcus, M.~P., Santorini, B., and Marcinkiewicz, M.~A.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn
  {T}reebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.
\newblock URL \url{https://www.aclweb.org/anthology/J93-2004}.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{gradacc}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pp.\  1--9, Brussels, Belgium, October 2018. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6301}.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6301}.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling, 2019.

\bibitem[Parisotto et~al.(2019)Parisotto, Song, Rae, Pascanu, Gulcehre,
  Jayakumar, Jaderberg, Kaufman, Clark, Noury, et~al.]{stabilizing}
Parisotto, E., Song, H.~F., Rae, J.~W., Pascanu, R., Gulcehre, C., Jayakumar,
  S.~M., Jaderberg, M., Kaufman, R.~L., Clark, A., Noury, S., et~al.
\newblock Stabilizing transformers for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.06764}, 2019.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, and
  Ku]{imtransf}
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., and Ku, A.
\newblock Image transformer.
\newblock \emph{CoRR}, abs/1802.05751, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.05751}.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{pytorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{routing}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{arXiv}, 2003.05997, 2020.

\bibitem[Sanh et~al.(2020)Sanh, Debut, Chaumond, and Wolf]{distil}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock {DistilBERT, a distilled version of BERT}: smaller, faster, cheaper
  and lighter.
\newblock \emph{arXiv}, 1910.01108, 2020.

\bibitem[Shen et~al.(2018)Shen, Zhang, Yi, Yan, and Zhao]{factatt}
Shen, Z., Zhang, M., Yi, S., Yan, J., and Zhao, H.
\newblock Factorized attention: Self-attention with linear complexities.
\newblock \emph{CoRR}, abs/1812.01243, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.01243}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{lra}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv}, 2011.04006, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Bahri, and
  Metzler]{efftran}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv}, 9.20006732, 2020{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  5998--6008. Curran Associates,
  Inc., 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}.

\bibitem[Vishkin(2010)]{parsum}
Vishkin, U.
\newblock Thinking in parallel: Some basic data-parallel algorithms and
  techniques.
\newblock 2010.

\bibitem[Wu* et~al.(2020)Wu*, Liu*, Lin, Lin, and Han]{litetransf}
Wu*, Z., Liu*, Z., Lin, J., Lin, Y., and Han, S.
\newblock Lite transformer with long-short range attention.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByeMPlHKPH}.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{pre-ln}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan,
  Y., Wang, L., and Liu, T.-Y.
\newblock On layer normalization in the transformer architecture, 2020.

\bibitem[You et~al.(2019)You, Li, Hseu, Song, Demmel, and Hsieh]{lamb}
You, Y., Li, J., Hseu, J., Song, X., Demmel, J., and Hsieh, C.
\newblock Reducing {BERT} pre-training time from 3 days to 76 minutes.
\newblock \emph{CoRR}, abs/1904.00962, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.00962}.

\end{thebibliography}
