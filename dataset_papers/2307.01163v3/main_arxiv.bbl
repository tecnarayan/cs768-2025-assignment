\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adelani et~al.(2021)Adelani, Abbott, Neubig, D’souza, Kreutzer,
  Lignos, Palen-Michel, Buzaaba, Rijhwani, Ruder,
  et~al.]{adelani2021masakhaner}
David~Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia
  Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti
  Rijhwani, Sebastian Ruder, et~al.
\newblock Masakhaner: Named entity recognition for african languages.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 1116--1131, 2021.

\bibitem[Adelani et~al.(2022)Adelani, Neubig, Ruder, Rijhwani, Beukman,
  Palen-Michel, Lignos, Alabi, Muhammad, Nabende,
  et~al.]{adelani2022masakhaner}
David~Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani,
  Michael Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba~O Alabi,
  Shamsuddeen~H Muhammad, Peter Nabende, et~al.
\newblock Masakhaner 2.0: Africa-centric transfer learning for named entity
  recognition.
\newblock In \emph{2022 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2022), Abu Dhabi, United Arab Emirates, December 7-11,
  2022}, pages 4488--4508. Association for Computational Linguistics (ACL),
  2022.

\bibitem[Alabdulmohsin et~al.(2021)Alabdulmohsin, Maennel, and
  Keysers]{alabdulmohsin2021impact}
Ibrahim Alabdulmohsin, Hartmut Maennel, and Daniel Keysers.
\newblock The impact of reinitialization on generalization in convolutional
  neural networks.
\newblock \emph{arXiv preprint arXiv:2109.00267}, 2021.

\bibitem[Alabi et~al.(2022)Alabi, Adelani, Mosbach, and
  Klakow]{alabi-etal-2022-adapting}
Jesujoba~O. Alabi, David~Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow.
\newblock Adapting pre-trained language models to {A}frican languages via
  multilingual adaptive fine-tuning.
\newblock In \emph{Proceedings of the 29th International Conference on
  Computational Linguistics}, pages 4336--4349, Gyeongju, Republic of Korea,
  October 2022. International Committee on Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.coling-1.382}.

\bibitem[Anderson and Hulbert(2021)]{anderson2021}
Michael~C. Anderson and Justin~C. Hulbert.
\newblock Active forgetting: Adaptation of memory by prefrontal control.
\newblock \emph{Annual Review of Psychology}, 72\penalty0 (1):\penalty0 1--36,
  2021.
\newblock \doi{10.1146/annurev-psych-072720-094140}.
\newblock URL \url{https://doi.org/10.1146/annurev-psych-072720-094140}.
\newblock PMID: 32928060.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Ansell et~al.(2022)Ansell, Ponti, Korhonen, and
  Vuli{\'c}]{ansell2022composable}
Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli{\'c}.
\newblock Composable sparse fine-tuning for cross-lingual transfer.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1778--1796,
  2022.

\bibitem[Artetxe et~al.(2020)Artetxe, Ruder, and Yogatama]{artetxe2020cross}
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
\newblock On the cross-lingual transferability of monolingual representations.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 4623--4637, 2020.

\bibitem[Barrett and Zollman(2009)]{barrett2009role}
Jeffrey Barrett and Kevin~JS Zollman.
\newblock The role of forgetting in the evolution and learning of language.
\newblock \emph{Journal of Experimental \& Theoretical Artificial
  Intelligence}, 21\penalty0 (4):\penalty0 293--309, 2009.

\bibitem[Beaulieu et~al.(2020)Beaulieu, Frati, Miconi, Lehman, Stanley, Clune,
  and Cheney]{beaulieu2020learning}
Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth~O Stanley, Jeff
  Clune, and Nick Cheney.
\newblock Learning to continually learn.
\newblock \emph{arXiv preprint arXiv:2002.09571}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Chen et~al.(2022)Chen, Mishra, Franceschi, Minervini, Stenetorp, and
  Riedel]{chenrefactor}
Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus
  Stenetorp, and Sebastian Riedel.
\newblock Refactor gnns: Revisiting factorisation-based models from a
  message-passing perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Conneau et~al.(2018)Conneau, Rinott, Lample, Williams, Bowman,
  Schwenk, and Stoyanov]{conneau-etal-2018-xnli}
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman,
  Holger Schwenk, and Veselin Stoyanov.
\newblock {XNLI}: Evaluating cross-lingual sentence representations.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2475--2485, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1269}.
\newblock URL \url{https://aclanthology.org/D18-1269}.

\bibitem[Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and
  Stoyanov]{conneau-etal-2020-unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 8440--8451, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.747}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.747}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[D'Oro et~al.(2023)D'Oro, Schwarzer, Nikishin, Bacon, Bellemare, and
  Courville]{d2022sample}
Pierluca D'Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc~G
  Bellemare, and Aaron Courville.
\newblock Sample-efficient reinforcement learning by breaking the replay ratio
  barrier.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=OpC-9aBBVJe}.

\bibitem[Ebrahimi et~al.(2022)Ebrahimi, Mager, Oncevay, Chaudhary, Chiruzzo,
  Fan, Ortega, Ramos, Gonzales, Meza-Ruiz, et~al.]{ebrahimi2022americasnli}
Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis
  Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette~Rios Gonzales, Ivan
  Meza-Ruiz, et~al.
\newblock Americasnli: Evaluating zero-shot natural language understanding of
  pretrained multilingual models in truly low-resource languages.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 6279--6299,
  2022.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Frankle and Carbin(2019)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Gaya et~al.(2023)Gaya, Doan, Caccia, Soulier, Denoyer, and
  Raileanu]{gaya2022building}
Jean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer,
  and Roberta Raileanu.
\newblock Building a subspace of policies for scalable continual learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=UKr0MwZM6fL}.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo,
  Beltagy, Downey, and Smith]{gururangan2020don}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A Smith.
\newblock Don’t stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 8342--8360, 2020.

\bibitem[He et~al.(2021{\natexlab{a}})He, Gao, and Chen]{he2021debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
\newblock Debertav3: Improving deberta using electra-style pre-training with
  gradient-disentangled embedding sharing, 2021{\natexlab{a}}.

\bibitem[He et~al.(2021{\natexlab{b}})He, Liu, Gao, and Chen]{he2021deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=XPZIaotutsD}.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and
  McCandlish]{hernandez2021scaling}
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock \emph{arXiv preprint arXiv:2102.01293}, 2021.

\bibitem[Hu et~al.(2021)Hu, Wallis, Allen-Zhu, Li, Wang, Wang, Chen,
  et~al.]{hulora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang,
  Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Igl et~al.(2021)Igl, Farquhar, Luketina, Boehmer, and
  Whiteson]{igl2021transient}
Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon
  Whiteson.
\newblock Transient non-stationarity and generalisation in deep reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Qun8fv4qSby}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Khetarpal et~al.(2022)Khetarpal, Riemer, Rish, and
  Precup]{khetarpal2022towards}
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup.
\newblock Towards continual reinforcement learning: A review and perspectives.
\newblock \emph{Journal of Artificial Intelligence Research}, 75:\penalty0
  1401--1476, 2022.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Kudo and Richardson(2018)]{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 66--71, 2018.

\bibitem[Levy et~al.(2007)Levy, McVeigh, Marful, and
  Anderson]{levy2007inhibiting}
Benjamin~J Levy, Nathan~D McVeigh, Alejandra Marful, and Michael~C Anderson.
\newblock Inhibiting your native language: The role of retrieval-induced
  forgetting during second-language acquisition.
\newblock \emph{Psychological Science}, 18\penalty0 (1):\penalty0 29--34, 2007.

\bibitem[Lewis et~al.(2020)Lewis, Oguz, Rinott, Riedel, and
  Schwenk]{lewis2020mlqa}
Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk.
\newblock Mlqa: Evaluating cross-lingual extractive question answering.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7315--7330, 2020.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Pfeiffer, Korhonen, Vuli{\'c}, and
  Gurevych]{liu2023delving}
Chen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vuli{\'c}, and Iryna Gurevych.
\newblock Delving deeper into cross-lingual visual question answering.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EACL 2023}, pages 2408--2423, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Xie, Li, and
  Ma]{liu2022pretraining}
Hong Liu, Sang~Michael Xie, Zhiyuan Li, and Tengyu Ma.
\newblock Same pre-training loss, better downstream: Implicit bias matters for
  language models.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{CoRR}, abs/1907.11692, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.11692}.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lyle et~al.(2023)Lyle, Zheng, Nikishin, Avila~Pires, Pascanu, and
  Dabney]{pmlr-v202-lyle23b}
Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila~Pires, Razvan Pascanu,
  and Will Dabney.
\newblock Understanding plasticity in neural networks.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of
  \emph{Proceedings of Machine Learning Research}, pages 23190--23211. PMLR,
  23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/lyle23b.html}.

\bibitem[Mallya and Lazebnik(2018)]{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 7765--7773, 2018.

\bibitem[Marchisio et~al.(2022)Marchisio, Lewis, Chen, and
  Artetxe]{marchisio2022mini}
Kelly Marchisio, Patrick Lewis, Yihong Chen, and Mikel Artetxe.
\newblock Mini-model adaptation: Efficiently extending pretrained models to new
  languages via aligned shallow training.
\newblock \emph{arXiv preprint arXiv:2212.10503}, 2022.

\bibitem[McCloskey and Cohen(1989)]{MCCLOSKEY1989109}
Michael McCloskey and Neal~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock volume~24 of \emph{Psychology of Learning and Motivation}, pages
  109--165. Academic Press, 1989.
\newblock \doi{https://doi.org/10.1016/S0079-7421(08)60536-8}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0079742108605368}.

\bibitem[Mitchell et~al.(2021)Mitchell, Lin, Bosselut, Finn, and
  Manning]{mitchell2021fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D
  Manning.
\newblock Fast model editing at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Mitchell et~al.(2022)Mitchell, Lin, Bosselut, Manning, and
  Finn]{mitchell2022memory}
Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher~D Manning, and
  Chelsea Finn.
\newblock Memory-based model editing at scale.
\newblock In \emph{International Conference on Machine Learning}, pages
  15817--15831. PMLR, 2022.

\bibitem[Nikishin et~al.(2022)Nikishin, Schwarzer, D’Oro, Bacon, and
  Courville]{nikishin2022primacy}
Evgenii Nikishin, Max Schwarzer, Pierluca D’Oro, Pierre-Luc Bacon, and Aaron
  Courville.
\newblock The primacy bias in deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  16828--16847. PMLR, 2022.

\bibitem[Nikishin et~al.(2023)Nikishin, Oh, Ostrovski, Lyle, Pascanu, Dabney,
  and Barreto]{nikishin2023deep}
Evgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will
  Dabney, and Andre Barreto.
\newblock Deep reinforcement learning with plasticity injection.
\newblock In \emph{Workshop on Reincarnating Reinforcement Learning at ICLR
  2023}, 2023.
\newblock URL \url{https://openreview.net/forum?id=O9cJADBZT1}.

\bibitem[Nørby(2015)]{simon2015}
Simon Nørby.
\newblock Why forget? on the adaptive value of memory loss.
\newblock \emph{Perspectives on Psychological Science}, 10\penalty0
  (5):\penalty0 551--578, 2015.
\newblock \doi{10.1177/1745691615596787}.
\newblock URL \url{https://doi.org/10.1177/1745691615596787}.
\newblock PMID: 26385996.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages 48--53, 2019.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural networks}, 113:\penalty0 54--71, 2019.

\bibitem[Past{\"o}tter et~al.(2008)Past{\"o}tter, B{\"a}uml, and
  Hanslmayr]{pastotter2008oscillatory}
Bernhard Past{\"o}tter, Karl-Heinz B{\"a}uml, and Simon Hanslmayr.
\newblock Oscillatory brain activity before and after an internal context
  change—evidence for a reset of encoding processes.
\newblock \emph{NeuroImage}, 43\penalty0 (1):\penalty0 173--181, 2008.

\bibitem[Pfeiffer et~al.(2020)Pfeiffer, Vuli{\'c}, Gurevych, and
  Ruder]{pfeiffer-etal-2020-mad}
Jonas Pfeiffer, Ivan Vuli{\'c}, Iryna Gurevych, and Sebastian Ruder.
\newblock {MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask
  {C}ross-{L}ingual {T}ransfer.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 7654--7673, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.617}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.617}.

\bibitem[Pfeiffer et~al.(2021)Pfeiffer, Vuli{\'c}, Gurevych, and
  Ruder]{pfeiffer2021unks}
Jonas Pfeiffer, Ivan Vuli{\'c}, Iryna Gurevych, and Sebastian Ruder.
\newblock Unks everywhere: Adapting multilingual language models to new
  scripts.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 10186--10203, 2021.

\bibitem[Pfeiffer et~al.(2022)Pfeiffer, Goyal, Lin, Li, Cross, Riedel, and
  Artetxe]{pfeiffer2022lifting}
Jonas Pfeiffer, Naman Goyal, Xi~Lin, Xian Li, James Cross, Sebastian Riedel,
  and Mikel Artetxe.
\newblock Lifting the curse of multilinguality by pre-training modular
  transformers.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 3479--3495, 2022.

\bibitem[Radford and Narasimhan(2018)]{Radford2018ImprovingLU}
Alec Radford and Karthik Narasimhan.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2383--2392, 2016.

\bibitem[Ramkumar et~al.(2023)Ramkumar, Arani, and Zonooz]{ramkumar2023learn}
Vijaya Raghavan~T Ramkumar, Elahe Arani, and Bahram Zonooz.
\newblock Learn, unlearn and relearn: An online learning paradigm for deep
  neural networks.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=WN1O2MJDST}.

\bibitem[Ratcliff(1990)]{ratcliff1990connectionist}
Roger Ratcliff.
\newblock Connectionist models of recognition memory: constraints imposed by
  learning and forgetting functions.
\newblock \emph{Psychological review}, 97\penalty0 (2):\penalty0 285, 1990.

\bibitem[Rolnick et~al.(2019)Rolnick, Ahuja, Schwarz, Lillicrap, and
  Wayne]{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Schaul and Schmidhuber(2010)]{schaul2010metalearning}
Tom Schaul and J{\"u}rgen Schmidhuber.
\newblock Metalearning.
\newblock \emph{Scholarpedia}, 5\penalty0 (6):\penalty0 4650, 2010.

\bibitem[Schmidhuber(2013)]{schmidhuber2013powerplay}
J{\"u}rgen Schmidhuber.
\newblock Powerplay: Training an increasingly general problem solver by
  continually searching for the simplest still unsolvable problem.
\newblock \emph{Frontiers in psychology}, 4:\penalty0 313, 2013.

\bibitem[Schwarz et~al.(2018)Schwarz, Czarnecki, Luketina, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In \emph{International conference on machine learning}, pages
  4528--4537. PMLR, 2018.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Taha et~al.(2021)Taha, Shrivastava, and Davis]{taha2021knowledge}
Ahmed Taha, Abhinav Shrivastava, and Larry~S Davis.
\newblock Knowledge evolution in neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12843--12852, 2021.

\bibitem[Tang et~al.(2020)Tang, Tran, Li, Chen, Goyal, Chaudhary, Gu, and
  Fan]{Tang2020MultilingualTW}
Y.~Tang, C.~Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary,
  Jiatao Gu, and Angela Fan.
\newblock Multilingual translation with extensible multilingual pretraining and
  finetuning.
\newblock \emph{ArXiv}, abs/2008.00401, 2020.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:220936592}.

\bibitem[Thrun and Pratt(2012)]{thrun2012learning}
Sebastian Thrun and Lorien Pratt.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Veniat et~al.(2020)Veniat, Denoyer, and Ranzato]{veniat2020efficient}
Tom Veniat, Ludovic Denoyer, and Marc'Aurelio Ranzato.
\newblock Efficient continual learning with modular networks and task-driven
  priors.
\newblock \emph{arXiv preprint arXiv:2012.12631}, 2020.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang,
  Cheng, Glaese, Balle, Kasirzadeh, Kenton, Brown, Hawkins, Stepleton, Biles,
  Birhane, Haas, Rimell, Hendricks, Isaac, Legassick, Irving, and
  Gabriel]{dm_ethical_social_risks}
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato,
  Po{-}Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac
  Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba
  Birhane, Julia Haas, Laura Rimell, Lisa~Anne Hendricks, William Isaac, Sean
  Legassick, Geoffrey Irving, and Iason Gabriel.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{CoRR}, abs/2112.04359, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.04359}.

\bibitem[Weidinger et~al.(2022)Weidinger, Uesato, Rauh, Griffin, Huang, Mellor,
  Glaese, Cheng, Balle, Kasirzadeh, et~al.]{weidinger2022taxonomy}
Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang,
  John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
  et~al.
\newblock Taxonomy of risks posed by language models.
\newblock In \emph{2022 ACM Conference on Fairness, Accountability, and
  Transparency}, pages 214--229, 2022.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122, 2018.

\bibitem[Zhou et~al.(2022)Zhou, Vani, Larochelle, and
  Courville]{zhoufortuitous}
Hattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron Courville.
\newblock Fortuitous forgetting in connectionist networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
