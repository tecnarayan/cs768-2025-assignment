@inproceedings{allen2017linear,
  title={Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  booktitle={8th Innovations in Theoretical Computer Science Conference (ITCS 2017)},
  year={2017},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}

@article{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1802.04434},
  year={2018}
}

@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{lessard2016analysis,
  title={Analysis and design of optimization algorithms via integral quadratic constraints},
  author={Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={1},
  pages={57--95},
  year={2016},
  publisher={SIAM}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}

@article{su2014differential,
  title={A differential equation for modeling Nesterovâ€™s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={2510--2518},
  year={2014}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1-2},
  pages={365--397},
  year={2012},
  publisher={Springer}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{bubeck2015geometric,
  title={A geometric alternative to Nesterov's accelerated gradient descent},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  journal={arXiv preprint arXiv:1506.08187},
  year={2015}
}

@inproceedings{feldman2018generalization,
  title={Generalization bounds for uniformly stable algorithms},
  author={Feldman, Vitaly and Vondrak, Jan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9747--9757},
  year={2018}
}

@inproceedings{feldman2019high,
  title={High probability generalization bounds for uniformly stable algorithms with nearly optimal rate},
  author={Feldman, Vitaly and Vondrak, Jan},
  booktitle={Conference on Learning Theory},
  pages={1270--1279},
  year={2019}
}

@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@article{bassily2020stability,
  title={Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses},
  author={Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{devolder2014first,
  title={First-order methods of smooth convex optimization with inexact oracle},
  author={Devolder, Olivier and Glineur, Fran{\c{c}}ois and Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={146},
  number={1-2},
  pages={37--75},
  year={2014},
  publisher={Springer}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@inproceedings{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis R},
  booktitle={Advances in neural information processing systems},
  pages={451--459},
  year={2011}
}

@inproceedings{shalev2009stochastic,
  title={Stochastic Convex Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  booktitle={COLT},
  year={2009}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{schur1909characteristic,
  title={On the characteristic roots of a linear substitution with an application to the theory of integral equations},
  author={Schur, Issai},
  journal={Math. Ann},
  volume={66},
  pages={488--510},
  year={1909}
}

@inproceedings{wang2017memory,
  title={Memory and communication efficient distributed stochastic optimization with minibatch prox},
  author={Wang, Jialei and Wang, Weiran and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1882--1919},
  year={2017},
  organization={PMLR}
}

@inproceedings{agarwalAKTZ20,
  author    = {Naman Agarwal and
               Rohan Anil and
               Tomer Koren and
               Kunal Talwar and
               Cyril Zhang},
  title     = {Stochastic Optimization with Laggard Data Pipelines},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  year      = {2020}
}

@article{rogers1978finite,
  title={A finite sample distribution-free performance bound for local discrimination rules},
  author={Rogers, William H and Wagner, Terry J},
  journal={The Annals of Statistics},
  pages={506--514},
  year={1978},
  publisher={JSTOR}
}

@article{devroye1979distribution,
  title={Distribution-free inequalities for the deleted and holdout error estimates},
  author={Devroye, Luc and Wagner, Terry},
  journal={IEEE Transactions on Information Theory},
  volume={25},
  number={2},
  pages={202--207},
  year={1979},
  publisher={IEEE}
}

@article{feldman2016generalization,
  title={Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back},
  author={Feldman, Vitaly},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  pages={3576--3584},
  year={2016}
}

@inproceedings{wu2017bolt,
  title={Bolt-on differential privacy for scalable stochastic gradient descent-based analytics},
  author={Wu, Xi and Li, Fengan and Kumar, Arun and Chaudhuri, Kamalika and Jha, Somesh and Naughton, Jeffrey},
  booktitle={Proceedings of the 2017 ACM International Conference on Management of Data},
  pages={1307--1322},
  year={2017}
}

@article{bassily2019private,
  title={Private Stochastic Convex Optimization with Optimal Rates},
  author={Bassily, Raef and Feldman, Vitaly and Talwar, Kunal and Guha Thakurta, Abhradeep},
  journal={Advances in neural information processing systems},
  year={2019}
}

@inproceedings{feldman2020private,
  title={Private stochastic convex optimization: optimal rates in linear time},
  author={Feldman, Vitaly and Koren, Tomer and Talwar, Kunal},
  booktitle={Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={439--449},
  year={2020}
}

@inproceedings{charles2018stability,
  title={Stability and generalization of learning algorithms that converge to global optima},
  author={Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={745--754},
  year={2018},
  organization={PMLR}
}

@inproceedings{koren2015fast,
  title={Fast rates for exp-concave empirical risk minimization},
  author={Koren, Tomer and Levy, Kfir Y},
  booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1},
  pages={1477--1485},
  year={2015}
}

@inproceedings{gonen2017fast,
  title={Fast rates for empirical risk minimization of strict saddle problems},
  author={Gonen, Alon and Shalev-Shwartz, Shai},
  booktitle={Conference on Learning Theory},
  pages={1043--1063},
  year={2017},
  organization={PMLR}
}

