\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Duchi(2011)]{agarwal2011distributed}
Alekh Agarwal and John~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Arjevani et~al.(2020)Arjevani, Shamir, and Srebro]{arjevani2020tight}
Yossi Arjevani, Ohad Shamir, and Nathan Srebro.
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock In \emph{Algorithmic Learning Theory}, pages 111--132. PMLR, 2020.

\bibitem[Arjevani et~al.(2022)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C. Duchi, Dylan~J. Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{Mathematical Programming}, pages 1--50, 2022.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  344--353. PMLR, 2019.

\bibitem[Assran et~al.(2020)Assran, Aytekin, Feyzmahdavian, Johansson, and
  Rabbat]{assran2020advances}
Mahmoud Assran, Arda Aytekin, Hamid~Reza Feyzmahdavian, Mikael Johansson, and
  Michael~G. Rabbat.
\newblock Advances in asynchronous parallel and distributed optimization.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (11):\penalty0
  2013--2031, 2020.

\bibitem[Aviv et~al.(2021)Aviv, Hakimi, Schuster, and Levy]{aviv21asynchronous}
Rotem~Zamir Aviv, Ido Hakimi, Assaf Schuster, and Kfir~Yehuda Levy.
\newblock Asynchronous distributed learning: Adapting to gradient delays
  without prior knowledge.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume 139, pages 436--445, 2021.

\bibitem[Baudet(1978)]{baudet1978asynchronous}
Gerard~M. Baudet.
\newblock Asynchronous iterative methods for multiprocessors.
\newblock \emph{Journal of the ACM (JACM)}, 25\penalty0 (2):\penalty0 226--244,
  1978.

\bibitem[Ben-Nun and Hoefler(2019)]{ben2019demystifying}
Tal Ben-Nun and Torsten Hoefler.
\newblock Demystifying parallel and distributed deep learning: An in-depth
  concurrency analysis.
\newblock \emph{ACM Computing Surveys (CSUR)}, 52\penalty0 (4):\penalty0 1--43,
  2019.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock {signSGD}: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  560--569, 2018.

\bibitem[Bubeck(2015)]{convexoptim_bubeck}
Sébastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends in Machine Learning}, 8\penalty0
  (3-4):\penalty0 231--357, 2015.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{carmon2017lower1}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points {I}.
\newblock \emph{arXiv preprint arXiv:1710.11606}, 2017.

\bibitem[Chai et~al.(2021)Chai, Chen, Anwar, Zhao, Cheng, and
  Rangwala]{chai2021fedat}
Zheng Chai, Yujing Chen, Ali Anwar, Liang Zhao, Yue Cheng, and Huzefa Rangwala.
\newblock {FedAT}: a high-performance and communication-efficient federated
  learning system with asynchronous tiers.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--16, 2021.

\bibitem[Chen et~al.(2016)Chen, Pan, Monga, Bengio, and
  Jozefowicz]{chen2016revisiting}
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
\newblock Revisiting distributed synchronous {SGD}.
\newblock \emph{arXiv preprint arXiv:1604.00981}, 2016.

\bibitem[Cohen et~al.(2021)Cohen, Daniely, Drori, Koren, and
  Schain]{cohen2021asynchronous}
Alon Cohen, Amit Daniely, Yoel Drori, Tomer Koren, and Mariano Schain.
\newblock Asynchronous stochastic optimization robust to arbitrary delays.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and
  Sridharan]{cotter2011better}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages
  1647--1655, 2011.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang, et~al.
\newblock Large scale distributed deep networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jan):\penalty0 165--202, 2012.

\bibitem[Even et~al.(2021)Even, Hendrikx, and Massoulié]{even2021delays}
Mathieu Even, Hadrien Hendrikx, and Laurent Massoulié.
\newblock Decentralized optimization with heterogeneous delays: a
  continuous-time approach.
\newblock \emph{arXiv:2106.03585}, 2021.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.
\newblock URL \url{https://arxiv.org/abs/1309.5549}.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
Robert~M. Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin,
  and Peter Richt{\'a}rik.
\newblock {SGD}: General analysis and improved rates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5200--5209, 2019.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Horv{\'a}th et~al.(2021)Horv{\'a}th, Laskaridis, Almeida, Leontiadis,
  Venieris, and Lane]{horvath2021fjord}
Samuel Horv{\'a}th, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis,
  Stylianos~I. Venieris, and Nicholas~D. Lane.
\newblock {FjORD}: Fair and accurate federated learning under heterogeneous
  targets with ordered dropout.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Joulani et~al.(2016)Joulani, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{joulani2016delay}
Pooria Joulani, Andr{\'a}s Gy{\"o}rgy, and Csaba Szepesv{\'a}ri.
\newblock Delay-tolerant online convex optimization: Unified analysis and
  adaptive-gradient algorithms.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Rouayheb, Evans, Gardner,
  Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo,
  Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konečný, Korolova,
  Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh,
  Raykova, Qi, Ramage, Raskar, Song, Song, Stich, Sun, Suresh, Tramèr,
  Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{kairouz2019advances}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, Rafael G.~L. D'Oliveira, Salim~El Rouayheb, David Evans,
  Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip~B.
  Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,
  Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
  Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi
  Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
  Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage,
  Ramesh Raskar, Dawn Song, Weikang Song, Sebastian~U. Stich, Ziteng Sun,
  Ananda~Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang,
  Li~Xiong, Zheng Xu, Qiang Yang, Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning, 2019.

\bibitem[Khaled and Richt\'{a}rik(2020)]{khaled2019better}
Ahmed Khaled and Peter Richt\'{a}rik.
\newblock Better theory for {SGD} in the nonconvex world.
\newblock \emph{arXiv Preprint arXiv:2002.03329}, 2020.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem[Koloskova et~al.(2022)Koloskova, Stich, and
  Jaggi]{koloskova2022sharper}
Anastasia Koloskova, Sebastian~U. Stich, and Martin Jaggi.
\newblock Sharper convergence guarantees for asynchronous {SGD} for distributed
  and federated learning.
\newblock \emph{arXiv preprint arXiv:2206.08307}, 2022.

\bibitem[Leblond et~al.(2018)Leblond, Pedregosa, and
  Lacoste-Julien]{leblond2018improved}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock Improved asynchronous parallel optimization analysis for stochastic
  incremental methods.
\newblock \emph{Journal of Machine Learning Research}, 19:\penalty0 1--68,
  2018.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and
  Jordan]{mania2017perturbed}
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
  Ramchandran, and Michael~I. Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2202--2229, 2017.
\newblock \doi{10.1137/16M1057000}.

\bibitem[McMahan and Streeter(2014)]{mcmahan2014delay}
Brendan McMahan and Matthew~J. Streeter.
\newblock Delay-tolerant algorithms for asynchronous distributed online
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mishchenko et~al.(2018)Mishchenko, Iutzeler, Malick, and
  Amini]{mishchenko2018delay}
Konstantin Mishchenko, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Massih-Reza
  Amini.
\newblock A delay-tolerant proximal-gradient algorithm for distributed
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3584--3592, 2018.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1928--1937, 2016.

\bibitem[Moritz et~al.(2018)Moritz, Nishihara, Wang, Tumanov, Liaw, Liang,
  Elibol, Yang, Paul, Jordan, and Stoica]{moritz2018ray}
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw,
  Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael~I. Jordan, and
  Ion Stoica.
\newblock Ray: A distributed framework for emerging {AI} applications.
\newblock In \emph{13th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 18)}, pages 561--577, 2018.

\bibitem[Nadiradze et~al.(2021)Nadiradze, Sabour, Davies, Li, and
  Alistarh]{nadiradze2021asynchronous}
Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Shigang Li, and Dan
  Alistarh.
\newblock Asynchronous decentralized {SGD} with quantized and local updates.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Nair et~al.(2015)Nair, Srinivasan, Blackwell, Alcicek, Fearon,
  De~Maria, Panneershelvam, Suleyman, Beattie, Petersen,
  et~al.]{nair2015massively}
Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon,
  Alessandro De~Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles
  Beattie, Stig Petersen, et~al.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1507.04296}, 2015.

\bibitem[Nedi{\'c} and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedi{\'c} and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nedi{\'c} et~al.(2001)Nedi{\'c}, Bertsekas, and
  Borkar]{nedic2001distributed}
Angelia Nedi{\'c}, Dimitri~P. Bertsekas, and Vivek~S. Borkar.
\newblock Distributed asynchronous incremental subgradient methods.
\newblock \emph{Studies in Computational Mathematics}, 8\penalty0 (C):\penalty0
  381--407, 2001.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovskyyudin1983}
Arkadii~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley-Interscience, 1983.

\bibitem[Nguyen et~al.(2022)Nguyen, Malik, Zhan, Yousefpour, Rabbat, Malek, and
  Huba]{nguyen2022federated}
John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani
  Malek, and Dzmitry Huba.
\newblock Federated learning with buffered asynchronous aggregation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3581--3607, 2022.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Ryabinin et~al.(2021)Ryabinin, Gorbunov, Plokhotnyuk, and
  Pekhimenko]{ryabinin2021moshpit}
Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko.
\newblock Moshpit {SGD}: Communication-efficient decentralized training on
  heterogeneous unreliable devices.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Sra et~al.(2016)Sra, Yu, Li, and Smola]{sra2016adadelay}
Suvrit Sra, Adams~Wei Yu, Mu~Li, and Alexander~J. Smola.
\newblock Adadelay: Delay adaptive distributed stochastic optimization.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 957--965.
  PMLR, 2016.

\bibitem[Stich(2018)]{stich2018local}
Sebastian~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Stich and Karimireddy(2020)]{stich2020error}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed updates.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--36,
  2020.

\bibitem[Tsitsiklis et~al.(1986)Tsitsiklis, Bertsekas, and
  Athans]{tsitsiklis1986distributed}
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans.
\newblock Distributed asynchronous deterministic and stochastic gradient
  optimization algorithms.
\newblock \emph{IEEE transactions on automatic control}, 31\penalty0
  (9):\penalty0 803--812, 1986.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, Mcmahan,
  Shamir, and Srebro]{woodworth2020local}
Blake Woodworth, Kumar~Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins,
  Brendan Mcmahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local {SGD} better than minibatch {SGD}?
\newblock In \emph{International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020.

\bibitem[Woodworth et~al.(2018)Woodworth, Wang, Smith, McMahan, and
  Srebro]{woodworth2018graph}
Blake~E. Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Wu et~al.(2022)Wu, Magnusson, Feyzmahdavian, and
  Johansson]{wu2022delay}
Xuyang Wu, Sindri Magnusson, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Delay-adaptive step-sizes for asynchronous learning.
\newblock \emph{arXiv preprint arXiv:2202.08550}, 2022.

\bibitem[Zheng et~al.(2017)Zheng, Meng, Wang, Chen, Yu, Ma, and
  Liu]{zheng2017asynchronous}
Shuxin Zheng, Qi~Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and
  Tie-Yan Liu.
\newblock Asynchronous stochastic gradient descent with delay compensation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4120--4129, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Mertikopoulos, Bambos, Glynn, Ye, Li, and
  Fei-Fei]{zhou2018distributed}
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu
  Ye, Li-Jia Li, and Li~Fei-Fei.
\newblock Distributed asynchronous optimization with unbounded delays: How slow
  can you go?
\newblock In \emph{International Conference on Machine Learning}, pages
  5970--5979. PMLR, 2018.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J. Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2595--2603, 2010.

\end{thebibliography}
