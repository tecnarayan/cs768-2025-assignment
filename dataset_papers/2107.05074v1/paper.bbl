\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2018learning}
Z~Allen-Zhu, Y~Li, and Y~Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{Advances in neural information processing systems},
  2019{\natexlab{a}}.

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Can sgd learn recurrent neural networks with provable generalization?
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, 2019.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Amir et~al.(2021)Amir, Koren, and Livni]{amir2020gd}
Idan Amir, Tomer Koren, and Roi Livni.
\newblock Sgd generalizes better than gd (and regularization doesn't help).
\newblock \emph{arXiv preprint arXiv:2102.01117}, 2021.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  244--253. PMLR, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 7413--7424, 2019.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and
  Talwar]{bassily2020stability}
Raef Bassily, Vitaly Feldman, Crist{\'o}bal Guzm{\'a}n, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bottou and Bousquet(2011)]{bottou201113}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large-scale learning.
\newblock \emph{Optimization for machine learning}, page 351, 2011.

\bibitem[Dauber et~al.(2020)Dauber, Feder, Koren, and Livni]{dauber2020can}
Assaf Dauber, Meir Feder, Tomer Koren, and Roi Livni.
\newblock Can implicit bias explain generalization? stochastic convex
  optimization as a case study.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Feldman(2016)]{feldman2016generalization}
Vitaly Feldman.
\newblock Generalization of erm in stochastic convex optimization: The
  dimension strikes back.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicitb}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31,
  2018{\natexlab{b}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{c}})Gunasekar, Woodworth,
  Bhojanapalli, Neyshabur, and Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
  Nathan Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pages 1--10. IEEE, 2018{\natexlab{c}}.

\bibitem[Ji and Telgarsky(2018)]{ji2018risk}
Ziwei Ji and Matus Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[Keskar et~al.(2017)Keskar, Nocedal, Tang, Mudigere, and
  Smelyanskiy]{keskar2016large}
Nitish~Shirish Keskar, Jorge Nocedal, Ping Tak~Peter Tang, Dheevatsa Mudigere,
  and Mikhail Smelyanskiy.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR 2017}, 2017.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and
  Yuan]{kleinberg2018alternative}
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does sgd escape local minima?
\newblock In \emph{International Conference on Machine Learning}, pages
  2698--2707. PMLR, 2018.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma2018power}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3325--3334. PMLR, 2018.

\bibitem[Matou{\v{s}}ek and Vondr{\'a}k(2001)]{matouvsek2001probabilistic}
Ji{\v{r}}{\'\i} Matou{\v{s}}ek and Jan Vondr{\'a}k.
\newblock The probabilistic method.
\newblock \emph{Lecture Notes, Department of Applied Mathematics, Charles
  University, Prague}, 2001.

\bibitem[Nemirovski and Yudin(1983)]{nemirovskij1983problem}
Arkadi~Semenovi{\v{c}} Nemirovski and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2014)]{nesterov}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, Incorporated, 1 edition, 2014.
\newblock ISBN 1461346916.

\bibitem[Panchenko(2002)]{panchenko2002some}
Dmitriy Panchenko.
\newblock Some extensions of an inequality of vapnik and chervonenkis.
\newblock \emph{Electronic Communications in Probability}, 7:\penalty0 55--65,
  2002.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{ShalevSiSr07}
S.~Shalev-Shwartz, Y.~Singer, and N.~Srebro.
\newblock Pegasos: {P}rimal {E}stimated sub-{G}r{A}dient {SO}lver for {SVM}.
\newblock pages 807--814, 2007.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2009stochastic}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{Conference on learning theory}, 2009.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Vapnik(2013)]{vapnik2013nature}
Vladimir Vapnik.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhou et~al.(2018)Zhou, Yang, Zhang, Liang, and Tarokh]{zhou2019sgd}
Yi~Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh.
\newblock Sgd converges to global minimum in deep learning via star-convex
  path.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
