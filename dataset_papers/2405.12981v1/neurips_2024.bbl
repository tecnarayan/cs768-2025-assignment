\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2023{\natexlab{a}})Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai]{ainslie-etal-2023-gqa}
J.~Ainslie, J.~Lee-Thorp, M.~de~Jong, Y.~Zemlyanskiy, F.~Lebron, and S.~Sanghai.
\newblock {GQA}: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock In H.~Bouamor, J.~Pino, and K.~Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4895--4901, Singapore, Dec. 2023{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.298}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.298}.

\bibitem[Ainslie et~al.(2023{\natexlab{b}})Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebrón, and Sanghai]{ainslie2023gqa}
J.~Ainslie, J.~Lee-Thorp, M.~de~Jong, Y.~Zemlyanskiy, F.~Lebrón, and S.~Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023{\natexlab{b}}.

\bibitem[Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Bell, Berard, Burovski, Chauhan, Chourdia, Constable, Desmaison, DeVito, Ellison, Feng, Gong, Gschwind, Hirsh, Huang, Kalambarkar, Kirsch, Lazos, Lezcano, Liang, Liang, Lu, Luk, Maher, Pan, Puhrsch, Reso, Saroufim, Siraichi, Suk, Suo, Tillet, Wang, Wang, Wen, Zhang, Zhao, Zhou, Zou, Mathews, Chanan, Wu, and Chintala]{Ansel_PyTorch_2_Faster_2024}
J.~Ansel, E.~Yang, H.~He, N.~Gimelshein, A.~Jain, M.~Voznesensky, B.~Bao, P.~Bell, D.~Berard, E.~Burovski, G.~Chauhan, A.~Chourdia, W.~Constable, A.~Desmaison, Z.~DeVito, E.~Ellison, W.~Feng, J.~Gong, M.~Gschwind, B.~Hirsh, S.~Huang, K.~Kalambarkar, L.~Kirsch, M.~Lazos, M.~Lezcano, Y.~Liang, J.~Liang, Y.~Lu, C.~Luk, B.~Maher, Y.~Pan, C.~Puhrsch, M.~Reso, M.~Saroufim, M.~Y. Siraichi, H.~Suk, M.~Suo, P.~Tillet, E.~Wang, X.~Wang, W.~Wen, S.~Zhang, X.~Zhao, K.~Zhou, R.~Zou, A.~Mathews, G.~Chanan, P.~Wu, and S.~Chintala.
\newblock {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}.
\newblock In \emph{29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)}. ACM, Apr. 2024.
\newblock \doi{10.1145/3620665.3640366}.
\newblock URL \url{https://pytorch.org/assets/pytorch2-2.pdf}.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang, and Weinbach]{black2022gptneox20b}
S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He, C.~Leahy, K.~McDonell, J.~Phang, M.~Pieler, U.~S. Prashanth, S.~Purohit, L.~Reynolds, J.~Tow, B.~Wang, and S.~Weinbach.
\newblock Gpt-neox-20b: An open-source autoregressive language model, 2022.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, Van Den~Driessche, Lespiau, Damoc, Clark, De~Las~Casas, Guy, Menick, Ring, Hennigan, Huang, Maggiore, Jones, Cassirer, Brock, Paganini, Irving, Vinyals, Osindero, Simonyan, Rae, Elsen, and Sifre]{retro}
S.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican, G.~B. Van Den~Driessche, J.-B. Lespiau, B.~Damoc, A.~Clark, D.~De~Las~Casas, A.~Guy, J.~Menick, R.~Ring, T.~Hennigan, S.~Huang, L.~Maggiore, C.~Jones, A.~Cassirer, A.~Brock, M.~Paganini, G.~Irving, O.~Vinyals, S.~Osindero, K.~Simonyan, J.~Rae, E.~Elsen, and L.~Sifre.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and S.~Sabato, editors, \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 2206--2240. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/borgeaud22a.html}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{sparseattn}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{CoRR}, abs/1904.10509, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.10509}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham, H.~W. Chung, C.~Sutton, S.~Gehrmann, P.~Schuh, K.~Shi, S.~Tsvyashchenko, J.~Maynez, A.~Rao, P.~Barnes, Y.~Tay, N.~Shazeer, V.~Prabhakaran, E.~Reif, N.~Du, B.~Hutchinson, R.~Pope, J.~Bradbury, J.~Austin, M.~Isard, G.~Gur-Ari, P.~Yin, T.~Duke, A.~Levskaya, S.~Ghemawat, S.~Dev, H.~Michalewski, X.~Garcia, V.~Misra, K.~Robinson, L.~Fedus, D.~Zhou, D.~Ippolito, D.~Luan, H.~Lim, B.~Zoph, A.~Spiridonov, R.~Sepassi, D.~Dohan, S.~Agrawal, M.~Omernick, A.~M. Dai, T.~S. Pillai, M.~Pellat, A.~Lewkowycz, E.~Moreira, R.~Child, O.~Polozov, K.~Lee, Z.~Zhou, X.~Wang, B.~Saeta, M.~Diaz, O.~Firat, M.~Catasta, J.~Wei, K.~Meier-Hellstern, D.~Eck, J.~Dean, S.~Petrov, and N.~Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{dai-etal-2019-transformer}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~Le, and R.~Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length context.
\newblock In A.~Korhonen, D.~Traum, and L.~M{\`a}rquez, editors, \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 2978--2988, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1285}.
\newblock URL \url{https://aclanthology.org/P19-1285}.

\bibitem[Dao(2023)]{dao2023flashattention2}
T.~Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[DeepSeek-AI(2024)]{deepseekai2024deepseekv2}
DeepSeek-AI.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

\bibitem[Gao et~al.(2024)Gao, He, Sharma, Kang, Jevdjic, Deng, Yang, Yu, and Zuo]{gao2024attentionstore}
B.~Gao, Z.~He, P.~Sharma, Q.~Kang, D.~Jevdjic, J.~Deng, X.~Yang, Z.~Yu, and P.~Zuo.
\newblock Attentionstore: Cost-effective attention reuse across multi-turn conversations in large language model serving, 2024.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{lm-eval-harness}
L.~Gao, J.~Tow, B.~Abbasi, S.~Biderman, S.~Black, A.~DiPofi, C.~Foster, L.~Golding, J.~Hsu, A.~Le~Noac'h, H.~Li, K.~McDonell, N.~Muennighoff, C.~Ociepa, J.~Phang, L.~Reynolds, H.~Schoelkopf, A.~Skowron, L.~Sutawika, E.~Tang, A.~Thite, B.~Wang, K.~Wang, and A.~Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Ge et~al.(2024)Ge, Zhang, Liu, Zhang, Han, and Gao]{ge2024fastgen}
S.~Ge, Y.~Zhang, L.~Liu, M.~Zhang, J.~Han, and J.~Gao.
\newblock Model tells you what to discard: Adaptive kv cache compression for llms, 2024.

\bibitem[{Google}(2024)]{googleai2024context}
{Google}.
\newblock Context caching guide.
\newblock \url{https://ai.google.dev/gemini-api/docs/caching}, 2024.
\newblock Accessed: 2024-05-20.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
A.~Gu and T.~Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces, 2023.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020realm}
K.~Guu, K.~Lee, Z.~Tung, P.~Pasupat, and M.-W. Chang.
\newblock Realm: retrieval-augmented language model pre-training.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Hooper et~al.(2024)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer, and Gholami]{hooper2024kvquant}
C.~Hooper, S.~Kim, H.~Mohammadzadeh, M.~W. Mahoney, Y.~S. Shao, K.~Keutzer, and A.~Gholami.
\newblock Kvquant: Towards 10 million context length llm inference with kv cache quantization, 2024.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam, Le, Wu, and Chen]{10.5555/3454287.3454297}
Y.~Huang, Y.~Cheng, A.~Bapna, O.~Firat, M.~X. Chen, D.~Chen, H.~Lee, J.~Ngiam, Q.~V. Le, Y.~Wu, and Z.~Chen.
\newblock \emph{GPipe: efficient training of giant neural networks using pipeline parallelism}.
\newblock Curran Associates Inc., Red Hook, NY, USA, 2019.

\bibitem[Izacard et~al.(2024)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{atlas}
G.~Izacard, P.~Lewis, M.~Lomeli, L.~Hosseini, F.~Petroni, T.~Schick, J.~Dwivedi-Yu, A.~Joulin, S.~Riedel, and E.~Grave.
\newblock Atlas: few-shot learning with retrieval augmented language models.
\newblock \emph{J. Mach. Learn. Res.}, 24\penalty0 (1), mar 2024.
\newblock ISSN 1532-4435.

\bibitem[Kalamkar et~al.(2019)Kalamkar, Mudigere, Mellempudi, Das, Banerjee, Avancha, Vooturi, Jammalamadaka, Huang, Yuen, Yang, Park, Heinecke, Georganas, Srinivasan, Kundu, Smelyanskiy, Kaul, and Dubey]{kalamkar2019study}
D.~Kalamkar, D.~Mudigere, N.~Mellempudi, D.~Das, K.~Banerjee, S.~Avancha, D.~T. Vooturi, N.~Jammalamadaka, J.~Huang, H.~Yuen, J.~Yang, J.~Park, A.~Heinecke, E.~Georganas, S.~Srinivasan, A.~Kundu, M.~Smelyanskiy, B.~Kaul, and P.~Dubey.
\newblock A study of bfloat16 for deep learning training, 2019.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{linearattn}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are rnns: fast autoregressive transformers with linear attention.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Liu et~al.(2024)Liu, Li, Cheng, Ray, Huang, Zhang, Du, Yao, Lu, Ananthanarayanan, Maire, Hoffmann, Holtzman, and Jiang]{liu2024cachegen}
Y.~Liu, H.~Li, Y.~Cheng, S.~Ray, Y.~Huang, Q.~Zhang, K.~Du, J.~Yao, S.~Lu, G.~Ananthanarayanan, M.~Maire, H.~Hoffmann, A.~Holtzman, and J.~Jiang.
\newblock Cachegen: Kv cache compression and streaming for fast language model serving, 2024.

\bibitem[Liu et~al.(2023)Liu, Desai, Liao, Wang, Xie, Xu, Kyrillidis, and Shrivastava]{liu2023scissorhands}
Z.~Liu, A.~Desai, F.~Liao, W.~Wang, V.~Xie, Z.~Xu, A.~Kyrillidis, and A.~Shrivastava.
\newblock Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time, 2023.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017sgdr}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and Wu]{micikevicius2018mixed}
P.~Micikevicius, S.~Narang, J.~Alben, G.~Diamos, E.~Elsen, D.~Garcia, B.~Ginsburg, M.~Houston, O.~Kuchaiev, G.~Venkatesh, and H.~Wu.
\newblock Mixed precision training, 2018.

\bibitem[Mohtashami and Jaggi(2023)]{landmark}
A.~Mohtashami and M.~Jaggi.
\newblock Random-access infinite context length for transformers.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 54567--54585. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf}.

\bibitem[Munkhdalai et~al.(2024)Munkhdalai, Faruqui, and Gopal]{infiniattn}
T.~Munkhdalai, M.~Faruqui, and S.~Gopal.
\newblock Leave no context behind: Efficient infinite context transformers with infini-attention, 2024.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Köpf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Köpf, E.~Yang, Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library, 2019.

\bibitem[Peng et~al.(2024)Peng, Goldstein, Anthony, Albalak, Alcaide, Biderman, Cheah, Du, Ferdinan, Hou, Kazienko, GV, Kocoń, Koptyra, Krishna, au2, Muennighoff, Obeid, Saito, Song, Tu, Woźniak, Zhang, Zhao, Zhao, Zhou, Zhu, and Zhu]{peng2024eagle}
B.~Peng, D.~Goldstein, Q.~Anthony, A.~Albalak, E.~Alcaide, S.~Biderman, E.~Cheah, X.~Du, T.~Ferdinan, H.~Hou, P.~Kazienko, K.~K. GV, J.~Kocoń, B.~Koptyra, S.~Krishna, R.~M.~J. au2, N.~Muennighoff, F.~Obeid, A.~Saito, G.~Song, H.~Tu, S.~Woźniak, R.~Zhang, B.~Zhao, Q.~Zhao, P.~Zhou, J.~Zhu, and R.-J. Zhu.
\newblock Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence, 2024.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and Le]{ramachandran2017searching}
P.~Ramachandran, B.~Zoph, and Q.~V. Le.
\newblock Searching for activation functions, 2017.

\bibitem[Shazeer(2019)]{shazeer2019mqa}
N.~Shazeer.
\newblock Fast transformer decoding: One write-head is all you need, 2019.

\bibitem[Shazeer(2020)]{shazeer2020glu}
N.~Shazeer.
\newblock Glu variants improve transformer, 2020.

\bibitem[Sheng et~al.(2023)Sheng, Zheng, Yuan, Li, Ryabinin, Chen, Liang, R\'{e}, Stoica, and Zhang]{flexgen}
Y.~Sheng, L.~Zheng, B.~Yuan, Z.~Li, M.~Ryabinin, B.~Chen, P.~Liang, C.~R\'{e}, I.~Stoica, and C.~Zhang.
\newblock Flexgen: high-throughput generative inference of large language models with a single gpu.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, ICML'23. JMLR.org, 2023.

\bibitem[Shoeybi et~al.(2020)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2020megatronlm}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{cerebras2023slimpajama}
D.~Soboleva, F.~Al-Khateeb, R.~Myers, J.~R. Steeves, J.~Hestness, and N.~Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.
\newblock URL \url{https://huggingface.co/datasets/cerebras/SlimPajama-627B}.

\bibitem[Su et~al.(2023)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2023roformer}
J.~Su, Y.~Lu, S.~Pan, A.~Murtadha, B.~Wen, and Y.~Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2023.

\bibitem[Sun et~al.(2024)Sun, Dong, Zhu, Huang, Wang, Ma, Zhang, Wang, and Wei]{sun2024cache}
Y.~Sun, L.~Dong, Y.~Zhu, S.~Huang, W.~Wang, S.~Ma, Q.~Zhang, J.~Wang, and F.~Wei.
\newblock You only cache once: Decoder-decoder architectures for language models, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C. Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani+2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~u. Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2019)Wang, Cho, and Gu]{wang2019neural}
C.~Wang, K.~Cho, and J.~Gu.
\newblock Neural machine translation with byte-level subwords, 2019.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{CoRR}, abs/2006.04768, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.04768}.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{wu2022memorizing}
Y.~Wu, M.~N. Rabe, D.~Hutchins, and C.~Szegedy.
\newblock Memorizing transformers.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=TrjbxzRcnf-}.

\bibitem[Yang et~al.(2024)Yang, Wang, Shen, Panda, and Kim]{yang2024gated}
S.~Yang, B.~Wang, Y.~Shen, R.~Panda, and Y.~Kim.
\newblock Gated linear attention transformers with hardware-efficient training, 2024.

\bibitem[Zhang et~al.(2024)Zhang, Yi, Xu, and Shrivastava]{zhang2024kvonebit}
T.~Zhang, J.~Yi, Z.~Xu, and A.~Shrivastava.
\newblock Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Sheng, Zhou, Chen, Zheng, Cai, Song, Tian, R\'{e}, Barrett, Wang, and Chen]{h2o}
Z.~Zhang, Y.~Sheng, T.~Zhou, T.~Chen, L.~Zheng, R.~Cai, Z.~Song, Y.~Tian, C.~R\'{e}, C.~Barrett, Z.~A. Wang, and B.~Chen.
\newblock H2o: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 34661--34710. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf}.

\bibitem[Zhu et~al.(2023)Zhu, Li, Liu, Ma, and Wang]{zhu2023survey}
X.~Zhu, J.~Li, Y.~Liu, C.~Ma, and W.~Wang.
\newblock A survey on model compression for large language models, 2023.

\end{thebibliography}
