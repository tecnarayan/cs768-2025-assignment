\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abeille and Lazaric(2017)]{abeille2017linear}
Marc Abeille and Alessandro Lazaric.
\newblock Linear thompson sampling revisited.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 176--184.
  PMLR, 2017.

\bibitem[Agrawal et~al.(2021)Agrawal, Chen, and Jiang]{agrawal2020improved}
Priyank Agrawal, Jinglin Chen, and Nan Jiang.
\newblock Improved worst-case regret bounds for randomized least-squares value
  iteration.
\newblock \emph{Thirty-fifth AAAI conference on artificial intelligence}, 2021.

\bibitem[Agrawal and Goyal(2012)]{agrawal2012analysis}
Shipra Agrawal and Navin Goyal.
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock In \emph{Conference on learning theory}, pages 39--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Agrawal and Goyal(2017)]{agrawal2017near}
Shipra Agrawal and Navin Goyal.
\newblock Near-optimal regret bounds for thompson sampling.
\newblock \emph{J. ACM}, 64\penalty0 (5), September 2017.
\newblock ISSN 0004-5411.
\newblock \doi{10.1145/3088510}.
\newblock URL \url{https://doi.org/10.1145/3088510}.

\bibitem[Agrawal and Jia(2017{\natexlab{a}})]{agrawal2017optimistic}
Shipra Agrawal and Randy Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1184--1194, 2017{\natexlab{a}}.

\bibitem[Agrawal and Jia(2017{\natexlab{b}})]{agrawal2017posterior}
Shipra Agrawal and Randy Jia.
\newblock Posterior sampling for reinforcement learning: worst-case regret
  bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1184--1194, 2017{\natexlab{b}}.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Bartlett and Tewari(2009)]{bartlett2009regal}
Peter~L Bartlett and Ambuj Tewari.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pages 35--42. AUAI Press, 2009.

\bibitem[Brafman and Tennenholtz(2003)]{brafman2002r}
Ronen~I. Brafman and Moshe Tennenholtz.
\newblock R-max - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{J. Mach. Learn. Res.}, 3\penalty0 (Oct):\penalty0 213--231,
  March 2003.
\newblock ISSN 1532-4435.

\bibitem[Bubeck and Liu(2014)]{bubeck2014prior}
S{\'e}bastien Bubeck and Che-Yu Liu.
\newblock Prior-free and prior-dependent regret bounds for thompson sampling.
\newblock In \emph{2014 48th Annual Conference on Information Sciences and
  Systems (CISS)}, pages 1--9. IEEE, 2014.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cai et~al.(2019)Cai, Yang, Jin, and Wang]{cai2019provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock \emph{arXiv preprint arXiv:1912.05830}, 2019.

\bibitem[Chapelle and Li(2011)]{chapelle2011empirical}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of thompson sampling.
\newblock In \emph{Advances in neural information processing systems}, pages
  2249--2257, 2011.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS’17, page 5717–5727, Red Hook, NY,
  USA, 2017. Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research},
  pages 1507--1516, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2021episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In \emph{Algorithmic Learning Theory}, pages 578--598. PMLR, 2021.

\bibitem[Dong et~al.(2019)Dong, Wang, Chen, and Wang]{dong2019q}
Kefan Dong, Yuanhao Wang, Xiaoyu Chen, and Liwei Wang.
\newblock Q-learning with ucb exploration is sample efficient for
  infinite-horizon mdp.
\newblock \emph{arXiv preprint arXiv:1901.09311}, 2019.

\bibitem[Fortunato et~al.(2018)Fortunato, Azar, Piot, Menick, Hessel, Osband,
  Graves, Mnih, Munos, Hassabis, et~al.]{fortunato2018noisy}
Meire Fortunato, Mohammad~Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo
  Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis,
  et~al.
\newblock Noisy networks for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, and Lazaric]{fruit2018near}
Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric.
\newblock Near optimal exploration-exploitation in non-communicating markov
  decision processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2994--3004, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2019)Jin, Yang, Wang, and Jordan]{jin2019provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1907.05388}, 2019.

\bibitem[Jin et~al.(2020)Jin, Xu, Shi, Xiao, and Gu]{jin2020mots}
Tianyuan Jin, Pan Xu, Jieming Shi, Xiaokui Xiao, and Quanquan Gu.
\newblock Mots: Minimax optimal thompson sampling.
\newblock \emph{arXiv preprint arXiv:2003.01803}, 2020.

\bibitem[Kakade(2003)]{kakade2003sample}
Sham~M Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and Munos]{kaufmann2012thompson}
Emilie Kaufmann, Nathaniel Korda, and R{\'e}mi Munos.
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In \emph{International conference on algorithmic learning theory},
  pages 199--213. Springer, 2012.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Kolter and Ng(2009)]{kolter2009near}
J~Zico Kolter and Andrew~Y Ng.
\newblock Near-bayesian exploration in polynomial time.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 513--520, 2009.

\bibitem[Lai and Robbins(1985)]{lai1985asymptotically}
Tze~Leung Lai and Herbert Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lattimore and Hutter(2012)]{lattimore2012pac}
Tor Lattimore and Marcus Hutter.
\newblock Pac bounds for discounted mdps.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 320--334. Springer, 2012.

\bibitem[Li et~al.(2021)Li, Shi, Chen, Gu, and Chi]{li2021breaking}
Gen Li, Laixi Shi, Yuxin Chen, Yuantao Gu, and Yuejie Chi.
\newblock Breaking the sample complexity barrier to regret-optimal model-free
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical {B}ernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Menard et~al.(2021)Menard, Domingues, Shang, and Valko]{menard2021ucb}
Pierre Menard, Omar~Darwiche Domingues, Xuedong Shang, and Michal Valko.
\newblock Ucb momentum q-learning: Correcting the bias without forgetting.
\newblock \emph{arXiv preprint arXiv:2103.01312}, 2021.

\bibitem[Neu and Pike-Burke(2020)]{neu2020unifying}
Gergely Neu and Ciara Pike-Burke.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.01891}, 2020.

\bibitem[Osband and Van~Roy(2017)]{osband2017posterior}
Ian Osband and Benjamin Van~Roy.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2701--2710. JMLR. org, 2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3003--3011, 2013.

\bibitem[Osband et~al.(2014)Osband, Van~Roy, and Wen]{osband2014generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock \emph{arXiv preprint arXiv:1402.0635}, 2014.

\bibitem[Osband et~al.(2017)Osband, Russo, Wen, and Van~Roy]{osband2017deep}
Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van~Roy.
\newblock Deep exploration via randomized value functions.
\newblock \emph{arXiv preprint arXiv:1703.07608}, 2017.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8617--8629, 2018.

\bibitem[Pacchiano et~al.(2020)Pacchiano, Ball, Parker-Holder, Choromanski, and
  Roberts]{pacchiano2020optimism}
Aldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and
  Stephen Roberts.
\newblock On optimism in model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.11911}, 2020.

\bibitem[Russo(2019)]{russo2019worst}
Daniel Russo.
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14433--14443, 2019.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1153--1162, 2019.

\bibitem[Strehl and Littman(2008)]{strehl2008analysis}
Alexander~L Strehl and Michael~L Littman.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford, and
  Littman]{strehl2006pac}
Alexander~L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael~L
  Littman.
\newblock Pac model-free reinforcement learning.
\newblock In \emph{Proceedings of the 23rd international conference on Machine
  learning}, pages 881--888. ACM, 2006.

\bibitem[Szita and Szepesv{\'a}ri(2010)]{szita2010model}
Istv{\'a}n Szita and Csaba Szepesv{\'a}ri.
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In \emph{ICML}, 2010.

\bibitem[Talebi and Maillard(2018)]{talebi2018variance}
Mohammad~Sadegh Talebi and Odalric-Ambrym Maillard.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock \emph{arXiv preprint arXiv:1803.01626}, 2018.

\bibitem[Tan et~al.(2020)Tan, Xiong, and Dwaracherla]{tan2020parameterized}
Tian Tan, Zhihan Xiong, and Vikranth~R Dwaracherla.
\newblock Parameterized indexed value function for efficient exploration in
  reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5948--5955, 2020.

\bibitem[Vaswani et~al.(2020)Vaswani, Mehrabian, Durand, and
  Kveton]{pmlr-v108-vaswani20a}
Sharan Vaswani, Abbas Mehrabian, Audrey Durand, and Branislav Kveton.
\newblock Old dog learns new tricks: Randomized ucb for bandit problems.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, pages 1988--1998, 2020.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Kakade]{wang2020long}
Ruosong Wang, Simon~S Du, Lin~F Yang, and Sham~M Kakade.
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2005.00527}, 2020.

\bibitem[Xu and Tewari(2019)]{xu2019worst}
Ziping Xu and Ambuj Tewari.
\newblock Worst-case regret bound for perturbation based exploration in
  reinforcement learning.
\newblock \emph{Ann Arbor}, 1001:\penalty0 48109, 2019.

\bibitem[Yang et~al.(2020)Yang, Yang, and Du]{yang2020q}
Kunhe Yang, Lin~F Yang, and Simon~S Du.
\newblock {$Q$}-learning with logarithmic regret.
\newblock \emph{arXiv preprint arXiv:2006.09118}, 2020.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312, 2019.

\bibitem[Zanette et~al.(2020)Zanette, Brandfonbrener, Brunskill, Pirotta, and
  Lazaric]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and
  Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1954--1964, 2020.

\bibitem[Zhang and Ji(2019)]{zhang2019regret}
Zihan Zhang and Xiangyang Ji.
\newblock Regret minimization for reinforcement learning by evaluating the
  optimal bias function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2823--2832, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Ji, and
  Du]{zhang2020reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon~S Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zhou, and Ji]{zhang2020almost}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \emph{arXiv preprint arXiv:2004.10019}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Zhou, and Ji]{zhang2020model}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Model-free reinforcement learning: from clipped pseudo-regret to
  sample complexity.
\newblock \emph{arXiv preprint arXiv:2006.03864}, 2020{\natexlab{c}}.

\end{thebibliography}
