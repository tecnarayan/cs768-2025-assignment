\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angelova et~al.(2005)Angelova, Abu-Mostafam, and
  Perona]{angelova2005pruning}
Angelova, A., Abu-Mostafam, Y., and Perona, P.
\newblock Pruning training sets for learning of object categories.
\newblock In \emph{Computer Vision and Pattern Recognition, 2005. CVPR 2005.
  IEEE Computer Society Conference on}, volume~1, pp.\  494--501. IEEE, 2005.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock \emph{arXiv preprint arXiv:1706.05394}, 2017.

\bibitem[Brodley \& Friedl(1999)Brodley and Friedl]{brodley1999identifying}
Brodley, C.~E. and Friedl, M.~A.
\newblock Identifying mislabeled training data.
\newblock \emph{Journal of artificial intelligence research}, 11:\penalty0
  131--167, 1999.

\bibitem[Chapelle et~al.(2009)Chapelle, Scholkopf, and Zien]{chapelle2009semi}
Chapelle, O., Scholkopf, B., and Zien, A.
\newblock Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book
  reviews].
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (3):\penalty0 542--542, 2009.

\bibitem[Chow(1970)]{chow1970optimum}
Chow, C.
\newblock On optimum recognition error and reject tradeoff.
\newblock \emph{IEEE Transactions on information theory}, 16\penalty0
  (1):\penalty0 41--46, 1970.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Coates, A., Ng, A., and Lee, H.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  215--223, 2011.

\bibitem[Cortes et~al.(2016)Cortes, DeSalvo, and Mohri]{cortes2016learning}
Cortes, C., DeSalvo, G., and Mohri, M.
\newblock Learning with rejection.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  67--82. Springer, 2016.

\bibitem[De~Stefano et~al.(2000)De~Stefano, Sansone, and Vento]{de2000reject}
De~Stefano, C., Sansone, C., and Vento, M.
\newblock To reject or not to reject: that is the question-an answer in case of
  neural classifiers.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 30\penalty0 (1):\penalty0 84--94, 2000.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Computer Vision and Pattern Recognition, 2009. CVPR 2009.
  IEEE Conference on}, pp.\  248--255. Ieee, 2009.

\bibitem[Fr{\'e}nay \& Verleysen(2014)Fr{\'e}nay and
  Verleysen]{frenay2014classification}
Fr{\'e}nay, B. and Verleysen, M.
\newblock Classification in the presence of label noise: a survey.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  25\penalty0 (5):\penalty0 845--869, 2014.

\bibitem[Fumera \& Roli(2002)Fumera and Roli]{fumera2002support}
Fumera, G. and Roli, F.
\newblock Support vector machines with embedded reject option.
\newblock In \emph{Pattern recognition with support vector machines}, pp.\
  68--82. Springer, 2002.

\bibitem[Geifman \& El-Yaniv(2017)Geifman and El-Yaniv]{geifman2017selective}
Geifman, Y. and El-Yaniv, R.
\newblock Selective classification for deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4885--4894, 2017.

\bibitem[G{\"o}rnitz et~al.(2014)G{\"o}rnitz, Porbadnigk, Binder, Sannelli,
  Braun, M{\"u}ller, and Kloft]{gornitz2014learning}
G{\"o}rnitz, N., Porbadnigk, A., Binder, A., Sannelli, C., Braun, M.,
  M{\"u}ller, K.-R., and Kloft, M.
\newblock Learning and evaluation in presence of non-iid label noise.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  293--302,
  2014.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: robust training deep neural networks with extremely
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1804.06872}, 2018.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016baseline}
Hendrycks, D. and Gimpel, K.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and
  Gimpel]{hendrycks2018using}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock \emph{arXiv preprint arXiv:1802.05300}, 2018.

\bibitem[ImageNet()]{imagenet_stats}
ImageNet.
\newblock Imagenet statistics.
\newblock URL \url{http://image-net.org/about-stats}.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2309--2318, 2018.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436, 2015.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Wang, Li, Agustsson, and
  Van~Gool]{li2017webvision}
Li, W., Wang, L., Li, W., Agustsson, E., and Van~Gool, L.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{arXiv preprint arXiv:1708.02862}, 2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Yang, Song, Cao, Luo, and
  Li]{li2017learning}
Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, L.-J.
\newblock Learning from noisy labels with distillation.
\newblock In \emph{ICCV}, pp.\  1928--1936, 2017{\natexlab{b}}.

\bibitem[Moosavi-Dezfooli et~al.(2017)Moosavi-Dezfooli, Fawzi, Fawzi, and
  Frossard]{moosavi2017universal}
Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P.
\newblock Universal adversarial perturbations.
\newblock \emph{arXiv preprint}, 2017.

\bibitem[Nettleton et~al.(2010)Nettleton, Orriols-Puig, and
  Fornells]{nettleton2010study}
Nettleton, D.~F., Orriols-Puig, A., and Fornells, A.
\newblock A study of the effect of different types of noise on the precision of
  supervised learning techniques.
\newblock \emph{Artificial intelligence review}, 33\penalty0 (4):\penalty0
  275--306, 2010.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Nguyen, A., Yosinski, J., and Clune, J.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  427--436, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Menon, A.~K., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)}, pp.\
   2233--2241, 2017.

\bibitem[Porbadnigk et~al.(2014)Porbadnigk, G{\"o}rnitz, Sannelli, Binder,
  Braun, Kloft, and M{\"u}ller]{porbadnigk2014brain}
Porbadnigk, A.~K., G{\"o}rnitz, N., Sannelli, C., Binder, A., Braun, M., Kloft,
  M., and M{\"u}ller, K.-R.
\newblock When brain and behavior disagree: Tackling systematic label noise in
  eeg data with machine learning.
\newblock In \emph{Brain-Computer Interface (BCI), 2014 International Winter
  Workshop on}, pp.\  1--4. IEEE, 2014.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Rolnick et~al.(2017)Rolnick, Veit, Belongie, and
  Shavit]{rolnick2017deep}
Rolnick, D., Veit, A., Belongie, S., and Shavit, N.
\newblock Deep learning is robust to massive label noise.
\newblock \emph{arXiv preprint arXiv:1705.10694}, 2017.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh,
  Batra, et~al.]{selvaraju2017grad}
Selvaraju, R.~R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.,
  et~al.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{ICCV}, pp.\  618--626, 2017.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg2014striving}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{arXiv preprint arXiv:1412.6806}, 2014.

\bibitem[Sukhbaatar et~al.(2014)Sukhbaatar, Bruna, Paluri, Bourdev, and
  Fergus]{sukhbaatar2014training}
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fergus, R.
\newblock Training convolutional networks with noisy labels.
\newblock \emph{arXiv preprint arXiv:1406.2080}, 2014.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Turk()]{amturk}
Turk, A.~M.
\newblock https://www.mturk.com.

\bibitem[Vahdat(2017)]{vahdat2017toward}
Vahdat, A.
\newblock Toward robustness against label noise in training deep discriminative
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5596--5605, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.~R.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1805.07836}, 2018.

\bibitem[Zhu \& Wu(2004)Zhu and Wu]{zhu2004class}
Zhu, X. and Wu, X.
\newblock Class noise vs. attribute noise: A quantitative study.
\newblock \emph{Artificial intelligence review}, 22\penalty0 (3):\penalty0
  177--210, 2004.

\bibitem[Zhu et~al.(2003)Zhu, Wu, and Chen]{zhu2003eliminating}
Zhu, X., Wu, X., and Chen, Q.
\newblock Eliminating class noise in large datasets.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pp.\  920--927, 2003.

\end{thebibliography}
