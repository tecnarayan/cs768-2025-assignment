@article{piriyakulkij2024doing,
      title={Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning}, 
      author={Wasu Top Piriyakulkij and Kevin Ellis},
      year={2024},
      journal={CogSci}
}
@misc{patel2024datadreamer,
   title={DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows},
   author={Ajay Patel and Colin Raffel and Chris Callison-Burch},
   year={2024},
   eprint={2402.10379},
   archivePrefix={arXiv},
   primaryClass={cs.CL}
}
@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{meng2022generating,
  title={Generating training data with language models: Towards zero-shot language understanding},
  author={Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={462--477},
  year={2022}
}
@article{ye2022zerogen,
  title={Zerogen: Efficient zero-shot learning via dataset generation},
  author={Ye, Jiacheng and Gao, Jiahui and Li, Qintong and Xu, Hang and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2202.07922},
  year={2022}
}
@article{hafner2020mastering,
  title={Mastering atari with discrete world models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={ICLR},
  year={2021}
}
@inproceedings{NIPS2011_e53a0a29,
 author = {Chapelle, Olivier and Li, Lihong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An Empirical Evaluation of Thompson Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},
 volume = {24},
 year = {2011}
}
@inproceedings{liu2023optimistic,
  title={Optimistic mle: A generic model-based algorithm for partially observable sequential decision making},
  author={Liu, Qinghua and Netrapalli, Praneeth and Szepesvari, Csaba and Jin, Chi},
  booktitle={Proceedings of the 55th Annual ACM Symposium on Theory of Computing},
  pages={363--376},
  year={2023}
}
@inproceedings{liu2022partially,
  title={When is partially observable reinforcement learning not scary?},
  author={Liu, Qinghua and Chung, Alan and Szepesv{\'a}ri, Csaba and Jin, Chi},
  booktitle={Conference on Learning Theory},
  pages={5175--5220},
  year={2022},
  organization={PMLR}
}
@misc{sun2023interactive,
      title={Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks}, 
      author={Lingfeng Sun and Devesh K. Jha and Chiori Hori and Siddarth Jain and Radu Corcodel and Xinghao Zhu and Masayoshi Tomizuka and Diego Romeres},
      year={2023},
      eprint={2312.06876},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@misc{olausson2023selfrepair,
      title={Is Self-Repair a Silver Bullet for Code Generation?}, 
      author={Theo X. Olausson and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao and Armando Solar-Lezama},
      year={2023},
      eprint={2306.09896},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chollet2019measure,
      title={On the Measure of Intelligence}, 
      author={François Chollet},
      year={2019},
      eprint={1911.01547},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{shridhar2020alfworld,
  title={Alfworld: Aligning text and embodied environments for interactive learning},
  author={Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew},
  journal={arXiv preprint arXiv:2010.03768},
  year={2020}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{hume2007enquiry,
  title={An enquiry concerning human understanding},
  author={Hume, David},
  year={1748}
}
@article{bhagavatula2019abductive,
  title={Abductive commonsense reasoning},
  author={Bhagavatula, Chandra and Bras, Ronan Le and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-tau and Choi, Yejin},
  journal={arXiv preprint arXiv:1908.05739},
  year={2019}
}
@inproceedings{bosselut-etal-2019-comet,
    title = "{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction",
    author = "Bosselut, Antoine  and
      Rashkin, Hannah  and
      Sap, Maarten  and
      Malaviya, Chaitanya  and
      Celikyilmaz, Asli  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1470",
    doi = "10.18653/v1/P19-1470",
    pages = "4762--4779",
    abstract = "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
}
@inproceedings{bonet2011planning,
  title={Planning under partial observability by classical replanning: Theory and experiments},
  author={Bonet, Blai and Geffner, Hector},
  booktitle={Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence; july 16-22, 2011; Barcelona. Menlo Park, California: AAAI Press; 2011. p. 1936-1941.},
  year={2011},
  organization={Association for the Advancement of Artificial Intelligence (AAAI)}
}
@book{krr,
    title = {{Knowledge Representation and Reasoning}},
    publisher = {Morgan Kauffman},
    year = {2004}, 
    author={Ronald Brachman and Hector Levesque}
}
@book{SMCBook,
    editor = {Doucet, Arnaud and De Freitas, Nando and Gordon, Neil},
    title = {{Sequential Monte Carlo Methods in Practice}},
    publisher = {Springer},
    year = {2001}
}
@inproceedings{jung-etal-2022-maieutic,
    title = "Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations",
    author = "Jung, Jaehun  and
      Qin, Lianhui  and
      Welleck, Sean  and
      Brahman, Faeze  and
      Bhagavatula, Chandra  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.82",
    pages = "1266--1279",
    abstract = "Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20{\%} better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.",
}
@inproceedings{mitchell-etal-2022-enhancing,
    title = "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",
    author = "Mitchell, Eric  and
      Noh, Joseph  and
      Li, Siyan  and
      Armstrong, Will  and
      Agarwal, Ananth  and
      Liu, Patrick  and
      Finn, Chelsea  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.115",
    pages = "1754--1768",
    abstract = "While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers {\textless}i{\textgreater}Yes{\textless}/i{\textgreater} to {\textless}i{\textgreater}Is a sparrow a bird?{\textless}/i{\textgreater} and {\textless}i{\textgreater}Does a bird have feet?{\textless}/i{\textgreater} but answers {\textless}i{\textgreater}No{\textless}/i{\textgreater} to {\textless}i{\textgreater}Does a sparrow have feet?{\textless}/i{\textgreater}. To address this failure mode, we propose a framework, Consistency Correction through Relation Detection, or {\textless}b{\textgreater}ConCoRD{\textless}/b{\textgreater}, for boosting the consistency and accuracy of pre-trained NLP models using pre-trained natural language inference (NLI) models without fine-tuning or re-training. Given a batch of test inputs, ConCoRD samples several candidate outputs for each input and instantiates a factor graph that accounts for both the model{'}s belief about the likelihood of each answer choice in isolation and the NLI model{'}s beliefs about pair-wise answer choice compatibility. We show that a weighted MaxSAT solver can efficiently compute high-quality answer choices under this factor graph, improving over the raw model{'}s predictions. Our experiments demonstrate that ConCoRD consistently boosts accuracy and consistency of off-the-shelf closed-book QA and VQA models using off-the-shelf NLI models, notably increasing accuracy of LXMERT on ConVQA by 5{\%} absolute. See the project website (https://ericmitchell.ai/emnlp-2022-concord/) for code and data.",
}
@article{helmert2006fast,
  title={The fast downward planning system},
  author={Helmert, Malte},
  journal={Journal of Artificial Intelligence Research},
  volume={26},
  pages={191--246},
  year={2006}
}
@article{gulwani2015inductive,
  title={Inductive programming meets the real world},
  author={Gulwani, Sumit and Hern{\'a}ndez-Orallo, Jos{\'e} and Kitzelmann, Emanuel and Muggleton, Stephen H and Schmid, Ute and Zorn, Benjamin},
  journal={Communications of the ACM},
  volume={58},
  number={11},
  pages={90--99},
  year={2015},
  publisher={ACM New York, NY, USA}
}
@inproceedings{chen2021spreadsheetcoder,
  title={Spreadsheetcoder: Formula prediction from semi-structured context},
  author={Chen, Xinyun and Maniatis, Petros and Singh, Rishabh and Sutton, Charles and Dai, Hanjun and Lin, Max and Zhou, Denny},
  booktitle={International Conference on Machine Learning},
  pages={1661--1672},
  year={2021},
  organization={PMLR}
}
@article{hinton1995wake,
  title={The" wake-sleep" algorithm for unsupervised neural networks},
  author={Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
  journal={Science},
  volume={268},
  number={5214},
  pages={1158--1161},
  year={1995},
  publisher={American Association for the Advancement of Science}
}
@article{chen2023teaching,
  title={Teaching Large Language Models to Self-Debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}
@article{welleck2022generating,
  title={Generating Sequences by Learning to Self-Correct},
  author={Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  journal={ICLR},
  year={2023}
}
@article{sun2023adaplanner,
      title={AdaPlanner: Adaptive Planning from Feedback with Language Models}, 
      author={Haotian Sun and Yuchen Zhuang and Lingkai Kong and Bo Dai and Chao Zhang},
      year={2023},
      journal={NeurIPS}
}
@inproceedings{deckard:icml23,
  author = {Kolby Nottingham and Prithviraj Ammanabrolu and Alane Suhr and Yejin Choi and Hannaneh Hajishirzi and Sameer Singh and Roy Fox},
  title = { {Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling} },
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2023}
}
@article{ellis2023human,
      title={Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language}, 
      author={Kevin Ellis},
      year={2023},
      journal={NeurIPS}
}
@inproceedings{saycan2022arxiv,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    booktitle={arXiv preprint arXiv:2204.01691},
    year={2022}
}
@article{qiu2023phenomenal,
      title={Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement}, 
      author={Linlu Qiu and Liwei Jiang and Ximing Lu and Melanie Sclar and Valentina Pyatkin and Chandra Bhagavatula and Bailin Wang and Yoon Kim and Yejin Choi and Nouha Dziri and Xiang Ren},
      year={2024},
      journal={ICLR}
}
@inproceedings{
liu2023is,
title={Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and LINGMING ZHANG},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=1qvx610Cu7}
}
@article{wang2023hypothesis,
      title={Hypothesis Search: Inductive Reasoning with Language Models}, 
      author={Ruocheng Wang and Eric Zelikman and Gabriel Poesia and Yewen Pu and Nick Haber and Noah D. Goodman},
      year={2024},
      journal={ICLR}
}
@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}
@article{cropper2020inductive,
  title={Inductive general game playing},
  author={Cropper, Andrew and Evans, Richard and Law, Mark},
  journal={Machine Learning},
  volume={109},
  pages={1393--1434},
  year={2020},
  publisher={Springer}
}
@article{ghahramani2005bayesian,
  title={Bayesian sets},
  author={Ghahramani, Zoubin and Heller, Katherine A},
  journal={Advances in neural information processing systems},
  volume={18},
  year={2005}
}
@ARTICLE{9293398, author={G. {Sharma} and R. {Goyal} and D. {Liu} and E. {Kalogerakis} and S. {Maji}}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, title={Neural Shape Parsers for Constructive Solid Geometry}, year={2020}, volume={}, number={}, pages={1-1}, doi={10.1109/TPAMI.2020.3044749}}
@InProceedings{Sharma_2018_CVPR,
author = {Sharma, Gopal and Goyal, Rishabh and Liu, Difan and Kalogerakis, Evangelos and Maji, Subhransu},
title = {CSGNet: Neural Shape Parser for Constructive Solid Geometry},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}
@article{fawzi2022discovering,
  title={Discovering faster matrix multiplication algorithms with reinforcement learning},
  author={Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others},
  journal={Nature},
  volume={610},
  number={7930},
  pages={47--53},
  year={2022},
  publisher={Nature Publishing Group}
}
@article{jones2022PLAD,
  title={PLAD: Learning to Infer Shape Programs with Pseudo-Labels and Approximate Distributions},
  author={Jones, R. Kenny and Walke, Homer and Ritchie, Daniel},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}
@inproceedings{kania2020ucsgnet,
                        author = {Kania, Kacper
                                and Zi{\k{e}}ba, Maciej
                                and Kajdanowicz, Tomasz},
                        title = {UCSG-Net -- Unsupervised Discovering 
                                of Constructive Solid Geometry Tree},
                        booktitle = {arXiv},
                        year={2020}
                    }

@article{verma2019imitation,
  title={Imitation-projected programmatic reinforcement learning},
  author={Verma, Abhinav and Le, Hoang and Yue, Yisong and Chaudhuri, Swarat},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{qiu2021programmatic,
  title={Programmatic Reinforcement Learning without Oracles},
  author={Qiu, Wenjie and Zhu, He},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{inala2019synthesizing,
  title={Synthesizing programmatic policies that inductively generalize},
  author={Inala, Jeevana Priya and Bastani, Osbert and Tavares, Zenna and Solar-Lezama, Armando},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{verma2018programmatically,
  title={Programmatically interpretable reinforcement learning},
  author={Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle={International Conference on Machine Learning},
  pages={5045--5054},
  year={2018},
  organization={PMLR}
}
@inproceedings{mildenhall2020nerf,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}
@article{harnad1990symbol,
  title={The symbol grounding problem},
  author={Harnad, Stevan},
  journal={Physica D: Nonlinear Phenomena},
  volume={42},
  number={1-3},
  pages={335--346},
  year={1990},
  publisher={Elsevier}
}
@inproceedings{wang2019satnet,
  title={Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver},
  author={Wang, Po-Wei and Donti, Priya and Wilder, Bryan and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={6545--6554},
  year={2019},
  organization={PMLR}
}
@inproceedings{gaunt2017differentiable,
  title={Differentiable programs with neural libraries},
  author={Gaunt, Alexander L and Brockschmidt, Marc and Kushman, Nate and Tarlow, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={1213--1222},
  year={2017},
  organization={PMLR}
}
@inproceedings{NEURIPS2021_5c5a93a0,
 author = {Cui, Guofeng and Zhu, He},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11123--11135},
 publisher = {Curran Associates, Inc.},
 title = {Differentiable Synthesis of Program Architectures},
 url = {https://proceedings.neurips.cc/paper/2021/file/5c5a93a042235058b1ef7b0ac1e11b67-Paper.pdf},
 volume = {34},
 year = {2021}
}
@misc{xu2023llms,
      title={LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations}, 
      author={Yudong Xu and Wenhao Li and Pashootan Vaezipoor and Scott Sanner and Elias B. Khalil},
      year={2023},
      eprint={2305.18354},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{piantadosi2021computational,
  title={The computational origin of representation},
  author={Piantadosi, Steven T},
  journal={Minds and machines},
  volume={31},
  pages={1--58},
  year={2021},
  publisher={Springer}
}
@article{silver2010monte,
  title={Monte-Carlo planning in large POMDPs},
  author={Silver, David and Veness, Joel},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@inproceedings{DBLP:conf/sp/PearceA0DK22,
  author    = {Hammond Pearce and
               Baleegh Ahmad and
               Benjamin Tan and
               Brendan Dolan{-}Gavitt and
               Ramesh Karri},
  title     = {Asleep at the Keyboard? Assessing the Security of GitHub Copilot's
               Code Contributions},
  booktitle = {43rd {IEEE} Symposium on Security and Privacy, {SP} 2022, San Francisco,
               CA, USA, May 22-26, 2022},
  pages     = {754--768},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/SP46214.2022.9833571},
  doi       = {10.1109/SP46214.2022.9833571},
  timestamp = {Wed, 07 Dec 2022 23:10:41 +0100},
  biburl    = {https://dblp.org/rec/conf/sp/PearceA0DK22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{researchnight,
 year = {2023},
 title = {ACSU Research Night Spring 2023},
 howpublished={\url{https://www.cs.cornell.edu/events/research-night-spring-2023}}
}
@misc{copilot,
 year = {2021},
 title = {Github Copilot},
 howpublished={\url{https://github.com/features/copilot}}
}
@misc{arcathon,
 year = {2023},
 title = {ARCathon 2023: Global AI Competition},
 howpublished={\url{https://lab42.global/arcathon/}}
}
@article{carey2000origin,
  title={The origin of concepts},
  author={Carey, Susan},
  journal={Journal of Cognition and Development},
  volume={1},
  number={1},
  pages={37--41},
  year={2000},
  publisher={Taylor \& Francis}
}
@misc{kaggle,
 author = {Francois Chollet},
 year = {2019},
 title = {Abstraction and Reasoning Challenge},
 journal = {Kaggle},
 howpublished={\url{https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge}}
}
@article{yin2022natural,
  title={Natural language to code generation in interactive data science notebooks},
  author={Yin, Pengcheng and Li, Wen-Ding and Xiao, Kefan and Rao, Abhishek and Wen, Yeming and Shi, Kensen and Howland, Joshua and Bailey, Paige and Catasta, Michele and Michalewski, Henryk and others},
  journal={ACL},
  year={2023}
}
@misc{wang2023selfinstruct,
      title={Self-Instruct: Aligning Language Models with Self-Generated Instructions}, 
      author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
      year={2023},
      eprint={2212.10560},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}
@misc{sonic,
 year = {2023},
 title = {SoNIC, Cornell Bowers CIS DEI},
 howpublished={\url{https://diversity.cis.cornell.edu/programs/sonic/}}
}
@misc{bure,
 year = {2023},
 title = {Cornell Bowers CIS Undergraduate Research Experience (BURE)},
 howpublished={\url{https://www.cs.cornell.edu/undergrad/uresch/cornell-bowers-cis-undergraduate-research-experience-bure}}
}
@article{shinn2023reflexion,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023}
}
@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}
@software{artifact,
author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sabl\'{e}-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {DreamCoder Software and Data},
year = {2021},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3410302},
abstract = {
    <p>Source code for DreamCoder, pretrained checkpoints, and documentation</p>

},
keywords = {artificial intelligence, deep learning, program synthesis}
}
@misc{langchain,
 author = {Harrison Chase},
 year = {2023},
 title = {LangChain},
 journal = {GitHub},
 howpublished={\url{https://github.com/hwchase17/langchain}}
}
@Online{nyt,
 author = {Siobhan Roberts},
 year = {2022},
 title = {Is Geometry Language That Only Humans Know?},
 journal = {The New York Times},
 url = {https://www.nytimes.com/2022/03/22/science/geometry-math-brain-primates.html},
 urldate = {2022-03-22}
}
@Online{mitnews,
 author = {Adam Zewe},
 year = {2022},
 title = {{AI} that can learn the patterns of human language},
 journal = {MIT News},
 url = {https://news.mit.edu/2022/ai-learn-patterns-language-0830},
 urldate = {2022-08-30}
}
@article{cropper2022inductive,
  title={Inductive logic programming at 30},
  author={Cropper, Andrew and Duman{\v{c}}i{\'c}, Sebastijan and Evans, Richard and Muggleton, Stephen H},
  journal={Machine Learning},
  pages={1--26},
  year={2022},
  publisher={Springer}
}
@article{10.1145/3571234,
author = {Bowers, Matthew and Olausson, Theo X. and Wong, Lionel and Grand, Gabriel and Tenenbaum, Joshua B. and Ellis, Kevin and Solar-Lezama, Armando},
title = {Top-Down Synthesis for Library Learning},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571234},
doi = {10.1145/3571234},
abstract = {This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch’s scalability on corpora containing hundreds of complex programs that are intractable with prior deductive approaches and show empirically that it is robust to terminating the search procedure early—further allowing it to scale to challenging datasets by means of early stopping.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {41},
numpages = {32},
keywords = {Abstraction Learning, Library Learning, Program Synthesis}
}
@article{kalyan2018neural,
  title={Neural-guided deductive search for real-time program synthesis from examples},
  author={Kalyan, Ashwin and Mohta, Abhishek and Polozov, Oleksandr and Batra, Dhruv and Jain, Prateek and Gulwani, Sumit},
  journal={arXiv preprint arXiv:1804.01186},
  year={2018}
}
@article{korf1990real,
  title={Real-time heuristic search},
  author={Korf, Richard E},
  journal={Artificial intelligence},
  volume={42},
  number={2-3},
  pages={189--211},
  year={1990},
  publisher={Elsevier}
}
@inproceedings{10.5555/3495724.3497208,
author = {Gupta, Kavi and Christensen, Peter Ebert and Chen, Xinyun and Song, Dawn},
title = {Synthesize, execute and debug: learning to repair for neural program synthesis},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The use of deep learning techniques has achieved significant progress for program synthesis from input-output examples. However, when the program semantics become more complex, it still remains a challenge to synthesize programs that are consistent with the specification. In this work, we propose SED, a neural program generation framework that incorporates synthesis, execution, and debugging stages. Instead of purely relying on the neural program synthesizer to generate the final program, SED first produces initial programs using the neural program synthesizer component, then utilizes a neural program debugger to iteratively repair the generated programs. The integration of the debugger component enables SED to modify the programs based on the execution results and specification, which resembles the coding process of human programmers. On Karel, a challenging input-output program synthesis benchmark, SED reduces the error rate of the neural program synthesizer itself by a considerable margin, and outperforms the standard beam search for decoding.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1484},
numpages = {11},
location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {NIPS '20}
}
@misc{ni2024next,
      title={NExT: Teaching Large Language Models to Reason about Code Execution}, 
      author={Ansong Ni and Miltiadis Allamanis and Arman Cohan and Yinlin Deng and Kensen Shi and Charles Sutton and Pengcheng Yin},
      year={2024},
      eprint={2404.14662},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{balog2017deepcoder,
title={DeepCoder: Learning to Write Programs},
author={Matej Balog and Alexander L. Gaunt and Marc Brockschmidt and Sebastian Nowozin and Daniel Tarlow},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=ByldLrqlx}
}
@article{yao2023react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={ICLR},
  year={2023}
}
@inproceedings{kujanp2023hybrid,
title={Hybrid Search for Efficient Planning with Completeness Guarantees},
author={Kalle Kujanp{\"a}{\"a} and Joni Pajarinen and Alexander Ilin},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=bY0c46ZtXa}
}
@inproceedings{chung2023thinker,
title={Thinker: Learning to Plan and Act},
author={Stephen Chung and Ivan Anokhin and David Krueger},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=mumEBl0arj}
}
@article{feng2020solving,
  title={Solving hard AI planning instances using curriculum-driven deep reinforcement learning},
  author={Feng, Dieqiao and Gomes, Carla P and Selman, Bart},
  journal={IJCAI},
  year={2020}
}
@inproceedings{chrestien2023optimize,
title={Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal},
author={Leah Chrestien and Stefan Edelkamp and Antonin Komenda and Tom{\'a}{\v{s}} Pevn{\'y}},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Mgy6sgslPY}
}
@inproceedings{NEURIPS2020_2051bd70,
 author = {Feng, Dieqiao and Gomes, Carla P and Selman, Bart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3141--3152},
 publisher = {Curran Associates, Inc.},
 title = {A Novel Automated Curriculum Strategy to Solve Hard Sokoban Planning Instances},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/2051bd70fc110a2208bdbd4a743e7f79-Paper.pdf},
 volume = {33},
 year = {2020}
}
@book{vygotsky1980mind,
  title={Mind in society: The development of higher psychological processes},
  author={Vygotsky, Lev Semenovich},
  year={1980},
  publisher={Harvard university press}
}
@misc{feng2022left,
      title={Left Heavy Tails and the Effectiveness of the Policy and Value Networks in DNN-based best-first search for Sokoban Planning}, 
      author={Dieqiao Feng and Carla Gomes and Bart Selman},
      year={2022},
      eprint={2206.14298},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{goyal2021neural,
  title={Neural Production Systems: Learning Rule-Governed Visual Dynamics},
  author={Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas and Mozer, Michael and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2103.01937},
  year={2021}
}
@misc{sehgal2023neurosymbolic,
      title={Neurosymbolic Grounding for Compositional World Models}, 
      author={Atharva Sehgal and Arya Grayeli and Jennifer J. Sun and Swarat Chaudhuri},
      year={2023},
      eprint={2310.12690},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{hu2023look,
      title={Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning},
      author={Yingdong Hu and Fanqi Lin and Tong Zhang and Li Yi and Yang Gao},
      journal={arXiv preprint arXiv:2311.17842},
      year={2023}
    }
@article{cropper2021learning,
  title={Learning programs by learning from failures},
  author={Cropper, Andrew and Morel, Rolf},
  journal={Machine Learning},
  volume={110},
  pages={801--856},
  year={2021},
  publisher={Springer}
}
@article{ma2023eureka,
    title   = {Eureka: Human-Level Reward Design via Coding Large Language Models},
    author  = {Yecheng Jason Ma and William Liang and Guanzhi Wang and De-An Huang and Osbert Bastani and Dinesh Jayaraman and Yuke Zhu and Linxi Fan and Anima Anandkumar},
    year    = {2023},
    journal = {arXiv preprint arXiv: Arxiv-2310.12931}
}
@misc{law2020ilasp,
      title={The ILASP system for Inductive Learning of Answer Set Programs}, 
      author={Mark Law and Alessandra Russo and Krysia Broda},
      year={2020},
      eprint={2005.00904},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{konidaris2018skills,
  title={From skills to symbols: Learning symbolic representations for abstract high-level planning},
  author={Konidaris, George and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={215--289},
  year={2018}
}
@article{mao2022pdsketch,
  title={PDSketch: Integrated Domain Programming, Learning, and Planning},
  author={Mao, Jiayuan and Lozano-P{\'e}rez, Tom{\'a}s and Tenenbaum, Josh and Kaelbling, Leslie},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36972--36984},
  year={2022}
}
@inproceedings{kansky2017schema,
  title={Schema networks: Zero-shot transfer with a generative causal model of intuitive physics},
  author={Kansky, Ken and Silver, Tom and M{\'e}ly, David A and Eldawy, Mohamed and L{\'a}zaro-Gredilla, Miguel and Lou, Xinghua and Dorfman, Nimrod and Sidor, Szymon and Phoenix, Scott and George, Dileep},
  booktitle={International conference on machine learning},
  pages={1809--1818},
  year={2017},
  organization={PMLR}
}
@article{saad2019bayesian,
  title={Bayesian synthesis of probabilistic programs for automatic data modeling},
  author={Saad, Feras A and Cusumano-Towner, Marco F and Schaechtle, Ulrich and Rinard, Martin C and Mansinghka, Vikash K},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={POPL},
  pages={1--32},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@article{tolman1930introduction,
  title={Introduction and removal of reward, and maze performance in rats.},
  author={Tolman, Edward Chace and Honzik, Charles H},
  journal={University of California publications in psychology},
  year={1930}
}
@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}
@misc{key2022i,
      title={I Speak, You Verify: Toward Trustworthy Neural Program Synthesis}, 
      author={Darren Key and Wen-Ding Li and Kevin Ellis},
      year={2022},
      eprint={2210.00848},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@article{goodman2016pragmatic,
  title={Pragmatic language interpretation as probabilistic inference},
  author={Goodman, Noah D and Frank, Michael C},
  journal={Trends in cognitive sciences},
  volume={20},
  number={11},
  pages={818--829},
  year={2016},
  publisher={Elsevier}
}
@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={Nature},
  year={2022}
}
@article{stengel2024regal,
  title={ReGAL: Refactoring Programs to Discover Generalizable Abstractions},
  author={Stengel-Eskin, Elias and Prasad, Archiki and Bansal, Mohit},
  journal={arXiv preprint arXiv:2401.16467},
  year={2024}
}
@article{turtle,
author = {David D. Thornburg},
title = {Friends of the Turtle},
journal = {Compute!},
year = {1983},
month = {March}
}
@article{raymond1964formal,
  title={A formal theory of inductive inference i},
  author={Raymond, J Solomono},
  journal={Information and Control},
  volume={7},
  pages={1--22},
  year={1964}
}
@inproceedings{bovsnjak2017programming,
  title={Programming with a differentiable forth interpreter},
  author={Bo{\v{s}}njak, Matko and Rockt{\"a}schel, Tim and Naradowsky, Jason and Riedel, Sebastian},
  booktitle={International conference on machine learning},
  pages={547--556},
  year={2017},
  organization={PMLR}
}
@inproceedings{Wong2021LeveragingLT,
  title={Leveraging Language to Learn Program Abstractions and Search Heuristics},
  author={Catherine Wong and Kevin Ellis and Joshua B. Tenenbaum and Jacob Andreas},
  booktitle={ICML},
  year={2021}
}
@book{russell2010artificial,
  title={Artificial intelligence a modern approach},
  author={Russell, Stuart J},
  year={2020},
  publisher={Pearson Education, Inc.}
}
@article{rodriguez2022rlang,
  title={RLang: A Declarative Language for Expression Prior Knowledge for Reinforcement Learning},
  author={Rodriguez-Sanchez, Rafael and Spiegel, Benjamin and Wang, Jennifer and Patel, Roma and Tellex, Stefanie and Konidaris, George},
  journal={arXiv preprint arXiv:2208.06448},
  year={2022}
}
@article{evans2021making,
  title={Making sense of raw input},
  author={Evans, Richard and Bo{\v{s}}njak, Matko and Buesing, Lars and Ellis, Kevin and Pfau, David and Kohli, Pushmeet and Sergot, Marek},
  journal={Artificial Intelligence},
  volume={299},
  pages={103521},
  year={2021},
  publisher={Elsevier}
}
@article{rex,
      title={Code Repair with LLMs gives an Exploration-Exploitation Tradeoff}, 
      author={Hao Tang and Keya Hu and Jin Peng Zhou and Sicheng Zhong and Wei-Long Zheng and Xujie Si and Kevin Ellis},
      year={2024},
      journal={arXiv}
}
@misc{fleet,
  author = {Steven Piantadosi},
  title = {Fleet},
  howpublished = "\url{https://github.com/piantado/Fleet/}",
  year = {2023}, 
  note = "[Online GitHub repository]"
}
@article{chib1995understanding,
  title={Understanding the metropolis-hastings algorithm},
  author={Chib, Siddhartha and Greenberg, Edward},
  journal={The american statistician},
  volume={49},
  number={4},
  pages={327--335},
  year={1995},
  publisher={Taylor \& Francis}
}
@inproceedings{torlak2013growing,
  title={Growing solver-aided languages with Rosette},
  author={Torlak, Emina and Bodik, Rastislav},
  booktitle={Proceedings of the 2013 ACM international symposium on New ideas, new paradigms, and reflections on programming \& software},
  pages={135--152},
  year={2013}
}
@article{wang2017program,
  title={Program synthesis using abstraction refinement},
  author={Wang, Xinyu and Dillig, Isil and Singh, Rishabh},
  journal={Proceedings of the ACM on Programming Languages},
  volume={2},
  number={POPL},
  pages={1--30},
  year={2017},
  publisher={ACM New York, NY, USA}
}
@article{settles2009active,
  title={Active learning literature survey},
  author={Settles, Burr},
  year={2009},
  publisher={University of Wisconsin-Madison Department of Computer Sciences}
}
@inproceedings{diuk2008object,
  title={An object-oriented representation for efficient reinforcement learning},
  author={Diuk, Carlos and Cohen, Andre and Littman, Michael L},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={240--247},
  year={2008}
}
@article{tong2001support,
  title={Support vector machine active learning with applications to text classification},
  author={Tong, Simon and Koller, Daphne},
  journal={Journal of machine learning research},
  volume={2},
  number={Nov},
  pages={45--66},
  year={2001}
}
@article{spelke2007core,
  title={Core knowledge},
  author={Spelke, Elizabeth S and Kinzler, Katherine D},
  journal={Developmental science},
  volume={10},
  number={1},
  pages={89--96},
  year={2007},
  publisher={Wiley Online Library}
}
@article{xu2022graphs,
  title={Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus},
  author={Xu, Yudong and Khalil, Elias B and Sanner, Scott},
  journal={arXiv preprint arXiv:2210.09880},
  year={2022}
}
@article{acquaviva2021communicating,
  title={Communicating Natural Programs to Humans and Machines},
  author={Acquaviva, Samuel and Pu, Yewen and Kryven, Marta and Wong, Catherine and Ecanow, Gabrielle E and Nye, Maxwell and Sechopoulos, Theodoros and Tessler, Michael Henry and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2106.07824},
  year={2021}
}
@article{mao2019neuro,
  title={The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision},
  author={Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B and Wu, Jiajun},
  journal={ICLR},
  year={2019}
}
@article{cambronero2023flashfill++,
  title={Flashfill++: Scaling programming by example by cutting to the chase},
  author={Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Perelman, Daniel and Radhakrishna, Arjun and Simon, Clint and Tiwari, Ashish},
  journal={Proceedings of the ACM on Programming Languages},
  volume={7},
  number={POPL},
  pages={952--981},
  year={2023},
  publisher={ACM New York, NY, USA}
}
@article{lau2003programming,
  title={Programming by demonstration using version space algebra},
  author={Lau, Tessa and Wolfman, Steven A and Domingos, Pedro and Weld, Daniel S},
  journal={Machine Learning},
  volume={53},
  pages={111--156},
  year={2003},
  publisher={Springer}
}
@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA}
}
@article{topan2021techniques,
  title={Techniques for Symbol Grounding with SATNet},
  author={Topan, Sever and Rolnick, David and Si, Xujie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20733--20744},
  year={2021}
}
@book{solar2008program,
  title={Program synthesis by sketching},
  author={Solar-Lezama, Armando},
  year={2008},
  publisher={University of California, Berkeley}
}
@inproceedings{DBLP:conf/iclr/MaddisonMT17,
  author    = {Chris J. Maddison and
               Andriy Mnih and
               Yee Whye Teh},
  title     = {The Concrete Distribution: {A} Continuous Relaxation of Discrete Random
               Variables},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=S1jE5L5gl},
  timestamp = {Thu, 25 Jul 2019 14:26:01 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MaddisonMT17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{mo2019partnet,
  title={Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding},
  author={Mo, Kaichun and Zhu, Shilin and Chang, Angel X and Yi, Li and Tripathi, Subarna and Guibas, Leonidas J and Su, Hao},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={909--918},
  year={2019}
}
@article{zuidema2020five,
  title={Five ways in which computational modeling can help advance cognitive science: Lessons from artificial grammar learning},
  author={Zuidema, Willem and French, Robert M and Alhama, Raquel G and Ellis, Kevin and O'Donnell, Timothy J and Sainburg, Tim and Gentner, Timothy Q},
  journal={Topics in cognitive science},
  volume={12},
  number={3},
  pages={925--941},
  year={2020},
  publisher={Wiley Online Library}
}
@article{polikarpova2016program,
  title={Program synthesis from polymorphic refinement types},
  author={Polikarpova, Nadia and Kuraj, Ivan and Solar-Lezama, Armando},
  journal={ACM SIGPLAN Notices},
  volume={51},
  number={6},
  pages={522--538},
  year={2016},
  publisher={ACM New York, NY, USA}
}
@article{pending,
  title={Synthesizing Theories of Human Language With Bayesian Program Induction},
  author={Ellis, Kevin and Albright, Adam and Solar-Lezama, Armando and Tenenbaum, Joshua and O'Donnell,  Timothy J},
  journal={Nature Communications},
  year={2022},
}
@article{rule2024symbolic,
  title = {Symbolic metaprogram search improves learning efficiency and explains rule learning in humans},
  author = {Rule, Joshua S. and Piantadosi, Steven T. and Cropper, Andrew and Ellis, Kevin and Nye, Maxwell and Tenenbaum, Joshua B.},
  year = {2024},
  journal = {Nature Communications}
}
@article{Chris,
  title={Phonological Interactions, Process Types, and Minimum Description Length Principles},
  author={Yang, Christopher and Ellis, Kevin},
  publisher={Cognitive Science},
year={2021}
}
@article{sable2021language,
  title={A language of thought for the mental representation of geometric shapes},
  author={Sabl{\'e}-Meyer, Mathias and Ellis, Kevin and Tenenbaum, Joshua and Dehaene, Stanislas},
  publisher={Cognitive Psychology},
year={2021}
}
@article{tian2020learning,
  title={Learning abstract structure for drawing by efficient motor program induction},
  author={Tian, Lucas and Ellis, Kevin and Kryven, Marta and Tenenbaum, Josh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2686--2697},
  year={2020}
}
@inproceedings{xiang2020sapien,
  title={Sapien: A simulated part-based interactive environment},
  author={Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11097--11107},
  year={2020}
}
@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5828--5839},
  year={2017}
}
@article{ellis2023dreamcoder,
  title={DreamCoder: growing generalizable, interpretable knowledge with wake--sleep Bayesian program learning},
  author={Ellis, Kevin and Wong, Lionel and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Anaya Pozo, Lore and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  journal={Philosophical Transactions of the Royal Society A},
  volume={381},
  number={2251},
  pages={20220050},
  year={2023},
  publisher={The Royal Society}
}
@inproceedings{NEURIPS2018_7aa685b3,
 author = {Ellis, Kevin and Morales, Lucas and Sabl\'{e}-Meyer, Mathias and Solar-Lezama, Armando and Tenenbaum, Josh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Libraries of Subroutines for Neurally\textendash Guided Bayesian Program Induction},
 url = {https://proceedings.neurips.cc/paper/2018/file/7aa685b3b1dc1d6780bf36f7340078c9-Paper.pdf},
 volume = {31},
 year = {2018}
}
@article{dehaene2006core,
  title={Core knowledge of geometry in an Amazonian indigene group},
  author={Dehaene, Stanislas and Izard, V{\'e}ronique and Pica, Pierre and Spelke, Elizabeth},
  journal={Science},
  volume={311},
  number={5759},
  pages={381--384},
  year={2006},
  publisher={American Association for the Advancement of Science}
}
@article{marcus1999rule,
  title={Rule learning by seven-month-old infants},
  author={Marcus, Gary F and Vijayan, Sugumaran and Bandi Rao, Shoba and Vishton, Peter M},
  journal={Science},
  volume={283},
  number={5398},
  pages={77--80},
  year={1999},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{jha2010oracle,
  title={Oracle-guided component-based program synthesis},
  author={Jha, Susmit and Gulwani, Sumit and Seshia, Sanjit A and Tiwari, Ashish},
  booktitle={2010 ACM/IEEE 32nd International Conference on Software Engineering},
  volume={1},
  pages={215--224},
  year={2010},
  organization={IEEE}
}
@article{piantadosi2012bootstrapping,
  title={Bootstrapping in a language of thought: A formal model of numerical concept learning},
  author={Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
  journal={Cognition},
  volume={123},
  number={2},
  pages={199--217},
  year={2012},
  publisher={Elsevier}
}
@InProceedings{pmlr-v139-vedantam21a,
  title = 	 {CURI: A Benchmark for Productive Concept Learning Under Uncertainty},
  author =       {Vedantam, Ramakrishna and Szlam, Arthur and Nickel, Maximillian and Morcos, Ari and Lake, Brenden M},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10519--10529},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/vedantam21a/vedantam21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/vedantam21a.html},
  abstract = 	 {Humans can learn and reason under substantial uncertainty in a space of infinitely many compositional, productive concepts. For example, if a scene with two blue spheres qualifies as “daxy,” one can reason that the underlying concept may require scenes to have “only blue spheres” or “only spheres” or “only two objects.” In contrast, standard benchmarks for compositional reasoning do not explicitly capture a notion of reasoning under uncertainty or evaluate compositional concept acquisition. We introduce a new benchmark, Compositional Reasoning Under Uncertainty (CURI) that instantiates a series of few-shot, meta-learning tasks in a productive concept space to evaluate different aspects of systematic generalization under uncertainty, including splits that test abstract understandings of disentangling, productive generalization, learning boolean operations, variable binding, etc. Importantly, we also contribute a model-independent “compositionality gap” to evaluate the difficulty of generalizing out-of-distribution along each of these axes, allowing objective comparison of the difficulty of each compositional split. Evaluations across a range of modeling choices and splits reveal substantial room for improvement on the proposed benchmark.}
}

@inproceedings{kumar2022using,
title={Using natural language and program abstractions to instill human inductive biases in machines},
author={Sreejan Kumar and Carlos G Correa and Ishita Dasgupta and Raja Marjieh and Michael Hu and Robert D. Hawkins and Jonathan Cohen and Nathaniel Daw and Karthik R Narasimhan and Thomas L. Griffiths},
booktitle={NeurIPS},
year={2022},
url={https://openreview.net/forum?id=buXZ7nIqiwE}
}
@article{li2023starcoder,
  title={StarCoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}
@article{Nijkamp2022CG,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint},
  year={2022}
}
@article{king2004functional,
  title={Functional genomic hypothesis generation and experimentation by a robot scientist},
  author={King, Ross D and Whelan, Kenneth E and Jones, Ffion M and Reiser, Philip GK and Bryant, Christopher H and Muggleton, Stephen H and Kell, Douglas B and Oliver, Stephen G},
  journal={Nature},
  volume={427},
  number={6971},
  pages={247--252},
  year={2004},
  publisher={Nature Publishing Group UK London}
}
@misc{rodriguezsanchez2023rlang,
      title={RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents}, 
      author={Rafael Rodriguez-Sanchez and Benjamin A. Spiegel and Jennifer Wang and Roma Patel and Stefanie Tellex and George Konidaris},
      year={2023},
      eprint={2208.06448},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@inproceedings{micheli2023transformers,
title={Transformers are Sample-Efficient World Models},
author={Vincent Micheli and Eloi Alonso and Fran{\c{c}}ois Fleuret},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=vhFu1Acb0xb}
}
@article{osera2015type,
  title={Type-and-example-directed program synthesis},
  author={Osera, Peter-Michael and Zdancewic, Steve},
  journal={ACM SIGPLAN Notices},
  volume={50},
  number={6},
  pages={619--630},
  year={2015},
  publisher={ACM New York, NY, USA}
}
@article{MinigridMiniworld23,
  author       = {Maxime Chevalier-Boisvert and Bolun Dai and Mark Towers and Rodrigo de Lazcano and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
  title        = {Minigrid \& Miniworld: Modular \& Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  journal      = {CoRR},
  volume       = {abs/2306.13831},
  year         = {2023},
}
@misc{hafner2023mastering,
      title={Mastering Diverse Domains through World Models}, 
      author={Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap},
      year={2023},
      eprint={2301.04104},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{hallak2015contextual,
  title={Contextual markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}
@article{10.1007/s10994-014-5471-y,
author = {Muggleton, Stephen H. and Lin, Dianhuan and Tamaddoni-Nezhad, Alireza},
title = {Meta-Interpretive Learning of Higher-Order Dyadic Datalog: Predicate Invention Revisited},
year = {2015},
issue_date = {July      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {100},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-014-5471-y},
doi = {10.1007/s10994-014-5471-y},
abstract = {Since the late 1990s predicate invention has been under-explored within inductive logic programming due to difficulties in formulating efficient search mechanisms. However, a recent paper demonstrated that both predicate invention and the learning of recursion can be efficiently implemented for regular and context-free grammars, by way of metalogical substitutions with respect to a modified Prolog meta-interpreter which acts as the learning engine. New predicate symbols are introduced as constants representing existentially quantified higher-order variables. The approach demonstrates that predicate invention can be treated as a form of higher-order logical reasoning. In this paper we generalise the approach of meta-interpretive learning (MIL) to that of learning higher-order dyadic datalog programs. We show that with an infinite signature the higher-order dyadic datalog class $$H^2_2$$H22 has universal Turing expressivity though $$H^2_2$$H22 is decidable given a finite signature. Additionally we show that Knuth---Bendix ordering of the hypothesis space together with logarithmic clause bounding allows our MIL implementation Metagol$$_{D}$$D to PAC-learn minimal cardinality $$H^2_2$$H22 definitions. This result is consistent with our experiments which indicate that Metagol$$_{D}$$D efficiently learns compact $$H^2_2$$H22 definitions involving predicate invention for learning robotic strategies, the East---West train challenge and NELL. Additionally higher-order concepts were learned in the NELL language learning domain. The Metagol code and datasets described in this paper have been made publicly available on a website to allow reproduction of results in this paper.},
journal = {Mach. Learn.},
month = {jul},
pages = {49–73},
numpages = {25},
keywords = {Learning recursion, Meta-interpretation, Abduction, Induction, Predicate invention}
}
@article{DBLP:journals/corr/abs-2112-00114,
  author    = {Maxwell I. Nye and
               Anders Johan Andreassen and
               Guy Gur{-}Ari and
               Henryk Michalewski and
               Jacob Austin and
               David Bieber and
               David Dohan and
               Aitor Lewkowycz and
               Maarten Bosma and
               David Luan and
               Charles Sutton and
               Augustus Odena},
  title     = {Show Your Work: Scratchpads for Intermediate Computation with Language
               Models},
  journal   = {CoRR},
  volume    = {abs/2112.00114},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.00114},
  eprinttype = {arXiv},
  eprint    = {2112.00114},
  timestamp = {Fri, 29 Apr 2022 17:42:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-00114.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glanois2022neuro,
  title={Neuro-Symbolic Hierarchical Rule Induction},
  author={Glanois, Claire and Jiang, Zhaohui and Feng, Xuening and Weng, Paul and Zimmer, Matthieu and Li, Dong and Liu, Wulong and Hao, Jianye},
  booktitle={International Conference on Machine Learning},
  pages={7583--7615},
  year={2022},
  organization={PMLR}
}
@inproceedings{Valkov2018HOUDINILL,
  title={HOUDINI: Lifelong Learning as Program Synthesis},
  author={Lazar Valkov and Dipak Chaudhari and Akash Srivastava and Charles Sutton and Swarat Chaudhuri},
  booktitle={NeurIPS},
  year={2018}
}
@inproceedings{robust,
author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
title = {RobustFill: Neural Program Learning under Noisy I/O},
year = {2017},
publisher = {JMLR.org},
abstract = {The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92\% accuracy on a real-world test set, compared to the 34\% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {990–998},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@article{10.1145/3428295,
author = {Barke, Shraddha and Peleg, Hila and Polikarpova, Nadia},
title = {Just-in-time learning for bottom-up enumerative synthesis},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428295},
doi = {10.1145/3428295},
abstract = {A key challenge in program synthesis is the astronomical size of the search space the synthesizer has to explore. In response to this challenge, recent work proposed to guide synthesis using learned probabilistic models. Obtaining such a model, however, might be infeasible for a problem domain where no high-quality training data is available. In this work we introduce an alternative approach to guided program synthesis: instead of training a model ahead of time we show how to bootstrap one just in time, during synthesis, by learning from partial solutions encountered along the way. To make the best use of the model, we also propose a new program enumeration algorithm we dub guided bottom-up search, which extends the efficient bottom-up search with guidance from probabilistic models.  We implement this approach in a tool called Probe, which targets problems in the popular syntax-guided synthesis (SyGuS) format. We evaluate Probe on benchmarks from the literature and show that it achieves significant performance gains both over unguided bottom-up search and over a state-of-the-art probability-guided synthesizer, which had been trained on a corpus of existing solutions. Moreover, we show that these performance gains do not come at the cost of solution quality: programs generated by Probe are only slightly more verbose than the shortest solutions and perform no unnecessary case-splitting.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {227},
numpages = {29},
keywords = {Domain-specific languages, Probabilistic models, Program Synthesis}
}
@article{friston2012history,
  title={The history of the future of the Bayesian brain},
  author={Friston, Karl},
  journal={NeuroImage},
  volume={62},
  number={2},
  pages={1230--1233},
  year={2012},
  publisher={Elsevier}
}
@article{bowers2012bayesian,
  title={Bayesian just-so stories in psychology and neuroscience.},
  author={Bowers, Jeffrey S and Davis, Colin J},
  journal={Psychological bulletin},
  volume={138},
  number={3},
  pages={389},
  year={2012},
  publisher={American Psychological Association}
}
@article{marcus2013robust,
  title={How robust are probabilistic models of higher-level cognition?},
  author={Marcus, Gary F and Davis, Ernest},
  journal={Psychological science},
  volume={24},
  number={12},
  pages={2351--2360},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}
@inproceedings{gershman2014amortized,
  title={Amortized inference in probabilistic reasoning},
  author={Gershman, Samuel and Goodman, Noah},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={36},
  number={36},
  year={2014}
}
@article{si2019synthesizing,
  title={Synthesizing datalog programs using numerical relaxation},
  author={Si, Xujie and Raghothaman, Mukund and Heo, Kihong and Naik, Mayur},
  journal={IJCAI},
  year={2019}
}
@inproceedings{sahoo2018learning,
  title={Learning equations for extrapolation and control},
  author={Sahoo, Subham and Lampert, Christoph and Martius, Georg},
  booktitle={International Conference on Machine Learning},
  pages={4442--4450},
  year={2018},
  organization={PMLR}
}
@inproceedings{genova2020local,
  title={Local deep implicit functions for 3d shape},
  author={Genova, Kyle and Cole, Forrester and Sud, Avneesh and Sarna, Aaron and Funkhouser, Thomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4857--4866},
  year={2020}
}
@article{hertz2022spaghetti,
  title={SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation},
  author={Hertz, Amir and Perel, Or and Giryes, Raja and Sorkine-Hornung, Olga and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2201.13168},
  year={2022}
}
@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}
@inproceedings{shi2023lambdabeam,
title={LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas},
author={Kensen Shi and Hanjun Dai and Wen-Ding Li and Kevin Ellis and Charles Sutton},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=qVMPXrX4FR}
}
@article{shah2020learning,
  title={Learning differentiable programs with admissible neural heuristics},
  author={Shah, Ameesh and Zhan, Eric and Sun, Jennifer and Verma, Abhinav and Yue, Yisong and Chaudhuri, Swarat},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={4940--4952},
  year={2020}
}
@inproceedings{shi2024exedecexecutiondecompositioncompositional,
      title={ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis}, 
      author={Kensen Shi and Joey Hong and Yinlin Deng and Pengcheng Yin and Manzil Zaheer and Charles Sutton},
      year={2024},
      booktitle={International Conference on Learning Representations}, 
}
@inproceedings{shi2021crossbeam,
  title={CrossBeam: Learning to Search in Bottom-Up Program Synthesis},
  author={Shi, Kensen and Dai, Hanjun and Ellis, Kevin and Sutton, Charles},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{tikz,
  title={Learning to infer graphics programs from hand-drawn images},
  author={Ellis, Kevin and Ritchie, Daniel and Solar-Lezama, Armando and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{tian2019learning,
  title={Learning to infer and execute 3d shape programs},
  author={Tian, Yonglong and Luo, Andrew and Sun, Xingyuan and Ellis, Kevin and Freeman, William T and Tenenbaum, Joshua B and Wu, Jiajun},
  journal={ICLR},
  year={2019}
}
@article{drori2022neural,
  title={A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level},
  author={Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={32},
  pages={e2123433119},
  year={2022},
  publisher={National Acad Sciences}
}
@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={ICLR},
  year={2023}
}
@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={ICML},
  year={2023}
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{silver2022predicate,
      title={Predicate Invention for Bilevel Planning}, 
      author={Tom Silver and Rohan Chitnis and Nishanth Kumar and Willie McClinton and Tomas Lozano-Perez and Leslie Pack Kaelbling and Joshua Tenenbaum},
      year={2022},
      eprint={2203.09634},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{chaudhuri2021neurosymbolic,
  title={Neurosymbolic Programming},
  author={Chaudhuri, Swarat and Ellis, Kevin and Polozov, Oleksandr and Singh, Rishabh and Solar-Lezama, Armando and Yue, Yisong and others},
  journal={Foundations and Trends{\textregistered} in Programming Languages},
  volume={7},
  number={3},
  year={2021},
  publisher={Now Publishers, Inc.}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@inproceedings{chen2018execution,
  title={Execution-guided neural program synthesis},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={ICLR},
  year={2018}
}
@inbook{10.5555/3454287.3455109,
author = {Ellis, Kevin and Nye, Maxwell and Pu, Yewen and Sosa, Felix and Tenenbaum, Joshua B. and Solar-Lezama, Armando},
title = {Write, Execute, Assess: Program Synthesis with a REPL},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a neural program synthesis approach integrating components which write, execute, and assess code to navigate the search space of possible programs. We equip the search process with an interpreter or a read-eval-print-loop (REPL), which immediately executes partially written programs, exposing their semantics. The REPL addresses a basic challenge of program synthesis: tiny changes in syntax can lead to huge changes in semantics. We train a pair of models, a policy that proposes the new piece of code to write, and a value function that assesses the prospects of the code written so-far. At test time we can combine these models with a Sequential Monte Carlo algorithm. We apply our approach to two domains: synthesizing text editing programs and inferring 2D and 3D graphics programs.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {822},
numpages = {10}
}
@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}
@article{manna1979synthesis,
  title={Synthesis: dreams→ programs},
  author={Manna, Zohar and Waldinger, Richard},
  journal={IEEE Transactions on Software Engineering},
  number={4},
  pages={294--328},
  year={1979},
  publisher={IEEE}
}
@inproceedings{zhan2020learning,
  title={Learning calibratable policies using programmatic style-consistency},
  author={Zhan, Eric and Tseng, Albert and Yue, Yisong and Swaminathan, Adith and Hausknecht, Matthew},
  booktitle={International Conference on Machine Learning},
  pages={11001--11011},
  year={2020},
  organization={PMLR}
}
@inproceedings{tseng2022automatic,
  title={Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis},
  author={Tseng, Albert and Sun, Jennifer J and Yue, Yisong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2211--2220},
  year={2022}
}
@article{tang2020towards,
  title={Towards scale-invariant graph-related problem solving by iterative homogeneous gnns},
  author={Tang, Hao and Huang, Zhiao and Gu, Jiayuan and Lu, Bao-Liang and Su, Hao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15811--15822},
  year={2020}
}
@inproceedings{bastani2021pac,
  title={PAC Synthesis of Machine Learning Programs},
  author={Bastani, Osbert},
  booktitle={Advances in Programming Languages and Neurosymbolic Systems Workshop},
  year={2021}
}
@book{solar2008program,
  title={Program synthesis by sketching},
  author={Solar-Lezama, Armando},
  year={2008},
  publisher={University of California, Berkeley}
}
@inproceedings{10.1145/1926385.1926423,
author = {Gulwani, Sumit},
title = {Automating String Processing in Spreadsheets Using Input-Output Examples},
year = {2011},
isbn = {9781450304900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1926385.1926423},
doi = {10.1145/1926385.1926423},
abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations.The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.},
booktitle = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {317–330},
numpages = {14},
keywords = {program synthesis, user intent, version space algebra, programming by example (pbe), spreadsheet programming, string manipulation},
location = {Austin, Texas, USA},
series = {POPL '11}
}
@inproceedings{Mao2019Program,
	title={{Program-Guided Image Manipulators}},
	author={Mao, Jiayuan and Zhang, Xiuming and Li, Yikai and Freeman, William T. and Tenenbaum, Joshua B. and Wu, Jiajun},
	booktitle={International Conference on Computer Vision},
	year={2019}
}
@article{10.1145/2858965.2814310,
author = {Polozov, Oleksandr and Gulwani, Sumit},
title = {FlashMeta: A Framework for Inductive Program Synthesis},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814310},
doi = {10.1145/2858965.2814310},
abstract = {Inductive synthesis, or programming-by-examples (PBE) is gaining prominence with disruptive applications for automating repetitive tasks in end-user programming. However, designing, developing, and maintaining an effective industrial-quality inductive synthesizer is an intellectual and engineering challenge, requiring 1-2 man-years of effort. Our novel observation is that many PBE algorithms are a natural fall-out of one generic meta-algorithm and the domain-specific properties of the operators in the underlying domain-specific language (DSL). The meta-algorithm propagates example-based constraints on an expression to its subexpressions by leveraging associated witness functions, which essentially capture the inverse semantics of the underlying operator. This observation enables a novel program synthesis methodology called data-driven domain-specific deduction (D4), where domain-specific insight, provided by the DSL designer, is separated from the synthesis algorithm. Our FlashMeta framework implements this methodology, allowing synthesizer developers to generate an efficient synthesizer from the mere DSL definition (if properties of the DSL operators have been modeled). In our case studies, we found that 10+ existing industrial-quality mass-market applications based on PBE can be cast as instances of D4. Our evaluation includes reimplementation of some prior works, which in FlashMeta become more efficient, maintainable, and extensible. As a result, FlashMeta-based PBE tools are deployed in several industrial products, including Microsoft PowerShell 3.0 for Windows 10, Azure Operational Management Suite, and Microsoft Cortana digital assistant.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {107–126},
numpages = {20},
keywords = {Inductive program synthesis, frameworks, programming by examples, search-based synthesis, deductive inference, domain-specific languages}
}
@inproceedings{ni2023lever,
  title={Lever: Learning to verify language-to-code generation with execution},
  author={Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Ves and Yih, Wen-tau and Wang, Sida I and Lin, Xi Victoria},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML'23)},
  year={2023}
}
@inproceedings{singh2015predicting,
author = {Singh, Rishabh and Gulwani, Sumit},
title = {Predicting a Correct Program in Programming by Example},
booktitle = {27th International Conference on Computer Aided Verification (CAV 2015)},
year = {2015},
month = {July},
abstract = {We study the problem of efficiently predicting a correct program from a large set of programs induced from few input-output examples in Programming-by-Example (PBE) systems. This is an important problem for making PBE systems usable so that users do not need to provide too many examples to learn the desired program. We first formalize the two classes of sharing that occurs in version-space algebra (VSA) based PBE systems, namely set-based sharing and path-based sharing. We then present a supervised machine learning approach for learning a hierarchical ranking function to efficiently predict a correct program. The key observation of our learning approach is that ranking any correct program higher than all incorrect programs is sufficient for generating the correct output on new inputs, which leads to a novel loss function in the gradient descent based learning algorithm. We evaluate our ranking technique for the FlashFill PBE system on over 175 benchmarks obtained from the Excel product team and help forums. Our ranking technique works in real-time, reduces the average number of examples required for learning the desired transformation from 4.17 to 1.48, and learns the transformation from just one input-output example for 74% of the benchmarks. The ranking scheme played a pivotal role in making FlashFill usable for millions of Excel users.},
url = {https://www.microsoft.com/en-us/research/publication/predicting-a-correct-program-in-programming-by-example/},
edition = {27th International Conference on Computer Aided Verification (CAV 2015)},
}
@inproceedings{10.1145/2814270.2814310,
author = {Polozov, Oleksandr and Gulwani, Sumit},
title = {FlashMeta: A Framework for Inductive Program Synthesis},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814310},
doi = {10.1145/2814270.2814310},
abstract = {Inductive synthesis, or programming-by-examples (PBE) is gaining prominence with disruptive applications for automating repetitive tasks in end-user programming. However, designing, developing, and maintaining an effective industrial-quality inductive synthesizer is an intellectual and engineering challenge, requiring 1-2 man-years of effort. Our novel observation is that many PBE algorithms are a natural fall-out of one generic meta-algorithm and the domain-specific properties of the operators in the underlying domain-specific language (DSL). The meta-algorithm propagates example-based constraints on an expression to its subexpressions by leveraging associated witness functions, which essentially capture the inverse semantics of the underlying operator. This observation enables a novel program synthesis methodology called data-driven domain-specific deduction (D4), where domain-specific insight, provided by the DSL designer, is separated from the synthesis algorithm. Our FlashMeta framework implements this methodology, allowing synthesizer developers to generate an efficient synthesizer from the mere DSL definition (if properties of the DSL operators have been modeled). In our case studies, we found that 10+ existing industrial-quality mass-market applications based on PBE can be cast as instances of D4. Our evaluation includes reimplementation of some prior works, which in FlashMeta become more efficient, maintainable, and extensible. As a result, FlashMeta-based PBE tools are deployed in several industrial products, including Microsoft PowerShell 3.0 for Windows 10, Azure Operational Management Suite, and Microsoft Cortana digital assistant.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {107–126},
numpages = {20},
keywords = {search-based synthesis, Inductive program synthesis, frameworks, deductive inference, programming by examples, domain-specific languages},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}


@article{10.1145/1925844.1926423,
author = {Gulwani, Sumit},
title = {Automating String Processing in Spreadsheets Using Input-Output Examples},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/1925844.1926423},
doi = {10.1145/1925844.1926423},
abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations.The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {317–330},
numpages = {14},
keywords = {spreadsheet programming, user intent, string manipulation, programming by example (pbe), program synthesis, version space algebra}
}
@book{ormrod2016human,
  title={Human learning},
  author={Ormrod, Jeanne Ellis},
  year={2016},
  publisher={Pearson Higher Ed}
}
@misc{prose,
title={Microsoft PROSE Public Benchmark Suite},
    year = {2022},
    note = {Available at https://github.com/microsoft/prose-benchmarks}
}
@misc{lew2022recursive,
      title={Recursive Monte Carlo and Variational Inference with Auxiliary Variables}, 
      author={Alexander K. Lew and Marco Cusumano-Towner and Vikash K. Mansinghka},
      year={2022},
      eprint={2203.02836},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ranganath2016hierarchical,
      title={Hierarchical Variational Models}, 
      author={Rajesh Ranganath and Dustin Tran and David M. Blei},
      year={2016},
      eprint={1511.02386},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{li2023starcoder,
  title={StarCoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}
@article{piantadosi2016logical,
  title={The logical primitives of thought: Empirical foundations for compositional cognitive models.},
  author={Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
  journal={Psychological review},
  volume={123},
  number={4},
  pages={392},
  year={2016},
  publisher={American Psychological Association}
}
@article{kemp2008discovery,
  title={The discovery of structural form},
  author={Kemp, Charles and Tenenbaum, Joshua B},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={31},
  pages={10687--10692},
  year={2008},
  publisher={National Acad Sciences}
}
@inproceedings{10.5555/1625275.1625673,
author = {De Raedt, Luc and Kimmig, Angelika and Toivonen, Hannu},
title = {ProbLog: a probabilistic prolog and its application in link discovery},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce ProbLog, a probabilistic extension of Prolog. A ProbLog program defines a distribution over logic programs by specifying for each clause the probability that it belongs to a randomly sampled program, and these probabilities are mutually independent. The semantics of ProbLog is then defined by the success probability of a query, which corresponds to the probability that the query succeeds in a randomly sampled program. The key contribution of this paper is the introduction of an effective solver for computing success probabilities. It essentially combines SLD-resolution with methods for computing the probability of Boolean formulae. Our implementation further employs an approximation algorithm that combines iterative deepening with binary decision diagrams. We report on experiments in the context of discovering links in real biological networks, a demonstration of the practical usefulness of the approach.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2468–2473},
numpages = {6},
location = {Hyderabad, India},
series = {IJCAI'07}
}
@inproceedings{10.5555/3023476.3023503,
author = {Goodman, Noah D. and Mansinghka, Vikash K. and Roy, Daniel and Bonawitz, Keith and Tenenbaum, Joshua B.},
title = {Church: a language for generative models},
year = {2008},
isbn = {0974903949},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Formal languages for probabilistic modeling enable re-use, modularity, and descriptive clarity, and can foster generic inference techniques. We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques.},
booktitle = {Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence},
pages = {220–229},
numpages = {10},
location = {Helsinki, Finland},
series = {UAI'08}
}
@book{Bishop:2006:PRM:1162264,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning},
 year = {2006},
 publisher = {Springer-Verlag New York, Inc.}
} 
@phdthesis{tenenbaum1999bayesian,
  title={A Bayesian framework for concept learning},
  author={Tenenbaum, Joshua Brett},
  year={1999},
  school={Massachusetts Institute of Technology}
}
@phdthesis{feras,
  title={Scalable Structure Learning, Inference, and Analysis with Probabilistic Programs},
  author={Saad, Feras},
  year={2022},
  school={Massachusetts Institute of Technology}
}
@book{murphy2012machine,
  title={Machine learning: a probabilistic perspective},
  author={Murphy, Kevin P},
  year={2012},
  publisher={MIT press}
}
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}
@misc{grand2023lilo,
      title={LILO: Learning Interpretable Libraries by Compressing and Documenting Code}, 
      author={Gabriel Grand and Lionel Wong and Matthew Bowers and Theo X. Olausson and Muxin Liu and Joshua B. Tenenbaum and Jacob Andreas},
      year={2023},
      eprint={2310.19791},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{wang2023voyager,
  title   = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author  = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2305.16291}
}
@article{goodman2008rational,
  title={A rational analysis of rule-based concept learning},
  author={Goodman, Noah D and Tenenbaum, Joshua B and Feldman, Jacob and Griffiths, Thomas L},
  journal={Cognitive science},
  volume={32},
  number={1},
  pages={108--154},
  year={2008},
  publisher={Wiley Online Library}
}
@article{muggleton2018ultra,
  title={Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP},
  author={Muggleton, Stephen H and Schmid, Ute and Zeller, Christina and Tamaddoni-Nezhad, Alireza and Besold, Tarek},
  journal={Machine Learning},
  volume={107},
  number={7},
  pages={1119--1140},
  year={2018},
  publisher={Springer}
}
@article{zhang2022coder,
  title={Coder Reviewer Reranking for Code Generation},
  author={Zhang, Tianyi and Yu, Tao and Hashimoto, Tatsunori B and Lewis, Mike and Yih, Wen-tau and Fried, Daniel and Wang, Sida I},
  journal={arXiv preprint arXiv:2211.16490},
  year={2022}
}
@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  pages={e253},
  year={2017},
  publisher={Cambridge University Press}
}

@article{evans2018learning,
  title={Learning explanatory rules from noisy data},
  author={Evans, Richard and Grefenstette, Edward},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={1--64},
  year={2018}
}
@inbook{10.1145/3453483.3454080,
author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sabl\'{e}-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {DreamCoder: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454080},
abstract = {We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of "wake-sleep" approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {835–850},
numpages = {16}
}
@article{gaunt2016terpret,
  title={TerpreT: A Probabilistic Programming Language for Program Induction},
  author={Gaunt, Alexander L and Brockschmidt, Marc and Singh, Rishabh and Kushman, Nate and Kohli, Pushmeet and Taylor, Jonathan and Tarlow, Daniel},
  journal={arXiv preprint arXiv:1608.04428},
  year={2016}
}
@misc{Mathematica,
  author = {Wolfram Research{,} Inc.},
  title = {Mathematica, {V}ersion 13.3},
  url = {https://www.wolfram.com/mathematica},
  note = {Champaign, IL, 2023}
}
@article{le2021hybrid,
  title={Hybrid memoised wake-sleep: approximate inference at the discrete-continuous interface},
  author={Le, Tuan Anh and Collins, Katherine M and Hewitt, Luke and Ellis, Kevin and Siddharth, N and Gershman, Samuel J and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2107.06393},
  year={2021}
}
@article{graves2016hybrid,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}
@incollection{ha2018worldmodels,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages = {2451--2463},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  note = "\url{https://worldmodels.github.io}",
}
@article{zhao2022toward,
  title={Toward Compositional Generalization in Object-Oriented World Modeling},
  author={Zhao, Linfeng and Kong, Lingzhi and Walters, Robin and Wong, Lawson LS},
  journal={arXiv preprint arXiv:2204.13661},
  year={2022}
}
@inproceedings{klyubin2005all,
  title={All else being equal be empowered},
  author={Klyubin, Alexander S and Polani, Daniel and Nehaniv, Chrystopher L},
  booktitle={European Conference on Artificial Life},
  pages={744--753},
  year={2005},
  organization={Springer}
}
@misc{tsividis2021humanlevel,
      title={Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning}, 
      author={Pedro A. Tsividis and Joao Loula and Jake Burga and Nathan Foss and Andres Campero and Thomas Pouncy and Samuel J. Gershman and Joshua B. Tenenbaum},
      year={2021},
      eprint={2107.12544},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{GIVAN2003163,
title = {Equivalence notions and model minimization in Markov decision processes},
journal = {Artificial Intelligence},
volume = {147},
number = {1},
pages = {163-223},
year = {2003},
note = {Planning with Uncertainty and Incomplete Information},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00376-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202003764},
author = {Robert Givan and Thomas Dean and Matthew Greig},
keywords = {Markov decision processes, State abstraction, Stochastic planning, Bisimulation, Knowledge representation, Factored state spaces},
abstract = {Many stochastic planning problems can be represented using Markov Decision Processes (MDPs). A difficulty with using these MDP representations is that the common algorithms for solving them run in time polynomial in the size of the state space, where this size is extremely large for most real-world planning problems of interest. Recent AI research has addressed this problem by representing the MDP in a factored form. Factored MDPs, however, are not amenable to traditional solution methods that call for an explicit enumeration of the state space. One familiar way to solve MDP problems with very large state spaces is to form a reduced (or aggregated) MDP with the same properties as the original MDP by combining “equivalent” states. In this paper, we discuss applying this approach to solving factored MDP problems—we avoid enumerating the state space by describing large blocks of “equivalent” states in factored form, with the block descriptions being inferred directly from the original factored representation. The resulting reduced MDP may have exponentially fewer states than the original factored MDP, and can then be solved using traditional methods. The reduced MDP found depends on the notion of equivalence between states used in the aggregation. The notion of equivalence chosen will be fundamental in designing and analyzing algorithms for reducing MDPs. Optimally, these algorithms will be able to find the smallest possible reduced MDP for any given input MDP and notion of equivalence (i.e., find the “minimal model” for the input MDP). Unfortunately, the classic notion of state equivalence from non-deterministic finite state machines generalized to MDPs does not prove useful. We present here a notion of equivalence that is based upon the notion of bisimulation from the literature on concurrent processes. Our generalization of bisimulation to stochastic processes yields a non-trivial notion of state equivalence that guarantees the optimal policy for the reduced model immediately induces a corresponding optimal policy for the original model. With this notion of state equivalence, we design and analyze an algorithm that minimizes arbitrary factored MDPs and compare this method analytically to previous algorithms for solving factored MDPs. We show that previous approaches implicitly derive equivalence relations that we define here.}
}
@article{rubenstein2017causal,
  title={Causal consistency of structural equation models},
  author={Rubenstein, Paul K and Weichwald, Sebastian and Bongers, Stephan and Mooij, Joris M and Janzing, Dominik and Grosse-Wentrup, Moritz and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1707.00819},
  year={2017}
}
@inproceedings{bootstrappingcognitiveagent,
  title={Bootstrapping Cognitive Agents with a Large Language Model},
  author={Zhu, Feiyu and Simmons, Reid},
  booktitle={AAAI},
  year={2024}
}
@article{liu2023reason,
      title={Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency},
      author={Liu, Zhihan and Hu, Hao and Zhang, Shenao and Guo, Hongyi and Ke, Shuqi and Liu, Boyi and Wang, Zhaoran},
      journal={arXiv preprint arXiv:2309.17382},
      year={2023}
}
@article{zhao2023large,
  title={Large Language Models as Commonsense Knowledge for Large-Scale Task Planning},
  author={Zhao, Zirui and Lee, Wee Sun and Hsu, David},
  journal={NeurIPS},
  year={2023}
}
@inproceedings{beckers2019abstracting,
  title={Abstracting causal models},
  author={Beckers, Sander and Halpern, Joseph Y},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={2678--2685},
  year={2019}
}
@article{moskvichev2023conceptarc,
  title={The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain},
  author={Moskvichev, Arseny and Odouard, Victor Vikram and Mitchell, Melanie},
  journal={arXiv preprint arXiv:2305.07141},
  year={2023}
}
@article{raychev2016learning,
  title={Learning programs from noisy data},
  author={Raychev, Veselin and Bielik, Pavol and Vechev, Martin and Krause, Andreas},
  journal={ACM Sigplan Notices},
  volume={51},
  number={1},
  pages={761--774},
  year={2016},
  publisher={ACM New York, NY, USA}
}
@inproceedings{weichain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny and others},
  booktitle={Advances in Neural Information Processing Systems}
}
@inproceedings{10.5555/3037176.3037207,
author = {Marthi, Bhaskara and Russell, Stuart and Wolfe, Jason},
title = {Angelic Semantics for High-Level Actions},
year = {2007},
isbn = {9781577353447},
publisher = {AAAI Press},
abstract = {High-level actions (HLAs) lie at the heart of hierarchical planning. Typically, an HLA admits multiple refinements into primitive action sequences. Correct descriptions of the effects of HLAs may be essential to their effective use, yet the literature is mostly silent. We propose an angelic semantics for HLAs, the key concept of which is the set of states reachable by some refinement of a high-level plan, representing uncertainty that will ultimately be resolved in the planning agent's own best interest. We describe upper and lower approximations to these reachable sets, and show that the resulting definition of a high-level solution automatically satisfies the upward and downward refinement properties. We define a STRIPS-like notation for such descriptions. A sound and complete hierarchical planning algorithm is given and its computational benefits are demonstrated.},
booktitle = {Proceedings of the Seventeenth International Conference on International Conference on Automated Planning and Scheduling},
pages = {232–239},
numpages = {8},
location = {Providence, Rhode Island, USA},
series = {ICAPS'07}
}
@inproceedings{le2022coderl,
title={Code{RL}: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},
author={Hung Le and Yue Wang and Akhilesh Deepak Gotmare and Silvio Savarese and Steven Hoi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=WaGvb7OzySA}
}
@misc{luo2023wizardcoder,
      title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct}, 
      author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
      year={2023},
      eprint={2306.08568},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hendrycksapps2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}
@article{zelikman2023parsel,
  title={Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions},
  author={Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah and Haber, Nick},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={31466--31523},
  year={2023}
}
@inproceedings{NEURIPS2022_5762c579,
 author = {Inala, Jeevana Priya and Wang, Chenglong and Yang, Mei and Codas, Andres and Encarnaci\'{o}n, Mark and Lahiri, Shuvendu and Musuvathi, Madanlal and Gao, Jianfeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {13419--13432},
 publisher = {Curran Associates, Inc.},
 title = {Fault-Aware Neural Code Rankers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/5762c579d09811b7639be2389b3d07be-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}
@inproceedings{10.5555/3104322.3104404,
author = {Liang, Percy and Jordan, Michael I. and Klein, Dan},
title = {Learning programs: a hierarchical Bayesian approach},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We are interested in learning programs for multiple related tasks given only a few training examples per task. Since the program for a single task is underdetermined by its data, we introduce a nonparametric hierarchical Bayesian prior over programs which shares statistical strength across multiple tasks. The key challenge is to parametrize this multi-task sharing. For this, we introduce a new representation of programs based on combinatory logic and provide an MCMC algorithm that can perform safe program transformations on this representation to reveal shared inter-program substructures.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {639–646},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}
@article{rope,
  title={From Perception to Programs: Regularize, Overparameterize, and Amortize},
  author={Tang, Hao and Ellis, Kevin},
  journal={International Conference on Machine Learning (ICML)},
  year={2023}
}
@article{ellis2015unsupervised,
  title={Unsupervised learning by program synthesis},
  author={Ellis, Kevin and Solar-Lezama, Armando and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{fijalkow2022scaling,
  title={Scaling neural program synthesis with distribution-based search},
  author={Fijalkow, Nathana{\"e}l and Lagarde, Guillaume and Matricon, Th{\'e}o and Ellis, Kevin and Ohlmann, Pierre and Potta, Akarsh Nayan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={6623--6630},
  year={2022}
}
@article{ellis2016sampling,
  title={Sampling for bayesian program learning},
  author={Ellis, Kevin and Solar-Lezama, Armando and Tenenbaum, Josh},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}
@inproceedings{ellis2017learning,
  title={Learning to Learn Programs from Examples: Going Beyond Program Structure.},
  author={Ellis, Kevin and Gulwani, Sumit},
  booktitle={IJCAI},
  pages={1638--1645},
  year={2017}
}
@article{genesereth2013international,
  title={The international general game playing competition},
  author={Genesereth, Michael and Bj{\"o}rnsson, Yngvi},
  journal={AI Magazine},
  volume={34},
  number={2},
  pages={107--107},
  year={2013}
}
@article{alvin2015automatic,
  title={Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System},
  author={Alvin, Chris and Gulwani, Sumit and Majumdar, Rupak and Mukhopadhyay, Supratik},
  journal={arXiv e-prints},
  pages={arXiv--1510},
  year={2015}
}
@article{schmidhuber2004optimal,
  title={Optimal ordered problem solver},
  author={Schmidhuber, J{\"u}rgen},
  journal={Machine Learning},
  volume={54},
  pages={211--254},
  year={2004},
  publisher={Springer}
}
@article{EVANS2021103521,
title = {Making sense of raw input},
journal = {Artificial Intelligence},
volume = {299},
pages = {103521},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103521},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000722},
author = {Richard Evans and Matko Bošnjak and Lars Buesing and Kevin Ellis and David Pfau and Pushmeet Kohli and Marek Sergot},
keywords = {Interpretable AI, Unsupervised theory learning, Neuro-symbolic integration},
abstract = {How should a machine intelligence perform unsupervised structure discovery over streams of sensory input? One approach to this problem is to cast it as an apperception task [1]. Here, the task is to construct an explicit interpretable theory that both explains the sensory sequence and also satisfies a set of unity conditions, designed to ensure that the constituents of the theory are connected in a relational structure. However, the original formulation of the apperception task had one fundamental limitation: it assumed the raw sensory input had already been parsed using a set of discrete categories, so that all the system had to do was receive this already-digested symbolic input, and make sense of it. But what if we don't have access to pre-parsed input? What if our sensory sequence is raw unprocessed information? The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. First, we extend the definition of the apperception task to include ambiguous (but still symbolic) input: sequences of sets of disjunctions. Next, we use a neural network to map raw sensory input to disjunctive input. Our binary neural network is encoded as a logic program, so the weights of the network and the rules of the theory can be solved jointly as a single SAT problem. This way, we are able to jointly learn how to perceive (mapping raw sensory information to concepts) and apperceive (combining concepts into declarative rules).}
}
@article{schmidt2009distilling,
  title={Distilling free-form natural laws from experimental data},
  author={Schmidt, Michael and Lipson, Hod},
  journal={science},
  volume={324},
  number={5923},
  pages={81--85},
  year={2009},
  publisher={American Association for the Advancement of Science}
}
@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}
@article{tenenbaum2011grow,
  title={How to grow a mind: Statistics, structure, and abstraction},
  author={Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and Goodman, Noah D},
  journal={science},
  volume={331},
  number={6022},
  pages={1279--1285},
  year={2011},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{Zambaldi2019DeepRL,
  title={Deep reinforcement learning with relational inductive biases},
  author={Vin{\'i}cius Flores Zambaldi and David Raposo and Adam Santoro and Victor Bapst and Yujia Li and Igor Babuschkin and Karl Tuyls and David P. Reichert and Timothy P. Lillicrap and Edward Lockhart and Murray Shanahan and Victoria Langston and Razvan Pascanu and Matthew M. Botvinick and Oriol Vinyals and Peter W. Battaglia},
  booktitle={ICLR},
  year={2019}
}
@INPROCEEDINGS{5980391,
  author={Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
  booktitle={2011 IEEE International Conference on Robotics and Automation}, 
  title={Hierarchical task and motion planning in the now}, 
  year={2011},
  volume={},
  number={},
  pages={1470-1477},
  doi={10.1109/ICRA.2011.5980391}}
@article{KONIDARIS20191,
title = {On the necessity of abstraction},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {1-7},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618302080},
author = {George Konidaris},
abstract = {A generally intelligent agent faces a dilemma: it requires a complex sensorimotor space to be capable of solving a wide range of problems, but many tasks are only feasible given the right problem-specific formulation. I argue that a necessary but understudied requirement for general intelligence is the ability to form task-specific abstract representations. I show that the reinforcement learning paradigm structures this question into how to learn action abstractions and how to learn state abstractions, and discuss the field's progress on these topics.}
}
@misc{wong2023word,
      title={From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought}, 
      author={Lionel Wong and Gabriel Grand and Alexander K. Lew and Noah D. Goodman and Vikash K. Mansinghka and Jacob Andreas and Joshua B. Tenenbaum},
      year={2023},
      eprint={2306.12672},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}
@inproceedings{valmeekam2023on,
title={On the Planning Abilities of Large Language Models - A Critical Investigation},
author={Karthik Valmeekam and Matthew Marquez and Sarath Sreedharan and Subbarao Kambhampati},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=X6dEqXIsEW}
}
@article{guan2023leveraging,
      title={Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning}, 
      author={Lin Guan and Karthik Valmeekam and Sarath Sreedharan and Subbarao Kambhampati},
      year={2023},
journal={NeurIPS}
}
@inproceedings{codeaspolicies2022,
    title={Code as Policies: Language Model Programs for Embodied Control},
    author={Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
    booktitle={arXiv preprint arXiv:2209.07753},
    year={2022}
}
@incollection{sutton1990integrated,
  title={Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={Machine learning proceedings 1990},
  pages={216--224},
  year={1990},
  publisher={Elsevier}
}
@article{russo2018tutorial,
  title={A tutorial on thompson sampling},
  author={Russo, Daniel J and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={1},
  pages={1--96},
  year={2018},
  publisher={Now Publishers, Inc.}
}
@article{whittle1981arm,
  title={Arm-acquiring bandits},
  author={Whittle, Peter},
  journal={The Annals of Probability},
  volume={9},
  number={2},
  pages={284--292},
  year={1981},
  publisher={Institute of Mathematical Statistics}
}
@article{wang2023demo2code,
      title={Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought}, 
      author={Huaxiaoyue Wang and Gonzalo Gonzalez-Pumariega and Yash Sharma and Sanjiban Choudhury},
      year={2023},
      journal={NeurIPS}
}
@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}
@misc{wong2023learning,
      title={Learning adaptive planning representations with natural language guidance}, 
      author={Lionel Wong and Jiayuan Mao and Pratyusha Sharma and Zachary S. Siegel and Jiahai Feng and Noa Korneev and Joshua B. Tenenbaum and Jacob Andreas},
      year={2023},
      eprint={2312.08566},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{xiang2023language,
  title={Language Models Meet World Models: Embodied Experiences Enhance Language Models},
  author={Xiang, Jiannan and Tao, Tianhua and Gu, Yi and Shu, Tianmin and Wang, Zirui and Yang, Zichao and Hu, Zhiting},
  journal={NeurIPS},
  year={2023}
}
@article{tsividis2021human,
  title={Human-level reinforcement learning through theory-based modeling, exploration, and planning},
  author={Tsividis, Pedro A and Loula, Joao and Burga, Jake and Foss, Nathan and Campero, Andres and Pouncy, Thomas and Gershman, Samuel J and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2107.12544},
  year={2021}
}
@article{autumn,
author = {Das, Ria and Tenenbaum, Joshua B. and Solar-Lezama, Armando and Tavares, Zenna},
title = {Combining Functional and Automata Synthesis to Discover Causal Reactive Programs},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571249},
doi = {10.1145/3571249},
abstract = {We present a new algorithm that synthesizes functional reactive programs from observation data. The key novelty is to iterate between a functional synthesis step, which attempts to generate a transition function over observed states, and an automata synthesis step, which adds any additional latent state necessary to fully account for the observations. We develop a functional reactive DSL called Autumn that can express a rich variety of causal dynamics in time-varying, Atari-style grid worlds, and apply our method to synthesize Autumn programs from data. We evaluate our algorithm on a benchmark suite of 30 Autumn programs as well as a third-party corpus of grid-world-style video games. We find that our algorithm synthesizes 27 out of 30 programs in our benchmark suite and 21 out of 27 programs from the third-party corpus, including several programs describing complex latent state transformations, and from input traces containing hundreds of observations. We expect that our approach will provide a template for how to integrate functional and automata synthesis in other induction domains.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {56},
numpages = {31},
keywords = {automata, reactive, causal, synthesis}
}
@book{alur2013syntax,
  title={Syntax-guided synthesis},
  author={Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo MK and Raghothaman, Mukund and Seshia, Sanjit A and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
  year={2013},
  publisher={IEEE}
}
@inproceedings{10.1145/2737924.2737977,
author = {Feser, John K. and Chaudhuri, Swarat and Dillig, Isil},
title = {Synthesizing data structure transformations from input-output examples},
year = {2015},
isbn = {9781450334686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737924.2737977},
doi = {10.1145/2737924.2737977},
abstract = {We present a method for example-guided synthesis of functional programs over recursive data structures. Given a set of input-output examples, our method synthesizes a program in a functional language with higher-order combinators like map and fold. The synthesized program is guaranteed to be the simplest program in the language to fit the examples. Our approach combines three technical ideas: inductive generalization, deduction, and enumerative search. First, we generalize the input-output examples into hypotheses about the structure of the target program. For each hypothesis, we use deduction to infer new input/output examples for the missing subexpressions. This leads to a new subproblem where the goal is to synthesize expressions within each hypothesis. Since not every hypothesis can be realized into a program that fits the examples, we use a combination of best-first enumeration and deduction to search for a hypothesis that meets our needs. We have implemented our method in a tool called λ2, and we evaluate this tool on a large set of synthesis problems involving lists, trees, and nested data structures. The experiments demonstrate the scalability and broad scope of λ2. A highlight is the synthesis of a program believed to be the world's earliest functional pearl.},
booktitle = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {229–239},
numpages = {11},
keywords = {search-based synthesis, programming by example, data transformations, automated deduction, Program synthesis},
location = {Portland, OR, USA},
series = {PLDI '15}
}




@article{li2023emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={ICLR},
  year={2023}
}
@article{chaudhuri2010smooth,
  title={Smooth interpretation},
  author={Chaudhuri, Swarat and Solar-Lezama, Armando},
  journal={ACM Sigplan Notices},
  volume={45},
  number={6},
  pages={279--291},
  year={2010},
  publisher={ACM New York, NY, USA}
}
@inproceedings{smith2018inference,
  title={An inference-based policy gradient method for learning options},
  author={Smith, Matthew and Hoof, Herke and Pineau, Joelle},
  booktitle={International Conference on Machine Learning},
  pages={4703--4712},
  year={2018},
  organization={PMLR}
}
@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@misc{rl-zoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{SchraderSokoban2018,
  author = {Schrader, Max-Philipp B.},
  title = {gym-sokoban},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/mpSchrader/gym-sokoban}},
  commit = {#CommitId}
}
@book{lieberman2001your,
  title={Your wish is my command: Programming by example},
  author={Lieberman, Henry},
  year={2001},
  publisher={Morgan Kaufmann}
}
@inproceedings{DBLP:conf/iclr/ShiDES22,
  author       = {Kensen Shi and
                  Hanjun Dai and
                  Kevin Ellis and
                  Charles Sutton},
  title        = {CrossBeam: Learning to Search in Bottom-Up Program Synthesis},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=qhC8mr2LEKq},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ShiDES22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{srivastava2022beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}
@misc{guo2024deepseekcoder,
      title={DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}, 
      author={Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
      year={2024},
      eprint={2401.14196},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@article{ellis2024human,
  title={Human-like few-shot learning via bayesian reasoning over natural language},
  author={Ellis, Kevin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{DBLP:journals/corr/abs-2402-06025,
  author       = {Top Piriyakulkij and
                  Kevin Ellis},
  title        = {Doing Experiments and Revising Rules with Natural Language and Probabilistic
                  Reasoning},
  journal      = {CoRR},
  volume       = {abs/2402.06025},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.06025},
  doi          = {10.48550/ARXIV.2402.06025},
  eprinttype    = {arXiv},
  eprint       = {2402.06025},
  timestamp    = {Fri, 16 Feb 2024 13:00:19 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-06025.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
wang2024hypothesis,
title={Hypothesis Search: Inductive Reasoning with Language Models},
author={Ruocheng Wang and Eric Zelikman and Gabriel Poesia and Yewen Pu and Nick Haber and Noah Goodman},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=G7UtIGQmjm}
}
@misc{kambhampati2024llms,
      title={LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks}, 
      author={Subbarao Kambhampati and Karthik Valmeekam and Lin Guan and Kaya Stechly and Mudit Verma and Siddhant Bhambri and Lucas Saldyt and Anil Murthy},
      year={2024},
      eprint={2402.01817},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{jinamodel,
  title = {jinaai/jina-embeddings-v2-base-code},
  howpublished = {\url{https://huggingface.co/jinaai/jina-embeddings-v2-base-code}},
  note = {Accessed: 2024-05-22}
}