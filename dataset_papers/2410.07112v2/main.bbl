\begin{thebibliography}{10}

\bibitem{anthropic2024Claude3}
Anthropic.
\newblock The {Claude} 3 model family: {Opus, Sonnet, Haiku}, 2024.

\bibitem{bai2023qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang
  Lin, Chang Zhou, and Jingren Zhou.
\newblock {Qwen-VL}: A frontier large vision-language model with versatile
  abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{beyer2024paligamma}
Lucas Beyer, Andreas Steiner, Andr{\'e}~Susano Pinto, Alexander Kolesnikov,
  Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael
  Tschannen, Emanuele Bugliarello, et~al.
\newblock {PaliGemma}: A versatile {3B VLM} for transfer.
\newblock {\em arXiv preprint arXiv:2407.07726}, 2024.

\bibitem{bommasani2022opportunities}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
  Niladri Chatterji, Annie Chen, Kathleen Creel, Jared~Quincy Davis, Dora
  Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John
  Etchemendy, Kawin Ethayarajh, Li~Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
  Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori
  Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny Hong, Kyle Hsu,
  Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,
  Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang~Wei
  Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal
  Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang~Lisa Li,
  Xuechen Li, Tengyu Ma, Ali Malik, Christopher~D. Manning, Suvir Mirchandani,
  Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak
  Narayanan, Ben Newman, Allen Nie, Juan~Carlos Niebles, Hamed Nilforoshan,
  Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon~Sung Park,
  Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich,
  Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher
  Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan
  Srinivasan, Alex Tamkin, Rohan Taori, Armin~W. Thomas, Florian Tramèr,
  Rose~E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang~Michael Xie,
  Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang,
  Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{chen-etal-2024-measuring}
Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran.
\newblock Measuring and improving chain-of-thought reasoning in vision-language
  models.
\newblock In {\em Proceedings of the 2024 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 192--210, 2024.

\bibitem{cui2023holistic}
Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and
  Huaxiu Yao.
\newblock Holistic analysis of hallucination in {GPT}-4v(ision): Bias and
  interference challenges.
\newblock {\em arXiv preprint arXiv:2311.03287}, 2023.

\bibitem{dai2024instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale~N Fung, and Steven Hoi.
\newblock {InstructBLIP}: Towards general-purpose vision-language models with
  instruction tuning.
\newblock In {\em Advances in neural information processing systems}, 2023.

\bibitem{das2024exams}
Rocktim Das, Simeon Hristov, Haonan Li, Dimitar Dimitrov, Ivan Koychev, and
  Preslav Nakov.
\newblock {EXAMS}-{V}: A multi-discipline multilingual multimodal exam
  benchmark for evaluating vision language models.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for
  Computational Linguistics}, pages 7768--7791, 2024.

\bibitem{eitz2012hdhso}
Mathias Eitz, James Hays, and Marc Alexa.
\newblock How do humans sketch objects?
\newblock {\em ACM Transactions on graphics (TOG)}, 31(4):1--10, 2012.

\bibitem{fraser2024examining}
Kathleen~C Fraser and Svetlana Kiritchenko.
\newblock Examining gender and racial bias in large vision--language models
  using a novel dataset of parallel images.
\newblock In {\em Proceedings of the 18th Conference of the European Chapter of
  the Association for Computational Linguistics}, pages 690--713, 2024.

\bibitem{geminiteam2024gemini1.5}
{Gemini Team}.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of
  tokens of context.
\newblock {\em arXiv preprint arXiv:2403.05530}, 2024.

\bibitem{balanced_vqa_v2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the {V} in {VQA} matter: Elevating the role of image
  understanding in {V}isual {Q}uestion {A}nswering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6904--6913, 2017.

\bibitem{realworldqa}
{Grok-1.5 Team}.
\newblock Grok-1.5 vision preview, 2024.

\bibitem{gurari2018vizwiz}
Danna Gurari, Qing Li, Abigale~J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
  Jiebo Luo, and Jeffrey~P Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3608--3617, 2018.

\bibitem{hudson2018gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock {GQA}: A new dataset for real-world visual reasoning and
  compositional question answering.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 6700--6709, 2019.

\bibitem{karkkainen2019fairface}
Kimmo K{\"a}rkk{\"a}inen and Jungseock Joo.
\newblock {FairFace}: Face attribute dataset for balanced race, gender, and age
  for bias measurement and mitigation.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications
  of computer vision}, pages 1548--1558, 2021.

\bibitem{kiela2021hateful}
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh,
  Pratik Ringshia, and Davide Testuggine.
\newblock The hateful memes challenge: Detecting hate speech in multimodal
  memes.
\newblock {\em Advances in neural information processing systems},
  33:2611--2624, 2020.

\bibitem{laurençon2023obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet
  Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush,
  Douwe Kiela, et~al.
\newblock {OBELICS}: An open web-scale filtered dataset of interleaved
  image-text documents.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{laurençon2024matters}
Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?
\newblock {\em arXiv preprint arXiv:2405.02246}, 2024.

\bibitem{lee2024prometheusvision}
Seongyun Lee, Seungone Kim, Sue~Hyun Park, Geewook Kim, and Minjoon Seo.
\newblock Prometheus-vision: Vision-language model as a judge for fine-grained
  evaluation.
\newblock In {\em Findings of the Association for Computational Linguistics ACL
  2024}, 2024.

\bibitem{lee2023holistic}
Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon~Sung Park, Agrim
  Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente,
  et~al.
\newblock Holistic evaluation of text-to-image models.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~36, 2024.

\bibitem{li2023seed}
Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying
  Shan.
\newblock {SEED}-bench: Benchmarking multimodal large language models.
\newblock In {\em 2024 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 13299--13308, 2024.

\bibitem{li2023evaluating}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pages 292--305, 2023.

\bibitem{liang2023holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  Benjamin Newman, Binhang Yuan, Bobby Yan, Ce~Zhang, Christian~Alexander
  Cosgrove, Christopher~D Manning, Christopher Re, Diana Acosta-Navas,
  Drew~Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong,
  Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng,
  Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri~S. Chatterji,
  Omar Khattab, Peter Henderson, Qian Huang, Ryan~Andrew Chi, Sang~Michael Xie,
  Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi
  Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang,
  and Yuta Koreeda.
\newblock Holistic evaluation of language models.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{lin2015microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock {Microsoft COCO}: Common objects in context.
\newblock In {\em {European Conference on Computer Vision}}, pages 740--755.
  Springer, 2014.

\bibitem{lin2023text}
Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu~Jiang, and Ming-Hsuan Yang.
\newblock Text-driven image editing via learnable regions.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7059--7068, 2024.

\bibitem{liu2023visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In {\em Advances in neural information processing systems},
  volume~36, 2024.

\bibitem{liu2023queryrelevant}
Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu~Qiao.
\newblock Query-relevant images jailbreak large multi-modal models.
\newblock {\em arXiv preprint arXiv:2311.17600}, 2023.

\bibitem{lu2024mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh
  Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in
  visual contexts.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}.

\bibitem{mai2024mmmu}
Yifan Mai and Percy Liang.
\newblock Massive multitask language understanding (mmlu) on {HELM}, 2024.
\newblock Accessed Jun 03, 2024.

\bibitem{openai2024gpt4}
OpenAI.
\newblock {GPT-4} technical report, 2024.

\bibitem{openai2024hello}
{OpenAI}.
\newblock Hello {GPT-4}o, 2024.

\bibitem{padlewski2024vibe}
Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai
  Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et~al.
\newblock {Vibe-Eval}: A hard evaluation suite for measuring progress of
  multimodal language models.
\newblock {\em arXiv preprint arXiv:2405.02287}, 2024.

\bibitem{schwenk2022aokvqa}
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
  Roozbeh Mottaghi.
\newblock {A-OKVQA}: A benchmark for visual question answering using world
  knowledge.
\newblock In {\em 17th European Conference on Computer Vision}, pages 146--162.
  Springer, 2022.

\bibitem{song2022clip}
Haoyu Song, Li~Dong, Weinan Zhang, Ting Liu, and Furu Wei.
\newblock {CLIP} models are few-shot learners: Empirical studies on {VQA} and
  visual entailment.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics}, pages 6088--6100. Association for Computational
  Linguistics, 2022.

\bibitem{tao2024imgtrojan}
Xijia Tao, Shuai Zhong, Lei Li, Qi~Liu, and Lingpeng Kong.
\newblock {ImgTrojan}: Jailbreaking vision-language models with {ONE} image.
\newblock {\em arXiv preprint arXiv:2403.02910}, 2024.

\bibitem{geminiteam2024gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
  Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{thapliyal2022crossmodal3600}
Ashish~V Thapliyal, Jordi~Pont Tuset, Xi~Chen, and Radu Soricut.
\newblock Crossmodal-3600: A massively multilingual multimodal evaluation
  dataset.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 715--729, 2022.

\bibitem{tu2023unicorns}
Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han,
  Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie.
\newblock How many unicorns are in this image? {A} safety evaluation benchmark
  for vision {LLM}s.
\newblock {\em arXiv preprint arXiv:2311.16101}, 2023.

\bibitem{tu2023sight}
Haoqin Tu, Bingchen Zhao, Chen Wei, and Cihang Xie.
\newblock Sight beyond text: Multi-modal training enhances {LLM}s in
  truthfulness and ethics.
\newblock In {\em NeurIPS Workshop on Instruction Tuning and Instruction
  Following}, 2023.

\bibitem{W3Techs2024usage}
{W3Techs}.
\newblock Usage statistics of content languages for websites.
\newblock Accessed Sept 18, 2024.

\bibitem{wang2024white}
Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, and Yu-Gang
  Jiang.
\newblock White-box multimodal jailbreaks against large vision-language models.
\newblock {\em arXiv preprint arXiv:2405.17894}, 2024.

\bibitem{wang2024mementos}
Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He,
  Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, Mohit Bansal, Huaxiu
  Yao, and Furong Huang.
\newblock Mementos: A comprehensive benchmark for multimodal large language
  model reasoning over image sequences.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for
  Computational Linguistics}, pages 416--442, 2024.

\bibitem{yin2023survey}
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke~Li, Xing Sun, Tong Xu, and Enhong Chen.
\newblock A survey on multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13549}, 2023.

\bibitem{ying2024jailbreak}
Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang, Xianglong
  Liu, and Dacheng Tao.
\newblock Jailbreak vision language models via bi-modal adversarial prompt.
\newblock {\em arXiv preprint arXiv:2406.04031}, 2024.

\bibitem{young2014image}
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics
  for semantic inference over event descriptions.
\newblock {\em Transactions of the Association for Computational Linguistics},
  2014.

\bibitem{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel
  Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock {MMMU}: A massive multi-discipline multimodal understanding and
  reasoning benchmark for expert {AGI}.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9556--9567, 2024.

\bibitem{zhao2022ood}
Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang,
  Ju~He, Alan~L Yuille, and Adam Kortylewski.
\newblock {OOD-CV}: A benchmark for robustness to out-of-distribution shifts of
  individual nuisances in natural images.
\newblock In {\em 17th European Conference on Computer Vision}, pages 163--180.
  Springer, 2022.

\bibitem{pmlr-v229-zitkovich23a}
Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu,
  Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke,
  Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet,
  Pannag~R. Sanketi, Grecia Salazar, Michael~S. Ryoo, Krista Reymann, Kanishka
  Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine,
  Lisa Lee, Tsang-Wei~Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry
  Kalashnikov, Ryan Julian, Nikhil~J. Joshi, Alex Irpan, Brian Ichter, Jasmine
  Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu,
  Pete Florence, Chelsea Finn, Kumar~Avinava Dubey, Danny Driess, Tianli Ding,
  Krzysztof~Marcin Choromanski, Xi~Chen, Yevgen Chebotar, Justice Carbajal,
  Noah Brown, Anthony Brohan, Montserrat~Gonzalez Arenas, and Kehang Han.
\newblock {RT-2}: Vision-language-action models transfer web knowledge to
  robotic control.
\newblock In {\em Proceedings of The 7th Conference on Robot Learning}, 2023.

\end{thebibliography}
