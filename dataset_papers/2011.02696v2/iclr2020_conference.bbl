\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bibas et~al.(2019)Bibas, Fogel, and Feder]{Bibas2019DeepNetworks}
Koby Bibas, Yaniv Fogel, and Meir Feder.
\newblock {Deep pnml: Predictive normalized maximum likelihood for deep neural
  networks}.
\newblock \emph{arXiv preprint arXiv:1904.12286}, 2019.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{Blundell2015WeightNetworks}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock {Weight Uncertainty in Neural Networks}.
\newblock \emph{32nd International Conference on Machine Learning, ICML 2015},
  2:\penalty0 1613--1622, 5 2015.

\bibitem[Cook \& Weisberg(1982)Cook and Weisberg]{cook1982residuals}
R~Dennis Cook and Sanford Weisberg.
\newblock \emph{Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem[Cover \& Thomas(2006)Cover and Thomas]{cover2006elements}
Thomas~M. Cover and Joy~A. Thomas.
\newblock \emph{Elements of Information Theory (Wiley Series in
  Telecommunications and Signal Processing)}.
\newblock Wiley-Interscience, USA, 2006.
\newblock ISBN 0471241954.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{Dusenberry2020EfficientFactors}
Michael~W Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-an Ma, Jasper Snoek,
  Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran.
\newblock {Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors}.
\newblock \emph{arXiv preprint arXiv:2005.07186}, 2020.

\bibitem[Fogel \& Feder(2018{\natexlab{a}})Fogel and
  Feder]{Fogel2018UniversalData}
Yaniv Fogel and Meir Feder.
\newblock {Universal Supervised Learning for Individual Data}.
\newblock 12 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1812.09520}.

\bibitem[Fogel \& Feder(2018{\natexlab{b}})Fogel and
  Feder]{Fogel2018UniversalLog-Loss}
Yaniv Fogel and Meir Feder.
\newblock {Universal batch learning with log-loss}.
\newblock In \emph{2018 IEEE International Symposium on Information Theory
  (ISIT)}, pp.\  21--25, 2018{\natexlab{b}}.

\bibitem[Gal \& Ghahramani(2015)Gal and Ghahramani]{Gal2015DropoutLearning}
Yarin Gal and Zoubin Ghahramani.
\newblock {Dropout as a Bayesian Approximation: Representing Model Uncertainty
  in Deep Learning}.
\newblock \emph{33rd International Conference on Machine Learning, ICML 2016},
  3:\penalty0 1651--1660, 6 2015.

\bibitem[Giordano et~al.(2019)Giordano, Stephenson, Liu, Jordan, and
  Broderick]{giordano2019swiss}
Ryan Giordano, William Stephenson, Runjing Liu, Michael Jordan, and Tamara
  Broderick.
\newblock A swiss army infinitesimal jackknife.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1139--1147, 2019.

\bibitem[Grunwald(2004)]{Grunwald2004APrinciple}
Peter Grunwald.
\newblock {A tutorial introduction to the minimum description length
  principle}.
\newblock 6 2004.
\newblock URL \url{http://arxiv.org/abs/math/0406077}.

\bibitem[Gr{\"{u}}nwald et~al.(2017)Gr{\"{u}}nwald, Van~Ommen, and
  {others}]{Grunwald2017InconsistencyIt}
Peter Gr{\"{u}}nwald, Thijs Van~Ommen, and {others}.
\newblock {Inconsistency of Bayesian inference for misspecified linear models,
  and a proposal for repairing it}.
\newblock \emph{Bayesian Analysis}, 12\penalty0 (4):\penalty0 1069--1103, 2017.

\bibitem[Gr{\"{u}}nwald(2007)]{Grunwald2007TheLearning}
Peter~D Gr{\"{u}}nwald.
\newblock \emph{{The Minimum Description Length Principle (Adaptive Computation
  and Machine Learning)}}.
\newblock The MIT Press, 2007.
\newblock ISBN 0262072815.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{Guo2017OnNetworks}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock {On calibration of modern neural networks}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1321--1330, 2017.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hinton \& van Camp(1993)Hinton and van Camp]{Hinton1993KeepingWeights}
Geoffrey~E. Hinton and Drew van Camp.
\newblock {Keeping neural networks simple by minimizing the description length
  of the weights}.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, pp.\  5--13, 1993.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter1997FlatMinima}
Sepp Hochreiter and JÃ¼rgen Schmidhuber.
\newblock {Flat minima}.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{Hoffman2013StochasticInference}
Matthew~D Hoffman, David~M Blei, Chong Wang, and John Paisley.
\newblock {Stochastic variational inference}.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{Izmailov2018AveragingGeneralization}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock {Averaging Weights Leads to Wider Optima and Better Generalization}.
\newblock In \emph{UAI}, 2018.

\bibitem[Kakade et~al.(2006)Kakade, Seeger, and
  Foster]{Kakade2006Worst-caseModels}
Sham~M Kakade, Matthias~W Seeger, and Dean~P Foster.
\newblock {Worst-case bounds for Gaussian process models}.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  619--626, 2006.

\bibitem[Koh \& Liang(2017)Koh and Liang]{pmlr-v70-koh17a}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock Proceedings of Machine Learning Research, 2017.

\bibitem[Krizhevsky(2012)]{Krizhevsky2012LearningImages}
Alex Krizhevsky.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock \emph{University of Toronto}, 6 2012.

\bibitem[Lakshminarayanan et~al.(2016)Lakshminarayanan, Pritzel, and
  Blundell]{Lakshminarayanan2016SimpleEnsembles}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock {Simple and Scalable Predictive Uncertainty Estimation using Deep
  Ensembles}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{Maddox2019ALearning}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock {A simple baseline for bayesian uncertainty in deep learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{Naeini2015ObtainingBinning}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock {Obtaining well calibrated probabilities using bayesian binning}.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019trust}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D~Sculley, Sebastian
  Nowozin, Joshua~V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift, 2019.

\bibitem[Rissanen(1989)]{Rissanen1989StochasticTheory}
Jorma Rissanen.
\newblock \emph{{Stochastic Complexity in Statistical Inquiry Theory}}.
\newblock World Scientific Publishing Co., Inc., USA, 1989.
\newblock ISBN 9971508591.

\bibitem[Rissanen \& Roos(2007)Rissanen and
  Roos]{Rissanen2007ConditionalModels}
Jorma Rissanen and Teemu Roos.
\newblock {Conditional NML universal models}.
\newblock In \emph{2007 Information Theory and Applications Workshop}, pp.\
  337--341, 2007.

\bibitem[Rissanen(1996)]{Rissanen1996FisherComplexity}
Jorma~J. Rissanen.
\newblock {Fisher information and stochastic complexity}.
\newblock \emph{IEEE Transactions on Information Theory}, 42\penalty0
  (1):\penalty0 40--47, 1996.
\newblock ISSN 00189448.
\newblock \doi{10.1109/18.481776}.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, volume~6. International Conference
  on Representation Learning, 2018.

\bibitem[Roos et~al.(2008)Roos, Silander, Kontkanen, and
  Myllymaki]{Roos2008BayesianModels}
Teemu Roos, Tomi Silander, Petri Kontkanen, and Petri Myllymaki.
\newblock {Bayesian network structure learning using factorized NML universal
  models}.
\newblock In \emph{2008 Information Theory and Applications Workshop}, pp.\
  272--276, 2008.

\bibitem[Shtarkov(1987)]{Shtarkov1987UniversalMessages}
Yuri Shtarkov.
\newblock {Universal sequential coding of single messages}.
\newblock \emph{Problems of Information Transmission}, 23\penalty0
  (3):\penalty0 186, 1987.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and
  Zisserman]{Simonyan2014VeryRecognition}
Karen Simonyan and Andrew Zisserman.
\newblock {Very deep convolutional networks for large-scale image recognition}.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava2014Dropout:Overfitting}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock {Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 2014.

\bibitem[Sun et~al.(2020)Sun, Wang, Zhuang, Miller, Hardt, and Efros]{sun19ttt}
Yu~Sun, Xiaolong Wang, Liu Zhuang, John Miller, Moritz Hardt, and Alexei~A.
  Efros.
\newblock Test-time training with self-supervision for generalization under
  distribution shifts.
\newblock In \emph{ICML}, 2020.

\bibitem[Vovk(1990)]{Vovk1990AggregatingStrategies}
Volodimir~G Vovk.
\newblock {Aggregating Strategies}.
\newblock In \emph{Proceedings of the Third Annual Workshop on Computational
  Learning Theory}, 1990.

\end{thebibliography}
