@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{mordatch2017emergence,
  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},
  author={Mordatch, Igor and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1703.04908},
  year={2017}
}

@article{lowe2017multi,
  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  journal={Neural Information Processing Systems (NIPS)},
  year={2017}
}

@article{zheng2017magent,
  title={Magent: A many-agent reinforcement learning platform for artificial collective intelligence},
  author={Zheng, Lianmin and Yang, Jiacheng and Cai, Han and Zhang, Weinan and Wang, Jun and Yu, Yong},
  journal={arXiv preprint arXiv:1712.00600},
  year={2017}
}

@article{zha2019rlcard,
  title={RLCard: A Toolkit for Reinforcement Learning in Card Games},
  author={Zha, Daochen and Lai, Kwei-Herng and Cao, Yuanpu and Huang, Songyi and Wei, Ruzhe and Guo, Junyu and Hu, Xia},
  journal={arXiv preprint arXiv:1910.04376},
  year={2019}
}

@inproceedings{gupta2017cooperative,
  title={Cooperative multi-agent control using deep reinforcement learning},
  author={Gupta, Jayesh K and Egorov, Maxim and Kochenderfer, Mykel},
  booktitle={International Conference on Autonomous Agents and Multiagent Systems},
  pages={66--83},
  year={2017},
  organization={Springer}
}

% From Gupta et al.
@article{vidal2002probabilistic,
  title={Probabilistic pursuit-evasion games: theory, implementation, and experimental evaluation},
  author={Vidal, Rene and Shakernia, Omid and Kim, H Jin and Shim, David Hyunchul and Sastry, Shankar},
  journal={IEEE transactions on robotics and automation},
  volume={18},
  number={5},
  pages={662--669},
  year={2002},
  publisher={IEEE}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{schrittwieser2019mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1911.08265},
  year={2019}
}

@article{terry2020arcade,
  Title = {Multiplayer Support for the Arcade Learning Environment},
  Author = {Terry, J K and Black, Benjamin},
  journal={arXiv preprint arXiv:2009.09341},
  year={2020}
}

@article{gerald1995temporal,
author = {Tesauro, Gerald},
title = {Temporal Difference Learning and TD-Gammon},
year = {1995},
issue_date = {March 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/203330.203343},
doi = {10.1145/203330.203343},
journal = {Commun. ACM},
month = mar,
pages = {58–68},
numpages = {11}
}

@article{bard2019hanabi,
  author    = {Nolan Bard and
               Jakob N. Foerster and
               Sarath Chandar and
               Neil Burch and
               Marc Lanctot and
               H. Francis Song and
               Emilio Parisotto and
               Vincent Dumoulin and
               Subhodeep Moitra and
               Edward Hughes and
               Iain Dunning and
               Shibl Mourad and
               Hugo Larochelle and
               Marc G. Bellemare and
               Michael Bowling},
  title     = {The Hanabi Challenge: {A} New Frontier for {AI} Research},
  journal   = {CoRR},
  volume    = {abs/1902.00506},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00506},
  archivePrefix = {arXiv},
  eprint    = {1902.00506},
  timestamp = {Wed, 24 Jul 2019 18:56:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-00506.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{OpenAI_dota,
    author = {OpenAI},
    title = {OpenAI Five},
    year = {2018},
    howpublished = {\url{https://blog.openai.com/openai-five/}}}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@misc{ale_python_interface,
  author = {Ben Goodrich},
  title = {ale\_python\_interface},
  year = {2015},
  publisher = {GitHub},
  note = {GitHub repository},
  howpublished = {\url{https://github.com/bbitmaster/ale_python_interface}}
}

@inproceedings{Pokle2018AnalysisOE,
  title={Analysis of Emergent Behavior in Multi Agent Environments using Deep Reinforcement Learning},
  author={Stefanie Anna Baby Ling Li Ashwini Pokle},
  year={2018}
}
@inproceedings{Subramanian2020MultiTM,
  title={Multi Type Mean Field Reinforcement Learning},
  author={Sriram Ganapathi Subramanian and P. Poupart and Matthew E. Taylor and N. Hegde},
  booktitle={AAMAS},
  year={2020}
}

@inproceedings{Chen2019FactorizedQF,
  title={Factorized Q-learning for large-scale multi-agent systems},
  author={Y. Chen and M. Zhou and Ying Wen and Y. Yang and Y. Su and W. Zhang and Dell Zhang and J. Wang and Han Liu},
  booktitle={DAI '19},
  year={2019}
}

@inproceedings{Palmer2020IndependentLA,
  title={Independent Learning Approaches: Overcoming Multi-Agent Learning Pathologies In Team-Games},
  author={G. Palmer},
  year={2020}
}

@incollection{kidzinski2018learningtorun,
  author      = "Kidzi\'nski, {\L}ukasz and Mohanty, Sharada P and Ong, Carmichael and Hicks, Jennifer and Francis, Sean and Levine, Sergey and Salath\'e, Marcel and Delp, Scott",
  title       = "Learning to Run challenge: Synthesizing physiologically accurate motion using deep reinforcement learning",
  editor      = "Escalera, Sergio and Weimer, Markus",
  booktitle   = "NIPS 2017 Competition Book",
  publisher   = "Springer",
  address     = "Springer",
  year        = 2018
}

@misc{Edouard2018Driving,
  author = {Leurent, Edouard},
  title = {An Environment for Autonomous Driving Decision-Making},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/eleurent/highway-env}},
}
@article{zamora2016extending,
  title={Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo},
  author={Zamora, Iker and Lopez, Nestor Gonzalez and Vilches, Victor Mayoral and Cordero, Alejandro Hernandez},
  journal={arXiv preprint arXiv:1608.05742},
  year={2016}
}
@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}
@inproceedings{liang2018rllib,
    Author = {Eric Liang and
              Richard Liaw and
              Robert Nishihara and
              Philipp Moritz and
              Roy Fox and
              Ken Goldberg and
              Joseph E. Gonzalez and
              Michael I. Jordan and
              Ion Stoica},
    Title = {{RLlib}: Abstractions for Distributed Reinforcement Learning},
    Booktitle = {International Conference on Machine Learning ({ICML})},
    Year = {2018}
}
@misc{tensorforce,
  author       = {Kuhnle, Alexander and Schaarschmidt, Michael and Fricke, Kai},
  title        = {Tensorforce: a TensorFlow library for applied reinforcement learning},
  howpublished = {Web page},
  url          = {https://github.com/tensorforce/tensorforce},
  year         = {2017}
}


@article{deepmind_soccer,
  author    = {Siqi Liu and
               Guy Lever and
               Josh Merel and
               Saran Tunyasuvunakool and
               Nicolas Heess and
               Thore Graepel},
  title     = {Emergent Coordination Through Competition},
  journal   = {CoRR},
  volume    = {abs/1902.07151},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.07151},
  archivePrefix = {arXiv},
  eprint    = {1902.07151},
  timestamp = {Mon, 22 Jul 2019 16:19:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-07151.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AEC_Games,
  Title = {Agent Environment Cycle Games},
  Author = {Terry, J K and Grammel, Nathaniel and Black, Benjamin and Hari, Ananth and Santos, Luis and Horsch, Caroline},
  journal={arXiv preprint arXiv:2009.13051},
  year={2020}
}


@article{SuperSuit2020,
  Title = {SuperSuit: Simple Microwrappers for Reinforcement Learning Environments},
  Author = {Terry, J K and Black, Benjamin and Hari, Ananth},
  journal={arXiv preprint arXiv:2008.08932},
  year={2020}
}

@article{cobbe2019leveraging,
  title={Leveraging procedural generation to benchmark reinforcement learning},
  author={Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
  journal={arXiv preprint arXiv:1912.01588},
  year={2019}
}

@article{Horgan2018APEX,
  author    = {Dan Horgan and
               John Quan and
               David Budden and
               Gabriel Barth{-}Maron and
               Matteo Hessel and
               Hado van Hasselt and
               David Silver},
  title     = {Distributed Prioritized Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1803.00933},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.00933},
  archivePrefix = {arXiv},
  eprint    = {1803.00933},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-00933.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{openspiel2019,
  author    = {Marc Lanctot and
               Edward Lockhart and
               Jean{-}Baptiste Lespiau and
               Vin{\'{\i}}cius Flores Zambaldi and
               Satyaki Upadhyay and
               Julien P{\'{e}}rolat and
               Sriram Srinivasan and
               Finbarr Timbers and
               Karl Tuyls and
               Shayegan Omidshafiei and
               Daniel Hennes and
               Dustin Morrill and
               Paul Muller and
               Timo Ewalds and
               Ryan Faulkner and
               J{\'{a}}nos Kram{\'{a}}r and
               Bart De Vylder and
               Brennan Saeta and
               James Bradbury and
               David Ding and
               Sebastian Borgeaud and
               Matthew Lai and
               Julian Schrittwieser and
               Thomas W. Anthony and
               Edward Hughes and
               Ivo Danihelka and
               Jonah Ryan{-}Davis},
  title     = {OpenSpiel: {A} Framework for Reinforcement Learning in Games},
  journal   = {CoRR},
  volume    = {abs/1908.09453},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09453},
  archivePrefix = {arXiv},
  eprint    = {1908.09453},
  timestamp = {Fri, 09 Oct 2020 09:46:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09453.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{smac2019,
  author    = {Mikayel Samvelyan and
               Tabish Rashid and
               Christian Schr{\"{o}}der de Witt and
               Gregory Farquhar and
               Nantas Nardelli and
               Tim G. J. Rudner and
               Chia{-}Man Hung and
               Philip H. S. Torr and
               Jakob N. Foerster and
               Shimon Whiteson},
  title     = {The StarCraft Multi-Agent Challenge},
  journal   = {CoRR},
  volume    = {abs/1902.04043},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.04043},
  archivePrefix = {arXiv},
  eprint    = {1902.04043},
  timestamp = {Tue, 21 May 2019 18:03:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-04043.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{leibo0217ssd,
  author    = {Joel Z. Leibo and
               Vin{\'{\i}}cius Flores Zambaldi and
               Marc Lanctot and
               Janusz Marecki and
               Thore Graepel},
  title     = {Multi-agent Reinforcement Learning in Sequential Social Dilemmas},
  journal   = {CoRR},
  volume    = {abs/1702.03037},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.03037},
  archivePrefix = {arXiv},
  eprint    = {1702.03037},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LeiboZLMG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bansal2017competition,
  author    = {Trapit Bansal and
               Jakub Pachocki and
               Szymon Sidor and
               Ilya Sutskever and
               Igor Mordatch},
  title     = {Emergent Complexity via Multi-Agent Competition},
  journal   = {CoRR},
  volume    = {abs/1710.03748},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.03748},
  archivePrefix = {arXiv},
  eprint    = {1710.03748},
  timestamp = {Mon, 13 Aug 2018 16:47:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-03748.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{SSDOpenSource,
author = {Vinitsky, Eugene and Jaques, Natasha and Leibo, Joel and Castenada, Antonio and Hughes, Edward},
title = {An Open Source Implementation of Sequential Social Dilemma Games},
year = {2019},
publisher = {GitHub},
note = {GitHub repository},
howpublished = {\url{https://github.com/eugenevinitsky/sequential_social_dilemma_games/}}
}

@inproceedings{cobbe2020leveraging,
  title={Leveraging procedural generation to benchmark reinforcement learning},
  author={Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John},
  booktitle={International conference on machine learning},
  pages={2048--2056},
  year={2020},
  organization={PMLR}
}

@misc{tianshou,
  author = {Weng, Jiayi and Zhang, Minghao and Duburcq, Alexis and You, Kaichao and Yan, Dong and Su, Hang and Zhu, Jun},
  title = {Tianshou},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/thu-ml/tianshou}},
}

@misc{nota2020autonomous,
  author = {Nota, Chris},
  title = {The Autonomous Learning Library},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/cpnota/autonomous-learning-library}},
}

@misc{magym,
      author = {Koul, Anurag},
      title = {ma-gym: Collection of multi-agent environments based on OpenAI gym.},
      year = {2019},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/koulanurag/ma-gym}},
    }
    
@misc{slimevolleygym,
  author = {David Ha},
  title = {Slime Volleyball Gym Environment},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hardmaru/slimevolleygym}},
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}


@article{liang2017rllib,
  title={RLlib: Abstractions for distributed reinforcement learning},
  author={Liang, Eric and Liaw, Richard and Moritz, Philipp and Nishihara, Robert and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph E and Jordan, Michael I and Stoica, Ion},
  journal={arXiv preprint arXiv:1712.09381},
  year={2017}
}


@article{bernstein2002DecPOMDP,
author = {Bernstein, Daniel S. and Givan, Robert and Immerman, Neil and Zilberstein, Shlomo},
title = {The Complexity of Decentralized Control of Markov Decision Processes},
journal = {Mathematics of Operations Research},
volume = {27},
number = {4},
pages = {819-840},
year = {2002},
doi = {10.1287/moor.27.4.819.297},

URL = { 
        https://doi.org/10.1287/moor.27.4.819.297
    
},
eprint = { 
        https://doi.org/10.1287/moor.27.4.819.297
    
}
,
    abstract = { We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we consider provably do not admit polynomial-time algorithms. Furthermore, assuming EXP ≠ NEXP, the problems require superexponential time to solve in the worst case. }
}

@inproceedings{boutilier1996planning,
  title={Planning, learning and coordination in multiagent decision processes},
  author={Boutilier, Craig},
  booktitle={Proceedings of the 6th conference on Theoretical aspects of rationality and knowledge},
  pages={195--210},
  year={1996},
  organization={Morgan Kaufmann Publishers Inc.}
}

@article {Shapley1953stochasticgames,
	author = {Shapley, L. S.},
	title = {Stochastic Games},
	volume = {39},
	number = {10},
	pages = {1095--1100},
	year = {1953},
	doi = {10.1073/pnas.39.10.1095},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{pettingZoo2020,
  Title = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
  Author = {Terry, J K and Black, Benjamin and Jayakumar, Mario and Hari, Ananth and Santos, Luis and Dieffendahl, Clemens and Williams, Niall L and Lokesh, Yashas and Sullivan, Ryan and Horsch, Caroline and Ravi, Praveen},
  journal={arXiv preprint arXiv:2009.14471},
  year={2020}
}

@inproceedings{lanctot2017unified,
  title={A unified game-theoretic approach to multiagent reinforcement learning},
  author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4190--4203},
  year={2017}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{yang2018mean,
  title={Mean field multi-agent reinforcement learning},
  author={Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:1802.05438},
  year={2018}
}

@techreport{deepmindcontrolsuite2018,
  title = {Deep{Mind} Control Suite},
  author = {Yuval Tassa and Yotam Doron and Alistair Muldal and Tom Erez
            and Yazhe Li and Diego de Las Casas and David Budden and Abbas
            Abdolmaleki and Josh Merel and Andrew Lefrancq and Timothy Lillicrap
            and Martin Riedmiller},
  year = 2018,
  month = jan,
  howpublished = {https://arxiv.org/abs/1801.00690},
  url = {https://arxiv.org/abs/1801.00690},
  volume = {abs/1504.04804},
  institution = {DeepMind},
}

@article{berner2019dota,
  title={Dota 2 with Large Scale Deep Reinforcement Learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{TerryParameterSharing,
  Title = {Revisiting Parameter Sharing In Multi-Agent Deep Reinforcement Learning},
  Author = {Terry, J K and Grammel, Nathaniel and Hari, Ananth and Santos, Luis and Black, Benjamin},
  journal={arXiv preprint arXiv:2005.13625},
  year={2020}
}


@article{horgan2018distributed,
  title={Distributed prioritized experience replay},
  author={Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}


@article{leibo2017multi,
  title={Multi-agent reinforcement learning in sequential social dilemmas},
  author={Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  journal={arXiv preprint arXiv:1702.03037},
  year={2017}
}

@inproceedings{hughes2018inequity,
  title={Inequity aversion improves cooperation in intertemporal social dilemmas},
  author={Hughes, Edward and Leibo, Joel Z and Phillips, Matthew and Tuyls, Karl and Due{\~n}ez-Guzman, Edgar and Casta{\~n}eda, Antonio Garc{\'\i}a and Dunning, Iain and Zhu, Tina and McKee, Kevin and Koster, Raphael and others},
  booktitle={Advances in neural information processing systems},
  pages={3326--3336},
  year={2018}
}


@inproceedings{amato2014planning,
author = {Amato, Christopher and Konidaris, George D. and Kaelbling, Leslie P.},
title = {Planning with Macro-Actions in Decentralized POMDPs},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for decentralized decision making under uncertainty. However, they typically model a problem at a low level of granularity, where each agent's actions are primitive operations lasting exactly one time step. We address the case where each agent has macro-actions: temporally extended actions which may require different amounts of time to execute. We model macro-actions as 'options' in a factored Dec-POMDP model, focusing on options which depend only on information available to an individual agent while executing. This enables us to model systems where coordination decisions only occur at the level of deciding which macro-actions to execute, and the macro-actions themselves can then be executed to completion. The core technical difficulty when using options in a Dec-POMDP is that the options chosen by the agents no longer terminate at the same time. We present extensions of two leading Dec-POMDP algorithms for generating a policy with options and discuss the resulting form of optimality. Our results show that these algorithms retain agent coordination while allowing near-optimal solutions to be generated for significantly longer horizons and larger state-spaces than previous Dec-POMDP methods.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1273–1280},
numpages = {8},
keywords = {decentralized pomdps, hierarchy, planning under uncertainty},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{messias2013asynchronous,
  title={Asynchronous execution in multiagent POMDPs: Reasoning over partially-observable events},
  author={Messias, Joao V and Spaan, Matthijs TJ and Lima, Pedro U},
  booktitle={AAMAS},
  volume={13},
  pages={9--14},
  year={2013}
}

@book{osborne1994gametheory,
  title={A course in game theory},
  author={Osborne, Martin J and Rubinstein, Ariel},
  year={1994},
  publisher={MIT press}
}

@misc{stable-baselines3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@misc{cleanrl,
  author = {Huang, Shengyi and Dossa, Rousslan  and Chang Ye},
  title = {CleanRL: High-quality Single-file Implementation of Deep Reinforcement Learning algorithms},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/vwxyzjn/cleanrl/}},
}

@article{SuperSuit,
  Title = {SuperSuit: Simple Microwrappers for Reinforcement Learning Environments},
  Author = {Terry, J K and Black, Benjamin and Hari, Ananth},
  journal={arXiv preprint arXiv:2008.08932},
  year={2020}
}

@misc{AI-Traineree,
  author = {Dawid Laszuk},
  title = {AI-Traineree},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/laszukdawid/ai-traineree}},
}

@misc{Fanorona_AEC,
  author = {Abhijeet Krishnan},
  title = {Fanorona AEC},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AbhijeetKrishnan/fanorona-aec}}},

@misc{pz_dilemma,
  author = {Abhijeet Krishnan},
  title = {Arjun Prakash},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/arjun-prakash/pz_dilemma}}},



 @misc{openai, 
 title={openai/gym dependents},
 url={https://web.archive.org/web/20210527224052/https://github.com/openai/gym/network/dependents?dependent_type=PACKAGE},
 journal={GitHub}, 
 author={GitHub},
 year=2021} 
 
 @misc{rl-zoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}

@article{terry2020revisiting,
  title={Revisiting parameter sharing in multi-agent deep reinforcement learning},
  author={Terry, J K and Grammel, Nathaniel and Hari, Ananth and Santos, Luis and Black, Benjamin},
  journal={arXiv preprint arXiv:2005.13625},
  year={2020}
}