\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Duchi(2012)]{agarwal2012distributed}
Alekh Agarwal and John~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{Proc. {CDC}}, pages 5451--5452, 2012.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and Rabbat]{assran2019sgp}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael~G. Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{Proc. {ICML}}, volume~97, pages 344--353, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock In \emph{{ICLR}}, 2016.

\bibitem[Davis et~al.(1930)Davis, Gardner, and Gardner]{davis1930socialwomen}
Allison Davis, Burleigh~Bradford Gardner, and Mary~R Gardner.
\newblock \emph{{Deep South}: A social anthropological study of caste and
  class}.
\newblock Univ of South Carolina Press, 1930.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Li]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li{-}Jia Li, Kai Li, and Fei{-}Fei Li.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{Proc. {CVPR}}, pages 248--255, 2009.

\bibitem[Georgopoulos(2011)]{Georgopoulos2011}
Leonidas Georgopoulos.
\newblock \emph{Definitive Consensus for Distributed Data Inference}.
\newblock PhD thesis, EPFL, 2011.

\bibitem[Hagberg et~al.(2008)Hagberg, Swart, and S~Chult]{hagberg2008networkx}
Aric Hagberg, Pieter Swart, and Daniel S~Chult.
\newblock Exploring network structure, dynamics, and function using {NetworkX}.
\newblock Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
  (United States), 2008.

\bibitem[Hendrickx et~al.(2014)Hendrickx, Jungers, Olshevsky, and
  Vankeerberghen]{Hendrickx2014:finitetime}
Julien~M. Hendrickx, RaphaÃ«l~M. Jungers, Alexander Olshevsky, and Guillaume
  Vankeerberghen.
\newblock Graph diameter, eigenvalues, and minimum-time consensus.
\newblock \emph{Automatica}, 50\penalty0 (2):\penalty0 635--640, 2014.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proc. {ICML}}, volume~37, pages 448--456, 2015.

\bibitem[Jeaugey(2019)]{nvidia2019speeding}
Sylvain Jeaugey.
\newblock Massively scale your deep learning training with {NCCL} 2.4.
\newblock
  \url{https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/},
  2019.
\newblock [Online; accessed 21-May-2019].

\bibitem[Johansson et~al.(2009)Johansson, Rabi, and
  Johansson]{johansson2009randomized}
Bj{\"{o}}rn Johansson, Maben Rabi, and Mikael Johansson.
\newblock A randomized incremental subgradient method for distributed
  optimization in networked systems.
\newblock \emph{{SIAM} J. Optim.}, 20\penalty0 (3):\penalty0 1157--1170, 2009.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Rouayheb, Evans, Gardner,
  Garrett, Gasc{\'{o}}n, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo,
  Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konecn{\'{y}}, Korolova,
  Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, {\"{O}}zg{\"{u}}r,
  Pagh, Raykova, Qi, Ramage, Raskar, Song, Song, Stich, Sun, Suresh,
  Tram{\`{e}}r, Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and
  Zhao]{kairouz2019federated}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aur{\'{e}}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, Rafael G.~L. D'Oliveira, Salim~El Rouayheb, David Evans,
  Josh Gardner, Zachary Garrett, Adri{\`{a}} Gasc{\'{o}}n, Badih Ghazi,
  Phillip~B. Gibbons, Marco Gruteser, Za{\"{\i}}d Harchaoui, Chaoyang He, Lie
  He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,
  Gauri Joshi, Mikhail Khodak, Jakub Konecn{\'{y}}, Aleksandra Korolova,
  Farinaz Koushanfar, Sanmi Koyejo, Tancr{\`{e}}de Lepoint, Yang Liu, Prateek
  Mittal, Mehryar Mohri, Richard Nock, Ayfer {\"{O}}zg{\"{u}}r, Rasmus Pagh,
  Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang
  Song, Sebastian~U. Stich, Ziteng Sun, Ananda~Theertha Suresh, Florian
  Tram{\`{e}}r, Praneeth Vepakomma, Jianyu Wang, Li~Xiong, Zheng Xu, Qiang
  Yang, Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv}, abs/1912.04977, 2019.

\bibitem[Kempe et~al.(2003)Kempe, Dobra, and Gehrke]{kempe2003pushsum}
David Kempe, Alin Dobra, and Johannes Gehrke.
\newblock Gossip-based computation of aggregate information.
\newblock In \emph{{FOCS}}, pages 482--491, 2003.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Ko(2010)]{Ko2010}
Chih-Kai Ko.
\newblock \emph{On Matrix Factorization and Scheduling forFinite-time
  Average-consensus}.
\newblock PhD thesis, California Institute of Technology, 2010.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2021unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U. Stich.
\newblock A unified theory of decentralized {SGD} with changing topology and
  local updates.
\newblock In \emph{Proc. {ICML}}, volume 119, pages 5381--5393, 2020.

\bibitem[Krizhevsky(2012)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{University of Toronto}, 05 2012.

\bibitem[Krizhevsky et~al.()Krizhevsky, Nair, and Hinton]{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 ({Canadian Institute for Advanced Research}).

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017dpsgd}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho{-}Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{{NeurIPS}}, pages 5330--5340, 2017.

\bibitem[Lin et~al.(2021)Lin, Karimireddy, Stich, and
  Jaggi]{lin2021quasiglobal}
Tao Lin, Sai~Praneeth Karimireddy, Sebastian~U. Stich, and Martin Jaggi.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock \emph{CoRR}, abs/2102.04761, 2021.

\bibitem[Liu et~al.(2020)Liu, Brock, Simonyan, and Le]{liu2020evonorm}
Hanxiao Liu, Andy Brock, Karen Simonyan, and Quoc Le.
\newblock Evolving normalization-activation layers.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[Lorenzo and Scutari(2016)]{Lorenzo2016GT-first-paper}
Paolo~Di Lorenzo and Gesualdo Scutari.
\newblock Next: In-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, 2016.

\bibitem[Lu and De~Sa(2021)]{pmlr-v139-lu21a}
Yucheng Lu and Christopher De~Sa.
\newblock Optimal complexity in decentralized training.
\newblock In \emph{Proc. {ICML}}, volume 139, pages 7111--7123, 18--24 Jul
  2021.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017fedavg}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proc. {ICOAI}}, volume~54, pages 1273--1282, 2017.

\bibitem[Nedic(2020)]{nedic2020review}
Angelia Nedic.
\newblock Distributed gradient methods for convex machine learning problems in
  networks: Distributed optimization.
\newblock \emph{{IEEE} Signal Process. Mag.}, 37\penalty0 (3):\penalty0
  92--101, 2020.

\bibitem[Nedic and Olshevsky(2016)]{nedic2016sgp}
Angelia Nedic and Alex Olshevsky.
\newblock Stochastic gradient-push for strongly convex functions on
  time-varying directed graphs.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 61\penalty0 (12):\penalty0
  3936--3947, 2016.

\bibitem[Nedi{\'c} and Ozdaglar(2010)]{nedic2010convergence}
Angelia Nedi{\'c} and Asuman Ozdaglar.
\newblock Convergence rate for consensus with delays.
\newblock \emph{Journal of Global Optimization}, 47\penalty0 (3):\penalty0
  437--456, 2010.

\bibitem[Nedic and Ozdaglar(2009)]{Nedic2009:distributedsubgrad}
Angelia Nedic and Asuman~E. Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 54\penalty0 (1):\penalty0
  48--61, 2009.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017diging}
Angelia Nedic, Alex Olshevsky, and Wei Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{{SIAM} J. Optim.}, 27\penalty0 (4):\penalty0 2597--2633, 2017.

\bibitem[Perlman(1985)]{perlman85spanningtreeprotocol}
Radia~J. Perlman.
\newblock An algorithm for distributed computation of a spanningtree in an
  extended {LAN}.
\newblock In \emph{{SIGCOMM}}, pages 44--53, 1985.

\bibitem[Pu and Nedic(2018)]{pu2018dsgt}
Shi Pu and Angelia Nedic.
\newblock Distributed stochastic gradient tracking methods.
\newblock \emph{CoRR}, abs/1805.11454, 2018.

\bibitem[Pu et~al.(2021)Pu, Shi, Xu, and Nedic]{pu2021pushpull}
Shi Pu, Wei Shi, Jinming Xu, and Angelia Nedic.
\newblock Push-pull gradient methods for distributed optimization in networks.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 66\penalty0 (1):\penalty0
  1--16, 2021.

\bibitem[Sanders et~al.(2009)Sanders, Speck, and Tr{\"{a}}ff]{sanders2009trees}
Peter Sanders, Jochen Speck, and Jesper~Larsson Tr{\"{a}}ff.
\newblock Two-tree algorithms for full bandwidth broadcast, reduction and scan.
\newblock \emph{Parallel Comput.}, 35\penalty0 (12):\penalty0 581--594, 2009.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of {BERT:} smaller, faster, cheaper
  and lighter.
\newblock \emph{CoRR}, abs/1910.01108, 2019.

\bibitem[Stich(2019)]{stich2019unified}
Sebastian~U. Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock \emph{CoRR}, abs/1907.04232, 2019.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d2}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D\({}^{\mbox{2}}\): Decentralized training over decentralized data.
\newblock In \emph{Proc. {ICML}}, volume~80, pages 4855--4863, 2018.

\bibitem[Tsianos and Rabbat(2011)]{tsianos2011distributed}
Konstantinos~I. Tsianos and Michael~G. Rabbat.
\newblock Distributed consensus and optimization under communication delays.
\newblock In \emph{Allerton}, pages 974--982, 2011.

\bibitem[Tsianos et~al.(2012)Tsianos, Lawlor, and Rabbat]{tsianos2012push}
Konstantinos~I. Tsianos, Sean~F. Lawlor, and Michael~G. Rabbat.
\newblock Push-sum distributed dual averaging for convex optimization.
\newblock In \emph{Proc. {CDC}}, pages 5453--5458, 2012.

\bibitem[Xi and Khan(2017)]{xi2017dextra}
Chenguang Xi and Usman~A. Khan.
\newblock {DEXTRA:} {A} fast algorithm for optimization over directed graphs.
\newblock \emph{{IEEE} Trans. Automat. Contr.}, 62\penalty0 (10):\penalty0
  4980--4993, 2017.

\bibitem[Xi et~al.(2018)Xi, Mai, Xin, Abed, and Khan]{xi2017linear}
Chenguang Xi, Van~Sy Mai, Ran Xin, Eyad~H. Abed, and Usman~A. Khan.
\newblock Linear convergence in optimization over directed graphs with
  row-stochastic matrices.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 63\penalty0 (10):\penalty0
  3558--3565, 2018.

\bibitem[Xiao and Boyd(2004)]{xiao2004distavg}
Lin Xiao and Stephen~P. Boyd.
\newblock Fast linear iterations for distributed averaging.
\newblock \emph{Syst. Control. Lett.}, 53\penalty0 (1):\penalty0 65--78, 2004.

\bibitem[Xin and Khan(2018)]{xin2018linear}
Ran Xin and Usman~A. Khan.
\newblock A linear algorithm for optimization over directed graphs with
  geometric convergence.
\newblock \emph{{IEEE} Control. Syst. Lett.}, 2\penalty0 (3):\penalty0
  315--320, 2018.

\bibitem[Xin and Khan(2020)]{xin2019distributed}
Ran Xin and Usman~A. Khan.
\newblock Distributed heavy-ball: {A} generalization and acceleration of
  first-order methods with gradient tracking.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 65\penalty0 (6):\penalty0
  2627--2633, 2020.

\bibitem[Xin et~al.(2019)Xin, Xi, and Khan]{xin2019frost}
Ran Xin, Chenguang Xi, and Usman~A. Khan.
\newblock {FROST} - fast row-stochastic optimization with uncoordinated
  step-sizes.
\newblock \emph{{EURASIP} J. Adv. Signal Process.}, 2019:\penalty0 1, 2019.

\bibitem[Yuan et~al.(2019)Yuan, Ying, Zhao, and Sayed]{yuan2019exact-diff-1}
Kun Yuan, Bicheng Ying, Xiaochuan Zhao, and Ali~H. Sayed.
\newblock Exact diffusion for distributed optimization and learning - part {I:}
  algorithm development.
\newblock \emph{{IEEE} Trans. Signal Process.}, 67\penalty0 (3):\penalty0
  708--723, 2019.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Huang, Zhang, Pan, Xu, and
  Yin]{yuan2021decentlam}
Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and
  Wotao Yin.
\newblock Decentlam: Decentralized momentum {SGD} for large-batch deep
  training.
\newblock \emph{CoRR}, abs/2104.11981, 2021.

\bibitem[Yurochkin et~al.(2019)Yurochkin, Agarwal, Ghosh, Greenewald, Hoang,
  and Khazaeni]{yurochkin2019bayesian}
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan~H. Greenewald,
  Trong~Nghia Hoang, and Yasaman Khazaeni.
\newblock Bayesian nonparametric federated learning of neural networks.
\newblock In \emph{Proc. {ICML}}, volume~97, pages 7252--7261, 2019.

\bibitem[Zhang and You(2020)]{zhang2020GT-non-convex}
Jiaqi Zhang and Keyou You.
\newblock Decentralized stochastic gradient tracking for non-convex empirical
  risk minimization, 2020.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{zhang2015character}
Xiang Zhang, Junbo~Jake Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{{NeurIPS}}, pages 649--657, 2015.

\bibitem[Zhang et~al.(2019)Zhang, Xie, Cai, and Fu]{zhang2019bp}
Zhaorong Zhang, Kan Xie, Qianqian Cai, and Minyue Fu.
\newblock A bp-like distributed algorithm for weighted average consensus.
\newblock In \emph{Proc. {ASCC}}, pages 728--733, 2019.

\end{thebibliography}
