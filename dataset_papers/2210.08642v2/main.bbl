\begin{thebibliography}{}

\bibitem[Bassen et~al., 2020]{bassen2020reinforcement}
Bassen, J., Balaji, B., Schaarschmidt, M., Thille, C., Painter, J., Zimmaro,
  D., Games, A., Fast, E., and Mitchell, J.~C. (2020).
\newblock Reinforcement learning for the adaptive scheduling of educational
  activities.
\newblock In {\em Proceedings of the 2020 CHI Conference on Human Factors in
  Computing Systems}, pages 1--12.

\bibitem[Burman, 1989]{burman1989comparative}
Burman, P. (1989).
\newblock A comparative study of ordinary cross-validation, v-fold
  cross-validation and the repeated learning-testing methods.
\newblock {\em Biometrika}, 76(3):503--514.

\bibitem[Cheng et~al., 2021]{cheng2021heuristic}
Cheng, C.-A., Kolobov, A., and Swaminathan, A. (2021).
\newblock Heuristic-guided reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Chernozhukov et~al., 2016]{chernozhukov2016double}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey,
  W., and Robins, J. (2016).
\newblock Double/debiased machine learning for treatment and causal parameters.
\newblock {\em arXiv preprint arXiv:1608.00060}.

\bibitem[Chernozhukov et~al., 2018]{chernozhukov2018generic}
Chernozhukov, V., Demirer, M., Duflo, E., and Fernandez-Val, I. (2018).
\newblock Generic machine learning inference on heterogeneous treatment effects
  in randomized experiments, with an application to immunization in india.
\newblock Technical report, National Bureau of Economic Research.

\bibitem[Dietterich, 1998]{dietterich1998approximate}
Dietterich, T.~G. (1998).
\newblock Approximate statistical tests for comparing supervised classification
  learning algorithms.
\newblock {\em Neural computation}, 10(7):1895--1923.

\bibitem[Dubitzky et~al., 2007]{dubitzky2007fundamentals}
Dubitzky, W., Granzow, M., and Berrar, D.~P. (2007).
\newblock {\em Fundamentals of data mining in genomics and proteomics}.
\newblock Springer Science \& Business Media.

\bibitem[Efron, 1983]{efron1983estimating}
Efron, B. (1983).
\newblock Estimating the error rate of a prediction rule: improvement on
  cross-validation.
\newblock {\em Journal of the American statistical association},
  78(382):316--331.

\bibitem[Efron, 1986]{efron1986biased}
Efron, B. (1986).
\newblock How biased is the apparent error rate of a prediction rule?
\newblock {\em Journal of the American statistical Association},
  81(394):461--470.

\bibitem[Farajtabar et~al., 2018]{farajtabar2018more}
Farajtabar, M., Chow, Y., and Ghavamzadeh, M. (2018).
\newblock More robust doubly robust off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  1447--1456. PMLR.

\bibitem[Fu et~al., 2020]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020).
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}.

\bibitem[Fujimoto et~al., 2019]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D. (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2052--2062. PMLR.

\bibitem[Futoma et~al., 2020]{futoma2020popcorn}
Futoma, J., Hughes, M.~C., and Doshi-Velez, F. (2020).
\newblock Popcorn: Partially observed prediction constrained reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2001.04032}.

\bibitem[Geisser, 1975]{geisser1975predictive}
Geisser, S. (1975).
\newblock The predictive sample reuse method with applications.
\newblock {\em Journal of the American statistical Association},
  70(350):320--328.

\bibitem[Gilpin, 1993]{gilpin1993table}
Gilpin, A.~R. (1993).
\newblock Table for conversion of kendall's tau to spearman's rho within the
  context of measures of magnitude of effect for meta-analysis.
\newblock {\em Educational and psychological measurement}, 53(1):87--92.

\bibitem[Guo et~al., 2020]{guo2020batch}
Guo, Y., Feng, S., Le~Roux, N., Chi, E., Lee, H., and Chen, M. (2020).
\newblock Batch reinforcement learning through continuation method.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Jiang et~al., 2015]{jiang2015dependence}
Jiang, N., Kulesza, A., Singh, S., and Lewis, R. (2015).
\newblock The dependence of effective planning horizon on model accuracy.
\newblock In {\em Proceedings of the 2015 International Conference on
  Autonomous Agents and Multiagent Systems}, pages 1181--1189. Citeseer.

\bibitem[Kidambi et~al., 2020]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2020).
\newblock Morel: Model-based offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  33:21810--21823.

\bibitem[Komorowski et~al., 2018]{komorowski2018artificial}
Komorowski, M., Celi, L.~A., Badawi, O., Gordon, A.~C., and Faisal, A.~A.
  (2018).
\newblock The artificial intelligence clinician learns optimal treatment
  strategies for sepsis in intensive care.
\newblock {\em Nature medicine}, 24(11):1716--1720.

\bibitem[Krzanowski and Hand, 1997]{krzanowski1997assessing}
Krzanowski, W. and Hand, D. (1997).
\newblock Assessing error rate estimators: the leave-one-out method
  reconsidered.
\newblock {\em Australian Journal of Statistics}, 39(1):35--46.

\bibitem[Kumar et~al., 2021]{kumar2021a}
Kumar, A., Singh, A., Tian, S., Finn, C., and Levine, S. (2021).
\newblock A workflow for offline model-free robotic reinforcement learning.
\newblock In {\em 5th Annual Conference on Robot Learning}.

\bibitem[Kumar et~al., 2020]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020).
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1179--1191.

\bibitem[Le et~al., 2019]{le2019batch}
Le, H., Voloshin, C., and Yue, Y. (2019).
\newblock Batch policy learning under constraints.
\newblock In {\em International Conference on Machine Learning}, pages
  3703--3712. PMLR.

\bibitem[Lee et~al., 2021]{lee2021model}
Lee, J.~N., Tucker, G., Nachum, O., and Dai, B. (2021).
\newblock Model selection in batch policy optimization.
\newblock {\em arXiv preprint arXiv:2112.12320}.

\bibitem[Levine et~al., 2020]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}.

\bibitem[Liao et~al., 2020]{liao2020personalized}
Liao, P., Greenewald, K., Klasnja, P., and Murphy, S. (2020).
\newblock Personalized heartsteps: A reinforcement learning algorithm for
  optimizing physical activity.
\newblock {\em Proceedings of the ACM on Interactive, Mobile, Wearable and
  Ubiquitous Technologies}, 4(1):1--22.

\bibitem[Liu et~al., 2019]{DBLP:conf/uai/LiuSAB19}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2019).
\newblock Off-policy policy gradient with stationary distribution correction.
\newblock In Globerson, A. and Silva, R., editors, {\em Proceedings of the
  Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, {UAI}
  2019, Tel Aviv, Israel, July 22-25, 2019}, volume 115 of {\em Proceedings of
  Machine Learning Research}, pages 1180--1190. {AUAI} Press.

\bibitem[Liu et~al., 2020]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2020).
\newblock Provably good batch reinforcement learning without great exploration.
\newblock {\em Advances in neural information processing systems}, 33.

\bibitem[Mandlekar et~al., 2020]{mandlekar2020iris}
Mandlekar, A., Ramos, F., Boots, B., Savarese, S., Fei-Fei, L., Garg, A., and
  Fox, D. (2020).
\newblock Iris: Implicit reinforcement without interaction at scale for
  learning control from offline robot manipulation data.
\newblock In {\em 2020 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 4414--4420. IEEE.

\bibitem[Mandlekar et~al., 2021]{mandlekar2021what}
Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R.,
  Fei-Fei, L., Savarese, S., Zhu, Y., and Mart{\'\i}n-Mart{\'\i}n, R. (2021).
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock In {\em 5th Annual Conference on Robot Learning}.

\bibitem[Mandlekar et~al., 2018]{mandlekar2018roboturk}
Mandlekar, A., Zhu, Y., Garg, A., Booher, J., Spero, M., Tung, A., Gao, J.,
  Emmons, J., Gupta, A., Orbay, E., et~al. (2018).
\newblock Roboturk: A crowdsourcing platform for robotic skill learning through
  imitation.
\newblock In {\em Conference on Robot Learning}, pages 879--893. PMLR.

\bibitem[Mazoure et~al., 2021]{mazoure2021improving}
Mazoure, B., Mineiro, P., Srinath, P., Sedeh, R.~S., Precup, D., and
  Swaminathan, A. (2021).
\newblock Improving long-term metrics in recommendation systems using
  short-horizon reinforcement learning.
\newblock {\em arXiv preprint arXiv:2106.00589}.

\bibitem[Metelli et~al., 2018]{metelli2018policy}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M. (2018).
\newblock Policy optimization via importance sampling.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Miyaguchi, 2022]{miyaguchi2022theoretical}
Miyaguchi, K. (2022).
\newblock A theoretical framework of almost hyperparameter-free hyperparameter
  selection methods for offline policy evaluation.
\newblock {\em arXiv preprint arXiv:2201.02300}.

\bibitem[Oberst and Sontag, 2019]{oberst2019counterfactual}
Oberst, M. and Sontag, D. (2019).
\newblock Counterfactual off-policy evaluation with gumbel-max structural
  causal models.
\newblock In {\em International Conference on Machine Learning}, pages
  4881--4890. PMLR.

\bibitem[Owen, 2013]{owen2013monte}
Owen, A.~B. (2013).
\newblock Monte carlo theory, methods and examples.

\bibitem[Paine et~al., 2020]{paine2020hyperparameter}
Paine, T.~L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A.,
  Wang, Z., and de~Freitas, N. (2020).
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2007.09055}.

\bibitem[Pomerleau, 1991]{pomerleau1991efficient}
Pomerleau, D.~A. (1991).
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock {\em Neural computation}, 3(1):88--97.

\bibitem[Precup, 2000]{precup2000eligibility}
Precup, D. (2000).
\newblock Eligibility traces for off-policy policy evaluation.
\newblock {\em Computer Science Department Faculty Publication Series},
  page~80.

\bibitem[Shi et~al., 2020]{shi2020does}
Shi, C., Wan, R., Song, R., Lu, W., and Leng, L. (2020).
\newblock Does the markov decision process fit the data: Testing for the markov
  property in sequential decision making.
\newblock In {\em International Conference on Machine Learning}, pages
  8807--8817. PMLR.

\bibitem[Siegel et~al., 2020]{Siegel2020Keep}
Siegel, N., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert, M.,
  Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020).
\newblock Keep doing what worked: Behavior modelling priors for offline
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Stone, 1974]{stone1974cross}
Stone, M. (1974).
\newblock Cross-validatory choice and assessment of statistical predictions.
\newblock {\em Journal of the royal statistical society: Series B
  (Methodological)}, 36(2):111--133.

\bibitem[Su et~al., 2020]{su2020adaptive}
Su, Y., Srinath, P., and Krishnamurthy, A. (2020).
\newblock Adaptive estimator selection for off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  9196--9205. PMLR.

\bibitem[Tang and Wiens, 2021]{tang2021model}
Tang, S. and Wiens, J. (2021).
\newblock Model selection for offline reinforcement learning: Practical
  considerations for healthcare settings.
\newblock In {\em Machine Learning for Healthcare Conference}, pages 2--35.
  PMLR.

\bibitem[Thomas and Brunskill, 2016]{thomas2016data}
Thomas, P. and Brunskill, E. (2016).
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2139--2148. PMLR.

\bibitem[Thomas et~al., 2015a]{pmlr-v37-thomas15}
Thomas, P., Theocharous, G., and Ghavamzadeh, M. (2015a).
\newblock High confidence policy improvement.
\newblock In Bach, F. and Blei, D., editors, {\em Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of {\em Proceedings
  of Machine Learning Research}, pages 2380--2388, Lille, France. PMLR.

\bibitem[Thomas et~al., 2015b]{thomas2015high}
Thomas, P., Theocharous, G., and Ghavamzadeh, M. (2015b).
\newblock High confidence policy improvement.
\newblock In {\em International Conference on Machine Learning}, pages
  2380--2388. PMLR.

\bibitem[Thomas et~al., 2019]{thomas2019preventing}
Thomas, P.~S., Castro~da Silva, B., Barto, A.~G., Giguere, S., Brun, Y., and
  Brunskill, E. (2019).
\newblock Preventing undesirable behavior of intelligent machines.
\newblock {\em Science}, 366(6468):999--1004.

\bibitem[Voloshin et~al., 2021]{voloshin2021empirical}
Voloshin, C., Le, H.~M., Jiang, N., and Yue, Y. (2021).
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}.

\bibitem[Xie and Jiang, 2021]{xie2021batch}
Xie, T. and Jiang, N. (2021).
\newblock Batch value-function approximation with only realizability.
\newblock In {\em International Conference on Machine Learning}, pages
  11404--11413. PMLR.

\bibitem[Yin et~al., 2021]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X. (2021).
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock {\em Advances in neural information processing systems}, 34.

\bibitem[Yu et~al., 2020]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T. (2020).
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  33:14129--14142.

\bibitem[Zanette and Brunskill, 2019]{zanette2019tighter}
Zanette, A. and Brunskill, E. (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em International Conference on Machine Learning}, pages
  7304--7312. PMLR.

\bibitem[Zhang and Jiang, 2021]{zhang2021towards}
Zhang, S. and Jiang, N. (2021).
\newblock Towards hyperparameter-free policy selection for offline
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\end{thebibliography}
