\begin{thebibliography}{10}

\bibitem{azar2023general}
M.~G. Azar, M.~Rowland, B.~Piot, D.~Guo, D.~Calandriello, M.~Valko, and R.~Munos.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock {\em arXiv preprint arXiv:2310.12036}, 2023.

\bibitem{bai2022training}
Y.~Bai, A.~Jones, K.~Ndousse, A.~Askell, A.~Chen, N.~DasSarma, D.~Drain, S.~Fort, D.~Ganguli, T.~Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock {\em arXiv preprint arXiv:2204.05862}, 2022.

\bibitem{bradley1952rank}
R.~A. Bradley and M.~E. Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock {\em Biometrika}, (3/4):324--345, 1952.

\bibitem{chen2021spann}
Q.~Chen, B.~Zhao, H.~Wang, M.~Li, C.~Liu, Z.~Li, M.~Yang, and J.~Wang.
\newblock Spann: Highly-efficient billion-scale approximate nearest neighborhood search.
\newblock {\em NeurIPS}, 2021.

\bibitem{christiano2017deep}
P.~F. Christiano, J.~Leike, T.~Brown, M.~Martic, S.~Legg, and D.~Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock {\em NeurIPS}, 2017.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{frome2013devise}
A.~Frome, G.~S. Corrado, J.~Shlens, S.~Bengio, J.~Dean, M.~Ranzato, and T.~Mikolov.
\newblock Devise: A deep visual-semantic embedding model.
\newblock {\em NeurIPS}, 2013.

\bibitem{gadre2024datacomp}
S.~Y. Gadre, G.~Ilharco, A.~Fang, J.~Hayase, G.~Smyrnis, T.~Nguyen, R.~Marten, M.~Wortsman, D.~Ghosh, J.~Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock {\em NeurIPS}, 2023.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, 2020.

\bibitem{turnbull2010oxford}
A.~S. Hornby, J.~Turnbull, D.~Lea, D.~Parkinson, P.~Phillips, B.~Francis, S.~Webb, V.~Bull, and M.~Ashby.
\newblock Oxford advanced learner’s dictionary.
\newblock {\em International Student’s Edition}, 2010.

\bibitem{hosu2019effective}
V.~Hosu, B.~Goldlucke, and D.~Saupe.
\newblock Effective aesthetics prediction with multi-level spatially pooled features.
\newblock In {\em CVPR}, 2019.

\bibitem{KOSMOS}
S.~Huang, L.~Dong, W.~Wang, Y.~Hao, S.~Singhal, S.~Ma, T.~Lv, L.~Cui, O.~K. Mohammed, B.~Patra, et~al.
\newblock Language is not all you need: Aligning perception with language models.
\newblock {\em NeurIPS}, 2023.

\bibitem{gettyimages}
G.~Images.
\newblock Getty images.
\newblock \url{https://www.gettyimages.com}.

\bibitem{align}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung, Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In {\em ICML}, 2021.

\bibitem{joshi2011aesthetics}
D.~Joshi, R.~Datta, E.~Fedorovskaya, Q.-T. Luong, J.~Z. Wang, J.~Li, and J.~Luo.
\newblock Aesthetics and emotions in images.
\newblock {\em IEEE Signal Processing Magazine}, 28(5):94--115, 2011.

\bibitem{kazemi2020preference}
H.~Kazemi, F.~Taherkhani, and N.~Nasrabadi.
\newblock Preference-based image generation.
\newblock In {\em WACV}, 2020.

\bibitem{kong2016photo}
S.~Kong, X.~Shen, Z.~Lin, R.~Mech, and C.~Fowlkes.
\newblock Photo aesthetics ranking network with attributes and content adaptation.
\newblock In {\em ECCV}, 2016.

\bibitem{lee2023aligning}
K.~Lee, H.~Liu, M.~Ryu, O.~Watkins, Y.~Du, C.~Boutilier, P.~Abbeel, M.~Ghavamzadeh, and S.~S. Gu.
\newblock Aligning text-to-image models using human feedback.
\newblock {\em arXiv preprint arXiv:2302.12192}, 2023.

\bibitem{li2022blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In {\em ICML}, 2022.

\bibitem{li2021align}
J.~Li, R.~Selvaraju, A.~Gotmare, S.~Joty, C.~Xiong, and S.~C.~H. Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock {\em NeurIPS}, 2021.

\bibitem{li2021learning}
W.~Li, X.~Huang, J.~Lu, J.~Feng, and J.~Zhou.
\newblock Learning probabilistic ordinal embeddings for uncertainty-aware regression.
\newblock In {\em CVPR}, 2021.

\bibitem{li2020oscar}
X.~Li, X.~Yin, C.~Li, P.~Zhang, X.~Hu, L.~Zhang, L.~Wang, H.~Hu, L.~Dong, F.~Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language tasks.
\newblock In {\em ECCV}, 2020.

\bibitem{alpaca_eval}
X.~Li, T.~Zhang, Y.~Dubois, R.~Taori, I.~Gulrajani, C.~Guestrin, P.~Liang, and T.~B. Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan, P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{llava}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock {\em NeurIPS}, 2023.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{liu2022swin}
Z.~Liu, H.~Hu, Y.~Lin, Z.~Yao, Z.~Xie, Y.~Wei, J.~Ning, Y.~Cao, Z.~Zhang, L.~Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In {\em CVPR}, 2022.

\bibitem{lu2019vilbert}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.
\newblock {\em NeurIPS}, 2019.

\bibitem{luo2011content}
W.~Luo, X.~Wang, and X.~Tang.
\newblock Content-based photo quality assessment.
\newblock In {\em ICCV}, 2011.

\bibitem{bingsearch}
Microsoft.
\newblock Bing image search.
\newblock \url{https://www.bing.com/images/details/{0}}.

\bibitem{murray2012ava}
N.~Murray, L.~Marchesotti, and F.~Perronnin.
\newblock Ava: A large-scale database for aesthetic visual analysis.
\newblock In {\em CVPR}, 2012.

\bibitem{nieto2022understanding}
D.~V. Nieto, L.~Celona, and C.~Fernandez-Labrador.
\newblock Understanding aesthetics with language: A photo critique dataset for aesthetic assessment.
\newblock {\em arXiv preprint arXiv:2206.08614}, 2022.

\bibitem{gpt4v}
OpenAI.
\newblock Gpt-4v.
\newblock \url{https://openai.com/research/gpt-4v-system-card}, 2023.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang, S.~Agarwal, K.~Slama, A.~Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em NeurIPS}, 2022.

\bibitem{pinto2023tuning}
A.~S. Pinto, A.~Kolesnikov, Y.~Shi, L.~Beyer, and X.~Zhai.
\newblock Tuning computer vision models with task rewards.
\newblock In {\em ICML}, 2023.

\bibitem{clip}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em ICML}, 2021.

\bibitem{rafailov2024direct}
R.~Rafailov, A.~Sharma, E.~Mitchell, C.~D. Manning, S.~Ermon, and C.~Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock {\em NeurIPS}, 2023.

\bibitem{rennie2017self}
S.~J. Rennie, E.~Marcheret, Y.~Mroueh, J.~Ross, and V.~Goel.
\newblock Self-critical sequence training for image captioning.
\newblock In {\em CVPR}, 2017.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em CVPR}, 2022.

\bibitem{ipa}
C.~Schuhmann.
\newblock Laion-aesthetics predictor v2.
\newblock \url{https://github.com/christophschuhmann/improved-aesthetic-predictor}, 2022.

\bibitem{schuhmann2022laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti, T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock {\em NeurIPS}, 2022.

\bibitem{shao2021intern}
J.~Shao, S.~Chen, Y.~Li, K.~Wang, Z.~Yin, Y.~He, J.~Teng, Q.~Sun, M.~Gao, J.~Liu, et~al.
\newblock Intern: A new learning paradigm towards general vision.
\newblock {\em arXiv preprint arXiv:2111.08687}, 2021.

\bibitem{stiennon2020learning}
N.~Stiennon, L.~Ouyang, J.~Wu, D.~Ziegler, R.~Lowe, C.~Voss, A.~Radford, D.~Amodei, and P.~F. Christiano.
\newblock Learning to summarize with human feedback.
\newblock {\em NeurIPS}, 2020.

\bibitem{su2019vl}
W.~Su, X.~Zhu, Y.~Cao, B.~Li, L.~Lu, F.~Wei, and J.~Dai.
\newblock Vl-bert: Pre-training of generic visual-linguistic representations.
\newblock {\em ICLR}, 2020.

\bibitem{talebi2018nima}
H.~Talebi and P.~Milanfar.
\newblock Nima: Neural image assessment.
\newblock {\em TIP}, 2018.

\bibitem{wang2024secrets}
B.~Wang, R.~Zheng, L.~Chen, Y.~Liu, S.~Dou, C.~Huang, W.~Shen, S.~Jin, E.~Zhou, C.~Shi, et~al.
\newblock Secrets of rlhf in large language models part ii: Reward modeling.
\newblock {\em arXiv preprint arXiv:2401.06080}, 2024.

\bibitem{clipiqa}
J.~Wang, K.~C. Chan, and C.~C. Loy.
\newblock Exploring clip for assessing the look and feel of images.
\newblock In {\em AAAI}, 2023.

\bibitem{wang2016learning}
L.~Wang, Y.~Li, and S.~Lazebnik.
\newblock Learning deep structure-preserving image-text embeddings.
\newblock In {\em CVPR}, 2016.

\bibitem{beit3}
W.~Wang, H.~Bao, L.~Dong, J.~Bjorck, Z.~Peng, Q.~Liu, K.~Aggarwal, O.~K. Mohammed, S.~Singhal, S.~Som, et~al.
\newblock Image as a foreign language: Beit pretraining for all vision and vision-language tasks.
\newblock {\em arXiv preprint arXiv:2208.10442}, 2022.

\bibitem{wang2024visionllm}
W.~Wang, Z.~Chen, X.~Chen, J.~Wu, X.~Zhu, G.~Zeng, P.~Luo, T.~Lu, J.~Zhou, Y.~Qiao, et~al.
\newblock Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
\newblock {\em NeurIPS}, 2023.

\bibitem{wei2023iclip}
Y.~Wei, Y.~Cao, Z.~Zhang, H.~Peng, Z.~Yao, Z.~Xie, H.~Hu, and B.~Guo.
\newblock iclip: Bridging image classification and contrastive language-image pre-training for visual recognition.
\newblock In {\em CVPR}, 2023.

\bibitem{xie2022simmim}
Z.~Xie, Z.~Zhang, Y.~Cao, Y.~Lin, J.~Bao, Z.~Yao, Q.~Dai, and H.~Hu.
\newblock Simmim: A simple framework for masked image modeling.
\newblock In {\em CVPR}, 2022.

\bibitem{xu2024imagereward}
J.~Xu, X.~Liu, Y.~Wu, Y.~Tong, Q.~Li, M.~Ding, J.~Tang, and Y.~Dong.
\newblock Imagereward: Learning and evaluating human preferences for text-to-image generation.
\newblock {\em NeurIPS}, 2024.

\bibitem{unicl}
J.~Yang, C.~Li, P.~Zhang, B.~Xiao, C.~Liu, L.~Yuan, and J.~Gao.
\newblock Unified contrastive learning in image-text-label space.
\newblock In {\em CVPR}, 2022.

\bibitem{maniqa}
S.~Yang, T.~Wu, S.~Shi, S.~Lao, Y.~Gong, M.~Cao, J.~Wang, and Y.~Yang.
\newblock Maniqa: Multi-dimension attention network for no-reference image quality assessment.
\newblock In {\em CVPR}, 2022.

\bibitem{young2014image}
P.~Young, A.~Lai, M.~Hodosh, and J.~Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
\newblock {\em Transactions of the Association for Computational Linguistics}, 2014.

\bibitem{yu2022coca}
J.~Yu, Z.~Wang, V.~Vasudevan, L.~Yeung, M.~Seyedhosseini, and Y.~Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock {\em Transactions on Machine Learning Research}, 2022.

\bibitem{yuan2021florence}
L.~Yuan, D.~Chen, Y.-L. Chen, N.~Codella, X.~Dai, J.~Gao, H.~Hu, X.~Huang, B.~Li, C.~Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock {\em arXiv preprint arXiv:2111.11432}, 2021.

\bibitem{yuan2023rrhf}
Z.~Yuan, H.~Yuan, C.~Tan, W.~Wang, S.~Huang, and F.~Huang.
\newblock Rrhf: Rank responses to align language models with human feedback without tears.
\newblock {\em NeurIPS}, 2023.

\bibitem{zhang2022contrastive}
Y.~Zhang, H.~Jiang, Y.~Miura, C.~D. Manning, and C.~P. Langlotz.
\newblock Contrastive learning of medical visual representations from paired images and text.
\newblock In {\em Machine Learning for Healthcare Conference}, 2022.

\bibitem{zheng2023secrets}
R.~Zheng, S.~Dou, S.~Gao, Y.~Hua, W.~Shen, B.~Wang, Y.~Liu, S.~Jin, Q.~Liu, Y.~Zhou, et~al.
\newblock Secrets of rlhf in large language models part i: Ppo.
\newblock {\em arXiv preprint arXiv:2307.04964}, 2023.

\bibitem{ziegler2019fine}
D.~M. Ziegler, N.~Stiennon, J.~Wu, T.~B. Brown, A.~Radford, D.~Amodei, P.~Christiano, and G.~Irving.
\newblock Fine-tuning language models from human preferences.
\newblock {\em arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
