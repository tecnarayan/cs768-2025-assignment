\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  449--458. PMLR, 2017.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Castro et~al.(2018)Castro, Moitra, Gelada, Kumar, and
  Bellemare]{castro2018dopamine}
Pablo~Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc~G
  Bellemare.
\newblock Dopamine: A research framework for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.06110}, 2018.

\bibitem[Chen et~al.(2017)Chen, Sidor, Abbeel, and Schulman]{chen2017ucb}
Richard~Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman.
\newblock Ucb exploration via q-ensembles.
\newblock \emph{arXiv preprint arXiv:1706.01502}, 2017.

\bibitem[Choi et~al.(2019)Choi, Lee, and Oh]{choi2019distributional}
Yunho Choi, Kyungjae Lee, and Songhwai Oh.
\newblock Distributional deep reinforcement learning with a mixture of
  gaussians.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pages 9791--9797. IEEE, 2019.

\bibitem[Chow and Ghavamzadeh(2014)]{chow2014algorithms}
Yinlam Chow and Mohammad Ghavamzadeh.
\newblock Algorithms for cvar optimization in mdps.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Chow et~al.(2015)Chow, Tamar, Mannor, and Pavone]{chow2015risk}
Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone.
\newblock Risk-sensitive and robust decision-making: a cvar optimization
  approach.
\newblock \emph{arXiv preprint arXiv:1506.02188}, 2015.

\bibitem[Chow et~al.(2017)Chow, Ghavamzadeh, Janson, and Pavone]{chow2017risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6070--6120, 2017.

\bibitem[Ciosek et~al.(2019)Ciosek, Vuong, Loftin, and
  Hofmann]{ciosek2019better}
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann.
\newblock Better exploration with optimistic actor-critic.
\newblock \emph{arXiv preprint arXiv:1910.12807}, 2019.

\bibitem[Clements et~al.(2019)Clements, Van~Delft, Robaglia, Slaoui, and
  Toth]{clements2019estimating}
William~R Clements, Bastien Van~Delft, Beno{\^\i}t-Marie Robaglia, Reda~Bahi
  Slaoui, and S{\'e}bastien Toth.
\newblock Estimating risk and uncertainty in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.09638}, 2019.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Will Dabney, Georg Ostrovski, David Silver, and R{\'e}mi Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1096--1105. PMLR, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Will Dabney, Mark Rowland, Marc Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018{\natexlab{b}}.

\bibitem[Dhaene et~al.(2012)Dhaene, Kukush, Linders, and
  Tang]{dhaene2012remarks}
Jan Dhaene, Alexander Kukush, Dani{\"e}l Linders, and Qihe Tang.
\newblock Remarks on quantiles and distortion risk measures.
\newblock \emph{European Actuarial Journal}, 2\penalty0 (2):\penalty0 319--328,
  2012.

\bibitem[Esfahani and Kuhn(2018)]{esfahani2018data}
Peyman~Mohajerin Esfahani and Daniel Kuhn.
\newblock Data-driven distributionally robust optimization using the
  wasserstein metric: Performance guarantees and tractable reformulations.
\newblock \emph{Mathematical Programming}, 171\penalty0 (1):\penalty0 115--166,
  2018.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Ishfaq et~al.(2021)Ishfaq, Cui, Nguyen, Ayoub, Yang, Wang, Precup, and
  Yang]{ishfaq2021randomized}
Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang,
  Doina Precup, and Lin Yang.
\newblock Randomized exploration in reinforcement learning with general value
  function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4607--4616. PMLR, 2021.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Keramati et~al.(2020)Keramati, Dann, Tamkin, and
  Brunskill]{keramati2020being}
Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill.
\newblock Being optimistic to be conservative: Quickly learning a cvar policy.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4436--4443, 2020.

\bibitem[Machado et~al.(2018)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{machado2018revisiting}
Marlos~C Machado, Marc~G Bellemare, Erik Talvitie, Joel Veness, Matthew
  Hausknecht, and Michael Bowling.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  523--562, 2018.

\bibitem[Mavrin et~al.(2019)Mavrin, Yao, Kong, Wu, and
  Yu]{mavrin2019distributional}
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu.
\newblock Distributional reinforcement learning for efficient exploration.
\newblock In \emph{International conference on machine learning}, pages
  4424--4434. PMLR, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Moerland et~al.(2018)Moerland, Broekens, and
  Jonker]{moerland2018potential}
Thomas~M Moerland, Joost Broekens, and Catholijn~M Jonker.
\newblock The potential of the return distribution for exploration in rl.
\newblock \emph{arXiv preprint arXiv:1806.04242}, 2018.

\bibitem[Moskovitz et~al.(2021)Moskovitz, Parker-Holder, Pacchiano, Arbel, and
  Jordan]{moskovitz2021tactical}
Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, and Michael
  Jordan.
\newblock Tactical optimism and pessimism for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12849--12863, 2021.

\bibitem[Nguyen-Tang et~al.(2021)Nguyen-Tang, Gupta, and
  Venkatesh]{nguyen2021distributional}
Thanh Nguyen-Tang, Sunil Gupta, and Svetha Venkatesh.
\newblock Distributional reinforcement learning via moment matching.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9144--9152, 2021.

\bibitem[Oh et~al.(2022)Oh, Kim, and Yun]{oh2022risk}
Jihwan Oh, Joonkee Kim, and Se-Young Yun.
\newblock Risk perspective exploration in distributional reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2206.14170}, 2022.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4026--4034, 2016.

\bibitem[Osband et~al.(2019)Osband, Van~Roy, Russo, Wen,
  et~al.]{osband2019deep}
Ian Osband, Benjamin Van~Roy, Daniel~J Russo, Zheng Wen, et~al.
\newblock Deep exploration via randomized value functions.
\newblock \emph{J. Mach. Learn. Res.}, 20\penalty0 (124):\penalty0 1--62, 2019.

\bibitem[Quan and Ostrovski(2020)]{dqnzoo2020github}
John Quan and Georg Ostrovski.
\newblock {DQN} {Zoo}: Reference implementations of {DQN}-based agents, 2020.
\newblock URL \url{http://github.com/deepmind/dqn_zoo}.

\bibitem[Rigter et~al.(2021)Rigter, Lacerda, and Hawes]{rigter2021risk}
Marc Rigter, Bruno Lacerda, and Nick Hawes.
\newblock Risk-averse bayes-adaptive reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.05762}, 2021.

\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
  et~al.]{rockafellar2000optimization}
R~Tyrrell Rockafellar, Stanislav Uryasev, et~al.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of risk}, 2:\penalty0 21--42, 2000.

\bibitem[Shapiro et~al.(2021)Shapiro, Dentcheva, and
  Ruszczynski]{shapiro2021lectures}
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski.
\newblock \emph{Lectures on stochastic programming: modeling and theory}.
\newblock SIAM, 2021.

\bibitem[Smirnova et~al.(2019)Smirnova, Dohmatob, and
  Mary]{smirnova2019distributionally}
Elena Smirnova, Elvis Dohmatob, and J{\'e}r{\'e}mie Mary.
\newblock Distributionally robust reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1902.08708}, 2019.

\bibitem[Stanko and Macek(2019)]{stanko2019risk}
Silvestr Stanko and Karel Macek.
\newblock Risk-averse distributional reinforcement learning: A cvar
  optimization approach.
\newblock In \emph{IJCCI}, pages 412--423, 2019.

\bibitem[Tang and Agrawal(2018)]{tang2018exploration}
Yunhao Tang and Shipra Agrawal.
\newblock Exploration by distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1805.01907}, 2018.

\bibitem[Tversky and Kahneman(1992)]{tversky1992advances}
Amos Tversky and Daniel Kahneman.
\newblock Advances in prospect theory: Cumulative representation of
  uncertainty.
\newblock \emph{Journal of Risk and uncertainty}, 5\penalty0 (4):\penalty0
  297--323, 1992.

\bibitem[Wang(2000)]{wang2000class}
Shaun~S Wang.
\newblock A class of distortion operators for pricing financial and insurance
  risks.
\newblock \emph{Journal of risk and insurance}, pages 15--36, 2000.

\bibitem[Yang et~al.(2019)Yang, Zhao, Lin, Qin, Bian, and Liu]{yang2019fully}
Derek Yang, Li~Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu.
\newblock Fully parameterized quantile function for distributional
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 6193--6202, 2019.

\bibitem[Yang(2020)]{yang2020wasserstein}
Insoon Yang.
\newblock Wasserstein distributionally robust stochastic control: A data-driven
  approach.
\newblock \emph{IEEE Transactions on Automatic Control}, 2020.

\bibitem[Zhang and Yao(2019)]{zhang2019quota}
Shangtong Zhang and Hengshuai Yao.
\newblock Quota: The quantile option architecture for reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5797--5804, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Liu, and Whiteson]{zhang2020mean}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock Mean-variance policy iteration for risk-averse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2004.10888}, 2020.

\bibitem[Zhou et~al.(2021)Zhou, Zhu, Kuang, and Zhang]{zhou2021non}
Fan Zhou, Zhoufan Zhu, Qi~Kuang, and Liwen Zhang.
\newblock Non-decreasing quantile function network with efficient exploration
  for distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2105.06696}, 2021.

\end{thebibliography}
