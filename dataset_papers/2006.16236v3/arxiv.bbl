\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau14}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{Proceedings of the 5th International Conference on Learning
  Representations}, San Diego, CA, USA, 2015.

\bibitem[Blanc \& Rendle(2017)Blanc and Rendle]{blanc2017adaptive}
Blanc, G. and Rendle, S.
\newblock Adaptive sampled softmax with kernel based sampling.
\newblock \emph{arXiv preprint arXiv:1712.00527}, 2017.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{clark2020electra}
Clark, K., Luong, M.-T., Le, Q.~V., and Manning, C.~D.
\newblock {ELECTRA}: Pre-training text encoders as discriminators rather than
  generators.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Clevert et~al.(2015)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Clevert, D.-A., Unterthiner, T., and Hochreiter, S.
\newblock Fast and accurate deep network learning by exponential linear units
  ({ELU}s).
\newblock \emph{arXiv preprint arXiv:1511.07289}, 2015.

\bibitem[Cordonnier et~al.(2020)Cordonnier, Loukas, and
  Jaggi]{Cordonnier2020On}
Cordonnier, J.-B., Loukas, A., and Jaggi, M.
\newblock On the relationship between self-attention and convolutional layers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai-etal-2019-transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2978--2988, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, {\L}.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem[Goodman(2001)]{goodman2001classes}
Goodman, J.
\newblock Classes for fast maximum entropy training.
\newblock In \emph{2001 IEEE International Conference on Acoustics, Speech, and
  Signal Processing. Proceedings (Cat. No. 01CH37221)}, volume~1, pp.\
  561--564. IEEE, 2001.

\bibitem[Graves et~al.(2006)Graves, Fern{\'a}ndez, Gomez, and
  Schmidhuber]{graves2006connectionist}
Graves, A., Fern{\'a}ndez, S., Gomez, F., and Schmidhuber, J.
\newblock Connectionist temporal classification: labelling unsegmented sequence
  data with recurrent neural networks.
\newblock In \emph{Proceedings of the 23rd international conference on Machine
  learning}, pp.\  369--376, 2006.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lample et~al.(2019)Lample, Sablayrolles, Ranzato, Denoyer, and
  Jegou]{lample19}
Lample, G., Sablayrolles, A., Ranzato, M.~A., Denoyer, L., and Jegou, H.
\newblock Large memory layers with product keys.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\' Alch\'{e}-Buc,
  F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  8546--8557. Curran Associates, Inc., 2019.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{Lan2020}
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock 2010.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and
  Han]{liu2019variance}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2019.

\bibitem[Liu et~al.(2020)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2020roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Ro{BERT}a: A robustly optimized {BERT} pretraining approach, 2020.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{paul19}
Michel, P., Levy, O., and Neubig, G.
\newblock Are sixteen heads really better than one?
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'~Alch\'{e}-Buc,
  F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  14014--14024. Curran Associates, Inc., 2019.

\bibitem[Mnih \& Hinton(2009)Mnih and Hinton]{mnih2009scalable}
Mnih, A. and Hinton, G.~E.
\newblock A scalable hierarchical distributed language model.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1081--1088, 2009.

\bibitem[Morin \& Bengio(2005)Morin and Bengio]{morin2005hierarchical}
Morin, F. and Bengio, Y.
\newblock Hierarchical probabilistic neural network language model.
\newblock In \emph{Aistats}, volume~5, pp.\  246--252. Citeseer, 2005.

\bibitem[Parmar et~al.(2019)Parmar, Ramachandran, Vaswani, Bello, Levskaya, and
  Shlens]{parmar-etal-19}
Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens,
  J.
\newblock Stand-alone self-attention in vision models.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d'~Alch\'{e}-Buc,
  F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  68--80. Curran Associates, Inc., 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Paul \& Baker(1992)Paul and Baker]{paul1992design}
Paul, D.~B. and Baker, J.~M.
\newblock The design for the wall street journal-based csr corpus.
\newblock In \emph{Proceedings of the workshop on Speech and Natural Language},
  pp.\  357--362. Association for Computational Linguistics, 1992.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, , and
  Sutskever]{Radford2018ImprovingLU}
Radford, A., Narasimhan, K., Salimans, T., , and Sutskever, I.
\newblock Improving language understanding by generative pre-training.
\newblock In \emph{OpenAI report}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rawat et~al.(2019)Rawat, Chen, Yu, Suresh, and
  Kumar]{rawat2019sampled}
Rawat, A.~S., Chen, J., Yu, F. X.~X., Suresh, A.~T., and Kumar, S.
\newblock Sampled softmax with random fourier features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  13834--13844, 2019.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Shen et~al.(2020)Shen, Zhang, Zhao, Yi, and Li]{shen2020efficient}
Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H.
\newblock Efficient attention: Attention with linear complexities.
\newblock \emph{arXiv preprint arXiv:1812.01243}, 2020.

\bibitem[Song et~al.(2019)Song, Tan, Qin, Lu, and Liu]{pmlr-v97-song19d}
Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y.
\newblock {MASS}: Masked sequence to sequence pre-training for language
  generation.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5926--5936, Long
  Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Sperber et~al.(2018)Sperber, Niehues, Neubig, Stüker, and
  Waibel]{sperber18interspeech}
Sperber, M., Niehues, J., Neubig, G., Stüker, S., and Waibel, A.
\newblock Self-attentional acoustic models.
\newblock In \emph{19th Annual Conference of the International Speech
  Communication Association (InterSpeech 2018)}, Hyderabad, India, September
  2018.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sukhbaatar-etal-2019}
Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.
\newblock Adaptive attention span in transformers.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  331--335, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{Sutskever14}
Sutskever, I., Vinyals, O., and Le, Q.~V.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, pp.\
  3104--3112. Curran Associates, Inc., 2014.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and
  Salakhutdinov]{tsai2019transformer}
Tsai, Y.-H.~H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R.
\newblock Transformer dissection: An unified understanding for transformer{'}s
  attention via the lens of kernel.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4343--4352, Hong Kong,
  China, November 2019. Association for Computational Linguistics.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani_attn}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{zhilin19}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J.~G., Salakhutdinov, R., and Le, Q.~V.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{CoRR}, abs/1906.08237, 2019.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and Wasserblat]{zafrir19}
Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.
\newblock {Q8BERT:} quantized 8bit {BERT}.
\newblock \emph{CoRR}, abs/1910.06188, 2019.

\end{thebibliography}
