
@article{adams2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  year = {2007},
  month = oct,
  journal = {arXiv:0710.3742 [stat]},
  eprint = {0710.3742},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
}

@article{albrecht2014,
  title = {A {{Statistical Approach}} to {{Tracing}} the {{Historical Development}} of {{Major}} and {{Minor Pitch Distributions}}, 1400-1750},
  author = {Albrecht, Joshua D. and Huron, David},
  year = {2014},
  month = feb,
  journal = {Music Perception},
  volume = {31},
  number = {3},
  pages = {223--243},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.31.3.223},
  langid = {english},
}

@inproceedings{anandkumar2011,
  title = {Spectral Methods for Learning Multivariate Latent Tree Structure},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Anandkumar, Animashree and Chaudhuri, Kamalika and Hsu, Daniel J and Kakade, Sham M and Song, Le and Zhang, Tong},
  editor = {{Shawe-Taylor}, J. and Zemel, R. and Bartlett, P. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
}

@article{andrieu2010,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods: Particle {{Markov Chain Monte Carlo Methods}}},
  shorttitle = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  issn = {13697412, 14679868},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L\'evy-driven stochastic volatility model.},
  langid = {english},
}

@article{bacciu2012,
  title = {Compositional Generative Mapping for Tree-Structured Data\textemdash{{Part I}}: Bottom-up Probabilistic Modeling of Trees},
  shorttitle = {Compositional Generative Mapping for Tree-Structured Data\textemdash{{Part I}}},
  author = {Bacciu, Davide and Micheli, Alessio and Sperduti, Alessandro},
  year = {2012},
  journal = {IEEE transactions on neural networks and learning systems},
  volume = {23},
  number = {12},
  pages = {1987--2002},
  publisher = {{IEEE}},
}

@article{baker1979,
  title = {Trainable Grammars for Speech Recognition},
  author = {Baker, James K.},
  year = {1979},
  journal = {The Journal of the Acoustical Society of America},
  volume = {65},
  number = {S1},
  pages = {S132--S132},
  publisher = {{Acoustical Society of America}},
}

@article{barto03,
  title = {Recent {{Advances}} in {{Hierarchical Reinforcement Learning}}},
  author = {Barto, Andrew G. and Mahadevan, Sridhar},
  year = {2003},
  journal = {Discrete Event Dynamic Systems},
  volume = {13},
  number = {4},
  pages = {341--379},
  issn = {0924-6703},
  abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
  owner = {robert},
  keywords = {Engineering},
}

@book{bishop07,
  title = {Pattern {{Recognition}} and {{Machine Learning}} ({{Information Science}} and {{Statistics}})},
  author = {Bishop, Christopher M.},
  year = {2007},
  edition = {1st ed. 2006. Corr. 2nd printing},
  publisher = {{Springer}},
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
  howpublished = {Hardcover},
  owner = {robert},
  keywords = {book,machine_learning,pattern_classification},
}

@book{brooks2011,
  title = {Handbook of Markov Chain Monte Carlo},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year = {2011},
  publisher = {{CRC press}}
}

@article{burg2020,
  title = {An {{Evaluation}} of {{Change Point Detection Algorithms}}},
  author = {van den Burg, Gerrit J. J. and Williams, Christopher K. I.},
  year = {2020},
  month = may,
  journal = {arXiv:2003.06222 [cs, stat]},
  eprint = {2003.06222},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Change point detection is an important part of time series analysis, as the presence of a change point indicates an abrupt and significant change in the data generating process. While many algorithms for change point detection exist, little attention has been paid to evaluating their performance on real-world time series. Algorithms are typically evaluated on simulated data and a small number of commonly-used series with unreliable ground truth. Clearly this does not provide sufficient insight into the comparative performance of these algorithms. Therefore, instead of developing yet another change point detection method, we consider it vastly more important to properly evaluate existing algorithms on real-world data. To achieve this, we present the first data set specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains. Each time series was annotated by five expert human annotators to provide ground truth on the presence and location of change points. We analyze the consistency of the human annotators, and describe evaluation metrics that can be used to measure algorithm performance in the presence of multiple ground truth annotations. Subsequently, we present a benchmark study where 14 existing algorithms are evaluated on each of the time series in the data set. This study shows that binary segmentation (Scott and Knott, 1974) and Bayesian online change point detection (Adams and MacKay, 2007) are among the best performing methods. Our aim is that this data set will serve as a proving ground in the development of novel change point detection algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62M10,Computer Science - Machine Learning,G.3,Statistics - Machine Learning,Statistics - Methodology},
}

@article{callender2008,
  title = {Generalized {{Voice}}-{{Leading Spaces}}},
  author = {Callender, Clifton and Quinn, Ian and Tymoczko, Dmitri},
  year = {2008},
  month = apr,
  journal = {Science},
  volume = {320},
  number = {5874},
  pages = {346--348},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1153021},
  abstract = {Western musicians traditionally classify pitch sequences by disregarding the effects of five musical transformations: octave shift, permutation, transposition, inversion, and cardinality change. We model this process mathematically, showing that it produces 32 equivalence relations on chords, 243 equivalence relations on chord sequences, and 32 families of geometrical quotient spaces, in which both chords and chord sequences are represented. This model reveals connections between music-theoretical concepts, yields new analytical tools, unifies existing geometrical representations, and suggests a way to understand similarity between chord types. A geometric representation of Western music theory, in which distance represents similarity of chord types, reveals relations among diverse musical concepts. A geometric representation of Western music theory, in which distance represents similarity of chord types, reveals relations among diverse musical concepts.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {18420928},
}

@article{chiang2020,
  title = {Factor {{Graph Grammars}}},
  author = {Chiang, David and Riley, Darcey},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
}

@article{choi2011,
  title = {Learning Latent Tree Graphical Models},
  author = {Choi, Myung Jin and Tan, Vincent YF and Anandkumar, Animashree and Willsky, Alan S.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {1771--1812},
  publisher = {{Journal of Machine Learning Research}},
}

@inproceedings{cohen2017,
  title = {Latent-{{Variable PCFGs}}: Background and {{Applications}}},
  shorttitle = {Latent-Variable {{PCFGs}}},
  booktitle = {Proceedings of the 15th {{Meeting}} on the {{Mathematics}} of {{Language}}},
  author = {Cohen, Shay B.},
  year = {2017},
  pages = {47--58},
  publisher = {{Association for Computational Linguistics}},
  address = {{London, UK}},
  doi = {10.18653/v1/W17-3405},
}

@article{crouse1998,
  title = {Wavelet-Based Statistical Signal Processing Using Hidden {{Markov}} Models},
  author = {Crouse, M.S. and Nowak, R.D. and Baraniuk, R.G.},
  year = {1998},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {46},
  number = {4},
  pages = {886--902},
  issn = {1053587X},
  doi = {10.1109/78.668544},
  abstract = {Wavelet-based statistical signal processing techniques such as denoising and detection typically model the wavelet coefficients as independent or jointly Gaussian. These models are unrealistic for many real-world signals. In this paper, we develop a new framework for statistical signal processing based on wavelet-domain hidden Markov models (HMM's) that concisely models the statistical dependencies and non-Gaussian statistics encountered in real-world signals. Wavelet-domain HMM's are designed with the intrinsic properties of the wavelet transform in mind and provide powerful, yet tractable, probabilistic signal models. Efficient expectation maximization algorithms are developed for fitting the HMM's to observational signal data. The new framework is suitable for a wide range of applications, including signal estimation, detection, classification, prediction, and even synthesis. To demonstrate the utility of wavelet-domain HMM's, we develop novel algorithms for signal denoising, classification, and detection.},
  langid = {english},
}

@inproceedings{crouse2011,
  title = {A Look at {{Gaussian}} Mixture Reduction Algorithms},
  booktitle = {14th International Conference on Information Fusion},
  author = {Crouse, David F. and Willett, Peter and Pattipati, Krishna and Svensson, Lennart},
  year = {2011},
  pages = {1--8},
  publisher = {{IEEE}},
}

@article{dash2004,
  title = {Model Averaging for Prediction with Discrete {{Bayesian}} Networks},
  author = {Dash, Denver and Cooper, Gregory F.},
  year = {2004},
  journal = {Journal of Machine Learning Research},
  volume = {5},
  number = {Sep},
  pages = {1177--1203},
}

@inproceedings{dieleman2018,
  ids = {dielemanChallengeRealisticMusic2018a},
  title = {The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale},
  shorttitle = {The Challenge of Realistic Music Generation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dieleman, Sander and {van den Oord}, Aaron and Simonyan, Karen},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {7989--7999},
  publisher = {{Curran Associates, Inc.}},
}

@article{diligenti2003,
  title = {Hidden Tree {{Markov}} Models for Document Image Classification},
  author = {Diligenti, M. and Frasconi, P. and Gori, M.},
  year = {2003},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {25},
  number = {4},
  pages = {519--523},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2003.1190578},
  keywords = {Data mining,Explosives,Feature extraction,Hidden Markov models,Image classification,Image recognition,Machine learning,Multi-layer neural network,Organizing,Probability distribution},
}

@incollection{drewes1997,
  title = {Hyperedge Replacement Graph Grammars},
  booktitle = {Handbook {{Of Graph Grammars And Computing By Graph Transformation}}: Volume 1: Foundations},
  author = {Drewes, Frank and Kreowski, H.-J. and Habel, Annegret},
  year = {1997},
  pages = {95--162},
  publisher = {{World Scientific}},
}

@article{drton2017,
  title = {Structure {{Learning}} in {{Graphical Modeling}}},
  author = {Drton, Mathias and Maathuis, Marloes H.},
  year = {2017},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {4},
  number = {1},
  pages = {365--393},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-060116-053803},
  abstract = {A graphical model is a statistical model that is associated with a graph whose nodes correspond to variables of interest. The edges of the graph reflect allowed conditional dependencies among the variables. Graphical models have computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. More recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. We review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or Markov random fields) and the PC algorithm and score-based search methods for directed graphical models (or Bayesian networks). We further review extensions that account for effects of latent variables and heterogeneous data sources.},
  langid = {english},
}

@inproceedings{duvenaud2013structure,
  title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{International Conference}} on {{Machine Learning}}-{{Volume}} 28},
  author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B and Ghahramani, Zoubin},
  year = {2013},
  pages = {III-1166},
  organization = {{JMLR. org}},
}

@article{earley1970,
  title = {An Efficient Context-Free Parsing Algorithm},
  author = {Earley, Jay},
  year = {1970},
  journal = {Communications of the ACM},
  volume = {13},
  number = {2},
  pages = {94--102},
  publisher = {{ACM New York, NY, USA}},
}

@inproceedings{eaton2007,
  title = {Bayesian Structure Learning Using Dynamic Programming and {{MCMC}}},
  booktitle = {Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence ({{UAI}} 2007)},
  author = {Eaton, Daniel and Murphy, Kevin},
  year = {2007},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
}

@inproceedings{eisner2016,
  title = {Inside-{{Outside}} and {{Forward}}-{{Backward Algorithms Are Just Backprop}} (Tutorial Paper)},
  booktitle = {Proceedings of the {{Workshop}} on {{Structured Prediction}} for {{NLP}}},
  author = {Eisner, Jason},
  year = {2016},
  pages = {1--17},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, TX}},
  doi = {10.18653/v1/W16-5901},
  abstract = {A probabilistic or weighted grammar implies a posterior probability distribution over possible parses of a given input sentence. One often needs to extract information from this distribution, by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states. This requires an algorithm such as inside-outside or forward-backward that is tailored to the grammar formalism. Conveniently, each such algorithm can be obtained by automatically differentiating an ``inside'' algorithm that merely computes the log-probability of the evidence (the sentence). This mechanical procedure produces correct and efficient code. As for any other instance of back-propagation, it can be carried out manually or by software. This pedagogical paper carefully spells out the construction and relates it to traditional and nontraditional views of these algorithms.},
  langid = {english},
}

@incollection{engelfriet1997,
  title = {Node Replacement Graph Grammars},
  booktitle = {Handbook {{Of Graph Grammars And Computing By Graph Transformation}}: Volume 1: Foundations},
  author = {Engelfriet, Joost and Rozenberg, Grzegorz},
  year = {1997},
  pages = {1--94},
  publisher = {{World Scientific}},
}

@inproceedings{foscarin2019parse,
  title = {A {{Parse}}-Based {{Framework}} for {{Coupled Rhythm Quantization}} and {{Score Structuring}}},
  booktitle = {International {{Conference}} on {{Mathematics}} and {{Computation}} in {{Music}}},
  author = {Foscarin, Francesco and Jacquemard, Florent and Rigaux, Philippe and Sakai, Masahiko},
  year = {2019},
  pages = {248--260},
  organization = {{Springer}},
}

@inproceedings{franceschi2019,
  title = {Learning Discrete Structures for Graph Neural Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Franceschi, Luca and Niepert, Mathias and Pontil, Massimiliano and He, Xiao},
  year = {2019},
  pages = {1972--1982},
  publisher = {{PMLR}},
}

@inproceedings{frey2003,
  title = {Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models},
  booktitle = {{{UAI}} '03, Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence, Acapulco, Mexico, August 7-10 2003},
  author = {Frey, Brendan J.},
  year = {2003},
  pages = {257--264},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/conf/uai/Frey03},
  crossref = {uai2003},
  timestamp = {Wed, 06 May 2015 15:02:57 +0200},
}

@article{geib2009,
  title = {A Probabilistic Plan Recognition Algorithm Based on Plan Tree Grammars},
  author = {Geib, Christopher W. and Goldman, Robert P.},
  year = {2009},
  month = jul,
  journal = {Artificial Intelligence},
  volume = {173},
  number = {11},
  pages = {1101--1132},
  issn = {00043702},
  doi = {10.1016/j.artint.2009.01.003},
  abstract = {We present the PHATT algorithm for plan recognition. Unlike previous approaches to plan recognition, PHATT is based on a model of plan execution. We show that this clarifies several difficult issues in plan recognition including the execution of multiple interleaved root goals, partially ordered plans, and failing to observe actions. We present the PHATT algorithm's theoretical basis, and an implementation based on tree structures. We also investigate the algorithm's complexity, both analytically and empirically. Finally, we present PHATT's integrated constraint reasoning for parametrized actions and temporal constraints. \textcopyright{} 2009 Elsevier B.V. All rights reserved.},
  langid = {english},
}

@inproceedings{gens2013,
  title = {Learning the Structure of Sum-Product Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Gens, Robert and Pedro, Domingos},
  year = {2013},
  pages = {873--880},
  publisher = {{PMLR}},
}

@book{ghallab_automated_2016,
  title = {Automated {{Planning}} and {{Acting}}},
  author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
  year = {2016},
  publisher = {{Cambridge University Press}},
}

@article{golin1991,
  title = {Parsing Visual Languages with Picture Layout Grammars},
  author = {Golin, Eric J.},
  year = {1991},
  month = dec,
  journal = {Journal of Visual Languages \& Computing},
  volume = {2},
  number = {4},
  pages = {371--393},
  issn = {1045-926X},
  doi = {10.1016/S1045-926X(05)80005-9},
  abstract = {Visual programming languages are languages for programming using visual expressions. Picture layout grammars are a mechanism for defining the syntax of visual languages. They allow the specification of both the logical structure and two-dimensinal layout of a visual language. Spatial parsing is the process of analysing an input picture to determine its syntactic structure. This paper describes a parsing algorithm for visual languages defined by picture layout grammars. The algorithm is a general parser for visual languages, in that both the grammar specification and the picture are inputs to the algorithm. The result of parsing is an augmented tree expressing the underlying structure of the input picture, according to the grammar specification.},
  langid = {english},
}

@book{goodfellow2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT press}},
}

@article{goodman1999,
  title = {Semiring Parsing},
  author = {Goodman, Joshua},
  year = {1999},
  journal = {Computational Linguistics},
  volume = {25},
  number = {4},
  pages = {573--605},
  publisher = {{MIT Press}},
}

@inproceedings{granroth2012,
  title = {Statistical Parsing for Harmonic Analysis of Jazz Chord Sequences},
  booktitle = {{{ICMC}}},
  author = {{Granroth-Wilding}, Mark and Steedman, Mark},
  year = {2012},
}

@inproceedings{grill2015,
  title = {Music {{Boundary Detection Using Neural Networks}} on {{Combined Features}} and {{Two}}-{{Level Annotations}}.},
  booktitle = {{{ISMIR}}},
  author = {Grill, Thomas and Schl{\"u}ter, Jan},
  year = {2015},
  pages = {531--537},
}

@inproceedings{grosse2012,
  title = {Exploiting Compositionality to Explore a Large Space of Model Structures},
  booktitle = {Proceedings of the {{Twenty}}-\-{{Eighth Conference}}},
  author = {Grosse, Roger B. and Salakhutdinov, Ruslan R. and Freeman, William T. and Tenenbaum, Joshua B.},
  year = {2012},
}

@article{grune2007,
  title = {Parsing Techniques},
  author = {Grune, Dick and Jacobs, Ceriel JH},
  year = {2007},
  journal = {Monographs in Computer Science. Springer,},
  pages = {13},
}

@article{grzegorczyk2008,
  title = {Improving the Structure {{MCMC}} Sampler for {{Bayesian}} Networks by Introducing a New Edge Reversal Move},
  author = {Grzegorczyk, Marco and Husmeier, Dirk},
  year = {2008},
  journal = {Machine Learning},
  volume = {71},
  number = {2-3},
  pages = {265},
  publisher = {{Springer}},
}

@inproceedings{hall2012,
  title = {Training Factored {{PCFGs}} with Expectation Propagation},
  booktitle = {Proceedings of the 2012 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}}},
  author = {Hall, David and Klein, Dan},
  year = {2012},
  pages = {1146--1156},
}

@inproceedings{harasim2019,
  title = {Harmonic {{Syntax}} in {{Time}}: Rhythm {{Improves Grammatical Models}} of {{Harmony}}},
  shorttitle = {Harmonic {{Syntax}} in {{Time}}},
  booktitle = {Proceedings of the 20th {{International Society}} for {{Music Information Retrieval Conference}}, {{ISMIR}} 2019, {{Delft}}, {{The Netherlands}}, {{November}} 4-8, 2019},
  author = {Harasim, Daniel and O'Donnell, Timothy J. and Rohrmeier, Martin},
  editor = {Flexer, Arthur and Peeters, Geoffroy and Urbano, Juli{\'a}n and Volk, Anja},
  year = {2019},
  pages = {335--342},
  isbn = {978-1-73272-991-9},
}

@article{harasim2021,
  title = {Exploring the Foundations of Tonality: Statistical Cognitive Modeling of Modes in the History of {{Western}} Classical Music},
  shorttitle = {Exploring the Foundations of Tonality},
  author = {Harasim, Daniel and Moss, Fabian C. and Ramirez, Matthias and Rohrmeier, Martin},
  year = {2021},
  month = jan,
  journal = {Humanities and Social Sciences Communications},
  volume = {8},
  number = {1},
  pages = {1--11},
  publisher = {{Palgrave}},
  issn = {2662-9992},
  doi = {10.1057/s41599-020-00678-6},
  abstract = {Tonality is one of the most central theoretical concepts for the analysis of Western classical music. This study presents a novel approach for the study of its historical development, exploring in particular the concept of mode. Based on a large dataset of approximately 13,000 musical pieces in MIDI format, we present two models to infer both the number and characteristics of modes of different historical periods from first principles: a geometric model of modes as clusters of musical pieces in a non-Euclidean space, and a cognitively plausible Bayesian model of modes as Dirichlet distributions. We use the geometric model to determine the optimal number of modes for five historical epochs via unsupervised learning and apply the probabilistic model to infer the characteristics of the modes. Our results show that the inference of four modes is most plausible in the Renaissance, that two modes\textendash corresponding to major and minor\textendash are most appropriate in the Baroque and Classical eras, whereas no clear separation into distinct modes is found for the 19th century.},
  copyright = {2021 The Author(s)},
  langid = {english},
}

@inproceedings{harasimGeneralized2018,
  title = {A {{Generalized Parsing Framework}} for {{Generative Models}} of {{Harmonic Syntax}}},
  booktitle = {Proceedings of the 19th {{International Society}} for {{Music Information Retrieval Conference}}},
  author = {Harasim, Daniel and Rohrmeier, Martin and O'Donnell, Timothy J.},
  year = {2018},
  pages = {152--159},
  address = {{Paris}},
  doi = {10.5281/zenodo.1492367},
}

@article{heckerman1995,
  title = {Learning {{Bayesian}} Networks: The Combination of Knowledge and Statistical Data},
  shorttitle = {Learning {{Bayesian}} Networks},
  author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
  year = {1995},
  journal = {Machine learning},
  volume = {20},
  number = {3},
  pages = {197--243},
  publisher = {{Springer}},
}

@article{heckerman2013,
  title = {Learning {{Bayesian}} Networks: A Unification for Discrete and {{Gaussian}} Domains},
  shorttitle = {Learning {{Bayesian}} Networks},
  author = {Heckerman, David and Geiger, Dan},
  year = {2013},
  journal = {arXiv preprint arXiv:1302.4957},
  eprint = {1302.4957},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
}

@inproceedings{heng2008,
  title = {Automatic Chord Recognition for Music Classification and Retrieval},
  booktitle = {2008 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {{Heng-Tze Cheng} and {Yi-Hsuan Yang} and {Yu-Ching Lin} and {I-Bin Liao} and {Homer H. Chen}},
  year = {2008},
  month = jun,
  pages = {1505--1508},
  issn = {1945-788X},
  doi = {10.1109/ICME.2008.4607732},
  abstract = {As one of the most important mid-level features of music, chord contains rich information of harmonic structure that is useful for music information retrieval. In this paper, we present a chord recognition system based on the N-gram model. The system is time-efficient, and its accuracy is comparable to existing systems. We further propose a new method to construct chord features for music emotion classification and evaluate its performance on commercial song recordings. Experimental results demonstrate the advantage of using chord features for music classification and retrieval.},
  keywords = {Accuracy,audio signal processing,automatic chord recognition,Chord,commercial song recordings,emotion recognition,Feature extraction,Harmonic analysis,Hidden Markov models,Histograms,information retrieval,music,Music,music classification,music emotion classification,music information retrieval,N-gram,N-gram model,pattern classification,signal classification,Training},
}

@article{hoffman2013,
  title = {Stochastic Variational Inference},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {The Journal of Machine Learning Research},
  volume = {14},
  number = {1},
  pages = {1303--1347},
}

@inproceedings{huang2005,
  title = {Better K-Best Parsing},
  booktitle = {Proceedings of the {{Ninth International Workshop}} on {{Parsing Technology}} - {{Parsing}} '05},
  author = {Huang, Liang and Chiang, David},
  year = {2005},
  pages = {53--64},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.3115/1654494.1654500},
  abstract = {We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel's implementation of Collins' lexicalized PCFG model, and on Chiang's CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.},
  langid = {english},
}

@inproceedings{huang2020,
  title = {Guaranteed Scalable Learning of Latent Tree Models},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Huang, Furong and Naresh, Niranjan Uma and Perros, Ioakeim and Chen, Robert and Sun, Jimeng and Anandkumar, Anima},
  year = {2020},
  pages = {883--893},
  publisher = {{PMLR}},
}

@inproceedings{huber2008b,
  title = {Progressive {{Gaussian}} Mixture Reduction},
  booktitle = {2008 11th {{International Conference}} on {{Information Fusion}}},
  author = {Huber, Marco F. and Hanebeck, Uwe D.},
  year = {2008},
  month = jun,
  pages = {1--8},
  keywords = {Algorithm design and analysis,Approximation algorithms,Approximation methods,Differential equations,Distance measurement,Gaussian mixture reduction,homotopy continuation,Merging,nonlinear optimization,Optimization},
}

@inproceedings{jacquemard2015,
  title = {A Structural Theory of Rhythm Notation Based on Tree Representations and Term Rewriting},
  booktitle = {International {{Conference}} on {{Mathematics}} and {{Computation}} in {{Music}}},
  author = {Jacquemard, Florent and {Donat-Bouillud}, Pierre and Bresson, Jean},
  year = {2015},
  pages = {3--15},
  publisher = {{Springer}},
}

@inproceedings{jin2020,
  title = {Graph Structure Learning for Robust Graph Neural Networks},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Jin, Wei and Ma, Yao and Liu, Xiaorui and Tang, Xianfeng and Wang, Suhang and Tang, Jiliang},
  year = {2020},
  pages = {66--74},
}

@incollection{jordan1999,
  title = {An {{Introduction}} to {{Variational Methods}} for {{Graphical Models}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  editor = {Jordan, Michael I.},
  year = {1999},
  pages = {105--161},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5014-9_5},
  abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
  isbn = {978-94-010-6104-9 978-94-011-5014-9},
  langid = {english},
}

@book{jurafsky2000,
  title = {Speech \& Language Processing},
  author = {Jurafsky, Dan},
  year = {2000},
  publisher = {{Pearson Education India}},
}

@article{kalman1960,
  title = {A New Approach to Linear Filtering and Prediction Problems},
  author = {Kalman, Rudolph Emil},
  year = {1960},
  journal = {Transactions of the ASME \textendash{} Journal of Basic Engineering},
}

@article{kasami1966efficient,
  title = {An Efficient Recognition and Syntax-Analysis Algorithm for Context-Free Languages},
  author = {Kasami, Tadao},
  year = {1966},
  journal = {Coordinated Science Laboratory Report no. R-257},
  publisher = {{Coordinated Science Laboratory, University of Illinois at Urbana-Champaign}},
}

@inproceedings{kim2019,
  title = {Compound {{Probabilistic Context}}-{{Free Grammars}} for {{Grammar Induction}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
  year = {2019},
  pages = {2369--2385},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1228},
  abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
}

@article{koelsch2013,
  title = {Processing of Hierarchical Syntactic Structure in Music},
  author = {Koelsch, Stefan and Rohrmeier, Martin and Torrecuso, R. and Jentschke, S.},
  year = {2013},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {38},
  pages = {15443--15448},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1300272110},
  langid = {english},
}

@book{koller_probabilistic_2009,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  shorttitle = {Probabilistic Graphical Models},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  publisher = {{MIT press}},
}

@inproceedings{korzeniowski2018,
  title = {Genre-{{Agnostic Key Classification}} with {{Convolutional Neural Networks}}},
  booktitle = {Proceedings of the 19th {{International Society}} for {{Music Information Retrieval Conference}}},
  author = {Korzeniowski, Filip and Widmer, Gerhard},
  year = {2018},
  address = {{Paris, France}},
}

@article{krumhansl1982,
  title = {Tracing the Dynamic Changes in Perceived Tonal Organization in a Spatial Representation of Musical Keys.},
  author = {Krumhansl, Carol L. and Kessler, Edward J.},
  year = {1982},
  journal = {Psychological review},
  volume = {89},
  number = {4},
  pages = {334},
  publisher = {{American Psychological Association}},
}

@article{lee2008,
  title = {Acoustic {{Chord Transcription}} and {{Key Extraction From Audio Using Key}}-{{Dependent HMMs Trained}} on {{Synthesized Audio}}},
  author = {Lee, Kyogu and Slaney, Malcolm},
  year = {2008},
  month = feb,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {16},
  number = {2},
  pages = {291--301},
  issn = {1558-7924},
  doi = {10.1109/TASL.2007.914399},
  abstract = {We describe an acoustic chord transcription system that uses symbolic data to train hidden Markov models and gives best-of-class frame-level recognition results. We avoid the extremely laborious task of human annotation of chord names and boundaries-which must be done to provide machine learning models with ground truth-by performing automatic harmony analysis on symbolic music files. In parallel, we synthesize audio from the same symbolic files and extract acoustic feature vectors which are in perfect alignment with the labels. We, therefore, generate a large set of labeled training data with a minimal amount of human labor. This allows for richer models. Thus, we build 24 key-dependent HMMs, one for each key, using the key information derived from symbolic data. Each key model defines a unique state-transition characteristic and helps avoid confusions seen in the observation vector. Given acoustic input, we identify a musical key by choosing a key model with the maximum likelihood, and we obtain the chord sequence from the optimal state path of the corresponding key model, both of which are returned by a Viterbi decoder. This not only increases the chord recognition accuracy, but also gives key information. Experimental results show the models trained on synthesized data perform very well on real recordings, even though the labels automatically generated from symbolic data are not 100\% accurate. We also demonstrate the robustness of the tonal centroid feature, which outperforms the conventional chroma feature.},
  keywords = {Acoustic chord transcription,acoustic chord transcription system,acoustic feature vector extraction,acoustic signal processing,audio signal processing,audio synthesis,automatic harmony analysis,chord sequence,Data mining,electronic music,feature extraction,Feature extraction,frame-level recognition,hidden Markov model (HMM),hidden Markov models,Hidden Markov models,Humans,key extraction,key-dependent HMM training,key-dependent models,Machine learning,machine learning models,maximum likelihood,maximum likelihood decoding,Maximum likelihood decoding,musical acoustics,musical key extraction,optimal state path,Performance analysis,Robustness,symbolic music files,tonal centroid feature,Training data,unique state-transition characteristics,Viterbi algorithm,Viterbi decoder,Viterbi decoding},
}

@inproceedings{lee2013,
  title = {Online Incremental Structure Learning of Sum\textendash Product Networks},
  booktitle = {International {{Conference}} on {{Neural Information Processing}}},
  author = {Lee, Sang-Woo and Heo, Min-Oh and Zhang, Byoung-Tak},
  year = {2013},
  pages = {220--227},
  publisher = {{Springer}},
}

@book{leimkuhler2005,
  title = {Simulating {{Hamiltonian Dynamics}}},
  author = {Leimkuhler, Benedict and Reich, Sebastian},
  year = {2005},
  series = {Cambridge {{Monographs}} on {{Applied}} and {{Computational Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511614118},
  isbn = {978-0-521-77290-7},
}

@book{lerdahl1983,
  title = {A Generative Theory of Tonal Music},
  author = {Lerdahl, Fred and Jackendoff, Ray},
  year = {1983},
  publisher = {{MIT press}},
}

@inproceedings{lieckModelling2020,
  title = {Modelling {{Hierarchical Key Structure With Pitch Scapes}}},
  booktitle = {Proceedings of the 21st {{International Society}} for {{Music Information Retrieval Conference}}},
  author = {Lieck, Robert and Rohrmeier, Martin},
  year = {2020},
  pages = {811--818},
  address = {{Montr\'eal, Canada}},
  doi = {10.5281/zenodo.4245558},
  author+an = {1=highlight},
  pdf = {http://robert-lieck.com/literature/pdfs/UV5637PE/Lieck\_and\_Rohrmeier\_-\_2020\_-\_Modelling\_Hierarchical\_Key\_Structure\_with\_Pitch\_Sc.pdf},
}

@inproceedings{ma2015,
  title = {A Complete Recipe for Stochastic Gradient {{MCMC}}},
  booktitle = {Advances in Neural Information Processing Systems 28},
  author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  year = {2015},
  pages = {2917--2925},
  address = {{Montreal, Quebec, Canada}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/MaCF15.bib},
  timestamp = {Fri, 06 Mar 2020 17:00:35 +0100},
}

@inproceedings{maezawa2019,
  title = {Music {{Boundary Detection Based}} on a {{Hybrid Deep Model}} of {{Novelty}}, {{Homogeneity}}, {{Repetition}} and {{Duration}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Maezawa, Akira},
  year = {2019},
  month = may,
  pages = {206--210},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2019.8683249},
  abstract = {Current state-of-the-art music boundary detection methods use local features for boundary detection, but such an approach fails to explicitly incorporate the statistical properties of the detected segments. This paper presents a music boundary detection method that simultaneously considers a fitness measure based on the boundary posterior probability, the likelihood of the segmentation duration sequence, and the acoustic consistency within a segment. Evaluation shows that our method improves segmentation F0.58-measure by about 10 points compared to DNN with peak-picking, a popular scheme used in the state-of-the-art music boundary detectors.},
  keywords = {acoustic consistency,acoustic signal processing,audio signal processing,boundary posterior probability,deep learning,duration model,feature extraction,fitness measure,hybrid deep model,learning (artificial intelligence),local features,music,music boundary detection,music boundary detection method,music information retrieval,neural nets,probability,segmentation duration sequence,statistical properties},
}

@book{manning1999,
  title = {Foundations of Statistical Natural Language Processing},
  author = {Manning, Christopher and Schutze, Hinrich},
  year = {1999},
  publisher = {{MIT press}},
}

@article{mcallester2008,
  title = {Case-Factor Diagrams for Structured Probabilistic Modeling},
  author = {McAllester, David and Collins, Michael and Pereira, Fernando},
  year = {2008},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {74},
  number = {1},
  pages = {84--96},
  publisher = {{Elsevier}},
  doi = {10.1016/j.jcss.2007.04.015},
  langid = {english},
}

@article{meila2006,
  title = {Tractable {{Bayesian}} Learning of Tree Belief Networks},
  author = {Meil{\u a}, Marina and Jaakkola, Tommi},
  year = {2006},
  journal = {Statistics and Computing},
  volume = {16},
  number = {1},
  pages = {77--92},
  publisher = {{Springer}},
}

@inproceedings{milios2018,
  title = {Dirichlet-Based {{Gaussian}} Processes for Large-Scale Calibrated Classification},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Milios, Dimitrios and Camoriano, Raffaello and Michiardi, Pietro and Rosasco, Lorenzo and Filippone, Maurizio},
  year = {2018},
  pages = {6005--6015},
}

@inproceedings{minka2009,
  title = {Gates},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Minka, Tom and Winn, John},
  year = {2009},
  pages = {1073--1080},
}

@inproceedings{molina2018,
  title = {Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains},
  shorttitle = {Mixed Sum-Product Networks},
  booktitle = {Thirty-Second {{AAAI}} Conference on Artificial Intelligence},
  author = {Molina, Alejandro and Vergari, Antonio and Di Mauro, Nicola and Natarajan, Sriraam and Esposito, Floriana and Kersting, Kristian},
  year = {2018},
}

@inproceedings{muller2012,
  title = {A {{Scape Plot Representation}} for {{Visualizing Repetitive Structures}} of {{Music Recordings}}.},
  booktitle = {{{ISMIR}}},
  author = {M{\"u}ller, Meinard and Jiang, Nanzhu},
  year = {2012},
  pages = {97--102},
  publisher = {{Citeseer}},
}

@book{muller2015,
  title = {Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications},
  shorttitle = {Fundamentals of Music Processing},
  author = {{Meinard M\"uller}},
  year = {2015},
  publisher = {{Springer}},
}

@phdthesis{murphy2002,
  title = {Dynamic {{Bayesian Networks}}: Representation, {{Inference}} and {{Learning}}},
  author = {Murphy, Kevin Patrick},
  year = {2002},
  owner = {robert},
  school = {University of California, Berkeley},
}

@inproceedings{neal11,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  booktitle = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Neal, Radford M.},
  editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year = {2011},
  pages = {113--162},
  publisher = {{CRC Press}},
  owner = {robert},
}

@article{orguner2007,
  title = {Analysis of Single {{Gaussian}} Approximation of {{Gaussian}} Mixtures in {{Bayesian}} Filtering Applied to Mixed Multiple-Model Estimation},
  author = {Orguner, U. and Dem{\i}rekler, M.},
  year = {2007},
  month = jun,
  journal = {International Journal of Control},
  volume = {80},
  number = {6},
  pages = {952--967},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207170701261952},
  abstract = {This paper examines the effect of the moment-matched single Gaussian approximation, which is made in various multiple-model filtering applications to approximate a Gaussian mixture, on the Bayesian filter performance. The estimation error caused by the approximation is analysed for both the prediction and the measurement updates of a Bayesian filter. An approximate formula is found for the covariance of the error caused by the approximation for a general Gaussian mixture with arbitrary components. The calculated error covariance is used for obtaining a mixed multiple-model estimation algorithm which has a performance near that of GPB2 with less computations.},
  annotation = {\_eprint: https://doi.org/10.1080/00207170701261952},
}

@book{oruanaidh1996,
  ids = {ruanaidh2012},
  title = {Numerical {{Bayesian}} Methods Applied to Signal Processing},
  author = {{\'O} Ruanaidh, Joseph JK and Fitzgerald, William J.},
  year = {1996},
  publisher = {{Springer Science \& Business Media}},
}

@incollection{paszke2019,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
}

@techreport{petersen06,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and Larsen, Jan and Strimmer, Korbinian and Christiansen, Lars and Hansen, Kai and He, Liguo and Thibaut, Loic and Bar{\~a}o, Miguel and Hattinger, Stephan and Sima, Vasile and The, We},
  year = {2006},
  owner = {robert},
}

@inproceedings{poon2011,
  title = {Sum-Product Networks: A New Deep Architecture},
  shorttitle = {Sum-Product Networks},
  booktitle = {2011 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCV Workshops}})},
  author = {Poon, Hoifung and Domingos, Pedro},
  year = {2011},
  month = nov,
  pages = {689--690},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICCVW.2011.6130310},
  abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sumproduct networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.},
  isbn = {978-1-4673-0063-6 978-1-4673-0062-9 978-1-4673-0061-2},
  langid = {english},
}

@phdthesis{raffel2016,
  title = {Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-Midi Alignment and Matching},
  author = {Raffel, Colin},
  year = {2016},
  school = {Columbia University},
}

@inproceedings{ranganath2016,
  title = {Hierarchical {{Variational Models}}},
  booktitle = {Proceedings of the 33nd {{International Conference}} on {{Machine Learning}}, {{ICML}} 2016, {{New York City}}, {{NY}}, {{USA}}, {{June}} 19-24, 2016},
  author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M.},
  editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
  year = {2016},
  series = {{{JMLR Workshop}} and {{Conference Proceedings}}},
  volume = {48},
  pages = {324--333},
  publisher = {{JMLR.org}},
}

@book{rasmussen2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts / London, England}},
  owner = {robert},
  keywords = {\#nosource},
}

@book{robert2004,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-4145-2},
  isbn = {978-1-4419-1939-7 978-1-4757-4145-2},
  langid = {english},
}

@incollection{rohrmeier_pearce_2018,
  title = {Musical {{Syntax I}}: Theoretical {{Perspectives}}},
  shorttitle = {Musical {{Syntax I}}},
  booktitle = {Springer {{Handbook}} of {{Systematic Musicology}}},
  author = {Rohrmeier, Martin and Pearce, Marcus},
  editor = {Bader, Rolf},
  year = {2018},
  pages = {473--486},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-55004-5_25},
  isbn = {978-3-662-55002-1 978-3-662-55004-5},
  langid = {english},
}

@article{rohrmeier2011,
  title = {Towards a Generative Syntax of Tonal Harmony},
  author = {Rohrmeier, Martin},
  year = {2011},
  month = mar,
  journal = {Journal of Mathematics and Music},
  volume = {5},
  number = {1},
  pages = {35--53},
  issn = {1745-9737, 1745-9745},
  doi = {10.1080/17459737.2011.573676},
  langid = {english},
}

@article{rohrmeier2020,
  title = {The {{Syntax}} of {{Jazz Harmony}}: Diatonic {{Tonality}}, {{Phrase Structure}}, and {{Form}}},
  shorttitle = {The {{Syntax}} of {{Jazz Harmony}}},
  author = {Rohrmeier, Martin},
  year = {2020},
  month = apr,
  journal = {Music Theory and Analysis (MTA)},
  volume = {7},
  number = {1},
  pages = {1--63},
  doi = {10.11116/MTA.7.1.1},
  abstract = {The regularities underlying the structure building of chord sequences, harmonic phrases, and combinations of phrases constitute a central research problem in music theory. This article proposes a formalization of Jazz harmony with a generative framework based on formal grammars, in which syntactic structure tightly corresponds with the functional interpretation of the sequence. It assumes that chords establish nested hierarchical dependencies that are characterized by two core types: preparation and prolongation. The approach expresses diatonic harmony, embedded modulation, borrowing, and substitution within a single grammatical framework. It is argued in the second part that the proposed framework models not only core phrase structure, but also relations between phrases and the syntactic structures underlying the main forms of Jazz standards. As a special case, the Blues form relies heavily on the plagal derivation from the tonic and is analyzed in comparison with other analytical approaches to the Blues. The proposed theory is specified to a sufficient level of detail that it lends itself to computational implementation and empirical exploration, and this way it makes a step towards music theory building that embraces the close links between formal, mathematical, and computational methods.},
  keywords = {GENERATIVE MODELING,HARMONY,JAZZ,MUSIC,MUSIC THEORY,SYNTAX THEORY},
}

@inproceedings{rooshenas2014,
  title = {Learning Sum-Product Networks with Direct and Indirect Variable Interactions},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Rooshenas, Amirmohammad and Lowd, Daniel},
  year = {2014},
  pages = {710--718},
  publisher = {{PMLR}},
}

@book{rozenberg1997,
  title = {Handbook of Graph Grammars and Computing by Graph Transformation},
  author = {Rozenberg, Grzegorz},
  year = {1997},
  volume = {1},
  publisher = {{World scientific}},
}

@article{rush2020,
  title = {Torch-{{Struct}}: Deep {{Structured Prediction Library}}},
  shorttitle = {Torch-{{Struct}}},
  author = {Rush, Alexander M.},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.00876 [cs, stat]},
  eprint = {2002.00876},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks. TorchStruct includes a broad collection of probabilistic structures accessed through a simple and flexible distribution-based API that connects to any deep learning model. The library utilizes batched, vectorized operations and exploits auto-differentiation to produce readable, fast, and testable code. Internally, we also include a number of general-purpose optimizations to provide cross-algorithm efficiency. Experiments show significant performance gains over fast baselines and casestudies demonstrate the benefits of the library. Torch-Struct is available at https://github. com/harvardnlp/pytorch-struct.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
}

@inproceedings{salimans2015,
  title = {Markov Chain Monte Carlo and Variational Inference: Bridging the Gap},
  shorttitle = {Markov Chain Monte Carlo and Variational Inference},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  year = {2015},
  pages = {1218--1226},
}

@inproceedings{sapp2001,
  title = {Harmonic {{Visualizations}} of {{Tonal Music}}.},
  booktitle = {Proc. {{International Computer Music Conference}} ({{ICMC}})},
  author = {Sapp, Craig Stuart},
  year = {2001},
  address = {{Havana, Cuba}},
}

@inproceedings{shao2020,
  title = {Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures},
  shorttitle = {Conditional Sum-Product Networks},
  booktitle = {International {{Conference}} on {{Probabilistic Graphical Models}}},
  author = {Shao, Xiaoting and Molina, Alejandro and Vergari, Antonio and Stelzner, Karl and Peharz, Robert and Liebig, Thomas and Kersting, Kristian},
  year = {2020},
  pages = {401--412},
  publisher = {{PMLR}},
}

@inproceedings{shibata2019,
  title = {Statistical {{Music Structure Analysis Based}} on a {{Homogeneity}}-, {{Repetitiveness}}-, and {{Regularity}}-{{Aware Hierarchical Hidden Semi}}-{{Markov Model}}},
  booktitle = {Proceedings of the 20th {{International Society}} for {{Music Information Retrieval Conference}}, {{ISMIR}} 2019, {{Delft}}, {{The Netherlands}}, {{November}} 4-8, 2019},
  author = {Shibata, Go and Nishikimi, Ryo and Nakamura, Eita and Yoshii, Kazuyoshi},
  editor = {Flexer, Arthur and Peeters, Geoffroy and Urbano, Juli{\'a}n and Volk, Anja},
  year = {2019},
  pages = {268--275},
  isbn = {978-1-73272-991-9},
}

@misc{sloane2003,
  title = {The On-Line Encyclopedia of Integer Sequences},
  author = {Sloane, Neil JA},
  year = {2003},
}

@inproceedings{socher2011,
  title = {Parsing Natural Scenes and Natural Language with Recursive Neural Networks},
  booktitle = {{{ICML}}},
  author = {Socher, Richard and Lin, Cliff Chiung-Yu and Ng, Andrew Y. and Manning, Christopher D.},
  year = {2011},
}

@inproceedings{socher2013,
  title = {Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y. and Potts, Christopher},
  year = {2013},
  pages = {1631--1642},
}

@inproceedings{socher2013a,
  title = {Parsing with Compositional Vector Grammars},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Socher, Richard and Bauer, John and Manning, Christopher D. and Ng, Andrew Y.},
  year = {2013},
  pages = {455--465},
}

@article{temperley_whats_1999,
  title = {What's Key for Key? The {{Krumhansl}}-{{Schmuckler}} Key-Finding Algorithm Reconsidered},
  shorttitle = {What's Key for Key?},
  author = {Temperley, David},
  year = {1999},
  journal = {Music Perception: An Interdisciplinary Journal},
  volume = {17},
  number = {1},
  pages = {65--100},
}

@article{temperley2008,
  title = {Pitch-{{Class Distribution}} and the {{Identification}} of {{Key}}},
  author = {Temperley, David and Marvin, Elizabeth West},
  year = {2008},
  month = feb,
  journal = {Music Perception},
  volume = {25},
  number = {3},
  pages = {193--212},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2008.25.3.193},
  langid = {english},
}

@inproceedings{tralie2019,
  title = {Enhanced {{Hierarchical Music Structure Annotations}} via {{Feature Level Similarity Fusion}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tralie, Christopher J. and McFee, Brian},
  year = {2019},
  month = may,
  pages = {201--205},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2019.8683492},
  abstract = {We describe a novel pipeline to automatically discover hierarchies of repeated sections in musical audio. The proposed method uses similarity network fusion (SNF) to combine different frame-level features into clean affinity matrices, which are then used as input to spectral clustering. While prior spectral clustering approaches to music structure analysis have pre-processed affinity matrices with heuristics specifically designed for this task, we show that the SNF approach directly yields segmentations which agree better with human annotators, as measured by the "L-measure" metric for hierarchical annotations. Furthermore, the SNF approach immediately supports arbitrarily many input features, allowing us to simultaneously discover structure encoded in timbral, harmonic, and rhythmic representations without any changes to the base algorithm.},
  keywords = {audio signal processing,clean affinity matrices,enhanced hierarchical music structure annotations,feature level similarity,frame-level features,harmonic representations,hierarchies,human annotators,matrix algebra,music,music structure analysis,musical audio,pattern clustering,pipeline,pre-processed affinity matrices,prior spectral clustering approaches,repeated sections,rhythmic representations,similarity network fusion,SNF approach,spectral clustering,timbral representations},
}

@inproceedings{trapp2019,
  title = {Bayesian {{Learning}} of {{Sum}}-{{Product Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Trapp, Martin and Peharz, Robert and Ge, Hong and Pernkopf, Franz and Ghahramani, Zoubin},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'{\aftergroup\ignorespaces} {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {6344--6355},
  publisher = {{Curran Associates, Inc.}},
}

@article{truong2020,
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  year = {2020},
  month = feb,
  journal = {Signal Processing},
  volume = {167},
  pages = {107299},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2019.107299},
  abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
  langid = {english},
}

@book{uai2003,
  title = {{{UAI}} '03, Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence, Acapulco, Mexico, August 7-10 2003},
  editor = {Meek, Christopher and Kj{\ae}rulff, Uffe},
  year = {2003},
  publisher = {{Morgan Kaufmann}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/conf/uai/2003},
  isbn = {0-12-705664-5},
  timestamp = {Thu, 15 Dec 2011 17:17:43 +0100}
}

@inproceedings{vergari2015,
  title = {Simplifying, Regularizing and Strengthening Sum-Product Network Structure Learning},
  booktitle = {Joint {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Vergari, Antonio and Di Mauro, Nicola and Esposito, Floriana},
  year = {2015},
  pages = {343--358},
  publisher = {{Springer}},
}

@inproceedings{vergari2019,
  title = {Automatic {{Bayesian}} Density Analysis},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Vergari, Antonio and Molina, Alejandro and Peharz, Robert and Ghahramani, Zoubin and Kersting, Kristian and Valera, Isabel},
  year = {2019},
  volume = {33},
  pages = {5207--5215},
}

@article{wainwright2008,
  ids = {wainwrightGraphicalModelsExponential2007},
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2008},
  month = jan,
  journal = {Foundations and Trends in Machine Learning},
  volume = {1},
  number = {1\textendash 2},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  issue_date = {January 2008},
  langid = {english},
  numpages = {305},
  owner = {robert},
}

@techreport{welch1995,
  title = {An Introduction to the {{Kalman}} Filter},
  author = {Welch, Greg and Bishop, Gary},
  year = {1995},
  institution = {{University of North Carolina}},
}

@article{younger1967recognition,
  title = {Recognition and Parsing of Context-Free Languages in Time N3},
  author = {Younger, Daniel H},
  year = {1967},
  journal = {Information and control},
  volume = {10},
  number = {2},
  pages = {189--208},
  publisher = {{Elsevier}},
}

@inproceedings{yu2019,
  title = {Dag-Gnn: Dag Structure Learning with Graph Neural Networks},
  shorttitle = {Dag-Gnn},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Yu, Yue and Chen, Jie and Gao, Tian and Yu, Mo},
  year = {2019},
  pages = {7154--7163},
  publisher = {{PMLR}},
}

@inproceedings{zhang-etal-2020-fast,
  title = {Fast and Accurate Neural {{CRF}} Constituency Parsing},
  booktitle = {Proceedings of {{IJCAI}}},
  author = {Zhang, Yu and Zhou, Houquan and Li, Zhenghua},
  year = {2020},
  pages = {4046--4053},
  doi = {10.24963/ijcai.2020/560}
}

@inproceedings{zhao2018,
  title = {Gaussian {{Mixture Latent Vector Grammars}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhao, Yanpeng and Zhang, Liwen and Tu, Kewei},
  year = {2018},
  pages = {1181--1189},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1109},
  langid = {english}
}


