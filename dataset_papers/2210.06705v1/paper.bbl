\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2009)Agarwal, Wainwright, Bartlett, and
  Ravikumar]{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter Bartlett, and Pradeep Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 22, 2009.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (98):\penalty0 1--76, 2021.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem[Arora et~al.(2015)Arora, Ge, Ma, and Moitra]{AroraGMM15}
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra.
\newblock Simple, efficient, and neural algorithms for sparse coding.
\newblock In Peter Gr{\"{u}}nwald, Elad Hazan, and Satyen Kale, editors,
  \emph{Proceedings of The 28th Conference on Learning Theory, {COLT} 2015,
  Paris, France, July 3-6, 2015}, 2015.

\bibitem[Attouch et~al.(2010)Attouch, Bolte, Redont, and Soubeyran]{kl1}
Hédy Attouch, Jérôme Bolte, Patrick Redont, and Antoine Soubeyran.
\newblock Proximal alternating minimization and projection methods for
  nonconvex problems: An approach based on the kurdyka-Łojasiewicz inequality.
\newblock \emph{Mathematics of Operations Research}, 35\penalty0 (2):\penalty0
  438--457, 2010.
\newblock ISSN 0364765X, 15265471.

\bibitem[Azulay et~al.(2021)Azulay, Moroshko, Nacson, Woodworth, Srebro,
  Globerson, and Soudry]{GF1}
Shahar Azulay, Edward Moroshko, Mor~Shpigel Nacson, Blake~E. Woodworth, Nathan
  Srebro, Amir Globerson, and Daniel Soudry.
\newblock On the implicit bias of initialization shape: Beyond infinitesimal
  mirror descent.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning, {ICML} 2021, 18-24 July
  2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pages 468--477. {PMLR}, 2021.

\bibitem[Bansal and Gupta(2017)]{bansal2017potential}
Nikhil Bansal and Anupam Gupta.
\newblock Potential-function proofs for first-order methods.
\newblock \emph{arXiv preprint arXiv:1712.04581}, 2017.

\bibitem[Bi et~al.(2022)Bi, Zhang, and Lavaei]{bi2022local}
Yingjie Bi, Haixiang Zhang, and Javad Lavaei.
\newblock Local and global linear convergence of general low-rank matrix
  recovery problems.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 10129--10137, 2022.

\bibitem[Bu et~al.(2019)Bu, Mesbahi, Fazel, and Mesbahi]{bu2019lqr}
Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi.
\newblock Lqr through the lens of first order methods: Discrete-time case.
\newblock \emph{arXiv preprint arXiv:1907.08921}, 2019.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Candes et~al.(2015)Candes, Li, and Soltanolkotabi]{candes2015phase}
Emmanuel~J Candes, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (4):\penalty0 1985--2007, 2015.

\bibitem[Cencini and Ginelli(2013)]{cencini2013lyapunov}
Massimo Cencini and Francesco Ginelli.
\newblock Lyapunov analysis: from dynamical systems theory to applications.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  46\penalty0 (25):\penalty0 250301, 2013.

\bibitem[Chatterjee(2022)]{chatterjee2022convergence}
Sourav Chatterjee.
\newblock Convergence of gradient descent for deep neural networks.
\newblock \emph{arXiv preprint arXiv:2203.16462}, 2022.

\bibitem[Chatterji et~al.(2021)Chatterji, Long, and Bartlett]{GF2}
Niladri~S. Chatterji, Philip~M. Long, and Peter~L. Bartlett.
\newblock The interplay between implicit bias and benign overfitting in
  two-layer linear networks.
\newblock \emph{CoRR}, abs/2108.11489, 2021.

\bibitem[Chellaboina and Haddad(2008)]{chellaboina2008nonlinear}
VijaySekhar Chellaboina and Wassim~M Haddad.
\newblock \emph{Nonlinear dynamical systems and control: A Lyapunov-based
  approach}.
\newblock Princeton University Press, 2008.

\bibitem[Chen et~al.(2019)Chen, Chi, Fan, and Ma]{chen2019gradient}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock \emph{Mathematical Programming}, 176\penalty0 (1):\penalty0 5--37,
  2019.

\bibitem[Chizat and Bach(2018)]{GF3}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock 2018.
\newblock URL \url{http://arxiv.org/abs/1812.07956}.
\newblock cite arxiv:1812.07956.

\bibitem[Clarke(2004)]{clarke2004lyapunov}
Francis Clarke.
\newblock Lyapunov functions and feedback in nonlinear control.
\newblock In \emph{Optimal control, stabilization and nonsmooth analysis},
  pages 267--282. Springer, 2004.

\bibitem[De~Sa et~al.(2015)De~Sa, Re, and Olukotun]{de2015global}
Christopher De~Sa, Christopher Re, and Kunle Olukotun.
\newblock Global convergence of stochastic gradient descent for some non-convex
  matrix problems.
\newblock In \emph{International conference on machine learning}, pages
  2332--2341. PMLR, 2015.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Elkabetz and Cohen(2021)]{elkabetz2021continuous}
Omer Elkabetz and Nadav Cohen.
\newblock Continuous vs. discrete optimization of deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Fang et~al.(1994)Fang, Loparo, and Feng]{fang1994inequalities}
Yuguang Fang, Kenneth~A Loparo, and Xiangbo Feng.
\newblock Inequalities for the trace of matrix product.
\newblock \emph{IEEE Transactions on Automatic Control}, 39\penalty0
  (12):\penalty0 2489--2490, 1994.

\bibitem[Fatkhullin and Polyak(2021)]{fatkhullin2021optimizing}
Ilyas Fatkhullin and Boris Polyak.
\newblock Optimizing static linear feedback: Gradient method.
\newblock \emph{SIAM Journal on Control and Optimization}, 59\penalty0
  (5):\penalty0 3887--3911, 2021.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicitb}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{arXiv preprint arXiv:1806.00468}, 2018{\natexlab{b}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{c}})Gunasekar, Woodworth,
  Bhojanapalli, Neyshabur, and Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
  Nathan Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pages 1--10. IEEE, 2018{\natexlab{c}}.

\bibitem[Gunasekar et~al.(2021)Gunasekar, Woodworth, and
  Srebro]{gunasekar2021mirrorless}
Suriya Gunasekar, Blake Woodworth, and Nathan Srebro.
\newblock Mirrorless mirror descent: A natural derivation of mirror descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2305--2313. PMLR, 2021.

\bibitem[Jain et~al.(2017)Jain, Jin, Kakade, and Netrapalli]{jain2017global}
Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli.
\newblock Global convergence of non-convex gradient descent for computing
  matrix squareroot.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 479--488.
  PMLR, 2017.

\bibitem[Ji and Telgarsky(2018)]{ji2018risk}
Ziwei Ji and Matus Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[Jin et~al.(2016{\natexlab{a}})Jin, Kakade, and
  Netrapalli]{jin2016provable}
Chi Jin, Sham~M Kakade, and Praneeth Netrapalli.
\newblock Provable efficient online matrix completion via non-convex stochastic
  gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 29,
  2016{\natexlab{a}}.

\bibitem[Jin et~al.(2016{\natexlab{b}})Jin, Kakade, and Netrapalli]{mcomplete}
Chi Jin, Sham~M. Kakade, and Praneeth Netrapalli.
\newblock Provable efficient online matrix completion via non-convex stochastic
  gradient descent.
\newblock In \emph{NIPS}, 2016{\natexlab{b}}.

\bibitem[Kale et~al.(2021)Kale, Sekhari, and Sridharan]{sgdnotimp}
Satyen Kale, Ayush Sekhari, and Karthik Sridharan.
\newblock {SGD:} the role of implicit regularization, batch-size and
  multiple-epochs.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems 2021, NeurIPS 2021,
  December 6-14, 2021, virtual}, pages 27422--27433, 2021.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{pl1}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-\l{}ojasiewicz condition.
\newblock In \emph{European Conference on Machine Learning and Knowledge
  Discovery in Databases - Volume 9851}, ECML PKDD 2016, page 795–811,
  Berlin, Heidelberg, 2016. Springer-Verlag.
\newblock ISBN 9783319461274.
\newblock \doi{10.1007/978-3-319-46128-1_50}.

\bibitem[Kellett(2015)]{converse}
Christopher~M. Kellett.
\newblock Classical converse theorems in lyapunov's second method.
\newblock \emph{Discrete and Continuous Dynamical Systems - B}, 20\penalty0
  (8):\penalty0 2333--2360, 2015.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and Yuan]{onepoint}
Robert Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does sgd escape local minima?
\newblock In Jennifer~G. Dy and Andreas Krause, editors, \emph{ICML}, volume~80
  of \emph{Proceedings of Machine Learning Research}, pages 2703--2712. PMLR,
  2018.

\bibitem[Kovachki and Stuart(2021)]{kovachki2021continuous}
Nikola~B Kovachki and Andrew~M Stuart.
\newblock Continuous time analysis of momentum methods.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (17):\penalty0 1--40, 2021.

\bibitem[Krichene(2016{\natexlab{a}})]{krichene2016continuous}
Walid Krichene.
\newblock Continuous and discrete dynamics for online learning and convex
  optimization.
\newblock \emph{Ph. D. Dissertation}, 2016{\natexlab{a}}.

\bibitem[Krichene(2016{\natexlab{b}})]{krichene2016lyapunov}
Walid Krichene.
\newblock \emph{A Lyapunov Approach to Accelerated First-Order Optimization In
  Continuous and Discrete Time}.
\newblock PhD thesis, University of California, Berkeley, 2016{\natexlab{b}}.

\bibitem[Krichene et~al.(2015)Krichene, Bayen, and Bartlett]{NIPS2015_f60bb6bb}
Walid Krichene, Alexandre Bayen, and Peter~L Bartlett.
\newblock Accelerated mirror descent in continuous and discrete time.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.

\bibitem[Kurdyka(1998)]{kl2}
Krzysztof Kurdyka.
\newblock On gradients of functions definable in o-minimal structures.
\newblock \emph{Annales de l'institut Fourier}, 48\penalty0 (3):\penalty0
  769--783, 1998.

\bibitem[Lojasiewicz(1963)]{pl3}
Stanislaw Lojasiewicz.
\newblock A topological property of real analytic subsets.
\newblock \emph{Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es
  partielles}, 117\penalty0 (87-89):\penalty0 2, 1963.

\bibitem[Ma et~al.(2018)Ma, Wang, Chi, and Chen]{ma2018implicit}
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval and matrix completion.
\newblock In \emph{International Conference on Machine Learning}, pages
  3345--3354. PMLR, 2018.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  6820--6829. PMLR, 2020.

\bibitem[Mei et~al.(2021)Mei, Gao, Dai, Szepesvari, and
  Schuurmans]{mei2021leveraging}
Jincheng Mei, Yue Gao, Bo~Dai, Csaba Szepesvari, and Dale Schuurmans.
\newblock Leveraging non-uniformity in first-order non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7555--7564. PMLR, 2021.

\bibitem[Mohammadi et~al.(2019)Mohammadi, Zare, Soltanolkotabi, and
  Jovanovi{\'c}]{mohammadi2019global}
Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo~R
  Jovanovi{\'c}.
\newblock Global exponential convergence of gradient methods over the nonconvex
  landscape of the linear quadratic regulator.
\newblock In \emph{2019 IEEE 58th Conference on Decision and Control (CDC)},
  pages 7474--7479. IEEE, 2019.

\bibitem[Nemirovski et~al.()Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovskistochastic}
A~Nemirovski, A~Juditsky, G~Lan, and A~Shapiro.
\newblock Stochastic approximation approach to stochastic programming.
\newblock In \emph{SIAM J. Optim}. Citeseer.

\bibitem[Nemirovskij and Yudin(1983)]{nemirovskij1983problem}
Arkadij~Semenovi{\v{c}} Nemirovskij and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Orvieto and Lucchi(2019)]{orvieto2019continuous}
Antonio Orvieto and Aurelien Lucchi.
\newblock Continuous-time models for stochastic optimization algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Polyak(1963)]{pl2}
B.T. Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, 1963.
\newblock ISSN 0041-5553.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and Tewari]{NIPS2010_76cf99d3}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock Smoothness, low noise and fast rates.
\newblock In J.~Lafferty, C.~Williams, J.~Shawe-Taylor, R.~Zemel, and
  A.~Culotta, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~23. Curran Associates, Inc., 2010.

\bibitem[Su et~al.(2014)Su, Boyd, and Candes]{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling nesterov’s accelerated
  gradient method: theory and insights.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Tan and Vershynin(2019)]{tan2019online}
Yan~Shuo Tan and Roman Vershynin.
\newblock Online stochastic gradient descent with arbitrary initialization
  solves non-smooth, non-convex phase retrieval.
\newblock \emph{arXiv preprint arXiv:1910.12837}, 2019.

\bibitem[Vardi and Shamir(2021)]{vardi2021implicit}
Gal Vardi and Ohad Shamir.
\newblock Implicit regularization in relu networks with the square loss.
\newblock In \emph{Conference on Learning Theory}, pages 4224--4258. PMLR,
  2021.

\bibitem[Wilson(2018)]{wilson2018lyapunov}
Ashia Wilson.
\newblock \emph{Lyapunov arguments in optimization}.
\newblock University of California, Berkeley, 2018.

\bibitem[Wilson et~al.(2021{\natexlab{a}})Wilson, Recht, and
  Jordan]{JMLR:v22:20-195}
Ashia~C. Wilson, Ben Recht, and Michael~I. Jordan.
\newblock A lyapunov analysis of accelerated methods in optimization.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (113):\penalty0 1--34, 2021{\natexlab{a}}.

\bibitem[Wilson et~al.(2021{\natexlab{b}})Wilson, Recht, and
  Jordan]{wilson2021lyapunov}
Ashia~C Wilson, Ben Recht, and Michael~I Jordan.
\newblock A lyapunov analysis of accelerated methods in optimization.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (113):\penalty0 1--34, 2021{\natexlab{b}}.

\bibitem[Wojtowytsch(2021{\natexlab{a}})]{wojtowytsch2021discrete}
Stephan Wojtowytsch.
\newblock Stochastic gradient descent with noise of machine learning type. part
  i: Discrete time analysis.
\newblock \emph{arXiv preprint arXiv:2105.01650}, 2021{\natexlab{a}}.

\bibitem[Wojtowytsch(2021{\natexlab{b}})]{wojtowytsch2021stochastic}
Stephan Wojtowytsch.
\newblock Stochastic gradient descent with noise of machine learning type. part
  ii: Continuous time analysis.
\newblock \emph{arXiv preprint arXiv:2106.02588}, 2021{\natexlab{b}}.

\bibitem[Yuan et~al.(2022)Yuan, Gower, and Lazaric]{yuan2022general}
Rui Yuan, Robert~M Gower, and Alessandro Lazaric.
\newblock A general sample complexity analysis of vanilla policy gradient.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3332--3380. PMLR, 2022.

\bibitem[Zeng et~al.(2018)Zeng, Ouyang, Lau, Lin, and Yao]{zeng2018global}
Jinshan Zeng, Shikang Ouyang, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao.
\newblock Global convergence in deep learning with variable splitting via the
  kurdyka-{\l}ojasiewicz property.
\newblock \emph{arXiv preprint arXiv:1803.00225}, 9, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Orvieto, Daneshmand, Hofmann, and
  Smith]{zhang2021revisiting}
Peiyuan Zhang, Antonio Orvieto, Hadi Daneshmand, Thomas Hofmann, and Roy~S
  Smith.
\newblock Revisiting the role of euler numerical integration on acceleration
  and stability in convex optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3979--3987. PMLR, 2021.

\end{thebibliography}
