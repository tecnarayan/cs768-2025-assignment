@article{phan2019thompson,
  title={Thompson sampling and approximate inference},
  author={Phan, My and Abbasi Yadkori, Yasin and Domke, Justin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{zhang2021feel,
  title={Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning},
  author={Zhang, Tong},
  journal={arXiv preprint arXiv:2110.00871},
  year={2021}
}


@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  year={1989},
  publisher={Elsevier}
}
@article{rolnick2019experience,
  title={Experience replay for continual learning},
  author={Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{ecoffet2021first,
  title={\href{https://www.nature.com/articles/s41586-020-03157-9}{First return, then explore}},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={Nature},
  volume={590},
  year={2021},
  pages={580--586},
  number={7847},
  publisher={Nature Publishing Group}
}


@article{aytar2018playing,
  title={\href{https://papers.nips.cc/paper/2018/hash/35309226eb45ec366ca86a4329a2b7c3-Abstract.html}{Playing hard exploration games by watching youtube}},
  author={Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Thomas and Wang, Ziyu and De Freitas, Nando},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2020},
  organization={PMLR}
}

@article{agrawal2017posterior,
  title={Posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  journal={arXiv preprint arXiv:1705.07041},
  year={2017}
}

@inproceedings{tziortziotis2013linear,
  title={\href{https://www.ijcai.org/Proceedings/13/Papers/255.pdf}{Linear Bayesian reinforcement learning}},
  author={Tziortziotis, Nikolaos and Dimitrakakis, Christos and Blekas, Konstantinos},
  booktitle={International joint conference on artificial intelligence},
  year={2013}
}

@article{yang2021exploration,
  title={Exploration in deep reinforcement learning: a comprehensive survey},
  author={Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Hao, Jianye and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
  journal={arXiv preprint arXiv:2109.06668},
  year={2021}
}


@inproceedings{snoek2015scalable,
  title={\href{http://proceedings.mlr.press/v37/snoek15.html}{Scalable bayesian optimization using deep neural networks}},
  author={Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  year={2015},
  pages = 	 {2171--2180},
  organization={PMLR}
}


@book{Rasmussen2004,
  title={\href{https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning}{Gaussian processes for machine learning}},
  author={Rasmussen, Carl Edward},
  year={2005},
  publisher={MIT press}
}


@inproceedings{azar2017minimax,
  title={\href{https://proceedings.mlr.press/v70/azar17a.html}{Minimax regret bounds for reinforcement learning}},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  year={2017},
  organization={PMLR},
  pages = 	 {263--272},
  volume = 	 {70},
}


@article{neco_a_01539,
    author = {Ramesh, Aditya and Rauber, Paulo and Conserva, Michelangelo and Schmidhuber, Jürgen},
    title = "{Recurrent Neural-Linear Posterior Sampling for Nonstationary Contextual Bandits}",
    journal = {Neural Computation},
    volume = {34},
    year = {2022},
}



@inproceedings{
riquelme2018deep,
title={\href{https://openreview.net/forum?id=SyYe6k-CW}{Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling}},
author={Carlos Riquelme and George Tucker and Jasper Snoek},
booktitle={International Conference on Learning Representations},
year={2018},
}

@inproceedings{zanette2019tighter,
  title={\href{https://proceedings.mlr.press/v97/zanette19a.html}{Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds}},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  year={2019},
  pages = 	 {7304--7312},
  organization={PMLR}
}


@misc{rainbowdqn,
  doi = {10.48550/ARXIV.1710.02298},
  
  url = {https://arxiv.org/abs/1710.02298},
  
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{menard2021ucb,
  title={\href{http://proceedings.mlr.press/v139/menard21b.html}{UCB Momentum Q-learning: Correcting the bias without forgetting}},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  year={2021},
  pages = 	 {7609--7618},
  organization={PMLR}
}


@incollection{SUTTON1990216,
title = {Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming},
editor = {Bruce Porter and Raymond Mooney},
booktitle = {Machine Learning Proceedings 1990},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
year = {1990},
isbn = {978-1-55860-141-3},
doi = {https://doi.org/10.1016/B978-1-55860-141-3.50030-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558601413500304},
author = {Richard S. Sutton},
abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.}
}

@article{osband2013more,
  title={\href{https://papers.nips.cc/paper/2013/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html}{(More) efficient reinforcement learning via posterior sampling}},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}


@article{munos2008finite,
  title={\href{https://www.jmlr.org/papers/v9/munos08a.html}{Finite-Time Bounds for Fitted Value Iteration}},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={5},
  pages   = {815--857},
  year={2008}
}

@ARTICLE{osband2019-jv,
  title     = "Deep Exploration via Randomized Value Functions",
  author    = "Osband, I and Van Roy, B and Russo, D J and Wen, Z",
  abstract  = "We study the use of randomized value functions to guide deep
               exploration in reinforcement learning. This offers an elegant
               means for synthesizing statistically and computationally
               efficient exploration with common practical approaches to value
               function learning. We present several reinforcement learning
               algorithms that leverage randomized value functions and
               demonstrate their efficacy through computational studies. We
               also prove a regret bound that establishes statistical
               efficiency with a tabular representation.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  year      =  2019
}

@article{agrawal2017optimistic,
  title={\href{https://papers.nips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html}{Optimistic posterior sampling for reinforcement learning: worst-case regret bounds}},
  author={Agrawal, Shipra and Jia, Randy},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
}


@article{mlp-universal,
title = "Multilayer feedforward networks are universal approximators",
journal = "Neural Networks",
volume = "2",
number = "5",
year = "1989",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(89)90020-8",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
note={\newline DOI: 10.1016/0893-6080(89)90020-8}
}

@ARTICLE{rbf-universal,
author={J. {Park} and I. W. {Sandberg}},
journal={Neural Computation},
title={Universal {A}pproximation {U}sing {R}adial-{B}asis-{F}unction {N}etworks},
year={1991},
volume={3},
number={2},
keywords={},
doi={10.1162/neco.1991.3.2.246},
ISSN={0899-7667},
note={DOI: 10.1162/neco.1991.3.2.246}
}

@article{russo2019worst,
  title={\href{https://papers.nips.cc/paper/2019/hash/451ae86722d26a608c2e174b2b2773f1-Abstract.html}{Worst-case regret bounds for exploration via randomized value functions}},
  author={Russo, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{ostrovski2017count,
  title={Count-based exploration with neural density models},
  author={Ostrovski, Georg and Bellemare, Marc G and Oord, A{\"a}ron and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  year={2017},
  organization={PMLR}
}


@article{bellemare2016unifying,
  title={\href{https://papers.nips.cc/paper/2016/hash/afda332245e2af431fb7b672a68b659d-Abstract.html}{Unifying count-based exploration and intrinsic motivation}},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{tang2017exploration,
  title={\# exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@inproceedings{Ballard1987ModularLI,
  title={Modular Learning in Neural Networks},
  author={Dana H. Ballard},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={1987}
}
% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bellman1958-ru,
  title    = "Dynamic programming and stochastic control processes",
  author   = "Bellman, Richard",
  abstract = "Consider a system S specified at any time t by a finite
              dimensional vector x(t) satisfying a vector differential equation
              dx/dt = g[x, r(t), f(t)], x(0) = c, where c is the initial state,
              r(t) is a random forcing term possessing a known distribution,
              and f(t) is a forcing term chosen, via a feedback process, so as
              to minimize the expected value of a functional J(x) = ƒ0T h(x −
              y, t) dG(t), where y(t) is a known function, or chosen so as to
              minimize the functional defined by the probability that max0 ≦ t
              ≦ T h(x − y, t) exceed a specified bound. It is shown how the
              functional equation technique of dynamic programming may be used
              to obtain a new computational and analytic approach to problems
              of this genre. The limited memory capacity of present-day digital
              computers limits the routine application of these techniques to
              first and second order systems at the moment, with limited
              application to higher order systems.",
  journal  = "Information and Control",
  volume   =  1,
  number   =  3,
  month    =  sep,
  year     =  1958
}


@ARTICLE{Watkins1992-yl,
  title    = "Q-learning",
  author   = "Watkins, Christopher J C H and Dayan, Peter",
  abstract = "Q-learning (Watkins, 1989) is a simple way for agents to learn
              how to act optimally in controlled Markovian domains. It amounts
              to an incremental method for dynamic programming which imposes
              limited computational demands. It works by successively improving
              its evaluations of the quality of particular actions at
              particular states.",
  journal  = "Machine Learning",
  volume   =  8,
  number   =  3,
  month    =  may,
  year     =  1992
}


@article{Ha2018-jd,
  title={\href{https://papers.nips.cc/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html}{Recurrent world models facilitate policy evolution}},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}



@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{schaul2016,
  author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  title={\href{http://arxiv.org/abs/1511.05952}{Prioritized Experience Replay}},
  booktitle={International Conference on Learning Representations},
  year={2016},
}


@inproceedings{aggarwal2001surprising,
  title={On the surprising behavior of distance metrics in high dimensional space},
  author={Aggarwal, Charu C and Hinneburg, Alexander and Keim, Daniel A},
  booktitle={International conference on database theory},
  year={2001},
  organization={Springer}
}

@article{Schrittwieser2019-df,
  title={\href{https://www.nature.com/articles/s41586-020-03051-4}{Mastering atari, go, chess and shogi by planning with a learned model}},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{Landolfi2019-mo,
  title={A Model-based Approach for Sample-efficient Multi-task Reinforcement Learning},
  author={Landolfi, Nicholas C and Thomas, Garrett and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.04964},
  year={2019}
}



@article{DBLP:journals/corr/abs-2006-16712,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Jonker, Catholijn M},
  journal={arXiv preprint arXiv:2006.16712},
  year={2020}
}

@article{campbell2002deep,
  title={Deep blue},
  author={Campbell, Murray and Hoane Jr, A Joseph and Hsu, Feng-hsiung},
  journal={Artificial intelligence},
  volume={134},
  number={1-2},
  year={2002},
  publisher={Elsevier}
}


@inproceedings{Kaiser2019-ts,
  title={\href{https://openreview.net/forum?id=S1xCPJHtDB}{Model Based Reinforcement Learning for Atari}},
  author={Kaiser, {\L}ukasz and Babaeizadeh, Mohammad and Mi{\l}os, Piotr and Osi{\'n}ski, B{\l}a{\.z}ej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}



@article{DBLP:journals/corr/abs-2102-05207,
  author    = {Zhangjie Cao and
               Minae Kwon and
               Dorsa Sadigh},
  title     = {Transfer Reinforcement Learning across Homotopy Classes},
  journal   = {CoRR},
  volume    = {abs/2102.05207},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.05207},
  archivePrefix = {arXiv},
  eprint    = {2102.05207},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-05207.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Sekar2020-fl,
  title={Planning to explore via self-supervised world models},
  author={Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR}
}

@INPROCEEDINGS{Chua2018-lk,
  title     = "Deep Reinforcement Learning in a Handful of Trials using
               Probabilistic Dynamics Models",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and
               Levine, Sergey",
  editor    = "Bengio, S and Wallach, H and Larochelle, H and Grauman, K and
               Cesa-Bianchi, N and Garnett, R",
  publisher = "Curran Associates, Inc.",
  volume    =  31,
  year      =  2018
}

@article{Taiga2019-mv,
  title={Benchmarking bonus-based exploration methods on the arcade learning environment},
  author={Ta{\"\i}ga, Adrien Ali and Fedus, William and Machado, Marlos C and Courville, Aaron and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1908.02388},
  year={2019}
}
@article{Stadie2015-gl,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}



@BOOK{Sutton2018-yv,
  title     = "Reinforcement Learning: An Introduction",
  author    = "Sutton, Richard S and Barto, Andrew G",
  abstract  = "The significantly expanded and updated new edition of a widely
               used text on reinforcement learning, one of the most active
               research areas in artificial intelligence. Reinforcement
               learning, one of the most active research areas in artificial
               intelligence, is a computational approach to learning whereby an
               agent tries to maximize the total amount of reward it receives
               while interacting with a complex, uncertain environment. In
               Reinforcement Learning, Richard Sutton and Andrew Barto provide
               a clear and simple account of the field's key ideas and
               algorithms. This second edition has been significantly expanded
               and updated, presenting new topics and updating coverage of
               other topics. Like the first edition, this second edition
               focuses on core online learning algorithms, with the more
               mathematical material set off in shaded boxes. Part I covers as
               much of reinforcement learning as possible without going beyond
               the tabular case for which exact solutions can be found. Many
               algorithms presented in this part are new to the second edition,
               including UCB, Expected Sarsa, and Double Learning. Part II
               extends these ideas to function approximation, with new sections
               on such topics as artificial neural networks and the Fourier
               basis, and offers expanded treatment of off-policy learning and
               policy-gradient methods. Part III has new chapters on
               reinforcement learning's relationships to psychology and
               neuroscience, as well as an updated case-studies chapter
               including AlphaGo and AlphaGo Zero, Atari game playing, and IBM
               Watson's wagering strategy. The final chapter discusses the
               future societal impacts of reinforcement learning.",
  publisher = "A Bradford Book",
  year      =  2018,
  address   = "Cambridge, MA, USA"
}

@BOOK{Camacho1999-vb,
  title     = "Model Predictive Control",
  author    = "Camacho, Eduardo F and Bordons, Carlos",
  publisher = "Springer, London",
  year      =  1999
}


@INPROCEEDINGS{Coulom2007-wd,
  title     = "Efficient Selectivity and Backup Operators in {Monte-Carlo} Tree
               Search",
  booktitle = "Computers and Games",
  author    = "Coulom, R{\'e}mi",
  abstract  = "A Monte-Carlo evaluation consists in estimating a position by
               averaging the outcome of several random continuations. The
               method can serve as an evaluation function at the leaves of a
               min-max tree. This paper presents a new framework to combine
               tree search with Monte-Carlo evaluation, that does not separate
               between a min-max phase and a Monte-Carlo phase. Instead of
               backing-up the min-max value close to the root, and the average
               value at some depth, a more general backup operator is defined
               that progressively changes from averaging to min-max as the
               number of simulations grows. This approach provides a
               fine-grained control of the tree growth, at the level of
               individual simulations, and allows efficient selectivity. The
               resulting algorithm was implemented in a 9$\times$9 Go-playing
               program, Crazy Stone, that won the 10th KGS computer-Go
               tournament.",
  publisher = "Springer Berlin Heidelberg",
  year      =  2007
}
@article{Henaff2017-mw,
  title={Model-based planning with discrete and continuous actions},
  author={Henaff, Mikael and Whitney, William F and LeCun, Yann},
  journal={arXiv preprint arXiv:1705.07177},
  year={2017}
}



@ARTICLE{Rosenblatt1958-lg,
  title     = "The perceptron: a probabilistic model for information storage
               and organization in the brain",
  author    = "Rosenblatt, F",
  abstract  = "The first of these questions is in the province of sensory
               physiology, and is the only one for which appreciable
               understanding has been achieved. This article will be concerned
               primarily with the second and third questions, which are still
               subject to a vast amount of speculation, and where the few
               relevant facts currently supplied by neurophysiology have not
               yet been integrated into an acceptable theory. With regard to
               the second question, two alternative positions have been
               maintained. The first suggests that storage of sensory
               information is in the form of coded representations or images,
               with some sort of one-to-one mapping between the sensory
               stimulus",
  journal   = "Psychol. Rev.",
  publisher = "American Psychological Association (APA)",
  volume    =  65,
  number    =  6,
  month     =  nov,
  year      =  1958,
  keywords  = "PERCEPTION",
  language  = "en"
}


@ARTICLE{9141230,  author={Yang, Dujia and Qin, Xiaowei and Xu, Xiaodong and Li, Chensheng and Wei, Guo},  journal={IEEE Access},   title={Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory},   year={2020},  volume={8},  number={}}

@article{Mnih2015-dx,
  title={\href{https://www.nature.com/articles/nature14236}{Human-level control through deep reinforcement learning}},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}



@ARTICLE{Silver2016-df,
  title     = "Mastering the game of Go with deep neural networks and tree
               search",
  author    = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
               Arthur and Sifre, Laurent and van den Driessche, George and
               Schrittwieser, Julian and Antonoglou, Ioannis and
               Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and
               Grewe, Dominik and Nham, John and Kalchbrenner, Nal and
               Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and
               Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis",
  journal   = "{Nature}",
  publisher = "Nature Publishing Group",
  volume    =  529,
  number    =  7587,
  pages     = "484--489",
  month     =  jan,
  year      =  2016,
  language  = "en"
}


@article{recordmean,
  author = {Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien},  
  title = {\href{https://arxiv.org/abs/1908.04683}{Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field}},
  
  publisher = {arXiv},
  
  journal={arXiv preprint arXiv:1908.04683},
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bellemare2013arcade,
  title={\href{https://jair.org/index.php/jair/article/view/10819}{The arcade learning environment: An evaluation platform for general agents}},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}
@inproceedings{conserva2022hardness,
  title={Hardness in Markov Decision Processes: Theory and Practice},
  author={Conserva, Michelangelo and Rauber, Paulo},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems},
 volume = {35},
}

@inproceedings{masci2011stacked,
  title={\href{https://link.springer.com/chapter/10.1007/978-3-642-21735-7_7}{Stacked convolutional auto-encoders for hierarchical feature extraction}},
  author={Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
  booktitle={International conference on artificial neural networks},
  year={2011},
  pages={52--59},
  organization={Springer}
}


@incollection{BOTEV201335,
title = {\href{https://www.sciencedirect.com/science/article/abs/pii/B9780444538598000035}{Chapter 3 - The Cross-Entropy Method for Optimization}},
publisher = {Elsevier},
volume = {31},
year = {2013},
booktitle = {Handbook of Statistics},
issn = {0169-7161},
pages = {35-59},
author = {Zdravko I. Botev and Dirk P. Kroese and Reuven Y. Rubinstein and Pierre L’Ecuyer}
}

@book{24286,
	author = {Camacho, E. F. and Bordons, C.},
	title = {\href{https://link.springer.com/book/10.1007/978-0-85729-398-5}{Model Predictive control}},
	publisher = {Springer London},
	year = {2007},
	series = {Advanced Textbooks in Control and Signal Processing},
}

@inproceedings{NEURIPS2019_c2073ffa,
 author = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {\href{https://proceedings.neurips.cc/paper/2019/hash/c2073ffa77b5357a498057413bb09d3a-Abstract.html}{Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}},
 volume = {32},
 year = {2019}
}


@article{kumar2020discor,
  title={\href{https://proceedings.neurips.cc/paper/2020/hash/d7f426ccbc6db7e235c57958c21c5dfa-Abstract.html}{Discor: Corrective feedback in reinforcement learning via distribution correction}},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{flennerhag2020temporal,
  title={\href{https://arxiv.org/abs/2010.02255}{Temporal difference uncertainties as a signal for exploration}},
  author={Flennerhag, Sebastian and Wang, Jane X and Sprechmann, Pablo and Visin, Francesco and Galashov, Alexandre and Kapturowski, Steven and Borsa, Diana L and Heess, Nicolas and Barreto, Andre and Pascanu, Razvan},
  journal={arXiv preprint arXiv:2010.02255},
  year={2020}
}

@article{osband2016deep,
  title={\href{https://papers.nips.cc/paper/2016/hash/8d8818c8e140c64c743113f563cf750f-Abstract.html}{Deep exploration via bootstrapped DQN}},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{engel2003bayes,
  title={\href{https://www.aaai.org/Library/ICML/2003/icml03-023.php}{Bayes meets Bellman: The Gaussian process approach to temporal difference learning}},
  author={Engel, Yaakov and Mannor, Shie and Meir, Ron},
  booktitle={International Conference on Machine Learning},
  year={2003},
  pages={154--161},
  organization={PMLR}
}

@inproceedings{engel2005reinforcement,
  title={\href{https://icml.cc/Conferences/2005/proceedings/papers/026_Reinforcement_EngelEtAl.pdf}{Reinforcement learning with Gaussian processes}},
  author={Engel, Yaakov and Mannor, Shie and Meir, Ron},
  booktitle={International Conference on Machine Learning},
  year={2005},
  pages={201--208},
  organization={PMLR}
}


@inproceedings{mai2021sample,
  title={Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation},
  author={Mai, Vincent and Mani, Kaustubh and Paull, Liam},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{plappert2018parameter,
  title={Parameter Space Noise for Exploration},
  author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{ian2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Ian, Osband and Benjamin, Van Roy and Daniel, Russo},
  booktitle={International Conference on Neural Information Processing Systems},
  year={2013}
}


@inproceedings{o2018uncertainty,
  title={\href{https://proceedings.mlr.press/v80/odonoghue18a.html}{The uncertainty bellman equation and exploration}},
  author={O’Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  booktitle={International Conference on Machine Learning},
  year={2018},
  pages = 	 {3839--3848},
  organization={PMLR}
}


@article{barreto2017successor,
  title={Successor features for transfer in reinforcement learning},
  author={Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}


@article{berner2019dota,
  title={\href{https://arxiv.org/abs/1912.06680}{Dota 2 with large scale deep reinforcement learning}},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint},
  year={2019}
}

@article{vinyals2019grandmaster,
  title={\href{https://www.nature.com/articles/s41586-019-1724-z}{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}


@article{dayan1993improving,
  title={Improving generalization for temporal difference learning: The successor representation},
  author={Dayan, Peter},
  journal={Neural computation},
  volume={5},
  number={4},
  pages={613--624},
  year={1993},
  publisher={MIT Press}
}


@article{janz2019successor,
  title={\href{https://proceedings.neurips.cc/paper/2019/hash/1b113258af3968aaf3969ca67e744ff8-Abstract.html}{Successor uncertainties: exploration and uncertainty in temporal difference learning}},
  author={Janz, David and Hron, Jiri and Mazur, Przemys{\l}aw and Hofmann, Katja and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Tschiatschek, Sebastian},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@INCOLLECTION{Tesauro1995-pe,
  title     = "{TD-Gammon}: A {Self-Teaching} Backgammon Program",
  booktitle = "Applications of Neural Networks",
  author    = "Tesauro, Gerald",
  editor    = "Murray, Alan F",
  abstract  = "This chapter describes TD-Gammon, a neural network that is able
               to teach itself to play backgammon solely by playing against
               itself and learning from the results. TD-Gammon uses a recently
               proposed reinforcement learning algorithm called TD($\lambda$)
               (Sutton, 1988), and is apparently the first application of this
               algorithm to a complex nontrivial task. Despite starting from
               random initial weights (and hence random initial strategy),
               TD-Gammon achieves a surprisingly strong level of play. With
               zero knowledge built in at the start of learning (i.e. given
               only a ``raw'' description of the board state), the network
               learns to play the entire game at a strong intermediate level
               that surpasses not only conventional commercial programs, but
               also comparable networks trained via supervised learning on a
               large corpus of human expert games. The hidden units in the
               network have apparently discovered useful features, a
               longstanding goal of computer games research.",
  publisher = "Springer US",
  pages     = "267--285",
  year      =  1995,
  address   = "Boston, MA"
}


@ARTICLE{Vinyals2019-sn,
  title     = "Grandmaster level in {StarCraft} {II} using multi-agent
               reinforcement learning",
  author    = "Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M
               and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung
               and Choi, David H and Powell, Richard and Ewalds, Timo and
               Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss,
               Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and
               Cai, Trevor and Agapiou, John P and Jaderberg, Max and
               Vezhnevets, Alexander S and Leblond, R{\'e}mi and Pohlen, Tobias
               and Dalibard, Valentin and Budden, David and Sulsky, Yury and
               Molloy, James and Paine, Tom L and Gulcehre, Caglar and Wang,
               Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and
               Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and
               Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and
               Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and
               Silver, David",
  abstract  = "Many real-world applications require artificial agents to
               compete and coordinate with other agents in complex
               environments. As a stepping stone to this goal, the domain of
               StarCraft has emerged as an important challenge for artificial
               intelligence research, owing to its iconic and enduring status
               among the most difficult professional esports and its relevance
               to the real world in terms of its raw complexity and multi-agent
               challenges. Over the course of a decade and numerous
               competitions1--3, the strongest agents have simplified important
               aspects of the game, utilized superhuman capabilities, or
               employed hand-crafted sub-systems4. Despite these advantages, no
               previous agent has come close to matching the overall skill of
               top StarCraft players. We chose to address the challenge of
               StarCraft using general-purpose learning methods that are in
               principle applicable to other complex domains: a multi-agent
               reinforcement learning algorithm that uses data from both human
               and agent games within a diverse league of continually adapting
               strategies and counter-strategies, each represented by deep
               neural networks5,6. We evaluated our agent, AlphaStar, in the
               full game of StarCraft II, through a series of online games
               against human players. AlphaStar was rated at Grandmaster level
               for all three StarCraft races and above 99.8\% of officially
               ranked human players. AlphaStar uses a multi-agent reinforcement
               learning algorithm and has reached Grandmaster level, ranking
               among the top 0.2\% of human players for the real-time strategy
               game StarCraft II.",
  journal   = "{Nature}",
  publisher = "Nature Publishing Group",
  volume    =  575,
  number    =  7782,
  pages     = "350--354",
  month     =  oct,
  year      =  2019,
  language  = "en"
}


@article{OpenAI2019-ij,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dkebiak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}


@misc{mlpblog,
  title = {A Simple overview of Multilayer Perceptron},
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2020/12/mlp-multilayer-perceptron-simple-overview/}},
  note = {Accessed: 2019-12-13}
  
}


@misc{aeblog,
  title = {All about autoencoders},
  howpublished = {\url{https://pythonmachinelearning.pro/all-about-autoencoders/}},
  note = {Accessed: 2017-10-30}
  
}

@article{silva2020novel,
  title={A Novel Approach to Condition Monitoring of the Cutting Process Using Recurrent Neural Networks},
  author={Silva, Rui and Ara{\'u}jo, Ant{\'o}nio},
  journal={Sensors},
  volume={20},
  number={16},
  pages={4493},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{phung2019high,
  title={A high-accuracy model average ensemble of convolutional neural networks for classification of cloud image patches on small datasets},
  author={Phung, Van Hiep and Rhee, Eun Joo and others},
  journal={Applied Sciences},
  volume={9},
  number={21},
  pages={4500},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{ballard1987modular,
  title={Modular learning in neural networks.},
  author={Ballard, Dana H},
  booktitle={AAAI},
  pages={279--284},
  year={1987}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{Gou2019-om,
  title="{DQN with model-based exploration: efficient learning on environments with sparse rewards}",
  author={Gou, Stephen Zhen and Liu, Yuyang},
  journal={arXiv preprint arXiv:1903.09295},
  year={2019}
}

@MISC{coumans2021,
author =   {Erwin Coumans and Yunfei Bai},
title =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
howpublished = {\url{http://pybullet.org}},
year = {2016--2021}
}


@article{Brockman2016-ri,
  title="{Openai gym}",
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{Raffin2020-ly,
  title={Generalized state-dependent exploration for deep reinforcement learning in robotics},
  author={Raffin, Antonin and Stulp, Freek},
  journal={arXiv preprint arXiv:2005.05719},
  year={2020}
}

@article{Deisenroth2015-jc,
  title={Gaussian processes for data-efficient learning in robotics and control},
  author={Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={2},
  pages={408--423},
  year={2013},
  publisher={IEEE}
}
@inproceedings{Finn2015-gt,
  title={Deep spatial autoencoders for visuomotor learning},
  author={Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={512--519},
  year={2016},
  organization={IEEE}
}
@article{Watter2015-zk,
  title={Embed to control: A locally linear latent dynamics model for control from raw images},
  author={Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1506.07365},
  year={2015}
}
@article{Wahlstrom2014-ym,
  title={Learning deep dynamical models from image pixels},
  author={Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B and Deisenroth, Marc Peter},
  journal={IFAC-PapersOnLine},
  volume={48},
  number={28},
  pages={1059--1064},
  year={2015},
  publisher={Elsevier}
}




@ARTICLE{Bellman1957-fo,
  title     = "A Markovian Decision Process",
  author    = "Bellman, Richard",
  journal   = "Journal of Mathematics and Mechanics",
  publisher = "Indiana University Mathematics Department",
  volume    =  6,
  number    =  5,
  pages     = "679--684",
  year      =  1957
}


@ARTICLE{Pan2010-hg,
  title    = "A Survey on Transfer Learning",
  author   = "Pan, Sinno Jialin and Yang, Qiang",
  abstract = "A major assumption in many machine learning and data mining
              algorithms is that the training and future data must be in the
              same feature space and have the same distribution. However, in
              many real-world applications, this assumption may not hold. For
              example, we sometimes have a classification task in one domain of
              interest, but we only have sufficient training data in another
              domain of interest, where the latter data may be in a different
              feature space or follow a different data distribution. In such
              cases, knowledge transfer, if done successfully, would greatly
              improve the performance of learning by avoiding much expensive
              data-labeling efforts. In recent years, transfer learning has
              emerged as a new learning framework to address this problem. This
              survey focuses on categorizing and reviewing the current progress
              on transfer learning for classification, regression, and
              clustering problems. In this survey, we discuss the relationship
              between transfer learning and other related machine learning
              techniques such as domain adaptation, multitask learning and
              sample selection bias, as well as covariate shift. We also
              explore some potential future issues in transfer learning
              research.",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  volume   =  22,
  number   =  10,
  pages    = "1345--1359",
  month    =  oct,
  year     =  2010,
  keywords = "Machine learning;Training data;Data mining;Knowledge
              transfer;Space technology;Knowledge engineering;Machine learning
              algorithms;Labeling;Learning systems;Testing;Transfer
              learning;survey;machine learning;data mining."
}
@article{Zhuang2019-rk,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}



@article{Zhu2020-jj,
  title={Transfer learning in deep reinforcement learning: A survey},
  author={Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu},
  journal={arXiv preprint arXiv:2009.07888},
  year={2020}
}

@techreport{fix1952discriminatory,
  title={Discriminatory analysis-nonparametric discrimination: Small sample performance},
  author={Fix, Evelyn and Hodges Jr, Joseph L},
  year={1952},
  institution={California Univ Berkeley}
}
@article{Silver2017-jq,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}


@article{chen2021improving,
  title={Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings},
  author={Chen, Lili and Lee, Kimin and Srinivas, Aravind and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2103.02886},
  year={2021}
}
@ARTICLE{Silver2017-cv,
  title     = "Mastering the game of Go without human knowledge",
  author    = "Silver, David and Schrittwieser, Julian and Simonyan, Karen and
               Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert,
               Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and
               Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre,
               Laurent and van den Driessche, George and Graepel, Thore and
               Hassabis, Demis",
  abstract  = "A long-standing goal of artificial intelligence is an algorithm
               that learns, tabula rasa, superhuman proficiency in challenging
               domains. Recently, AlphaGo became the first program to defeat a
               world champion in the game of Go. The tree search in AlphaGo
               evaluated positions and selected moves using deep neural
               networks. These neural networks were trained by supervised
               learning from human expert moves, and by reinforcement learning
               from self-play. Here we introduce an algorithm based solely on
               reinforcement learning, without human data, guidance or domain
               knowledge beyond game rules. AlphaGo becomes its own teacher: a
               neural network is trained to predict AlphaGo's own move
               selections and also the winner of AlphaGo's games. This neural
               network improves the strength of the tree search, resulting in
               higher quality move selection and stronger self-play in the next
               iteration. Starting tabula rasa, our new program AlphaGo Zero
               achieved superhuman performance, winning 100--0 against the
               previously published, champion-defeating AlphaGo. Starting from
               zero knowledge and without human data, AlphaGo Zero was able to
               teach itself to play Go and to develop novel strategies that
               provide new insights into the oldest of games. To beat world
               champions at the game of Go, the computer program AlphaGo has
               relied largely on supervised learning from millions of human
               expert moves. David Silver and colleagues have now produced a
               system called AlphaGo Zero, which is based purely on
               reinforcement learning and learns solely from self-play.
               Starting from random moves, it can reach superhuman level in
               just a couple of days of training and five million games of
               self-play, and can now beat all previous versions of AlphaGo.
               Because the machine independently discovers the same fundamental
               principles of the game that took humans millennia to
               conceptualize, the work suggests that such principles have some
               universal character, beyond human bias.",
  journal   = "{Nature}",
  publisher = "Nature Publishing Group",
  volume    =  550,
  number    =  7676,
  pages     = "354--359",
  month     =  oct,
  year      =  2017,
  language  = "en"
}



@inproceedings{Zhu2016-cg,
  title={Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  author={Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3357--3364},
  year={2017},
  organization={IEEE}
}

@article{Eysenbach2020-to,
  title={Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers},
  author={Eysenbach, Benjamin and Asawa, Swapnil and Chaudhari, Shreyas and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2006.13916},
  year={2020}
}
@article{Parisotto2015-af,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{DBLP:journals/corr/abs-2004-11362,
  author    = {Prannay Khosla and
               Piotr Teterwak and
               Chen Wang and
               Aaron Sarna and
               Yonglong Tian and
               Phillip Isola and
               Aaron Maschinot and
               Ce Liu and
               Dilip Krishnan},
  title     = {Supervised Contrastive Learning},
  journal   = {CoRR},
  volume    = {abs/2004.11362},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.11362},
  archivePrefix = {arXiv},
  eprint    = {2004.11362},
  timestamp = {Sat, 23 Jan 2021 01:11:03 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-11362.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{Henaff2020-nj,
  title     = "{Data-Efficient} Image Recognition with Contrastive Predictive
               Coding",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  author    = "Henaff, Olivier",
  editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
  abstract  = "Human observers can learn to recognize new categories of images
               from a handful of examples, yet doing so with artificial ones
               remains an open challenge. We hypothesize that data-efficient
               recognition is enabled by representations which make the
               variability in natural signals more predictable. We therefore
               revisit and improve Contrastive Predictive Coding, an
               unsupervised objective for learning such representations. This
               new implementation produces features which support
               state-of-the-art linear classification accuracy on the ImageNet
               dataset. When used as input for non-linear classification with
               deep neural networks, this representation allows us to use 2-5x
               less labels than classifiers trained directly on image pixels.
               Finally, this unsupervised representation substantially improves
               transfer learning to object detection on the PASCAL VOC dataset,
               surpassing fully supervised pre-trained ImageNet classifiers.",
  publisher = "PMLR",
  volume    =  119,
  pages     = "4182--4192",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020
}

@article{Rusu2015-bu,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}
@article{Hinton2015-cw,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{Teh2017-gj,
  title={Distral: Robust multitask reinforcement learning},
  author={Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1707.04175},
  year={2017}
}



@INCOLLECTION{Schaal2004-ud,
  title     = "Estimating Future Reward in Reinforcement Learning Animats using
               Associative Learning",
  booktitle = "From animals to animats 8: Proceedings of the Eighth
               International Conference on the Simulation of Adaptive Behavior",
  author    = "Schaal, Stefan and Ijspeert, Auke Jan and Billard, Aude and
               Vijayakumar, Sethu and Meyer, Jean-Arcady",
  abstract  = "We introduce a model of animat reinforcement learning where an
               associative learning element is placed between the animat's
               reinforcement learning component and its internal reinforcement
               functions. This element forms an impression of the sensory
               stimuli present near a goal and uses it to make an initial
               estimate of the value of newly discovered state-action pairs in
               tasks where reward is necessarily delayed. We then describe the
               implementation of Peaches 'n Cream, a simulated robot that
               implements one version of the model where behaviour-based
               reinforcement learning is used in a puck foraging task. The
               results suggest that once given an initial simple task to learn
               from, the resulting associations significantly speed up learning
               in a later, more complex task, and provide further evidence that
               other learning methods may be used in conjunction with
               reinforcement learning to make it feasible for situated agents.",
  publisher = "MIT Press",
  pages     = "297--304",
  year      =  2004
}

@article{Yosinski2014-ze,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={arXiv preprint arXiv:1411.1792},
  year={2014}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fernandez2006-tf,
  title     = "Reusing and Building a Policy Library",
  author    = "Fern{\'a}ndez, F and Veloso, M M",
  abstract  = "Policy Reuse is a method to improve reinforcement learning with
               the ability to solve multiple tasks by building upon past
               problem solving experience, as accumulated in a Policy Library .
               Given a new task, a Policy Reuse learner uses the past policies
               in the library as a …",
  journal   = "ICAPS",
  publisher = "aaai.org",
  year      =  2006
}

@ARTICLE{Carroll2002-bh,
  title    = "Fixed vs. Dynamic {Sub-Transfer} in Reinforcement Learning",
  author   = "Carroll, J L and Peterson, Todd S",
  abstract = "We survey various task transfer methods in Qlearning and present
              a variation on fixed sub-transfer which we call dynamic
              sub-transfer. We discuss the benefits and drawbacks of dynamic
              sub-transfer as compared with the other transfer methods, and we
              describe qualitatively the situations where this method would be
              preferred over the fixed version of sub-transfer. We test this
              method against several other transfer methods in a simple three
              room grid world where portions of the source's policy are
              relevant to the target task and other portions are not. In this
              situation we found that dynamic sub-transfer converged to the
              optimal solution, avoiding the suboptimality inherent in fixed
              sub-transfer, while also avoiding some of the convergence
              problems often experienced by fixed sub-transfer.",
  journal  = "ICMLA",
  year     =  2002
}


@INPROCEEDINGS{Hasselt2010-oj,
  title     = "{Double Q-learning}",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Hasselt, Hado",
  editor    = "Lafferty, J and Williams, C and Shawe-Taylor, J and Zemel, R and
               Culotta, A",
  publisher = "Curran Associates, Inc.",
  volume    =  23,
  year      =  2010
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wiering2005-xn,
  title     = "{QV} (lambda)-learning: A new on-policy reinforcement learning
               algrithm",
  author    = "Wiering, M A",
  abstract  = "Reinforcement learning algorithms (Sutton \& Barto, 1998;
               Kaelbling et al., 1996) are very suitable for learning to
               control an agent by letting it interact with an environment.
               Currently, there are three well-known value function based
               reinforcement learning (RL) algorithms that …",
  journal   = "Proceedings of the 7th european workshop on",
  publisher = "dspace.library.uu.nl",
  year      =  2005
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

@article{Wiering1997-fo,
  title="{HQ-learning}",
  author={Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal={Adaptive Behavior},
  volume={6},
  number={2},
  pages={219--246},
  year={1997},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

@article{Wiering1998-uk,
  title="{Fast online Q ($\lambda$)}",
  author={Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal={Machine Learning},
  volume={33},
  number={1},
  pages={105--115},
  year={1998},
  publisher={Springer}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@inproceedings{sabatelli2018deep,
  title={Deep transfer learning for art classification problems},
  author={Sabatelli, Matthia and Kestemont, Mike and Daelemans, Walter and Geurts, Pierre},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wiering2014-ba,
  title     = "Model-based multi-objective reinforcement learning",
  author    = "Wiering, M A and Withagen, M and {others}",
  abstract  = "This paper describes a novel multi-objective reinforcement
               learning algorithm. The proposed algorithm first learns a model
               of the multi-objective sequential decision making problem, after
               which this learned model is used by a multi-objective dynamic
               programming …",
  journal   = "2014 IEEE Symposium on",
  publisher = "ieeexplore.ieee.org",
  year      =  2014
}

@article{Rusu2016-br,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}



@article{zhao2021consciousnessinspired,
  title={A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning},
  author={Zhao, Mingde and Liu, Zhen and Luan, Sitao and Zhang, Shuyuan and Precup, Doina and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2106.02097},
  year={2021}
}
% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@article{Zhang2018-eo,
  title={Decoupling dynamics and reward for transfer learning},
  author={Zhang, Amy and Satija, Harsh and Pineau, Joelle},
  journal={arXiv preprint arXiv:1804.10689},
  year={2018}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.


@article{Wiering_undated-hb,
  title={Reinforcement learning},
  author={Wiering, Marco A and Van Otterlo, Martijn},
  journal={Adaptation, learning, and optimization},
  volume={12},
  number={3},
  year={2012},
  publisher={Springer}
}
% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_Hasselt2007-gf,
  title     = "Reinforcement learning in continuous action spaces",
  author    = "Van Hasselt, H and Wiering, M A",
  abstract  = "Quite some research has been done on reinforcement learning in
               continuous environments, but the research on problems where the
               actions can also be chosen from a continuous space is much more
               limited. We present a new class of algorithms named continuous
               actor …",
  journal   = "2007 IEEE International",
  publisher = "ieeexplore.ieee.org",
  year      =  2007
}



@article{janner2019trust,
  title={\href{https://papers.nips.cc/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html}{When to trust your model: Model-based policy optimization}},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@INPROCEEDINGS{Wiering1998-hm,
    author = {Marco Wiering and Jürgen Schmidhuber},
    title = {Efficient Model-Based Exploration},
    booktitle = {PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON SIMULATION OF ADAPTIVE BEHAVIOR: FROM ANIMALS TO ANIMATS 6},
    year = {1998},
    pages = {223--228},
    publisher = {MIT Press/Bradford Books}
}

@inproceedings{osband2020bsuite,
    title={\href{https://openreview.net/forum?id=rygf-kSYwH}{Behaviour Suite for Reinforcement Learning}},
    author={Osband, Ian and
            Doron, Yotam and
            Hessel, Matteo and
            Aslanides, John and
            Sezener, Eren and
            Saraiva, Andre and
            McKinney, Katrina and
            Lattimore, Tor and
            {Sz}epesv{\'a}ri, Csaba and
            Singh, Satinder and
            Van Roy, Benjamin and
            Sutton, Richard and
            Silver, David and
            van Hasselt, Hado},
    booktitle={International Conference on Learning Representations},
    year={2020}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sabatelli2018-im,
  title     = "Deep quality-value ({DQV}) learning",
  author    = "Sabatelli, M and Louppe, G and Geurts, P and {others}",
  abstract  = "We introduce a novel Deep Reinforcement Learning (DRL) algorithm
               called Deep Quality- Value (DQV) Learning. DQV uses
               temporal-difference learning to train a Value neural network and
               uses this network for training a second Quality-value network
               that learns to …",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  year      =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sabatelli2018-lb,
  title     = "Deep transfer learning for art classification problems",
  author    = "Sabatelli, M and Kestemont, M and {others}",
  abstract  = "In this paper we investigate whether Deep Convolutional Neural
               Networks (DCNNs), which have obtained state of the art results
               on the ImageNet challenge, are able to perform equally well on
               three different art classification problems. In particular, we
               assess whether it is …",
  journal   = "Proceedings of the",
  publisher = "openaccess.thecvf.com",
  year      =  2018
}



@mastersthesis{Sasso2021,
    author     =     {Remo Sasso},
    title     =     {{Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning}},
    institution = "{Department of Artificial Intelligence}",
    school     =     {University of Groningen},
    address     =     {the Netherlands},
    year     =     {2021},
    }


@book{rnnref,
title = "Recurrent neural networks: design and applications",
author = "L.R Medsker and L.C Jain",
year = "1999",
language = "Dutch",
publisher = "CRC Press",
}

@inproceedings{rezende2014stochastic,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
year = {2014},
publisher = {JMLR.org},
booktitle = {Proceedings of the 31st International Conference on Machine Learning - Volume 32},
pages = {II–1278–II–1286},
location = {Beijing, China},
series = {ICML'14}
}
@article{munos2016safe,
  title={\href{https://papers.nips.cc/paper/2016/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html}{Safe and efficient off-policy reinforcement learning}},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}
@misc{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{kingma2014adam,
  title={\href{http://arxiv.org/abs/1412.6980}{Adam: A Method for Stochastic Optimization}},
  author={Kingma, Diederik P and Ba, Jimmy},
    booktitle={International Conference on Learning Representations},
  year={2015}
}

@InProceedings{tirinzoni2018importance,
  title = 	 {Importance Weighted Transfer of Samples in Reinforcement Learning},
  author =       {Tirinzoni, Andrea and Sessa, Andrea and Pirotta, Matteo and Restelli, Marcello},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4936--4945},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR}
}


@inproceedings{sampletransferlazaric,
author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
title = {Transfer of Samples in Batch Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1390156.1390225},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {544–551},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{mlp-universal,
title = "Multilayer feedforward networks are universal approximators",
journal = "Neural Networks",
volume = "2",
number = "5",
pages = "359 - 366",
year = "1989",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(89)90020-8",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
note={\newline DOI: 10.1016/0893-6080(89)90020-8}
}

@ARTICLE{rbf-universal,
author={J. {Park} and I. W. {Sandberg}},
journal={Neural Computation},
title={Universal {A}pproximation {U}sing {R}adial-{B}asis-{F}unction {N}etworks},
year={1991},
volume={3},
number={2},
pages={246-257},
keywords={},
doi={10.1162/neco.1991.3.2.246},
ISSN={0899-7667},
note={DOI: 10.1162/neco.1991.3.2.246}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bellman1958-ru,
  title    = "Dynamic programming and stochastic control processes",
  author   = "Bellman, Richard",
  abstract = "Consider a system S specified at any time t by a finite
              dimensional vector x(t) satisfying a vector differential equation
              dx/dt = g[x, r(t), f(t)], x(0) = c, where c is the initial state,
              r(t) is a random forcing term possessing a known distribution,
              and f(t) is a forcing term chosen, via a feedback process, so as
              to minimize the expected value of a functional J(x) = ƒ0T h(x −
              y, t) dG(t), where y(t) is a known function, or chosen so as to
              minimize the functional defined by the probability that max0 ≦ t
              ≦ T h(x − y, t) exceed a specified bound. It is shown how the
              functional equation technique of dynamic programming may be used
              to obtain a new computational and analytic approach to problems
              of this genre. The limited memory capacity of present-day digital
              computers limits the routine application of these techniques to
              first and second order systems at the moment, with limited
              application to higher order systems.",
  journal  = "Information and Control",
  volume   =  1,
  number   =  3,
  pages    = "228--239",
  month    =  sep,
  year     =  1958
}


@ARTICLE{Watkins1992-yl,
  title    = "Q-learning",
  author   = "Watkins, Christopher J C H and Dayan, Peter",
  abstract = "Q-learning (Watkins, 1989) is a simple way for agents to learn
              how to act optimally in controlled Markovian domains. It amounts
              to an incremental method for dynamic programming which imposes
              limited computational demands. It works by successively improving
              its evaluations of the quality of particular actions at
              particular states.",
  journal  = "Machine Learning",
  volume   =  8,
  number   =  3,
  pages    = "279--292",
  month    =  may,
  year     =  1992
}




@inproceedings{aggarwal2001surprising,
  title={On the surprising behavior of distance metrics in high dimensional space},
  author={Aggarwal, Charu C and Hinneburg, Alexander and Keim, Daniel A},
  booktitle={International conference on database theory},
  pages={420--434},
  year={2001},
  organization={Springer}
}

@article{Landolfi2019-mo,
  title={A Model-based Approach for Sample-efficient Multi-task Reinforcement Learning},
  author={Landolfi, Nicholas C and Thomas, Garrett and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.04964},
  year={2019}
}



@article{DBLP:journals/corr/abs-2006-16712,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Jonker, Catholijn M},
  journal={arXiv preprint arXiv:2006.16712},
  year={2020}
}

@article{campbell2002deep,
  title={Deep blue},
  author={Campbell, Murray and Hoane Jr, A Joseph and Hsu, Feng-hsiung},
  journal={Artificial intelligence},
  volume={134},
  number={1-2},
  pages={57--83},
  year={2002},
  publisher={Elsevier}
}


@article{DBLP:journals/corr/abs-2102-05207,
  author    = {Zhangjie Cao and
               Minae Kwon and
               Dorsa Sadigh},
  title     = {Transfer Reinforcement Learning across Homotopy Classes},
  journal   = {CoRR},
  volume    = {abs/2102.05207},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.05207},
  archivePrefix = {arXiv},
  eprint    = {2102.05207},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-05207.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Sekar2020-fl,
  title={Planning to explore via self-supervised world models},
  author={Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  booktitle={International Conference on Machine Learning},
  pages={8583--8592},
  year={2020},
  organization={PMLR}
}

@INPROCEEDINGS{Chua2018-lk,
  title     = "Deep Reinforcement Learning in a Handful of Trials using
               Probabilistic Dynamics Models",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and
               Levine, Sergey",
  editor    = "Bengio, S and Wallach, H and Larochelle, H and Grauman, K and
               Cesa-Bianchi, N and Garnett, R",
  publisher = "Curran Associates, Inc.",
  volume    =  31,
  year      =  2018
}

@article{Taiga2019-mv,
  title={Benchmarking bonus-based exploration methods on the arcade learning environment},
  author={Ta{\"\i}ga, Adrien Ali and Fedus, William and Machado, Marlos C and Courville, Aaron and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1908.02388},
  year={2019}
}
@article{Stadie2015-gl,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}



@BOOK{Sutton2018-yv,
  title     = "Reinforcement Learning: An Introduction",
  author    = "Sutton, Richard S and Barto, Andrew G",
  abstract  = "The significantly expanded and updated new edition of a widely
               used text on reinforcement learning, one of the most active
               research areas in artificial intelligence. Reinforcement
               learning, one of the most active research areas in artificial
               intelligence, is a computational approach to learning whereby an
               agent tries to maximize the total amount of reward it receives
               while interacting with a complex, uncertain environment. In
               Reinforcement Learning, Richard Sutton and Andrew Barto provide
               a clear and simple account of the field's key ideas and
               algorithms. This second edition has been significantly expanded
               and updated, presenting new topics and updating coverage of
               other topics. Like the first edition, this second edition
               focuses on core online learning algorithms, with the more
               mathematical material set off in shaded boxes. Part I covers as
               much of reinforcement learning as possible without going beyond
               the tabular case for which exact solutions can be found. Many
               algorithms presented in this part are new to the second edition,
               including UCB, Expected Sarsa, and Double Learning. Part II
               extends these ideas to function approximation, with new sections
               on such topics as artificial neural networks and the Fourier
               basis, and offers expanded treatment of off-policy learning and
               policy-gradient methods. Part III has new chapters on
               reinforcement learning's relationships to psychology and
               neuroscience, as well as an updated case-studies chapter
               including AlphaGo and AlphaGo Zero, Atari game playing, and IBM
               Watson's wagering strategy. The final chapter discusses the
               future societal impacts of reinforcement learning.",
  publisher = "A Bradford Book",
  year      =  2018,
  address   = "Cambridge, MA, USA"
}

@BOOK{Camacho1999-vb,
  title     = "Model Predictive Control",
  author    = "Camacho, Eduardo F and Bordons, Carlos",
  publisher = "Springer, London",
  year      =  1999
}


@INPROCEEDINGS{Coulom2007-wd,
  title     = "Efficient Selectivity and Backup Operators in {Monte-Carlo} Tree
               Search",
  booktitle = "Computers and Games",
  author    = "Coulom, R{\'e}mi",
  abstract  = "A Monte-Carlo evaluation consists in estimating a position by
               averaging the outcome of several random continuations. The
               method can serve as an evaluation function at the leaves of a
               min-max tree. This paper presents a new framework to combine
               tree search with Monte-Carlo evaluation, that does not separate
               between a min-max phase and a Monte-Carlo phase. Instead of
               backing-up the min-max value close to the root, and the average
               value at some depth, a more general backup operator is defined
               that progressively changes from averaging to min-max as the
               number of simulations grows. This approach provides a
               fine-grained control of the tree growth, at the level of
               individual simulations, and allows efficient selectivity. The
               resulting algorithm was implemented in a 9$\times$9 Go-playing
               program, Crazy Stone, that won the 10th KGS computer-Go
               tournament.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "72--83",
  year      =  2007
}
@article{Henaff2017-mw,
  title={Model-based planning with discrete and continuous actions},
  author={Henaff, Mikael and Whitney, William F and LeCun, Yann},
  journal={arXiv preprint arXiv:1705.07177},
  year={2017}
}



@ARTICLE{Rosenblatt1958-lg,
  title     = "The perceptron: a probabilistic model for information storage
               and organization in the brain",
  author    = "Rosenblatt, F",
  abstract  = "The first of these questions is in the province of sensory
               physiology, and is the only one for which appreciable
               understanding has been achieved. This article will be concerned
               primarily with the second and third questions, which are still
               subject to a vast amount of speculation, and where the few
               relevant facts currently supplied by neurophysiology have not
               yet been integrated into an acceptable theory. With regard to
               the second question, two alternative positions have been
               maintained. The first suggests that storage of sensory
               information is in the form of coded representations or images,
               with some sort of one-to-one mapping between the sensory
               stimulus",
  journal   = "Psychol. Rev.",
  publisher = "American Psychological Association (APA)",
  volume    =  65,
  number    =  6,
  pages     = "386--408",
  month     =  nov,
  year      =  1958,
  keywords  = "PERCEPTION",
  language  = "en"
}






@ARTICLE{Silver2016-df,
  title     = "Mastering the game of Go with deep neural networks and tree
               search",
  author    = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
               Arthur and Sifre, Laurent and van den Driessche, George and
               Schrittwieser, Julian and Antonoglou, Ioannis and
               Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and
               Grewe, Dominik and Nham, John and Kalchbrenner, Nal and
               Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and
               Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis",
  abstract  = "The game of Go has long been viewed as the most challenging of
               classic games for artificial intelligence owing to its enormous
               search space and the difficulty of evaluating board positions
               and moves. Here we introduce a new approach to computer Go that
               uses `value networks' to evaluate board positions and `policy
               networks' to select moves. These deep neural networks are
               trained by a novel combination of supervised learning from human
               expert games, and reinforcement learning from games of
               self-play. Without any lookahead search, the neural networks
               play Go at the level of state-of-the-art Monte Carlo tree search
               programs that simulate thousands of random games of self-play.
               We also introduce a new search algorithm that combines Monte
               Carlo simulation with value and policy networks. Using this
               search algorithm, our program AlphaGo achieved a 99.8\% winning
               rate against other Go programs, and defeated the human European
               Go champion by 5 games to 0. This is the first time that a
               computer program has defeated a human professional player in the
               full-sized game of Go, a feat previously thought to be at least
               a decade away. A computer Go program based on deep neural
               networks defeats a human professional player to achieve one of
               the grand challenges of artificial intelligence. The victory in
               1997 of the chess-playing computer Deep Blue in a six-game
               series against the then world champion Gary Kasparov was seen as
               a significant milestone in the development of artificial
               intelligence. An even greater challenge remained --- the ancient
               game of Go. Despite decades of refinement, until recently the
               strongest computers were still playing Go at the level of human
               amateurs. Enter AlphaGo. Developed by Google DeepMind, this
               program uses deep neural networks to mimic expert players, and
               further improves its performance by learning from games played
               against itself. AlphaGo has achieved a 99\% win rate against the
               strongest other Go programs, and defeated the reigning European
               champion Fan Hui 5--0 in a tournament match. This is the first
               time that a computer program has defeated a human professional
               player in even games, on a full, 19 x 19 board, in even games
               with no handicap.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  529,
  number    =  7587,
  pages     = "484--489",
  month     =  jan,
  year      =  2016,
  language  = "en"
}



@INCOLLECTION{Tesauro1995-pe,
  title     = "{TD-Gammon}: A {Self-Teaching} Backgammon Program",
  booktitle = "Applications of Neural Networks",
  author    = "Tesauro, Gerald",
  editor    = "Murray, Alan F",
  abstract  = "This chapter describes TD-Gammon, a neural network that is able
               to teach itself to play backgammon solely by playing against
               itself and learning from the results. TD-Gammon uses a recently
               proposed reinforcement learning algorithm called TD($\lambda$)
               (Sutton, 1988), and is apparently the first application of this
               algorithm to a complex nontrivial task. Despite starting from
               random initial weights (and hence random initial strategy),
               TD-Gammon achieves a surprisingly strong level of play. With
               zero knowledge built in at the start of learning (i.e. given
               only a ``raw'' description of the board state), the network
               learns to play the entire game at a strong intermediate level
               that surpasses not only conventional commercial programs, but
               also comparable networks trained via supervised learning on a
               large corpus of human expert games. The hidden units in the
               network have apparently discovered useful features, a
               longstanding goal of computer games research.",
  publisher = "Springer US",
  pages     = "267--285",
  year      =  1995,
  address   = "Boston, MA"
}


@ARTICLE{Vinyals2019-sn,
  title     = "Grandmaster level in {StarCraft} {II} using multi-agent
               reinforcement learning",
  author    = "Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M
               and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung
               and Choi, David H and Powell, Richard and Ewalds, Timo and
               Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss,
               Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and
               Cai, Trevor and Agapiou, John P and Jaderberg, Max and
               Vezhnevets, Alexander S and Leblond, R{\'e}mi and Pohlen, Tobias
               and Dalibard, Valentin and Budden, David and Sulsky, Yury and
               Molloy, James and Paine, Tom L and Gulcehre, Caglar and Wang,
               Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and
               Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and
               Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and
               Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and
               Silver, David",
  abstract  = "Many real-world applications require artificial agents to
               compete and coordinate with other agents in complex
               environments. As a stepping stone to this goal, the domain of
               StarCraft has emerged as an important challenge for artificial
               intelligence research, owing to its iconic and enduring status
               among the most difficult professional esports and its relevance
               to the real world in terms of its raw complexity and multi-agent
               challenges. Over the course of a decade and numerous
               competitions1--3, the strongest agents have simplified important
               aspects of the game, utilized superhuman capabilities, or
               employed hand-crafted sub-systems4. Despite these advantages, no
               previous agent has come close to matching the overall skill of
               top StarCraft players. We chose to address the challenge of
               StarCraft using general-purpose learning methods that are in
               principle applicable to other complex domains: a multi-agent
               reinforcement learning algorithm that uses data from both human
               and agent games within a diverse league of continually adapting
               strategies and counter-strategies, each represented by deep
               neural networks5,6. We evaluated our agent, AlphaStar, in the
               full game of StarCraft II, through a series of online games
               against human players. AlphaStar was rated at Grandmaster level
               for all three StarCraft races and above 99.8\% of officially
               ranked human players. AlphaStar uses a multi-agent reinforcement
               learning algorithm and has reached Grandmaster level, ranking
               among the top 0.2\% of human players for the real-time strategy
               game StarCraft II.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  575,
  number    =  7782,
  pages     = "350--354",
  month     =  oct,
  year      =  2019,
  language  = "en"
}


@article{OpenAI2019-ij,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}


@misc{mlpblog,
  title = {A Simple overview of Multilayer Perceptron},
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2020/12/mlp-multilayer-perceptron-simple-overview/}},
  note = {Accessed: 2019-12-13}
  
}


@misc{aeblog,
  title = {All about autoencoders},
  howpublished = {\url{https://pythonmachinelearning.pro/all-about-autoencoders/}},
  note = {Accessed: 2017-10-30}
  
}

@article{silva2020novel,
  title={A Novel Approach to Condition Monitoring of the Cutting Process Using Recurrent Neural Networks},
  author={Silva, Rui and Ara{\'u}jo, Ant{\'o}nio},
  journal={Sensors},
  volume={20},
  number={16},
  pages={4493},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{phung2019high,
  title={A high-accuracy model average ensemble of convolutional neural networks for classification of cloud image patches on small datasets},
  author={Phung, Van Hiep and Rhee, Eun Joo and others},
  journal={Applied Sciences},
  volume={9},
  number={21},
  pages={4500},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{ballard1987modular,
  title={Modular learning in neural networks.},
  author={Ballard, Dana H},
  booktitle={AAAI},
  pages={279--284},
  year={1987}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{Gou2019-om,
  title="{DQN with model-based exploration: efficient learning on environments with sparse rewards}",
  author={Gou, Stephen Zhen and Liu, Yuyang},
  journal={arXiv preprint arXiv:1903.09295},
  year={2019}
}

@MISC{coumans2021,
author =   {Erwin Coumans and Yunfei Bai},
title =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
howpublished = {\url{http://pybullet.org}},
year = {2016--2021}
}


@article{Brockman2016-ri,
  title="{Openai gym}",
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{Raffin2020-ly,
  title={Generalized state-dependent exploration for deep reinforcement learning in robotics},
  author={Raffin, Antonin and Stulp, Freek},
  journal={arXiv preprint arXiv:2005.05719},
  year={2020}
}

@article{Deisenroth2015-jc,
  title={Gaussian processes for data-efficient learning in robotics and control},
  author={Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={2},
  pages={408--423},
  year={2013},
  publisher={IEEE}
}
@inproceedings{Finn2015-gt,
  title={Deep spatial autoencoders for visuomotor learning},
  author={Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={512--519},
  year={2016},
  organization={IEEE}
}
@misc{wang2019exploring,
      title={Exploring Model-based Planning with Policy Networks}, 
      author={Tingwu Wang and Jimmy Ba},
      year={2019},
      eprint={1906.08649},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2021planning,
      title={Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning}, 
      author={Xiyao Wang and Junge Zhang and Wenzhen Huang and Qiyue Yin},
      year={2021},
      eprint={2010.12914},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pathak2017curiositydriven,
      title={Curiosity-driven Exploration by Self-supervised Prediction}, 
      author={Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
      year={2017},
      eprint={1705.05363},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{haarnoja2018soft,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{janner2021trust,
      title={When to Trust Your Model: Model-Based Policy Optimization}, 
      author={Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
      year={2021},
      eprint={1906.08253},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{dyna,
author = {Sutton, Richard S.},
title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
year = {1991},
issue_date = {Aug. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {0163-5719},
url = {https://doi.org/10.1145/122344.122377},
doi = {10.1145/122344.122377},
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
journal = {SIGART Bull.},
month = {jul},
pages = {160–163},
numpages = {4}
}


@misc{ye2021mastering,
      title={Mastering Atari Games with Limited Data}, 
      author={Weirui Ye and Shaohuai Liu and Thanard Kurutach and Pieter Abbeel and Yang Gao},
      year={2021},
      eprint={2111.00210},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Watter2015-zk,
  title={Embed to control: A locally linear latent dynamics model for control from raw images},
  author={Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1506.07365},
  year={2015}
}

@article{Nagabandi2018NeuralND,
  title={Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning},
  author={Anusha Nagabandi and Gregory Kahn and Ronald S. Fearing and Sergey Levine},
  journal={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018},
  pages={7559-7566}
}

@article{Wahlstrom2014-ym,
  title={Learning deep dynamical models from image pixels},
  author={Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B and Deisenroth, Marc Peter},
  journal={IFAC-PapersOnLine},
  volume={48},
  number={28},
  pages={1059--1064},
  year={2015},
  publisher={Elsevier}
}




@ARTICLE{Pan2010-hg,
  title    = "A Survey on Transfer Learning",
  author   = "Pan, Sinno Jialin and Yang, Qiang",
  abstract = "A major assumption in many machine learning and data mining
              algorithms is that the training and future data must be in the
              same feature space and have the same distribution. However, in
              many real-world applications, this assumption may not hold. For
              example, we sometimes have a classification task in one domain of
              interest, but we only have sufficient training data in another
              domain of interest, where the latter data may be in a different
              feature space or follow a different data distribution. In such
              cases, knowledge transfer, if done successfully, would greatly
              improve the performance of learning by avoiding much expensive
              data-labeling efforts. In recent years, transfer learning has
              emerged as a new learning framework to address this problem. This
              survey focuses on categorizing and reviewing the current progress
              on transfer learning for classification, regression, and
              clustering problems. In this survey, we discuss the relationship
              between transfer learning and other related machine learning
              techniques such as domain adaptation, multitask learning and
              sample selection bias, as well as covariate shift. We also
              explore some potential future issues in transfer learning
              research.",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  volume   =  22,
  number   =  10,
  pages    = "1345--1359",
  month    =  oct,
  year     =  2010,
  keywords = "Machine learning;Training data;Data mining;Knowledge
              transfer;Space technology;Knowledge engineering;Machine learning
              algorithms;Labeling;Learning systems;Testing;Transfer
              learning;survey;machine learning;data mining."
}
@article{Zhuang2019-rk,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}



@article{Zhu2020-jj,
  title={Transfer learning in deep reinforcement learning: A survey},
  author={Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu},
  journal={arXiv preprint arXiv:2009.07888},
  year={2020}
}

@techreport{fix1952discriminatory,
  title={Discriminatory analysis-nonparametric discrimination: Small sample performance},
  author={Fix, Evelyn and Hodges Jr, Joseph L},
  year={1952},
  institution={California Univ Berkeley}
}
@article{Silver2017-jq,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}


@article{chen2021improving,
  title={Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings},
  author={Chen, Lili and Lee, Kimin and Srinivas, Aravind and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2103.02886},
  year={2021}
}
@ARTICLE{Silver2017-cv,
  title     = "Mastering the game of Go without human knowledge",
  author    = "Silver, David and Schrittwieser, Julian and Simonyan, Karen and
               Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert,
               Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and
               Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre,
               Laurent and van den Driessche, George and Graepel, Thore and
               Hassabis, Demis",
  abstract  = "A long-standing goal of artificial intelligence is an algorithm
               that learns, tabula rasa, superhuman proficiency in challenging
               domains. Recently, AlphaGo became the first program to defeat a
               world champion in the game of Go. The tree search in AlphaGo
               evaluated positions and selected moves using deep neural
               networks. These neural networks were trained by supervised
               learning from human expert moves, and by reinforcement learning
               from self-play. Here we introduce an algorithm based solely on
               reinforcement learning, without human data, guidance or domain
               knowledge beyond game rules. AlphaGo becomes its own teacher: a
               neural network is trained to predict AlphaGo's own move
               selections and also the winner of AlphaGo's games. This neural
               network improves the strength of the tree search, resulting in
               higher quality move selection and stronger self-play in the next
               iteration. Starting tabula rasa, our new program AlphaGo Zero
               achieved superhuman performance, winning 100--0 against the
               previously published, champion-defeating AlphaGo. Starting from
               zero knowledge and without human data, AlphaGo Zero was able to
               teach itself to play Go and to develop novel strategies that
               provide new insights into the oldest of games. To beat world
               champions at the game of Go, the computer program AlphaGo has
               relied largely on supervised learning from millions of human
               expert moves. David Silver and colleagues have now produced a
               system called AlphaGo Zero, which is based purely on
               reinforcement learning and learns solely from self-play.
               Starting from random moves, it can reach superhuman level in
               just a couple of days of training and five million games of
               self-play, and can now beat all previous versions of AlphaGo.
               Because the machine independently discovers the same fundamental
               principles of the game that took humans millennia to
               conceptualize, the work suggests that such principles have some
               universal character, beyond human bias.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  550,
  number    =  7676,
  pages     = "354--359",
  month     =  oct,
  year      =  2017,
  language  = "en"
}


@inproceedings{Zhu2016-cg,
  title={Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  author={Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3357--3364},
  year={2017},
  organization={IEEE}
}

@article{Eysenbach2020-to,
  title={Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers},
  author={Eysenbach, Benjamin and Asawa, Swapnil and Chaudhari, Shreyas and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2006.13916},
  year={2020}
}
@article{Parisotto2015-af,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{DBLP:journals/corr/abs-2004-11362,
  author    = {Prannay Khosla and
               Piotr Teterwak and
               Chen Wang and
               Aaron Sarna and
               Yonglong Tian and
               Phillip Isola and
               Aaron Maschinot and
               Ce Liu and
               Dilip Krishnan},
  title     = {Supervised Contrastive Learning},
  journal   = {CoRR},
  volume    = {abs/2004.11362},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.11362},
  archivePrefix = {arXiv},
  eprint    = {2004.11362},
  timestamp = {Sat, 23 Jan 2021 01:11:03 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-11362.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{Henaff2020-nj,
  title     = "{Data-Efficient} Image Recognition with Contrastive Predictive
               Coding",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  author    = "Henaff, Olivier",
  editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
  abstract  = "Human observers can learn to recognize new categories of images
               from a handful of examples, yet doing so with artificial ones
               remains an open challenge. We hypothesize that data-efficient
               recognition is enabled by representations which make the
               variability in natural signals more predictable. We therefore
               revisit and improve Contrastive Predictive Coding, an
               unsupervised objective for learning such representations. This
               new implementation produces features which support
               state-of-the-art linear classification accuracy on the ImageNet
               dataset. When used as input for non-linear classification with
               deep neural networks, this representation allows us to use 2-5x
               less labels than classifiers trained directly on image pixels.
               Finally, this unsupervised representation substantially improves
               transfer learning to object detection on the PASCAL VOC dataset,
               surpassing fully supervised pre-trained ImageNet classifiers.",
  publisher = "PMLR",
  volume    =  119,
  pages     = "4182--4192",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020
}

@article{Rusu2015-bu,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}
@article{Hinton2015-cw,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{Teh2017-gj,
  title={Distral: Robust multitask reinforcement learning},
  author={Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1707.04175},
  year={2017}
}



@INCOLLECTION{Schaal2004-ud,
  title     = "Estimating Future Reward in Reinforcement Learning Animats using
               Associative Learning",
  booktitle = "From animals to animats 8: Proceedings of the Eighth
               International Conference on the Simulation of Adaptive Behavior",
  author    = "Schaal, Stefan and Ijspeert, Auke Jan and Billard, Aude and
               Vijayakumar, Sethu and Meyer, Jean-Arcady",
  abstract  = "We introduce a model of animat reinforcement learning where an
               associative learning element is placed between the animat's
               reinforcement learning component and its internal reinforcement
               functions. This element forms an impression of the sensory
               stimuli present near a goal and uses it to make an initial
               estimate of the value of newly discovered state-action pairs in
               tasks where reward is necessarily delayed. We then describe the
               implementation of Peaches 'n Cream, a simulated robot that
               implements one version of the model where behaviour-based
               reinforcement learning is used in a puck foraging task. The
               results suggest that once given an initial simple task to learn
               from, the resulting associations significantly speed up learning
               in a later, more complex task, and provide further evidence that
               other learning methods may be used in conjunction with
               reinforcement learning to make it feasible for situated agents.",
  publisher = "MIT Press",
  pages     = "297--304",
  year      =  2004
}

@article{Yosinski2014-ze,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={arXiv preprint arXiv:1411.1792},
  year={2014}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fernandez2006-tf,
  title     = "Reusing and Building a Policy Library",
  author    = "Fern{\'a}ndez, F and Veloso, M M",
  abstract  = "Policy Reuse is a method to improve reinforcement learning with
               the ability to solve multiple tasks by building upon past
               problem solving experience, as accumulated in a Policy Library .
               Given a new task, a Policy Reuse learner uses the past policies
               in the library as a …",
  journal   = "ICAPS",
  publisher = "aaai.org",
  year      =  2006
}

@ARTICLE{Carroll2002-bh,
  title    = "Fixed vs. Dynamic {Sub-Transfer} in Reinforcement Learning",
  author   = "Carroll, J L and Peterson, Todd S",
  abstract = "We survey various task transfer methods in Qlearning and present
              a variation on fixed sub-transfer which we call dynamic
              sub-transfer. We discuss the benefits and drawbacks of dynamic
              sub-transfer as compared with the other transfer methods, and we
              describe qualitatively the situations where this method would be
              preferred over the fixed version of sub-transfer. We test this
              method against several other transfer methods in a simple three
              room grid world where portions of the source's policy are
              relevant to the target task and other portions are not. In this
              situation we found that dynamic sub-transfer converged to the
              optimal solution, avoiding the suboptimality inherent in fixed
              sub-transfer, while also avoiding some of the convergence
              problems often experienced by fixed sub-transfer.",
  journal  = "ICMLA",
  year     =  2002
}


@INPROCEEDINGS{Hasselt2010-oj,
  title     = "{Double Q-learning}",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Hasselt, Hado",
  editor    = "Lafferty, J and Williams, C and Shawe-Taylor, J and Zemel, R and
               Culotta, A",
  publisher = "Curran Associates, Inc.",
  volume    =  23,
  year      =  2010
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wiering2005-xn,
  title     = "{QV} (lambda)-learning: A new on-policy reinforcement learning
               algrithm",
  author    = "Wiering, M A",
  abstract  = "Reinforcement learning algorithms (Sutton \& Barto, 1998;
               Kaelbling et al., 1996) are very suitable for learning to
               control an agent by letting it interact with an environment.
               Currently, there are three well-known value function based
               reinforcement learning (RL) algorithms that …",
  journal   = "Proceedings of the 7th european workshop on",
  publisher = "dspace.library.uu.nl",
  year      =  2005
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

@article{Wiering1997-fo,
  title="{HQ-learning}",
  author={Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal={Adaptive Behavior},
  volume={6},
  number={2},
  pages={219--246},
  year={1997},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

@article{Wiering1998-uk,
  title="{Fast online Q ($\lambda$)}",
  author={Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal={Machine Learning},
  volume={33},
  number={1},
  pages={105--115},
  year={1998},
  publisher={Springer}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@inproceedings{sabatelli2018deep,
  title={Deep transfer learning for art classification problems},
  author={Sabatelli, Matthia and Kestemont, Mike and Daelemans, Walter and Geurts, Pierre},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wiering2014-ba,
  title     = "Model-based multi-objective reinforcement learning",
  author    = "Wiering, M A and Withagen, M and {others}",
  abstract  = "This paper describes a novel multi-objective reinforcement
               learning algorithm. The proposed algorithm first learns a model
               of the multi-objective sequential decision making problem, after
               which this learned model is used by a multi-objective dynamic
               programming …",
  journal   = "2014 IEEE Symposium on",
  publisher = "ieeexplore.ieee.org",
  year      =  2014
}

@article{Rusu2016-br,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}



@article{zhao2021consciousnessinspired,
  title={A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning},
  author={Zhao, Mingde and Liu, Zhen and Luan, Sitao and Zhang, Shuyuan and Precup, Doina and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2106.02097},
  year={2021}
}
% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@article{Zhang2018-eo,
  title={Decoupling dynamics and reward for transfer learning},
  author={Zhang, Amy and Satija, Harsh and Pineau, Joelle},
  journal={arXiv preprint arXiv:1804.10689},
  year={2018}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.


@article{Wiering_undated-hb,
  title={Reinforcement learning},
  author={Wiering, Marco A and Van Otterlo, Martijn},
  journal={Adaptation, learning, and optimization},
  volume={12},
  number={3},
  year={2012},
  publisher={Springer}
}
% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_Hasselt2007-gf,
  title     = "Reinforcement learning in continuous action spaces",
  author    = "Van Hasselt, H and Wiering, M A",
  abstract  = "Quite some research has been done on reinforcement learning in
               continuous environments, but the research on problems where the
               actions can also be chosen from a continuous space is much more
               limited. We present a new class of algorithms named continuous
               actor …",
  journal   = "2007 IEEE International",
  publisher = "ieeexplore.ieee.org",
  year      =  2007
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.


@INPROCEEDINGS{Wiering1998-hm,
    author = {Marco Wiering and Jürgen Schmidhuber},
    title = {Efficient Model-Based Exploration},
    booktitle = {PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON SIMULATION OF ADAPTIVE BEHAVIOR: FROM ANIMALS TO ANIMATS 6},
    year = {1998},
    pages = {223--228},
    publisher = {MIT Press/Bradford Books}
}
@inproceedings{badia2020up,
  title={Never Give Up: Learning Directed Exploration Strategies},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Martin and Pritzel, Alexander and Bolt, Andrew and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@misc{wang2021planning,
      title={Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning}, 
      author={Xiyao Wang and Junge Zhang and Wenzhen Huang and Qiyue Yin},
      year={2021},
      eprint={2010.12914},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oord2016conditional,
      title={Conditional Image Generation with PixelCNN Decoders}, 
      author={Aaron van den Oord and Nal Kalchbrenner and Oriol Vinyals and Lasse Espeholt and Alex Graves and Koray Kavukcuoglu},
      year={2016},
      eprint={1606.05328},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{sekar2020planning,
      title={Planning to Explore via Self-Supervised World Models}, 
      author={Ramanan Sekar and Oleh Rybkin and Kostas Daniilidis and Pieter Abbeel and Danijar Hafner and Deepak Pathak},
      year={2020},
      eprint={2005.05960},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{fan2021modelbased,
  title={\href{https://proceedings.mlr.press/v139/fan21b.html}{Model-based Reinforcement Learning for Continuous Control with Posterior Sampling}},
  author={Fan, Ying and Ming, Yifei},
  booktitle={International Conference on Machine Learning},
  year={2021},
  pages = 	 {3078--3087},
  organization={PMLR}
}


@INPROCEEDINGS{yaoensemble,
  author={Yao, Yao and Xiao, Li and An, Zhicheng and Zhang, Wanpeng and Luo, Dijun},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation}, 
  year={2021},
  volume={},
  number={},
  pages={4202-4208},
  doi={10.1109/ICRA48506.2021.9561842}}

@misc{choi2019contingencyaware,
      title={Contingency-Aware Exploration in Reinforcement Learning}, 
      author={Jongwook Choi and Yijie Guo and Marcin Moczulski and Junhyuk Oh and Neal Wu and Mohammad Norouzi and Honglak Lee},
      year={2019},
      eprint={1811.01483},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lin2018episodic,
      title={Episodic Memory Deep Q-Networks}, 
      author={Zichuan Lin and Tianqi Zhao and Guangwen Yang and Lintao Zhang},
      year={2018},
      eprint={1805.07603},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wayne2018unsupervised,
      title={Unsupervised Predictive Memory in a Goal-Directed Agent}, 
      author={Greg Wayne and Chia-Chun Hung and David Amos and Mehdi Mirza and Arun Ahuja and Agnieszka Grabska-Barwinska and Jack Rae and Piotr Mirowski and Joel Z. Leibo and Adam Santoro and Mevlana Gemici and Malcolm Reynolds and Tim Harley and Josh Abramson and Shakir Mohamed and Danilo Rezende and David Saxton and Adam Cain and Chloe Hillier and David Silver and Koray Kavukcuoglu and Matt Botvinick and Demis Hassabis and Timothy Lillicrap},
      year={2018},
      eprint={1803.10760},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{parisotto2017neural,
      title={Neural Map: Structured Memory for Deep Reinforcement Learning}, 
      author={Emilio Parisotto and Ruslan Salakhutdinov},
      year={2017},
      eprint={1702.08360},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{osband2019deep,
  title={\href{https://jmlr.org/papers/v20/18-339.html}{Deep Exploration via Randomized Value Functions}},
  author={Osband, Ian and Van Roy, Benjamin and Russo, Daniel J and Wen, Zheng},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages   = {1--62},
  number  = {124},
  year={2019}
}


@article{roboticmpem2020,
   title={Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces},
   volume={29},
   ISSN={1741-2633},
   url={http://dx.doi.org/10.1177/1059712320922916},
   DOI={10.1177/1059712320922916},
   number={6},
   journal={Adaptive Behavior},
   publisher={SAGE Publications},
   author={Schillaci, Guido and Pico Villalpando, Antonio and Hafner, Verena V and Hanappe, Peter and Colliaux, David and Wintz, Timothée},
   year={2020},
   month={Jun},
   pages={549–566}
}


@INPROCEEDINGS{8503252,
  author={Azizzadenesheli, Kamyar and Brunskill, Emma and Anandkumar, Animashree},
  booktitle={2018 Information Theory and Applications Workshop (ITA)}, 
  title={\href{https://ieeexplore.ieee.org/document/8503252}{Efficient Exploration Through Bayesian Deep Q-Networks}}, 
  year={2018},
  pages={1-9},
  }




@inproceedings{osband2016generalization,
  title={\href{http://proceedings.mlr.press/v48/osband16.html}{Generalization and exploration via randomized value functions}},
  author={Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
  booktitle={International Conference on Machine Learning},
  pages={2377--2386},
  year={2016},
  organization={PMLR}
}



@article{osband2018randomized,
  title={\href{https://papers.nips.cc/paper/2018/hash/5a7b238ba0f6502e5d6be14424b20ded-Abstract.html}{Randomized prior functions for deep reinforcement learning}},
  author={Osband, Ian and Aslanides, John and Cassirer, Albin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@misc{machado2019countbased,
      title={Count-Based Exploration with the Successor Representation}, 
      author={Marlos C. Machado and Marc G. Bellemare and Michael Bowling},
      year={2019},
      eprint={1807.11622},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{strens2000bayesian,
  title={A Bayesian framework for reinforcement learning},
  author={Strens, Malcolm},
  booktitle={ICML},
  volume={2000},
  pages={943--950},
  year={2000}
}


@inproceedings{
fortunato2018noisy,
title={Noisy Networks For Exploration},
author={Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Matteo Hessel and Ian Osband and Alex Graves and Volodymyr Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rywHCPkAW},
}

@misc{choshen2018dora,
      title={DORA The Explorer: Directed Outreaching Reinforcement Action-Selection}, 
      author={Leshem Choshen and Lior Fox and Yonatan Loewenstein},
      year={2018},
      eprint={1804.04012},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{curi2020efficient,
  title={\href{https://papers.nips.cc/paper/2020/hash/a36b598abb934e4528412e5a2127b931-Abstract.html}{Efficient model-based reinforcement learning through optimistic policy search and planning}},
  author={Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@inproceedings{badia2019never,
  title={\href{https://openreview.net/forum?id=Sye57xStvB}{Never Give Up: Learning Directed Exploration Strategies}},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Martin and Pritzel, Alexander and Bolt, Andrew and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@inproceedings{savinov2018episodic,
  title={\href{https://openreview.net/forum?id=SkeK3s0qKQ}{Episodic Curiosity through Reachability}},
  author={Savinov, Nikolay and Raichuk, Anton and Vincent, Damien and Marinier, Raphael and Pollefeys, Marc and Lillicrap, Timothy and Gelly, Sylvain},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@misc{fu2017ex2,
      title={EX2: Exploration with Exemplar Models for Deep Reinforcement Learning}, 
      author={Justin Fu and John D. Co-Reyes and Sergey Levine},
      year={2017},
      eprint={1703.01260},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ostrovski2017countbased,
      title={Count-Based Exploration with Neural Density Models}, 
      author={Georg Ostrovski and Marc G. Bellemare and Aaron van den Oord and Remi Munos},
      year={2017},
      eprint={1703.01310},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{STREHL20081309,
title = {An analysis of model-based Interval Estimation for Markov Decision Processes},
journal = {Journal of Computer and System Sciences},
volume = {74},
number = {8},
pages = {1309-1331},
year = {2008},
note = {Learning Theory 2005},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2007.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0022000008000767},
author = {Alexander L. Strehl and Michael L. Littman},
keywords = {Reinforcement learning, Learning theory, Markov Decision Processes},
abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less “online” cousins from the literature.}
}

@misc{stadie2015incentivizing,
      title={Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}, 
      author={Bradly C. Stadie and Sergey Levine and Pieter Abbeel},
      year={2015},
      eprint={1507.00814},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{savinov2019episodic,
      title={Episodic Curiosity through Reachability}, 
      author={Nikolay Savinov and Anton Raichuk and Raphaël Marinier and Damien Vincent and Marc Pollefeys and Timothy Lillicrap and Sylvain Gelly},
      year={2019},
      eprint={1810.02274},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ebert2018visual,
      title={Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control}, 
      author={Frederik Ebert and Chelsea Finn and Sudeep Dasari and Annie Xie and Alex Lee and Sergey Levine},
      year={2018},
      eprint={1812.00568},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{nair2020goalaware,
      title={Goal-Aware Prediction: Learning to Model What Matters}, 
      author={Suraj Nair and Silvio Savarese and Chelsea Finn},
      year={2020},
      eprint={2007.07170},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pathak2018zeroshot,
      title={Zero-Shot Visual Imitation}, 
      author={Deepak Pathak and Parsa Mahmoudieh and Guanghao Luo and Pulkit Agrawal and Dian Chen and Yide Shentu and Evan Shelhamer and Jitendra Malik and Alexei A. Efros and Trevor Darrell},
      year={2018},
      eprint={1804.08606},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{amos2018learning,
      title={Learning Awareness Models}, 
      author={Brandon Amos and Laurent Dinh and Serkan Cabi and Thomas Rothörl and Sergio Gómez Colmenarejo and Alistair Muldal and Tom Erez and Yuval Tassa and Nando de Freitas and Misha Denil},
      year={2018},
      eprint={1804.06318},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{sharma2020dynamicsaware,
      title={Dynamics-Aware Unsupervised Discovery of Skills}, 
      author={Archit Sharma and Shixiang Gu and Sergey Levine and Vikash Kumar and Karol Hausman},
      year={2020},
      eprint={1907.01657},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{henaff2019explicit,
      title={Explicit Explore-Exploit Algorithms in Continuous State Spaces}, 
      author={Mikael Henaff},
      year={2019},
      eprint={1911.00617},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{salimans2017evolution,
      title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning}, 
      author={Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
      year={2017},
      eprint={1703.03864},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{ball2020ready,
      title={Ready Policy One: World Building Through Active Learning}, 
      author={Philip Ball and Jack Parker-Holder and Aldo Pacchiano and Krzysztof Choromanski and Stephen Roberts},
      year={2020},
      eprint={2002.02693},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{Wiering98efficientmodel-based,
    author = {Marco Wiering and Jürgen Schmidhuber},
    title = {Efficient Model-Based Exploration},
    booktitle = {PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON SIMULATION OF ADAPTIVE BEHAVIOR: FROM ANIMALS TO ANIMATS 6},
    year = {1998},
    pages = {223--228},
    publisher = {MIT Press/Bradford Books}
}

@inproceedings{hafner2019learning,
  title={\href{https://proceedings.mlr.press/v97/hafner19a.html}{Learning latent dynamics for planning from pixels}},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International Conference on Machine Learning},
  year={2019},
  pages = 	 {2555--2565},
  organization={PMLR}
}

@inproceedings{hafner2020dream,
  title={\href{https://openreview.net/forum?id=S1lOTC4tDS}{Dream to Control: Learning Behaviors by Latent Imagination}},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  booktitle={International Conference on Learning Representations},
  year={2019}
}



@inproceedings{https://doi.org/10.48550/arxiv.1409.0473,
  title={\href{http://arxiv.org/abs/1409.0473}{Neural machine translation by jointly learning to align and translate}},
  author={Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2015}
}


@misc{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@inproceedings{rezende2014stochastic,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
year = {2014},
publisher = {JMLR.org},
booktitle = {Proceedings of the 31st International Conference on Machine Learning - Volume 32},
location = {Beijing, China},
series = {ICML'14}
}


@misc{effzero,
  doi = {10.48550/ARXIV.2111.00210},
  
  url = {https://arxiv.org/abs/2111.00210},
  
  author = {Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mastering Atari Games with Limited Data},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{rafail,
  doi = {10.48550/ARXIV.2012.11547},
  
  url = {https://arxiv.org/abs/2012.11547},
  
  author = {Rafailov, Rafael and Yu, Tianhe and Rajeswaran, Aravind and Finn, Chelsea},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Offline Reinforcement Learning from Images with Latent Space Models},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{NIPS2015_6ba3af5d,
 author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Action-Conditional Video Prediction using Deep Networks in Atari Games},
 url = {https://proceedings.neurips.cc/paper/2015/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf},
 volume = {28},
 year = {2015}
}


@inproceedings{Hafner2020-ry,
  title={\href{https://openreview.net/forum?id=0oabwyZbOu}{Mastering Atari with Discrete World Models}},
  author={Hafner, Danijar and Lillicrap, Timothy P and Norouzi, Mohammad and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{DBLP:journals/corr/LeibfriedKH16,
  author    = {Felix Leibfried and
               Nate Kushman and
               Katja Hofmann},
  title     = {A Deep Learning Approach for Joint Video Frame and Reward Prediction
               in Atari Games},
  journal   = {CoRR},
  volume    = {abs/1611.07078},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.07078},
  eprinttype = {arXiv},
  eprint    = {1611.07078},
  timestamp = {Mon, 13 Aug 2018 16:48:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LeibfriedKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Pathak2019SelfSupervisedEV,
  title={Self-Supervised Exploration via Disagreement},
  author={Deepak Pathak and Dhiraj Gandhi and Abhinav Kumar Gupta},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.04161}
}

@misc{achiam2018variational,
      title={Variational Option Discovery Algorithms}, 
      author={Joshua Achiam and Harrison Edwards and Dario Amodei and Pieter Abbeel},
      year={2018},
      eprint={1807.10299},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@inproceedings{eysenbach2018diversity,
  title={\href{https://openreview.net/forum?id=SJx63jRqFm}{Diversity is All You Need: Learning Skills without a Reward Function}},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@misc{achiam2018variational,
      title={Variational Option Discovery Algorithms}, 
      author={Joshua Achiam and Harrison Edwards and Dario Amodei and Pieter Abbeel},
      year={2018},
      eprint={1807.10299},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{gregor2016variational,
      title={Variational Intrinsic Control}, 
      author={Karol Gregor and Danilo Jimenez Rezende and Daan Wierstra},
      year={2016},
      eprint={1611.07507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{parameter-noise-plappert,
  author    = {Matthias Plappert and
               Rein Houthooft and
               Prafulla Dhariwal and
               Szymon Sidor and
               Richard Y. Chen and
               Xi Chen and
               Tamim Asfour and
               Pieter Abbeel and
               Marcin Andrychowicz},
  title     = {Parameter Space Noise for Exploration},
  journal   = {CoRR},
  volume    = {abs/1706.01905},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.01905},
  eprinttype = {arXiv},
  eprint    = {1706.01905},
  timestamp = {Mon, 03 Sep 2018 12:15:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PlappertHDSCCAA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{noisynets,
  author    = {Meire Fortunato and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Jacob Menick and
               Ian Osband and
               Alex Graves and
               Vlad Mnih and
               R{\'{e}}mi Munos and
               Demis Hassabis and
               Olivier Pietquin and
               Charles Blundell and
               Shane Legg},
  title     = {Noisy Networks for Exploration},
  journal   = {CoRR},
  volume    = {abs/1706.10295},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.10295},
  eprinttype = {arXiv},
  eprint    = {1706.10295},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FortunatoAPMOGM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ddqn,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  eprinttype = {arXiv},
  eprint    = {1509.06461},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bootstrapdqn,
  author    = {Ian Osband and
               Charles Blundell and
               Alexander Pritzel and
               Benjamin Van Roy},
  title     = {Deep Exploration via Bootstrapped {DQN}},
  journal   = {CoRR},
  volume    = {abs/1602.04621},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.04621},
  eprinttype = {arXiv},
  eprint    = {1602.04621},
  timestamp = {Mon, 13 Aug 2018 16:46:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OsbandBPR16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{atari,
   title={The Arcade Learning Environment: An Evaluation Platform for General Agents},
   volume={47},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.3912},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
   year={2013},
   month={Jun},
   }

@inproceedings{badia2020agent57,
  title={\href{https://proceedings.mlr.press/v119/badia20a.html}{Agent57: Outperforming the atari human benchmark}},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Zhaohan Daniel and Blundell, Charles},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR},
  pages = 	 {507--517},
}


@article{russo2018tutorial,
  title={A Tutorial on Thompson Sampling},
  author={Russo, Daniel J and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  journal={Foundations and Trends in Machine Learning},
  year={2018},
}


@INPROCEEDINGS{Pathak2017-jo,
  title     = "Curiosity-driven exploration by self-supervised prediction",
  booktitle = "Proceedings of the 34th International Conference on Machine
               Learning - Volume 70",
  author    = "Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and
               Darrell, Trevor",
  abstract  = "In many real-world scenarios, rewards extrinsic to the agent are
               extremely sparse, or absent altogether. In such cases, curiosity
               can serve as an intrinsic reward signal to enable the agent to
               explore its environment and learn skills that might be useful
               later in its life. We formulate curiosity as the error in an
               agent's ability to predict the consequence of its own actions in
               a visual feature space learned by a self-supervised inverse
               dynamics model. Our formulation scales to high-dimensional
               continuous state spaces like images, bypasses the difficulties
               of directly predicting pixels, and, critically, ignores the
               aspects of the environment that cannot affect the agent. The
               proposed approach is evaluated in two environments: VizDoom and
               Super Mario Bros. Three broad settings are investigated: 1)
               sparse extrinsic reward, where curiosity allows for far fewer
               interactions with the environment to reach the goal; 2)
               exploration with no extrinsic reward, where curiosity pushes the
               agent to explore more efficiently; and 3) generalization to
               unseen scenarios (e.g. new levels of the same game) where the
               knowledge gained from earlier experience helps the agent explore
               new places much faster than starting from scratch.",
  publisher = "JMLR.org",
  series    = "ICML'17",
  month     =  aug,
  year      =  2017,
  location  = "Sydney, NSW, Australia"
}

@inproceedings{rashid2020optimistic,
  title={\href{https://openreview.net/forum?id=r1xGP6VYwH}{Optimistic Exploration even with a Pessimistic Initialisation}},
  author={Rashid, Tabish and Peng, Bei and B{\"o}hmer, Wendelin and Whiteson, Shimon},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{chen2017ucb,
  title={\href{https://arxiv.org/abs/1706.01502}{Ucb exploration via q-ensembles}},
  author={Chen, Richard Y and Sidor, Szymon and Abbeel, Pieter and Schulman, John},
  journal={arXiv preprint arXiv:1706.01502},
  year={2017}
}


@inproceedings{bai2021principled,
  title={\href{https://proceedings.mlr.press/v139/bai21d.html}{Principled exploration via optimistic bootstrapping and backward induction}},
  author={Bai, Chenjia and Wang, Lingxiao and Han, Lei and Hao, Jianye and Garg, Animesh and Liu, Peng and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  year={2021},
  pages = 	 {577--587},
  organization={PMLR}
}


@inproceedings{tiapkin2022dirichlet,
  title={From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses},
  author={Tiapkin, Daniil and Belomestny, Denis and Moulines, Eric and Naumov, Alexey and Samsonov, Sergey and Tang, Yunhao and Valko, Michal and Menard, Pierre},
  booktitle={International Conference on Machine Learning},
  year={2022},
  organization={PMLR}
}


@article{CURL,
  author    = {Aravind Srinivas and
               Michael Laskin and
               Pieter Abbeel},
  title     = {{CURL:} Contrastive Unsupervised Representations for Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/2004.04136},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.04136},
  eprinttype = {arXiv},
  eprint    = {2004.04136},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-04136.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{RND,
  author    = {Yuri Burda and
               Harrison Edwards and
               Amos J. Storkey and
               Oleg Klimov},
  title     = {Exploration by Random Network Distillation},
  journal   = {CoRR},
  volume    = {abs/1810.12894},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.12894},
  eprinttype = {arXiv},
  eprint    = {1810.12894},
  timestamp = {Thu, 08 Nov 2018 10:57:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-12894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{RAD,
  author    = {Michael Laskin and
               Kimin Lee and
               Adam Stooke and
               Lerrel Pinto and
               Pieter Abbeel and
               Aravind Srinivas},
  title     = {Reinforcement Learning with Augmented Data},
  journal   = {CoRR},
  volume    = {abs/2004.14990},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.14990},
  eprinttype = {arXiv},
  eprint    = {2004.14990},
  timestamp = {Sun, 03 May 2020 17:39:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-14990.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@ARTICLE{Pritzel2017-ou,
  title    = "Neural Episodic Control",
  author   = "Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and
              Badia, A and Vinyals, Oriol and Hassabis, Demis and Wierstra,
              Daan and Blundell, Charles",
  abstract = "This work proposes Neural Episodic Control: a deep reinforcement
              learning agent that is able to rapidly assimilate new experiences
              and act upon them, and shows across a wide range of environments
              that the agent learns significantly faster than other
              state-of-the-art, general purpose deep reinforcementlearning
              agents. Deep reinforcement learning methods attain super-human
              performance in a wide range of environments. Such methods are
              grossly inefficient, often taking orders of magnitudes more data
              than humans to achieve reasonable performance. We propose Neural
              Episodic Control: a deep reinforcement learning agent that is
              able to rapidly assimilate new experiences and act upon them. Our
              agent uses a semi-tabular representation of the value function: a
              buffer of past experience containing slowly changing state
              representations and rapidly updated estimates of the value
              function. We show across a wide range of environments that our
              agent learns significantly faster than other state-of-the-art,
              general purpose deep reinforcement learning agents.",
  journal  = "ICML",
  year     =  2017
}

@misc{shyam2019modelbased,
      title={Model-Based Active Exploration}, 
      author={Pranav Shyam and Wojciech Jaśkowski and Faustino Gomez},
      year={2019},
      eprint={1810.12162},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{Schaul2015-yz,
  title     = "Universal Value Function Approximators",
  booktitle = "Proceedings of the 32nd International Conference on Machine
               Learning",
  author    = "Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver,
               David",
  editor    = "Bach, Francis and Blei, David",
  abstract  = "Value functions are a core component of reinforcement learning.
               The main idea is to to construct a single function approximator
               V(s; theta) that estimates the long-term reward from any state
               s, using parameters $\vartheta$. In this paper we introduce
               universal value function approximators (UVFAs) V(s,g;theta) that
               generalise not just over states s but also over goals g. We
               develop an efficient technique for supervised learning of UVFAs,
               by factoring observed values into separate embedding vectors for
               state and goal, and then learning a mapping from s and g to
               these factored embedding vectors. We show how this technique may
               be incorporated into a reinforcement learning algorithm that
               updates the UVFA solely from observed rewards. Finally, we
               demonstrate that a UVFA can successfully generalise to
               previously unseen goals.",
  publisher = "PMLR",
  volume    =  37,
  series    = "Proceedings of Machine Learning Research",
  year      =  2015,
  address   = "Lille, France"
}



% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sabatelli2018-im,
  title     = "Deep quality-value ({DQV}) learning",
  author    = "Sabatelli, M and Louppe, G and Geurts, P and {others}",
  abstract  = "We introduce a novel Deep Reinforcement Learning (DRL) algorithm
               called Deep Quality- Value (DQV) Learning. DQV uses
               temporal-difference learning to train a Value neural network and
               uses this network for training a second Quality-value network
               that learns to …",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  year      =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sabatelli2018-lb,
  title     = "Deep transfer learning for art classification problems",
  author    = "Sabatelli, M and Kestemont, M and {others}",
  abstract  = "In this paper we investigate whether Deep Convolutional Neural
               Networks (DCNNs), which have obtained state of the art results
               on the ImageNet challenge, are able to perform equally well on
               three different art classification problems. In particular, we
               assess whether it is …",
  journal   = "Proceedings of the",
  publisher = "openaccess.thecvf.com",
  year      =  2018
}

