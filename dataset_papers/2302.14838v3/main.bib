@misc{meyerson2023lmc,
  doi = {10.48550/ARXIV.2302.12170},
  
  url = {https://arxiv.org/abs/2302.12170},
  
  author = {Meyerson, Elliot and Nelson, Mark J. and Bradley, Herbie and Moradi, Arash and Hoover, Amy K. and Lehman, Joel},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Model Crossover: Variation through Few-Shot Prompting},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{odena2021program,
title	= {Program Synthesis with Large Language Models},
author	= {Augustus Odena and Charles Sutton and David Martin Dohan and Ellen Jiang and Henryk Michalewski and Jacob Austin and Maarten Paul Bosma and Maxwell Nye and Michael Terry and Quoc V. Le},
year	= {2021},
note	= {n/a},
booktitle	= {n/a},
pages	= {n/a},
address	= {n/a}
}



@inproceedings{
loshchilov2018adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{LeCun1998GradientbasedLA,
  title={Gradient-based learning applied to document recognition},
  author={Yann LeCun and L{\'e}on Bottou and Yoshua Bengio and Patrick Haffner},
  journal={Proc. IEEE},
  year={1998},
  volume={86},
  pages={2278-2324}
}

@inproceedings{andriy2007bilinear,
author = {Mnih, Andriy and Hinton, Geoffrey},
title = {Three New Graphical Models for Statistical Language Modelling},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273577},
doi = {10.1145/1273496.1273577},
abstract = {The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {641–648},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}


@software{haiku2020github,
  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
  title = {{H}aiku: {S}onnet for {JAX}},
  url = {http://github.com/deepmind/dm-haiku},
  version = {0.0.9},
  year = {2020},
}

@software{flax2020github,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.6.3},
  year = {2020},
}

@inproceedings{gilmer2017message_passing,
author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
title = {Neural Message Passing for Quantum Chemistry},
year = {2017},
publisher = {JMLR.org},
abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1263–1272},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{greydanus2020mnist1d,
  author    = {Sam Greydanus},
  title     = {Scaling *down* Deep Learning},
  journal   = {CoRR},
  volume    = {abs/2011.14439},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.14439},
  eprinttype = {arXiv},
  eprint    = {2011.14439},
  timestamp = {Tue, 01 Dec 2020 14:59:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-14439.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
xu2020algalignment,
title={What Can Neural Networks Reason About?},
author={Keyulu Xu and Jingling Li and Mozhi Zhang and Simon S. Du and Ken-ichi Kawarabayashi and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJxbJeHFPS}
}


@book{cormen2009clrs,
author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
title = {Introduction to Algorithms, Third Edition},
year = {2009},
isbn = {0262033844},
publisher = {The MIT Press},
edition = {3rd},
abstract = {If you had to buy just one text on algorithms, Introduction to Algorithms is a magnificent choice. The book begins by considering the mathematical foundations of the analysis of algorithms and maintains this mathematical rigor throughout the work. The tools developed in these opening sections are then applied to sorting, data structures, graphs, and a variety of selected algorithms including computational geometry, string algorithms, parallel models of computation, fast Fourier transforms (FFTs), and more. This book's strength lies in its encyclopedic range, clear exposition, and powerful analysis. Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book is a great resource on the basic tools used to analyze the performance of algorithms.}
}



@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Arun Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374}
}

@article{Dakhel2022GitHubCA,
  title={GitHub Copilot AI pair programmer: Asset or Liability?},
  author={Arghavan Moradi Dakhel and Vahid Majdinasab and Amin Nikanjam and Foutse Khomh and Michel C. Desmarais and Zhen Ming Jiang},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.15331}
}

@article{amlz,
  author    = {Esteban Real and
               Chen Liang and
               David R. So and
               Quoc V. Le},
  title     = {AutoML-Zero: Evolving Machine Learning Algorithms From Scratch},
  journal   = {CoRR},
  volume    = {abs/2003.03384},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.03384},
  eprinttype = {arXiv},
  eprint    = {2003.03384},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-03384.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{autorl,
  doi = {10.48550/ARXIV.2101.03958},
  
  url = {https://arxiv.org/abs/2101.03958},
  
  author = {Co-Reyes, John D. and Miao, Yingjie and Peng, Daiyi and Real, Esteban and Levine, Sergey and Le, Quoc V. and Lee, Honglak and Faust, Aleksandra},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Evolving Reinforcement Learning Algorithms},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{autorl_ac,
  doi = {10.48550/ARXIV.2204.04292},
  
  url = {https://arxiv.org/abs/2204.04292},
  
  author = {Garau-Luis, Juan Jose and Miao, Yingjie and Co-Reyes, John D. and Parisi, Aaron and Tan, Jie and Real, Esteban and Faust, Aleksandra},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Evolving Generalizable Actor-Critic Algorithms},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{velo,
  doi = {10.48550/ARXIV.2211.09760},
  
  url = {https://arxiv.org/abs/2211.09760},
  
  author = {Metz, Luke and Harrison, James and Freeman, C. Daniel and Merchant, Amil and Beyer, Lucas and Bradbury, James and Agrawal, Naman and Poole, Ben and Mordatch, Igor and Roberts, Adam and Sohl-Dickstein, Jascha},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {VeLO: Training Versatile Learned Optimizers by Scaling Up},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{enas,
  doi = {10.48550/ARXIV.1802.03268},
  
  url = {https://arxiv.org/abs/1802.03268},
  
  author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Neural Architecture Search via Parameter Sharing},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{nas_og,
  doi = {10.48550/ARXIV.1611.01578},
  
  url = {https://arxiv.org/abs/1611.01578},
  
  author = {Zoph, Barret and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Architecture Search with Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{pushgp,
	doi = {10.1007/s10710-021-09414-8},
  
	url = {https://doi.org/10.1007%2Fs10710-021-09414-8},
  
	year = 2021,
	month = {oct},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {22},
  
	number = {4},
  
	pages = {395--428},
  
	author = {Michael A. Lones},
  
	title = {Evolving continuous optimisers from scratch},
  
	journal = {Genetic Programming and Evolvable Machines}
}
@misc{evolution_llm,
  doi = {10.48550/ARXIV.2206.08896},
  
  url = {https://arxiv.org/abs/2206.08896},
  
  author = {Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O.},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Evolution through Large Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{context_policy_iteration,
  doi = {10.48550/ARXIV.2210.03821},
  
  url = {https://arxiv.org/abs/2210.03821},
  
  author = {Brooks, Ethan and Walls, Logan and Lewis, Richard L. and Singh, Satinder},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {In-Context Policy Iteration},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{optformer,
  doi = {10.48550/ARXIV.2205.13320},
  
  url = {https://arxiv.org/abs/2205.13320},
  
  author = {Chen, Yutian and Song, Xingyou and Lee, Chansoo and Wang, Zi and Zhang, Qiuyi and Dohan, David and Kawakami, Kazuya and Kochanski, Greg and Doucet, Arnaud and Ranzato, Marc'aurelio and Perel, Sagi and de Freitas, Nando},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Learning Universal Hyperparameter Optimizers with Transformers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@misc{transformers,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{decision_rl_prompting,
  doi = {10.48550/ARXIV.2206.13499},
  
  url = {https://arxiv.org/abs/2206.13499},
  
  author = {Xu, Mengdi and Shen, Yikang and Zhang, Shun and Lu, Yuchen and Zhao, Ding and Tenenbaum, Joshua B. and Gan, Chuang},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Prompting Decision Transformer for Few-Shot Policy Generalization},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}

@misc{brown20,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{thoppilan22,
  doi = {10.48550/ARXIV.2201.08239},
  
  url = {https://arxiv.org/abs/2201.08239},
  
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LaMDA: Language Models for Dialog Applications},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{workshop22,
  doi = {10.48550/ARXIV.2211.05100},
  
  url = {https://arxiv.org/abs/2211.05100},
  
  author = {{BigScience Workshop} and {:} and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}




@misc{decision_rl,
  doi = {10.48550/ARXIV.2106.01345},
  
  url = {https://arxiv.org/abs/2106.01345},
  
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{autoloss_zero,
  doi = {10.48550/ARXIV.2103.14026},
  
  url = {https://arxiv.org/abs/2103.14026},
  
  author = {Li, Hao and Fu, Tianwen and Dai, Jifeng and Li, Hongsheng and Huang, Gao and Zhu, Xizhou},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{symbolic_optimizer_search,
  doi = {10.48550/ARXIV.2209.08907},
  
  url = {https://arxiv.org/abs/2209.08907},
  
  author = {Raymond, Christian and Chen, Qi and Xue, Bing and Zhang, Mengjie},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Symbolic Model-Agnostic Loss Functions via Meta-Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{symbolic_optimizer_search,
  doi = {10.48550/ARXIV.1709.07417},
  
  url = {https://arxiv.org/abs/1709.07417},
  
  author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Optimizer Search with Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{chowdhery22,
  doi = {10.48550/ARXIV.2204.02311},
  
  url = {https://arxiv.org/abs/2204.02311},
  
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PaLM: Scaling Language Modeling with Pathways},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{du21,
  doi = {10.48550/ARXIV.2112.06905},
  
  url = {https://arxiv.org/abs/2112.06905},
  
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Noorbakhsh2021PretrainedLM,
  title={Pretrained Language Models are Symbolic Mathematics Solvers too!},
  author={Kimia Noorbakhsh and Modar Sulaiman and Mahdi Sharifi and Kallol Roy and Pooyan Jamshidi},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.03501}
}

@article{Wei2022ChainOT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed Huai-hsin Chi and Quoc Le and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903}
}

@article{Xu2022ASE,
  title={A systematic evaluation of large language models of code},
  author={Frank F. Xu and Uri Alon and Graham Neubig and Vincent J. Hellendoorn},
  journal={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  year={2022}
}

@article{Wang2021CodeT5IU,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Yue Wang and Weishi Wang and Shafiq R. Joty and Steven C. H. Hoi},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.00859}
}

@article{Ahmad2021UnifiedPF,
  title={Unified Pre-training for Program Understanding and Generation},
  author={Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.06333}
}

@article{Feng2020CodeBERTAP,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.08155}
}

@misc{zhang2022,
  doi = {10.48550/ARXIV.2205.01068},
  
  url = {https://arxiv.org/abs/2205.01068},
  
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OPT: Open Pre-trained Transformer Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Kojima2022LargeLM,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.11916}
}

@article{Sanh2021MultitaskPT,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal V. Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault F{\'e}vry and Jason Alan Fries and Ryan Teehan and Stella Rose Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08207}
}

@inproceedings{Velivckovic2022TheCA,
  title={The CLRS Algorithmic Reasoning Benchmark},
  author={Petar Veli\v{c}kovi\'{c} and Adri\`{a} Puigdom\`{e}nech Badia and David Budden and Razvan Pascanu and Andrea Banino and Mikhail Dashevskiy and Raia Hadsell and Charles Blundell},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{clrs_generalist,
  title={A Generalist Neural Algorithmic Learner},
  author={Borja Ibarz and Vitaly Kurin and George Papamakarios and Kyriacos Nikiforou and Mehdi Abbana Bennani and R. Csord{\'a}s and Andrew Dudzik and Matko Bovsnjak and Alex Vitvitskyi and Yulia Rubanova and Andreea Deac and Beatrice Bevilacqua and Yaroslav Ganin and Charles Blundell and Petar Veli\v{c}kovi\'{c}},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.11142}
}

@article{Min2022RethinkingTR,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.12837}
}

@inproceedings{Liu2021WhatMG,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
  booktitle={Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},
  year={2021}
}

@article{Rubin2021LearningTR,
  title={Learning To Retrieve Prompts for In-Context Learning},
  author={Ohad Rubin and Jonathan Herzig and Jonathan Berant},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.08633}
}

@inproceedings{Lu2021FantasticallyOP,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}


@article{Zhou2022LargeLM,
  title={Large Language Models Are Human-Level Prompt Engineers},
  author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.01910}
}

@article{Zhao2021CalibrateBU,
  title={Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  author={Tony Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.09690}
}

@article{Wei2021FinetunedLM,
  title={Finetuned Language Models Are Zero-Shot Learners},
  author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.01652}
}

@article{Ouyang2022TrainingLM,
  title={Training language models to follow instructions with human feedback},
  author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.02155}
}

@article{Sanh2021MultitaskPT,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal V. Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault F{\'e}vry and Jason Alan Fries and Ryan Teehan and Stella Rose Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08207}
}

@inproceedings{Real2018RegularizedEF,
  title={Regularized Evolution for Image Classifier Architecture Search},
  author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V. Le},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{Real2017LargeScaleEO,
  title={Large-Scale Evolution of Image Classifiers},
  author={Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01041}
}

@article{Elsken2018EfficientMN,
  title={Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution},
  author={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  journal={arXiv: Machine Learning},
  year={2018}
}

@article{So2019TheET,
  title={The Evolved Transformer},
  author={David R. So and Chen Liang and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.11117}
}

@article{Liu2020EvolvingNL,
  title={Evolving Normalization-Activation Layers},
  author={Hanxiao Liu and Andrew Brock and Karen Simonyan and Quoc V. Le},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.02967}
}

@article{Li2019RandomSA,
  title={Random Search and Reproducibility for Neural Architecture Search},
  author={Liam Li and Ameet S. Talwalkar},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.07638}
}

@article{Sciuto2019EvaluatingTS,
  title={Evaluating the Search Phase of Neural Architecture Search},
  author={Christian Sciuto and Kaicheng Yu and Martin Jaggi and Claudiu Cristian Musat and Mathieu Salzmann},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.08142}
}

@article{Bender2020CanWS,
  title={Can Weight Sharing Outperform Random Architecture Search? An Investigation With TuNAS},
  author={Gabriel Bender and Hanxiao Liu and Bo Chen and Grace Chu and Shuyang Cheng and Pieter-Jan Kindermans and Quoc V. Le},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={14311-14320}
}

@inproceedings{Real2020AutoMLZeroEM,
  title={AutoML-Zero: Evolving Machine Learning Algorithms From Scratch},
  author={Esteban Real and Chen Liang and David R. So and Quoc V. Le},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@misc{So21,
  doi = {10.48550/ARXIV.2109.08668},
  
  url = {https://arxiv.org/abs/2109.08668},
  
  author = {So, David R. and Mańke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Primer: Searching for Efficient Transformers for Language Modeling},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Qian2022LimitationsOL,
  title={Limitations of Language Models in Arithmetic and Symbolic Induction},
  author={Jingu Qian and Hong Wang and Zekun Li and SHIYANG LI and Xifeng Yan},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.05051}
}

@inproceedings{Kudo2018SentencePieceAS,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Taku Kudo and John Richardson},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018}
}

@inproceedings{Koza1993GeneticP,
  title={Genetic programming - on the programming of computers by means of natural selection},
  author={John R. Koza},
  booktitle={Complex Adaptive Systems},
  year={1993}
}

@article{Lehman2022EvolutionTL,
  title={Evolution through Large Models},
  author={Joel Lehman and Jonathan Gordon and Shawn Jain and Kamal Ndousse and Cathy Yeh and Kenneth O. Stanley},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.08896}
}

@article{Yao1999EvolvingAN,
  title={Evolving artificial neural networks},
  author={Xin Yao},
  journal={Proc. IEEE},
  year={1999},
  volume={87},
  pages={1423-1447}
}

@article{williams1992,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Williams, R. J.},
  biburl = {https://www.bibsonomy.org/bibtex/294224c3e53bfe80ade7218b3a0283465/idsia},
  citeulike-article-id = {2374762},
  interhash = {b90d65a735ae02a940f5075b0fd7ebe7},
  intrahash = {94224c3e53bfe80ade7218b3a0283465},
  journal = {Machine Learning},
  keywords = {daanbib},
  pages = {229--256},
  priority = {2},
  timestamp = {2008-03-11T15:05:47.000+0100},
  title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  volume = 8,
  year = 1992
}

@TechReport{williams1988,
  author      = {Williams, Ronald J.},
  title       = {Toward a theory of reinforcement-learning connectionist systems},
  institution = {Northeastern University, College of Computer Science},
  year        = {1988},
  number      = {NU-CCS-88-3},
  owner       = {rkamalapurkar},
  timestamp   = {2016.05.27},
}
@misc{prompt_tune,
  doi = {10.48550/ARXIV.2104.08691},
  
  url = {https://arxiv.org/abs/2104.08691},
  
  author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cascades,
  doi = {10.48550/ARXIV.2207.10342},
  
  url = {https://arxiv.org/abs/2207.10342},
  
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Model Cascades},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{dong2021nats,
  title   = {{NATS-Bench}: Benchmarking NAS Algorithms for Architecture Topology and Size},
  author  = {Dong, Xuanyi and Liu, Lu and Musial, Katarzyna and Gabrys, Bogdan},
  doi     = {10.1109/TPAMI.2021.3054824},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year    = {2021},
  note    = {\mbox{doi}:\url{10.1109/TPAMI.2021.3054824}}
}
