\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5048--5058, 2017.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{barreto17}
Barreto, A., Dabney, W., Munos, R., Hunt, J., Schaul, T., van Hasselt, H., and
  Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4055--4065, 2017.

\bibitem[Barto \& Mahadevan(2003)Barto and Mahadevan]{barto03}
Barto, A. and Mahadevan, S.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock \emph{Discrete Event Dynamic Systems}, 13\penalty0 (1-2):\penalty0
  41--77, 2003.

\bibitem[Bertsekas \& Tsitsiklis(1991)Bertsekas and Tsitsiklis]{bertsekas91}
Bertsekas, D. and Tsitsiklis, J.
\newblock An analysis of stochastic shortest path problems.
\newblock \emph{Mathematics of Operations Research}, 16\penalty0 (3):\penalty0
  580--595, 1991.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox15}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{32nd Conference on Uncertainty in Artificial Intelligence},
  2016.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Pong, Zhou, Dalal, Abbeel, and
  Levine]{haarnoja18}
Haarnoja, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S.
\newblock Composable deep reinforcement learning for robotic manipulation.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation}, pp.\  6244--6251. IEEE, 2018.

\bibitem[Hunt et~al.(2019)Hunt, Barreto, Lillicrap, and Heess]{hunt19}
Hunt, J., Barreto, A., Lillicrap, T., and Heess, N.
\newblock Composing entropic policies using divergence correction.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   2911--2920. PMLR, 2019.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch10}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[James \& Collins(2006)James and Collins]{james06}
James, H. and Collins, E.
\newblock An analysis of transient {M}arkov decision processes.
\newblock \emph{Journal of Applied Probability}, 43\penalty0 (3):\penalty0
  603--621, 2006.

\bibitem[Kaelbling(1993)]{kaelbling93}
Kaelbling, L.~P.
\newblock Learning to achieve goals.
\newblock In \emph{International Joint Conferences on Artificial Intelligence},
  pp.\  1094--1099, 1993.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine16}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap16}
Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
  D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Mirowski et~al.(2017)Mirowski, Pascanu, Viola, Soyer, Ballard, Banino,
  Denil, Goroshin, Sifre, Kavukcuoglu, et~al.]{mirowski2016learning}
Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A.,
  Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., et~al.
\newblock Learning to navigate in complex environments.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih15}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A., Veness, J., Bellemare, M.,
  Graves, A., Riedmiller, M., Fidjeland, A., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Peng et~al.(2019)Peng, Chang, Zhang, Abbeel, and Levine]{peng19}
Peng, X., Chang, M., Zhang, G., Abbeel, P., and Levine, S.
\newblock {MCP}: Learning composable hierarchical control with multiplicative
  compositional policies.
\newblock \emph{arXiv preprint arXiv:1905.09808}, 2019.

\bibitem[Saxe et~al.(2017)Saxe, Earle, and Rosman]{saxe17}
Saxe, A., Earle, A., and Rosman, B.
\newblock Hierarchy through composition with multitask {LMDP}s.
\newblock \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 70:\penalty0 3017--3026, 2017.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and Silver]{schaul15}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, volume~37 of \emph{Proceedings of Machine Learning Research}, pp.\
   1312--1320, Lille, France, 2015. PMLR.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver17}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton99}
Sutton, R., Precup, D., and Singh, S.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction
  in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Todorov(2007)]{todorov07}
Todorov, E.
\newblock Linearly-solvable {Markov} decision problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1369--1376, 2007.

\bibitem[Todorov(2009)]{todorov09}
Todorov, E.
\newblock Compositionality of optimal control laws.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1856--1864, 2009.

\bibitem[Van~Niekerk et~al.(2019)Van~Niekerk, James, Earle, and
  Rosman]{vanniekerk19}
Van~Niekerk, B., James, S., Earle, A., and Rosman, B.
\newblock Composing value functions in reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   6401--6409. PMLR, 2019.

\bibitem[Veeriah et~al.(2018)Veeriah, Oh, and Singh]{veeriah2018many}
Veeriah, V., Oh, J., and Singh, S.
\newblock Many-goals reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.09605}, 2018.

\bibitem[Watkins(1989)]{watkins89}
Watkins, C.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge, 1989.

\end{thebibliography}
