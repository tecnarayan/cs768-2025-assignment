@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@INPROCEEDINGS{dilip2011sparsity,
  author={Krishnan, Dilip and Tay, Terence and Fergus, Rob},
  booktitle={CVPR 2011}, 
  title={Blind deconvolution using a normalized sparsity measure}, 
  year={2011},
  volume={},
  number={},
  pages={233-240},
  doi={10.1109/CVPR.2011.5995521}}

@article{WangTPAMI22,
  title={Neural Graph Matching Network: Learning Lawler's Quadratic Assignment Problem with Extension to Hypergraph and Multiple-graph Matching},
  author={Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022}
}

@InProceedings{li2015convergent,
  title = 	 {Convergent Learning: Do different neural networks learn the same representations?},
  author = 	 {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
  booktitle = 	 {Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015},
  pages = 	 {196--212},
  year = 	 {2015},
  editor = 	 {Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
  volume = 	 {44},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Montreal, Canada},
  month = 	 {11 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v44/li15convergent.pdf},
  url = 	 {https://proceedings.mlr.press/v44/li15convergent.html}
}


@inproceedings{
    frankle2018lottery,
    title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
    author={Jonathan Frankle and Michael Carbin},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@inproceedings{HechtNielsen1990ONTA,
  title={ON THE ALGEBRAIC STRUCTURE OF FEEDFORWARD NETWORK WEIGHT SPACES},
  author={Robert Hecht-Nielsen},
  year={1990}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@inproceedings{freeman2017topology,
    title={Topology and Geometry of Half-Rectified Network Optimization},
    author={C. Daniel Freeman and Joan Bruna},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=Bk0FWVcgx}
}

@inproceedings{
    nguyen2018loss,
    title={On the loss landscape of a class of deep neural networks with no bad local valleys},
    author={Quynh Nguyen and Mahesh Chandra Mukkamala and Matthias Hein},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=HJgXsjA5tQ},
}

@inproceedings{nguyen2019connected,
  title={On connected sublevel sets in deep learning},
  author={Nguyen, Quynh},
  booktitle={International conference on machine learning},
  pages={4790--4799},
  year={2019},
  organization={PMLR}
}

@article{kuditipudi2019explaining,
  title={Explaining landscape connectivity of low-cost solutions for multilayer nets},
  author={Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Ge, Rong and Arora, Sanjeev},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{liang2018understanding,
  title={Understanding the loss surface of neural networks for binary classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018},
  organization={PMLR}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@misc{lubana2023mechanistic,
    title={Mechanistic Mode Connectivity},
    author={Ekdeep Singh Lubana and Eric J Bigelow and Robert P. Dick and David Krueger and Hidenori Tanaka},
    year={2023},
    url={https://openreview.net/forum?id=NZZoABNZECq}
}

@inproceedings{
    yunis2022on,
    title={On Convexity and Linear Mode Connectivity in Neural Networks},
    author={David Yunis and Kumar Kshitij Patel and Pedro Henrique Pamplona Savarese and Gal Vardi and Jonathan Frankle and Matthew Walter and Karen Livescu and Michael Maire},
    booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
    year={2022},
    url={https://openreview.net/forum?id=TZQ3PKL3fPr}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@inproceedings{ashmore2015method,
  title={A method for finding similarity between multi-layer perceptrons by Forward Bipartite Alignment},
  author={Ashmore, Stephen and Gashler, Michael},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2015},
  organization={IEEE}
}


@article{tatro2020optimizing,
  title={Optimizing mode connectivity via neuron alignment},
  author={Tatro, Norman and Chen, Pin-Yu and Das, Payel and Melnyk, Igor and Sattigeri, Prasanna and Lai, Rongjie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15300--15311},
  year={2020}
}


@inproceedings{entezari2022the,
    title={The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},
    author={Rahim Entezari and Hanie Sedghi and Olga Saukh and Behnam Neyshabur},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=dNigytemkL}
}

@inproceedings{ainsworth2023git,
    title={Git Re-Basin: Merging Models modulo Permutation Symmetries},
    author={Samuel Ainsworth and Jonathan Hayase and Siddhartha Srinivasa},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=CQsmMYmlP5T}
}

@inproceedings{Wang2020Federated,
    title={Federated Learning with Matched Averaging},
    author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=BkluqlSFDS}
}

@article{singh2020model,
  title={Model fusion via optimal transport},
  author={Singh, Sidak Pal and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22045--22055},
  year={2020}
}

@inproceedings{DBLP:conf/icml/LiuLWXSY22,
  author={Chang Liu and Chenfei Lou and Runzhong Wang and Alan Yuhan Xi and Li Shen and Junchi Yan},
  title={Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning},
  year={2022},
  cdate={1640995200000},
  pages={13857-13869},
  url={https://proceedings.mlr.press/v162/liu22k.html},
  booktitle={ICML}
}
% crossref={conf/icml/2022}c
@inproceedings{
    Li2020An,
    title={An Exponential Learning Rate Schedule for Deep Learning},
    author={Zhiyuan Li and Sanjeev Arora},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rJg8TeSFDH}
}

@article{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	publisher={Citeseer}
}

@article{lecun1998gradient,
	title={Gradient-based learning applied to document recognition},
	author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	journal={Proceedings of the IEEE},
	volume={86},
	number={11},
	pages={2278--2324},
	year={1998},
	publisher={Ieee}
}

@inproceedings{simonyan2015vgg,
  author       = {Karen Simonyan and
                  Andrew Zisserman},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.1556},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{kaiming2016residual,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}

@article{venturi2018spurious,
  author       = {Luca Venturi and
                  Afonso S. Bandeira and
                  Joan Bruna},
  title        = {Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes},
  journal      = {J. Mach. Learn. Res.},
  volume       = {20},
  pages        = {133:1--133:34},
  year         = {2019},
  url          = {http://jmlr.org/papers/v20/18-674.html},
  timestamp    = {Thu, 18 Jun 2020 22:13:25 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/VenturiBB19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Lenc, Karel and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}

@inproceedings{bansal2021revisiting,
    title={Revisiting Model Stitching to Compare Neural Representations},
    author={Yamini Bansal and Preetum Nakkiran and Boaz Barak},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=ak06J5jNR4}
}

@article{koopmans1957assignment,
  title={Assignment problems and the location of economic activities},
  author={Koopmans, Tjalling C and Beckmann, Martin},
  journal={Econometrica: journal of the Econometric Society},
  pages={53--76},
  year={1957},
  publisher={JSTOR}
}

@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}

@inproceedings{
serra2021scaling,
title={Scaling Up Exact Neural Network Compression by Re{LU} Stability},
author={Thiago Serra and Xin Yu and Abhinav Kumar and Srikumar Ramalingam},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=tqQ-8MuSqm}
}

@inproceedings{NEURIPS2019_46a4378f,
 author = {Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Ge, Rong and Arora, Sanjeev},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2018_51d92be1,
 author = {Yu, Tianshu and Yan, Junchi and Wang, Yilin and Liu, Wei and Li, baoxin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generalizing Graph Matching beyond Quadratic Assignment Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf},
 volume = {31},
 year = {2018}
}


@article{xu2023benign,
  title={Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data},
  author={Xu, Zhiwei and Wang, Yutong and Frei, Spencer and Vardi, Gal and Hu, Wei},
  journal={arXiv preprint arXiv:2310.02541},
  year={2023}
}

@article{cao2022benign,
  title={Benign overfitting in two-layer convolutional neural networks},
  author={Cao, Yuan and Chen, Zixiang and Belkin, Misha and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={25237--25250},
  year={2022}
}


@InProceedings{pmlr-v202-zhu23h,
  title = 	 {Benign Overfitting in Deep Neural Networks under Lazy Training},
  author =       {Zhu, Zhenyu and Liu, Fanghui and Chrysos, Grigorios and Locatello, Francesco and Cevher, Volkan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {43105--43128},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zhu23h/zhu23h.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zhu23h.html},
  abstract = 	 {This paper focuses on over-parameterized deep neural networks (DNNs) with ReLU activation functions and proves that when the data distribution is well-separated, DNNs can achieve Bayes-optimal test error for classification while obtaining (nearly) zero-training error under the lazy training regime. For this purpose, we unify three interrelated concepts of overparameterization, benign overfitting, and the Lipschitz constant of DNNs. Our results indicate that interpolating with smoother functions leads to better generalization. Furthermore, we investigate the special case where interpolating smooth ground-truth functions is performed by DNNs under the Neural Tangent Kernel (NTK) regime for generalization. Our result demonstrates that the generalization error converges to a constant order that only depends on label noise and initialization noise, which theoretically verifies benign overfitting. Our analysis provides a tight lower bound on the normalized margin under non-smooth activation functions, as well as the minimum eigenvalue of NTK under high-dimensional settings, which has its own interest in learning theory.}
}
