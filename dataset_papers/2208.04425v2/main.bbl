\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{{Convex Optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock {Language Models are Few-Shot Learners}.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[{Carreira-Perpinan} and
  Idelbayev(2018)]{carreira2018learningcompression}
M.~A. {Carreira-Perpinan} and Y.~Idelbayev.
\newblock {``Learning-Compression'' Algorithms for Neural Net Pruning}.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Chen and Rockafellar(1997)]{chen1997convergence}
G.~H. Chen and R.~T. Rockafellar.
\newblock {Convergence Rates in Forward--Backward Splitting}.
\newblock \emph{SIAM Journal on Optimization}, 7\penalty0 (2):\penalty0
  421--444, 1997.

\bibitem[Cotter et~al.(2019)Cotter, Jiang, Gupta, Wang, Narayan, You, and
  Sridharan]{cotter2019}
A.~Cotter, H.~Jiang, M.~Gupta, S.~Wang, T.~Narayan, S.~You, and K.~Sridharan.
\newblock {Optimization with Non-Differentiable Constraints with Applications
  to Fairness, Recall, Churn, and Other Goals}.
\newblock In \emph{{JMLR}}, 2019.

\bibitem[Dai et~al.(2018)Dai, Zhu, Guo, and Wipf]{dai2018vib}
B.~Dai, C.~Zhu, B.~Guo, and D.~Wipf.
\newblock {Compressing Neural Networks using the Variational Information
  Bottleneck}.
\newblock In \emph{ICML}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, {Kai Li}, and {Li
  Fei-Fei}]{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, {Kai Li}, and {Li Fei-Fei}.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{{CVPR}}, 2009.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and Elsenl]{evci2020rigl}
U.~Evci, T.~Gale, J.~Menick, P.~S. Castro, and E.~Elsenl.
\newblock {Rigging the Lottery: Making All Tickets Winners}.
\newblock In \emph{{ICML}}, 2020.

\bibitem[Evci et~al.(2022)Evci, Ioannou, Keskin, and
  Dauphin]{evci2022GradientFlowa}
U.~Evci, Y.~Ioannou, C.~Keskin, and Y.~Dauphin.
\newblock Gradient {{Flow}} in {{Sparse Neural Networks}} and {{How Lottery
  Tickets Win}}.
\newblock In \emph{{{AAAI}}}, 2022.

\bibitem[Fioretto et~al.(2020)Fioretto, Van~Hentenryck, Mak, Tran, Baldo, and
  Lombardi]{fioretto2020LagrangianDuality}
F.~Fioretto, P.~Van~Hentenryck, T.~W.~K. Mak, C.~Tran, F.~Baldo, and
  M.~Lombardi.
\newblock {Lagrangian Duality for Constrained Deep Learning}.
\newblock In \emph{Joint {{European Conference}} on {{Machine Learning}} and
  {{Knowledge Discovery}} in {{Databases}}}, 2020.

\bibitem[Frank and Wolfe(1956)]{frankwolfe}
M.~Frank and P.~Wolfe.
\newblock {An algorithm for quadratic programming}.
\newblock \emph{Naval Research Logistics Quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Frankle and Carbin(2019)]{frankle2019lottery}
J.~Frankle and M.~Carbin.
\newblock {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural
  Networks}.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and Carbin]{frankle2020}
J.~Frankle, G.~K. Dziugaite, D.~Roy, and M.~Carbin.
\newblock {Linear Mode Connectivity and the Lottery Ticket Hypothesis}.
\newblock In \emph{{ICML}}, 2020.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
T.~Gale, E.~Elsen, and S.~Hooker.
\newblock {The State of Sparsity in Deep Neural Networks}.
\newblock In \emph{{NeurIPS - Workshop in ODML-CDNNR}}, 2019.

\bibitem[Gallego-Posada and Ramirez(2022)]{cooper}
J.~Gallego-Posada and J.~Ramirez.
\newblock {Cooper: a toolkit for Lagrangian-based constrained optimization}.
\newblock \texttt{\url{https://github.com/cooper-org/cooper}}, 2022.

\bibitem[Gidel et~al.(2019)Gidel, Berard, Vignoud, Vincent, and
  Lacoste-Julien]{gidel2018variational}
G.~Gidel, H.~Berard, G.~Vignoud, P.~Vincent, and S.~Lacoste-Julien.
\newblock {A Variational Inequality Perspective on Generative Adversarial
  Networks}.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Y.~Guo, A.~Yao, and Y.~Chen.
\newblock {Dynamic Network Surgery for Efficient DNNs}.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deepcompression}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock {Deep Compression: Compressing Deep Neural Networks with Pruning,
  Trained Quantization and Huffman Coding}.
\newblock In \emph{{ICLR}}, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015resnets}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock \emph{arXiv:1512.03385}, 2015.

\bibitem[Hooker et~al.(2019)Hooker, Courville, Clark, Dauphin, and
  Frome]{hooker2019compressednnsforget}
S.~Hooker, A.~Courville, G.~Clark, Y.~Dauphin, and A.~Frome.
\newblock {What Do Compressed Deep Neural Networks Forget?}
\newblock \emph{arXiv:1911.05248}, 2019.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
M.~Jaggi.
\newblock {Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization}.
\newblock In \emph{{ICML}}, 2013.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
E.~Jang, S.~Gu, and B.~Poole.
\newblock {Categorical Reparameterization with Gumbel-Softmax}.
\newblock In \emph{{ICLR}}, 2017.

\bibitem[Kingma and Ba(2015)]{adam}
D.~P. Kingma and J.~Ba.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Korpelevich(1976)]{korpelevich1976extragradient}
G.~M. Korpelevich.
\newblock {The extragradient method for finding saddle points and other
  problems}.
\newblock \emph{Matecon}, 1976.

\bibitem[Kundu et~al.(2020)Kundu, Nazemi, Pedram, Chugg, and
  Beerel]{kundu2020pre}
S.~Kundu, M.~Nazemi, M.~Pedram, K.~M. Chugg, and P.~A. Beerel.
\newblock {Pre-defined Sparsity for Low-Complexity Convolutional Neural
  Networks}.
\newblock \emph{IEEE Transactions on Computers}, 69\penalty0 (7):\penalty0
  1045--1058, 2020.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
Y.~LeCun, J.~S. Denker, and S.~A. Solla.
\newblock {Optimal Brain Damage}.
\newblock In \emph{NeurIPS}, 1990.

\bibitem[Lemaire et~al.(2019)Lemaire, Achkar, and
  Jodoin]{lemaire2019structured}
C.~Lemaire, A.~Achkar, and P.-M. Jodoin.
\newblock {Structured Pruning of Neural Networks With Budget-Aware
  Regularization}.
\newblock In \emph{{CVPR}}, 2019.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Karpathy, and Johnson]{tinyImagenet}
F.-F. Li, A.~Karpathy, and J.~Johnson.
\newblock {Tiny ImageNet}.
\newblock \texttt{\url{https://www.kaggle.com/c/tiny-imagenet}},
  2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Kadav, Durdanovic, Samet, and
  Graf]{li2017l1pruning}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf.
\newblock {Pruning Filters for Efficient ConvNets}.
\newblock In \emph{{ICLR}}, 2017{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020gradient}
T.~Lin, C.~Jin, and M.~Jordan.
\newblock {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems}.
\newblock In \emph{{ICML}}, 2020.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2017learning}
C.~Louizos, M.~Welling, and D.~P. Kingma.
\newblock {Learning Sparse Neural Networks through $L_0$ Regularization}.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2016concrete}
C.~J. Maddison, A.~Mnih, and Y.~W. Teh.
\newblock {The Concrete Distribution: A Continuous Relaxation of Discrete
  Random Variables}.
\newblock In \emph{{ICLR}}, 2017.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Schwartz, and
  Shamir]{malach2020}
E.~Malach, G.~Yehudai, S.~Shalev-Schwartz, and O.~Shamir.
\newblock {Proving the Lottery Ticket Hypothesis: Pruning is All You Need}.
\newblock In \emph{{ICML}}, 2020.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
D.~Molchanov, A.~Ashukha, and D.~Vetrov.
\newblock {Variational Dropout Sparsifies Deep Neural Networks}.
\newblock In \emph{{ICML}}, 2017.

\bibitem[Mostafa and Wang(2019)]{mostafa2019parameter}
H.~Mostafa and X.~Wang.
\newblock {Parameter Efficient Training of Deep Convolutional Neural Networks
  by Dynamic Sparse Reparameterization}.
\newblock In \emph{ICML}, 2019.

\bibitem[Nandwani et~al.(2019)Nandwani, Pathak, {Mausam}, and
  Singla]{nandwani2019PrimalDual}
Y.~Nandwani, A.~Pathak, {Mausam}, and P.~Singla.
\newblock {A Primal Dual Formulation For Deep Learning With Constraints}.
\newblock In \emph{{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}}, 2019.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{neklyudov2017structured}
K.~Neklyudov, D.~Molchanov, A.~Ashukha, and D.~P. Vetrov.
\newblock {Structured Bayesian Pruning via Log-Normal Multiplicative Noise}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
A.~Nemirovski.
\newblock {Prox-Method with Rate of Convergence O(1/T) for Variational
  Inequalities with Lipschitz Continuous Monotone Operators and Smooth
  Convex-Concave Saddle Point Problems}.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock {PyTorch: An Imperative Style, High-Performance Deep Learning
  Library}, 2019.

\bibitem[Savarese et~al.(2020)Savarese, Silva, and Maire]{savarese2020winning}
P.~Savarese, H.~Silva, and M.~Maire.
\newblock {Winning the Lottery with Continuous Sparsification}.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[Thimm and Fiesler(1995)]{thimm1995evaluating}
G.~Thimm and E.~Fiesler.
\newblock {Evaluating Pruning Methods}.
\newblock In \emph{ESANN}, 1995.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
K.~Ullrich, E.~Meeds, and M.~Welling.
\newblock {Soft Weight-Sharing for Neural Network Compression}.
\newblock In \emph{{ICLR}}, 2017.

\bibitem[von Neumann(1928)]{neumann1928theorie}
J.~von Neumann.
\newblock {Zur Theorie der Gesellschaftsspiele}.
\newblock \emph{{Mathematische Annalen}}, 100\penalty0 (1):\penalty0 295--320,
  1928.

\bibitem[Wang et~al.(2021)Wang, Qin, Zhang, and Fu]{wang2021NeuralPruning}
H.~Wang, C.~Qin, Y.~Zhang, and Y.~Fu.
\newblock {Neural Pruning via Growing Regularization}.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruykoK16}
S.~Zagoruyko and N.~Komodakis.
\newblock {Wide Residual Networks}.
\newblock In \emph{{BMVC}}, 2016.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Xu, and Zhang]{zhou2021effective}
X.~Zhou, W.~Zhang, H.~Xu, and T.~Zhang.
\newblock {Effective Sparsification of Neural Networks With Global Sparsity
  Constraint}.
\newblock In \emph{{CVPR}}, 2021.

\bibitem[Zhu and Gupta(2017)]{zhuPruneNotPrune2017}
M.~Zhu and S.~Gupta.
\newblock {To Prune, or Not to Prune: Exploring the Efficacy of Pruning for
  Model Compression}.
\newblock \emph{arXiv:1710.01878}, 2017.

\end{thebibliography}
