\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[cis()]{cisco}
Using netflow sampling to select the network traffic to track.
\newblock URL
  \url{https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/netflow/configuration/xe-16/test/nf-xe-16-book/nflow-filt-samp-traff-xe.html}.

\bibitem[Acharya et~al.(2018)Acharya, Falahatgar, Jafarpour, Orlitsky, and
  Suresh]{acharya2018maximum}
Acharya, J., Falahatgar, M., Jafarpour, A., Orlitsky, A., and Suresh, A.~T.
\newblock Maximum selection and sorting with adversarial comparators.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2427--2457, 2018.

\bibitem[Andoni \& Razenshteyn(2015)Andoni and Razenshteyn]{andoni2015optimal}
Andoni, A. and Razenshteyn, I.
\newblock Optimal data-dependent hashing for approximate near neighbors.
\newblock In \emph{Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing}, pp.\  793--801, 2015.

\bibitem[Andoni et~al.(2018)Andoni, Indyk, and
  Razenshteyn]{andoni2018approximate}
Andoni, A., Indyk, P., and Razenshteyn, I.
\newblock Approximate nearest neighbor search in high dimensions.
\newblock In \emph{Proceedings of the International Congress of Mathematicians:
  Rio de Janeiro 2018}, pp.\  3287--3318. World Scientific, 2018.

\bibitem[Bun et~al.(2019)Bun, Kamath, Steinke, and Wu]{bun2019private}
Bun, M., Kamath, G., Steinke, T., and Wu, S.~Z.
\newblock Private hypothesis selection.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Canonne(2020)]{canonne2020survey}
Canonne, C.~L.
\newblock A survey on distribution testing: Your data is big. but is it blue?
\newblock \emph{Theory of Computing}, pp.\  1--100, 2020.

\bibitem[Canonne et~al.(2019)Canonne, Kamath, McMillan, Smith, and
  Ullman]{canonne2019structure}
Canonne, C.~L., Kamath, G., McMillan, A., Smith, A., and Ullman, J.
\newblock The structure of optimal private tests for simple hypotheses.
\newblock In \emph{Proceedings of the 51st Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  310--321, 2019.

\bibitem[Daskalakis \& Kamath(2014)Daskalakis and Kamath]{daskalakis2014faster}
Daskalakis, C. and Kamath, G.
\newblock Faster and sample near-optimal algorithms for proper learning
  mixtures of gaussians.
\newblock In \emph{Conference on Learning Theory}, pp.\  1183--1213. PMLR,
  2014.

\bibitem[Devroye \& Lugosi(2001)Devroye and Lugosi]{devroye2001combinatorial}
Devroye, L. and Lugosi, G.
\newblock \emph{Combinatorial methods in density estimation}.
\newblock Springer series in statistics. Springer, 2001.
\newblock ISBN 978-0-387-95117-1.

\bibitem[Diakonikolas(2016)]{diakonikolas2016learning}
Diakonikolas, I.
\newblock Learning structured distributions.
\newblock \emph{Handbook of Big Data}, 267:\penalty0 10--1201, 2016.

\bibitem[Diakonikolas et~al.(2019)Diakonikolas, Kamath, Kane, Li, Moitra, and
  Stewart]{diakonikolas2019robust}
Diakonikolas, I., Kamath, G., Kane, D., Li, J., Moitra, A., and Stewart, A.
\newblock Robust estimators in high-dimensions without the computational
  intractability.
\newblock \emph{SIAM Journal on Computing}, 48\penalty0 (2):\penalty0 742--864,
  2019.

\bibitem[Gopi et~al.(2020)Gopi, Kamath, Kulkarni, Nikolov, Wu, and
  Zhang]{gopi2020locally}
Gopi, S., Kamath, G., Kulkarni, J., Nikolov, A., Wu, Z.~S., and Zhang, H.
\newblock Locally private hypothesis selection.
\newblock In \emph{Conference on Learning Theory}, pp.\  1785--1816. PMLR,
  2020.

\bibitem[Har-Peled et~al.(2012)Har-Peled, Indyk, and
  Motwani]{har2012approximate}
Har-Peled, S., Indyk, P., and Motwani, R.
\newblock Approximate nearest neighbor: Towards removing the curse of
  dimensionality.
\newblock 2012.

\bibitem[Indyk(2001)]{indyk2001approximate}
Indyk, P.
\newblock On approximate nearest neighbors under $\ell{\infty}$ norm.
\newblock \emph{Journal of Computer and System Sciences}, 63\penalty0
  (4):\penalty0 627--638, 2001.

\bibitem[Kamath et~al.(2020)Kamath, Singhal, and Ullman]{kamath2020private}
Kamath, G., Singhal, V., and Ullman, J.
\newblock Private mean estimation of heavy-tailed distributions.
\newblock In \emph{Conference on Learning Theory}, pp.\  2204--2235. PMLR,
  2020.

\bibitem[Kamath et~al.(2015)Kamath, Orlitsky, Pichapati, and
  Suresh]{kamath2015learning}
Kamath, S., Orlitsky, A., Pichapati, D., and Suresh, A.~T.
\newblock On learning distributions from their samples.
\newblock In \emph{Conference on Learning Theory}, pp.\  1066--1100. PMLR,
  2015.

\bibitem[Scheffe(1947)]{Scheffe1947useful}
Scheffe, H.
\newblock {A Useful Convergence Theorem for Probability Distributions}.
\newblock \emph{The Annals of Mathematical Statistics}, 18\penalty0
  (3):\penalty0 434--438, 1947.
\newblock \doi{10.1214/aoms/1177730390}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177730390}.

\bibitem[Suresh et~al.(2014)Suresh, Orlitsky, Acharya, and
  Jafarpour]{suresh2014spherical}
Suresh, A.~T., Orlitsky, A., Acharya, J., and Jafarpour, A.
\newblock Near-optimal-sample estimators for spherical gaussian mixtures.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and
  Weinberger, K. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.

\bibitem[Szpankowski(2011)]{szpankowski2011average}
Szpankowski, W.
\newblock \emph{Average case analysis of algorithms on sequences}.
\newblock John Wiley \& Sons, 2011.

\bibitem[Valiant \& Valiant(2011)Valiant and Valiant]{valiant2011power}
Valiant, G. and Valiant, P.
\newblock The power of linear estimators.
\newblock In \emph{2011 IEEE 52nd Annual Symposium on Foundations of Computer
  Science}, pp.\  403--412. IEEE, 2011.

\bibitem[Yatracos(1985)]{yatracos1985rates}
Yatracos, Y.~G.
\newblock Rates of convergence of minimum distance estimators and kolmogorov's
  entropy.
\newblock \emph{The Annals of Statistics}, 13\penalty0 (2):\penalty0 768--774,
  1985.

\end{thebibliography}
