\begin{thebibliography}{10}

\bibitem{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep {L}earning {S}caling is {P}redictable, {E}mpirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling {L}aws for {N}eural {L}anguage {M}odels.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language {M}odels are {F}ew-shot {L}earners.
\newblock In {\em NeurIPS}, pages 1877--1901, 2020.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning {T}ransferable {V}isual {M}odels from {N}atural {L}anguage
  {S}upervision.
\newblock In {\em ICML}, pages 8748--8763, 2021.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An {I}mage is {W}orth 16x16 {W}ords: Transformers for {I}mage
  {R}ecognition at {S}cale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond {N}eural {S}caling {L}aws: {B}eating {P}ower {L}aw {S}caling
  via {D}ata {P}runing.
\newblock In {\em NeurIPS}, pages 19523--19536, 2022.

\bibitem{wei2021learning}
Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu.
\newblock Learning with {N}oisy {L}abels {R}evisited: A {S}tudy using
  {R}eal-world {H}uman {A}nnotations.
\newblock {\em arXiv preprint arXiv:2110.12088}, 2021.

\bibitem{li2017webvision}
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van~Gool.
\newblock {Webvision Database: Visual Learning and Understanding from Web
  Data}.
\newblock {\em arXiv preprint arXiv:1708.02862}, 2017.

\bibitem{xiao2015learning}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock {Learning from Massive Noisy Labeled Data for Image Classification}.
\newblock In {\em CVPR}, pages 2691--2699, 2015.

\bibitem{song2022learning}
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee.
\newblock Learning from {N}oisy {L}abels with {D}eep {N}eural {N}etworks: A
  {S}urvey.
\newblock {\em TNNLS}, pages 1--19, 2022.

\bibitem{song2019selfie}
Hwanjun Song, Minseok Kim, and Jae-Gil Lee.
\newblock {Selfie: Refurbishing Unclean Samples for Robust Deep Learning}.
\newblock In {\em ICML}, pages 5907--5915, 2019.

\bibitem{xie2020unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le.
\newblock {Unsupervised Data Augmentation for Consistency Training}.
\newblock In {\em NeurIPS}, pages 6256--6268, 2020.

\bibitem{li2020dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock {Dividemix: Learning with Noisy Labels as Semi-supervised Learning}.
\newblock {\em arXiv preprint arXiv:2002.07394}, 2020.

\bibitem{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi Tachet~des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock {An Empirical Study of Example Forgetting during Deep Neural Network
  Learning}.
\newblock {\em arXiv preprint arXiv:1812.05159}, 2018.

\bibitem{paul2021deep}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep {L}earning on a {D}ata {D}iet: {F}inding {I}mportant {E}xamples
  {E}arly in {T}raining.
\newblock In {\em NeurIPS}, pages 20596--20607, 2021.

\bibitem{guo2022deepcore}
Chengcheng Guo, Bo~Zhao, and Yanbing Bai.
\newblock {Deepcore: A Comprehensive Library for Coreset Selection in Deep
  Learning}.
\newblock In {\em DEXA}, pages 181--195, 2022.

\bibitem{park2022meta}
Dongmin Park, Yooju Shin, Jihwan Bang, Youngjun Lee, Hwanjun Song, and Jae-Gil
  Lee.
\newblock {Meta-Query-Net: Resolving Purity-Informativeness Dilemma in Open-set
  Active Learning}.
\newblock In {\em NeurIPS}, pages 31416--31429, 2022.

\bibitem{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock {Co-teaching: Robust Training of Deep Neural Networks with Extremely
  Noisy Labels}.
\newblock In {\em NeurIPS}, pages 8536--8546, 2018.

\bibitem{killamsetty2021glister}
Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and
  Rishabh Iyer.
\newblock {Glister: Generalization based Data Subset Selection for Efficient
  and Robust Learning}.
\newblock In {\em AAAI}, pages 8110--8118, 2021.

\bibitem{goldberger2017training}
Jacob Goldberger and Ehud Ben-Reuven.
\newblock {Training Deep Neural Networks Using a Noise Adaptation Layer}.
\newblock In {\em ICLR}, 2017.

\bibitem{han2018masking}
Bo~Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya~Zhang, and
  Masashi Sugiyama.
\newblock {Masking: A New Perspective of Noisy Supervision}.
\newblock In {\em NeurIPS}, pages 5841--5851, 2018.

\bibitem{yao2018deep}
Jiangchao Yao, Jiajie Wang, Ivor~W Tsang, Ya~Zhang, Jun Sun, Chengqi Zhang, and
  Rui Zhang.
\newblock {Deep Learning from Noisy Image Labels with Quality Embedding}.
\newblock {\em TIP}, 28:1909--1922, 2018.

\bibitem{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and P~Shanti Sastry.
\newblock {Robust Loss Functions under Label Noise for Deep Neural Networks}.
\newblock In {\em AAAI}, pages 1919--1925, 2017.

\bibitem{hendrycks2018using}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
\newblock {Using Trusted Data to Train Deep Networks on Labels Corrupted by
  Severe Noise}.
\newblock In {\em NeurIPS}, pages 10477--10486, 2018.

\bibitem{ma2020normalized}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James
  Bailey.
\newblock {Normalized Loss Functions for Deep Learning with Noisy Labels}.
\newblock In {\em ICML}, pages 6543--6553, 2020.

\bibitem{jiang2018mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock {Mentornet: Learning Data-driven Curriculum for Very Deep Neural
  Networks on Corrupted Labels}.
\newblock In {\em ICML}, pages 2304--2313, 2018.

\bibitem{wei2020combating}
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo~An.
\newblock {Combating Noisy Labels by Agreement: A Joint Training Method with
  Co-regularization}.
\newblock In {\em CVPR}, pages 13726--13735, 2020.

\bibitem{wu2020topological}
Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen.
\newblock {A Topological Filter for Learning with Label Noise}.
\newblock In {\em NeurIPS}, pages 21382--21393, 2020.

\bibitem{zhou2021robust}
Tianyi Zhou, Shengjie Wang, and Jeff Bilmes.
\newblock {Robust Curriculum Learning: From Clean Label Detection to Noisy
  Label Self-correction}.
\newblock In {\em ICLR}, 2021.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock {Randaugment: Practical Automated Data Augmentation with a Reduced
  Search Space}.
\newblock In {\em NeurIPS}, pages 18613--18624, 2020.

\bibitem{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock {Early-learning Regularization Prevents Memorization of Noisy
  Labels}.
\newblock In {\em NeurIPS}, pages 20331--20342, 2020.

\bibitem{cheng2020learning}
Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu.
\newblock {Learning with Instance-dependent Label Noise: A Sample Sieve
  Approach}.
\newblock {\em arXiv preprint arXiv:2010.02347}, 2020.

\bibitem{liu2022robust}
Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You.
\newblock {Robust Training under Label Noise by Over-parameterization}.
\newblock In {\em ICML}, pages 14153--14172, 2022.

\bibitem{wang2014active}
Dan Wang and Yi~Shang.
\newblock {A New Active Labeling Method for Deep Learning}.
\newblock In {\em IJCNN}, pages 112--119, 2014.

\bibitem{roth2006margin}
Dan Roth and Kevin Small.
\newblock {Margin-based Active Learning for Structured Output Spaces}.
\newblock In {\em ECML}, pages 413--424, 2006.

\bibitem{joshi2009multi}
Ajay~J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.
\newblock {Multi-class Active Learning for Image Classification}.
\newblock In {\em CVPR}, pages 2372--2379, 2009.

\bibitem{chen2012super}
Yutian Chen, Max Welling, and Alex Smola.
\newblock {Super-samples from Kernel Herding}.
\newblock {\em arXiv preprint arXiv:1203.3472}, 2012.

\bibitem{sener2018active}
Ozan Sener and Silvio Savarese.
\newblock {Active Learning for Convolutional Neural Networks: A Core-Set
  Approach}.
\newblock In {\em ICLR}, 2018.

\bibitem{killamsetty2021grad}
Krishnateja Killamsetty, S~Durga, Ganesh Ramakrishnan, Abir De, and Rishabh
  Iyer.
\newblock {Grad-match: Gradient Matching based Data Subset Selection for
  Efficient Deep Model Training}.
\newblock In {\em ICML}, pages 5464--5474, 2021.

\bibitem{mirzasoleiman2020coresets}
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
\newblock {Coresets for Data-efficient Training of Machine Learning Models}.
\newblock In {\em ICML}, pages 6950--6960, 2020.

\bibitem{park2022active}
Dongmin Park, Dimitris Papailiopoulos, and Kangwook Lee.
\newblock {Active Learning is A Strong Baseline for Data Subset Selection}.
\newblock In {\em NeurIPS Workshop}, 2022.

\bibitem{zheng2022coverage}
Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash.
\newblock {Coverage-centric Coreset Selection for High Pruning Rates}.
\newblock {\em ICLR}, 2022.

\bibitem{xia2022moderate}
Xiaobo Xia, Jiale Liu, Jun Yu, Xu~Shen, Bo~Han, and Tongliang Liu.
\newblock {Moderate Coreset: A Universal Method of Data Selection for
  Real-world Data-efficient Deep Learning}.
\newblock In {\em ICLR}, 2022.

\bibitem{englesson2021consistency}
Erik Englesson and Hossein Azizpour.
\newblock {Consistency Regularization Can Improve Robustness to Label Noise}.
\newblock {\em arXiv preprint arXiv:2110.01242}, 2021.

\bibitem{wei2020theoretical}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock {Theoretical Analysis of Self-training with Deep Networks on
  Unlabeled Data}.
\newblock {\em arXiv preprint arXiv:2010.03622}, 2020.

\bibitem{citovsky2021batch}
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand
  Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar.
\newblock {Batch Active Learning at Scale}.
\newblock In {\em NeurIPS}, pages 11933--11944, 2021.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock Technical report, University of Toronto, 2009.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {Imagenet: A Large-scale Hierarchical Image Database}.
\newblock In {\em CVPR}, pages 248--255, 2009.

\bibitem{chen2019understanding}
Pengfei Chen, Ben~Ben Liao, Guangyong Chen, and Shengyu Zhang.
\newblock {Understanding and Utilizing Deep Neural Networks Trained with Noisy
  Labels}.
\newblock In {\em ICML}, pages 1062--1070, 2019.

\bibitem{coleman2019selection}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
  Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock {Selection via Proxy: Efficient Data Selection for Deep Learning}.
\newblock {\em arXiv preprint arXiv:1906.11829}, 2019.

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Identity Mappings in Deep Residual Networks}.
\newblock In {\em ECCV}, pages 630--645, 2016.

\bibitem{szegedy2017inception}
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi.
\newblock {Inception-v4, inception-resnet and the Impact of Residual
  Connections on Learning}.
\newblock In {\em AAAI}, pages 4278--4284, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{feige1998threshold}
Uriel Feige.
\newblock {A Threshold of ln n for Approximating Set Cover}.
\newblock {\em Journal of the ACM (JACM)}, 45(4):634--652, 1998.

\bibitem{yu2020multi}
Qing Yu, Daiki Ikami, Go~Irie, and Kiyoharu Aizawa.
\newblock {Multi-task Curriculum Framework for Open-set Semi-supervised
  Learning}.
\newblock In {\em ECCV}, pages 438--454, 2020.

\bibitem{park2021task}
Dongmin Park, Hwanjun Song, MinSeok Kim, and Jae-Gil Lee.
\newblock {Task-Agnostic Undesirable Feature Deactivation Using
  Out-of-Distribution Data}.
\newblock In {\em NeurIPS}, pages 4040--4052, 2021.

\bibitem{de2021continual}
Matthias De~Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia,
  Ale{\v{s}} Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.
\newblock {A Continual Learning Survey: Defying Forgetting in Classification
  Tasks}.
\newblock {\em PAMI}, 44(7):3366--3385, 2021.

\bibitem{elsken2019neural}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock {Neural Architecture Search: A Survey}.
\newblock {\em JMLR}, 20(1):1997--2017, 2019.

\end{thebibliography}
