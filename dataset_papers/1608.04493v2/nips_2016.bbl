\begin{thebibliography}{10}

\bibitem{chen2015}
Wenlin Chen, James~T. Wilson, Stephen Tyree, Kilian~Q. Weinberger, and Yixin
  Chen.
\newblock Compressing neural networks with the hashing trick.
\newblock In {\em ICML}, 2015.

\bibitem{courbariaux2015}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binary{C}onnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em NIPS}, 2015.

\bibitem{courbariaux2016}
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to +1 or -1.
\newblock {\em arXiv preprint arXiv:1602.02830v3}, 2016.

\bibitem{denil2013}
Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando~de
  Freitas.
\newblock Predicting parameters in deep learning.
\newblock In {\em NIPS}, 2013.

\bibitem{denton2014}
Emily~L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In {\em NIPS}, 2014.

\bibitem{gong2014}
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.
\newblock Compressing deep convolutional networks using vector quantization.
\newblock {\em arXiv preprint arXiv:1412.6115}, 2014.

\bibitem{han2016isca}
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark~A Horowitz, and
  William~J. Dally.
\newblock {EIE}: Efficient inference engine on compressed deep neural network.
\newblock In {\em ISCA}, 2016.

\bibitem{han2016iclr}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and {H}uffman coding.
\newblock In {\em ICLR}, 2016.

\bibitem{han2015}
Song Han, Jeff Pool, John Tran, and William~J. Dally.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In {\em NIPS}, 2015.

\bibitem{hassibi1993}
Babak Hassibi and David~G. Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In {\em NIPS}, 1993.

\bibitem{he2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{jia2014}
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross
  Girshick, Sergio Guadarrama, and Trevor Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In {\em ACM MM}, 2014.

\bibitem{krizhevsky2012}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NIPS}, 2012.

\bibitem{lebedev2015}
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor
  Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock In {\em ICLR}, 2015.

\bibitem{lecun1998}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lecun1989}
Yann LeCun, John~S Denker, Sara~A Solla, Richard~E Howard, and Lawrence~D
  Jackel.
\newblock Optimal brain damage.
\newblock In {\em NIPS}, 1989.

\bibitem{lodish2000}
Harvey Lodish, Arnold Berk, S~Lawrence Zipursky, Paul Matsudaira, David
  Baltimore, and James Darnell.
\newblock {\em Molecular Cell Biology: Neurotransmitters, Synapses, and Impulse
  Transmission}.
\newblock W. H. Freeman, 2000.

\bibitem{mathieu2013}
Michael Mathieu, Mikael Henaff, and Yann LeCun.
\newblock Fast training of convolutional networks through {FFT}s.
\newblock {\em arXiv preprint arXiv:1312.5851}, 2013.

\bibitem{simonyan2014}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em ICLR}, 2014.

\bibitem{vanhoucke2011}
Vincent Vanhoucke, Andrew Senior, and Mark~Z. Mao.
\newblock Improving the speed of neural networks on {CPU}s.
\newblock In {\em NIPS Workshop}, 2011.

\bibitem{yang2015}
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de~Freitas, Alex Smola,
  Le~Song, and Ziyu Wang.
\newblock Deep fried convnets.
\newblock In {\em ICCV}, 2015.

\bibitem{zhang2015}
Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun.
\newblock Efficient and accurate approximations of nonlinear convolutional
  networks.
\newblock In {\em CVPR}, 2015.

\end{thebibliography}
