\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Kipf and Welling(2016)]{kipf2016semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{ICLR}, 2016.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
William~L Hamilton, Zhitao Ying, and Jure Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2018)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock In \emph{ICLR}, 2019.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{arXiv preprint arXiv:2005.00687}, 2020.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Zhu, and
  Song]{chen2018stochastic}
Jianfei Chen, Jun Zhu, and Le~Song.
\newblock Stochastic training of graph convolutional networks with variance
  reduction.
\newblock In \emph{ICML}, 2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Ma, and Xiao]{chen2018fastgcn}
Jie Chen, Tengfei Ma, and Cao Xiao.
\newblock Fastgcn: Fast learning with graph convolutional networks via
  importance sampling.
\newblock In \emph{ICLR}, 2018{\natexlab{b}}.

\bibitem[Zou et~al.(2019)Zou, Hu, Wang, Jiang, Sun, and Gu]{zou2019layer}
Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu.
\newblock Layer-dependent importance sampling for training deep and large graph
  convolutional networks.
\newblock \emph{arXiv preprint arXiv:1911.07323}, 2019.

\bibitem[Chiang et~al.(2019)Chiang, Liu, Si, Li, Bengio, and
  Hsieh]{chiang2019cluster}
Wei-Lin Chiang, Xuanqing Liu, Si~Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.
\newblock Cluster-gcn: An efficient algorithm for training deep and large graph
  convolutional networks.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 257--266, 2019.

\bibitem[Zeng et~al.(2019)Zeng, Zhou, Srivastava, Kannan, and
  Prasanna]{zeng2019graphsaint}
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor
  Prasanna.
\newblock Graphsaint: Graph sampling based inductive learning method.
\newblock In \emph{ICLR}, 2019.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`o}, and
  Veli{\v{c}}kovi{\'c}]{corso2020principal}
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li{\`o}, and Petar
  Veli{\v{c}}kovi{\'c}.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock \emph{NeurIPS}, 33, 2020.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
Micha{\"e}l Defferrard, Xavier Bresson, and Pierre Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Levie et~al.(2018)Levie, Monti, Bresson, and
  Bronstein]{levie2018cayleynets}
Ron Levie, Federico Monti, Xavier Bresson, and Michael~M Bronstein.
\newblock Cayleynets: Graph convolutional neural networks with complex rational
  spectral filters.
\newblock \emph{IEEE Transactions on Signal Processing}, 2018.

\bibitem[Bianchi et~al.(2021)Bianchi, Grattarola, Livi, and
  Alippi]{bianchi2021graph}
Filippo~Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi.
\newblock Graph neural networks with convolutional arma filters.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[YaronLipman(2020)]{yaronlipman2020global}
OmriPuny HeliBen-Hamu YaronLipman.
\newblock Global attention improves graph networks generalization.
\newblock \emph{arXiv preprint arXiv:2006.07846}, 2020.

\bibitem[Rong et~al.(2020)Rong, Bian, Xu, Xie, Wei, Huang, and
  Huang]{rong2020self}
Yu~Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and
  Junzhou Huang.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock \emph{NeurIPS}, 33, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Zhang, Xia, and Sun]{zhang2020graph}
Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li~Sun.
\newblock Graph-bert: Only attention is needed for learning graph
  representations.
\newblock \emph{arXiv preprint arXiv:2001.05140}, 2020.

\bibitem[Klicpera et~al.(2019)Klicpera, Wei{\ss}enberger, and
  G{\"u}nnemann]{klicpera2019diffusion}
Johannes Klicpera, Stefan Wei{\ss}enberger, and Stephan G{\"u}nnemann.
\newblock Diffusion improves graph learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Atwood and Towsley(2016)]{atwood2016diffusion}
James Atwood and Don Towsley.
\newblock Diffusion-convolutional neural networks.
\newblock In \emph{NeurIPS}, pages 1993--2001, 2016.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{morris2019weisfeiler}
Christopher Morris, Martin Ritzert, Matthias Fey, William~L Hamilton, Jan~Eric
  Lenssen, Gaurav Rattan, and Martin Grohe.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{AAAI}, 2019.

\bibitem[Maron et~al.(2019)Maron, Ben-Hamu, Serviansky, and
  Lipman]{maron2019provably}
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman.
\newblock Provably powerful graph networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Du, Kawarabayashi, and
  Jegelka]{xu2019can}
Keyulu Xu, Jingling Li, Mozhi Zhang, Simon~S Du, Ken-ichi Kawarabayashi, and
  Stefanie Jegelka.
\newblock What can neural networks reason about?
\newblock In \emph{ICLR}, 2020.

\bibitem[Loukas(2020)]{loukas2019graph}
Andreas Loukas.
\newblock What graph neural networks cannot learn: depth vs width.
\newblock In \emph{ICLR}, 2020.

\bibitem[Balcilar et~al.(2021)Balcilar, Guillaume, H{\'e}roux, Ga{\"u}z{\`e}re,
  Adam, and Honeine]{balcilar2021analyzing}
Muhammet Balcilar, Renton Guillaume, Pierre H{\'e}roux, Benoit Ga{\"u}z{\`e}re,
  S{\'e}bastien Adam, and Paul Honeine.
\newblock Analyzing the expressive power of graph neural networks in a spectral
  perspective.
\newblock In \emph{ICLR}, 2021.

\bibitem[Huang et~al.(2018)Huang, Zhang, Rong, and Huang]{huang2018adaptive}
Wenbing Huang, Tong Zhang, Yu~Rong, and Junzhou Huang.
\newblock Adaptive sampling towards fast graph representation learning.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Johnson and Lindenstrauss(1984)]{johnson1984extensions}
William~B Johnson and Joram Lindenstrauss.
\newblock Extensions of lipschitz mappings into a hilbert space.
\newblock \emph{Contemporary mathematics}, 26\penalty0 (189-206):\penalty0 1,
  1984.

\bibitem[Kane and Nelson(2014)]{kane2014sparser}
Daniel~M Kane and Jelani Nelson.
\newblock Sparser johnson-lindenstrauss transforms.
\newblock \emph{Journal of the ACM (JACM)}, 61\penalty0 (1):\penalty0 1--23,
  2014.

\bibitem[Gray and Neuhoff(1998)]{gray1998quantization}
Robert~M. Gray and David~L. Neuhoff.
\newblock Quantization.
\newblock \emph{IEEE transactions on information theory}, 44\penalty0
  (6):\penalty0 2325--2383, 1998.

\bibitem[Oord et~al.(2017)Oord, Vinyals, and Kavukcuoglu]{oord2017neural}
Aaron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock \emph{arXiv preprint arXiv:1711.00937}, 2017.

\bibitem[Fey and Lenssen(2019)]{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock \emph{arXiv preprint arXiv:1903.02428}, 2019.

\bibitem[Li et~al.(2015)Li, Tarlow, Brockschmidt, and Zemel]{li2015gated}
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
\newblock Gated graph sequence neural networks.
\newblock In \emph{ICLR}, 2015.

\bibitem[Ding and He(2004)]{ding2004k}
Chris Ding and Xiaofeng He.
\newblock K-means clustering via principal component analysis.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~29, 2004.

\bibitem[Wu et~al.(2019)Wu, Souza, Zhang, Fifty, Yu, and
  Weinberger]{wu2019simplifying}
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
  Weinberger.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  6861--6871. PMLR, 2019.

\bibitem[Bojchevski et~al.(2020)Bojchevski, Klicpera, Perozzi, Kapoor, Blais,
  R{\'o}zemberczki, Lukasik, and G{\"u}nnemann]{bojchevski2020scaling}
Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin
  Blais, Benedek R{\'o}zemberczki, Michal Lukasik, and Stephan G{\"u}nnemann.
\newblock Scaling graph neural networks with approximate pagerank.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 2464--2473, 2020.

\bibitem[Rossi et~al.(2020)Rossi, Frasca, Chamberlain, Eynard, Bronstein, and
  Monti]{rossi2020sign}
Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael
  Bronstein, and Federico Monti.
\newblock Sign: Scalable inception graph neural networks.
\newblock \emph{arXiv preprint arXiv:2004.11198}, 2020.

\bibitem[Tailor et~al.(2020)Tailor, Fernandez-Marques, and
  Lane]{tailor2020degree}
Shyam~A Tailor, Javier Fernandez-Marques, and Nicholas~D Lane.
\newblock Degree-quant: Quantization-aware training for graph neural networks.
\newblock \emph{arXiv preprint arXiv:2008.05000}, 2020.

\bibitem[Feng et~al.(2020)Feng, Wang, Li, Yang, Peng, and
  Ding]{feng2020sgquant}
Boyuan Feng, Yuke Wang, Xu~Li, Shu Yang, Xueqiao Peng, and Yufei Ding.
\newblock Sgquant: Squeezing the last bit on graph neural networks with
  specialized quantization.
\newblock In \emph{2020 IEEE 32nd International Conference on Tools with
  Artificial Intelligence (ICTAI)}, pages 1044--1052. IEEE, 2020.

\bibitem[Razavi et~al.(2019)Razavi, Oord, and Vinyals]{razavi2019generating}
Ali Razavi, Aaron van~den Oord, and Oriol Vinyals.
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock \emph{arXiv preprint arXiv:1906.00446}, 2019.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
Lars Maal{\o}e, Marco Fraccaro, Valentin Li{\'e}vin, and Ole Winther.
\newblock Biva: A very deep hierarchy of latent variables for generative
  modeling.
\newblock \emph{arXiv preprint arXiv:1902.02102}, 2019.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{arXiv preprint arXiv:2006.09882}, 2020.

\bibitem[Baevski et~al.(2019)Baevski, Schneider, and Auli]{baevski2019vq}
Alexei Baevski, Steffen Schneider, and Michael Auli.
\newblock vq-wav2vec: Self-supervised learning of discrete speech
  representations.
\newblock \emph{arXiv preprint arXiv:1910.05453}, 2019.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock \emph{arXiv preprint arXiv:2006.11477}, 2020.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Geng et~al.(2021)Geng, Guo, Chen, Li, Wei, and Lin]{geng2021attention}
Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke~Wei, and Zhouchen Lin.
\newblock Is attention better than matrix decomposition.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wu and Flierl(2019)]{wu2019learning}
Hanwei Wu and Markus Flierl.
\newblock Learning product codebooks using vector-quantized autoencoders for
  image retrieval.
\newblock In \emph{2019 IEEE Global Conference on Signal and Information
  Processing (GlobalSIP)}, pages 1--5. IEEE, 2019.

\bibitem[Berthelot et~al.(2018)Berthelot, Raffel, Roy, and
  Goodfellow]{berthelot2018understanding}
David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow.
\newblock Understanding and improving interpolation in autoencoders via an
  adversarial regularizer.
\newblock \emph{arXiv preprint arXiv:1807.07543}, 2018.

\bibitem[Dasoulas et~al.(2021)Dasoulas, Scaman, and
  Virmaux]{dasoulas2021lipschitz}
George Dasoulas, Kevin Scaman, and Aladin Virmaux.
\newblock Lipschitz normalization for self-attention layers with application to
  graph neural networks.
\newblock \emph{arXiv preprint arXiv:2103.04886}, 2021.

\bibitem[Fey et~al.(2021)Fey, Lenssen, Weichert, and
  Leskovec]{fey2021gnnautoscale}
Matthias Fey, Jan~E Lenssen, Frank Weichert, and Jure Leskovec.
\newblock Gnnautoscale: Scalable and expressive graph neural networks via
  historical embeddings.
\newblock \emph{arXiv preprint arXiv:2106.05609}, 2021.

\end{thebibliography}
