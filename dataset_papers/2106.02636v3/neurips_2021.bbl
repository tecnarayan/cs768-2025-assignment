\begin{thebibliography}{128}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abu-El-Haija et~al.(2016)Abu-El-Haija, Kothari, Lee, Natsev, Toderici,
  Varadarajan, and Vijayanarasimhan]{abu2016youtube}
Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici,
  Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan.
\newblock Youtube-8m: A large-scale video classification benchmark.
\newblock \emph{arXiv preprint arXiv:1609.08675}, 2016.

\bibitem[Agrawal et~al.(2016{\natexlab{a}})Agrawal, Chandrasekaran, Batra,
  Parikh, and Bansal]{agrawal_sort_2016}
Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, and Mohit
  Bansal.
\newblock Sort {Story}: {Sorting} {Jumbled} {Images} and {Captions} into
  {Stories}.
\newblock In \emph{Proceedings of the 2016 {Conference} on {Empirical}
  {Methods} in {Natural} {Language} {Processing}}, pages 925--931,
  2016{\natexlab{a}}.

\bibitem[Agrawal et~al.(2016{\natexlab{b}})Agrawal, Nair, Abbeel, Malik, and
  Levine]{agrawal2016learning}
Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine.
\newblock Learning to poke by poking: experiential learning of intuitive
  physics.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 5092--5100, 2016{\natexlab{b}}.

\bibitem[Akbari et~al.(2021)Akbari, Yuan, Qian, Chuang, Chang, Cui, and
  Gong]{akbari2021vatt}
Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin
  Cui, and Boqing Gong.
\newblock {VATT:} transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock \emph{arXiv preprint arXiv:2104.11178}, 2021.

\bibitem[Alayrac et~al.(2016)Alayrac, Bojanowski, Agrawal, Sivic, Laptev, and
  Lacoste-Julien]{alayrac2016unsupervised}
Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan
  Laptev, and Simon Lacoste-Julien.
\newblock Unsupervised learning from narrated instruction videos.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4575--4583, 2016.

\bibitem[Alayrac et~al.(2017)Alayrac, Laptev, Sivic, and
  Lacoste-Julien]{alayrac2017joint}
Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien.
\newblock Joint discovery of object states and manipulation actions.
\newblock In \emph{ICCV}, 2017.

\bibitem[Alayrac et~al.(2020)Alayrac, Recasens, Schneider, Arandjelovi{\'c},
  Ramapuram, De~Fauw, Smaira, Dieleman, and Zisserman]{alayrac2020self}
Jean-Baptiste Alayrac, Adri{\`a} Recasens, Rosalia Schneider, Relja
  Arandjelovi{\'c}, Jason Ramapuram, Jeffrey De~Fauw, Lucas Smaira, Sander
  Dieleman, and Andrew Zisserman.
\newblock Self-supervised multimodal versatile networks.
\newblock \emph{arXiv preprint arXiv:2006.16228}, 2020.

\bibitem[Alberti et~al.(2019)Alberti, Ling, Collins, and
  Reitter]{alberti2019fusion}
Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter.
\newblock Fusion of detected objects in text for visual question answering.
\newblock \emph{arXiv preprint arXiv:1908.05054}, 2019.

\bibitem[Amrani et~al.(2020)Amrani, Ben-Ari, Rotman, and
  Bronstein]{amrani2020noise}
Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein.
\newblock Noise estimation using density estimation for self-supervised
  multimodal learning.
\newblock \emph{arXiv preprint arXiv:2003.03186}, 2020.

\bibitem[Anderson et~al.(2018)Anderson, He, Buehler, Teney, Johnson, Gould, and
  Zhang]{Anderson2017updown}
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
  Gould, and Lei Zhang.
\newblock Bottom-up and top-down attention for image captioning and visual
  question answering.
\newblock In \emph{CVPR}, 2018.

\bibitem[Battaglia et~al.(2016)Battaglia, Pascanu, Lai, Rezende, and
  kavukcuoglu]{battaglia2016interaction}
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo~Jimenez Rezende, and Koray
  kavukcuoglu.
\newblock Interaction networks for learning about objects, relations and
  physics.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 4509--4517, 2016.

\bibitem[Bender and Friedman(2019)]{bender2018data}
Emily Bender and Batya Friedman.
\newblock Data statements for nlp: Toward mitigating system bias and enabling
  better science.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2019.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 610--623, 2021.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and
  Vincent]{bengio2013representation}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 35\penalty0 (8):\penalty0 1798--1828, 2013.

\bibitem[Bishop(2018)]{bishop2018anxiety}
Sophie Bishop.
\newblock Anxiety, panic and self-optimization: Inequalities and the youtube
  algorithm.
\newblock \emph{Convergence}, 24\penalty0 (1):\penalty0 69--84, 2018.

\bibitem[Blei et~al.(2003)Blei, Ng, and Jordan]{blei2003latent}
David~M Blei, Andrew~Y Ng, and Michael~I Jordan.
\newblock Latent dirichlet allocation.
\newblock \emph{Journal of machine Learning research}, 3\penalty0
  (Jan):\penalty0 993--1022, 2003.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv e-prints}, pages arXiv--2108, 2021.

\bibitem[Carlini et~al.(2020)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, et~al.]{carlini2020extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock \emph{arXiv preprint arXiv:2012.07805}, 2020.

\bibitem[Chang et~al.(2019)Chang, Huang, Sui, Fei-Fei, and
  Niebles]{chang2019d3tw}
Chien-Yi Chang, De-An Huang, Yanan Sui, Li~Fei-Fei, and Juan~Carlos Niebles.
\newblock D3tw: Discriminative differentiable dynamic time warping for weakly
  supervised action alignment and segmentation.
\newblock In \emph{CVPR}, 2019.

\bibitem[Chaturvedi et~al.(2017)Chaturvedi, Peng, and
  Roth]{chaturvedi_story_2017}
Snigdha Chaturvedi, Haoruo Peng, and Dan Roth.
\newblock Story {Comprehension} for {Predicting} {What} {Happens} {Next}.
\newblock In \emph{Proceedings of the 2017 {Conference} on {Empirical}
  {Methods} in {Natural} {Language} {Processing}}, pages 1603--1614, 2017.

\bibitem[Chen et~al.(2021)Chen, Huang, He, Long, Zeng, Wen, Tan, and
  Gan]{chen2021rspnet}
Peihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen,
  Mingkui Tan, and Chuang Gan.
\newblock Rspnet: Relative speed perception for unsupervised video
  representation learning.
\newblock In \emph{AAAI}, 2021.

\bibitem[Chen et~al.(2019)Chen, Li, Yu, Kholy, Ahmed, Gan, Cheng, and
  Liu]{chen2019uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed~El Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock {UNITER}: Learning universal image-text representations.
\newblock \emph{arXiv preprint arXiv:1909.11740}, 2019.

\bibitem[Choi et~al.(2020)Choi, On, Heo, Seo, Jang, Lee, Lee, and
  Zhang]{choi2020dramaqa}
Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Seungchan
  Lee, Minsu Lee, and Byoung-Tak Zhang.
\newblock {DramaQA:} character-centered video story understanding with
  hierarchical qa.
\newblock \emph{arXiv preprint arXiv:2005.03356}, 2020.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning]{clark2019does}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D Manning.
\newblock What does bert look at? an analysis of bert’s attention.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
  and Interpreting Neural Networks for NLP}, pages 276--286, 2019.

\bibitem[Croft(2012)]{croft2012verbs}
William Croft.
\newblock \emph{Verbs: Aspect and causal structure}.
\newblock OUP Oxford, 2012.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Computer Vision and Pattern Recognition, 2009. CVPR 2009.
  IEEE Conference on}, pages 248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dixon(2008)]{dixon2008crime}
Travis~L Dixon.
\newblock Crime news and racialized beliefs: Understanding the relationship
  between local news viewing and perceptions of african americans and crime.
\newblock \emph{Journal of Communication}, 58\penalty0 (1):\penalty0 106--125,
  2008.

\bibitem[Dixon and Linz(2000)]{dixon2000overrepresentation}
Travis~L Dixon and Daniel Linz.
\newblock Overrepresentation and underrepresentation of african americans and
  latinos as lawbreakers on television news.
\newblock \emph{Journal of communication}, 50\penalty0 (2):\penalty0 131--154,
  2000.

\bibitem[D{\"o}ring and Mohseni(2019)]{doring2019male}
Nicola D{\"o}ring and M~Rohangis Mohseni.
\newblock Male dominance and sexism on youtube: results of three content
  analyses.
\newblock \emph{Feminist Media Studies}, 19\penalty0 (4):\penalty0 512--524,
  2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Epstein et~al.(2021)Epstein, Wu, Schmid, and Sun]{epstein2021learning}
Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun.
\newblock Learning temporal dynamics from cycles in narrated video.
\newblock \emph{arXiv preprint arXiv:2101.02337}, 2021.

\bibitem[Ferraro et~al.(2016)Ferraro, Mostafazadeh, Misra, Agrawal, Devlin,
  Girshick, He, Kohli, Batra, Zitnick, et~al.]{ferraro2016visual}
Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob
  Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C~Lawrence
  Zitnick, et~al.
\newblock Visual storytelling.
\newblock \emph{arXiv preprint arXiv:1604.03968}, 2016.

\bibitem[Finn et~al.(2016)Finn, Goodfellow, and Levine]{finn2016unsupervised}
Chelsea Finn, Ian Goodfellow, and Sergey Levine.
\newblock Unsupervised learning for physical interaction through video
  prediction.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 64--72, 2016.

\bibitem[Fouhey et~al.(2018)Fouhey, Kuo, Efros, and Malik]{fouhey2018lifestyle}
David~F Fouhey, Wei-cheng Kuo, Alexei~A Efros, and Jitendra Malik.
\newblock From lifestyle vlogs to everyday interactions.
\newblock In \emph{CVPR}, 2018.

\bibitem[Gan et~al.(2020)Gan, Chen, Li, Zhu, Cheng, and Liu]{gan2020large}
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu~Cheng, and Jingjing Liu.
\newblock Large-scale adversarial training for vision-and-language
  representation learning.
\newblock \emph{arXiv preprint arXiv:2006.06195}, 2020.

\bibitem[Gebru et~al.(2018)Gebru, Morgenstern, Vecchione, Vaughan, Wallach,
  Daume{\'e}~III, and Crawford]{gebru2018datasheets}
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer~Wortman Vaughan,
  Hanna Wallach, Hal Daume{\'e}~III, and Kate Crawford.
\newblock Datasheets for datasets.
\newblock \emph{arXiv preprint arXiv:1803.09010}, 2018.

\bibitem[Gilliam~Jr et~al.(1996)Gilliam~Jr, Iyengar, Simon, and
  Wright]{gilliam1996crime}
Franklin~D Gilliam~Jr, Shanto Iyengar, Adam Simon, and Oliver Wright.
\newblock Crime in black and white: The violent, scary world of local news.
\newblock \emph{Harvard International Journal of press/politics}, 1\penalty0
  (3):\penalty0 6--23, 1996.

\bibitem[Gordon and Van~Durme(2013)]{gordon2013reporting}
Jonathan Gordon and Benjamin Van~Durme.
\newblock Reporting bias and knowledge acquisition.
\newblock In \emph{Proceedings of the 2013 workshop on Automated knowledge base
  construction}, pages 25--30. ACM, 2013.

\bibitem[Green(2019)]{green2019good}
Ben Green.
\newblock Good” isn’t good enough.
\newblock In \emph{Proceedings of the AI for Social Good workshop at NeurIPS},
  2019.

\bibitem[Grice(1975)]{grice1975logic}
Herbert~P Grice.
\newblock Logic and conversation.
\newblock In \emph{Speech acts}, pages 41--58. Brill, 1975.

\bibitem[Haraway(1988)]{haraway1988situated}
Donna Haraway.
\newblock Situated knowledges: The science question in feminism and the
  privilege of partial perspective.
\newblock \emph{Feminist studies}, 14\penalty0 (3):\penalty0 575--599, 1988.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Heider(2014)]{heider2014white}
Don Heider.
\newblock \emph{White news: Why local news programs don't cover people of
  color}.
\newblock Routledge, 2014.

\bibitem[Heilbron et~al.(2015)Heilbron, Escorcia, Ghanem, and
  Niebles]{caba2015activitynet}
Fabian~Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan~Carlos Niebles.
\newblock Activitynet: A large-scale video benchmark for human activity
  understanding.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 961--970, 2015.

\bibitem[Hessel et~al.(2019)Hessel, Pang, Zhu, and
  Soricut]{hessel-etal-2019-case}
Jack Hessel, Bo~Pang, Zhenhai Zhu, and Radu Soricut.
\newblock A case study on combining {ASR} and visual features for generating
  instructional video captions.
\newblock In \emph{CoNLL}, November 2019.

\bibitem[Hessel et~al.(2020)Hessel, Zhu, Pang, and Soricut]{hessel2020beyond}
Jack Hessel, Zhenhai Zhu, Bo~Pang, and Radu Soricut.
\newblock Beyond instructional videos: Probing for more diverse visual-textual
  grounding on youtube.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Forbes, and
  Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Huang et~al.(2017)Huang, Lim, Fei-Fei, and
  Carlos~Niebles]{Huang_2017_CVPR}
De-An Huang, Joseph~J. Lim, Li~Fei-Fei, and Juan Carlos~Niebles.
\newblock Unsupervised visual-linguistic reference resolution in instructional
  videos.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, July 2017.

\bibitem[Huang et~al.(2016)Huang, Ferraro, Mostafazadeh, Misra, Agrawal,
  Devlin, Girshick, He, Kohli, Batra, Zitnick, Parikh, Vanderwende, Galley, and
  Mitchell]{huang2016visual}
Ting-Hao~Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra,
  Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli,
  Dhruv Batra, C.~Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel
  Galley, and Margaret Mitchell.
\newblock Visual storytelling.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1233--1239, San Diego, California, June 2016.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1147}.
\newblock URL \url{https://www.aclweb.org/anthology/N16-1147}.

\bibitem[Jain and Wallace(2019)]{jain2019attention}
Sarthak Jain and Byron~C Wallace.
\newblock Attention is not explanation.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 3543--3556, 2019.

\bibitem[Jang et~al.(2017)Jang, Song, Yu, Kim, and Kim]{jang2017tgif}
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim.
\newblock Tgif-qa: Toward spatio-temporal reasoning in visual question
  answering.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2017). Honolulu, Hawaii}, pages 2680--8, 2017.

\bibitem[Jiang et~al.(2020)Jiang, Misra, Rohrbach, Learned-Miller, and
  Chen]{jiang2020defense}
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
  Chen.
\newblock In defense of grid features for visual question answering.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10267--10276, 2020.

\bibitem[Joshi et~al.(2020)Joshi, Chen, Liu, Weld, Zettlemoyer, and
  Levy]{joshi2020spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer
  Levy.
\newblock Spanbert: Improving pre-training by representing and predicting
  spans.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 64--77, 2020.

\bibitem[Kang et~al.(2015)Kang, Dabbish, Fruchter, and Kiesler]{kang2015my}
Ruogu Kang, Laura Dabbish, Nathaniel Fruchter, and Sara Kiesler.
\newblock “my data just goes everywhere:” user mental models of the
  internet and implications for privacy and security.
\newblock In \emph{Eleventh Symposium On Usable Privacy and Security
  ($\{$SOUPS$\}$ 2015)}, pages 39--52, 2015.

\bibitem[Kim et~al.(2020)Kim, Jeong, Kim, Kang, and
  Kwak]{Kim2020SelfsupervisedPA}
Seonhoon Kim, Seohyeong Jeong, Eun-Byul Kim, Inho Kang, and Nojun Kwak.
\newblock Self-supervised pre-training and contrastive representation learning
  for multiple-choice video qa.
\newblock \emph{ArXiv}, abs/2009.08043, 2020.

\bibitem[Kim et~al.(2021)Kim, Son, and Kim]{kim2021vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock Vilt: Vision-and-language transformer without convolution or region
  supervision.
\newblock \emph{arXiv preprint arXiv:2102.03334}, 2021.

\bibitem[Kitani et~al.(2012)Kitani, Ziebart, Bagnell, and
  Hebert]{kitani2012activity}
Kris~M Kitani, Brian~D Ziebart, James~Andrew Bagnell, and Martial Hebert.
\newblock Activity forecasting.
\newblock In \emph{European Conference on Computer Vision}, pages 201--214.
  Springer, 2012.

\bibitem[Krishna et~al.(2017{\natexlab{a}})Krishna, Hata, Ren, Fei-Fei, and
  Niebles]{krishna_dense-captioning_2017}
Ranjay Krishna, Kenji Hata, Frederic Ren, Li~Fei-Fei, and Juan~Carlos Niebles.
\newblock Dense-{Captioning} {Events} in {Videos}.
\newblock In \emph{International {Conference} on {Computer} {Vision} ({ICCV})},
  2017{\natexlab{a}}.

\bibitem[Krishna et~al.(2017{\natexlab{b}})Krishna, Zhu, Groth, Johnson, Hata,
  Kravitz, Chen, Kalantidis, Li, Shamma, et~al.]{visualgenome}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{International Journal of Computer Vision}, 123\penalty0
  (1):\penalty0 32--73, 2017{\natexlab{b}}.

\bibitem[Kuehne et~al.(2011)Kuehne, Jhuang, Garrote, Poggio, and
  Serre]{Kuehne11}
H.~Kuehne, H.~Jhuang, E.~Garrote, T.~Poggio, and T.~Serre.
\newblock {HMDB}: a large video database for human motion recognition.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision (ICCV)}, 2011.

\bibitem[Kuehne et~al.(2019)Kuehne, Iqbal, Richard, and Gall]{kuehne2019mining}
Hilde Kuehne, Ahsan Iqbal, Alexander Richard, and Juergen Gall.
\newblock Mining youtube-a dataset for learning fine-grained action concepts
  from webly supervised video data.
\newblock \emph{arXiv preprint arXiv:1906.01012}, 2019.

\bibitem[Kuhn(1955)]{kuhn1955hungarian}
Harold~W Kuhn.
\newblock The hungarian method for the assignment problem.
\newblock \emph{Naval research logistics quarterly}, 2\penalty0 (1-2):\penalty0
  83--97, 1955.

\bibitem[Lei et~al.(2018)Lei, Yu, Bansal, and Berg]{lei2018tvqa}
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara~L Berg.
\newblock Tvqa: Localized, compositional video question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Lei et~al.(2019)Lei, Yu, Berg, and Bansal]{lei2019tvqa}
Jie Lei, Licheng Yu, Tamara~L Berg, and Mohit Bansal.
\newblock Tvqa+: Spatio-temporal grounding for video question answering.
\newblock In \emph{Tech Report, arXiv}, 2019.

\bibitem[Lei et~al.(2020)Lei, Yu, Berg, and Bansal]{lei2020more}
Jie Lei, Licheng Yu, Tamara~L Berg, and Mohit Bansal.
\newblock What is more likely to happen next? video-and-language future event
  prediction.
\newblock \emph{arXiv preprint arXiv:2010.07999}, 2020.

\bibitem[Lei et~al.(2021)Lei, Li, Zhou, Gan, Berg, Bansal, and
  Liu]{lei2021less}
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara~L Berg, Mohit Bansal, and
  Jingjing Liu.
\newblock Less is more: Clipbert for video-and-language learning via sparse
  sampling.
\newblock \emph{arXiv preprint arXiv:2102.06183}, 2021.

\bibitem[Li et~al.(2020)Li, Duan, Fang, Gong, Jiang, and Zhou]{li2020unicoder}
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou.
\newblock Unicoder-vl: A universal encoder for vision and language by
  cross-modal pre-training.
\newblock In \emph{AAAI}, pages 11336--11344, 2020.

\bibitem[Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang]{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock \emph{arXiv preprint arXiv:1908.03557}, 2019.

\bibitem[Li et~al.(2018)Li, Li, and Vasconcelos]{li2018resound}
Yingwei Li, Yi~Li, and Nuno Vasconcelos.
\newblock Resound: Towards action recognition without representation bias.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 513--528, 2018.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft {COCO}: Common objects in context.
\newblock In \emph{European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2016)Lu, Krishna, Bernstein, and Fei-Fei]{lu2016visual}
Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li~Fei-Fei.
\newblock Visual relationship detection with language priors.
\newblock In \emph{European Conference on Computer Vision}, 2016.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock {ViLBERT}: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13--23, 2019.

\bibitem[Maharaj et~al.(2017)Maharaj, Ballas, Rohrbach, Courville, and
  Pal]{maharaj2017dataset}
Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron~C Courville, and
  Christopher~Joseph Pal.
\newblock A dataset and exploration of models for understanding video data
  through fill-in-the-blank question-answering.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2017.
\newblock URL
  \url{http://openaccess.thecvf.com/content_cvpr_2017/papers/Maharaj_A_Dataset_and_CVPR_2017_paper.pdf}.

\bibitem[Malmaud et~al.(2015)Malmaud, Huang, Rathod, Johnston, Rabinovich, and
  Murphy]{malmaud2015s}
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew
  Rabinovich, and Kevin Murphy.
\newblock What's cookin'? interpreting cooking videos using text, speech and
  vision.
\newblock In \emph{NAACL}, 2015.

\bibitem[Marwick and {b}oyd(2014)]{marwick2014networked}
Alice~E Marwick and {d}anah {b}oyd.
\newblock Networked privacy: How teenagers negotiate context in social media.
\newblock \emph{New media \& society}, 16\penalty0 (7):\penalty0 1051--1067,
  2014.

\bibitem[McCallum(2002)]{mccallum2002mallet}
Andrew~Kachites McCallum.
\newblock Mallet: A machine learning for language toolkit.
\newblock 2002.
\newblock URL \url{http://mallet.cs.umass.edu}.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and
  Sivic]{miech2019howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
  Laptev, and Josef Sivic.
\newblock How{T}o100{M}: {L}earning a {T}ext-{V}ideo {E}mbedding by {W}atching
  {H}undred {M}illion {N}arrated {V}ideo {C}lips.
\newblock In \emph{ICCV}, 2019.

\bibitem[Miech et~al.(2020)Miech, Alayrac, Smaira, Laptev, Sivic, and
  Zisserman]{miech2019end}
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic,
  and Andrew Zisserman.
\newblock End-to-end learning of visual representations from uncurated
  instructional videos.
\newblock In \emph{CVPR}, 2020.

\bibitem[Misra et~al.(2016)Misra, Zitnick, and Hebert]{misra2016shuffle}
Ishan Misra, C~Lawrence Zitnick, and Martial Hebert.
\newblock Shuffle and learn: unsupervised learning using temporal order
  verification.
\newblock In \emph{European Conference on Computer Vision}, pages 527--544.
  Springer, 2016.

\bibitem[Molyneaux et~al.(2008)Molyneaux, O’Donnell, Gibson, Singer,
  et~al.]{molyneaux2008exploring}
Heather Molyneaux, Susan O’Donnell, Kerri Gibson, Janice Singer, et~al.
\newblock Exploring the gender divide on youtube: An analysis of the creation
  and reception of vlogs.
\newblock \emph{American Communication Journal}, 10\penalty0 (2):\penalty0
  1--14, 2008.

\bibitem[Moriya et~al.(2019)Moriya, Sanabria, Metze, and
  Jones]{moriya2019grounding}
Yasufumi Moriya, Ramon Sanabria, Florian Metze, and Gareth~JF Jones.
\newblock Grounding object detections with transcriptions.
\newblock \emph{arXiv preprint arXiv:1906.06147}, 2019.

\bibitem[M{\"u}ller(2007)]{muller2007dynamic}
Meinard M{\"u}ller.
\newblock Dynamic time warping.
\newblock \emph{Information retrieval for music and motion}, pages 69--84,
  2007.

\bibitem[Palaskar et~al.(2019)Palaskar, Libovick{\`y}, Gella, and
  Metze]{palaskar2019multimodal}
Shruti Palaskar, Jindrich Libovick{\`y}, Spandana Gella, and Florian Metze.
\newblock Multimodal abstractive summarization for how2 videos.
\newblock \emph{arXiv preprint arXiv:1906.07901}, 2019.

\bibitem[Prest et~al.(2012)Prest, Leistner, Civera, Schmid, and
  Ferrari]{prest2012learning}
Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and
  Vittorio Ferrari.
\newblock Learning object class detectors from weakly annotated video.
\newblock In \emph{2012 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 3282--3289. IEEE, 2012.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019gpttwo}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock Technical report, OpenAI, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Ribeiro et~al.(2020)Ribeiro, Ottoni, West, Almeida, and
  Meira~Jr]{ribeiro2020auditing}
Manoel~Horta Ribeiro, Raphael Ottoni, Robert West, Virg{\'\i}lio~AF Almeida,
  and Wagner Meira~Jr.
\newblock Auditing radicalization pathways on youtube.
\newblock In \emph{Proceedings of the 2020 conference on fairness,
  accountability, and transparency}, pages 131--141, 2020.

\bibitem[Richards(2012)]{richards2012dangers}
Neil~M Richards.
\newblock The dangers of surveillance.
\newblock \emph{Harv. L. Rev.}, 126:\penalty0 1934, 2012.

\bibitem[Rohrbach et~al.(2017)Rohrbach, Torabi, Rohrbach, Tandon, Pal,
  Larochelle, Courville, and Schiele]{lsmdc}
Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Chris Pal, Hugo
  Larochelle, Aaron Courville, and Bernt Schiele.
\newblock Movie description.
\newblock \emph{International Journal of Computer Vision}, 2017.
\newblock URL
  \url{http://link.springer.com/article/10.1007/s11263-016-0987-1?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst}.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4510--4520, 2018.

\bibitem[Savsunenko()]{savsunenko2018tfimage}
Oleksandr Savsunenko.
\newblock How tensorflow’s tf.image.resize stole 60 days of my life.
\newblock Technical report, Hacker Noon.

\bibitem[Schank and Abelson(1975)]{Schank1975}
Roger~C. Schank and Robert~P. Abelson.
\newblock Scripts, plans, and knowledge.
\newblock In \emph{Proceedings of the 4th International Joint Conference on
  Artificial Intelligence - Volume 1}, IJCAI'75, pages 151--157, San Francisco,
  CA, USA, 1975. Morgan Kaufmann Publishers Inc.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1624626.1624649}.

\bibitem[Sener et~al.(2015)Sener, Zamir, Savarese, and
  Saxena]{sener2015unsupervised}
Ozan Sener, Amir~R Zamir, Silvio Savarese, and Ashutosh Saxena.
\newblock Unsupervised semantic parsing of video collections.
\newblock In \emph{ICCV}, 2015.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennrich2016neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1715--1725,
  2016.

\bibitem[Serrano and Smith(2019)]{serrano2019attention}
Sofia Serrano and Noah~A Smith.
\newblock Is attention interpretable?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2931--2951, 2019.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565,
  2018.

\bibitem[Shen et~al.(2021)Shen, Li, Tan, Bansal, Rohrbach, Chang, Yao, and
  Keutzer]{shen2021much}
Sheng Shen, Liunian~Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei
  Chang, Zhewei Yao, and Kurt Keutzer.
\newblock How much can clip benefit vision-and-language tasks?
\newblock \emph{arXiv preprint arXiv:2107.06383}, 2021.

\bibitem[Shi et~al.(2019)Shi, Ji, Liang, Duan, Chen, Niu, and
  Zhou]{shi2019dense}
Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen, Zhendong Niu, and Ming
  Zhou.
\newblock Dense procedure captioning in narrated instructional videos.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 6382--6391, 2019.

\bibitem[Shi and Demberg(2019)]{shi2019next}
Wei Shi and Vera Demberg.
\newblock Next sentence prediction helps implicit discourse relation
  classification within and across domains.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 5794--5800, 2019.

\bibitem[Soomro et~al.(2012)Soomro, Zamir, and Shah]{soomro2012dataset}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock A dataset of 101 human action classes from videos in the wild.
\newblock \emph{Center for Research in Computer Vision}, 2\penalty0 (11), 2012.

\bibitem[Strangelove(2020)]{strangelove2020watching}
Michael Strangelove.
\newblock \emph{Watching YouTube}.
\newblock University of Toronto press, 2020.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 3645--3650, 2019.

\bibitem[Sun et~al.(2019{\natexlab{a}})Sun, Baradel, Murphy, and
  Schmid]{sun2019contrastive}
Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid.
\newblock Contrastive bidirectional transformer for temporal representation
  learning.
\newblock \emph{arXiv preprint arXiv:1906.05743}, 2019{\natexlab{a}}.

\bibitem[Sun et~al.(2019{\natexlab{b}})Sun, Myers, Vondrick, Murphy, and
  Schmid]{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock {VideoBERT}: A joint model for video and language representation
  learning.
\newblock In \emph{ICCV}, 2019{\natexlab{b}}.

\bibitem[Tan and Bansal(2019)]{tan2019lxmert}
Hao Tan and Mohit Bansal.
\newblock {LXMERT}: Learning cross-modality encoder representations from
  transformers.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Tang et~al.(2021)Tang, Lei, and Bansal]{tang2021decembert}
Zineng Tang, Jie Lei, and Mohit Bansal.
\newblock Decembert: Learning from noisy instructional videos via dense
  captions and entropy minimization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2415--2426, 2021.

\bibitem[Torabi et~al.(2016)Torabi, Tandon, and
  Sigal]{lsmdc2016MovieAnnotationRetrieval}
Atousa Torabi, Niket Tandon, and Leon Sigal.
\newblock Learning language-visual embedding for movie understanding with
  natural-language.
\newblock \emph{arXiv preprint}, 2016.
\newblock URL \url{http://arxiv.org/pdf/1609.08124v1.pdf}.

\bibitem[Torralba and Efros(2011)]{torralba2011unbiased}
Antonio Torralba and Alexei~A Efros.
\newblock Unbiased look at dataset bias.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
  Conference on}, pages 1521--1528. IEEE, 2011.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Walker et~al.(2016)Walker, Doersch, Gupta, and
  Hebert]{walker2016uncertain}
Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert.
\newblock An uncertain future: Forecasting from static images using variational
  autoencoders.
\newblock In \emph{European Conference on Computer Vision}, pages 835--851.
  Springer, 2016.

\bibitem[Waseem et~al.(2021)Waseem, Lulz, Bingel, and
  Augenstein]{waseem2021disembodied}
Zeerak Waseem, Smarika Lulz, Joachim Bingel, and Isabelle Augenstein.
\newblock Disembodied machine learning: On the illusion of objectivity in nlp.
\newblock \emph{arXiv preprint arXiv:2101.11974}, 2021.

\bibitem[Wei et~al.(2018)Wei, Lim, Zisserman, and Freeman]{wei2018learning}
Donglai Wei, Joseph~J Lim, Andrew Zisserman, and William~T Freeman.
\newblock Learning and using the arrow of time.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8052--8060, 2018.

\bibitem[Wiegreffe and Pinter(2019)]{wiegreffe2019attention}
Sarah Wiegreffe and Yuval Pinter.
\newblock Attention is not not explanation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 11--20, 2019.

\bibitem[Xu et~al.(2017)Xu, Zhao, Xiao, Wu, Zhang, He, and Zhuang]{xu2017video}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In \emph{Proceedings of the 25th ACM international conference on
  Multimedia}, pages 1645--1653, 2017.

\bibitem[Yang et~al.(2020)Yang, Miech, Sivic, Laptev, and Schmid]{yang2020just}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock \emph{arXiv preprint arXiv:2012.00451}, 2020.

\bibitem[Yu et~al.(2020)Yu, Tang, Yin, Sun, Tian, Wu, and Wang]{yu2020ernie}
Fei Yu, Jiji Tang, Weichong Yin, Yu~Sun, Hao Tian, Hua Wu, and Haifeng Wang.
\newblock Ernie-vil: Knowledge enhanced vision-language representations through
  scene graph.
\newblock \emph{arXiv preprint arXiv:2006.16934}, 2020.

\bibitem[Yu et~al.(2014)Yu, Jiang, and Hauptmann]{yu2014instructional}
Shoou-I Yu, Lu~Jiang, and Alexander Hauptmann.
\newblock Instructional videos for unsupervised harvesting and learning of
  action examples.
\newblock In \emph{ACM MM}, 2014.

\bibitem[Yu et~al.(2018)Yu, Kim, and Kim]{yu2018joint}
Youngjae Yu, Jongseok Kim, and Gunhee Kim.
\newblock A joint sequence fusion model for video question answering and
  retrieval.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 471--487, 2018.

\bibitem[Yu et~al.(2019)Yu, Xu, Yu, Yu, Zhao, Zhuang, and
  Tao]{yu2019activityqa}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng
  Tao.
\newblock {ActivityNet-QA:} a dataset for understanding complex web videos via
  question answering.
\newblock In \emph{AAAI}, pages 9127--9134, 2019.

\bibitem[Zellers et~al.(2019{\natexlab{a}})Zellers, Bisk, Farhadi, and
  Choi]{zellers2019recognition}
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock From recognition to cognition: Visual commonsense reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6720--6731, 2019{\natexlab{a}}.

\bibitem[Zellers et~al.(2019{\natexlab{b}})Zellers, Holtzman, Rashkin, Bisk,
  Farhadi, Roesner, and Choi]{zellers2019grover}
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
  Franziska Roesner, and Yejin Choi.
\newblock Defending against neural fake news.
\newblock In \emph{Advances in Neural Information Processing Systems 32},
  2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Li, Hu, Yang, Zhang, Wang, Choi, and
  Gao]{zhang2021vinvl}
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
  Yejin Choi, and Jianfeng Gao.
\newblock Vinvl: Revisiting visual representations in vision-language models.
\newblock \emph{arXiv preprint arXiv:2101.00529}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Jiang, Miura, Manning, and
  Langlotz]{zhang2020contrastive}
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher~D Manning, and Curtis~P
  Langlotz.
\newblock Contrastive learning of medical visual representations from paired
  images and text.
\newblock \emph{arXiv preprint arXiv:2010.00747}, 2020.

\bibitem[Zhu and Yang(2020)]{zhuactbert}
Linchao Zhu and Yi~Yang.
\newblock {ActBERT}: Learning global-local video-text representations.
\newblock In \emph{CVPR}, 2020.

\bibitem[Zuboff(2015)]{zuboff2015big}
Shoshana Zuboff.
\newblock Big other: surveillance capitalism and the prospects of an
  information civilization.
\newblock \emph{Journal of Information Technology}, 30\penalty0 (1):\penalty0
  75--89, 2015.

\end{thebibliography}
