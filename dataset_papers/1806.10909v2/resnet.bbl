\begin{thebibliography}{10}

\bibitem{arora2018optimization}
S.~Arora, N.~Cohen, and E.~Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock {\em arXiv:1802.06509}, 2018.

\bibitem{barron1993universal}
A.~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information theory}, 39(3):930--945, 1993.

\bibitem{beise2018on}
H.~Beise, S.~D. Da~Cruz, and U.~Schroder.
\newblock On decision regions of narrow deep neural networks.
\newblock {\em arXiv:1807.01194}, 2018.

\bibitem{brutzkus2017sgd}
A.~Brutzkus, A.~Globerson, E.~Malach, and S.~Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem{choromanska2015loss}
A.~Choromanska, M.~Henaff, M.~Mathieu, G.~B. Arous, and Y.~LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In {\em The International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2015.

\bibitem{cohen2016expressive}
N.~Cohen, O.~Sharir, and A.~Shashua.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock In {\em Conference on Learning Theory (COLT)}, 2016.

\bibitem{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314,
  1989.

\bibitem{du2017gradient}
S.~S. Du, J.~Lee, Y.~Tian, B.~Poczos, and A.~Singh.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock {\em arXiv preprint arXiv:1712.00779}, 2017.

\bibitem{eldan2016power}
R.~Eldan and O.~Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In {\em Conference on Learning Theory (COLT)}, 2016.

\bibitem{funahashi1989approximate}
K.~Funahashi.
\newblock On the approximate realization of continuous mappings by neural
  networks.
\newblock {\em Neural networks}, 2(3):183--192, 1989.

\bibitem{hanin2017approximating}
B.~Hanin and M.~Sellke.
\newblock Approximating continuous functions by relu nets of minimal width.
\newblock {\em arXiv:1710.11278}, 2017.

\bibitem{hardt2016identity}
M.~Hardt and T.~Ma.
\newblock Identity matters in deep learning.
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE conference on computer vision and pattern recognition
  (CVPR)}, 2016.

\bibitem{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2016.

\bibitem{hornik1989multilayer}
K.~Hornik, M.~Stinchcombe, and H.~White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural networks}, 2(5):359--366, 1989.

\bibitem{kawaguchi2016deep}
K.~Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem{alexnet}
A.~Krizhevsky, I.~Sutskever, and G.~Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 1097--1105, 2012.

\bibitem{kuurkova1992kolmogorov}
V.~K{u}rkov{\'a}.
\newblock Kolmogorov's theorem and multilayer neural networks.
\newblock {\em Neural networks}, 5(3):501--506, 1992.

\bibitem{lenet}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{liang2016deep}
S.~Liang and R.~Srikant.
\newblock Why deep neural networks for function approximation?
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem{lu2017expressive}
Z.~Lu, H.~Pu, F.~Wang, Z.~Hu, and L.~Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem{mhaskar1996neural}
H.~N. Mhaskar.
\newblock Neural networks for optimal approximation of smooth and analytic
  functions.
\newblock {\em Neural computation}, 8(1):164--177, 1996.

\bibitem{mhaskar2016deep}
H.~N. Mhaskar and T.~Poggio.
\newblock Deep vs. shallow networks: An approximation theory perspective.
\newblock {\em Analysis and Applications}, 14(06):829--848, 2016.

\bibitem{nguyen2017loss}
Q.~Nguyen and M.~Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In {\em Proceedings of the International Conferences on Machine
  Learning (ICML)}, 2017.

\bibitem{rolnick2018the}
D.~Rolnick and M.~Tegmark.
\newblock The power of deeper networks for expressing natural functions.
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem{shalev2017weight}
S.~Shalev-Shwartz, O.~Shamir, and S.~Shammah.
\newblock Weight sharing is crucial to succesful optimization.
\newblock {\em arXiv:1706.00687}, 2017.

\bibitem{2018arXiv180406739S}
O.~Shamir.
\newblock Are resnets provably better than linear predictors?
\newblock {\em arXiv:1804.06739}, 2018.

\bibitem{vgg}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem{dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em The Journal of Machine Learning Research (JMLR)},
  15(1):1929--1958, 2014.

\bibitem{szegedygoing}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, A.~Rabinovich, J.~Rick~Chang, et~al.
\newblock Going deeper with convolutions.
\newblock In {\em IEEE conference on computer vision and pattern recognition
  (CVPR)}, 2015.

\bibitem{szymanski2014deep}
L.~Szymanski and B.~McCane.
\newblock Deep networks are effective encoders of periodicity.
\newblock {\em IEEE transactions on neural networks and learning systems},
  25(10):1816--1827, 2014.

\bibitem{telgarsky2016benefits}
M.~Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In {\em Conference on Learning Theory (COLT)}, 2016.

\bibitem{yarotsky2017error}
D.~Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\bibitem{yarotsky2018optimal}
D.~Yarotsky.
\newblock Optimal approximation of continuous functions by very deep relu
  networks.
\newblock {\em arXiv preprint arXiv:1802.03620}, 2018.

\bibitem{yuSrJa17}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Global optimality conditions for deep neural networks.
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em The International Conference on Learning Representations
  (ICLR)}, 2016.

\end{thebibliography}
