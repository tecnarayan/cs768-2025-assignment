
@misc{khalifePowerGraphNeural2024,
	title = {On the power of graph neural networks and the role of the activation function},
	doi = {10.48550/arXiv.2307.04661},
	abstract = {In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. It was also known prior to our work that with ReLU (piecewise linear) activations, bounded GNNs are weaker than unbounded GNNs [ACI+22]. Our approach adds to this result by extending it to handle any piecewise polynomial activation function, which goes towards answering an open question formulated by [2021, Grohe] more completely. Our second result states that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of depth two (our results hold for activations like the sigmoid, hyperbolic tan and others). This shows how the power of graph neural networks can change drastically if one changes the activation function of the neural networks. The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Khalife, Sammy and Basu, Amitabh},
	month = may,
	year = {2024},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{telgarskyNeuralNetworksRational2017,
	title = {Neural {Networks} and {Rational} {Functions}},
	abstract = {Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degree O(polylog(1/ϵ))O(polylog(1/ϵ))O(polylog(1/{\textbackslash}epsilon)) which is ϵϵ{\textbackslash}epsilon-close, and similarly for any rational function there exists a ReLU network of size O(polylog(1/ϵ))O(polylog(1/ϵ))O(polylog(1/{\textbackslash}epsilon)) which is ϵϵ{\textbackslash}epsilon-close. By contrast, polynomials need degree Ω(poly(1/ϵ))Ω(poly(1/ϵ)){\textbackslash}Omega(poly(1/{\textbackslash}epsilon)) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Telgarsky, Matus},
	month = jul,
	year = {2017},
	pages = {3387--3393},
}

@misc{casoRenormalizedGraphNeural2023,
	title = {Renormalized {Graph} {Neural} {Networks}},
	doi = {10.48550/arXiv.2306.00707},
	abstract = {Graph Neural Networks (GNNs) have become essential for studying complex data, particularly when represented as graphs. Their value is underpinned by their ability to reflect the intricacies of numerous areas, ranging from social to biological networks. GNNs can grapple with non-linear behaviors, emerging patterns, and complex connections; these are also typical characteristics of complex systems. The renormalization group (RG) theory has emerged as the language for studying complex systems. It is recognized as the preferred lens through which to study complex systems, offering a framework that can untangle their intricate dynamics. Despite the clear benefits of integrating RG theory with GNNs, no existing methods have ventured into this promising territory. This paper proposes a new approach that applies RG theory to devise a novel graph rewiring to improve GNNs' performance on graph-related tasks. We support our proposal with extensive experiments on standard benchmarks and baselines. The results demonstrate the effectiveness of our method and its potential to remedy the current limitations of GNNs. Finally, this paper marks the beginning of a new research direction. This path combines the theoretical foundations of RG, the magnifying glass of complex systems, with the structural capabilities of GNNs. By doing so, we aim to enhance the potential of GNNs in modeling and unraveling the complexities inherent in diverse systems.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Caso, Francesco and Trappolini, Giovanni and Bacciu, Andrea and Liò, Pietro and Silvestri, Fabrizio},
	month = jun,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability},
}

@inproceedings{zhaoPairNormTacklingOversmoothing2020,
	title = {{PairNorm}: {Tackling} {Oversmoothing} in {GNNs}},
	shorttitle = {{PairNorm}},
	abstract = {The performance of graph neural nets (GNNs) is known to gradually decrease with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to quantify oversmoothing. Our main contribution is PairNorm, a novel normalization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too similar. What is more, PairNorm is fast, easy to implement without any change to network architecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that PairNorm makes deeper GCN, GAT, and SGC models more robust against oversmoothing, and significantly boosts performance for a new problem setting that benefits from deeper GNNs. Code is available at https://github.com/LingxiaoShawn/PairNorm.},
	urldate = {2024-08-06},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Zhao, Lingxiao and Akoglu, Leman},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{borgwardtProteinFunctionPrediction2005,
	title = {Protein function prediction via graph kernels},
	volume = {21 Suppl 1},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/bti1007},
	abstract = {MOTIVATION: Computational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.
RESULTS: Our graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.
AVAILABILITY: More information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.},
	language = {eng},
	journal = {Bioinformatics (Oxford, England)},
	author = {Borgwardt, Karsten M. and Ong, Cheng Soon and Schönauer, Stefan and Vishwanathan, S. V. N. and Smola, Alex J. and Kriegel, Hans-Peter},
	month = jun,
	year = {2005},
	keywords = {Algorithms, Computational Biology, Databases, Protein, Enzymes, Models, Statistical, Protein Conformation, Protein Structure, Secondary, Sequence Analysis, Protein, Software},
	pages = {i47--56},
}

@inproceedings{morrisTUDatasetCollectionBenchmark2020,
	title = {{TUDataset}: {A} collection of benchmark datasets for learning with graphs},
	url = {www.graphlearning.io},
	abstract = {Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks. However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging, consequently hindering advancements in this area. To address this, we introduce the TUDATASET for graph classiﬁcation and regression. The collection consists of over 120 datasets of varying sizes from a wide range of applications. We provide Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. Here, we give an overview of the datasets, standardized evaluation procedures, and provide baseline experiments. All datasets are available at www.graphlearning.io. The experiments are fully reproducible from the code available at www.github.com/chrsmrrs/ tudataset.},
	language = {en},
	booktitle = {{ICML} 2020 {Workshop} on {Graph} {Representation} {Learning} and {Beyond} ({GRL}+ 2020)},
	author = {Morris, Christopher and Kriege, Nils M and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
	year = {2020},
}

@article{debnathStructureactivityRelationshipMutagenic1991,
	title = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. {Correlation} with molecular orbital energies and hydrophobicity},
	volume = {34},
	issn = {0022-2623},
	doi = {10.1021/jm00106a046},
	number = {2},
	urldate = {2024-08-05},
	journal = {Journal of Medicinal Chemistry},
	author = {Debnath, Asim Kumar and Lopez de Compadre, Rosa L. and Debnath, Gargi and Shusterman, Alan J. and Hansch, Corwin},
	month = feb,
	year = {1991},
	pages = {786--797},
}

@article{chenLearningGraphNormalization2022,
	title = {Learning graph normalization for graph neural networks},
	volume = {493},
	issn = {0925-2312},
	abstract = {Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and node representations in each layer are computed through propagating and aggregating the neighboring node features. To effectively train a GNN with multiple layers, normalization techniques are necessary. Though existing normalization techniques have achieved good results in helping GNNs training, but they seldom consider the structure information of the graph. In this paper, we propose two graph-aware normalization techniques, namely adjacency-wise normalization and graph-wise normalization, which fully take into account the structure information of the graph. Furthermore, we propose a novel approach, termed Attentive Graph Normalization (AGN), which learns a weighted combination of multiple graph-aware normalization methods, aiming to automatically select the optimal combination of multiple normalization methods for a specific task. We conduct extensive experiments on eleven benchmark datasets, including three single-graph and eight multiple-graph datasets, and the experimental results provide a comprehensive evaluation on the effectiveness of our proposals.},
	urldate = {2024-09-23},
	journal = {Neurocomputing},
	author = {Chen, Yihao and Tang, Xin and Qi, Xianbiao and Li, Chun-Guang and Xiao, Rong},
	month = jul,
	year = {2022},
	keywords = {Attentive graph normalization, Graph neural network, Graph normalization, Normalization method},
	pages = {613--625},
}

@article{luanRevisitingHeterophilyGraph2022,
	title = {Revisiting {Heterophily} {For} {Graph} {Neural} {Networks}},
	volume = {35},
	language = {en},
	urldate = {2024-10-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Luan, Sitao and Hua, Chenqing and Lu, Qincheng and Zhu, Jiaqi and Zhao, Mingde and Zhang, Shuyuan and Chang, Xiao-Wen and Precup, Doina},
	month = dec,
	year = {2022},
	pages = {1362--1375},
}

@inproceedings{ruschGraphCoupledOscillatorNetworks2022,
	address = {Baltimore, Maryland, USA},
	title = {Graph-{Coupled} {Oscillator} {Networks}},
	volume = {162},
	abstract = {We propose Graph-Coupled Oscillator Networks (GraphCON), a novel framework for deep learning on graphs. It is based on discretizations of a second-order system of ordinary differential equations (ODEs), which model a network of nonlinear controlled and damped oscillators, coupled via the adjacency structure of the underlying graph. The ﬂexibility of our framework permits any basic GNN layer (e.g. convolutional or attentional) as the coupling function, from which a multi-layer deep neural network is built up via the dynamics of the proposed ODEs. We relate the oversmoothing problem, commonly encountered in GNNs, to the stability of steady states of the underlying ODE and show that zero-Dirichlet energy steady states are not stable for our proposed ODEs. This demonstrates that the proposed framework mitigates the oversmoothing problem. Moreover, we prove that GraphCON mitigates the exploding and vanishing gradients problem to facilitate training of deep multi-layer GNNs. Finally, we show that our approach offers competitive performance with respect to the state-of-the-art on a variety of graphbased learning tasks.},
	language = {en},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rusch, T Konstantin and Chamberlain, Benjamin P and Rowbottom, James and Mishra, Siddhartha and Bronstein, Michael M},
	year = {2022},
}

@misc{liGNNSKANHarnessingPower2024,
	title = {{GNN}-{SKAN}: {Harnessing} the {Power} of {SwallowKAN} to {Advance} {Molecular} {Representation} {Learning} with {GNNs}},
	shorttitle = {{GNN}-{SKAN}},
	url = {http://arxiv.org/abs/2408.01018},
	doi = {10.48550/arXiv.2408.01018},
	abstract = {Effective molecular representation learning is crucial for advancing molecular property prediction and drug design. Mainstream molecular representation learning approaches are based on Graph Neural Networks (GNNs). However, these approaches struggle with three significant challenges: insufficient annotations, molecular diversity, and architectural limitations such as over-squashing, which leads to the loss of critical structural details. To address these challenges, we introduce a new class of GNNs that integrates the Kolmogorov-Arnold Networks (KANs), known for their robust data-fitting capabilities and high accuracy in small-scale AI + Science tasks. By incorporating KANs into GNNs, our model enhances the representation of molecular structures. We further advance this approach with a variant called SwallowKAN (SKAN), which employs adaptive Radial Basis Functions (RBFs) as the core of the non-linear neurons. This innovation improves both computational efficiency and adaptability to diverse molecular structures. Building on the strengths of SKAN, we propose a new class of GNNs, GNN-SKAN, and its augmented variant, GNN-SKAN+, which incorporates a SKAN-based classifier to further boost performance. To our knowledge, this is the first work to integrate KANs into GNN architectures tailored for molecular representation learning. Experiments across 6 classification datasets, 6 regression datasets, and 4 few-shot learning datasets demonstrate that our approach achieves new state-of-the-art performance in terms of accuracy and computational cost.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Li, Ruifeng and Li, Mingqian and Liu, Wei and Chen, Hongyang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.01018 [cs]},
	keywords = {68T99, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, J.2.4},
}

@inproceedings{nguyenRevisitingOversmoothingOversquashing2023,
	address = {Honolulu, Hawaii, USA},
	series = {{ICML}'23},
	title = {Revisiting over-smoothing and over-squashing using ollivier-ricci curvature},
	volume = {202},
	abstract = {Graph Neural Networks (GNNs) had been demonstrated to be inherently susceptible to the problems of over-smoothing and over-squashing. These issues prohibit the ability of GNNs to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connection between the local graph geometry and the occurrence of both of these issues, thereby providing a unified framework for studying them at a local scale using the Ollivier-Ricci curvature. Specifically, we demonstrate that oversmoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of simultaneously addressing both over-smoothing and over-squashing.},
	urldate = {2024-08-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Nguyen, Khang and Nong, Hieu and Nguyen, Vinh and Ho, Nhat and Osher, Stanley and Nguyen, Tan},
	month = jul,
	year = {2023},
	pages = {25956--25979},
}

@inproceedings{ioffeBatchNormalizationAccelerating2015,
	address = {Lille, France},
	series = {{ICML}'15},
	title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
	shorttitle = {Batch normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	urldate = {2024-08-06},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = jul,
	year = {2015},
	pages = {448--456},
}

@inproceedings{zhouDeeperGraphNeural2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Towards deeper graph neural networks with differentiable group normalization},
	isbn = {978-1-71382-954-6},
	abstract = {Graph neural networks (GNNs), which learn the representation of a node by aggregating its neighbors, have become an effective computational tool in downstream applications. Over-smoothing is one of the key issues which limit the performance of GNNs as the number of layers increases. It is because the stacked aggregators would make node representations converge to indistinguishable vectors. Several attempts have been made to tackle the issue by bringing linked node pairs close and unlinked pairs distinct. However, they often ignore the intrinsic community structures and would result in sub-optimal performance. The representations of nodes within the same community/class need be similar to facilitate the classification, while different classes are expected to be separated in embedding space. To bridge the gap, we introduce two over-smoothing metrics and a novel technique, i.e., differentiable group normalization (DGN). It normalizes nodes within the same group independently to increase their smoothness, and separates node distributions among different groups to significantly alleviate the over-smoothing issue. Experiments on real-world datasets demonstrate that DGN makes GNN models more robust to over-smoothing and achieves better performance with deeper GNNs.},
	urldate = {2024-08-06},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Zhou, Kaixiong and Huang, Xiao and Li, Yuening and Zha, Daochen and Chen, Rui and Hu, Xia},
	month = dec,
	year = {2020},
	pages = {4917--4928},
}

@inproceedings{fernRandomProjectionHigh,
	address = {Washington DC},
	title = {Random {Projection} for {High} {Dimensional} {Data} {Clustering}: {A} {Cluster} {Ensemble} {Approach}},
	abstract = {We investigate how random projection can best be used for clustering high dimensional data. Random projection has been shown to have promising theoretical properties. In practice, however, we ﬁnd that it results in highly unstable clustering performance. Our solution is to use random projection in a cluster ensemble approach. Empirical results show that the proposed approach achieves better and more robust clustering performance compared to not only single runs of random projection/clustering but also clustering with PCA, a traditional data reduction method for high dimensional data. To gain insights into the performance improvement obtained by our ensemble method, we analyze and identify the inﬂuence of the quality and the diversity of the individual clustering solutions on the ﬁnal ensemble performance.},
	language = {en},
	booktitle = {Proceedings of the {Twentieth} {International} {Conference} on {Machine} {Learning} ({ICML}-2003)},
	author = {Fern, Xiaoli Zhang and Brodley, Carla E},
	year = {2003},
}

@inproceedings{ben-davidStabilityKMeansClustering2007,
	address = {Berlin, Heidelberg},
	title = {Stability of k-{Means} {Clustering}},
	isbn = {978-3-540-72927-3},
	doi = {10.1007/978-3-540-72927-3_4},
	abstract = {We consider the stability of k-means clustering problems. Clustering stability is a common heuristics used to determine the number of clusters in a wide variety of clustering applications. We continue the theoretical analysis of clustering stability by establishing a complete characterization of clustering stability in terms of the number of optimal solutions to the clustering optimization problem. Our results complement earlier work of Ben-David, von Luxburg and Pál, by settling the main problem left open there. Our analysis shows that, for probability distributions with finite support, the stability of k-means clusterings depends solely on the number of optimal solutions to the underlying optimization problem for the data distribution. These results challenge the common belief and practice that view stability as an indicator of the validity, or meaningfulness, of the choice of a clustering algorithm and number of clusters.},
	language = {en},
	booktitle = {Learning {Theory}},
	publisher = {Springer},
	author = {Ben-David, Shai and Pál, Dávid and Simon, Hans Ulrich},
	editor = {Bshouty, Nader H. and Gentile, Claudio},
	year = {2007},
	keywords = {Cluster Algorithm, Cluster Problem, Hessian Matrix, Optimal Partition, Risk Function},
	pages = {20--34},
}

@book{manningIntroductionInformationRetrieval2009,
	title = {Introduction to {Information} {Retrieval}},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher and Raghavan, Prabhakar and Schuetze, Hinrich},
	year = {2009},
}

@misc{zhuDeepGraphContrastive2020,
	address = {The Thirty-seventh International Conference on Machine Learning},
	title = {Deep {Graph} {Contrastive} {Representation} {Learning}},
	author = {Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
	year = {2020},
}

@article{zhouGraphNeuralNetworks2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	shorttitle = {Graph neural networks},
	doi = {10.1016/j.aiopen.2021.01.001},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	urldate = {2024-02-09},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	month = jan,
	year = {2020},
	keywords = {Deep learning, Graph neural network},
	pages = {57--81},
}

@inproceedings{wuSimplifyingGraphConvolutional2019,
	title = {Simplifying {Graph} {Convolutional} {Networks}},
	isbn = {2640-3498},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
	year = {2019},
}

@article{wangGraphNeuralNetworks2023,
	title = {Graph {Neural} {Networks} for {Molecules}},
	volume = {36},
	journal = {Machine Learning in Molecular Sciences},
	author = {Wang, Yuyang and Li, Zijie and Barati Farimani, Amir},
	editor = {Qu, Chen and Liu, Hanchao},
	year = {2023},
	keywords = {Graph neural network, Graph representation learning, Molecular generation, Molecular modeling, Molecular simulation, Quantitative structure-activity relationship, Self-supervised learning},
	pages = {21--66},
}

@article{waikhomSurveyGraphNeural2023,
	title = {A survey of graph neural networks in various learning paradigms: methods, applications, and challenges},
	volume = {56},
	shorttitle = {A survey of graph neural networks in various learning paradigms},
	doi = {10.1007/s10462-022-10321-2},
	abstract = {In the last decade, deep learning has reinvigorated the machine learning field. It has solved many problems in computer vision, speech recognition, natural language processing, and other domains with state-of-the-art performances. In these domains, the data is generally represented in the Euclidean space. Various other domains conform to non-Euclidean space, for which a graph is an ideal representation. Graphs are suitable for representing the dependencies and inter-relationships between various entities. Traditionally, handcrafted features for graphs are incapable of providing the necessary inference for various tasks from this complex data representation. Recently, there has been an emergence of employing various advances in deep learning for graph-based tasks (called Graph Neural Networks (GNNs)). This article introduces preliminary knowledge regarding GNNs and comprehensively surveys GNNs in different learning paradigms—supervised, unsupervised, semi-supervised, self-supervised, and few-shot or meta-learning. The taxonomy of each graph-based learning setting is provided with logical divisions of methods falling in the given learning setting. The approaches for each learning task are analyzed from theoretical and empirical standpoints. Further, we provide general architecture design guidelines for building GNN models. Various applications and benchmark datasets are also provided, along with open challenges still plaguing the general applicability of GNNs.},
	language = {en},
	urldate = {2024-02-08},
	journal = {Artificial Intelligence Review},
	author = {Waikhom, Lilapati and Patgiri, Ripon},
	month = jul,
	year = {2023},
	keywords = {Deep learning, Graph, Graph neural network, Neural network},
	pages = {6295--6364},
}

@inproceedings{velickovicGraphAttentionNetworks2018,
	title = {Graph {Attention} {Networks}},
	booktitle = {The {Thirty}-fifth {International} {Conference} on {Machine} {Learning}},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	year = {2018},
}

@inproceedings{trimmelERAEnhancedRational2022,
	title = {{ERA}: {Enhanced} {Rational} {Activations}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision}},
	author = {Trimmel, Martin and Zanfir, Mihai and Hartley, Richard and Sminchisescu, Cristian},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Activation function, Deep learning, Neural networks, Rational activation},
}

@inproceedings{toppingUnderstandingOversquashingBottlenecks2021,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	booktitle = {The {Fortieth} {International} {Conference} on {Machine} {Learning}},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	year = {2021},
}

@inproceedings{shirzadExphormerSparseTransformers2023,
	title = {Exphormer: {Sparse} {Transformers} for {Graphs}},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	author = {Shirzad, Hamed and Velingker, Ameya and Venkatachalam, Balaji and Sutherland, Danica J and Sinop, Ali Kemal},
	year = {2023},
}

@inproceedings{shiMaskedLabelPrediction2021,
	title = {Masked {Label} {Prediction}: {Unified} {Message} {Passing} {Model} for {Semi}-{Supervised} {Classification}},
	isbn = {1045-0823},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Shi, Yunsheng and Huang, Zhengjie and Feng, Shikun and Zhong, Hui and Wang, Wenjing and Sun, Yu},
	year = {2021},
	pages = {1548},
}

@inproceedings{shiRevisitingOversmoothingBERT2021,
	title = {Revisiting {Over}-smoothing in {BERT} from the {Perspective} of {Graph}},
	booktitle = {The {Tenth} {International} {Conference} on {Learning} {Representations}},
	author = {Shi, Han and Gao, Jiahui and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng and Lee, Stephen M. S. and Kwok, James},
	year = {2021},
}

@misc{shchurPitfallsGraphNeural2019,
	title = {Pitfalls of {Graph} {Neural} {Network} {Evaluation}},
	url = {http://arxiv.org/abs/1811.05868},
	doi = {10.48550/arXiv.1811.05868},
	abstract = {Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and Günnemann, Stephan},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{senCollectiveClassificationNetwork2008,
	title = {Collective {Classification} in {Network} {Data}},
	volume = {29},
	doi = {10.1609/aimag.v29i3.2157},
	abstract = {Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.},
	language = {en},
	number = {3},
	urldate = {2024-03-08},
	journal = {AI Magazine},
	author = {Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina},
	month = sep,
	year = {2008},
}

@article{scarselliGraphNeuralNetwork2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	doi = {10.1109/TNN.2008.2005605},
	abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
	number = {1},
	urldate = {2024-03-20},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	month = dec,
	year = {2008},
	keywords = {Biological system modeling, Biology, Chemistry, Computer vision, Data engineering, Data mining, Graphical domains, Neural networks, Parameter estimation, Pattern recognition, Supervised learning, graph neural networks (GNNs), graph processing, recursive neural networks},
	pages = {61 -- 80},
}

@misc{ruschSurveyOversmoothingGraph2023,
	title = {A {Survey} on {Oversmoothing} in {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2303.10993},
	doi = {10.48550/arXiv.2303.10993},
	abstract = {Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Rusch, T. Konstantin and Bronstein, Michael M. and Mishra, Siddhartha},
	month = mar,
	year = {2023},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{ruschGradientGatingDeep2022,
	title = {Gradient {Gating} for {Deep} {Multi}-{Rate} {Learning} on {Graphs}},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Rusch, T. Konstantin and Chamberlain, Benjamin Paul and Mahoney, Michael W. and Bronstein, Michael M. and Mishra, Siddhartha},
	month = feb,
	year = {2023},
}

@article{rozemberczkiMultiScaleAttributedNode2021,
	title = {Multi-{Scale} attributed node embedding},
	volume = {9},
	doi = {10.1093/comnet/cnab014},
	abstract = {We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.},
	number = {2},
	urldate = {2024-03-01},
	journal = {Journal of Complex Networks},
	author = {Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},
	month = apr,
	year = {2021},
}

@misc{rossiEdgeDirectionalityImproves2023,
	title = {Edge {Directionality} {Improves} {Learning} on {Heterophilic} {Graphs}},
	url = {http://arxiv.org/abs/2305.10498},
	doi = {10.48550/arXiv.2305.10498},
	abstract = {Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outgoing edges. We prove that Dir-GNN matches the expressivity of the Directed Weisfeiler-Lehman test, exceeding that of conventional MPNNs. In extensive experiments, we validate that while our framework leaves performance unchanged on homophilic datasets, it leads to large gains over base models such as GCN, GAT and GraphSage on heterophilic benchmarks, outperforming much more complex methods and achieving new state-of-the-art results.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Rossi, Emanuele and Charpentier, Bertrand and Di Giovanni, Francesco and Frasca, Fabrizio and Günnemann, Stephan and Bronstein, Michael},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@inproceedings{peiGeomGCNGeometricGraph2020,
	title = {Geom-{GCN}: {Geometric} {Graph} {Convolutional} {Networks}},
	shorttitle = {Geom-{GCN}},
	abstract = {Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.},
	urldate = {2024-03-08},
	booktitle = {The {Eighth} {International} {Conference} on {Learning} {Representations}},
	publisher = {arXiv},
	author = {Pei, Hongbin and Wei, Bingzhe and Chang, Kevin Chen-Chuan and Lei, Yu and Yang, Bo},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ntRevisitingGraphNeural2019,
	title = {Revisiting {Graph} {Neural} {Networks}: {All} {We} {Have} is {Low}-{Pass} {Filters}},
	shorttitle = {Revisiting {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1905.09550},
	doi = {10.48550/arXiv.1905.09550},
	abstract = {Graph neural networks have become one of the most important techniques to solve machine learning problems on graph-structured data. Recent work on vertex classification proposed deep and distributed learning models to achieve high performance and scalability. However, we find that the feature vectors of benchmark datasets are already quite informative for the classification task, and the graph structure only provides a means to denoise the data. In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property. We further investigate their resilience to feature noise and propose some insights on GCN-based graph neural network design.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {NT, Hoang and Maehara, Takanori},
	month = may,
	year = {2019},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Spectral Theory, Statistics - Machine Learning},
}

@article{nauckPredictingBasinStability2022,
	title = {Predicting basin stability of power grids using graph neural networks},
	volume = {24},
	doi = {10.1088/1367-2630/ac54c9},
	abstract = {The prediction of dynamical stability of power grids becomes more important and challenging with increasing shares of renewable energy sources due to their decentralized structure, reduced inertia and volatility. We investigate the feasibility of applying graph neural networks (GNN) to predict dynamic stability of synchronisation in complex power grids using the single-node basin stability (SNBS) as a measure. To do so, we generate two synthetic datasets for grids with 20 and 100 nodes respectively and estimate SNBS using Monte-Carlo sampling. Those datasets are used to train and evaluate the performance of eight different GNN-models. All models use the full graph without simplifications as input and predict SNBS in a nodal-regression-setup. We show that SNBS can be predicted in general and the performance significantly changes using different GNN-models. Furthermore, we observe interesting transfer capabilities of our approach: GNN-models trained on smaller grids can directly be applied on larger grids without the need of retraining.},
	language = {en},
	urldate = {2024-02-08},
	journal = {New Journal of Physics},
	author = {Nauck, Christian and Lindner, Michael and Schürholt, Konstantin and Zhang, Haoming and Schultz, Paul and Kurths, Jürgen and Isenhardt, Ingrid and Hellmann, Frank},
	month = apr,
	year = {2022},
}

@inproceedings{molinaPadeActivationUnits2019,
	title = {Padé {Activation} {Units}: {End}-to-end {Learning} of {Flexible} {Activation} {Functions} in {Deep} {Networks}},
	booktitle = {The {Eighth} {International} {Conference} on {Learning} {Representations}},
	author = {Molina, Alejandro and Schramowski, Patrick and Kersting, Kristian},
	year = {2019},
}

@article{mccallumAutomatingConstructionInternet2000,
	title = {Automating the {Construction} of {Internet} {Portals} with {Machine} {Learning}},
	volume = {3},
	doi = {10.1023/A:1009953814988},
	abstract = {Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains.},
	language = {en},
	urldate = {2024-03-08},
	journal = {Information Retrieval},
	author = {McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
	month = jul,
	year = {2000},
	keywords = {crawling, expectation-maximization, hidden Markov models, information extraction, naive Bayes, reinforcement learning, spidering, text classification, unlabeled data},
	pages = {127--163},
}

@misc{luoDistillingSelfKnowledgeContrastive2021,
	title = {Distilling {Self}-{Knowledge} {From} {Contrastive} {Links} to {Classify} {Graph} {Nodes} {Without} {Passing} {Messages}},
	url = {http://arxiv.org/abs/2106.08541},
	abstract = {Nowadays, Graph Neural Networks (GNNs) following the Message Passing paradigm become the dominant way to learn on graphic data. Models in this paradigm have to spend extra space to look up adjacent nodes with adjacency matrices and extra time to aggregate multiple messages from adjacent nodes. To address this issue, we develop a method called LinkDist that distils self-knowledge from connected node pairs into a Multi-Layer Perceptron (MLP) without the need to aggregate messages. Experiment with 8 real-world datasets shows the MLP derived from LinkDist can predict the label of a node without knowing its adjacencies but achieve comparable accuracy against GNNs in the contexts of semi- and full-supervised node classification. Moreover, LinkDist benefits from its Non-Message Passing paradigm that we can also distil self-knowledge from arbitrarily sampled node pairs in a contrastive way to further boost the performance of LinkDist.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Luo, Yi and Chen, Aiguo and Yan, Ke and Tian, Ling},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{liDeeperInsightsGraph2018,
	series = {1},
	title = {Deeper {Insights} {Into} {Graph} {Convolutional} {Networks} for {Semi}-{Supervised} {Learning}},
	volume = {32},
	doi = {10.1609/aaai.v32i1.11604},
	abstract = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
	language = {en},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Qimai and Han, Zhichao and Wu, Xiao-ming},
	month = apr,
	year = {2018},
	keywords = {graph-based learning},
}

@inproceedings{liTrainingGraphNeural2021,
	title = {Training {Graph} {Neural} {Networks} with 1000 {Layers}},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Li, Guohao and Müller, Matthias and Ghanem, Bernard and Koltun, Vladlen},
	year = {2021},
}

@article{leshnoMultilayerFeedforwardNetworks1993,
	title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	volume = {6},
	doi = {10.1016/S0893-6080(05)80131-5},
	abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.},
	number = {6},
	urldate = {2024-03-15},
	journal = {Neural Networks},
	author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
	month = jan,
	year = {1993},
	keywords = {(μ) approximation, Activation functions, Multilayer feedforward networks, Role of threshold, Universal approximation capabilities},
	pages = {861--867},
}

@article{lachaudMathematicalExpressivenessGraph2022,
	title = {Mathematical {Expressiveness} of {Graph} {Neural} {Networks}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	doi = {10.3390/math10244770},
	abstract = {Graph Neural Networks (GNNs) are neural networks designed for processing graph data. There has been a lot of focus on recent developments of graph neural networks concerning the theoretical properties of the models, in particular with respect to their mathematical expressiveness, that is, to map different graphs or nodes to different outputs; or, conversely, to map permutations of the same graph to the same output. In this paper, we review the mathematical expressiveness results of graph neural networks. We find that according to their mathematical properties, the GNNs that are more expressive than the standard graph neural networks can be divided into two categories: the models that achieve the highest level of expressiveness, but require intensive computation; and the models that improve the expressiveness of standard graph neural networks by implementing node identification and substructure awareness. Additionally, we present a comparison of existing architectures in terms of their expressiveness. We conclude by discussing the future lines of work regarding the theoretical expressiveness of graph neural networks.},
	language = {en},
	number = {24},
	urldate = {2024-03-01},
	journal = {Mathematics. Mathematical Foundations of Deep Neural Networks},
	author = {Lachaud, Guillaume and Conde-Cespedes, Patricia and Trocan, Maria},
	month = jan,
	year = {2022},
	keywords = {Weisfeiler–Leman algorithm, expressiveness, graph neural networks, message passing neural networks},
	pages = {4770},
}

@inproceedings{kipfSemiSupervisedClassificationGraph2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}},
	author = {Kipf, Thomas N. and Welling, Max},
	year = {2016},
}

@inproceedings{kelesisReducingOversmoothingGraph2023,
	title = {Reducing {Oversmoothing} in {Graph} {Neural} {Networks} by {Changing} the {Activation} {Function}},
	doi = {10.3233/FAIA230400},
	booktitle = {26th {European} {Conference} on {Artificial} {Intelligence} {ECAI} 2023},
	author = {Kelesis, Dimitrios and Vogiatzis, Dimitrios and Katsimpras, Georgios and Fotakis, Dimitris and Paliouras, Georgios},
	editor = {Gal, Kobi and Nowé, Ann and Nalepa, Grzegorz J. and Fairstein, Roy and Rădulescu, Roxana},
	year = {2023},
}

@article{jiangGraphNeuralNetwork2022,
	title = {Graph neural network for traffic forecasting: {A} survey},
	volume = {207},
	shorttitle = {Graph neural network for traffic forecasting},
	doi = {10.1016/j.eswa.2022.117921},
	abstract = {Traffic forecasting is important for the success of intelligent transportation systems. Deep learning models, including convolution neural networks and recurrent neural networks, have been extensively applied in traffic forecasting problems to model spatial and temporal dependencies. In recent years, to model the graph structures in transportation systems as well as contextual information, graph neural networks have been introduced and have achieved state-of-the-art performance in a series of traffic forecasting problems. In this survey, we review the rapidly growing body of research using different graph neural networks, e.g. graph convolutional and graph attention networks, in various traffic forecasting problems, e.g. road traffic flow and speed forecasting, passenger flow forecasting in urban rail transit systems, and demand forecasting in ride-hailing platforms. We also present a comprehensive list of open data and source codes for each problem and identify future research directions. To the best of our knowledge, this paper is the first comprehensive survey that explores the application of graph neural networks for traffic forecasting problems. We have also created a public GitHub repository where the latest papers, open data, and source codes will be updated.},
	urldate = {2024-02-08},
	journal = {Expert Systems with Applications},
	author = {Jiang, Weiwei and Luo, Jiayun},
	month = nov,
	year = {2022},
	keywords = {Deep learning, Graph attention network, Graph convolution network, Graph neural networks, Traffic forecasting},
}

@inproceedings{huangHigherorderGraphConvolutional2024,
	title = {Higher-order {Graph} {Convolutional} {Network} with {Flower}-{Petals} {Laplacians} on {Simplicial} {Complexes}},
	abstract = {Despite the recent successes of vanilla Graph Neural Networks (GNNs) on various tasks, their foundation on pairwise networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quantifiable measure of higher-order interaction strengths. The theoretical underpinnings of HiGCN's advanced expressiveness are rigorously demonstrated. Additionally, our empirical investigations reveal that the proposed model accomplishes state-of-the-art performance on a range of graph tasks and provides a scalable and flexible solution to explore higher-order interactions in graphs. Codes and datasets are available at https://github.com/Yiminghh/HiGCN.},
	urldate = {2024-02-09},
	booktitle = {The {Thirty}-{Eighth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Huang, Yiming and Zeng, Yujie and Wu, Qiang and Lü, Linyuan},
	month = jan,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Condensed Matter - Statistical Mechanics, Physics - Physics and Society},
}

@article{huangNormalizationTechniquesTraining2023,
	title = {Normalization {Techniques} in {Training} {DNNs}: {Methodology}, {Analysis} and {Application}},
	volume = {45},
	shorttitle = {Normalization {Techniques} in {Training} {DNNs}},
	doi = {10.1109/TPAMI.2023.3250241},
	abstract = {Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.},
	number = {8},
	urldate = {2024-03-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
	month = aug,
	year = {2023},
	keywords = {Batch normalization, Biological neural networks, Covariance matrices, Decorrelation, Optimization, Task analysis, Tensors, Training, deep neural networks, image classification, survey, weight normalization},
	pages = {10173 -- 10196},
}

@misc{hoangMitigatingDegreeBiases2023,
	title = {Mitigating {Degree} {Biases} in {Message} {Passing} {Mechanism} by {Utilizing} {Community} {Structures}},
	url = {https://arxiv.org/abs/2312.16788},
	doi = {10.48550/arXiv.2312.16788},
	abstract = {This study utilizes community structures to address node degree biases in message-passing (MP) via learnable graph augmentations and novel graph transformers. Recent augmentation-based methods showed that MP neural networks often perform poorly on low-degree nodes, leading to degree biases due to a lack of messages reaching low-degree nodes. Despite their success, most methods use heuristic or uniform random augmentations, which are non-differentiable and may not always generate valuable edges for learning representations. In this paper, we propose Community-aware Graph Transformers, namely CGT, to learn degree-unbiased representations based on learnable augmentations and graph transformers by extracting within community structures. We first design a learnable graph augmentation to generate more within-community edges connecting low-degree nodes through edge perturbation. Second, we propose an improved self-attention to learn underlying proximity and the roles of nodes within the community. Third, we propose a self-supervised learning task that could learn the representations to preserve the global graph structure and regularize the graph augmentations. Extensive experiments on various benchmark datasets showed CGT outperforms state-of-the-art baselines and significantly improves the node degree biases. The source code is available at https://github.com/NSLab-CUK/Community-aware-Graph-Transformer.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Hoang, Van Thuy and Lee, O.-Joun},
	month = dec,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@inproceedings{heHarnessingExplanationsLLMtoLM2023,
	title = {Harnessing {Explanations}: {LLM}-to-{LM} {Interpreter} for {Enhanced} {Text}-{Attributed} {Graph} {Representation} {Learning}},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {He, Xiaoxin and Bresson, Xavier and Laurent, Thomas and Perold, Adam and LeCun, Yann and Hooi, Bryan},
	year = {2023},
}

@inproceedings{goriNewModelLearning2005,
	title = {A new model for learning in graph domains},
	booktitle = {{IEEE} {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	year = {2005},
}

@misc{duanSimTeGFrustratinglySimple2023,
	title = {{SimTeG}: {A} {Frustratingly} {Simple} {Approach} {Improves} {Textual} {Graph} {Learning}},
	shorttitle = {{SimTeG}},
	url = {http://arxiv.org/abs/2308.02565},
	doi = {10.48550/arXiv.2308.02565},
	abstract = {Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or documents), which are widely prevalent. The representation learning of TGs involves two stages: (i) unsupervised feature extraction and (ii) supervised graph representation learning. In recent years, extensive efforts have been devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated. However, the former stage for most existing graph benchmarks still relies on traditional feature engineering techniques. More recently, with the rapid development of language models (LMs), researchers have focused on leveraging LMs to facilitate the learning of TGs, either by jointly training them in a computationally intensive framework (merging the two stages), or designing complex self-supervised training tasks for feature extraction (enhancing the first stage). In this work, we present SimTeG, a frustratingly Simple approach for Textual Graph learning that does not innovate in frameworks, models, and tasks. Instead, we first perform supervised parameter-efficient fine-tuning (PEFT) on a pre-trained LM on the downstream task, such as node classification. We then generate node embeddings using the last hidden states of finetuned LM. These derived features can be further utilized by any GNN for training on the same task. We evaluate our approach on two fundamental graph representation learning tasks: node classification and link prediction. Through extensive experiments, we show that our approach significantly improves the performance of various GNNs on multiple graph benchmarks.},
	urldate = {2024-05-12},
	publisher = {arXiv},
	author = {Duan, Keyu and Liu, Qian and Chua, Tat-Seng and Yan, Shuicheng and Ooi, Wei Tsang and Xie, Qizhe and He, Junxian},
	month = aug,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{bodyanskiyLearnableExtendedActivation2023,
	title = {Learnable {Extended} {Activation} {Function} for {Deep} {Neural} {Networks}},
	volume = {22},
	doi = {10.47839/ijc.22.3.3225},
	abstract = {This paper introduces Learnable Extended Activation Function (LEAF) - an adaptive activation function that combines the properties of squashing functions and rectifier units. Depending on the target architecture and data processing task, LEAF adapts its form during training to achieve lower loss values and improve the training results. While not suffering from the "vanishing gradient" effect, LEAF can directly replace SiLU, ReLU, Sigmoid, Tanh, Swish, and AHAF in feed-forward, recurrent, and many other neural network architectures.},
	language = {en},
	number = {3},
	urldate = {2024-03-15},
	journal = {International Journal of Computing},
	author = {Bodyanskiy, Yevgeniy and Kostiuk, Serhii},
	month = oct,
	year = {2023},
}

@misc{beggaDiffusionJumpGNNsHomophiliation2023,
	title = {Diffusion-{Jump} {GNNs}: {Homophiliation} via {Learnable} {Metric} {Filters}},
	shorttitle = {Diffusion-{Jump} {GNNs}},
	url = {http://arxiv.org/abs/2306.16976},
	doi = {10.48550/arXiv.2306.16976},
	abstract = {High-order Graph Neural Networks (HO-GNNs) have been developed to infer consistent latent spaces in the heterophilic regime, where the label distribution is not correlated with the graph structure. However, most of the existing HO-GNNs are hop-based, i.e., they rely on the powers of the transition matrix. As a result, these architectures are not fully reactive to the classification loss and the achieved structural filters have static supports. In other words, neither the filters' supports nor their coefficients can be learned with these networks. They are confined, instead, to learn combinations of filters. To address the above concerns, we propose Diffusion-jump GNNs a method relying on asymptotic diffusion distances that operates on jumps. A diffusion-pump generates pairwise distances whose projections determine both the support and coefficients of each structural filter. These filters are called jumps because they explore a wide range of scales in order to find bonds between scattered nodes with the same label. Actually, the full process is controlled by the classification loss. Both the jumps and the diffusion distances react to classification errors (i.e. they are learnable). Homophiliation, i.e., the process of learning piecewise smooth latent spaces in the heterophilic regime, is formulated as a Dirichlet problem: the known labels determine the border nodes and the diffusion-pump ensures a minimal deviation of the semi-supervised grouping from a canonical unsupervised grouping. This triggers the update of both the diffusion distances and, consequently, the jumps in order to minimize the classification error. The Dirichlet formulation has several advantages. It leads to the definition of structural heterophily, a novel measure beyond edge heterophily. It also allows us to investigate links with (learnable) diffusion distances, absorbing random walks and stochastic diffusion.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Begga, Ahmed and Escolano, Francisco and Lozano, Miguel Angel and Hancock, Edwin R.},
	month = jun,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{battagliaRelationalInductiveBiases2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	doi = {10.48550/arXiv.1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{askrDeepLearningDrug2023,
	title = {Deep learning in drug discovery: an integrative review and future challenges},
	volume = {56},
	shorttitle = {Deep learning in drug discovery},
	doi = {10.1007/s10462-022-10306-1},
	abstract = {Recently, using artificial intelligence (AI) in drug discovery has received much attention since it significantly shortens the time and cost of developing new drugs. Deep learning (DL)-based approaches are increasingly being used in all stages of drug development as DL technology advances, and drug-related data grows. Therefore, this paper presents a systematic Literature review (SLR) that integrates the recent DL technologies and applications in drug discovery Including, drug–target interactions (DTIs), drug–drug similarity interactions (DDIs), drug sensitivity and responsiveness, and drug-side effect predictions. We present a review of more than 300 articles between 2000 and 2022. The benchmark data sets, the databases, and the evaluation measures are also presented. In addition, this paper provides an overview of how explainable AI (XAI) supports drug discovery problems. The drug dosing optimization and success stories are discussed as well. Finally, digital twining (DT) and open issues are suggested as future research challenges for drug discovery problems. Challenges to be addressed, future research directions are identified, and an extensive bibliography is also included.},
	language = {en},
	urldate = {2024-02-08},
	journal = {Artificial Intelligence Review},
	author = {Askr, Heba and Elgeldawi, Enas and Aboul Ella, Heba and Elshaier, Yaseen A. M. M. and Gomaa, Mamdouh M. and Hassanien, Aboul Ella},
	month = jul,
	year = {2023},
	keywords = {Artificial intelligence, Deep learning, Digital twining, Drug discovery, Drug dosing optimization, Drug sensitivity and response, Drug side-effects, Drug–drug similarity, Drug–target interactions, Explainable artificial intelligence},
	pages = {5975--6037},
}

@article{apicellaSurveyModernTrainable2021,
	title = {A survey on modern trainable activation functions},
	volume = {138},
	doi = {10.1016/j.neunet.2021.01.026},
	abstract = {In neural networks literature, there is a strong interest in identifying and defining activation functions which can improve neural network performance. In recent years there has been a renovated interest in the scientific community in investigating activation functions which can be trained during the learning process, usually referred to as trainable, learnable or adaptable activation functions. They appear to lead to better network performance. Diverse and heterogeneous models of trainable activation function have been proposed in the literature. In this paper, we present a survey of these models. Starting from a discussion on the use of the term “activation function” in literature, we propose a taxonomy of trainable activation functions, highlight common and distinctive proprieties of recent and past models, and discuss main advantages and limitations of this type of approach. We show that many of the proposed approaches are equivalent to adding neuron layers which use fixed (non-trainable) activation functions and some simple local rule that constrains the corresponding weight layers.},
	urldate = {2024-03-15},
	journal = {Neural Networks},
	author = {Apicella, Andrea and Donnarumma, Francesco and Isgrò, Francesco and Prevete, Roberto},
	month = jun,
	year = {2021},
	keywords = {Activation functions, Learnable activation functions, Machine learning, Neural networks, Trainable activation functions},
	pages = {14--32},
}

@inproceedings{delfosseAdaptiveRationalActivations2024,
	title = {Adaptive {Rational} {Activations} to {Boost} {Deep} {Reinforcement} {Learning}},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Delfosse, Quentin and Schramowski, Patrick and Mundt, Martin and Molina, Alejandro and Kersting, Kristian},
	year = {2024},
}

@inproceedings{gilmerNeuralMessagePassing2017,
	title = {Neural message passing for {Quantum} chemistry},
	volume = {70},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = aug,
	year = {2017},
}

@misc{bronsteinGeometricDeepLearning2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478v2},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	language = {en},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bojchevskiDeepGaussianEmbedding2018,
	title = {Deep {Gaussian} {Embedding} of {Graphs}: {Unsupervised} {Inductive} {Learning} via {Ranking}},
	booktitle = {The {Sixth} {International} {Conference} on {Learning} {Representations}},
	author = {Bojchevski, Aleksandar and Günnemann, Stephan},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@book{bishopPatternRecognitionMachine2006,
	title = {Pattern recognition and machine learning},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
}

@inproceedings{alonBottleneckGraphNeural2020,
	title = {On the {Bottleneck} of {Graph} {Neural} {Networks} and its {Practical} {Implications}},
	booktitle = {The {Ninth} {International} {Conference} on {Learning} {Representations}},
	author = {Alon, Uri and Yahav, Eran},
	year = {2020},
}

@article{macqueenMethodsClassificationAnalysis1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {1},
	journal = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
	author = {MacQueen, J.},
	year = {1967},
}

@misc{OversquashingGraphNeural,
	title = {Over-squashing in {Graph} {Neural} {Networks} · {MIT} {Deep} {Learning} {Blogs}},
}

@inproceedings{buterezGraphNeuralNetworks2022,
	title = {Graph {Neural} {Networks} with {Adaptive} {Readouts}},
	author = {Buterez, David and Janet, Jon Paul and Kiddle, Steven J. and Oglic, Dino and Liò, Pietro},
	year = {2022},
}

@inproceedings{feyFastGraphRepresentation2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	booktitle = {Representation {Learning} on {Graphs} and {Manifolds}},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chiangClusterGCNEfficientAlgorithm2019,
	title = {Cluster-{GCN}: {An} {Efficient} {Algorithm} for {Training} {Deep} and {Large} {Graph} {Convolutional} {Networks}},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
	month = jul,
	year = {2019},
	keywords = {clustering, deep learning, graph convolutional networks, large-scale learning, semi-supervised learning},
}

@inproceedings{caiGraphNormPrincipledApproach2021,
	title = {{GraphNorm}: {A} {Principled} {Approach} to {Accelerating} {Graph} {Neural} {Network} {Training}},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Cai, Tianle and Luo, Shengjie and Xu, Keyulu and He, Di and Liu, Tie-Yan and Wang, Liwei},
	year = {2021},
	note = {ISSN: 2640-3498},
}

@inproceedings{boulleRationalNeuralNetworks2020,
	title = {Rational neural networks},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Boulle, Nicolas and Nakatsukasa, Yuji and Townsend, Alex},
	year = {2020},
}

@inproceedings{tangArnetMinerExtractionMining2008,
	title = {{ArnetMiner}: extraction and mining of academic social networks},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Tang, Jie and Zhang, Jing and Yao, Limin and Li, Juanzi and Zhang, Li and Su, Zhong},
	month = aug,
	year = {2008},
	keywords = {association search, expertise search, information extraction, name disambiguation, social network, topic modeling},
}

@inproceedings{gilesCiteSeerAutomaticCitation1998,
	title = {{CiteSeer}: an automatic citation indexing system},
	booktitle = {Proceedings of the third {ACM} conference on {Digital} libraries},
	author = {Giles, C. Lee and Bollacker, Kurt D. and Lawrence, Steve},
	month = may,
	year = {1998},
}

@inproceedings{huOpenGraphBenchmark2020,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	year = {2020},
}

@inproceedings{mooreVeryFastEMBased1998,
	title = {Very {Fast} {EM}-{Based} {Mixture} {Model} {Clustering} {Using} {Multiresolution} {Kd}-{Trees}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Moore, Andrew},
	year = {1998},
}

@misc{MathematicsFreeFullText,
	title = {Mathematics {\textbar} {Free} {Full}-{Text} {\textbar} {Mathematical} {Expressiveness} of {Graph} {Neural} {Networks}},
}

@inproceedings{hamiltonInductiveRepresentationLearning2017,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	year = {2017},
}

@inproceedings{izadiOptimizationGraphNeural2020,
	title = {Optimization of {Graph} {Neural} {Networks} with {Natural} {Gradient} {Descent}},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Izadi, Mohammad Rasool and Fang, Yihao and Stevenson, Robert and Lin, Lizhen},
	year = {2020},
	keywords = {Convergence, Fisher information, Graph neural network, Graph neural networks, Optimization, Semisupervised learning, Sensitivity, Time-frequency analysis, Training, natural gradient descent, network data},
}

@inproceedings{zhangLinkPredictionBased2018,
	title = {Link {Prediction} {Based} on {Graph} {Neural} {Networks}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhang, Muhan and Chen, Yixin},
	year = {2018},
}

@misc{leeUniversalApproximationLogconcave,
	title = {Universal {Approximation} {For} {Log}-concave {Distributions} {Using} {Well}-conditioned {Normalizing} {Flows}},
	abstract = {Normalizing ﬂows are a widely used class of latent-variable generative models with a tractable likelihood. Afﬁne-coupling models [Dinh et al., 2014, 2016] are a particularly common type of normalizing ﬂows, for which the Jacobian of the latent-to-observable-variable transformation is triangular, allowing the likelihood to be computed in linear time. Despite the widespread usage of afﬁne couplings, the special structure of the architecture makes understanding their representational power challenging. The question of universal approximation was only recently resolved by three parallel papers [Huang et al., 2020, Zhang et al., 2020, Koehler et al., 2020] – who showed reasonably regular distributions can be approximated arbitrarily well using afﬁne couplings—albeit with networks with a nearly-singular Jacobian. As ill-conditioned Jacobians are an obstacle for likelihood-based training, the fundamental question remains: which distributions can be approximated using well-conditioned afﬁne coupling ﬂows? In this paper, we show that any log-concave distribution can be approximated using well-conditioned afﬁne-coupling ﬂows. In terms of proof techniques, we uncover and leverage deep connections between afﬁne coupling architectures, underdamped Langevin dynamics (a stochastic differential equation often used to sample from Gibbs measures) and Hénon maps (a structured dynamical system that appears in the study of symplectic diffeomorphisms). Our results also inform the practice of training afﬁne couplings: we approximate a padded version of the input distribution with iid Gaussians—a strategy which Koehler et al. [2020] empirically observed to result in better-conditioned ﬂows, but had hitherto no theoretical grounding. Our proof can thus be seen as providing theoretical evidence for the beneﬁts of Gaussian padding when training normalizing ﬂows.},
	language = {en},
	publisher = {arXiv},
	author = {Lee, Holden and Pabbaraju, Chirag and Sevekari, Anish and Risteski, Andrej},
}

@inproceedings{vaswaniAttentionAllYou2017,
	title = {Attention is {All} you {Need}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{rakhshanTensorizedRandomProjections2020,
	title = {Tensorized {Random} {Projections}},
	url = {http://arxiv.org/abs/2003.05101},
	doi = {10.48550/arXiv.2003.05101},
	abstract = {We introduce a novel random projection technique for efficiently reducing the dimension of very high-dimensional tensors. Building upon classical results on Gaussian random projections and Johnson-Lindenstrauss transforms{\textasciitilde}(JLT), we propose two tensorized random projection maps relying on the tensor train{\textasciitilde}(TT) and CP decomposition format, respectively. The two maps offer very low memory requirements and can be applied efficiently when the inputs are low rank tensors given in the CP or TT format. Our theoretical analysis shows that the dense Gaussian matrix in JLT can be replaced by a low-rank tensor implicitly represented in compressed form with random factors, while still approximately preserving the Euclidean distance of the projected inputs. In addition, our results reveal that the TT format is substantially superior to CP in terms of the size of the random projection needed to achieve the same distortion ratio. Experiments on synthetic data validate our theoretical analysis and demonstrate the superiority of the TT decomposition.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Rakhshan, Beheshteh T. and Rabusseau, Guillaume},
	month = mar,
	year = {2020},
	note = {arXiv:2003.05101 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}
