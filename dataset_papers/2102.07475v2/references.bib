
@article{kaelbling_planning_1998,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.},
	language = {en},
	number = {1},
	urldate = {2021-04-25},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	keywords = {Partially observable Markov decision processes, Planning, Uncertainty},
	pages = {99--134},
}

@inproceedings{christianos_shared_2020,
	title = {Shared {Experience} {Actor}-{Critic} for {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {33},
	copyright = {All rights reserved},
	url = {https://proceedings.neurips.cc/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf},
	abstract = {Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called shared Experience Actor-Critic(SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christianos, Filippos and Schäfer, Lukas and Albrecht, Stefano V.},
	year = {2020},
	month = dec,
	pages = {10707--10717},
}

@article{papoudakis_comparative_2020,
	title = {Comparative {Evaluation} of {Multi}-{Agent} {Deep} {Reinforcement} {Learning} {Algorithms}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2006.07869},
	abstract = {Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learners, centralised training with decentralised execution, and value decomposition) in a diverse range of multi-agent learning tasks. Our results show that (1) algorithm performance depends strongly on environment properties and no algorithm learns efficiently across all learning tasks; (2) independent learners often achieve equal or better performance than more complex algorithms; (3) tested algorithms struggle to solve multi-agent tasks with sparse rewards. We report detailed empirical data, including a reliability analysis, and provide insights into the limitations of the tested algorithms.},
	urldate = {2021-01-25},
	journal = {arXiv:2006.07869 [cs, stat]},
	author = {Papoudakis, Georgios and Christianos, Filippos and Schäfer, Lukas and Albrecht, Stefano V.},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{papoudakis_dealing_2019,
	title = {Dealing with {Non}-{Stationarity} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1906.04737},
	abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
	urldate = {2021-02-05},
	journal = {arXiv:1906.04737 [cs, stat]},
	author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V.},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{panagopoulos_low-complexity_2020,
	title = {A low-complexity non-intrusive approach to predict the energy demand of buildings over short-term horizons},
	volume = {0},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1080/17512549.2020.1835712},
	doi = {10.1080/17512549.2020.1835712},
	abstract = {ABSTRACT Reliable, non-intrusive, short-term (of up to 12 h ahead) prediction of a building's energy demand is a critical component of intelligent energy management applications. A number of such approaches have been proposed over time, utilizing various statistical and, more recently, machine learning techniques, such as decision trees, neural networks and support vector machines. Importantly, all of these works barely outperform simple seasonal auto-regressive integrated moving average models, while their complexity is significantly higher. In this work, we propose a novel low-complexity non-intrusive approach that improves the predictive accuracy of the state-of-the-art by up to ∼ 10 \% . The backbone of our approach is a K-nearest neighbours search method, that exploits the demand pattern of the most similar historical days, and incorporates appropriate time-series pre-processing and easing. In the context of this work, we evaluate our approach against state-of-the-art methods and provide insights on their performance.},
	number = {0},
	journal = {Advances in Building Energy Research},
	author = {Panagopoulos, Athanasios Aris and Christianos, Filippos and Katsigiannis, Michail and Mykoniatis, Konstantinos and Pritoni, Marco and Panagopoulos, Orestis P. and Peffer, Therese and Chalkiadakis, Georgios and Culler, David E. and Jennings, Nicholas R. and {Timothy Lipman}},
	year = {2020},
	pages = {1--12},
}

@inproceedings{wang_influence-based_2020,
	title = {Influence-{Based} {Multi}-{Agent} {Exploration}},
	url = {https://iclr.cc/virtual_2020/poster_BJgy96EYvr.html},
	abstract = {Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence between the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent's behavior on expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate their exploration and learn policies to optimize the team performance. We show how to optimize these regularizers so that they can be easily integrated with policy gradient reinforcement learning. The resulting update rule draws a connection between coordinated exploration and intrinsic reward distribution. Finally, we empirically demonstrate the significant strength of our methods in a variety of multi-agent scenarios.},
	language = {en},
	urldate = {2021-04-19},
	author = {Wang, Tonghan and Wang, Jianhao and Wu, Yi and Zhang, Chongjie},
	month = apr,
	year = {2020},
}

@inproceedings{raileanu_ride_2020,
	title = {{RIDE}: {Rewarding} {Impact}-{Driven} {Exploration} for {Procedurally}-{Generated} {Environments}},
	shorttitle = {{RIDE}},
	url = {https://iclr.cc/virtual_2020/poster_rkg-TJBFPB.html},
	abstract = {Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.},
	language = {en},
	urldate = {2021-04-19},
	author = {Raileanu, Roberta and Rocktäschel, Tim},
	month = apr,
	year = {2020},
}

@inproceedings{fujimoto_off-policy_2019,
	title = {Off-{Policy} {Deep} {Reinforcement} {Learning} without {Exploration}},
	url = {http://proceedings.mlr.press/v97/fujimoto19a.html},
	abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection....},
	language = {en},
	urldate = {2021-04-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fujimoto, Scott and Meger, David and Precup, Doina},
	month = may,
	year = {2019},
	pages = {2052--2062},
}

@inproceedings{christianos_efficient_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {Multi}-criteria {Coalition} {Formation} {Using} {Hypergraphs} (with {Application} to the {V2G} {Problem})},
	copyright = {All rights reserved},
	isbn = {978-3-319-59294-7},
	doi = {10.1007/978-3-319-59294-7_9},
	abstract = {This paper proposes, for the first time in the literature, the use of hypergraphs for the efficient formation of effective coalitions. We put forward several formation methods that build on existing hypergraph pruning, transversal, and clustering algorithms, and exploit the hypergraph structure to identify agents with desirable characteristics. Our approach allows the near-instantaneous formation of high quality coalitions, adhering to multiple stated quality requirements. Moreover, our methods are shown to scale to dozens of thousands of agents within fractions of a second; with one of them scaling to even millions of agents within seconds. We apply our approach to the problem of forming coalitions to provide (electric) vehicle-to-grid (V2G) services. Ours is the first approach able to deal with large-scale, real-time coalition formation for the V2G problem, while taking multiple criteria into account for creating the electric vehicle coalitions.},
	language = {en},
	booktitle = {Multi-{Agent} {Systems} and {Agreement} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Christianos, Filippos and Chalkiadakis, Georgios},
	year = {2017},
	keywords = {Capacity Goals, Coalition Formation (CF), Hypergraph Clustering, Minimal Transversals, Optimal Coalition Structure},
	pages = {92--108},
}

@inproceedings{christianos_employing_2016,
	address = {NLD},
	series = {{ECAI}'16},
	title = {Employing hypergraphs for efficient coalition formation with application to the {V2G} problem},
	copyright = {All rights reserved},
	isbn = {978-1-61499-671-2},
	url = {https://doi.org/10.3233/978-1-61499-672-9-1604},
	doi = {10.3233/978-1-61499-672-9-1604},
	abstract = {This paper proposes, for the first time in the literature, the use of hypergraphs for the efficient formation of effective coalitions. We put forward several formation methods that build on existing hypergraph algorithms, and exploit hypergraph structure to identify agents with desirable characteristics. Our approach allows the near-instantaneous formation of high quality coalitions, while adhering to multiple stated requirements regarding coalition quality. Moreover, our methods are shown to scale to dozens of thousands of agents within fractions of a second; with one of them scaling to even millions of agents within seconds. We apply our approach to the problem of forming coalitions to provide (electric) vehicle-to-grid (V2G) services. Ours is the first approach able to deal with large-scale, realtime coalition formation for the V2G problem, while taking multiple criteria into account for creating electric vehicle coalitions.},
	urldate = {2021-02-21},
	booktitle = {Proceedings of the {Twenty}-second {European} {Conference} on {Artificial} {Intelligence}},
	publisher = {IOS Press},
	author = {Christianos, Filippos and Chalkiadakis, Georgios},
	month = aug,
	year = {2016},
	pages = {1604--1605},
}

@article{christianos_scaling_2021,
	title = {Scaling {Multi}-{Agent} {Reinforcement} {Learning} with {Selective} {Parameter} {Sharing}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2102.07475},
	abstract = {Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. Therefore, we propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns.},
	urldate = {2021-02-21},
	journal = {arXiv:2102.07475 [cs]},
	author = {Christianos, Filippos and Papoudakis, Georgios and Rahman, Arrasy and Albrecht, Stefano V.},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{rahman_open_2020,
	title = {Open {Ad} {Hoc} {Teamwork} using {Graph}-based {Policy} {Learning}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2006.10412},
	abstract = {Ad hoc teamwork is the challenging problem of designing an autonomous agent which can adapt quickly to collaborate with previously unknown teammates. Prior work in this area has focused on closed teams in which the number of agents is fixed. In this work, we consider open teams by allowing agents of varying types to enter and leave the team without prior notification. Our solution builds on graph neural networks to learn agent models and joint action-value decompositions under varying team sizes, which can be trained with reinforcement learning using a discounted returns objective. We demonstrate empirically that our approach effectively models the impact of other agents actions on the controlled agent's returns to produce policies which can robustly adapt to dynamic team composition and is able to effectively generalize to larger teams than were seen during training.},
	urldate = {2021-02-21},
	journal = {arXiv:2006.10412 [cs, stat]},
	author = {Rahman, Arrasy and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V.},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{papoudakis_local_2020,
	title = {Local {Information} {Opponent} {Modelling} {Using} {Variational} {Autoencoders}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2006.09447},
	abstract = {Modelling the behaviours of other agents (opponents) is essential for understanding how agents interact and making effective decisions. Existing methods for opponent modelling commonly assume knowledge of the local observations and chosen actions of the modelled opponents, which can significantly limit their applicability. We propose a new modelling technique based on variational autoencoders, which are trained to reconstruct the local actions and observations of the opponent based on embeddings which depend only on the local observations of the modelling agent (its observed world state, chosen actions, and received rewards). The embeddings are used to augment the modelling agent's decision policy which is trained via deep reinforcement learning; thus the policy does not require access to opponent observations. We provide a comprehensive evaluation and ablation study in diverse multi-agent tasks, showing that our method achieves comparable performance to an ideal baseline which has full access to opponent's information, and significantly higher returns than a baseline method which does not use the learned embeddings.},
	urldate = {2021-02-21},
	journal = {arXiv:2006.09447 [cs, stat]},
	author = {Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano V.},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@inproceedings{subramanian_multi_2020,
	address = {Richland, SC},
	title = {Multi {Type} {Mean} {Field} {Reinforcement} {Learning}},
	isbn = {978-1-4503-7518-4},
	abstract = {Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework.},
	urldate = {2021-02-05},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Subramanian, Sriram Ganapathi and Poupart, Pascal and Taylor, Matthew E. and Hegde, Nidhi},
	month = may,
	year = {2020},
	keywords = {many-agent learning, mean field methods, multiagent systems, reinforcement learning},
	pages = {411--419},
}

@article{albrecht_autonomous_2018,
	title = {Autonomous {Agents} {Modelling} {Other} {Agents}: {A} {Comprehensive} {Survey} and {Open} {Problems}},
	volume = {258},
	doi = {10.1016/j.artint.2018.01.002},
	journal = {Artificial Intelligence},
	author = {Albrecht, Stefano V. and Stone, Peter},
	year = {2018},
	publisher = {Elsevier},
	pages = {66--95},
}

@inproceedings{rangwala_learning_2020,
 author = {Rangwala, Murtaza and Williams, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10088--10098},
 publisher = {Curran Associates, Inc.},
 title = {{Learning Multi-Agent Communication through Structured Attentive Reasoning}},
 url = {https://proceedings.neurips.cc/paper/2020/file/72ab54f9b8c11fae5b923d7f854ef06a-Paper.pdf},
 volume = {33},
 month = dec,
 year = {2020}
}


@inproceedings{zhang_succinct_2020,
 author = {Zhang, Sai Qian and Zhang, Qi  and Lin, Jieyu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {17271--17282},
 publisher = {Curran Associates, Inc.},
 title = {{Succinct and Robust Multi-Agent Communication With Temporal Message Control}},
 url = {https://proceedings.neurips.cc/paper/2020/file/c82b013313066e0702d58dc70db033ca-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{sukhbaatar_learning_2016,
	title = {Learning {Multiagent} {Communication} with {Backpropagation}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sukhbaatar, Sainbayar and szlam, arthur and Fergus, Rob},
	year = {2016},
	pages = {2244--2252},
}

@inproceedings{foerster_learning_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	isbn = {978-1-5108-3881-9},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	urldate = {2021-02-05},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
	month = dec,
	year = {2016},
	pages = {2145--2153},
}

@inproceedings{he_opponent_2016,
	title = {Opponent {Modeling} in {Deep} {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v48/he16.html},
	abstract = {Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because of strategies’ complex interaction...},
	language = {en},
	urldate = {2021-02-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {He, He and Boyd-Graber, Jordan and Kwok, Kevin and Iii, Hal Daumé},
	month = jun,
	year = {2016},
	pages = {1804--1813},
}

@inproceedings{gupta_cooperative_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Cooperative {Multi}-agent {Control} {Using} {Deep} {Reinforcement} {Learning}},
	isbn = {978-3-319-71682-4},
	doi = {10.1007/978-3-319-71682-4_5},
	abstract = {This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communication. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. To effectively scale these algorithms beyond a trivial number of agents, we combine them with a multi-agent variant of curriculum learning. The algorithms are benchmarked on a suite of cooperative control tasks, including tasks with discrete and continuous actions, as well as tasks with dozens of cooperating agents. We report the performance of the algorithms using different neural architectures, training procedures, and reward structures. We show that policy gradient methods tend to outperform both temporal-difference and actor-critic methods and that curriculum learning is vital to scaling reinforcement learning algorithms in complex multi-agent domains.},
	language = {en},
	booktitle = {Autonomous {Agents} and {Multiagent} {Systems}},
	publisher = {Springer International Publishing},
	author = {Gupta, Jayesh K. and Egorov, Maxim and Kochenderfer, Mykel},
	year = {2017},
	pages = {66--83},
}

@inproceedings{iqbal_actor-attention-critic_2019,
	title = {Actor-{Attention}-{Critic} for {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v97/iqbal19a.html},
	abstract = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm tha...},
	language = {en},
	urldate = {2021-02-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Iqbal, Shariq and Sha, Fei},
	month = may,
	year = {2019},
	pages = {2961--2970},
}

@book{kaufman_finding_2009,
	title = {Finding {Groups} in {Data}: {An} {Introduction} to {Cluster} {Analysis}},
	volume = {344},
	publisher = {John Wiley \& Sons},
	author = {Kaufman, Leonard and Rousseeuw, Peter J},
	year = {2009},
}

@inproceedings{samvelyan_starcraft_2019,
	title = {The {StarCraft} {Multi}-{Agent} {Challenge}},
	volume = {4},
	isbn = {978-1-5108-9200-2},
	url = {https://youtu.be/VZ7zmQ_obZ0.},
	booktitle = {International {Joint} {Conference} on {Autonomous} {Agents} and {Multi}-{Agent} {Systems}},
	author = {Samvelyan, Mikayel and Rashid, Tabish and De Witt, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
	year = {2019},
	keywords = {Multi-agent learning, Reinforcement learning, StarCraft},
	pages = {2186--2188},
}

@inproceedings{albrecht_game-theoretic_2013,
	address = {Richland, SC},
	series = {{AAMAS} '13},
	title = {A {Game}-{Theoretic} {Model} and {Best}-{Response} {Learning} {Method} for {Ad} {Hoc} {Coordination} in {Multiagent} {Systems}},
	isbn = {978-1-4503-1993-5},
	abstract = {The ad hoc coordination problem is to design an ad hoc agent which is able to achieve optimal flexibility and efficiency in a multiagent system that admits no prior coordination between the ad hoc agent and the other agents. We conceptualise this problem formally as a stochastic Bayesian game in which the behaviour of a player is determined by its type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises a set of user-defined types to characterise players based on their observed behaviours. We evaluate HBA in the level-based foraging domain, showing that it outperforms several alternative algorithms using just a few user-defined types. We also report on a human-machine experiment in which the humans played Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms. The results show that HBA achieved equal efficiency but a significantly higher welfare and winning rate.},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the {International} {Conference} on {Autonomous} {Agents} and {Multi}-{Agent} {Systems}},
	author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
	month = may,
	year = {2013},
	keywords = {ad hoc coordination, harsanyi-bellman ad hoc coordination (HBA), stochastic bayesian games (SBG)},
	pages = {1155--1156},
}

@inproceedings{kingma_auto-encoding_2014,
 author = {Diederik P. Kingma and
Max Welling},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
 booktitle = {International Conference on Learning Representations},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {{Auto-Encoding Variational Bayes}},
 url = {http://arxiv.org/abs/1312.6114},
 year = {2014}
}

@article{davies_cluster_1979,
	title = {A {Cluster} {Separation} {Measure}},
	volume = {PAMI-1},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.1979.4766909},
	abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Davies, D. L. and Bouldin, D. W.},
	month = apr,
	year = {1979},
	keywords = {Algorithm design and analysis, Cluster, Clustering algorithms, Data analysis, Density measurement, Dispersion, Humans, Missiles, Multidimensional systems, Partitioning algorithms, Performance analysis, data partitions, multidimensional data analysis, parametric clustering, partitions, similarity measure},
	pages = {224--227},
}

@article{thorndike_who_1953,
	title = {Who {B}elongs in the {F}amily?},
	volume = {18},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289263},
	doi = {10.1007/BF02289263},
	language = {en},
	number = {4},
	urldate = {2021-01-26},
	journal = {Psychometrika},
	author = {Thorndike, Robert L.},
	month = dec,
	year = {1953},
	pages = {267--276},
}

@article{rousseeuw_silhouettes_1987,
	title = {Silhouettes: {A} {Graphical} {Aid} to the {Interpretation} and {Validation} of {Cluster} {Analysis}},
	volume = {20},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/0377042787901257},
	doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
	abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Rousseeuw, Peter J.},
	year = {1987},
	keywords = {Graphical display, classification, cluster analysis, clustering validity},
	pages = {53 -- 65},
}

@inproceedings{tan_multi-agent_1993,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {Independent} vs. {Cooperative} {Agents}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/citations?doi=10.1.1.55.8066},
	abstract = {Intelligent human agents exist in a cooperative social environment that facilitates learning. They learn not only by trialand -error, but also through cooperation by sharing instantaneous information, episodic experience, and learned knowledge. The key investigations of this paper are, "Given the same number of reinforcement learning agents, will cooperative agents outperform independent agents who do not communicate during learning?" and "What is the price for such cooperation?" Using independent agents as a benchmark, cooperative agents are studied in following ways: (1) sharing sensation, (2) sharing episodes, and (3) sharing learned policies. This paper shows that (a) additional sensation from another agent is beneficial if it can be used efficiently, (b) sharing learned policies or episodes among agents speeds up learning at the cost of communication, and (c) for joint tasks, agents engaging in partnership can significantly outperform independent agents although they may learn slo...},
	language = {en},
	urldate = {2021-01-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Tan, Ming},
	year = {1993},
}

@inproceedings{wang_roma_2020,
	title = {{ROMA}: {Multi}-{Agent} {Reinforcement} {Learning} with {Emergent} {Roles}},
	shorttitle = {{ROMA}},
	url = {http://proceedings.mlr.press/v119/wang20f.html},
	abstract = {The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based method...},
	language = {en},
	urldate = {2021-01-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Tonghan and Dong, Heng and Lesser, Victor and Zhang, Chongjie},
	month = nov,
	year = {2020},
	pages = {9876--9886},
}

@inproceedings{littman_markov_1994,
	title = {Markov {Games} as a {Framework} for {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/citations?doi=10.1.1.48.8623},
	abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
	language = {en},
	urldate = {2020-11-23},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Machine} {Learning}},
	author = {Littman, Michael L.},
	year = {1994},
}

@inproceedings{rashid_qmix_2018,
	title = {{QMIX}: {Monotonic} {Value} {Function} {Factorisation} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	shorttitle = {{QMIX}},
	url = {http://proceedings.mlr.press/v80/rashid18a.html},
	abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashio...},
	language = {en},
	urldate = {2020-11-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
	month = jul,
	year = {2018},
	keywords = {marl, rl},
	pages = {4295--4304},
}

@inproceedings{vezhnevets_options_2020,
	title = {{OPtions} as {REsponses}: {Grounding} behavioural hierarchies in multi-agent reinforcement learning},
	shorttitle = {{OPtions} as {REsponses}},
	url = {http://proceedings.mlr.press/v119/vezhnevets20a.html},
	abstract = {This paper investigates generalisation in multi-agent games, where the generality of the agent can be evaluated by playing against opponents it hasn’t seen during training. We propose two new games...},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Vezhnevets, Alexander and Wu, Yuhuai and Eckstein, Maria and Leblond, Rémi and Leibo, Joel Z.},
	month = nov,
	year = {2020},
	pages = {9733--9742},
}

@inproceedings{yang_mean_2018,
	title = {Mean {Field} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v80/yang18d.html},
	abstract = {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of ...},
	language = {en},
	urldate = {2020-12-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
	month = jul,
	year = {2018},
	pages = {5571--5580},
}

@inproceedings{engstrom_implementation_2020,
	title = {Implementation {Matters} in {Deep} {RL}: {A} {Case} {Study} on {PPO} and {TRPO}},
	shorttitle = {Implementation {Matters} in {Deep} {RL}},
	url = {https://iclr.cc/virtual_2020/poster_r1etN1rtPB.html},
	abstract = {We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms, Proximal Policy Optimization and Trust Region Policy Optimization. We investigate the consequences of "code-level optimizations:" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.},
	language = {en},
	urldate = {2020-11-23},
	booktitle = {Eighth {International} {Conference} on {Learning} {Representations}},
	author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
	month = apr,
	year = {2020},
}

@inproceedings{long_evolutionary_2020,
	title = {Evolutionary {Population} {Curriculum} for {Scaling} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {https://iclr.cc/virtual_2020/poster_SJxbHkrKDH.html},
	abstract = {In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on a popular MARL algorithm, MADDPG, and empirically show that our approach consistently outperforms baselines by a large margin as the number of agents grows exponentially. The source code and videos can be found at https://sites.google.com/view/epciclr2020.},
	language = {en},
	urldate = {2020-12-07},
	booktitle = {Eighth {International} {Conference} on {Learning} {Representations}},
	author = {Long, Qian and Zhou, Zihan and Gupta, Abhinav and Fang, Fei and Wu†, Yi and Wang†, Xiaolong},
	month = apr,
	year = {2020},
}

@inproceedings{foerster_counterfactual_2018,
	title = {Counterfactual {Multi}-{Agent} {Policy} {Gradients}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17193},
	abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
	language = {en},
	urldate = {2020-11-23},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	month = apr,
	year = {2018},
}

@inproceedings{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v48/mniha16.html},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present as...},
	language = {en},
	urldate = {2020-12-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	pages = {1928--1937},
}

@article{williams_simple_1992,
	title = {Simple {Statistical} {Gradient}-{Following} {Algorithms} for {Connectionist} {Reinforcement} {Learning}},
	volume = {8},
	issn = {0885-6125},
	url = {https://doi.org/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	number = {3–4},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	publisher = {Kluwer Academic Publishers},
	pages = {229--256},
}

@inproceedings{sunehag_value-decomposition_2018,
	address = {Richland, SC},
	series = {{AAMAS} '18},
	title = {Value-{Decomposition} {Networks} {For} {Cooperative} {Multi}-{Agent} {Learning} {Based} {On} {Team} {Reward}},
	abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
	urldate = {2020-12-15},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
	month = jul,
	year = {2018},
	keywords = {collaborative, dqn, multi-agent, neural networks, q-learning, reinforcement learning, value-decomposition},
	pages = {2085--2087},
}

@article{dulac-arnold_deep_2016,
	title = {Deep {Reinforcement} {Learning} in {Large} {Discrete} {Action} {Spaces}},
	url = {http://arxiv.org/abs/1512.07679},
	abstract = {Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.},
	urldate = {2020-12-07},
	journal = {arXiv:1512.07679 [cs, stat]},
	author = {Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
	month = apr,
	year = {2016},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{lowe_multi-agent_2017,
	title = {Multi-{Agent} {Actor}-{Critic} for {Mixed} {Cooperative}-{Competitive} {Environments}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html},
	language = {en},
	urldate = {2020-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
	year = {2017},
	pages = {6379--6390},
}
