\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018empirical}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Bao et~al.(2021)Bao, Dong, Piao, and Wei]{bao2021beit}
Bao, H., Dong, L., Piao, S., and Wei, F.
\newblock Beit: Bert pre-training of image transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chang et~al.(2023)Chang, Peng, and Chen]{chang2023llm4ts}
Chang, C., Peng, W.-C., and Chen, T.-F.
\newblock Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms.
\newblock \emph{arXiv preprint arXiv:2308.08469}, 2023.

\bibitem[Che et~al.(2023)Che, Liu, Zhou, Ren, Zhou, Sheng, Dai, and Dou]{che2023federated}
Che, T., Liu, J., Zhou, Y., Ren, J., Zhou, J., Sheng, V., Dai, H., and Dou, D.
\newblock Federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  7871--7888, 2023.

\bibitem[Chen(2024)]{chen2024model}
Chen, P.-Y.
\newblock Model reprogramming: Resource-efficient cross-domain machine learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pp.\  22584--22591, 2024.

\bibitem[Chen et~al.(2022)Chen, Wang, Peng, Wen, Zhou, and Sun]{chen2022learning}
Chen, W., Wang, W., Peng, B., Wen, Q., Zhou, T., and Sun, L.
\newblock Learning to rotate: Quaternion transformer for complicated periodical time series forecasting.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining}, pp.\  146--156, 2022.

\bibitem[Collins et~al.(2021)Collins, Hassani, Mokhtari, and Shakkottai]{collins2021exploiting}
Collins, L., Hassani, H., Mokhtari, A., and Shakkottai, S.
\newblock Exploiting shared representations for personalized federated learning.
\newblock In \emph{International conference on machine learning}, pp.\  2089--2099. PMLR, 2021.

\bibitem[Deldari et~al.(2022)Deldari, Xue, Saeed, He, Smith, and Salim]{deldari2022beyond}
Deldari, S., Xue, H., Saeed, A., He, J., Smith, D.~V., and Salim, F.~D.
\newblock Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data.
\newblock \emph{arXiv preprint arXiv:2206.02353}, 2022.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{DBLP:conf/naacl/DevlinCLT19}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019}, 2019.

\bibitem[Fan et~al.(2023)Fan, Kang, Ma, Chen, Wei, Fan, and Yang]{fan2023fate}
Fan, T., Kang, Y., Ma, G., Chen, W., Wei, W., Fan, L., and Yang, Q.
\newblock Fate-llm: A industrial grade federated learning framework for large language models.
\newblock \emph{arXiv preprint arXiv:2310.10049}, 2023.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International conference on machine learning}, pp.\  2790--2799. PMLR, 2019.

\bibitem[Jin et~al.(2023)Jin, Wen, Liang, Zhang, Xue, Wang, Zhang, Wang, Chen, Li, et~al.]{jin2023large}
Jin, M., Wen, Q., Liang, Y., Zhang, C., Xue, S., Wang, X., Zhang, J., Wang, Y., Chen, H., Li, X., et~al.
\newblock Large models for time series and spatio-temporal data: A survey and outlook.
\newblock \emph{arXiv preprint arXiv:2310.10196}, 2023.

\bibitem[Jin et~al.(2024)Jin, Wang, Ma, Chu, Zhang, Shi, Chen, Liang, Li, Pan, and Wen]{jin2024timellm}
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J.~Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q.
\newblock Time-{LLM}: Time series forecasting by reprogramming large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kim et~al.(2021)Kim, Kim, Tae, Park, Choi, and Choo]{kim2021reversible}
Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J.
\newblock Reversible instance normalization for accurate time-series forecasting against distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kitaev et~al.(2019)Kitaev, Kaiser, and Levskaya]{kitaev2019reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kuang et~al.(2023)Kuang, Qian, Li, Chen, Gao, Pan, Xie, Li, Ding, and Zhou]{kuang2023federatedscope}
Kuang, W., Qian, B., Li, Z., Chen, D., Gao, D., Pan, X., Xie, Y., Li, Y., Ding, B., and Zhou, J.
\newblock Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning.
\newblock \emph{arXiv preprint arXiv:2309.00363}, 2023.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and Liu]{lai2018modeling}
Lai, G., Chang, W.-C., Yang, Y., and Liu, H.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In \emph{The 41st international ACM SIGIR conference on research \& development in information retrieval}, pp.\  95--104, 2018.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0 429--450, 2020.

\bibitem[Liang et~al.(2024)Liang, Wen, Nie, Jiang, Jin, Song, Pan, and Wen]{liang2024foundation}
Liang, Y., Wen, H., Nie, Y., Jiang, Y., Jin, M., Song, D., Pan, S., and Wen, Q.
\newblock Foundation models for time series analysis: A tutorial and survey.
\newblock \emph{arXiv preprint arXiv:2403.14735}, 2024.

\bibitem[Liu et~al.(2024)Liu, Hu, Li, Diao, Liang, Hooi, and Zimmermann]{liu2024unitime}
Liu, X., Hu, J., Li, Y., Diao, S., Liang, Y., Hooi, B., and Zimmermann, R.
\newblock Unitime: A language-empowered unified model for cross-domain time series forecasting.
\newblock In \emph{Proceedings of the ACM Web Conference 2024}, 2024.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282. PMLR, 2017.

\bibitem[Misra et~al.(2023)Misra, Goyal, Runwal, and Chen]{misra2023reprogramming}
Misra, D., Goyal, A., Runwal, B., and Chen, P.~Y.
\newblock Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets.
\newblock \emph{arXiv preprint arXiv:2308.14969}, 2023.

\bibitem[Nie et~al.(2022)Nie, Nguyen, Sinthong, and Kalagnanam]{nie2022time}
Nie, Y., Nguyen, N.~H., Sinthong, P., and Kalagnanam, J.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Pfeiffer et~al.(2021)Pfeiffer, Kamath, R{\"u}ckl{\'e}, Cho, and Gurevych]{pfeiffer2021adapterfusion}
Pfeiffer, J., Kamath, A., R{\"u}ckl{\'e}, A., Cho, K., and Gurevych, I.
\newblock Adapterfusion: Non-destructive task composition for transfer learning.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pp.\  487--503, 2021.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and Januschowski]{salinas2020deepar}
Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent networks.
\newblock \emph{International journal of forecasting}, 36\penalty0 (3):\penalty0 1181--1191, 2020.

\bibitem[Su et~al.(2024)Su, Hu, Li, and Li]{su2024titanic}
Su, N., Hu, C., Li, B., and Li, B.
\newblock Titanic: Towards production federated learning with large language models.
\newblock In \emph{{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications}, 2024.

\bibitem[Sun et~al.(2022)Sun, Mendieta, Yang, and Chen]{sun2022conquering}
Sun, G., Mendieta, M., Yang, T., and Chen, C.
\newblock Conquering the communication constraints to enable large pre-trained models in federated learning.
\newblock \emph{arXiv preprint arXiv:2210.01708}, 2022.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou, H.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{International conference on machine learning}, pp.\  10347--10357. PMLR, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Wang et~al.(2022)Wang, Peng, Huang, Wang, Chen, and Xiao]{wang2022micn}
Wang, H., Peng, J., Huang, F., Wang, J., Chen, J., and Xiao, Y.
\newblock Micn: Multi-scale local and global context modeling for long-term series forecasting.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wen et~al.(2022)Wen, Yang, Zhou, and Sun]{wen2022robust}
Wen, Q., Yang, L., Zhou, T., and Sun, L.
\newblock Robust time series analysis and applications: An industrial perspective.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  4836--4837, 2022.

\bibitem[Woo et~al.(2024)Woo, Liu, Kumar, Xiong, Savarese, and Sahoo]{woo2024unified}
Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D.
\newblock Unified training of universal time series forecasting transformers.
\newblock \emph{arXiv preprint arXiv:2402.02592}, 2024.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Wu, H., Xu, J., Wang, J., and Long, M.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 22419--22430, 2021.

\bibitem[Wu et~al.(2022)Wu, Hu, Liu, Zhou, Wang, and Long]{wu2022timesnet}
Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{The eleventh international conference on learning representations}, 2022.

\bibitem[Yang et~al.(2021)Yang, Tsai, and Chen]{yang2021voice2series}
Yang, C.-H.~H., Tsai, Y.-Y., and Chen, P.-Y.
\newblock Voice2series: Reprogramming acoustic models for time series classification.
\newblock In \emph{International conference on machine learning}, pp.\  11808--11819. PMLR, 2021.

\bibitem[Ye et~al.(2024)Ye, Wang, Chai, Li, Li, Xu, Du, Wang, and Chen]{ye2024openfedllm}
Ye, R., Wang, W., Chai, J., Li, D., Li, Z., Xu, Y., Du, Y., Wang, Y., and Chen, S.
\newblock Openfedllm: Training large language models on decentralized private data via federated learning.
\newblock \emph{arXiv preprint arXiv:2402.06954}, 2024.

\bibitem[Yin et~al.(2023)Yin, Fu, Zhao, Li, Sun, Xu, and Chen]{yin2023survey}
Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E.
\newblock A survey on multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2306.13549}, 2023.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{zeng2023transformers}
Zeng, A., Chen, M., Zhang, L., and Xu, Q.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pp.\  11121--11128, 2023.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Vahidian, Kuo, Li, Zhang, Yu, Wang, and Chen]{zhang2024towards}
Zhang, J., Vahidian, S., Kuo, M., Li, C., Zhang, R., Yu, T., Wang, G., and Chen, Y.
\newblock Towards building the federatedgpt: Federated instruction tuning.
\newblock In \emph{ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  6915--6919. IEEE, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wen, Zhang, Cai, Jin, Liu, Zhang, Liang, Pang, Song, et~al.]{zhang2024self}
Zhang, K., Wen, Q., Zhang, C., Cai, R., Jin, M., Liu, Y., Zhang, J.~Y., Liang, Y., Pang, G., Song, D., et~al.
\newblock Self-supervised learning for time series analysis: Taxonomy, progress, and prospects.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{c}})Zhang, Zhou, Hu, Feng, Weng, and Chen]{zhang2024personalized}
Zhang, P., Zhou, Y., Hu, M., Feng, J., Weng, J., and Chen, M.
\newblock Personalized federated instruction tuning via neural architecture search.
\newblock \emph{arXiv preprint arXiv:2402.16919}, 2024{\natexlab{c}}.

\bibitem[Zhang et~al.(2022)Zhang, Zhao, Tsiligkaridis, and Zitnik]{zhang2022self}
Zhang, X., Zhao, Z., Tsiligkaridis, T., and Zitnik, M.
\newblock Self-supervised contrastive pre-training for time series via time-frequency consistency.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3988--4003, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Dai, Wang, Yu, Qu, and Xu]{zhang2023fedpetuning}
Zhang, Z., Yang, Y., Dai, Y., Wang, Q., Yu, Y., Qu, L., and Xu, Z.
\newblock Fedpetuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  9963--9977, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{zhou2021informer}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pp.\  11106--11115, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Wang, Sun, and Jin]{zhou2022fedformer}
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In \emph{International conference on machine learning}, pp.\  27268--27286. PMLR, 2022.

\bibitem[Zhou et~al.(2024)Zhou, Niu, Sun, Jin, et~al.]{zhou2024one}
Zhou, T., Niu, P., Sun, L., Jin, R., et~al.
\newblock One fits all: Power general time series analysis by pretrained lm.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\end{thebibliography}
