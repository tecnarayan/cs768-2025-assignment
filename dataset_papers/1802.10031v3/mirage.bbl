\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1329--1338, 2016.

\bibitem[Dud{\'\i}k et~al.(2011)Dud{\'\i}k, Langford, and Li]{dudik2011doubly}
Dud{\'\i}k, M., Langford, J., and Li, L.
\newblock Doubly robust policy evaluation and learning.
\newblock \emph{arXiv preprint arXiv:1103.4601}, 2011.

\bibitem[Grathwohl et~al.(2018)Grathwohl, Choi, Wu, Roeder, and
  Duvenaud]{grathwohl2018backpropagation}
Grathwohl, W., Choi, D., Wu, Y., Roeder, G., and Duvenaud, D.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Gruslys et~al.(2017)Gruslys, Azar, Bellemare, and
  Munos]{gruslys2017reactor}
Gruslys, A., Azar, M.~G., Bellemare, M.~G., and Munos, R.
\newblock The reactor: A sample-efficient actor-critic architecture.
\newblock \emph{arXiv preprint arXiv:1704.04651}, 2017.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{gu2016continuous}
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2829--2838, 2016.

\bibitem[Gu et~al.(2017{\natexlab{a}})Gu, Lillicrap, Ghahramani, Turner, and
  Levine]{gu2017q}
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R.~E., and Levine, S.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017{\natexlab{a}}.

\bibitem[Gu et~al.(2017{\natexlab{b}})Gu, Lillicrap, Turner, Ghahramani,
  Sch{\"o}lkopf, and Levine]{gu2017interpolated}
Gu, S., Lillicrap, T., Turner, R.~E., Ghahramani, Z., Sch{\"o}lkopf, B., and
  Levine, S.
\newblock Interpolated policy gradient: Merging on-policy and off-policy
  gradient estimation for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3849--3858, 2017{\natexlab{b}}.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Erez, and
  Tassa]{heess2015learning}
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and Tassa, Y.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2944--2952, 2015.

\bibitem[Jie \& Abbeel(2010)Jie and Abbeel]{jie2010connection}
Jie, T. and Abbeel, P.
\newblock On a connection between importance sampling and the likelihood ratio
  policy gradient.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1000--1008, 2010.

\bibitem[Kakade(2002)]{kakade2002natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1531--1538, 2002.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu \& Nocedal(1989)Liu and Nocedal]{liu1989limited}
Liu, D.~C. and Nocedal, J.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1-3):\penalty0
  503--528, 1989.

\bibitem[Liu et~al.(2018)Liu, Feng, Mao, Zhou, Peng, and Liu]{liu2018sample}
Liu, H., Feng, Y., Mao, Y., Zhou, D., Peng, J., and Liu, Q.
\newblock Action-dependent control variates for policy optimization via stein
  identity.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Mnih \& Gregor(2014)Mnih and Gregor]{mnih2014neural}
Mnih, A. and Gregor, K.
\newblock Neural variational inference and learning in belief networks.
\newblock \emph{arXiv preprint arXiv:1402.0030}, 2014.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1928--1937, 2016.

\bibitem[O'Donoghue et~al.(2016)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{o2016pgq}
O'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
\newblock Pgq: Combining policy gradient and q-learning.
\newblock \emph{arXiv preprint arXiv:1611.01626}, 2016.

\bibitem[Owen(2013)]{owen2013monte}
Owen, A.~B.
\newblock Monte carlo theory, methods and examples.
\newblock \emph{Monte Carlo Theory, Methods and Examples. Art Owen}, 2013.

\bibitem[Pardo et~al.(2017)Pardo, Tavakoli, Levdik, and
  Kormushev]{pardo2017time}
Pardo, F., Tavakoli, A., Levdik, V., and Kormushev, P.
\newblock Time limits in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1712.00378}, 2017.

\bibitem[Peters \& Schaal(2006)Peters and Schaal]{peters2006policy}
Peters, J. and Schaal, S.
\newblock Policy gradient methods for robotics.
\newblock In \emph{Intelligent Robots and Systems, 2006 IEEE/RSJ International
  Conference on}, pp.\  2219--2225. IEEE, 2006.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{ICML}, 2014.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Stengel(1986)]{stengel1986optimal}
Stengel, R.~F.
\newblock \emph{Optimal control and estimation}.
\newblock Courier Corporation, 1986.

\bibitem[Sutton(1990)]{sutton1990integrated}
Sutton, R.~S.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{Machine Learning Proceedings 1990}, pp.\  216--224.
  Elsevier, 1990.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}, volume~1.
\newblock MIT Press Cambridge, 1998.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1057--1063, 2000.

\bibitem[Tesauro(1995)]{tesauro1995temporal}
Tesauro, G.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Communications of the ACM}, 38\penalty0 (3):\penalty0 58--68,
  1995.

\bibitem[Thomas(2014)]{thomas2014bias}
Thomas, P.
\newblock Bias in natural actor-critic algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  441--448, 2014.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2139--2148, 2016.

\bibitem[Thomas \& Brunskill(2017)Thomas and Brunskill]{thomas2017policy}
Thomas, P.~S. and Brunskill, E.
\newblock Policy gradient methods for reinforcement learning with function
  approximation and action-dependent baselines.
\newblock \emph{arXiv preprint arXiv:1706.06643}, 2017.

\bibitem[Wang et~al.(2016{\natexlab{a}})Wang, Agarwal, and
  Dudik]{wang2016optimal}
Wang, Y.-X., Agarwal, A., and Dudik, M.
\newblock Optimal and adaptive off-policy evaluation in contextual bandits.
\newblock \emph{arXiv preprint arXiv:1612.01205}, 2016{\natexlab{a}}.

\bibitem[Wang et~al.(2016{\natexlab{b}})Wang, Bapst, Heess, Mnih, Munos,
  Kavukcuoglu, and de~Freitas]{wang2016sample}
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., and
  de~Freitas, N.
\newblock Sample efficient actor-critic with experience replay.
\newblock \emph{arXiv preprint arXiv:1611.01224}, 2016{\natexlab{b}}.

\bibitem[Weaver \& Tao(2001)Weaver and Tao]{weaver2001optimal}
Weaver, L. and Tao, N.
\newblock The optimal reward baseline for gradient-based reinforcement
  learning.
\newblock In \emph{Proceedings of the Seventeenth conference on Uncertainty in
  artificial intelligence}, pp.\  538--545. Morgan Kaufmann Publishers Inc.,
  2001.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock In \emph{Reinforcement Learning}, pp.\  5--32. Springer, 1992.

\bibitem[Wu et~al.(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade, Mordatch,
  and Abbeel]{wu2018variance}
Wu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,
  Mordatch, I., and Abbeel, P.
\newblock Variance reduction for policy gradient with action-dependent
  factorized baselines.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\end{thebibliography}
