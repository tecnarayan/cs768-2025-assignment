
@article{acceleratedSpars2019,
  title={Accelerating CNN Training by Pruning Activation Gradients},
  author={Xucheng Ye and P. Dai and J. Luo and X. Guo and Y. Qi and Jianlei Yang and Yiran Chen},
  journal={arXiv preprint arXiv:1908.00173},
  year={2019},
  url = {http://arxiv.org/abs/1908.00173}
  
}


@inproceedings{lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Jonathan Frankle and Michael Carbin},
  booktitle={ICLR},
  year={2018}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@inproceedings{n:mStructured,
  title={Learning N:M fine-grained structures sparse neural networks from scratch},
  author={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},
  booktitle={ICLR},
  year={2021}
}


@article{Mozer1989a,
  title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  author={Mozer, M. C. and Smolensky, P.},
  journal={Advances in neural information processing systems, pp. 107–115},
  year={1989a},
}
@article{Mozer1989b,
  title={Using Relevance to Reduce Network Size Automatically},
  author={Mozer, M. C. and Smolensky, P.},
  journal={Connection Science, 1(1):3–16},
  year={1989b},
}
@article{Karin1990,
  title={A simple procedure for pruning backpropagation trained neural networks},
  author={Karnin, E. D.},
  journal={IEEE transactions on neural networks, 1(2):239–242},
  year={1990},
}

@article{Han2015LearningBW,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Song Han and J. Pool and John Tran and W. Dally},
  journal={ArXiv},
  year={2015},
  volume={abs/1506.02626}
}



@inproceedings{l0regul,
title={Learning Sparse Neural Networks through L0 Regularization},
  author={Christos Louizos and M. Welling and Diederik P. Kingma},
  booktitle={ICLR},
  year={2018}
}

@article{Renda2020ComparingRA,
  title={Comparing Rewinding and Fine-tuning in Neural Network Pruning},
  author={A. Renda and Jonathan Frankle and Michael Carbin},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.02389}
}


@inproceedings{rewinding,
 title={Comparing Rewinding and Fine-tuning in Neural Network Pruning},
  author={A. Renda and Jonathan Frankle and Michael Carbin},
  booktitle={ICLR},
  year={2020}
}


@inproceedings{snip,
  title={SNIP: Single-shot Network Pruning based on Connection Sensitivity},
  author={N. Lee and Thalaiyasingam Ajanthan and P. Torr},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{l1Filter,
  title = {Pruning filters for efficient ConvNets},
  author = {Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
  booktitle = {ICLR},
  year= {2017}
}

@article{Nvidia,
  title={ a100 tensor core gpu architecture},
  author={Nvidia},
   year={2020},
  url = {http://https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}
}

@article{NvidiaTensorCores,
  title={NVIDIA Deep Learning Examples for Tensor Cores},
  author={Nvidia},
   year={2018},
  url = {https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch}
}



@article{Janowsky1989,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, S. A.},
  journal={Physical Review A, 39(12):6600–6603},
  year={1989},
  url = { https://link.aps.org/doi/10.1103/PhysRevA.39.6600}
}

@article{thinet,
  title={ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression},
  author={Jian-Hao Luo and Jianxin Wu and W. Lin},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={5068-5076}
}

@article{Fedus2021SwitchTS,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={W. Fedus and Barret Zoph and Noam Shazeer},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.03961}
}

@article{gpt3,
  title={Language Models are Few-Shot Learners},
  author={T. Brown and B. Mann and Nick Ryder and Melanie Subbiah and J. Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and G. Kr{\"u}ger and T. Henighan and R. Child and Aditya Ramesh and D. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and E. Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and J. Clark and Christopher Berner and Sam McCandlish and A. Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}

@inproceedings{Banner2018ScalableMF,
  title={Scalable Methods for 8-bit Training of Neural Networks},
  author={Ron Banner and Itay Hubara and Elad Hoffer and Daniel Soudry},
  booktitle={NeurIPS},
  year={2018}
}
@article{Nahshan2019LossAP,
  title={Loss Aware Post-training Quantization},
  author={Yury Nahshan and Brian Chmiel and Chaim Baskin and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.07190}
}

@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}



@inproceedings{blockSparsity,
  title={Learning structured sparsity in
deep neural networks},
  author={Wei Wen and Chunpeng Wu and Yandan Wang and Yiran Chen and Hai Li.},
  booktitle={In Advances in neural information processing systems, pp. 2074–2082},
  year={2016}
}

@inproceedings{markus2020,
Author = {Markus Nagel and Rana Ali Amjad and Mart van Baalen and Christos Louizos and Tijmen Blankevoort},
Title = {Up or Down? Adaptive Rounding for Post-Training Quantization},
  booktitle={ICML},
Year = {2020},
Eprint = {arXiv:2004.10568}
}

@article{Hubara2020ImprovingPT,
  title={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},
  author={Itay Hubara and Yury Nahshan and Yair Hanani and R. Banner and Daniel Soudry},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.10518}
}
@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017},
  publisher={JMLR. org}
}
@article{banner2018post,
  title={Post-training 4-bit quantization of convolution networks for rapid-deployment},
  author={Banner, Ron and Nahshan, Yury and Hoffer, Elad and Soudry, Daniel},
  journal={arXiv preprint arXiv:1810.05723},
  year={2018}
}
@article{finkelstein2019fighting,
  title={Fighting quantization bias with bias},
  author={Finkelstein, Alexander and Almog, Uri and Grobman, Mark},
  journal={arXiv preprint arXiv:1906.03193},
  year={2019}
}
@article{gray2017gpu,
  title={Gpu kernels for block-sparse weights},
  author={Gray, Scott and Radford, Alec and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1711.09224},
  volume={3},
  year={2017}
}
@article{bellec2017deep,
  title={Deep rewiring: Training very sparse deep networks},
  author={Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  journal={arXiv preprint arXiv:1711.05136},
  year={2017}
}
@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={International Conference on Machine Learning},
  pages={4646--4655},
  year={2019},
  organization={PMLR}
}
@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}
@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}
@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}

@inproceedings{tan2019mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2820--2828},
  year={2019}
}
@inproceedings{wu2019fbnet,
  title={Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search},
  author={Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10734--10742},
  year={2019}
}

@article{ahuja1988network,
  title={Network flows},
  author={Ahuja, Ravindra K and Magnanti, Thomas L and Orlin, James B},
  year={1988},
  publisher={Cambridge, Mass.: Alfred P. Sloan School of Management, Massachusetts~…}
}

@inproceedings{torchvision,
author = {Marcel, S\'{e}bastien and Rodriguez, Yann},
title = {Torchvision the Machine-Vision Package of Torch},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874254},
doi = {10.1145/1873951.1874254},
abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1485–1488},
numpages = {4},
keywords = {vision, pattern recognition, face detection and recognition, open source, machine learning},
location = {Firenze, Italy},
series = {MM '10}
}

@article{resnet,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}
@article{liu2020sparse,
  title={Sparse Systolic Tensor Array for Efficient CNN Hardware Acceleration},
  author={Liu, Zhi-Gang and Whatmough, Paul N and Mattina, Matthew},
  journal={arXiv preprint arXiv:2009.02381},
  year={2020}
}

@article{Elsen2020FastSC,
  title={Fast Sparse ConvNets},
  author={Erich Elsen and Marat Dukhan and Trevor Gale and Karen Simonyan},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={14617-14626}
}

@inproceedings{Gale2020SparseGK,
  title={Sparse GPU Kernels for Deep Learning},
  author={Trevor Gale and Matei A. Zaharia and Cliff Young and Erich Elsen},
  booktitle={SC},
  year={2020}
}

@article{Stosic2021SearchSF,
  title={Search Spaces for Neural Model Training},
  author={Darko Stosic and Dusan Stosic},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.12920}
}