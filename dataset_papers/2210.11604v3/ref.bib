
@inproceedings{li2022settling,
  title={Settling the horizon-dependence of sample complexity in reinforcement learning},
  author={Li, Yuanzhi and Wang, Ruosong and Yang, Lin F},
  booktitle={2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={965--976},
  year={2022},
  organization={IEEE}
}

@article{zhou2022computationally,
  title={Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs},
  author={Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2205.11507},
  year={2022}
}

@article{zhang2021improved,
  title={Improved variance-aware confidence sets for linear bandits and linear mixture mdp},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4342--4355},
  year={2021}
}


@article{wang2020long,
  title={Is long horizon reinforcement learning more difficult than short horizon reinforcement learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}

@article{yin2022near,
  title={Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism},
  author={Yin, Ming and Duan, Yaqi and Wang, Mengdi and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2203.05804},
  year={2022}
}

@article{kim2021improved,
  title={Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps},
  author={Kim, Yeoneung and Yang, Insoon and Jun, Kwang-Sung},
  journal={arXiv preprint arXiv:2111.03289},
  year={2021}
}

@misc{paper:latent_mdp,
  title={RL for latent MDPs: Regret guarantees and a lower bound},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}



@article{xiong2021randomized,
  title={Randomized exploration is near-optimal for tabular mdp},
  author={Xiong, Zhihan and Shen, Ruoqi and Du, Simon S},
  journal={arXiv preprint arXiv:2102.09703},
  year={2021}
}




@InProceedings{paper:particle_filtering,
  title={When is particle filtering efficient for planning in partially observed linear dynamical systems?},
  author={Simon Shaolei Du and Wei Hu and Zhiyuan Li and Ruoqi Shen and Zhao Song and Jiajun Wu},
  booktitle={UAI},
  year={2021}
}





@inproceedings{paper:sample_efficient_pomdp,
  title={Sample-efficient reinforcement learning of undercomplete POMDPs},
  author={Jin, Chi and Kakade, Sham and Krishnamurthy, Akshay and Liu, Qinghua},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18530--18539},
  year={2020}
}





@InProceedings{paper:block_mdp,
  title={Provably efficient RL with rich observations via latent state decoding},
  author={Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
  booktitle={International Conference on Machine Learning},
  pages={1665--1674},
  year={2019},
  organization={PMLR}
}





@inproceedings{paper:horizon_free_offline,
  title={Nearly horizon-free offline reinforcement learning},
  author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}





@article{paper:multimodel_mdp,
  title={Multi-model Markov decision processes},
  author={Steimle, Lauren N and Kaufman, David L and Denton, Brian T},
  journal={IISE Transactions},
  volume={53},
  number={10},
  pages={1124--1139},
  year={2021},
  publisher={Taylor \& Francis}
}





@inproceedings{paper:momdp_plan,
  title={MOMDPs: A Solution for Modelling Adaptive Management Problems},
  author={I. Chades and Josie Carwardine and Tara G. Martin and Sam Nicol and R{\'e}gis Sabbadin and Olivier Buffet},
  booktitle={AAAI},
  year={2012}
}





@article{paper:concurrent_mdp,
  title={Computation of weighted sums of rewards for concurrent MDPs},
  author={Peter Buchholz and Dimitri Scheftelowitsch},
  journal={Mathematical Methods of Operations Research},
  year={2019},
  volume={89},
  pages={1-42}
}





@article{paper:contextual_mdp,
  title={Contextual Markov Decision Processes},
  author={Assaf Hallak and Dotan Di Castro and Shie Mannor},
  journal={ArXiv},
  year={2015},
  volume={abs/1502.02259}
}





@article{paper:multitask_rl,
  title={Sample Complexity of Multi-task Reinforcement Learning},
  author={Emma Brunskill and Lihong Li},
  journal={ArXiv},
  year={2013},
  volume={abs/1309.6821}
}





@inproceedings{paper:cont_state_multitask_rl,
  title={PAC Continuous State Online Multitask Reinforcement Learning with Identification},
  author={Yao Liu and Zhaohan Daniel Guo and Emma Brunskill},
  booktitle={AAMAS},
  year={2016}
}





@article{paper:transfer_learning_survey,
  title={Transfer learning for reinforcement learning domains: A survey.},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  year={2009}
}





@inproceedings{paper:transfer,
  title={Transfer in Reinforcement Learning: A Framework and a Survey},
  author={Alessandro Lazaric},
  booktitle={Reinforcement Learning},
  year={2012}
}





@article{paper:multitask_transfer,
  title={Provably Efficient Multi-Task Reinforcement Learning with Model Transfer},
  author={Chicheng Zhang and Zhi Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.08622}
}






@InProceedings{paper:cdp_low_rank,
  title={Contextual decision processes with low bellman rank are pac-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}





@article{paper:curl_co,
  title={Understanding Curriculum Learning in Policy Optimization for Solving Combinatorial Optimization Problems},
  author={Zhou, Runlong and Tian, Yuandong and Wu, Yi and Du, Simon S},
  journal={arXiv preprint arXiv:2202.05423},
  year={2022}
}





@inproceedings{paper:eb_ssp,
  title={Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret},
  author={Tarbouriech, Jean and Zhou, Runlong and Du, Simon S and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}






@inproceedings{paper:mvp,
  title={Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon},
  author={Zihan Zhang and Xiangyang Ji and Simon Shaolei Du},
  booktitle={COLT},
  year={2021}
}





@article{paper:ucbadv,
  title={Almost optimal model-free reinforcement learning via reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}





@inproceedings{paper:euler,
  title={Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds},
  author={Andrea Zanette and Emma Brunskill},
  booktitle={ICML},
  year={2019}
}





@inproceedings{paper:ucbvi,
  title={Minimax Regret Bounds for Reinforcement Learning},
  author={Mohammad Gheshlaghi Azar and Ian Osband and R{\'e}mi Munos},
  booktitle={ICML},
  year={2017}
}





@inproceedings{paper:memless,
  title={Memoryless policies: Theoretical limitations and practical results},
  author={Littman, Michael L},
  booktitle={From Animals to Animats 3: Proceedings of the third international conference on simulation of adaptive behavior},
  volume={3},
  pages={238},
  year={1994},
  organization={Cambridge, MA}
}





@inproceedings{paper:lcb_ssp,
  title={Implicit finite-horizon approximation and efficient optimal algorithms for stochastic shortest path},
  author={Chen, Liyu and Jafarnia-Jahromi, Mehdi and Jain, Rahul and Luo, Haipeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}





@inproceedings{paper:ulcvi_ssp,
  title={Minimax regret for stochastic shortest path},
  author={Cohen, Alon and Efroni, Yonathan and Mansour, Yishay and Rosenberg, Aviv},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}





@inproceedings{paper:ubev,
  title={Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning},
  author={Christoph Dann and Tor Lattimore and Emma Brunskill},
  booktitle={NIPS},
  year={2017}
}





@inproceedings{paper:bernstein_ssp,
  title={Near-optimal Regret Bounds for Stochastic Shortest Path},
  author={Alon Cohen and Haim Kaplan and Y. Mansour and Aviv Rosenberg},
  booktitle={ICML},
  year={2020}
}





@inproceedings{paper:sarsop_pomdp,
  title={SARSOP: Efficient Point-Based POMDP Planning by Approximating Optimally Reachable Belief Spaces},
  author={Hanna Kurniawati and David Hsu and Wee Sun Lee},
  booktitle={Robotics: Science and Systems},
  year={2008}
}





@article{paper:pbvi_approx_pomdp,
  title={Anytime Point-Based Approximations for Large POMDPs},
  author={Joelle Pineau and Geoffrey J. Gordon and Sebastian Thrun},
  journal={J. Artif. Intell. Res.},
  year={2006},
  volume={27},
  pages={335-380}
}





@article{paper:perseus_pomdp,
  title={Perseus: Randomized Point-based Value Iteration for POMDPs},
  author={Matthijs T. J. Spaan and Nikos A. Vlassis},
  journal={J. Artif. Intell. Res.},
  year={2005},
  volume={24},
  pages={195-220}
}





@inproceedings{paper:pegasus_pomdp,
  title={PEGASUS: A policy search method for large MDPs and POMDPs},
  author={A. Ng and Michael I. Jordan},
  booktitle={UAI},
  year={2000}
}





@inproceedings{paper:hsvi_pomdp,
  title={Heuristic Search Value Iteration for POMDPs},
  author={Trey Smith and Reid G. Simmons},
  booktitle={UAI},
  year={2004}
}





@inproceedings{paper:rl_for_pomdp,
  title={Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems},
  author={T. Jaakkola and Satinder Singh and Michael I. Jordan},
  booktitle={NIPS},
  year={1994}
}





@inproceedings{paper:pbvi_pomdp,
  title={Point-based value iteration: An anytime algorithm for POMDPs},
  author={Joelle Pineau and Geoff Gordon and Sebastian Thrun},
  booktitle={IJCAI},
  year={2003}
}





@article{paper:m_step_pomdp,
  title={Provable Reinforcement Learning with a Short-Term Memory},
  author={Efroni, Yonathan and Jin, Chi and Krishnamurthy, Akshay and Miryoosefi, Sobhan},
  journal={arXiv preprint arXiv:2202.03983},
  year={2022}
}



@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture markov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}

@article{paper:reward_mixing_mdp,
  title={Reinforcement Learning in Reward-Mixing MDPs},
  author={Jeongyeol Kwon and Yonathan Efroni and Constantine Caramanis and Shie Mannor},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.03743}
}

@article{ren2021nearly,
  title={Nearly horizon-free offline reinforcement learning},
  author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15621--15634},
  year={2021}
}


@article{zhang2020nearly,
  title={Nearly minimax optimal reward-free reinforcement learning},
  author={Zhang, Zihan and Du, Simon S and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2010.05901},
  year={2020}
}




@inproceedings{paper:bennett,
  title={Empirical Bernstein Bounds and Sample-Variance Penalization},
  author={Andreas Maurer and Massimiliano Pontil},
  booktitle={COLT},
  year={2009}
}





@inproceedings{paper:euler_gp,
  title={Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies},
  author={Yonathan Efroni and Nadav Merlis and Mohammad Ghavamzadeh and Shie Mannor},
  booktitle={NeurIPS},
  year={2019}
}




@InProceedings{paper:episodic_lower_bound,
  title = 	 {Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited},
  author =       {Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {578--598},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/domingues21a/domingues21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/domingues21a.html},
  abstract = 	 {In this paper, we propose new problem-independent lower bounds on the sample complexity and regret in episodic MDPs, with a particular focus on the \emph{non-stationary case} in which the transition kernel is allowed to change in each stage of the episode. Our main contribution is a lower bound of $\Omega((H^3SA/\epsilon^2)\log(1/\delta))$ on the sample complexity of an $(\varepsilon,\delta)$-PAC algorithm for best policy identification in a non-stationary MDP, relying on a construction of “hard MDPs” which is different from the ones previously used in the literature. Using this same class of MDPs, we also provide a rigorous proof of the $\Omega(\sqrt{H^3SAT})$ regret bound for non-stationary MDPs. Finally, we discuss connections to PAC-MDP lower bounds.}
}




@article{paper:regret_bandit,
author = {Garivier, Aurélien and Ménard, Pierre and Stoltz, Gilles},
year = {2016},
month = {02},
pages = {},
title = {Explore First, Exploit Next: The True Shape of Regret in Bandit Problems},
volume = {44},
journal = {Mathematics of Operations Research},
doi = {10.1287/moor.2017.0928}
}




@book{paper:bandit_algorithms,
place={Cambridge}, title={Bandit Algorithms}, DOI={10.1017/9781108571401}, publisher={Cambridge University Press}, author={Lattimore, Tor and Szepesvári, Csaba}, year={2020}
}




@INPROCEEDINGS{paper:yao_minimax,
author={Yao, Andrew Chi-Chih},  booktitle={18th Annual Symposium on Foundations of Computer Science (sfcs 1977)},   title={Probabilistic computations: Toward a unified measure of complexity},   year={1977},  volume={},  number={},  pages={222-227},  doi={10.1109/SFCS.1977.24}}




@inproceedings{paper:good_rep,
title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
author={Simon S. Du and Sham M. Kakade and Ruosong Wang and Lin F. Yang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1genAVKPB}
}



@article{paper:ucrl2b,
  title={Improved analysis of ucrl2 with empirical bernstein inequality},
  author={Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:2007.05456},
  year={2020}
}



@inproceedings{paper:two_degree_mdp,
  title={PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}



@inproceedings{paper:lb_communication_complexity_symm,
  title={Lower bounds for number-in-hand multiparty communication complexity, made easy},
  author={Phillips, Jeff M and Verbin, Elad and Zhang, Qin},
  booktitle={Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms},
  pages={486--501},
  year={2012},
  organization={SIAM}
}



@inproceedings{paper:lb_message_passing,
  title={An optimal lower bound for distinct elements in the message passing model},
  author={Woodruff, David P and Zhang, Qin},
  booktitle={Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms},
  pages={718--733},
  year={2014},
  organization={SIAM}
}



@inproceedings{paper:communication_complexity_triangle,
  title={On the multiparty communication complexity of testing triangle-freeness},
  author={Fischer, Orr and Gershtein, Shay and Oshman, Rotem},
  booktitle={Proceedings of the ACM Symposium on Principles of Distributed Computing},
  pages={111--120},
  year={2017}
}



@inproceedings{paper:communication_complexity_opt,
  title={The communication complexity of optimization},
  author={Vempala, Santosh S and Wang, Ruosong and Woodruff, David P},
  booktitle={Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1733--1752},
  year={2020},
  organization={SIAM}
}






@inproceedings{paper:meta_world,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on robot learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}




@inproceedings{paper:meta_learning_vi,
  title={Meta-learning with shared amortized variational inference},
  author={Iakovleva, Ekaterina and Verbeek, Jakob and Alahari, Karteek},
  booktitle={International Conference on Machine Learning},
  pages={4572--4582},
  year={2020},
  organization={PMLR}
}




@article{paper:prob_meta_learning,
  title={Probabilistic model-agnostic meta-learning},
  author={Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}




@misc{paper:latent_variable_mdp,
  title={Latent-variable MDP models for adapting the interaction environment of diverse users},
  author={Ramamoorthy, Subramanian and Mahmud, MM Hassan and Rosman, Benjamin and Kohli, Pushmeet},
  journal={Tech. Rep.},
  year={2013},
  publisher={School Of Informatics, University ofEdinburgh}
}




@inproceedings{paper:hidden_parameter_mdp,
  title={Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations},
  author={Doshi-Velez, Finale and Konidaris, George},
  booktitle={IJCAI: proceedings of the conference},
  volume={2016},
  pages={1432},
  year={2016},
 
 organization={NIH Public Access}
}



@inproceedings{paper:policy_transfer_hidden_parameter_mdp,
  title={Direct policy transfer via hidden parameter markov decision processes},
  author={Yao, Jiayu and Killian, Taylor and Konidaris, George and Doshi-Velez, Finale},
  booktitle={LLARLA Workshop, FAIM},
  volume={2018},
  year={2018}
}




@article{paper:ucrl2,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}




@inproceedings{paper:kl_ucrl,
  title={Optimism in reinforcement learning and Kullback-Leibler divergence},
  author={Filippi, Sarah and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  booktitle={2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={115--122},
  year={2010},
  organization={IEEE}
}




@inproceedings{paper:orlc,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}




@article{paper:mdp_optimism,
  title={A unifying view of optimism in episodic reinforcement learning},
  author={Neu, Gergely and Pike-Burke, Ciara},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1392--1403},
  year={2020}
}



@InProceedings{paper:horizon_free,
  title={Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies},
  author={Zihan Zhang and Xiangyang Ji and Simon Shaolei Du},
  booktitle={Annual Conference Computational Learning Theory},
  year={2022}
}



@article{paper:optimistic_posterior_sampling,
  title={Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}




@article{paper:regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
  author={Bartlett, Peter L and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1205.2661},
  year={2012}
}




@inproceedings{paper:scal,
  title={Efficient bias-span-constrained exploration-exploitation in reinforcement learning},
  author={Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
  booktitle={International Conference on Machine Learning},
  pages={1578--1586},
  year={2018},
  organization={PMLR}
}




@article{paper:distribution_norm,
  title={How hard is my MDP?" The distribution-norm to the rescue"},
  author={Maillard, Odalric-Ambrym and Mann, Timothy A and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}




@inproceedings{paper:variance_aware_kl_ucrl,
  title={Variance-aware regret bounds for undiscounted reinforcement learning in mdps},
  author={Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
  booktitle={Algorithmic Learning Theory},
  pages={770--805},
  year={2018},
  organization={PMLR}
}




@inproceedings{paper:reward_free_exploration,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}




@inproceedings{paper:first_order_lfa,
  title={First-order regret in reinforcement learning with linear function approximation: A robust estimation approach},
  author={Wagenmaker, Andrew J and Chen, Yifang and Simchowitz, Max and Du, Simon and Jamieson, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={22384--22429},
  year={2022},
  organization={PMLR}
}




@article{paper:non_asymptotic_gap,
  title={Non-asymptotic gap-dependent regret bounds for tabular mdps},
  author={Simchowitz, Max and Jamieson, Kevin G},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}




@article{zhao2023variance,
  title={Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement Learning: Adaptivity and Computational Efficiency},
  author={Zhao, Heyang and He, Jiafan and Zhou, Dongruo and Zhang, Tong and Gu, Quanquan},
  journal={arXiv preprint arXiv:2302.10371},
  year={2023}
}




@article{li2023variance,
  title={Variance-aware robust reinforcement learning with linear function approximation with heavy-tailed rewards},
  author={Li, Xiang and Sun, Qiang},
  journal={arXiv preprint arXiv:2303.05606},
  year={2023}
}




@article{dai2022variance,
  title={Variance-Aware Sparse Linear Bandits},
  author={Dai, Yan and Wang, Ruosong and Du, Simon S},
  journal={arXiv preprint arXiv:2205.13450},
  year={2022}
}

@article{zhao2022bandit,
  title={Bandit learning with general function classes: Heteroscedastic noise and variance-dependent regret bounds},
  author={Zhao, Heyang and Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2202.13603},
  year={2022}
}