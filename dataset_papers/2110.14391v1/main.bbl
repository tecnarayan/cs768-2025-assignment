\begin{thebibliography}{10}

\bibitem{absil2009optimization}
P-A Absil, Robert Mahony, and Rodolphe Sepulchre.
\newblock {\em Optimization algorithms on matrix manifolds}.
\newblock Princeton University Press, 2009.

\bibitem{alimisis2021communication}
Foivos Alimisis, Peter Davies, and Dan Alistarh.
\newblock Communication-efficient distributed optimization with quantized
  preconditioners.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2021.

\bibitem{alistarh2020improved}
Dan Alistarh and Janne~H. Korhonen.
\newblock Improved communication lower bounds for distributed optimisation.
\newblock {\em arXiv preprint arXiv:2010.08222}, 2020.

\bibitem{balcan2014improved}
Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, and David Woodruff.
\newblock Improved distributed principal component analysis.
\newblock {\em arXiv preprint arXiv:1408.5823}, 2014.

\bibitem{bishop2006pattern}
Christopher~M Bishop.
\newblock {\em Pattern recognition and machine learning}.
\newblock Springer, 2006.

\bibitem{boutsidis2016optimal}
Christos Boutsidis, David~P Woodruff, and Peilin Zhong.
\newblock Optimal principal component analysis in distributed and streaming
  models.
\newblock In {\em Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pages 236--249, 2016.

\bibitem{bu2020note}
Jingjing Bu and Mehran Mesbahi.
\newblock A note on nesterov's accelerated method in nonconvex optimization: a
  weak estimate sequence approach, 2020.

\bibitem{davies2020distributed}
Peter Davies, Vijaykrishna Gurunathan, Niusha Moshrefi, Saleh Ashkboos, and Dan
  Alistarh.
\newblock Distributed variance reduction with optimal communication.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem{fan2019distributed}
Jianqing Fan, Dong Wang, Kaizheng Wang, and Ziwei Zhu.
\newblock Distributed estimation of principal eigenspaces.
\newblock {\em Annals of statistics}, 47(6):3009, 2019.

\bibitem{garber2017communication}
Dan Garber, Ohad Shamir, and Nathan Srebro.
\newblock Communication-efficient algorithms for distributed stochastic
  principal component analysis.
\newblock In {\em International Conference on Machine Learning}, pages
  1203--1212. PMLR, 2017.

\bibitem{horvath2019stochastic}
Samuel Horv{\'a}th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and
  Peter Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock {\em arXiv preprint arXiv:1904.05115}, 2019.

\bibitem{hotelling1933analysis}
Harold Hotelling.
\newblock Analysis of a complex of statistical variables into principal
  components.
\newblock {\em Journal of educational psychology}, 24(6):417, 1933.

\bibitem{huang2020communication}
Long-Kai Huang and Sinno Pan.
\newblock Communication-efficient distributed {PCA} by {R}iemannian
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  4465--4474. PMLR, 2020.

\bibitem{islamov2021distributed}
Rustem Islamov, Xun Qian, and Peter Richt{\'a}rik.
\newblock Distributed second order methods with fast rates and compressed
  communication.
\newblock {\em arXiv preprint arXiv:2102.07158}, 2021.

\bibitem{jaggi2014communication}
Martin Jaggi, Virginia Smith, Martin Tak{\'a}{\v{c}}, Jonathan Terhorst, Sanjay
  Krishnan, Thomas Hofmann, and Michael~I Jordan.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock {\em arXiv preprint arXiv:1409.1458}, 2014.

\bibitem{kannan2014principal}
Ravi Kannan, Santosh Vempala, and David Woodruff.
\newblock Principal component analysis and higher correlations for distributed
  data.
\newblock In {\em Conference on Learning Theory}, pages 1040--1057. PMLR, 2014.

\bibitem{karimi2020linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-\l{}ojasiewicz condition, 2020.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{konevcny2018randomized}
Jakub Kone{\v{c}}n{\`y} and Peter Richt{\'a}rik.
\newblock Randomized distributed mean estimation: Accuracy vs. communication.
\newblock {\em Frontiers in Applied Mathematics and Statistics}, 4:62, 2018.

\bibitem{liConciseFormulasArea2011}
S.~Li.
\newblock Concise {{Formulas}} for the {{Area}} and {{Volume}} of a
  {{Hyperspherical Cap}}.
\newblock {\em Asian Journal of Mathematics \& Statistics}, 4(1):66--70, 2011.

\bibitem{magnusson2020maintaining}
Sindri Magn{\'u}sson, Hossein Shokri-Ghadikolaei, and Na~Li.
\newblock On maintaining linear convergence of distributed learning and
  optimization under limited communication.
\newblock {\em IEEE Transactions on Signal Processing}, 68:6101--6116, 2020.

\bibitem{ng2001spectral}
Andrew Ng, Michael Jordan, and Yair Weiss.
\newblock On spectral clustering: Analysis and an algorithm.
\newblock {\em Advances in neural information processing systems}, 14:849--856,
  2001.

\bibitem{oja1985stochastic}
Erkki Oja and Juha Karhunen.
\newblock On stochastic approximation of the eigenvectors and eigenvalues of
  the expectation of a random matrix.
\newblock {\em Journal of mathematical analysis and applications},
  106(1):69--84, 1985.

\bibitem{shamir2015stochastic}
Ohad Shamir.
\newblock A stochastic pca and svd algorithm with an exponential convergence
  rate.
\newblock In {\em International Conference on Machine Learning}, pages
  144--152. PMLR, 2015.

\bibitem{shamir2016fast}
Ohad Shamir.
\newblock Fast stochastic algorithms for svd and pca: Convergence properties
  and convexity.
\newblock In {\em International Conference on Machine Learning}, pages
  248--256. PMLR, 2016.

\bibitem{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In {\em International conference on machine learning}, pages
  1000--1008. PMLR, 2014.

\bibitem{suresh2017distributed}
Ananda~Theertha Suresh, X~Yu Felix, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In {\em International Conference on Machine Learning}, pages
  3329--3337. PMLR, 2017.

\bibitem{vandereycken2013low}
Bart Vandereycken.
\newblock Low-rank matrix completion by riemannian optimization.
\newblock {\em SIAM Journal on Optimization}, 23(2):1214--1236, 2013.

\bibitem{xu2018convergence}
Zhiqiang Xu, Xin Cao, and Xin Gao.
\newblock Convergence analysis of gradient descent for eigenvector computation.
\newblock International Joint Conferences on Artificial Intelligence, 2018.

\bibitem{zhang2016riemannian}
Hongyi Zhang, Sashank~J Reddi, and Suvrit Sra.
\newblock Riemannian svrg: Fast stochastic optimization on riemannian
  manifolds.
\newblock {\em arXiv preprint arXiv:1605.07147}, 2016.

\bibitem{zhang2016first}
Hongyi Zhang and Suvrit Sra.
\newblock First-order methods for geodesically convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 1617--1638. PMLR, 2016.

\end{thebibliography}
