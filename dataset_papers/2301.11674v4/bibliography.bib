@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@article{Stute1993,
abstract = {Let ℱ={F$\theta$} be a parametric family of distribution functions, and denote with Fn the empirical d.f. of an i.i.d. sample. Goodness-of-fit tests of a composite hypothesis (contained in ℱ) are usually based on the so-called estimated empirical process. Typically, they are not distribution-free. In such a situation the bootstrap offers a useful alternative. It is the purpose of this paper to show that this approximation holds with probability one. A simulation study is included which demonstrates the validity of the bootstrap for several selected parametric families. {\textcopyright} 1993, Physica-Verlag GmbH. All rights reserved.},
author = {Stute, W. and Manteiga, W. G. and Quindimil, M. P.},
doi = {10.1007/BF02613687},
file = {:Users/fxbriol/Docs/Work/Papers/Other Statistics/Hypothesis testing/(1993, Stute) Bootstrap based goodness-of-fit-tests.pdf:pdf},
journal = {Metrika},
number = {1},
pages = {243--256},
title = {{Bootstrap based goodness-of-fit-tests}},
volume = {40},
year = {1993}
}


@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@Article{Ryan2015,
  author    = {Elizabeth G. Ryan and Christopher C. Drovandi and James M. McGree and Anthony N. Pettitt},
  journal   = {International Statistical Review},
  title     = {A Review of Modern Computational Algorithms for {B}ayesian Optimal Design},
  year      = {2015},
  month     = {jun},
  number    = {1},
  pages     = {128--154},
  volume    = {84},
  publisher = {Wiley},
}


@InProceedings{Kleinegesse2019,
  title = 	 {Efficient {B}ayesian Experimental Design for Implicit Models},
  author =       {Kleinegesse, Steven and Gutmann, Michael U.},
  booktitle = 	 {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  pages = 	 {476--485},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  OPTmonth = 	 {16--18 Apr},
  OPTpublisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/kleinegesse19a/kleinegesse19a.pdf},
  abstract = 	 {Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search or sampling-based methods. We find that this increases efficiency and allows us to consider higher design dimensions.}
}

@Article{Thomas2021,
  author    = {Owen Thomas and Ritabrata Dutta and Jukka Corander and Samuel Kaski and Michael U. Gutmann},
  journal   = {Bayesian Analysis},
  title     = {Likelihood-Free Inference by Ratio Estimation},
  year      = {2021},
  month     = {jan},
  number    = {-1},
  volume    = {-1},
  publisher = {Institute of Mathematical Statistics},
}

@article{Gretton2012JMLR,
abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
author = {Gretton, A and Borgwardt, K and Rasch, M J and Scholkopf, B},
journal = {Journal of Machine Learning Research},
pages = {723--773},
title = {{A kernel two-sample test}},
volume = {13},
year = {2012}
}

@inproceedings{Gretton2006,
 author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte and Sch\"{o}lkopf, Bernhard and Smola, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 OPTpublisher = {MIT Press},
 title = {A Kernel Method for the Two-Sample-Problem},
 volume = {19},
 year = {2006}
}

@article{Jiang2017,
 abstract = {Approximate Bayesian Computation (ABC) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic, but outside of special cases where the optimal summary statistics are known, it is unclear which guiding principles can be used to construct effective summary statistics. In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data: the resulting summary statistics are approximately posterior means of the parameters. With minimal model-specific tuning, our method constructs summary statistics for the Ising model and the moving-average model, which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors.},
 author = {Bai Jiang and Tung-Yu Wu and Charles Zheng and Wing H. Wong},
 journal = {Statistica Sinica},
 number = {4},
 pages = {1595--1618},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {LEARNING SUMMARY STATISTIC FOR APPROXIMATE {B}AYESIAN COMPUTATION VIA DEEP NEURAL NETWORK},
 volume = {27},
 year = {2017}
}

@article{Bi2021,
author = {Jiefeng Bi and Weining Shen and Weixuan Zhu},
title = {Random Forest Adjustment for Approximate {B}ayesian Computation},
journal = {Journal of Computational and Graphical Statistics},
volume = {0},
number = {0},
pages = {1-10},
year  = {2021},
publisher = {Taylor & Francis}
}

@Article{Drovandi2011,
  author    = {Christopher C. Drovandi and Anthony N. Pettitt},
  journal   = {Computational Statistics {\&} Data Analysis},
  title     = {Likelihood-free {B}ayesian estimation of multivariate quantile distributions},
  year      = {2011},
  number    = {9},
  pages     = {2541--2556},
  volume    = {55}
}

@InProceedings{Blum2010,
  author    = {Michael G.B. Blum},
  booktitle = {Proceedings of {COMPSTAT}},
  title     = {Choosing the Summary Statistics and the Acceptance Rate in Approximate {B}ayesian Computation},
  year      = {2010},
  pages     = {47--56},
}

@Article{Barnes2012,
  author    = {Chris P. Barnes and Sarah Filippi and Michael P. H. Stumpf and Thomas Thorne},
  journal   = {Statistics and Computing},
  title     = {Considerate approaches to constructing summary statistics for {ABC} model selection},
  year      = {2012},
  number    = {6},
  pages     = {1181--1197},
  volume    = {22},
}

@Article{Blum2013,
  author    = {M. G. B. Blum and M. A. Nunes and D. Prangle and S. A. Sisson},
  journal   = {Statistical Science},
  title     = {A Comparative Review of Dimension Reduction Methods in Approximate {B}ayesian Computation},
  year      = {2013},
  number    = {2},
  volume    = {28},
}

@Article{Joyce2008,
  author    = {Paul Joyce and Paul Marjoram},
  journal   = {Statistical Applications in Genetics and Molecular Biology},
  title     = {Approximately Sufficient Statistics and {B}ayesian Computation},
  year      = {2008},
  number    = {1},
  volume    = {7},
  publisher = {Walter de Gruyter {GmbH}},
}

@Article{Nunes2010,
  author    = {Matthew A Nunes and David J Balding},
  journal   = {Statistical Applications in Genetics and Molecular Biology},
  title     = {On Optimal Selection of Summary Statistics for Approximate {B}ayesian Computation},
  year      = {2010},
  number    = {1},
  volume    = {9},
}

@Book{Sisson2018,
  author    = {Scott A. Sisson},
  publisher = {Chapman and Hall/{CRC}},
  title     = {Handbook of Approximate {B}ayesian Computation},
  year      = {2018},
}


@Article{Beaumont2002,
  author    = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
  title     = {Approximate {B}ayesian Computation in Population Genetics},
  journal   = {Genetics},
  year      = {2002},
  volume    = {162},
  number    = {4},
  pages     = {2025--2035},
}

@Article{Beaumont2009,
  author    = {M. A. Beaumont and J.-M. Cornuet and J.-M. Marin and C. P. Robert},
  title     = {Adaptive approximate {B}ayesian computation},
  journal   = {Biometrika},
  year      = {2009},
  volume    = {96},
  number    = {4},
  pages     = {983--990},
}

@Article{Blum2010A,
  author    = {Michael G. B. Blum},
  title     = {Approximate {B}ayesian Computation: A Nonparametric Perspective},
  journal   = {J. of the Amer. Stat. Assoc.},
  year      = {2010},
  volume    = {105},
  number    = {491},
  pages     = {1178--1187},
  month     = {sep},
  publisher = {Informa {UK} Limited},
}

@Article{Pritchard1999,
  author    = {J. K. Pritchard and M. T. Seielstad and A. Perez-Lezaun and M. W. Feldman},
  title     = {{Population growth of human Y chromosomes: a study of Y chromosome microsatellites}},
  journal   = {Molecular Biology and Evolution},
  year      = {1999},
  volume    = {16},
  number    = {12},
  pages     = {1791--1798},
}

@Article{Beaumont2010,
  author    = {Mark A. Beaumont},
  title     = {Approximate {B}ayesian Computation in Evolution and Ecology},
  journal   = {Annual Review of Ecology, Evolution, and Systematics},
  year      = {2010},
  volume    = {41},
  number    = {1},
  pages     = {379--406},
  publisher = {Annual Reviews},
}

@Article{Lintusaari2016,
  author    = {Jarno Lintusaari and Michael U. Gutmann and Ritabrata Dutta and Samuel Kaski and Jukka Corander},
  journal   = {Systematic Biology},
  title     = {Fundamentals and Recent Developments in Approximate {B}ayesian Computation},
  year      = {2017},
    volume={66},
  pages     = {66--82}
}

@Article{Beaumont2019,
  author   = {Beaumont, Mark A.},
  title    = {Approximate {B}ayesian Computation},
  journal  = {Annual Review of Statistics and Its Application},
  year     = {2019},
  volume   = {6},
  number   = {1},
  pages    = {379-403},
  abstract = { Many of the statistical models that could provide an accurate, interesting, and testable explanation for the structure of a data set turn out to have intractable likelihood functions. The method of approximate Bayesian computation (ABC) has become a popular approach for tackling such models. This review gives an overview of the method and the main issues and challenges that are the subject of current research. }
}

@article{Akeret_2015,
	year = 2015,
	volume = {2015},
	number = {08},
	pages = {043--043},
	author = {Joël Akeret and Alexandre Refregier and Adam Amara and Sebastian Seehars and Caspar Hasner},
	title = {Approximate {B}ayesian computation for forward modeling in cosmology},
	journal = {Journal of Cosmology and Astroparticle Physics},
}

@ARTICLE{Bharti2020,
  author={A. {Bharti} and R. {Adeogun} and T. {Pedersen}},
  journal={IEEE Open Journal of Antennas and Propagation}, 
  title={Learning Parameters of Stochastic Radio Channel Models From Summaries}, 
  year={2020},
  volume={1},
  number={},
  pages={175-188}}


@article{Riabiz2020,
abstract = {The use of heuristics to assess the convergence and compress the output of Markov chain Monte Carlo can be sub-optimal in terms of the empirical approximations that are produced. Typically a number of the initial states are attributed to "burn in" and removed, whilst the chain can be "thinned" if compression is also required. In this paper we consider the problem of selecting a subset of states, of fixed cardinality, such that the approximation provided by their empirical distribution is close to optimal. A novel method is proposed, based on greedy minimisation of a kernel Stein discrepancy, that is suitable for problems where heavy compression is required. Theoretical results guarantee consistency of the method and its effectiveness is demonstrated in the challenging context of parameter inference for ordinary differential equations. Software is available in the "Stein Thinning" package in both Python and MATLAB, and example code is included.},
author = {Riabiz, M. and Chen, W. and Cockayne, J. and Swietach, P. and Niederer, S. A. and Mackey, L. and Oates, C. J.},
file = {:Users/fxbriol/Docs/Work/Papers/Stein method/Stein & Kernels/(2020, Riabiz) Optimal thinning of MCMC output.pdf:pdf},
journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
number = {4},
pages = {1059--1081},
title = {{Optimal thinning of MCMC output}},
volume = {84},
year = {2022}
}



@article{Mak2018,
author = {Mak, S. and Joseph, V. Roshan},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Mak, Joseph - 2018 - Support points.pdf:pdf},
journal = {Annals of Statistics},
number = {6A},
pages = {2562--2592},
title = {{Support points}},
volume = {46},
year = {2018}
}


@inproceedings{Bach2012,
abstract = {We show that the herding procedure of Welling (2009) takes exactly the form of a standard convex optimization algorithm--namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. The experiments indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm tends to approach more often the maximum entropy distribution, shedding more light on the learning bias behind herding.},
author = {Bach, F. and Lacoste-Julien, S. and Obozinski, G.},
booktitle = {Proceedings of the International Conference on Machine Learning},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Bach, Lacoste-Julien, Obozinski - 2012 - On the equivalence between herding and conditional gradient algorithms.pdf:pdf},
pages = {1355--1362},
title = {{On the equivalence between herding and conditional gradient algorithms}},
year = {2012}
}

@article{Dick2013,
abstract = {This paper is a contemporary review of QMC (‘quasi-Monte Carlo') methods, that is, equal-weight rules for the approximate evaluation of high-dimensional integrals over the unit cube [0, 1]s,where s may be large, or even infinite. Af- ter a general introduction, the paper surveys recent developments in lattice methods, digital nets, and related themes. Among those recent developments are methods of construction of both lattices and digital nets, to yield QMC rules that have a prescribed rate of convergence for sufficiently smooth func- tions, and ideally also guaranteed slow growth (or no growth) of the worst-case error as s increases. A crucial role is played by parameters called ‘weights', since a careful use of the weight parameters is needed to ensure that the worst-case errors in an appropriately weighted function space are bounded, or grow only slowly, as the dimension s increases. Important tools for the analysis are weighted function spaces, reproducing kernel Hilbert spaces, and discrepancy, all of which are discussed with an appropriate level of detail.},
author = {Dick, J. and Kuo, F. Y. and Sloan, I. H.},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Dick, Kuo, Sloan - 2013 - High-dimensional integration The quasi-Monte Carlo way.pdf:pdf},
journal = {Acta Numerica},
number = {April 2013},
pages = {133--288},
title = {{High-dimensional integration: The quasi-Monte Carlo way}},
volume = {22},
year = {2013}
}


@book{Dick2010,
author = {Dick, J. and Pillichshammer, F.},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Dick, Pillichshammer - 2010 - Digital Nets and Sequences - Discrepancy Theory and Quasi-Monte Carlo Integration.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Digital Nets and Sequences - Discrepancy Theory and Quasi-Monte Carlo Integration}},
year = {2010}
}


@inproceedings{Teymur2020,
abstract = {Several researchers have proposed minimisation of maximum mean discrepancy (MMD) as a method to quantise probability measures, i.e., to approximate a target distribution by a representative point set. Here we consider sequential algorithms that greedily minimise MMD over a discrete candidate set. We propose a novel non-myopic algorithm and, in order to both improve statistical efficiency and reduce computational cost, we investigate a variant that applies this technique to a mini-batch of the candidate set at each iteration. When the candidate points are sampled from the target, the consistency of these new algorithm - and their mini-batch variants - is established. We demonstrate the algorithms on a range of important computational problems, including optimisation of nodes in Bayesian cubature and the thinning of Markov chain output.},
archivePrefix = {arXiv},
arxivId = {2010.07064},
author = {Teymur, Onur and Gorham, Jackson and Riabiz, Marina and Oates, Chris. J.},
booktitle = {Artificial Intelligence and Statistics},
eprint = {2010.07064},
file = {:Users/fxbriol/Docs/Work/Papers/Kernel Methods/Kernel Embeddings/(2020, Teymur) Optimal quantisation of probability measures using maximum mean discrepancy.pdf:pdf},
pages = {1027--1035},
title = {{Optimal quantisation of probability measures using maximum mean discrepancy}},
url = {http://arxiv.org/abs/2010.07064},
year = {2021}
}


@article{Bardenet2020,
abstract = {We show that repulsive random variables can yield Monte Carlo methods with faster convergence rates than the typical $N^{-1/2}$, where $N$ is the number of integrand evaluations. More precisely, we propose stochastic numerical quadratures involving determinantal point processes associated with multivariate orthogonal polynomials, and we obtain root mean square errors that decrease as $N^{-(1+1/d)/2}$, where $d$ is the dimension of the ambient space. First, we prove a central limit theorem (CLT) for the linear statistics of a class of determinantal point processes, when the reference measure is a product measure supported on a hypercube, which satisfies the Nevai-class regularity condition, a result which may be of independent interest. Next, we introduce a Monte Carlo method based on these determinantal point processes, and prove a CLT with explicit limiting variance for the quadrature error, when the reference measure satisfies a stronger regularity condition. As a corollary, by taking a specific reference measure and using a construction similar to importance sampling, we obtain a general Monte Carlo method, which applies to any measure with continuously derivable density. Loosely speaking, our method can be interpreted as a stochastic counterpart to Gaussian quadrature, which, at the price of some convergence rate, is easily generalizable to any dimension and has a more explicit error term.},
author = {Bardenet, R{\'{e}}mi and Hardy, Adrien},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Bardenet, Hardy - 2016 - Monte Carlo with Determinantal Point Processes.pdf:pdf},
journal = {The Annals of Applied Probability},
number = {1},
pages = {368--417},
title = {{Monte Carlo with Determinantal Point Processes}},
volume = {30},
year = {2020}
}


@inproceedings{Dwivedi2021,
abstract = {We introduce kernel thinning, a simple algorithm for generating better-than-Monte-Carlo approximations to distributions $\mathbb{P}$ on $\mathbb{R}^d$. Given $n$ input points, a suitable reproducing kernel $\mathbf{k}$, and $\mathcal{O}(n^2)$ time, kernel thinning returns $\sqrt{n}$ points with comparable integration error for every function in the associated reproducing kernel Hilbert space. With high probability, the maximum discrepancy in integration error is $\mathcal{O}_d(n^{-\frac{1}{2}}\sqrt{\log n})$ for compactly supported $\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} \sqrt{(\log n)^{d+1}\log\log n})$ for sub-exponential $\mathbb{P}$. In contrast, an equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-\frac14})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\mathbb{R}^d$ and a wide range of common kernels. We use our results to derive explicit non-asymptotic maximum mean discrepancy bounds for Gaussian, Mat\'ern, and B-spline kernels and present two vignettes illustrating the practical benefits of kernel thinning over i.i.d. sampling and standard Markov chain Monte Carlo thinning.},
author = {Dwivedi, Raaz and Mackey, Lester},
booktitle = {Conference on Learning Theory},
file = {:Users/fxbriol/Docs/Work/Papers/Kernel Methods/Kernel Embeddings/(2021, Dwivedi) Kernel Thinning.pdf:pdf},
keywords = {coresets,hilbert space,markov chain monte carlo,maximum mean discrepancy,reproducing kernel,thinning},
title = {{Kernel Thinning}},
volume = {134},
year = {2021}
}

@inproceedings{Chen2019,
author = {Chen, W. Y. and Barp, A. and Briol, F-X. and Gorham, J. and Girolami, M. and Mackey, L. and Oates, C. J.},
booktitle = {Proceedings of the International Conference on Machine Learning},
file = {:Users/fxbriol/Docs/Work/Papers/Stein method/Stein & Kernels/(2019, Chen) Stein Point Markov Chain Monte Carlo.pdf:pdf},
pages = {1011--1021},
title = {{Stein point Markov chain Monte Carlo}},
year = {2019}
}



@inproceedings{Chen2018,
abstract = {An important task in computational statistics and machine learning is to approximate a posterior distribution $p(x)$ with an empirical measure supported on a set of representative points $\{x_i\}_{i=1}^n$. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when $n$ is small. To this end, we present `Stein Points'. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and $p(x)$. Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method.},
author = {Chen, W. Y. and Mackey, L. and Gorham, J. and Briol, F-X. and Oates, C. J.},
booktitle = {Proceedings of the International Conference on Machine Learning},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2018 - Stein points(2).pdf:pdf},
pages = {843--852},
title = {{Stein points}},
year = {2018}
}
  
  
@inproceedings{Chen2010,
abstract = {We extend the herding algorithm to continuous spaces by using the kernel\ntrick. The resulting "kernel herding" algorithm is an infinite memory\ndeterministic process that learns to approximate a PDF with a collection of\nsamples. We show that kernel herding decreases the error of expectations of\nfunctions in the Hilbert space at a rate O(1/T) which is much faster than the\nusual O(1/pT) for iid random samples. We illustrate kernel herding by\napproximating Bayesian predictive distributions.},
author = {Chen, Y. and Welling, M. and Smola, A.},
booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Welling, Smola - 2010 - Super-samples from kernel herding.pdf:pdf},
pages = {109--116},
title = {{Super-samples from kernel herding}},
year = {2010}
}

  
@article{Diaconis1988,
author = {Diaconis, P},
journal = {Statistical Decision Theory and Related Topics IV},
pages = {163--175},
title = {{Bayesian Numerical Analysis}},
year = {1988}
}


@article{OHagan1991,
author = {O'Hagan, A},
journal = {Journal of Statistical Planning and Inference},
keywords = {bayesian},
pages = {245--260},
title = {{Bayes-Hermite quadrature}},
volume = {29},
year = {1991}
}


@inproceedings{Rasmussen2003,
author = {Rasmussen, C and Ghahramani, Z},
booktitle = {Advances in Neural Information Processing Systems},
pages = {489--496},
title = {{Bayesian Monte Carlo}},
year = {2002}
}

  
  @article{Briol2019PI,
abstract = {A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
author = {Briol, F-X. and Oates, C. J. and Girolami, M. and Osborne, M. A. and Sejdinovic, D.},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Briol et al. - 2019 - Probabilistic integration A role in statistical computation (with discussion).pdf:pdf},
journal = {Statistical Science},
number = {1},
pages = {1--22},
title = {{Probabilistic integration: A role in statistical computation? (with discussion)}},
volume = {34},
year = {2019}
}
@article{Wynne2020,
abstract = {Gaussian processes are ubiquitous in machine learning, statistics, and applied mathematics. They provide a flexible modelling framework for approximating functions, whilst simultaneously quantifying uncertainty. However, this is only true when the model is well-specified, which is often not the case in practice. In this paper, we study the properties of Gaussian process means when the smoothness of the model and the likelihood function are misspecified. In this setting, an important theoretical question of practial relevance is how accurate the Gaussian process approximations will be given the difficulty of the problem, our model and the extent of the misspecification. The answer to this problem is particularly useful since it can inform our choice of model and experimental design. In particular, we describe how the experimental design and choice of kernel and kernel hyperparameters can be adapted to alleviate model misspecification.},
author = {Wynne, G. and Briol, F.-X. and Girolami, M.},
file = {:Users/fxbriol/Docs/Work/Papers/Kernel Methods/Gaussian Processes/(2020, Wynne) Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {123},
pages = {1--40},
title = {{Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness}},
volume = {22},
year = {2021}
}

@article{Niu2021,
abstract = {Intractable generative models are models for which the likelihood is unavailable but sampling is possible. Most approaches to parameter inference in this setting require the computation of some discrepancy between the data and the generative model. This is for example the case for minimum distance estimation and approximate Bayesian computation. These approaches require sampling a high number of realisations from the model for different parameter values, which can be a significant challenge when simulating is an expensive operation. In this paper, we propose to enhance this approach by enforcing "sample diversity" in simulations of our models. This will be implemented through the use of quasi-Monte Carlo (QMC) point sets. Our key results are sample complexity bounds which demonstrate that, under smoothness conditions on the generator, QMC can significantly reduce the number of samples required to obtain a given level of accuracy when using three of the most common discrepancies: the maximum mean discrepancy, the Wasserstein distance, and the Sinkhorn divergence. This is complemented by a simulation study which highlights that an improved accuracy is sometimes also possible in some settings which are not covered by the theory.},
author = {Niu, Z. and Meier, J. and Briol, F-X},
file = {:Users/fxbriol/Docs/Work/Papers/Other Statistics/Generative models/(2021, Niu) Discrepancy based inference for intractable generative models with QMC.pdf:pdf},
journal = {Electronic Journal of Statistics},
number = {1},
pages = {1411--1456},
title = {{Discrepancy-based inference for intractable generative models using quasi-Monte Carlo}},
volume = {17},
year = {2023}
}




@article{Karvonen2020,
abstract = {The Gaussian kernel plays a central role in machine learning, uncertainty quantification and scattered data approximation, but has received relatively little attention from a numerical analysis standpoint. The basic problem of finding an algorithm for efficient numerical integration of functions reproduced by Gaussian kernels has not been fully solved. In this article we construct two classes of algorithms that use $N$ evaluations to integrate $d$-variate functions reproduced by Gaussian kernels and prove the exponential or super-algebraic decay of their worst-case errors. In contrast to earlier work, no constraints are placed on the length-scale parameter of the Gaussian kernel. The first class of algorithms is obtained via an appropriate scaling of the classical Gauss-Hermite rules. For these algorithms we derive lower and upper bounds on the worst-case error of the forms $\exp(-c_1 N^{1/d}) N^{1/(4d)}$ and $\exp(-c_2 N^{1/d}) N^{-1/(4d)}$, respectively, for positive constants $c_1 > c_2$. The second class of algorithms we construct is more flexible and uses worst-case optimal weights for points that may be taken as a nested sequence. For these algorithms we only derive upper bounds, which are of the form $\exp(-c_3 N^{1/(2d)})$ for a positive constant $c_3$.},
author = {Karvonen, T. and Oates, C. J. and Girolami, M.},
file = {:Users/fxbriol/Docs/Work/Papers/Probabilistic Numerics/(2020, Karvonen) Integration in reproducing kernel Hilbert spaces of Gaussian kernels.pdf:pdf},
journal = {Mathematics of Computation},
number = {331},
pages = {2209--2233},
title = {{Integration in reproducing kernel Hilbert spaces of Gaussian kernels}},
volume = {90},
year = {2020}
}


@article{Kanagawa2017,
author = {Kanagawa, M. and Sriperumbudur, B. K. and Fukumizu, K.},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Kanagawa, Sriperumbudur, Fukumizu - 2020 - Convergence analysis of deterministic kernel-based quadrature rules in misspecified settings.pdf:pdf},
journal = {Foundations of Computational Mathematics},
keywords = {and phrases,bayesian quadrature,kernel-based quadrature rules,misspecified settings,reproducing kernel hilbert spaces,sobolev spaces},
pages = {155--194},
title = {{Convergence analysis of deterministic kernel-based quadrature rules in misspecified settings}},
volume = {20},
year = {2020}
}

  
@Article{Bharti2022,
  author    = {Ayush Bharti and Francois-Xavier Briol and Troels Pedersen},
  journal   = {{IEEE} Transactions on Antennas and Propagation},
  title     = {A General Method for Calibrating Stochastic Radio Channel Models With Kernels},
  year      = {2022},
  month     = {},
  number    = {6},
  pages     = {3986--4001},
  volume    = {70},
  doi       = {10.1109/tap.2021.3083761},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

  
 @Article{Fearnhead2012,
  author    = {Paul Fearnhead and Dennis Prangle},
  title     = {Constructing summary statistics for approximate {B}ayesian computation: semi-automatic approximate Bayesian computation},
  year      = {2012},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  number    = {3},
  pages     = {419--474},
  volume    = {74},
}

@article{prangle2015summary,
      title={Summary Statistics in Approximate Bayesian Computation}, 
      author={Dennis Prangle},
      year={2015},
      journal = {arXiv:1512.05633},
      eprint={1512.05633},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@Article{Wegmann2009,
  author    = {Daniel Wegmann and Christoph Leuenberger and Laurent Excoffier},
  title     = {Efficient Approximate {B}ayesian Computation Coupled With Markov Chain Monte Carlo Without Likelihood},
  year      = {2009},
  number    = {4},
  pages     = {1207--1218},
  volume    = {182},
  journal = {Genetics},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Aeschbacher2012,
  author    = {Simon Aeschbacher and Mark A Beaumont and Andreas Futschik},
  title     = {A Novel Approach for Choosing Summary Statistics in Approximate {B}ayesian Computation},
  journal   = {Genetics},
  year      = {2012},
  number    = {3},
  pages     = {1027--1047},
  volume    = {192},
}

@Article{Li2018,
  author    = {Wentao Li and Paul Fearnhead},
  title     = {Convergence of regression-adjusted approximate {B}ayesian computation},
  year      = {2018},
  journal = {Biometrika},
  number    = {2},
  pages     = {301--318},
  volume    = {105},
}

@Article{Marin2011,
  author    = {Jean-Michel Marin and Pierre Pudlo and Christian P. Robert and Robin J. Ryder},
  title     = {Approximate {B}ayesian computational methods},
  journal   = {Statistics and Computing},
  year      = {2011},
  number    = {6},
  pages     = {1167--1180},
  volume    = {22},
}

@Article{Daee2017,
  author    = {Pedram Daee and Tomi Peltola and Marta Soare and Samuel Kaski},
  title     = {Knowledge elicitation via sequential probabilistic inference for high-dimensional prediction},
  journal   = {Machine Learning},
  year      = {2017},
  number    = {9-10},
  pages     = {1599--1620},
  volume    = {106},
}

@inproceedings{Peltola2019,
 author = {Peltola, Tomi and \c{C}elikok, Mustafa Mert and Daee, Pedram and Kaski, Samuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Machine Teaching of Active Sequential Learners},
 volume = {32},
 year = {2019}
}

@article{chen2021neural,
      title={{Neural Approximate Sufficient Statistics for Implicit Models}}, 
      author={Yanzhi Chen and Dinghuai Zhang and Michael Gutmann and Aaron Courville and Zhanxing Zhu},
      year={2020},
      journal = {arXiv:2010.10079},
      eprint={2010.10079},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Diggle1984,
 ISSN = {00359246},
 abstract = {A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more fundamental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a log-likelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated log-likelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to ad hoc methods of parameter estimation are noted briefly, and illustrated by an example involving a model for the spatial pattern of displaced amacrine cells in the retina of a rabbit.},
 author = {Peter J. Diggle and Richard J. Gratton},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {193--227},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Monte Carlo Methods of Inference for Implicit Statistical Models},
 volume = {46},
 year = {1984}
}

@Article{Blum2017,
  author        = {Michael GB Blum},
  title         = {Regression approaches for Approximate {B}ayesian Computation},
  year          = {2017},
  month         = jul,
  abstract      = {This book chapter introduces regression approaches and regression adjustment for Approximate Bayesian Computation (ABC). Regression adjustment adjusts parameter values after rejection sampling in order to account for the imperfect match between simulations and observations. Imperfect match between simulations and observations can be more pronounced when there are many summary statistics, a phenomenon coined as the curse of dimensionality. Because of this imperfect match, credibility intervals obtained with regression approaches can be inflated compared to true credibility intervals. The chapter presents the main concepts underlying regression adjustment. A theorem that compares theoretical properties of posterior distributions obtained with and without regression adjustment is presented. Last, a practical application of regression adjustment in population genetics shows that regression adjustment shrinks posterior distributions compared to rejection approaches, which is a solution to avoid inflated credibility intervals.},
  journal = {arXiv:1707.01254},
  eprint        = {1707.01254},
  file          = {:http\://arxiv.org/pdf/1707.01254v1:PDF},
  keywords      = {stat.ME},
  primaryclass  = {stat.ME},
}

@Article{Blum2010nonlinear,
  author    = {Michael G. B. Blum and Olivier Fran{\c{c}}ois},
  title     = {Non-linear regression models for Approximate {B}ayesian Computation},
  journal = {Statistics and Computing},
  year      = {2010},
  number    = {1},
  pages     = {63--73},
  volume    = {20},
}

@Book{Hastie2009,
  author    = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
  publisher = {Springer New York},
  title     = {The Elements of Statistical Learning},
  year      = {2009}
}

@article{chaloner1995bayesian,
  title={{Bayesian experimental design: A review}},
  author={Chaloner, Kathryn and Verdinelli, Isabella},
  journal={Statistical Science},
  volume={10},
  number={3},
  pages={273--304},
  year={1995},
}

@article{ryan2016review,
  title={{A review of modern computational algorithms for {B}ayesian optimal design}},
  author={Ryan, Elizabeth G and Drovandi, Christopher C and McGree, James M and Pettitt, Anthony N},
  journal={International Statistical Review},
  volume={84},
  number={1},
  pages={128--154},
  year={2016},
}

@inproceedings{rainforth2018nesting,
  title={{On nesting Monte Carlo estimators}},
  author={Rainforth, Tom and Cornish, Rob and Yang, Hongseok and Warrington, Andrew and Wood, Frank},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  pages={4267--4276},
  year={2018},
}

@inproceedings{kleinegesse2019efficient,
  title={Efficient {b}ayesian experimental design for implicit models},
  author={Kleinegesse, Steven and Gutmann, Michael U},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  pages={476--485},
  year={2019},
  OPTorganization={PMLR}
}

@article{kleinegesse2021sequential,
  title={Sequential {B}ayesian experimental design for implicit models via mutual information},
  author={Kleinegesse, Steven and Drovandi, Christopher and Gutmann, Michael U},
  journal={Bayesian Analysis},
  volume={16},
  number={3},
  pages={773--802},
  year={2021},
}

@Article{House2015,
  author    = {Leanna House and Scotland Leman and Chao Han},
  journal   = {Statistical Analysis and Data Mining: The {ASA} Data Science Journal},
  title     = {Bayesian visual analytics: {BaVA}},
  year      = {2015},
  month     = {jan},
  number    = {1},
  pages     = {1--13},
  volume    = {8},
  publisher = {Wiley},
}

@Article{Cano2011,
  author    = {A. Cano and A. R. Masegosa and S. Moral},
  journal   = {{IEEE} Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title     = {A Method for Integrating Expert Knowledge When Learning Bayesian Networks From Data},
  year      = {2011},
  month     = {oct},
  number    = {5},
  pages     = {1382--1394},
  volume    = {41},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Book{OHagan2006,
  author    = {Anthony O{\textquotesingle}Hagan and Caitlin E. Buck and Alireza Daneshkhah and J. Richard Eiser and Paul H. Garthwaite and David J. Jenkinson and Jeremy E. Oakley and Tim Rakow},
  publisher = {John Wiley {\&} Sons, Ltd},
  title     = {Uncertain Judgements: Eliciting Experts{\textquotesingle} Probabilities},
  year      = {2006},
  month     = {jul},
}

@InProceedings{Soare2016,
  author    = {Marta Soare and Muhammad Ammad-Ud-Din and Samuel Kaski},
  booktitle = {2016 15th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
  title     = {Regression with n$\rightarrow$1 by Expert Knowledge Elicitation},
  year      = {2016},
  month     = {dec},
  publisher = {{IEEE}},
}

@InProceedings{Micallef2017,
  author    = {Luana Micallef and Iiris Sundin and Pekka Marttinen and Muhammad Ammad-ud-din and Tomi Peltola and Marta Soare and Giulio Jacucci and Samuel Kaski},
  booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
  title     = {Interactive Elicitation of Knowledge on Feature Relevance Improves Predictions in Small Data Sets},
  year      = {2017},
  month     = {mar},
  publisher = {{ACM}},
}

@InProceedings{Afrabandpey2017,
  author    = {Homayun Afrabandpey and Tomi Peltola and Samuel Kaski},
  booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
  title     = {Interactive Prior Elicitation of Feature Similarities for Small Sample Size Prediction},
  year      = {2017},
  month     = {jul},
  publisher = {{ACM}},
}

@Article{Sundin2018,
  author    = {Iiris Sundin and Tomi Peltola and Luana Micallef and Homayun Afrabandpey and Marta Soare and Muntasir Mamun Majumder and Pedram Daee and Chen He and Baris Serim and Aki Havulinna and Caroline Heckman and Giulio Jacucci and Pekka Marttinen and Samuel Kaski},
  journal   = {Bioinformatics},
  title     = {Improving genomics-based predictions for precision medicine through active elicitation of expert knowledge},
  year      = {2018},
  number    = {13},
  pages     = {i395--i403},
  volume    = {34},
}

@INPROCEEDINGS{Perez-Cruz2008,
  author={Perez-Cruz, Fernando},
  booktitle={Proceedings of the IEEE International Symposium on Information Theory}, 
  title={Kullback-Leibler divergence estimation of continuous distributions}, 
  year={2008},
  volume={},
  number={},
  pages={1666-1670}}
  
  @INPROCEEDINGS{Wang2006,
  author={Wang, Qing and Kulkarni, Sanjeev R. and Verdu, Sergio},
  booktitle={Proceedings of the IEEE International Symposium on Information Theory}, 
  title={A Nearest-Neighbor Approach to Estimating Divergence between Continuous Random Vectors}, 
  year={2006},
  volume={},
  number={},
  pages={242-246}}
  
@Article{pymc3_2016,
  author    = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
  journal   = {{PeerJ} Computer Science},
  title     = {Probabilistic programming in Python using {PyMC}3},
  year      = {2016},
  month     = {apr},
  pages     = {e55},
  volume    = {2},
  publisher = {{PeerJ}},
}

@Article{Rayner2002,
  author    = {G. D. Rayner and H. L. MacGillivray},
  journal   = {Statistics and Computing},
  title = {Numerical maximum likelihood estimation for the g-and-k and generalized g-and-h distributions},
  year      = {2002},
  number    = {1},
  pages     = {57--75},
  volume    = {12},
  publisher = {Springer Science and Business Media {LLC}},
}

@PhdThesis{Bharti_Thesis,
  author = {Ayush Bharti},
  school = {The Technical Faculty of IT and Design, Aalborg Univerity, Denmark},
  title  = {Calibrating Stochastic Radio Channel Models: An Approximate Bayesian Computation Approach},
  year   = {2021},
}

@Book{Goldsmith2005,
  author    = {Andrea Goldsmith},
  publisher = {Cambridge University Press},
  title     = {Wireless Communications},
  year      = {2005},
}

@inproceedings{Ramoni2021,
title = "{Bayesian Synthetic Likelihood for Calibration of Stochastic Radio Channel Model}",
author = "Adeogun, {Ramoni Ojekunle} and Larsen, {Claus Meyer} and Dennis Sand and Bovbjerg, {Holger Severin} and Fisker, {Peter Kj{\ae}r} and Gjerde, {Tor K}",
year = "2021",
booktitle = "Proceedings of the IEEE Vehicular Technology Conference (VTC)",
}

@Article{Turin1972,
  author   = {G. L. Turin and F. D. Clapp and T. L. Johnston and S. B. Fine and D. Lavry},
  title    = {A statistical model of urban multipath propagation},
  journal  = {IEEE Transactions on Vehicular Technology},
  year     = {1972},
  volume   = {21},
  number   = {1},
  pages    = {1-9},
}

@Article{Frazier2020,
  author    = {David T. Frazier and Christian P. Robert and Judith Rousseau},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title     = {Model misspecification in approximate {B}ayesian computation: consequences and diagnostics},
  year      = {2020},
  number    = {2},
  pages     = {421--444},
  volume    = {82},
  publisher = {Wiley},
}

@Article{Cranmer2020,
  author    = {Kyle Cranmer and Johann Brehmer and Gilles Louppe},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {The frontier of simulation-based inference},
  year      = {2020},
  number    = {48},
  pages     = {30055--30062},
  volume    = {117},
  publisher = {Proceedings of the National Academy of Sciences},
}

@inproceedings{AyushSPAWC19,
title = {{Estimator for Stochastic Channel Model without Multipath Extraction using Temporal Moments}},
author = {Ayush Bharti and Ramoni Adeogun and Troels Pedersen},
year = {2019},
pages={1-5}, 
booktitle = {20th IEEE Int. Workshop on Signal Process. Advances in Wireless Commun. (SPAWC)},
}

@Article{Marjoram2003,
  author    = {P. Marjoram and J. Molitor and V. Plagnol and S. Tavare},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {{Markov chain Monte Carlo without likelihoods}},
  year      = {2003},
  number    = {26},
  pages     = {15324--15328},
  volume    = {100},
}

@InProceedings{Lueckmann2019,
  title = 	 {Likelihood-free inference with emulator networks},
  author = {Lueckmann, Jan-Matthis and Bassetto, Giacomo and Karaletsos, Theofanis and Macke, Jakob H.},
  booktitle = {Proceedings of The 1st Symposium on Advances in Approximate {B}ayesian Inference},
  pages = 	 {32--53},
  year = 	 {2019},
}

@inproceedings{Papamakarios2016,
 author = {Papamakarios, George and Murray, Iain},
 booktitle = {Advances in Neural Information Processing Systems (NIPS)},
 pages = {1036-1044},
 title = {Fast $\epsilon$ -free Inference of Simulation Models with {B}ayesian Conditional Density Estimation},
 year = {2016}
}

@inproceedings{Lueckmann2016,
author = {Lueckmann, Jan-Matthis and Gon\c{c}alves, Pedro J. and Bassetto, Giacomo and \"{O}cal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H.},
title = {Flexible Statistical Inference for Mechanistic Models of Neural Dynamics},
year = {2017},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1289–1299},
}

@Article{Izbicki2019,
  author    = {Rafael Izbicki and Ann B. Lee and Taylor Pospisil},
  journal   = {Journal of Computational and Graphical Statistics},
  title     = {{ABC}{\textendash}{CDE}: Toward Approximate {B}ayesian Computation With Complex High-Dimensional Data and Limited Simulations},
  year      = {2019},
  number    = {3},
  pages     = {481--492},
  volume    = {28},
  publisher = {Informa {UK} Limited},
}

@article{Park2015,
abstract = {Complicated generative models often result in a situation where computing the likelihood of observed data is intractable, while simulating from the conditional density given a parameter value is relatively easy. Approximate Bayesian Computation (ABC) is a paradigm that enables simulation-based posterior inference in such cases by measuring the similarity between simulated and observed data in terms of a chosen set of summary statistics. However, there is no general rule to construct sufficient summary statistics for complex models. Insufficient summary statistics will "leak" information, which leads to ABC algorithms yielding samples from an incorrect (partial) posterior. In this paper, we propose a fully nonparametric ABC paradigm which circumvents the need for manually selecting summary statistics. Our approach, K2-ABC, uses maximum mean discrepancy (MMD) as a dissimilarity measure between the distributions over observed and simulated data. MMD is easily estimated as the squared difference between their empirical kernel embeddings. Experiments on a simulated scenario and a real-world biological problem illustrate the effectiveness of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {1502.02558},
author = {Park, M. and Jitkrittum, W. and Sejdinovic, D.},
eprint = {1502.02558},
file = {:home/francois/Dropbox/Work/Papers/Kernel Methods/Kernel Embeddings/(2016, Park) K2-ABC Approximate Bayesian Computation with Kernel Embeddings.pdf:pdf},
journal = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
pages = {398--407},
title = {{K2-ABC: approximate Bayesian computation with kernel embeddings}},
volume = {51},
year = {2015}
}

@article{Bernton2019,
author = {Bernton, E. and Jacob, P. E. and Gerber, M. and Robert, C. P.},
file = {:home/francois/Dropbox/Work/Papers/Probability Distributions/Distances between distributions/minimum distance estimation/(2019, Bernton) Approximate Bayesian computation with the Wasserstein distance.pdf:pdf},
issn = {14679868},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Approximate Bayesian computation,Generative models,Likelihood-free inference,Optimal transport,Wasserstein distance},
number = {2},
pages = {235--269},
title = {{Approximate Bayesian computation with the Wasserstein distance}},
volume = {81},
year = {2019}
}

@InProceedings{Jiang2018,
  title = 	 {Approximate {B}ayesian Computation with {K}ullback-{L}eibler Divergence as Data Discrepancy},
  author = 	 {Jiang, Bai},
  booktitle = 	 {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1711--1721},
  year = 	 {2018},
}

@article{Sriperumbudur2009,
abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be defined as the distance between distribution embeddings: we denote this as $\gamma_k$, indexed by the kernel function $k$ that defines the inner product in the RKHS. We present three theoretical properties of $\gamma_k$. First, we consider the question of determining the conditions on the kernel $k$ for which $\gamma_k$ is a metric: such $k$ are denoted {\em characteristic kernels}. Unlike pseudometrics, a metric is zero only when two distributions coincide, thus ensuring the RKHS embedding maps all distributions uniquely (i.e., the embedding is injective). While previously published conditions may apply only in restricted circumstances (e.g. on compact domains), and are difficult to check, our conditions are straightforward and intuitive: bounded continuous strictly positive definite kernels are characteristic. Alternatively, if a bounded continuous kernel is translation-invariant on $\bb{R}^d$, then it is characteristic if and only if the support of its Fourier transform is the entire $\bb{R}^d$. Second, we show that there exist distinct distributions that are arbitrarily close in $\gamma_k$. Third, to understand the nature of the topology induced by $\gamma_k$, we relate $\gamma_k$ to other popular metrics on probability measures, and present conditions on the kernel $k$ under which $\gamma_k$ metrizes the weak topology.},
author = {Sriperumbudur, B. K. and Gretton, A. and Fukumizu, K. and Sch{\"{o}}lkopf, B. and Lanckriet, G. R. G.},
journal = {Journal of Machine Learning Research},
title = {{Hilbert space embeddings and metrics on probability measures}},
volume = {11},
year = {2010}
}


@inproceedings{Key2021,
abstract = {Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of inference methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. One set of tools which can help are goodness-of-fit tests, where we test whether a dataset could have been generated by a fixed distribution. Kernel-based tests have been developed to for this problem, and these are popular due to their flexibility, strong theoretical guarantees and ease of implementation in a wide range of scenarios. In this paper, we extend this line of work to the more challenging composite goodness-of-fit problem, where we are instead interested in whether the data comes from any distribution in some parametric family. This is equivalent to testing whether a parametric model is well-specified for the data.},
author = {Key, O. and Fernandez, T. and Gretton, A. and Briol, F-X.},
booktitle = {NeurIPS 2021 Workshop Your Model Is Wrong: Robustness and Misspecification in Probabilistic Modeling},
file = {:Users/fxbriol/Docs/Work/Papers/Kernel Methods/Hypothesis Testing/(2021, Key) Composite goodness of fit testing with kernels.pdf:pdf},
title = {{Composite goodness-of-fit tests with kernels}},
year = {2021}
}


@Article{Prangle2020,
  author    = {Dennis Prangle},
  journal   = {The R Journal},
  title     = {{gk: An R Package for the g-and-k and Generalised g-and-h Distributions}},
  year      = {2020},
  number    = {1},
  pages     = {7},
  volume    = {12},
  publisher = {The R Foundation},
}

@Article{Csillery2012,
  author    = {Katalin Csill{\'{e}}ry and Olivier Fran{\c{c}}ois and Michael G. B. Blum},
  journal   = {Methods in Ecology and Evolution},
  title     = {abc: an R package for approximate Bayesian computation ({ABC})},
  year      = {2012},
  month     = {jan},
  number    = {3},
  pages     = {475--479},
  volume    = {3},
  publisher = {Wiley},
}

@Article{Bentley1975,
  author    = {Jon Louis Bentley},
  journal   = {Communications of the {ACM}},
  title     = {Multidimensional binary search trees used for associative searching},
  year      = {1975},
  number    = {9},
  pages     = {509--517},
  volume    = {18},
  publisher = {Association for Computing Machinery ({ACM})},
}

@InProceedings{Songrit2001,
author="Maneewongvatana, Songrit
and Mount, David M.",
title="On the Efficiency of Nearest Neighbor Searching with Data Clustered in Lower Dimensions",
booktitle= "Proceedings of the International Conference on Computational Science (ICCS)",
pages="842--851",
year="2001"
}

@InProceedings{Daee2018,
  author    = {Pedram Daee and Tomi Peltola and Aki Vehtari and Samuel Kaski},
  booktitle= {Proceedings of the International Conference on Intelligent User Interfaces (IUI)},
  title     = {User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction},
  year      = {2018},
  pages = {305-310}
}

@Article{Parr1980,
  author    = {W. C. Parr and W. R. Schucany},
  journal   = {Journal of the Americal Statistical Association},
  title     = {Minimum distance and robust estimation},
  year      = {1980},
  number    = {371},
  pages     = {616--624},
  volume    = {75}
}

@Article{Wood2010,
  author    = {Simon N. Wood},
  journal   = {Nature},
  title     = {Statistical inference for noisy nonlinear ecological dynamic systems},
  year      = {2010},
  number    = {7310},
  pages     = {1102--1104},
  volume    = {466},
  doi       = {10.1038/nature09319},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Cameron2012,
  author    = {E. Cameron and A. N. Pettitt},
  journal   = {Monthly Notices of the Royal Astronomical Society},
  title     = {Approximate {B}ayesian Computation for astronomical model analysis: a case study in galaxy demographics and morphological transformation at high redshift},
  year      = {2012},
  number    = {1},
  pages     = {44--65},
  volume    = {425},
  doi       = {10.1111/j.1365-2966.2012.21371.x},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Kopka2016,
  author    = {Piotr Kopka and Anna Wawrzynczak and Mieczyslaw Borysiewicz},
  journal   = {Atmospheric Environment},
  title     = {Application of the Approximate {B}ayesian Computation methods in the stochastic estimation of atmospheric contamination parameters for mobile sources},
  year      = {2016},
  pages     = {201--212},
  volume    = {145},
  doi       = {10.1016/j.atmosenv.2016.09.029},
  publisher = {Elsevier {BV}},
}



@inproceedings{Kajihara2018,
abstract = {We propose a novel approach to parameter esti-mation for simulator-based statistical models with intractable likelihoods. The proposed method is recursive application of kernel ABC and kernel herding to the same observed data. We provide a theoretical explanation regarding why this ap-proach works, showing (for the population setting) that the point estimate obtained with this method converges to the true parameter as recursion pro-ceeds, under a certain assumption. We conduct a variety of numerical experiments, including pa-rameter estimation for a real-world pedestrian flow simulator, and show that our method out-performs existing approaches in most cases.},
author = {Kajihara, T. and Yamazaki, K. and Kanagawa, M. and Fukumizu, K.},
booktitle = {International Conference on Machine Learning},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Kajihara et al. - Unknown - Kernel Recursive ABC Point Estimation with Intractable Likelihood.pdf:pdf},
pages = {2400--2409},
title = {{Kernel recursive ABC: Point estimation with intractable likelihood}},
year = {2018}
}



@Article{Gutmann2017,
  author    = {Michael U. Gutmann and Ritabrata Dutta and Samuel Kaski and Jukka Corander},
  journal   = {Statistics and Computing},
  title     = {Likelihood-free inference via classification},
  year      = {2017},
  number    = {2},
  pages     = {411--425},
  volume    = {28},
  doi       = {10.1007/s11222-017-9738-6},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Peyre2019,
  author    = {Gabriel Peyr{\'{e}} and Marco Cuturi},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  title     = {Computational Optimal Transport: With Applications to Data Science},
  year      = {2019},
  number    = {5-6},
  pages     = {355--607},
  volume    = {11},
  doi       = {10.1561/2200000073},
  publisher = {Now Publishers},
}

@Article{Muandet2017,
  author    = {Krikamol Muandet and Kenji Fukumizu and Bharath Sriperumbudur and Bernhard Schölkopf},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  title     = {Kernel Mean Embedding of Distributions: A Review and Beyond},
  year      = {2017},
  number    = {1-2},
  pages     = {1--141},
  volume    = {10},
  doi       = {10.1561/2200000060},
  publisher = {Now Publishers},
}

@inproceedings{Genevay2017,
abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles both these issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
author = {Genevay, A. and Peyr{\'{e}}, G. and Cuturi, M.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Genevay, Peyr{\'{e}}, Cuturi - 2018 - Learning generative models with Sinkhorn divergences.pdf:pdf},
pages = {1608--1617},
title = {{Learning generative models with Sinkhorn divergences}},
year = {2018}
}



@InProceedings{Mitrovic2016,
  title = 	 {{DR-ABC}: Approximate {B}ayesian Computation with Kernel-Based Distribution Regression},
  author = 	 {Mitrovic, Jovana and Sejdinovic, Dino and Teh, Yee-Whye},
  booktitle = 	 {Proceedings of the International Conference on Machine Learning},
  pages = 	 {1482--1491},
  year = 	 {2016},
  OPTvolume = 	 {48},
  pdf = 	 {http://proceedings.mlr.press/v48/mitrovic16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/mitrovic16.html},
  abstract = 	 {Performing exact posterior inference in complex generative models is often difficult or impossible due to an expensive to evaluate or intractable likelihood function. Approximate Bayesian computation (ABC) is an inference framework that constructs an approximation to the true likelihood based on the similarity between the observed and simulated data as measured by a predefined set of summary statistics. Although the choice of informative problem-specific summary statistics crucially influences the quality of the likelihood approximation and hence also the quality of the posterior sample in ABC, there are only few principled general-purpose approaches to the selection or construction of such summary statistics. In this paper, we develop a novel framework for solving this problem. We model the functional relationship between the data and the optimal choice (with respect to a loss function) of summary statistics using kernel-based distribution regression. Furthermore, we extend our approach to incorporate kernel-based regression from conditional distributions, thus appropriately taking into account the specific structure of the posited generative model. We show that our approach can be implemented in a computationally and statistically efficient way using the random Fourier features framework for large-scale kernel learning. In addition to that, our framework outperforms related methods by a large margin on toy and real-world data, including hierarchical and time series models.}
}

@article{Briol2019MMD,
author = {Briol, F-X. and Barp, A. and Duncan, A. B. and Girolami, M.},
file = {:home/francois/Dropbox/Work/Papers/Probability Distributions/Distances between distributions/minimum distance estimation/MMD_paper.pdf:pdf},
journal = {arXiv:1906.05944},
title = {{Statistical inference for generative models with maximum mean discrepancy}},
year = {2019}
}


@inproceedings{Li2015GMMN,
abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
author = {Li, Yujia and Swersky, Kevin and Zemel, Richard},
booktitle = {International Conference on Machine Learning},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Li, Swersky, Zemel - 2015 - Generative Moment Matching.pdf:pdf},
pages = {1718--1727},
title = {Generative Moment Matching Networks},
OPTvolume = {37},
year = {2015}
}


@article{Alquier2022,
abstract = {This paper deals with robust inference for parametric copula models. Estimation using Canonical Maximum Likelihood might be unstable, especially in the presence of outliers. We propose to use a procedure based on the Maximum Mean Discrepancy (MMD) principle. We derive non-asymptotic oracle inequalities, consistency and asymptotic normality of this new estimator. In particular, the oracle inequality holds without any assumption on the copula family, and can be applied in the presence of outliers or under misspecification. Moreover, in our MMD framework, the statistical inference of copula models for which there exists no density with respect to the Lebesgue measure on $[0,1]^d$, as the Marshall-Olkin copula, becomes feasible. A simulation study shows the robustness of our new procedures, especially compared to pseudo-maximum likelihood estimation. An R package implementing the MMD estimator for copula models is available.},
archivePrefix = {arXiv},
arxivId = {2010.00408},
author = {Alquier, P. and Ch{\'{e}}rief-Abdellatif, B.-E. and Derumigny, A. and Fermanian, J.-D.},
doi = {10.1080/01621459.2021.2024836},
eprint = {2010.00408},
file = {:Users/fxbriol/Docs/Work/Papers/Other Statistics/Generative models/(2022, Alquier) Estimation of copulas via MMD.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
pages = {1--39},
title = {{Estimation of copulas via Maximum Mean Discrepancy}},
year = {2022}
}

@article{Wenger2021,
abstract = {Probabilistic numerical methods (PNMs) solve numerical problems via probabilistic inference. They have been developed for linear algebra, optimization, integration and differential equation simulation. PNMs naturally incorporate prior information about a problem and quantify uncertainty due to finite computational resources as well as stochastic input. In this paper, we present ProbNum: a Python library providing state-of-the-art probabilistic numerical solvers. ProbNum enables custom composition of PNMs for specific problem classes via a modular design as well as wrappers for off-the-shelf use. Tutorials, documentation, developer guides and benchmarks are available online at www.probnum.org.},
archivePrefix = {arXiv},
arxivId = {2112.02100},
author = {Wenger, J. and Kr{\"{a}}mer, N. and Pf{\"{o}}rtner, M. and Schmidt, J. and Bosch, N. and Effenberger, N. and Zenn, J. and Gessner, A. and Karvonen, T. and Briol, F-X and Mahsereci, M. and Hennig, P.},
eprint = {2112.02100},
file = {:Users/fxbriol/Docs/Work/Papers/Probabilistic Numerics/(2021, Wenger) ProbNum - Probabilistic numerics in python.pdf:pdf},
journal = {arXiv:2112.02100},
keywords = {machine learning,numerical analysis,probabilistic numerics},
title = {{ProbNum: Probabilistic numerics in Python}},
url = {http://arxiv.org/abs/2112.02100},
year = {2021}
}


@inproceedings{Binkowski2018,
abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, while gradients of the generator's theoretical loss are biased in both cases. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cram\'er GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.},
author = {Bi{\'{n}}kowski, M. and Sutherland, D. J. and Arbel, M. and Gretton, A.},
booktitle = {International Conference on Learning Representation},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Bi{\'{n}}kowski et al. - 2018 - Demystifying MMD GANs.pdf:pdf},
title = {{Demystifying MMD GANs}},
year = {2018}
}

@inproceedings{Gunter2014,
author = {Gunter, T. and Garnett, R. and Osborne, M. and Hennig, P. and Roberts, S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Gunter et al. - 2014 - Sampling for inference in probabilistic models with fast Bayesian quadrature.pdf:pdf},
pages = {2789--2797},
title = {{Sampling for inference in probabilistic models with fast Bayesian quadrature}},
year = {2014}
}


@inproceedings{Bardenet2019,
author = {Belhadji, A. and Bardenet, R. and Chainais, P.},
booktitle = {Neural Information Processing Systems},
file = {:Users/fxbriol/Docs/Work/Papers/Probabilistic Numerics/(2019, Belhadji) Kernel quadrature with DPPs.pdf:pdf},
keywords = {determinantal point processes,kernel hilbert spaces,monte carlo integration,reproducing},
pages = {12927--12937},
title = {{Kernel quadrature with DPPs}},
year = {2019}
}



@inproceedings{Briol2015,
author = {Briol, F-X. and Oates, C. J. and Girolami, M. and Osborne, M. A.},
booktitle = {Neural Information Processing Systems},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Briol et al. - 2015 - Frank-Wolfe Bayesian quadrature Probabilistic integration with theoretical guarantees.pdf:pdf},
pages = {1162--1170},
title = {{Frank-Wolfe Bayesian quadrature: Probabilistic integration with theoretical guarantees}},
year = {2015}
}


@article{Legramanti2022,
abstract = {Classical implementations of approximate Bayesian computation (ABC) employ summary statistics to measure the discrepancy among the observed data and the synthetic samples generated from each proposed value of the parameter of interest. However, finding effective summaries is challenging for most of the complex models for which ABC is required. This issue has motivated a growing literature on summary-free versions of ABC that leverage the discrepancy among the empirical distributions of the observed and synthetic data, rather than focusing on selected summaries. The effectiveness of these solutions has led to an increasing interest in the properties of the corresponding ABC posteriors, with a focus on concentration and robustness in asymptotic regimes. Although recent contributions have made key advancements, current theory mostly relies on existence arguments which are not immediate to verify and often yield bounds that are not readily interpretable, thus limiting the methodological implications of theoretical results. In this article we address such aspects by developing a novel unified and constructive framework, based on the concept of Rademacher complexity, to study concentration and robustness of ABC posteriors within the general class of integral probability semimetrics (IPS), that includes routinely-implemented discrepancies such as Wasserstein distance and MMD, and naturally extends classical summary-based ABC. For rejection ABC based on the IPS class, we prove that the theoretical properties of the ABC posterior in terms of concentration and robustness directly relate to the asymptotic behavior of the Rademacher complexity of the class of functions associated to each discrepancy. This result yields a novel understanding of the practical performance of ABC with specific discrepancies, as shown also in empirical studies, and allows to develop new theory guiding ABC calibration.},
archivePrefix = {arXiv},
arxivId = {2206.06991},
author = {Legramanti, S. and Durante, D. and Alquier, P.},
eprint = {2206.06991},
file = {:Users/fxbriol/Docs/Work/Papers/Bayesian Statistics/ABC/(2022, Legramanti) Concentration and robustness of discrepancy-based ABC.pdf:pdf},
journal = {arXiv:2206.06991},
keywords = {abc,integral probability semimetric,ity,maximum mean discrepancy,rademacher complex-,wasserstein distance},
title = {{Concentration and robustness of discrepancy-based ABC via Rademacher complexity}},
url = {http://arxiv.org/abs/2206.06991},
year = {2022}
}




@inproceedings{cherief2020mmd,
  title={{MMD-B}ayes: Robust {B}ayesian estimation via maximum mean discrepancy},
  author={Ch{\'e}rief-Abdellatif, Badr-Eddine and Alquier, Pierre},
  booktitle={Proceesings of the 2nd Symposium on Advances in Approximate Bayesian Inference},
  pages={1--21},
  year={2020},
  OPTorganization={PMLR}
}

@article{Alquier2021,
author = {P. Alquier and M. Gerber},
journal = {arXiv:2006.00840, to appear at Biometrika},
title = {{Universal robust regression via maximum mean discrepancy}},
year = {2020}
}


@article{cherief2021,
abstract = {Many works in statistics aim at designing a universal estimation procedure. This question is of major interest, in particular because it leads to robust estimators, a very hot topic in statistics and machine learning. In this paper, we tackle the problem of universal estimation using a minimum distance estimator presented in Briol et al. (2019) based on the Maximum Mean Discrepancy. We show that the estimator is robust to both dependence and to the presence of outliers in the dataset. We also highlight the connections that may exist with minimum distance estimators using L2-distance. Finally, we provide a theoretical study of the stochastic gradient descent algorithm used to compute the estimator, and we support our findings with numerical simulations.},
author = {Ch{\'{e}}rief-Abdellatif, B-E. and Alquier, P.},
file = {:Users/fxbriol/Docs/Work/Papers/Probability Distributions/Distances between distributions/minimum distance estimation/(2019, Cherief-Abdellatif & Alquier) Finite sample properties of parametric MMD estimation - robustness to misspecification and d:},
journal = {arXiv:1912.05737, to appear at Bernoulli},
title = {{Finite sample properties of parametric MMD estimation: robustness to misspecification and dependence}},
year = {2022}
}


@inproceedings{Dellaporta2022,
    title={Robust {B}ayesian Inference for Simulator-based Models via the {MMD} Posterior Bootstrap}, 
    author={Charita Dellaporta and Jeremias Knoblauch and Theodoros Damoulas and François-Xavier Briol},
    year={2022},
    pages={943-970},
    booktitle={Proceedings of the International Conference in Artificial Intelligence and Statistics},
    OPTorganization={PMLR}
}

@Article{Bissiri2016,
  author    = {P. G. Bissiri and C. C. Holmes and S. G. Walker},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title     = {A general framework for updating belief distributions},
  year      = {2016},
  month     = {feb},
  number    = {5},
  pages     = {1103--1130},
  volume    = {78},
  doi       = {10.1111/rssb.12158},
  publisher = {Wiley},
}

@Article{Knoblauch2019,
  author        = {Jeremias Knoblauch and Jack Jewson and Theodoros Damoulas},
  title         = {Generalized Variational Inference: Three arguments for deriving new Posteriors},
  year          = {2019},
  abstract      = {We advocate an optimization-centric view on and introduce a novel generalization of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization problem (Csiszar, 1975; Donsker and Varadhan; 1975, Zellner; 1988). First, we use it to prove an optimality result of standard Variational Inference (VI): Under the proposed view, the standard Evidence Lower Bound (ELBO) maximizing VI posterior is preferable to alternative approximations of the Bayesian posterior. Next, we argue for generalizing standard Bayesian inference. The need for this arises in situations of severe misalignment between reality and three assumptions underlying standard Bayesian inference: (1) Well-specified priors, (2) well-specified likelihoods, (3) the availability of infinite computing power. Our generalization addresses these shortcomings with three arguments and is called the Rule of Three (RoT). We derive it axiomatically and recover existing posteriors as special cases, including the Bayesian posterior and its approximation by standard VI. In contrast, approximations based on alternative ELBO-like objectives violate the axioms. Finally, we study a special case of the RoT that we call Generalized Variational Inference (GVI). GVI posteriors are a large and tractable family of belief distributions specified by three arguments: A loss, a divergence and a variational family. GVI posteriors have appealing properties, including consistency and an interpretation as approximate ELBO. The last part of the paper explores some attractive applications of GVI in popular machine learning models, including robustness and more appropriate marginals. After deriving black box inference schemes for GVI posteriors, their predictive performance is investigated on Bayesian Neural Networks and Deep Gaussian Processes, where GVI can comprehensively improve upon existing methods.},
  journal        = {arXiv:1904.02063}
}

@article{Nguyen2020,
abstract = {Approximate Bayesian computation (ABC) has become an essential part of the Bayesian toolbox for addressing problems in which the likelihood is prohibitively expensive or entirely unknown, making it intractable. ABC defines a pseudo-posterior by comparing observed data with simulated data, traditionally based on some summary statistics, the elicitation of which is regarded as a key difficulty. Recently, using data discrepancy measures has been proposed in order to bypass the construction of summary statistics. Here we propose to use the importance-sampling ABC (IS-ABC) algorithm relying on the so-called two-sample energy statistic. We establish a new asymptotic result for the case where both the observed sample size and the simulated data sample size increase to infinity, which highlights to what extent the data discrepancy measure impacts the asymptotic pseudo-posterior. The result holds in the broad setting of IS-ABC methodologies, thus generalizing previous results that have been established only for rejection ABC algorithms. Furthermore, we propose a consistent V-statistic estimator of the energy statistic, under which we show that the large sample result holds, and prove that the rejection ABC algorithm, based on the energy statistic, generates pseudo-posterior distributions that achieves convergence to the correct limits, when implemented with rejection thresholds that converge to zero, in the finite sample setting. Our proposed energy statistic based ABC algorithm is demonstrated on a variety of models, including a Gaussian mixture, a moving-average model of order two, a bivariate beta and a multivariate g -and- k distribution. We find that our proposed method compares well with alternative discrepancy measures.},
author = {Nguyen, H. D. and Arbel, J. and Lu, H. and Forbes, F.},
journal = {IEEE Access},
keywords = {Approximate Bayesian computation,Kullback-Leibler divergence,Wasserstein distance,energy statistic,importance sampling,maximum mean discrepancy},
pages = {131683--131698},
title = {Approximate {B}ayesian Computation Via the Energy Statistic},
volume = {8},
year = {2020}
}

@inproceedings{Li2017MMDGAN,
abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
author = {Li, C.-L. and Chang, W.-C. and Cheng, Y. and Yang, Y. and P{\'{o}}czos, B.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - MMD GAN Towards deeper understanding of moment matching network.pdf:pdf},
pages = {2203--2213},
title = {{MMD GAN: Towards deeper understanding of moment matching network}},
year = {2017}
}


@inproceedings{Dziugaite2015,
abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD.},
author = {Dziugaite, G. K. and Roy, D. M. and Ghahramani, Z.},
booktitle = {Uncertainty in Artificial Intelligence},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Dziugaite, Roy, Ghahramani - 2015 - Training generative neural networks via maximum mean discrepancy optimization.pdf:pdf},
title = {{Training generative neural networks via maximum mean discrepancy optimization}},
year = {2015}
}


@inproceedings{Lyddon2018,
author = {Lyddon, Simon and Walker, Stephen and Holmes, Chris},
title = {Nonparametric Learning from {B}ayesian Models with Randomized Objective Functions},
year = {2018},
abstract = {Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {2075–2085},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{Niederer2019,
abstract = {The treatment of individual patients in cardiology practice increasingly relies on advanced imaging, genetic screening and devices. As the amount of imaging and other diagnostic data increases, paralleled by the greater capacity to personalize treatment, the difficulty of using the full array of measurements of a patient to determine an optimal treatment seems also to be paradoxically increasing. Computational models are progressively addressing this issue by providing a common framework for integrating multiple data sets from individual patients. These models, which are based on physiology and physics rather than on population statistics, enable computational simulations to reveal diagnostic information that would have otherwise remained concealed and to predict treatment outcomes for individual patients. The inherent need for patient-specific models in cardiology is clear and is driving the rapid development of tools and techniques for creating personalized methods to guide pharmaceutical therapy, deployment of devices and surgical interventions.},
author = {Niederer, S. A. and Lumens, J. and Trayanova, N. A.},
doi = {10.1038/s41569-018-0104-y},
file = {:Users/fxbriol/Docs/Work/Papers/Applications/Medicine/(2019, Niederer) Computational models cardiology - nature review.pdf:pdf},
issn = {17595010},
journal = {Nature Reviews Cardiology},
number = {2},
pages = {100--111},
pmid = {30361497},
publisher = {Springer US},
title = {{Computational models in cardiology}},
url = {http://dx.doi.org/10.1038/s41569-018-0104-y},
volume = {16},
year = {2019}
}


@book{Berlinet2004,
address = {New York},
author = {Berlinet, A. and Thomas-Agnan, C.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berlinet, Thomas-Agnan - 2004 - Reproducing Kernel Hilbert Spaces in Probability and Statistics.pdf:pdf},
publisher = {Springer Science+Business Media},
title = {{Reproducing Kernel Hilbert Spaces in Probability and Statistics}},
year = {2004}
}

@Book{Dick2010,
 author = {Dick, J},
 title = {Digital nets and sequences : discrepancy and quasi-Monte Carlo integration},
 publisher = {Cambridge University Press},
 year = {2010},
 address = {Cambridge New York}
 }

@inproceedings{Jennings1999,
  title={Agent-Based Computing: Promise and Perils},
  author={Nicholas R. Jennings},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  year={1999}
}

@Book{Wooldridge2009,
  author    = {Wooldridge, Michael},
  publisher = {John Wiley \& Sons Inc},
  title     = {An Introduction to MultiAgent Systems},
  year      = {2009},
  url       = {https://www.ebook.de/de/product/7515024/michael_wooldridge_an_introduction_to_multiagent_systems.html},
}

@article{Pacchiardi2021,
abstract = {We propose a framework for Bayesian Likelihood-Free Inference (LFI) based on Generalized Bayesian Inference using scoring rules (SR). SR are used to evaluate probabilistic models given an observation; a proper SR is minimised in expectation when the model corresponds to the true data generating process for the observation. Using a strictly proper SR, for which the above minimum is unique, ensures posterior consistency of our method. As the likelihood function is intractable for LFI, we employ consistent estimators of SR using model simulations in a pseudo-marginal MCMC; we show the target of such chain converges to the exact SR posterior with increasing number of simulations. Furthermore, we note popular LFI techniques like Bayesian Synthetic Likelihood (BSL) and semiparametric BSL can be seen as special cases of our framework using only proper (but not strictly so) SR. We provide empirical results validating our consistency result and show how related approaches do not enjoy this property. Practically, we use the Energy and Kernel Scores, but our general framework sets the stage for extensions with other scoring rules.},
author = {Pacchiardi, L. and Dutta, R.},
file = {:Users/fxbriol/Docs/Work/Papers/Bayesian Statistics/generalised Bayes/(2021, Pacchiardi) generalised Bayes scoring rules.pdf:pdf},
journal = {arXiv:2104.03889},
title = {{Generalized Bayesian likelihood-free inference using scoring rules estimators}},
year = {2021}
}

@Article{Kypraios2017,
  author    = {Theodore Kypraios and Peter Neal and Dennis Prangle},
  journal   = {Mathematical Biosciences},
  title     = {A tutorial introduction to {B}ayesian inference for stochastic epidemic models using Approximate {B}ayesian Computation},
  year      = {2017},
  pages     = {42--53},
  volume    = {287},
  doi       = {10.1016/j.mbs.2016.07.001},
  publisher = {Elsevier {BV}},
}

@Article{Wiqvist2021,
  author        = {Samuel Wiqvist and Jes Frellsen and Umberto Picchini},
  title         = {Sequential Neural Posterior and Likelihood Approximation},
  year          = {2021},
  OPTmonth         = feb,
  abstract      = {We introduce the sequential neural posterior and likelihood approximation (SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inference in implicit models, and therefore is a simulation-based inference method that only requires simulations from a generative model. SNPLA avoids Markov chain Monte Carlo sampling and correction-steps of the parameter proposal function that are introduced in similar methods, but that can be numerically unstable or restrictive. By utilizing the reverse KL divergence, SNPLA manages to learn both the likelihood and the posterior in a sequential manner. Over four experiments, we show that SNPLA performs competitively when utilizing the same number of model simulations as used in other methods, even though the inference problem for SNPLA is more complex due to the joint learning of posterior and likelihood function. Due to utilizing normalizing flows SNPLA generates posterior draws much faster (4 orders of magnitude) than MCMC-based methods.},
  journal = {arXiv:2102.06522},
  eprint        = {},
  file          = {:http\://arxiv.org/pdf/2102.06522v2:PDF},
  keywords      = {stat.ML, cs.LG, stat.ME},
  primaryclass  = {stat.ML},
}

@InProceedings{Lueckmann2021,
  title = 	 { Benchmarking Simulation-Based Inference },
  author =       {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
  booktitle = 	 {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  pages = 	 {343--351},
  year = 	 {2021},
  abstract = 	 { Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such ’likelihood-free’ algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms. }
}

@Article{Li2017Copula,
  author    = {J. Li and D.J. Nott and Y. Fan and S.A. Sisson},
  journal   = {Computational Statistics {\&} Data Analysis},
  title     = {Extending approximate {B}ayesian computation methods to high dimensions via a {G}aussian copula model},
  year      = {2017},
  month     = {Feb},
  pages     = {77--89},
  volume    = {106},
  doi       = {10.1016/j.csda.2016.07.005},
  publisher = {Elsevier {BV}},
}

@book{royden1988real,
  title={Real analysis},
  author={Royden, Halsey Lawrence and Fitzpatrick, Patrick},
  volume={32},
  year={1988},
  publisher={Macmillan New York}
}

@book{adams2003sobolev,
  title={Sobolev spaces},
  author={Adams, Robert A and Fournier, John JF},
  year={2003},
  publisher={Elsevier}
}


@InProceedings{Bharti22a,
  title = 	 {Approximate {B}ayesian Computation with Domain Expert in the Loop},
  author =       {Bharti, Ayush and Filstroff, Louis and Kaski, Samuel},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {1893--1905},
  year = 	 {2022},
  editor = 	 {},
  optvolume = 	 {162},
  series = 	 {},
  month = 	 {},
  publisher =    {},
  pdf = 	 {https://proceedings.mlr.press/v162/bharti22a/bharti22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/bharti22a.html},
  abstract = 	 {Approximate Bayesian computation (ABC) is a popular likelihood-free inference method for models with intractable likelihood functions. As ABC methods usually rely on comparing summary statistics of observed and simulated data, the choice of the statistics is crucial. This choice involves a trade-off between loss of information and dimensionality reduction, and is often determined based on domain knowledge. However, handcrafting and selecting suitable statistics is a laborious task involving multiple trial-and-error steps. In this work, we introduce an active learning method for ABC statistics selection which reduces the domain expert’s work considerably. By involving the experts, we are able to handle misspecified models, unlike the existing dimension reduction methods. Moreover, empirical results show better posterior estimates than with existing methods, when the simulation budget is limited.}
}

@book{Wendland2005,
    Author = "Wendland, Holger",
    Title = "Scattered Data Approximation",
    Publisher = "Cambridge University Press",
    Year = "2005",
    OPTSeries = "Cambridge Monographs on Applied and Computational Mathematics",
    OPTNumber = "17",
}

@article{Kirby2022,
abstract = {Turbine wake and farm blockage effects may significantly impact the power produced by large wind farms. In this study, we perform Large-Eddy Simulations (LES) of 50 infinitely large offshore wind farms with different turbine layouts and wind directions. The LES results are combined with the two-scale momentum theory (Nishino & Dunstan 2020, J. Fluid Mech. 894, A2) to investigate the aerodynamic performance of large but finite-sized farms as well. The power of infinitely large farms is found to be a strong function of the array density, whereas the power of large finite-sized farms depends on both the array density and turbine layout. An analytical model derived from the two-scale momentum theory predicts the impact of array density very well for all 50 farms investigated and can therefore be used as an upper limit to farm performance. We also propose a new method to quantify turbine-scale losses (due to turbine-wake interactions) and farm-scale losses (due to the reduction of farm-average wind speed). They both depend on the strength of atmospheric response to the farm, and our results suggest that, for large offshore wind farms, the farm-scale losses are typically more than twice as large as the turbine-scale losses. This is found to be due to a two-scale interaction between turbine wake and farm blockage effects, explaining why the impact of turbine layout on farm power varies with the strength of atmospheric response.},
author = {Kirby, A. and Nishino, T. and Dunstan, T.},
file = {:Users/fxbriol/Docs/Work/Papers/Applications/Wind Turbines/(2022, Kirby) Two-scale interaction of wake and blockage effects in large wind farms.pdf:pdf},
journal = {arXiv:2207.03148},
keywords = {added at the same,as these must be,authors should not enter,chosen by,during the typesetting,keywords on the manuscript,online submission process and,other classifications will be,process,see keyword pdf for,the author during the,the full list,will then be added},
title = {{Two-scale interaction of wake and blockage effects in large wind farms}},
year = {2022}
}

@article{Niayifar2016,
abstract = {Wind farm power production is known to be strongly affected by turbine wake effects. The purpose of this study is to develop and test a new analytical model for the prediction of wind turbine wakes and the associated power losses in wind farms. The new model is an extension of the one recently proposed by Bastankhah and Port{\'{e}}-Agel for the wake of stand-alone wind turbines. It satisfies the conservation of mass and momentum and assumes a self-similar Gaussian shape of the velocity deficit. The local wake growth rate is estimated based on the local streamwise turbulence intensity. Superposition of velocity deficits is used to model the interaction of the multiple wakes. Furthermore, the power production from the wind turbines is calculated using the power curve. The performance of the new analytical wind farm model is validated against power measurements and large-eddy simulation (LES) data from the Horns Rev wind farm for a wide range of wind directions, corresponding to a variety of full-wake and partial-wake conditions. A reasonable agreement is found between the proposed analytical model, LES data, and power measurements. Compared with a commonly used wind farm wake model, the new model shows a significant improvement in the prediction of wind farm power.},
author = {Niayifar, A. and Port{\'{e}}-Agel, F.},
file = {:Users/fxbriol/Docs/Work/Papers/Applications/Wind Turbines/(2016, Niayifar, Porte-Agel) Analytical modeling of wind farms - a new approach for power prediction.pdf:pdf},
journal = {Energies},
keywords = {Analytical model,Gaussian velocity deficit,Turbulence intensity,Velocity deficit superposition,Wake growth rate,Wind farm power production},
number = {9},
pages = {1--13},
title = {{Analytical modeling of wind farms: A new approach for power prediction}},
volume = {9},
year = {2016}
}

@article{Kirby2023,
  doi = {10.48550/ARXIV.2301.01699},
  
  url = {https://arxiv.org/abs/2301.01699},
  
  author = {Kirby, Andrew and Briol, François-Xavier and Dunstan, Thomas D. and Nishino, Takafumi},
  
  keywords = {Fluid Dynamics (physics.flu-dyn), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Data-driven modelling of turbine wake interactions and flow resistance in large wind farms},
  
  journal = {arXiv:2301.01699 },
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Nishino2016,
abstract = {A new theoretical approach is proposed to predict a practical upper limit to the efficiency of a very large wind farm. The new theory suggests that the efficiency of ideal turbines in an ideal very large wind farm depends primarily on a non-dimensional parameter $\lambda$/Cf0, where $\lambda$ is the ratio of the rotor swept area to the land area (for each turbine) and Cf0 is a natural friction coefficient observed before constructing the farm. When X/Cf approaches to zero, the new theory goes back to the classical actuator disc theory, yielding the well-known Betz limit. When $\lambda$/Cf0 increases to a large value, the maximum power coefficient of each turbine reduces whilst a normalised power density of the farm increases asymptotically to an upper limit. A CFD analysis of an infinitely large wind farm with 'aligned' and 'displaced' array configurations is also presented to validate a key assumption used in the new theory.},
author = {Nishino, T.},
file = {:Users/fxbriol/Docs/Work/Papers/Applications/Wind Turbines/(2016, Nishino) Two-scale momentum theory for very large wind farms.pdf:pdf},
journal = {Journal of Physics: Conference Series},
number = {3},
title = {{Two-scale momentum theory for very large wind farms}},
volume = {753},
year = {2016}
}

@article{Gutmann2016,
  author  = {Michael U. Gutmann and Jukka Corander},
  title   = {Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {125},
  pages   = {1--47},
  url     = {http://jmlr.org/papers/v17/15-017.html}
}

@InProceedings{Greenberg2019,
  title = 	 {Automatic Posterior Transformation for Likelihood-Free Inference},
  author =       {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
  booktitle = 	 {Proceedings of the International Conference on Machine Learning},
  pages = 	 {2404--2414},
  year = 	 {2019},
  OPTeditor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  OPTvolume = 	 {97},
  OPTseries = 	 {Proceedings of Machine Learning Research},
  OPTmonth = 	 {09--15 Jun},
  OPTpublisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/greenberg19a.html},
  abstract = 	 {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.}
}

@Article{Behrens2015,
  author    = {J. Behrens and F. Dias},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {New computational methods in tsunami science},
  year      = {2015},
  month     = {oct},
  number    = {2053},
  pages     = {20140382},
  volume    = {373},
  doi       = {10.1098/rsta.2014.0382},
  publisher = {The Royal Society},
}

@article{devore1993besov,
  title={Besov spaces on domains in $\mathbb{R}^d$},
  author={DeVore, Ronald A and Sharpley, Robert C},
  journal={Transactions of the American Mathematical Society},
  volume={335},
  number={2},
  pages={843--864},
  year={1993}
}

@book{stein1970singular,
  title={Singular integrals and differentiability properties of functions},
  author={Stein, Elias M},
  volume={2},
  year={1970},
  publisher={Princeton university press}
}

@article {DREAM,
    title = {DREAM: A fluid-kinetic framework for tokamak disruption runaway electron simulations},
    journal = {Computer Physics Communications},
    volume = {268},
    pages = {108098},
    year = {2021},
    issn = {0010-4655},
    doi = {10.1016/j.cpc.2021.108098},
    url = {https://doi.org/10.1016/j.cpc.2021.108098},
    author = {Mathias Hoppe and Ola Embreus and Tünde Fülöp}
}

@article{Garreau2018,
  author = {Garreau, D. and Jitkrittum, W. and Kanagawa, M.},
  title = {Large sample analysis of the median heuristic},
  journal = {arXiv:1707.07269},
  year = {2017}
}

@Article{Catana2014,
  author    = {Catalin Catana and Theodore Kypraios and M{\u{a}}d{\u{a}}lin Gu{\c{t}}{\u{a}}},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  title     = {Maximum likelihood versus likelihood-free quantum system identification in the atom maser},
  year      = {2014},
  month     = {sep},
  number    = {41},
  pages     = {415302},
  volume    = {47},
  doi       = {10.1088/1751-8113/47/41/415302},
  publisher = {{IOP} Publishing},
}


@article{Teckentrup2020,
abstract = {This work is concerned with the convergence of Gaussian process regression. A particular focus is on hierarchical Gaussian process regression, where hyper-parameters appearing in the mean and covariance structure of the Gaussian process emulator are a-priori unknown and are learned from the data, along with the posterior mean and covariance. We work in the framework of empirical Bayes, where a point estimate of the hyper-parameters is computed, using the data, and then used within the standard Gaussian process prior to posterior update. We provide a convergence analysis that (i) holds for a given, deterministic function f to be emulated, and (ii) shows that convergence of Gaussian process regression is unaffected by the additional learning of hyper-parameters from data and is guaranteed in a wide range of scenarios. As the primary motivation for the work is the use of Gaussian process regression to approximate the data likelihood in Bayesian inverse problems, we provide a bound on the error introduced in the Bayesian posterior distribution in this context.},
author = {Teckentrup, Aretha L.},
file = {:Users/fxbriol/Docs/Work/Papers/Kernel Methods/Gaussian Processes/(2019, Teckentrup) Convergence of Gaussian Process Regression With Estimated Hyper-parameters and Applications in Bayesian Inverse Problems.pdf:pdf},
journal = {SIAM-ASA Journal on Uncertainty Quantification},
keywords = {Bayesian inference,Empirical Bayes,Gaussian process regression,Hierarchical,Inverse problem,Posterior consistency,Surrogate model},
number = {4},
pages = {1310--1337},
title = {{Convergence of Gaussian process regression with estimated hyper-parameters and applications in Bayesian inverse problems}},
volume = {8},
year = {2020}
}


@inproceedings{Genevay2018,
abstract = {Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on \emph{Sinkhorn divergences} (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength $\varepsilon$, between OT ($\varepsilon=0$) and MMD ($\varepsilon=\infty$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively $O(n^3\log n)$, $O(n^2)$ and $n^2$ operations given a sample size $n$), much less is known in terms of their \emph{sample complexity}, namely the gap between these quantities, when evaluated using finite samples \emph{vs.} their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, $1/n^{1/d}$ for OT in dimension $d$ and $1/\sqrt{n}$ for MMD, that for SDs has only been studied empirically. In this paper, we \emph{(i)} derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer $\varepsilon$, \emph{(ii)} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and \emph{(iii)} provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in $1/\sqrt{n}$ (as in MMD), with a constant that depends however on $\varepsilon$, making the bridge between OT and MMD complete.},
archivePrefix = {arXiv},
arxivId = {1810.02733},
author = {Genevay, A. and Chizat, L. and Bach, F. and Cuturi, M. and Peyr{\'{e}}, G.},
booktitle = {International Conference on Artificial Intelligence and Statistics},
eprint = {1810.02733},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Genevay et al. - 2019 - Sample complexity of Sinkhorn divergences.pdf:pdf},
title = {{Sample complexity of Sinkhorn divergences}},
year = {2019}
}


@book{Rasmussen2006,
author = {Rasmussen, C. and Williams, C.},
file = {:Users/fxbriol/Library/Application Support/Mendeley Desktop/Downloaded/Rasmussen, Williams - 2006 - Gaussian Processes for Machine Learning.pdf:pdf},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2006}
}

@article{Constantine1996,
 author = {Constantine, G. M. and Savits, Thomas H.},
 year = {1996},
 title = {A multivariate Faa di Bruno formula with applications},
 pages = {503--520},
 volume = {348},
 number = {2},
 journal = {Transactions of the American Mathematical Society},
}

@book{evans2018measure,
  title={Measure theory and fine properties of functions},
  author={Evans, Lawrence C and Garzepy, Ronald F},
  year={2018},
  publisher={Routledge}
}

@ARTICLE{SciPy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{aronszajn1950theory,
  title={Theory of reproducing kernels},
  author={Aronszajn, Nachman},
  journal={Transactions of the American mathematical society},
  volume={68},
  number={3},
  pages={337--404},
  year={1950}
}

@Book{Verbeek2018,
  author    = {Verbeek, Marno},
  publisher = {Wiley},
  title     = {A Guide to Modern Econometrics, Fifth Edition},
  year      = {2018},
  isbn      = {1119401151},
  month     = may,
  ean       = {9781119401155},
  pagetotal = {522},
  url       = {https://www.ebook.de/de/product/41311245/marno_verbeek_a_guide_to_modern_econometrics_fifth_edition.html},
}

@Article{Jagadeeswaran2019,
  author    = {R. Jagadeeswaran and Fred J. Hickernell},
  journal   = {Statistics and Computing},
  title     = {Fast automatic Bayesian cubature using lattice sampling},
  year      = {2019},
  month     = {sep},
  number    = {6},
  pages     = {1215--1229},
  volume    = {29},
  doi       = {10.1007/s11222-019-09895-9},
  publisher = {Springer Science and Business Media {LLC}},
}

@ARTICLE{Karvonen2019-kk,
  title     = "Symmetry exploits for Bayesian cubature methods",
  author    = "Karvonen, Toni and S{\"a}rkk{\"a}, Simo and Oates, Chris J",
  abstract  = "Abstract Bayesian cubature provides a flexible framework for
               numerical integration, in which a priori knowledge on the
               integrand can be encoded and exploited. This additional
               flexibility, compared to many classical cubature methods, comes
               at a computational cost which is cubic in the number of
               evaluations of the integrand. It has been recently observed that
               fully symmetric point sets can be exploited in order to
               reduce---in some cases substantially---the computational cost of
               the standard Bayesian cubature method. This work identifies
               several additional symmetry exploits within the Bayesian
               cubature framework. In particular, we go beyond earlier work in
               considering non-symmetric measures and, in addition to the
               standard Bayesian cubature method, present exploits for the
               Bayes--Sard cubature method and the multi-output Bayesian
               cubature method.",
  journal   = "Stat. Comput.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  29,
  number    =  6,
  pages     = "1231--1248",
  month     =  nov,
  year      =  2019,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@ARTICLE{Karvonen2019-mu,
  title     = "Gaussian kernel quadrature at scaled {Gauss--Hermite} nodes",
  author    = "Karvonen, Toni and S{\"a}rkk{\"a}, Simo",
  journal   = "BIT",
  publisher = "Springer Science and Business Media LLC",
  volume    =  59,
  number    =  4,
  pages     = "877--902",
  month     =  dec,
  year      =  2019,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}