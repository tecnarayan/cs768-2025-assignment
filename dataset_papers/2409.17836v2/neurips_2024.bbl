\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida, J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and Vojnovic]{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and encoding.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov, Khirirat, and Renggli]{alistarh2018convergence}
D.~Alistarh, T.~Hoefler, M.~Johansson, N.~Konstantinov, S.~Khirirat, and C.~Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Baziotis et~al.(2020)Baziotis, Haddow, and Birch]{baziotis2020language}
C.~Baziotis, B.~Haddow, and A.~Birch.
\newblock Language model prior for low-resource neural machine translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2020.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and Anandkumar]{bernstein2018signsgd}
J.~Bernstein, Y.-X. Wang, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Boutell(1997)]{boutell1997png}
T.~Boutell.
\newblock Png (portable network graphics) specification version 1.0.
\newblock Technical report, 1997.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Chang et~al.(2019)Chang, Wang, Peng, and Chiu]{chang2019all}
W.-L. Chang, H.-P. Wang, W.-H. Peng, and W.-C. Chiu.
\newblock All about structure: Adapting structural information across domains for boosting semantic segmentation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Coalson(2008)]{coalson2008flac}
J.~Coalson.
\newblock Free lossless audio codec, 2008.
\newblock URL \url{https://xiph.org/flac}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Deletang et~al.(2024)Deletang, Ruoss, Duquenne, Catt, Genewein, Mattern, Grau-Moya, Wenliang, Aitchison, Orseau, et~al.]{deletang2024language}
G.~Deletang, A.~Ruoss, P.-A. Duquenne, E.~Catt, T.~Genewein, C.~Mattern, J.~Grau-Moya, L.~K. Wenliang, M.~Aitchison, L.~Orseau, et~al.
\newblock Language modeling is compression.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Deutsch(1996)]{deutsch1996gzip}
P.~Deutsch.
\newblock Gzip file format specification version 4.3.
\newblock RFC, 1996.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2023gptq}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Gandelsman et~al.(2019)Gandelsman, Shocher, and Irani]{gandelsman2019double}
Y.~Gandelsman, A.~Shocher, and M.~Irani.
\newblock " double-dip": unsupervised image decomposition via coupled deep-image-priors.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Geng and Liu(2023)]{openlm2023openllama}
X.~Geng and H.~Liu.
\newblock Openllama: An open reproduction of llama, May 2023.
\newblock URL \url{https://github.com/openlm-research/open_llama}.

\bibitem[Gruver et~al.(2024)Gruver, Finzi, Qiu, and Wilson]{gruver2024large}
N.~Gruver, M.~Finzi, S.~Qiu, and A.~G. Wilson.
\newblock Large language models are zero-shot time series forecasters.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2024.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{ha2016hypernetworks}
D.~Ha, A.~Dai, and Q.~V. Le.
\newblock Hypernetworks.
\newblock \emph{arXiv preprint arXiv:1609.09106}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem[He et~al.(2020)He, Wang, Zenk, and Fritz]{he2020cossgd}
Y.~He, H.-P. Wang, M.~Zenk, and M.~Fritz.
\newblock Cossgd: Communication-efficient federated learning with a simple cosine-based quantization.
\newblock \emph{arXiv preprint arXiv:2012.08241}, 2020.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Hooper et~al.(2024)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer, and Gholami]{hooper2024kvquant}
C.~Hooper, S.~Kim, H.~Mohammadzadeh, M.~W. Mahoney, Y.~S. Shao, K.~Keutzer, and A.~Gholami.
\newblock Kvquant: Towards 10 million context length llm inference with kv cache quantization.
\newblock \emph{arXiv preprint arXiv:2401.18079}, 2024.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister]{hsieh2023distilling}
C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H.~Nakhost, Y.~Fujii, A.~Ratner, R.~Krishna, C.-Y. Lee, and T.~Pfister.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.
\newblock \emph{arXiv preprint arXiv:2305.02301}, 2023.

\bibitem[Jelinek(1998)]{jelinek1998statistical}
F.~Jelinek.
\newblock \emph{Statistical methods for speech recognition}.
\newblock MIT press, 1998.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le and Yang(2015)]{le2015tiny}
Y.~Le and X.~Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Lindstrom and Isenburg(2006)]{lindstrom2006fast}
P.~Lindstrom and M.~Isenburg.
\newblock Fast and efficient compression of floating-point data.
\newblock \emph{IEEE transactions on visualization and computer graphics}, 2006.

\bibitem[Ma et~al.(2023)Ma, Fang, and Wang]{ma2023llm}
X.~Ma, G.~Fang, and X.~Wang.
\newblock Llm-pruner: On the structural pruning of large language models.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Ichter, Driess, Arenas, Rao, Sadigh, and Zeng]{mirchandani2023large}
S.~Mirchandani, F.~Xia, P.~Florence, B.~Ichter, D.~Driess, M.~G. Arenas, K.~Rao, D.~Sadigh, and A.~Zeng.
\newblock Large language models as general pattern machines.
\newblock In \emph{Conference on Robot Learning}, 2023.

\bibitem[Pavlov(2019)]{igor20197z}
I.~Pavlov.
\newblock 7z format, 2019.
\newblock URL \url{http://www.7-zip.org/7z.html}.

\bibitem[Rissanen and Langdon(1979)]{rissanen1979arithmetic}
J.~Rissanen and G.~G. Langdon.
\newblock Arithmetic coding.
\newblock \emph{IBM Journal of research and development}, 23\penalty0 (2):\penalty0 149--162, 1979.

\bibitem[Sayood(2017)]{sayood2017introduction}
K.~Sayood.
\newblock \emph{Introduction to data compression}.
\newblock Morgan Kaufmann, 2017.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015neural}
R.~Sennrich, B.~Haddow, and A.~Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem[Shannon(2001)]{shannon2001mathematical}
C.~E. Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{ACM SIGMOBILE mobile computing and communications review}, 5\penalty0 (1):\penalty0 3--55, 2001.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2015very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2015.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Ulyanov et~al.(2018)Ulyanov, Vedaldi, and Lempitsky]{ulyanov2018deep}
D.~Ulyanov, A.~Vedaldi, and V.~Lempitsky.
\newblock Deep image prior.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Wang et~al.(2021)Wang, Yu, and Fritz]{wang2021hijack}
H.-P. Wang, N.~Yu, and M.~Fritz.
\newblock Hijack-gan: Unintended-use of pretrained, black-box gans.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem[Wang et~al.(2022)Wang, Stich, He, and Fritz]{wang2022progfed}
H.-P. Wang, S.~Stich, Y.~He, and M.~Fritz.
\newblock Progfed: effective, communication, and computation efficient federated learning by progressive training.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Chen, Kerkouche, and Fritz]{wang2024fedlap}
H.-P. Wang, D.~Chen, R.~Kerkouche, and M.~Fritz.
\newblock Fedlap-dp: Federated learning by sharing differentially private loss approximations.
\newblock \emph{Proceedings on Privacy Enhancing Technologies (PoPETs)}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Xu, Zhou, Zang, Darrell, Liu, and You]{wang2024neural}
K.~Wang, Z.~Xu, Y.~Zhou, Z.~Zang, T.~Darrell, Z.~Liu, and Y.~You.
\newblock Neural network diffusion.
\newblock \emph{arXiv preprint arXiv:2402.13144}, 2024{\natexlab{b}}.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed optimization.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Xiong et~al.(2023)Xiong, Wang, Cheng, Yu, and Hsieh]{xiong2023feddm}
Y.~Xiong, R.~Wang, M.~Cheng, F.~Yu, and C.-J. Hsieh.
\newblock Feddm: Iterative distribution matching for communication-efficient federated learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu]{zhang2024tinyllama}
P.~Zhang, G.~Zeng, T.~Wang, and W.~Lu.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{arXiv preprint arXiv:2401.02385}, 2024.

\end{thebibliography}
