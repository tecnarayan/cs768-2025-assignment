\begin{thebibliography}{}

\bibitem[Alistarh et~al., 2017]{quantized-sgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017).
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 30}, pages 1709--1720. Curran Associates, Inc.

\bibitem[Asi et~al., 2021]{asi2021private}
Asi, H., Duchi, J., Fallah, A., Javidbakht, O., and Talwar, K. (2021).
\newblock Private adaptive gradient methods for convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  383--392. PMLR.

\bibitem[Banerjee et~al., 2005]{banerjee2005clustering}
Banerjee, A., Merugu, S., Dhillon, I.~S., and Ghosh, J. (2005).
\newblock Clustering with bregman divergences.
\newblock {\em Journal of machine learning research}, 6(10).

\bibitem[Barndorff-Nielsen, 2014]{barndorff2014information}
Barndorff-Nielsen, O. (2014).
\newblock {\em Information and exponential families: in statistical theory}.
\newblock John Wiley \& Sons.

\bibitem[Bassily et~al., 2020]{bassily2020stability}
Bassily, R., Feldman, V., Guzm{\'a}n, C., and Talwar, K. (2020).
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Bassily et~al., 2019]{bassily2019private}
Bassily, R., Feldman, V., Talwar, K., and Guha~Thakurta, A. (2019).
\newblock Private stochastic convex optimization with optimal rates.
\newblock {\em Advances in neural information processing systems}.

\bibitem[Bassily et~al., 2014]{bassily2014private}
Bassily, R., Smith, A., and Thakurta, A. (2014).
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em 2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, pages 464--473. IEEE.

\bibitem[Bernstein et~al., 2018a]{bernstein2018signsgd}
Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A. (2018a).
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR.

\bibitem[Bernstein et~al., 2018b]{bernstein2018signsgdnoise}
Bernstein, J., Zhao, J., Azizzadenesheli, K., and Anandkumar, A. (2018b).
\newblock signsgd with majority vote is communication efficient and fault
  tolerant.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Boucheron et~al., 2013]{boucheron2013concentration}
Boucheron, S., Lugosi, G., and Massart, P. (2013).
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press.

\bibitem[Bousquet and Elisseeff, 2002]{bousquet2002stability}
Bousquet, O. and Elisseeff, A. (2002).
\newblock Stability and generalization.
\newblock {\em Journal of Machine Learning Research}, 2:499--526.

\bibitem[Bousquet et~al., 2020]{bousquet2020sharper}
Bousquet, O., Klochkov, Y., and Zhivotovskiy, N. (2020).
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In {\em Conference on Learning Theory}, pages 610--626. PMLR.

\bibitem[Brown, 1986]{brown1986fundamentals}
Brown, L.~D. (1986).
\newblock Fundamentals of statistical exponential families: with applications
  in statistical decision theory.
\newblock Ims.

\bibitem[Bu et~al., 2019]{bu2019tightening}
Bu, Y., Zou, S., and Veeravalli, V.~V. (2019).
\newblock Tightening mutual information based bounds on generalization error.
\newblock In {\em 2019 IEEE International Symposium on Information Theory
  (ISIT)}, pages 587--591. IEEE.

\bibitem[Bun et~al., 2018]{bun2018composable}
Bun, M., Dwork, C., Rothblum, G.~N., and Steinke, T. (2018).
\newblock Composable and versatile privacy via truncated cdp.
\newblock In {\em Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 74--86.

\bibitem[Canonne et~al., 2020]{canonne2020discrete}
Canonne, C.~L., Kamath, G., and Steinke, T. (2020).
\newblock The discrete gaussian for differential privacy.
\newblock In {\em NeurIPS}.

\bibitem[Chen et~al., 2019]{chen2019distributed}
Chen, X., Chen, T., Sun, H., Wu, Z.~S., and Hong, M. (2019).
\newblock Distributed training with heterogeneous data: Bridging median-and
  mean-based algorithms.
\newblock {\em arXiv preprint arXiv:1906.01736}.

\bibitem[Damian et~al., 2021]{damian2021label}
Damian, A., Ma, T., and Lee, J. (2021).
\newblock Label noise sgd provably prefers flat global minimizers.
\newblock {\em arXiv preprint arXiv:2106.06530}.

\bibitem[Devroye and Wagner, 1979]{devroye1979distribution}
Devroye, L. and Wagner, T. (1979).
\newblock Distribution-free inequalities for the deleted and holdout error
  estimates.
\newblock {\em IEEE Transactions on Information Theory}, 25(2):202--207.

\bibitem[Elisseeff et~al., 2005]{elisseeff2005stability}
Elisseeff, A., Evgeniou, T., Pontil, M., and Kaelbing, L.~P. (2005).
\newblock Stability of randomized learning algorithms.
\newblock {\em Journal of Machine Learning Research}, 6(1).

\bibitem[Farghly and Rebeschini, 2021]{farghly2021timeindependent}
Farghly, T. and Rebeschini, P. (2021).
\newblock Time-independent generalization bounds for {SGLD} in non-convex
  settings.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.,
  editors, {\em Advances in Neural Information Processing Systems}.

\bibitem[Feldman and Vondrak, 2018]{feldman2018generalization}
Feldman, V. and Vondrak, J. (2018).
\newblock Generalization bounds for uniformly stable algorithms.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 9770--9780.

\bibitem[Feldman and Vondrak, 2019]{feldman2019high}
Feldman, V. and Vondrak, J. (2019).
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In {\em Conference on Learning Theory}, pages 1270--1279. PMLR.

\bibitem[Gr{\"u}nwald et~al., 2021]{grunwald2021pac}
Gr{\"u}nwald, P., Steinke, T., and Zakynthinou, L. (2021).
\newblock Pac-bayes, mac-bayes and conditional mutual information: Fast rate
  bounds that handle general vc classes.
\newblock {\em arXiv preprint arXiv:2106.09683}.

\bibitem[Haghifam et~al., 2020]{haghifam2020sharpened}
Haghifam, M., Negrea, J., Khisti, A., Roy, D.~M., and Dziugaite, G.~K. (2020).
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Hardt et~al., 2016]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y. (2016).
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em International Conference on Machine Learning}, pages
  1225--1234.

\bibitem[Hellstr{\"o}m and Durisi, 2020]{hellstrom2020generalization}
Hellstr{\"o}m, F. and Durisi, G. (2020).
\newblock Generalization bounds via information density and conditional
  information density.
\newblock {\em IEEE Journal on Selected Areas in Information Theory},
  1(3):824--839.

\bibitem[Hellstr{\"o}m and Durisi, 2021]{hellstrom2021fast}
Hellstr{\"o}m, F. and Durisi, G. (2021).
\newblock Fast-rate loss bounds via conditional information measures with
  applications to neural networks.
\newblock In {\em 2021 IEEE International Symposium on Information Theory
  (ISIT)}, pages 952--957. IEEE.

\bibitem[Jiang and Agrawal, 2018]{jiang2018}
Jiang, P. and Agrawal, G. (2018).
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 31}, pages 2525--2536. Curran Associates, Inc.

\bibitem[Jin et~al., 2017]{jin2017escape}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I. (2017).
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732.

\bibitem[Jin et~al., 2019]{jin2019nonconvex}
Jin, C., Netrapalli, P., Ge, R., Kakade, S.~M., and Jordan, M.~I. (2019).
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock {\em arXiv preprint arXiv:1902.04811}.

\bibitem[Jin et~al., 2020]{jin2020stochastic}
Jin, R., Huang, Y., He, X., Wu, T., and Dai, H. (2020).
\newblock Stochastic-sign sgd for federated learning with theoretical
  guarantees.
\newblock {\em arXiv preprint arXiv:2002.10940}.

\bibitem[Kairouz et~al., 2020]{kairouz2020dimension}
Kairouz, P., Ribero, M., Rush, K., and Thakurta, A. (2020).
\newblock Dimension independence in unconstrained private erm via adaptive
  preconditioning.
\newblock {\em arXiv preprint arXiv:2008.06570}.

\bibitem[Krizhevsky, 2009]{cifar10}
Krizhevsky, A. (2009).
\newblock Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}.
\newblock Technical Report Vol. 1. No. 4., {University of Toronto}.

\bibitem[LeCun et~al., 1998]{mnist}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[Lei and Ying, 2020]{Lei2020}
Lei, Y. and Ying, Y. (2020).
\newblock Fine-grained analysis of stability and generalization for stochastic
  gradient descent.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Li et~al., 2019]{li2019connecting}
Li, B., Chen, C., Liu, H., and Carin, L. (2019).
\newblock On connecting stochastic gradient mcmc and differential privacy.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 557--566. PMLR.

\bibitem[Li et~al., 2020]{Li2020On}
Li, J., Luo, X., and Qiao, M. (2020).
\newblock On generalization error bounds of noisy gradient methods for
  non-convex learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Mou et~al., 2018]{mou2018generalization}
Mou, W., Wang, L., Zhai, X., and Zheng, K. (2018).
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock In {\em Conference on Learning Theory}, pages 605--638. PMLR.

\bibitem[Negrea et~al., 2019]{negrea2019information}
Negrea, J., Haghifam, M., Dziugaite, G.~K., Khisti, A., and Roy, D.~M. (2019).
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Neu et~al., 2021]{neu2021informationtheoretic}
Neu, G., Dziugaite, G.~K., Haghifam, M., and Roy, D.~M. (2021).
\newblock Information-theoretic generalization bounds for stochastic gradient
  descent.
\newblock In {\em COLT}.

\bibitem[Pensia et~al., 2018]{pensia2018generalization}
Pensia, A., Jog, V., and Loh, P.-L. (2018).
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In {\em 2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 546--550. IEEE.

\bibitem[Pollard, 2002]{pollard2002user}
Pollard, D. (2002).
\newblock {\em A user's guide to measure theoretic probability}.
\newblock Number~8. Cambridge University Press.

\bibitem[Raginsky et~al., 2017]{raginsky2017non}
Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017).
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In {\em Conference on Learning Theory}, pages 1674--1703. PMLR.

\bibitem[Rodr{\'\i}guez-G{\'a}lvez et~al., 2021]{rodriguez2021random}
Rodr{\'\i}guez-G{\'a}lvez, B., Bassi, G., Thobaben, R., and Skoglund, M.
  (2021).
\newblock On random subset generalization error bounds and the stochastic
  gradient langevin dynamics algorithm.
\newblock In {\em 2020 IEEE Information Theory Workshop (ITW)}, pages 1--5.
  IEEE.

\bibitem[Rogers and Wagner, 1978]{rogers1978finite}
Rogers, W.~H. and Wagner, T.~J. (1978).
\newblock A finite sample distribution-free performance bound for local
  discrimination rules.
\newblock {\em The Annals of Statistics}, pages 506--514.

\bibitem[Russo and Zou, 2016]{russo2016controlling}
Russo, D. and Zou, J. (2016).
\newblock Controlling bias in adaptive data analysis using information theory.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1232--1240.
  PMLR.

\bibitem[Sason and Verdu, 2016]{save16}
Sason, I. and Verdu, S. (2016).
\newblock $f$-divergence inequalities.
\newblock {\em IEEE Transactions on Information Theory}, 62.

\bibitem[Shalev-Shwartz et~al., 2009]{ShalevShwartz2009StochasticCO}
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. (2009).
\newblock Stochastic convex optimization.
\newblock In {\em COLT}.

\bibitem[Shamir and Zhang, 2013]{shamir2013stochastic}
Shamir, O. and Zhang, T. (2013).
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In {\em International conference on machine learning}, pages 71--79.
  PMLR.

\bibitem[Steinke and Zakynthinou, 2020]{steinke2020reasoning}
Steinke, T. and Zakynthinou, L. (2020).
\newblock Reasoning about generalization via conditional mutual information.
\newblock In {\em Conference on Learning Theory}, pages 3437--3452. PMLR.

\bibitem[Tsybakov, 2008]{tsybakov2008introduction}
Tsybakov, A.~B. (2008).
\newblock {\em Introduction to nonparametric estimation}.
\newblock Springer Science \& Business Media.

\bibitem[Wainwright and Jordan, 2008]{wainwright2008graphical}
Wainwright, M.~J. and Jordan, M.~I. (2008).
\newblock {\em Graphical models, exponential families, and variational
  inference}.
\newblock Now Publishers Inc.

\bibitem[Wang et~al., 2021a]{wang2021optimizing}
Wang, B., Zhang, H., Zhang, J., Meng, Q., Chen, W., and Liu, T.-Y. (2021a).
\newblock Optimizing information-theoretical generalization bound via
  anisotropic noise of {SGLD}.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.,
  editors, {\em Advances in Neural Information Processing Systems}.

\bibitem[Wang and Xu, 2019]{wang2019differentially}
Wang, D. and Xu, J. (2019).
\newblock Differentially private empirical risk minimization with smooth
  non-convex loss functions: A non-stationary view.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 1182--1189.

\bibitem[Wang et~al., 2021b]{wang2021analyzing}
Wang, H., Huang, Y., Gao, R., and Calmon, F. (2021b).
\newblock Analyzing the generalization capability of sgld using properties of
  gaussian channels.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Wang et~al., 2015]{wang2015privacy}
Wang, Y.-X., Fienberg, S., and Smola, A. (2015).
\newblock Privacy for free: Posterior sampling and stochastic gradient monte
  carlo.
\newblock In {\em International Conference on Machine Learning}, pages
  2493--2502. PMLR.

\bibitem[Welling and Teh, 2011]{WellingT11}
Welling, M. and Teh, Y.~W. (2011).
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em International Conference on Machine Learning}, ICML '11,
  pages 681--688.

\bibitem[Xiao et~al., 2017]{fashion-mnist}
Xiao, H., Rasul, K., and Vollgraf, R. (2017).
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.

\bibitem[Xu and Raginsky, 2017]{xu2017information}
Xu, A. and Raginsky, M. (2017).
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock {\em Advances in Neural Information Processing Systems},
  2017:2525--2534.

\bibitem[Yang et~al., 2019]{yang2019swalp}
Yang, G., Zhang, T., Kirichenko, P., Bai, J., Wilson, A.~G., and De~Sa, C.
  (2019).
\newblock Swalp: Stochastic weight averaging in low-precision training.
\newblock {\em 36th International Conference on Machine Learning (ICML)}.

\bibitem[Zhang et~al., 2017]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017).
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net.

\bibitem[Zhang et~al., 2021]{zhang2021wide}
Zhang, H., Mironov, I., and Hejazinia, M. (2021).
\newblock Wide network learning with differential privacy.
\newblock {\em arXiv preprint arXiv:2103.01294}.

\bibitem[Zhou et~al., 2021]{zhou2021individually}
Zhou, R., Tian, C., and Liu, T. (2021).
\newblock Individually conditional individual mutual information bound on
  generalization error.
\newblock In {\em 2021 IEEE International Symposium on Information Theory
  (ISIT)}, pages 670--675. IEEE.

\bibitem[Zhou et~al., 2020]{zhou2020bypassing}
Zhou, Y., Wu, S., and Banerjee, A. (2020).
\newblock Bypassing the ambient dimension: Private sgd with gradient subspace
  identification.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}
