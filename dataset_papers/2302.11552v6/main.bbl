\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Besag(1994)]{besag1994comments}
Julian Besag.
\newblock {Comments on “Representations of knowledge in complex systems” by
  U. Grenander and MI Miller}.
\newblock \emph{J. Roy. Statist. Soc. Ser. B}, 56\penalty0 (591-592):\penalty0
  4, 1994.

\bibitem[Brock et~al.(2018)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock \emph{arXiv preprint arXiv:1809.11096}, 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Burda et~al.(2015)Burda, Grosse, and Salakhutdinov]{burda2015accurate}
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.
\newblock Accurate and conservative estimates of mrf log-likelihood using
  reverse annealing.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  102--110.
  PMLR, 2015.

\bibitem[Chen \& Duvenaud(2019)Chen and Duvenaud]{chen2019neural}
Ricky~TQ Chen and David~K Duvenaud.
\newblock Neural networks with cheap differential operators.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander~Quinn Nichol.
\newblock Diffusion models beat {GAN}s on image synthesis.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and generalization in energy-based models.
\newblock \emph{arXiv preprint arXiv:1903.08689}, 2019.

\bibitem[Du et~al.(2020{\natexlab{a}})Du, Li, and
  Mordatch]{du2020compositional}
Yilun Du, Shuang Li, and Igor Mordatch.
\newblock Compositional visual generation with energy based models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6637--6647, 2020{\natexlab{a}}.

\bibitem[Du et~al.(2020{\natexlab{b}})Du, Li, Tenenbaum, and
  Mordatch]{du2020improved}
Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch.
\newblock Improved contrastive divergence training of energy based models.
\newblock \emph{arXiv preprint arXiv:2012.01316}, 2020{\natexlab{b}}.

\bibitem[Duane et~al.(1987)Duane, Kennedy, Pendleton, and
  Roweth]{duane1987hybrid}
Simon Duane, Anthony~D Kennedy, Brian~J Pendleton, and Duncan Roweth.
\newblock Hybrid {M}onte {C}arlo.
\newblock \emph{Physics letters B}, 195\penalty0 (2):\penalty0 216--222, 1987.

\bibitem[Durmus \& Moulines(2019)Durmus and Moulines]{durmus2019high}
Alain Durmus and Eric Moulines.
\newblock High-dimensional {B}ayesian inference via the unadjusted {L}angevin
  algorithm.
\newblock \emph{Bernoulli}, 25\penalty0 (4A):\penalty0 2854--2882, 2019.

\bibitem[Gao et~al.(2021)Gao, Song, Poole, Wu, and Kingma]{gao2021learning}
Ruiqi Gao, Yang Song, Ben Poole, Ying~Nian Wu, and Diederik~P Kingma.
\newblock Learning energy-based models by diffusion recovery likelihood.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=v_1Soh8QUNc}.

\bibitem[Geffner \& Domke(2021)Geffner and Domke]{geffner2021mcmc}
Tomas Geffner and Justin Domke.
\newblock {MCMC} variational inference via uncorrected {H}amiltonian annealing.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 639--651, 2021.

\bibitem[Geffner et~al.(2022)Geffner, Papamakarios, and Mnih]{geffner2022score}
Tomas Geffner, George Papamakarios, and Andriy Mnih.
\newblock Score modeling for simulation-based inference.
\newblock \emph{arXiv preprint arXiv:2209.14249}, 2022.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{grathwohl2019your}
Will Grathwohl, Kuan-Chieh Wang, J{\"o}rn-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock \emph{arXiv preprint arXiv:1912.03263}, 2019.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Kelly, Hashemi, Norouzi, Swersky, and
  Duvenaud]{grathwohl2021no}
Will Grathwohl, Jacob~Jin Kelly, Milad Hashemi, Mohammad Norouzi, Kevin
  Swersky, and David Duvenaud.
\newblock No {MCMC} for me: Amortized sampling for fast and stable training of
  energy-based models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local
  {N}ash equilibrium.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural Computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Ho \& Salimans(2022)Ho and Salimans]{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock \emph{arXiv preprint arXiv:2207.12598}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and
  Hinton]{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Johnson et~al.(2017)Johnson, Hariharan, Van Der~Maaten, Fei-Fei,
  Lawrence~Zitnick, and Girshick]{johnson2017clevr}
Justin Johnson, Bharath Hariharan, Laurens Van Der~Maaten, Li~Fei-Fei,
  C~Lawrence~Zitnick, and Ross Girshick.
\newblock Clevr: A diagnostic dataset for compositional language and elementary
  visual reasoning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  2901--2910, 2017.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond,
  Eccles, Keeling, Gimeno, Lago, et~al.]{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin~Dal Lago,
  et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{arXiv preprint arXiv:2203.07814}, 2022.

\bibitem[Liu et~al.(2021)Liu, Li, Du, Tenenbaum, and Torralba]{liu2021learning}
Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba.
\newblock Learning to compose visual relations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Liu et~al.(2022)Liu, Li, Du, Torralba, and
  Tenenbaum]{liu2022compositional}
Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua~B Tenenbaum.
\newblock Compositional visual generation with composable diffusion models.
\newblock \emph{arXiv preprint arXiv:2206.01714}, 2022.

\bibitem[Mayraz \& Hinton(2000)Mayraz and Hinton]{mayraz2000recognizing}
Guy Mayraz and Geoffrey~E Hinton.
\newblock Recognizing hand-written digits using hierarchical products of
  experts.
\newblock \emph{Advances in Neural Information Processing Systems}, 13, 2000.

\bibitem[Neal(1996)]{neal1996monte}
Radford~M Neal.
\newblock Monte {C}arlo implementation.
\newblock In \emph{Bayesian Learning for Neural Networks}, pp.\  55--98.
  Springer, 1996.

\bibitem[Neal(2001)]{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and Computing}, 11\penalty0 (2):\penalty0 125--139,
  2001.

\bibitem[Nijkamp et~al.(2020)Nijkamp, Hill, Han, Zhu, and
  Wu]{nijkamp2020anatomy}
Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying~Nian Wu.
\newblock On the anatomy of mcmc-based maximum likelihood learning of
  energy-based models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5272--5280, 2020.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Roberts \& Tweedie(1996)Roberts and Tweedie]{roberts1996exponential}
Gareth~O Roberts and Richard~L Tweedie.
\newblock Exponential convergence of {L}angevin distributions and their
  discrete approximations.
\newblock \emph{Bernoulli}, pp.\  341--363, 1996.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Ayan, Mahdavi, Lopes, et~al.]{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S~Sara Mahdavi,
  Rapha~Gontijo Lopes, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock \emph{arXiv preprint arXiv:2205.11487}, 2022.

\bibitem[Salimans \& Ho(2021)Salimans and Ho]{salimans2021should}
Tim Salimans and Jonathan Ho.
\newblock Should ebms model the energy or the score?
\newblock In \emph{Energy Based Models Workshop-ICLR 2021}, 2021.

\bibitem[Salimans \& Ho(2022)Salimans and Ho]{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2202.00512}, 2022.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2256--2265. PMLR, 2015.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wang et~al.(2021)Wang, Jiao, Xu, Wang, and Yang]{wang2021deep}
Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang.
\newblock Deep generative learning via schr{\"o}dinger bridge.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10794--10804. PMLR, 2021.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2635--2644. PMLR, 2016.

\bibitem[Xie et~al.(2021)Xie, Zheng, and Li]{xie2021learning}
Jianwen Xie, Zilong Zheng, and Ping Li.
\newblock Learning energy-based model with variational auto-encoder as
  amortized sampler.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  10441--10451, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\end{thebibliography}
