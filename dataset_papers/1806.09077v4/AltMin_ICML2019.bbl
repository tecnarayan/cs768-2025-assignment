\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Askari et~al.(2018)Askari, Negiar, Sambharya, and {El
  Ghaoui}]{Askari2018}
Askari, A., Negiar, G., Sambharya, R., and {El Ghaoui}, L.
\newblock Lifted neural networks.
\newblock arXiv:1805.01532 [cs.LG], 2018.

\bibitem[Balakrishnan et~al.(2017)Balakrishnan, Wainwright, and
  Yu]{balakrishnan2017}
Balakrishnan, S., Wainwright, M.~J., and Yu, B.
\newblock Statistical guarantees for the em algorithm: From population to
  sample-based analysis.
\newblock \emph{Ann. Statist.}, 45\penalty0 (1):\penalty0 77--120, 02 2017.
\newblock \doi{10.1214/16-AOS1435}.
\newblock URL \url{https://doi.org/10.1214/16-AOS1435}.

\bibitem[Bartunov et~al.(2018)Bartunov, Santoro, Richards, Marris, Hinton, and
  Lillicrap]{bartunov2018assessing}
Bartunov, S., Santoro, A., Richards, B., Marris, L., Hinton, G.~E., and
  Lillicrap, T.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9390--9400, 2018.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Bengio, Y., Simard, P., and Frasconi, P.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Carreira-Perpi\~{n}\'{a}n \& Wang(2014)Carreira-Perpi\~{n}\'{a}n and
  Wang]{carreira2014distributed}
Carreira-Perpi\~{n}\'{a}n, M. and Wang, W.
\newblock Distributed optimization of deeply nested systems.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  10--19, 2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem[Gotmare et~al.(2018)Gotmare, Thomas, Brea, and
  Jaggi]{gotmare2018decoupling}
Gotmare, A., Thomas, V., Brea, J., and Jaggi, M.
\newblock Decoupling backpropagation using constrained optimization methods.
\newblock \emph{Proc. of ICML 2018 Workshop on Credit Assignment in Deep
  Learning and Deep Reinforcement Learning}, 2018.

\bibitem[Guerguiev et~al.(2017)Guerguiev, Lillicrap, and
  Richards]{guerguiev2017towards}
Guerguiev, J., Lillicrap, T.~P., and Richards, B.~A.
\newblock Towards deep learning with segregated dendrites.
\newblock \emph{ELife}, 6:\penalty0 e22901, 2017.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krotov \& Hopfield(2019)Krotov and Hopfield]{krotov2019unsupervised}
Krotov, D. and Hopfield, J.~J.
\newblock Unsupervised learning by competing hidden units.
\newblock \emph{Proceedings of the National Academy of Sciences}, pp.\
  201820458, 2019.

\bibitem[Lange et~al.(2014)Lange, Z{\"{u}}hlke, Holz, and
  Villmann]{DBLP:conf/esann/LangeZHV14}
Lange, M., Z{\"{u}}hlke, D., Holz, O., and Villmann, T.
\newblock Applications of lp-norms and their smooth approximations for gradient
  based learning vector quantization.
\newblock In \emph{ESANN}, 2014.

\bibitem[Lau et~al.(2018)Lau, Zeng, Wu, and Yao]{lau2018proximal}
Lau, T. T.-K., Zeng, J., Wu, B., and Yao, Y.
\newblock A proximal block coordinate descent algorithm for deep neural network
  training.
\newblock \emph{arXiv preprint arXiv:1803.09082}, 2018.

\bibitem[Le et~al.(2011)Le, Ngiam, Coates, Lahiri, Prochnow, and
  Ng]{le2011optimization}
Le, Q.~V., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A.~Y.
\newblock On optimization methods for deep learning.
\newblock In \emph{Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, pp.\  265--272. Omnipress,
  2011.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{le2015simple}
Le, Q.~V., Jaitly, N., and Hinton, G.~E.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[LeCun(1986)]{le1986learning}
LeCun, Y.
\newblock Learning process in an asymmetric threshold network.
\newblock In \emph{Disordered systems and biological organization}, pp.\
  233--240. Springer, 1986.

\bibitem[LeCun(1987)]{yann1987modeles}
LeCun, Y.
\newblock \emph{Mod{\`e}les connexionnistes de l’apprentissage}.
\newblock PhD thesis, PhD thesis, These de Doctorat, Universite Paris 6, 1987.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[LeCun et~al.(1988)LeCun, Touresky, Hinton, and
  Sejnowski]{lecun1988theoretical}
LeCun, Y., Touresky, D., Hinton, G., and Sejnowski, T.
\newblock A theoretical framework for back-propagation.
\newblock In \emph{Proceedings of the 1988 connectionist models summer school},
  pp.\  21--28. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{LeCun1998}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee2015difference}
Lee, D.-H., Zhang, S., Fischer, A., and Bengio, Y.
\newblock Difference target propagation.
\newblock In \emph{{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}}, pp.\  498--515. Springer, 2015.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and Sapiro]{mairal2009online}
Mairal, J., Bach, F., Ponce, J., and Sapiro, G.
\newblock Online dictionary learning for sparse coding.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, 2009.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{nair2010rectified}
Nair, V. and Hinton, G.~E.
\newblock {Rectified linear units improve Restricted Boltzmann Machines}.
\newblock In \emph{Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pp.\  807--814, 2010.

\bibitem[Nesterov(2014)]{Nesterov:2014:ILC:2670022}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, Incorporated, 1 edition, 2014.
\newblock ISBN 1461346916, 9781461346913.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1310--1318, 2013.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Riedmiller \& Braun(1993)Riedmiller and Braun]{riedmiller1993direct}
Riedmiller, M. and Braun, H.
\newblock A direct adaptive method for faster backpropagation learning: The
  rprop algorithm.
\newblock In \emph{Neural Networks, 1993., IEEE International Conference on},
  pp.\  586--591. IEEE, 1993.

\bibitem[Ring(1994)]{ring1994continual}
Ring, M.~B.
\newblock \emph{Continual learning in reinforcement environments}.
\newblock PhD thesis, University of Texas at Austin Austin, Texas 78712, 1994.

\bibitem[Robbins \& Monro(1985)Robbins and Monro]{robbins1985stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock In \emph{Herbert Robbins Selected Papers}, pp.\  102--109. Springer,
  1985.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning representations by back-propagating errors.
\newblock \emph{nature}, 323\penalty0 (6088):\penalty0 533, 1986.

\bibitem[Sacramento et~al.(2018)Sacramento, Costa, Bengio, and
  Senn]{sacramento2018dendritic}
Sacramento, J., Costa, R.~P., Bengio, Y., and Senn, W.
\newblock Dendritic cortical microcircuits approximate the backpropagation
  algorithm.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8721--8732, 2018.

\bibitem[Schmidt et~al.(2007)Schmidt, Fung, and
  Rosales]{10.1007/978-3-540-74958-5_28}
Schmidt, M., Fung, G., and Rosales, R.
\newblock Fast optimization methods for l1 regularization: A comparative study
  and two new approaches.
\newblock In Kok, J.~N., Koronacki, J., Mantaras, R. L.~d., Matwin, S.,
  Mladeni{\v{c}}, D., and Skowron, A. (eds.), \emph{ECML}, 2007.

\bibitem[Taylor et~al.(2016)Taylor, Burmeister, Xu, Singh, Patel, and
  Goldstein]{taylor2016training}
Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., and Goldstein, T.
\newblock Training neural networks without gradients: A scalable admm approach.
\newblock In \emph{International conference on machine learning}, pp.\
  2722--2731, 2016.

\bibitem[Thomas~Frerix(2018)]{Frerix-et-al-18}
Thomas~Frerix, Thomas~Möllenhoff, M. M. D.~C.
\newblock Proximal backpropagation.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://arxiv.org/abs/1706.04638}.

\bibitem[Thrun(1995)]{thrun1995lifelong}
Thrun, S.
\newblock A lifelong learning perspective for mobile robot control.
\newblock In \emph{Intelligent Robots and Systems}, pp.\  201--214. Elsevier,
  1995.

\bibitem[Thrun(1998)]{thrun1998lifelong}
Thrun, S.
\newblock Lifelong learning algorithms.
\newblock In \emph{Learning to learn}, pp.\  181--209. Springer, 1998.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Whittington \& Bogacz(2019)Whittington and
  Bogacz]{whittington2019theories}
Whittington, J.~C. and Bogacz, R.
\newblock Theories of error back-propagation in the brain.
\newblock \emph{Trends in cognitive sciences}, 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Zeiler, M.~D.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zeng et~al.(2018)Zeng, Lau, Lin, and Yao]{zeng2018block}
Zeng, J., Lau, T. T.-K., Lin, S., and Yao, Y.
\newblock Global convergence in deep learning with variable splitting via the
  kurdyka-lojasiewicz property.
\newblock \emph{arXiv preprint arXiv:1803.00225}, 2018.

\bibitem[Zhang \& Kleijn(2017)Zhang and Kleijn]{Zhang2017}
Zhang, G. and Kleijn, W.~B.
\newblock Training deep neural networks via optimization over graphs.
\newblock arXiv:1702.03380 [cs.LG], 2017.

\bibitem[Zhang \& Brand(2017)Zhang and Brand]{zhang2017convergent}
Zhang, Z. and Brand, M.
\newblock {Convergent block coordinate descent for training Tikhonov
  regularized deep neural networks}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1719--1728, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Chen, and Saligrama]{zhang2016efficient}
Zhang, Z., Chen, Y., and Saligrama, V.
\newblock Efficient training of very deep neural networks for supervised
  hashing.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1487--1495, 2016.

\end{thebibliography}
