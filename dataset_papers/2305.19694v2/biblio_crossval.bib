@inproceedings{golovanov2019large,
  title={Large-scale transfer learning for natural language generation},
  author={Golovanov, Sergey and Kurbanov, Rauf and Nikolenko, Sergey and Truskovskyi, Kyryl and Tselousov, Alexander and Wolf, Thomas},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6053--6058},
  year={2019}
}
@inproceedings{andrews2016transfer,
  title={Transfer representation-learning for anomaly detection},
  author={Andrews, Jerone and Tanay, Thomas and Morton, Edward J and Griffin, Lewis D},
  year={2016},
  organization={JMLR}
}
@inproceedings{cho2019efficacy,
  title={On the efficacy of knowledge distillation},
  author={Cho, Jang Hyun and Hariharan, Bharath},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4794--4802},
  year={2019}
}
@inproceedings{shafahi2020adversarially,
  title={Adversarially robust transfer learning},
  author={Shafahi, Ali and Saadatpanah, Parsa and Zhu, Chen and Ghiasi, Amin and Studer, Christoph and Jacobs, David and Goldstein, Tom},
  booktitle={8th International Conference on Learning Representations (ICLR 2020)(virtual)},
  year={2020},
  organization={International Conference on Learning Representations}
}

@inproceedings{Denevi2020,
 author = {Denevi, Giulia and Pontil, Massimiliano and Ciliberto, Carlo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {964--974},
 publisher = {Curran Associates, Inc.},
 title = {The Advantage of Conditional Meta-Learning for Biased Regularization and Fine Tuning},
 volume = {33},
 year = {2020}
}@inproceedings{staerman2019functional,
  title={Functional isolation forest},
  author={Staerman, Guillaume and Mozharovskyi, Pavlo and Cl{\'e}men{\c{c}}on, Stephan and d’Alch{\'e}-Buc, Florence},
  booktitle={Asian Conference on Machine Learning},
  pages={332--347},
  year={2019},
  organization={PMLR}
}
@phdthesis{staerman2022functionalthese,
  title={Functional anomaly detection and robust estimation},
  author={Staerman, Guillaume},
  year={2022},
  school={Institut polytechnique de Paris}
}
@article{colombo2022learning,
  title={Learning disentangled textual representations via statistical measures of similarity},
  author={Colombo, Pierre and Staerman, Guillaume and Noiry, Nathan and Piantanida, Pablo},
  journal={arXiv preprint arXiv:2205.03589},
  year={2022}
}
@article{staerman2022fadin,
  title={FaDIn: Fast Discretized Inference for Hawkes Processes with General Parametric Kernels},
  author={Staerman, Guillaume and Allain, C{\'e}dric and Gramfort, Alexandre and Moreau, Thomas},
  journal={arXiv preprint arXiv:2210.04635},
  year={2022}
}
@article{colombo2022beyond,
  title={Beyond Mahalanobis Distance for Textual OOD Detection},
  author={Colombo, Pierre and Dadalto, Eduardo and Staerman, Guillaume and Noiry, Nathan and Piantanida, Pablo},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17744--17759},
  year={2022}
}
@article{campi2023ataxic,
  title={Ataxic speech disorders and Parkinson’s disease diagnostics via stochastic embedding of empirical mode decomposition},
  author={Campi, Marta and Peters, Gareth W and Toczydlowska, Dorota},
  journal={Plos one},
  volume={18},
  number={4},
  pages={e0284667},
  year={2023},
  publisher={Public Library of Science San Francisco, CA USA}
}
@article{chandola2009anomaly,
  title={Anomaly detection: A survey},
  author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  journal={ACM computing surveys (CSUR)},
  volume={41},
  number={3},
  pages={1--58},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{picothalfspace,
  title={A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection},
  author={Picot, Marine and Granese, Federica and Staerman, Guillaume and Romanelli, Marco and Messina, Francisco and Piantanida, Pablo and Colombo, Pierre},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@article{colombo2021automatic,
  title={Automatic text evaluation through the lens of Wasserstein barycenters},
  author={Colombo, Pierre and Staerman, Guillaume and Clavel, Chlo{\'e} and Piantanida, Pablo},
  journal={arXiv preprint arXiv:2108.12463},
  year={2021}
}
@article{darrin2023unsupervised,
  title={Unsupervised layer-wise score aggregation for textual ood detection},
  author={Darrin, Maxime and Staerman, Guillaume and Gomes, Eduardo Dadalto C{\^a}mara and Cheung, Jackie CK and Piantanida, Pablo and Colombo, Pierre},
  journal={arXiv preprint arXiv:2302.09852},
  year={2023}
}
@article{staerman2021pseudo,
  title={A pseudo-metric between probability distributions based on depth-trimmed regions},
  author={Staerman, Guillaume and Mozharovskyi, Pavlo and Colombo, Pierre and Cl{\'e}men{\c{c}}on, St{\'e}phan and d'Alch{\'e}-Buc, Florence},
  journal={arXiv preprint arXiv:2103.12711},
  year={2021}
}
@phdthesis{campi2022statistical,
  title={A Statistical Perspective of the Empirical Mode Decomposition},
  author={Campi, Marta},
  year={2022},
  school={UCL (University College London)}
}
@article{wiener2023cervical,
  title={Cervical vestibular evoked myogenic potentials in healthy children: Normative values for bone and air conduction},
  author={Wiener-Vacher, Sylvette R and Campi, Marta and Boizeau, Priscilla and Thai-Van, Hung},
  journal={Frontiers in Neurology},
  volume={14},
  year={2023},
  publisher={Frontiers Media SA}
}
@inproceedings{laforgue2021generalization,
  title={Generalization bounds in the presence of outliers: a median-of-means study},
  author={Laforgue, Pierre and Staerman, Guillaume and Cl{\'e}men{\c{c}}on, Stephan},
  booktitle={International Conference on Machine Learning},
  pages={5937--5947},
  year={2021},
  organization={PMLR}
}
@article{staerman2021affine,
  title={Affine-invariant integrated rank-weighted depth: Definition, properties and finite sample analysis},
  author={Staerman, Guillaume and Mozharovskyi, Pavlo and Cl{\'e}men{\c{c}}on, St{\'e}phan},
  journal={arXiv preprint arXiv:2106.11068},
  year={2021}
}
@inproceedings{staerman2021ot,
  title={When ot meets mom: Robust estimation of wasserstein distance},
  author={Staerman, Guillaume and Laforgue, Pierre and Mozharovskyi, Pavlo and d’Alch{\'e}-Buc, Florence},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={136--144},
  year={2021},
  organization={PMLR}
}
@article{staerman2022functional,
  title={Functional anomaly detection: a benchmark study},
  author={Staerman, Guillaume and Adjakossa, Eric and Mozharovskyi, Pavlo and Hofer, Vera and Sen Gupta, Jayant and Cl{\'e}men{\c{c}}on, Stephan},
  journal={International Journal of Data Science and Analytics},
  pages={1--17},
  year={2022},
  publisher={Springer}
}
@inproceedings{staerman2020area,
  title={The area of the convex hull of sampled curves: a robust functional statistical depth measure},
  author={Staerman, Guillaume and Mozharovskyi, Pavlo and Cl{\'e}men, St{\'e}phan and others},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={570--579},
  year={2020},
  organization={PMLR}
}
@article{campi2021machine,
  title={Machine learning mitigants for speech based cyber risk},
  author={Campi, Marta and Peters, Gareth W and Azzaoui, Nourddine and Matsui, Tomoko},
  journal={IEEE Access},
  volume={9},
  pages={136831--136860},
  year={2021},
  publisher={IEEE}
}
@InProceedings{denevi19a,
  title = 	 {Learning-to-Learn Stochastic Gradient Descent with Biased Regularization},
  author =       {Denevi, Giulia and Ciliberto, Carlo and Grazzi, Riccardo and Pontil, Massimiliano},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1566--1575},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR}
}

@InProceedings{balcan2019,
  title = 	 {Provable Guarantees for Gradient-Based Meta-Learning},
  author =       {Balcan, Maria-Florina and Khodak, Mikhail and Talwalkar, Ameet},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {424--433},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR}
}


@inproceedings{Khodak2019,
 author = {Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Gradient-Based Meta-Learning Methods},
 volume = {32},
 year = {2019}
}

@inproceedings{ruder2019transfer,
  title={Transfer learning in natural language processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: Tutorials},
  pages={15--18},
  year={2019}
}
@article{rifkin2003regularized,
  title={Regularized least-squares classification},
  author={Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso and others},
  journal={Nato Science Series Sub Series III Computer and Systems Sciences},
  volume={190},
  pages={131--154},
  year={2003},
  publisher={Citeseer}
}

@article{zhang2019vr,
  title={Vr-goggles for robots: Real-to-sim domain adaptation for visual control},
  author={Zhang, Jingwei and Tai, Lei and Yun, Peng and Xiong, Yufeng and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={2},
  pages={1148--1155},
  year={2019},
  publisher={IEEE}
}

@inproceedings{bousmalis2018using,
  title={Using simulation and domain adaptation to improve efficiency of deep robotic grasping},
  author={Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and others},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={4243--4250},
  year={2018},
  organization={IEEE}
}

@article{liu2019survey,
  title={A survey of sentiment analysis based on transfer learning},
  author={Liu, Ruijun and Shi, Yuqian and Ji, Changjiang and Jia, Ming},
  journal={IEEE Access},
  volume={7},
  pages={85401--85412},
  year={2019},
  publisher={IEEE}
}

@article{dredze2007frustratingly,
  title={Frustratingly hard domain adaptation for dependency parsing},
  author={Dredze, Mark and Blitzer, John and Talukdar, Partha Pratim and Ganchev, Kuzman and Gra{\c{c}}a, Joao V and Pereira, Fernando},
  year={2007}
}
 @inproceedings{blitzer2007biographies,
  title={Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification},
  author={Blitzer, John and Dredze, Mark and Pereira, Fernando},
  booktitle={Proceedings of the 45th annual meeting of the association of computational linguistics},
  pages={440--447},
  year={2007}
}

@article{morvant2012parsimonious,
  title={Parsimonious unsupervised and semi-supervised domain adaptation with good similarity functions},
  author={Morvant, Emilie and Habrard, Amaury and Ayache, St{\'e}phane},
  journal={Knowledge and Information Systems},
  volume={33},
  number={2},
  pages={309--349},
  year={2012},
  publisher={Springer}
}
@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}
@inproceedings{kutin2002,
	author = {Kutin, Samuel and Niyogi, Partha},
	title = {Almost-Everywhere Algorithmic Stability and Generalization Error},
	year = {2002},
	isbn = {1558608974},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	abstract = {We explore in some detail the notion of algorithmic stability as a viable framework for analyzing the generalization error of learning algorithms. We introduce the new notion of training stability of a learning algorithm and show that, in a general setting, it is sufficient for good bounds on generalization error. In the PAC setting, training stability is both necessary and sufficient for learnability.The approach based on training stability makes no reference to VC dimension or VC entropy. There is no need to prove uniform convergence, and generalization error is bounded directly via an extended McDiarmid inequality. As a result it potentially allows us to deal with a broader class of learning algorithms than Empirical Risk Minimization.We also explore the relationships among VC dimension, generalization error, and various notions of stability. Several examples of learning algorithms are considered.},
	booktitle = {Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence},
	pages = {275–282},
	numpages = {8},
	location = {Alberta, Canada},
	series = {UAI'02}
}

@inproceedings{li2007bayesian,
  title={A bayesian divergence prior for classiffier adaptation},
  author={Li, Xiao and Bilmes, Jeff},
  booktitle={Artificial Intelligence and Statistics},
  pages={275--282},
  year={2007},
  organization={PMLR}
}

@inproceedings{perrot2015theoretical,
  title={A theoretical analysis of metric hypothesis transfer learning},
  author={Perrot, Micha{\"e}l and Habrard, Amaury},
  booktitle={International Conference on Machine Learning},
  pages={1708--1717},
  year={2015},
  organization={PMLR}
}
@article{dugas2000incorporating,
  title={Incorporating second-order functional knowledge for better option pricing},
  author={Dugas, Charles and Bengio, Yoshua and B{\'e}lisle, Fran{\c{c}}ois and Nadeau, Claude and Garcia, Ren{\'e}},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{blitzer2007learning,
  title={Learning bounds for domain adaptation},
  author={Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{bartlett2006convexity,
  title={Convexity, classification, and risk bounds},
  author={Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  pages={138--156},
  year={2006},
  publisher={Taylor \& Francis}
}


@inproceedings{wang2019characterizing,
  title={Characterizing and avoiding negative transfer},
  author={Wang, Zirui and Dai, Zihang and P{\'o}czos, Barnab{\'a}s and Carbonell, Jaime},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11293--11302},
  year={2019}
}
@article{austern2020asymptotics,
  title={Asymptotics of cross-validation},
  author={Austern, Morgane and Zhou, Wenda},
  journal={arXiv preprint arXiv:2001.11111},
  year={2020}
}

@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={1--40},
  year={2016},
  publisher={SpringerOpen}
}


@inproceedings{scholkopf2001generalized,
  title={A generalized representer theorem},
  author={Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J},
  booktitle={International conference on computational learning theory},
  pages={416--426},
  year={2001},
  organization={Springer}
}
@article{mansour2009domain,
  title={Domain adaptation: Learning bounds and algorithms},
  author={Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={arXiv preprint arXiv:0902.3430},
  year={2009}
}

@InProceedings{wang2018approximate,
  title = 	 {Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions},
  author =       {Wang, Shuaiwen and Zhou, Wenda and Lu, Haihao and Maleki, Arian and Mirrokni, Vahab},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5228--5237},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR}
}


@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@article{wibisono2009sufficient,
  title={Sufficient conditions for uniform stability of regularization algorithms},
  author={Wibisono, Andre and Rosasco, Lorenzo and Poggio, Tomaso},
  journal={Computer Science and Artificial Intelligence Laboratory Technical Report, MIT-CSAIL-TR-2009-060},
  year={2009}
}


@article{du2017hypothesis,
  title={Hypothesis transfer learning via transformation functions},
  author={Du, Simon S and Koushik, Jayanth and Singh, Aarti and P{\'o}czos, Barnab{\'a}s},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{kuzborskij2017fast,
  title={Fast rates by transferring from auxiliary hypotheses},
  author={Kuzborskij, Ilja and Orabona, Francesco},
  journal={Machine Learning},
  volume={106},
  number={2},
  pages={171--195},
  year={2017},
  publisher={Springer}
}

@inproceedings{cortes2015adaptation,
  title={Adaptation algorithm and theory based on generalized discrepancy},
  author={Cortes, Corinna and Mohri, Mehryar and Mu{\~n}oz Medina, Andr{\'e}s},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={169--178},
  year={2015}
}
@article{zhang2012generalization,
  title={Generalization bounds for domain adaptation},
  author={Zhang, Chao and Zhang, Lei and Ye, Jieping},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@inproceedings{zhang2019bridging,
  title={Bridging theory and algorithm for domain adaptation},
  author={Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={7404--7413},
  year={2019},
  organization={PMLR}
}
@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@inproceedings{bousquet2020sharper,
 	title={Sharper bounds for uniformly stable algorithms},
 	author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
 	booktitle={Conference on Learning Theory},
 	pages={610--626},
 	year={2020},
 	organization={PMLR}
 }

@article{nikita2021stability,
	title={Stability and Deviation Optimal Risk Bounds with Convergence Rate $ O (1/n) $},
	author={Klochkov, Yegor and Zhivotovskiy, Nikita},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={5065--5076},
	year={2021}
}

@article{elisseeff05a,
	author  = {Andre Elisseeff and Theodoros Evgeniou and Massimiliano Pontil},
	title   = {Stability of Randomized Learning Algorithms},
	journal = {Journal of Machine Learning Research},
	year    = {2005},
	volume  = {6},
	number  = {3},
	pages   = {55-79}
}
@InProceedings{charles18a,
	title = 	 {Stability and Generalization of Learning Algorithms that Converge to Global Optima},
	author =       {Charles, Zachary and Papailiopoulos, Dimitris},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {745--754},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	abstract = 	 {We establish novel generalization bounds for learning algorithms that converge to global minima. We derive black-box stability results that only depend on the convergence of a learning algorithm and the geometry around the minimizers of the empirical risk function. The results are shown for non-convex loss functions satisfying the Polyak-Lojasiewicz (PL) and the quadratic growth (QG) conditions, which we show arise for 1-layer neural networks with leaky ReLU activations and deep neural networks with linear activations. We use our results to establish the stability of first-order methods such as stochastic gradient descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and the stochastic variance reduced gradient method (SVRG), in both the PL and the strongly convex setting. Our results match or improve state-of-the-art generalization bounds and can easily extend to similar optimization algorithms. Finally, although our results imply comparable stability for SGD and GD in the PL setting, we show that there exist simple quadratic models with multiple local minima where SGD is stable but GD is not.}
}



@inproceedings{orabona2009,
	title={Model adaptation with least-squares SVM for adaptive hand prosthetics},
	author={Orabona, Francesco and Castellini, Claudio and Caputo, Barbara and Fiorilla, Angelo Emanuele and Sandini, Giulio},
	booktitle={2009 IEEE International Conference on Robotics and Automation},
	pages={2897--2903},
	year={2009},
	organization={IEEE}
}

@inproceedings{kuzborskij2013,
	title={Stability and hypothesis transfer learning},
	author={Kuzborskij, Ilja and Orabona, Francesco},
	booktitle={International Conference on Machine Learning},
	pages={942--950},
	year={2013},
	organization={PMLR}
}



@article{Zhang2004,
	abstract = {We study how closely the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function. The measurement of closeness is characterized by the loss function used in the estimation. We show that such a classification scheme can be generally regarded as a (nonmaximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions that have appeared in the literature. Furthermore, the theoretical insight allows us to design good loss functions with desirable properties. Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization. This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines. It also shows their limitations and suggests possible improvements.},
	author = {Tong Zhang},
	journal = {The Annals of Statistics},
	number = {1},
	pages = {56--85},
	publisher = {Institute of Mathematical Statistics},
	title = {Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization},
	volume = {32},
	year = {2004}
}
@book{nesterov2018lectures,
	title={Lectures on convex optimization},
	author={Nesterov, Yurii and others},
	volume={137},
	year={2018},
	publisher={Springer}
}
@article{koyejo2014,
	title={Consistent binary classification with generalized performance metrics},
	author={Koyejo, Oluwasanmi O and Natarajan, Nagarajan and Ravikumar, Pradeep K and Dhillon, Inderjit S},
	journal={Advances in neural information processing systems},
	volume={27},
	year={2014}
}

@article{dhouib2018revisiting,
  title={Revisiting similarity learning for domain adaptation},
  author={Dhouib, Sofiane and Redko, Ievgen},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{Chawla2004,
	author = {Chawla, Nitesh V. and Japkowicz, Nathalie and Kotcz, Aleksander},
	title = {Editorial: Special Issue on Learning from Imbalanced Data Sets},
	year = {2004},
	issue_date = {June 2004},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {1},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/1007730.1007733},
	doi = {10.1145/1007730.1007733},
	journal = {SIGKDD Explor. Newsl.},
	month = {jun},
	pages = {1–6},
	numpages = {6}
}
@article{Lopez2013,
	title = "An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics",
	abstract = "Training classifiers with datasets which suffer of imbalanced class distributions is an important problem in data mining. This issue occurs when the number of examples representing the class of interest is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. We shortly review the many issues in machine learning and applications of this problem, by introducing the characteristics of the imbalanced dataset scenario in classification, presenting the specific metrics for evaluating performance in class imbalanced learning and enumerating the proposed solutions. In particular, we will describe preprocessing, cost-sensitive learning and ensemble techniques, carrying out an experimental study to contrast these approaches in an intra and inter-family comparison. We will carry out a thorough discussion on the main issues related to using data intrinsic characteristics in this classification problem. This will help to improve the current models with respect to: the presence of small disjuncts, the lack of density in the training data, the overlapping between classes, the identification of noisy data, the significance of the borderline instances, and the dataset shift between the training and the test distributions. Finally, we introduce several approaches and recommendations to address these problems in conjunction with imbalanced data, and we will show some experimental examples on the behavior of the learning algorithms on data with such intrinsic characteristics.",
	keywords = "Cost-sensitive learning, Dataset shift, Imbalanced dataset, Noisy data, Sampling, Small disjuncts",
	author = "Victoria L{\'o}pez and Alberto Fern{\'a}ndez and Salvador Garc{\'i}a and Vasile Palade and Francisco Herrera",
	year = "2013",
	month = nov,
	day = "20",
	doi = "10.1016/j.ins.2013.07.007",
	language = "English",
	volume = "250",
	pages = "113--141",
	journal = "Information Sciences",
	issn = "0020-0255",
	publisher = "Elsevier",
}
@book{books/daglib/0035701,
  added-at = {2015-08-07T00:00:00.000+0200},
  author = {Györfi, László and Kohler, Michael and Krzyzak, Adam and Walk, Harro},
  biburl = {https://www.bibsonomy.org/bibtex/24d74d79227652e2aae67f48c2698b87f/dblp},
  ee = {http://dx.doi.org/10.1007/b97848},
  interhash = {da0b7e3a5f86be0b3e6664a481c4fbf0},
  intrahash = {4d74d79227652e2aae67f48c2698b87f},
  isbn = {978-0-387-95441-7},
  keywords = {dblp},
  pages = {I-XVI, 1-647},
  publisher = {Springer},
  series = {Springer series in statistics},
  timestamp = {2015-08-08T11:33:20.000+0200},
  title = {A Distribution-Free Theory of Nonparametric Regression.},
  year = 2002
}
@ARTICLE{3915,
  author={Devroye, L.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Automatic pattern recognition: a study of the probability of error}, 
  year={1988},
  volume={10},
  number={4},
  pages={530-543},
  doi={10.1109/34.3915}}

@inproceedings{goix15,
  title={Learning the dependence structure of rare events: a non-asymptotic study},
  author={Goix, Nicolas and Sabourin, Anne and Cl{\'e}mençon, St{\'e}phan},
  booktitle={Conference on Learning Theory},
  pages={843--860},
  year={2015},
  organization={PMLR}
}
@phdthesis{cornec10,
  TITLE = {{Probability bounds for the cross-validation estimate in the context of the statistical learning theory and statistical models applied to economics and finance}},
  AUTHOR = {Cornec, Matthieu},
  SCHOOL = {{Universit{\'e} de Paris-Nanterre}},
  YEAR = {2009},
  MONTH = Jun,
  KEYWORDS = {cross-validation ; stability ; concentration inequality ; bagging ; Empirical risk minimisation ; Kalman filter},
  TYPE = {Thesis},
  PDF = {https://pastel.archives-ouvertes.fr/tel-00530876/file/CORNEC_thesis.pdf},
  HAL_ID = {tel-00530876},
  HAL_VERSION = {v1},
}
@incollection{arlot:hal-01485508,
  TITLE = {{Validation crois{\'e}e}},
  AUTHOR = {Arlot, Sylvain},
  BOOKTITLE = {{Apprentissage statistique et donn\'ees massives}},
  EDITOR = {Myriam Maumy-Bertrand and Gilbert Saporta and Christine Thomas-Agnan},
  PUBLISHER = {{Editions Technip}},
  YEAR = {2018},
  MONTH = May,
  KEYWORDS = {overpenalization ; estimator selection ; model selection ; risk estimation ; V-fold penalization ; cross-validation ; V-fold cross-validation ; bias-corrected cross-validation ; validation crois{\'e}e ; s{\'e}lection d'estimateurs ; leave-one-out ; leave-p-out ; validation crois{\'e}e V-fold ; validation crois{\'e}e corrig{\'e}e ; p{\'e}nalisation V-fold ; estimation du risque ; s{\'e}lection de mod{\`e}les ; surp{\'e}nalisation},
  PDF = {https://hal.archives-ouvertes.fr/hal-01485508/file/hal_JES_validation-croisee.pdf},
  HAL_ID = {hal-01485508},
  HAL_VERSION = {v1},
}

@incollection{McDiarmid98conc,
	Author = {Colin McDiarmid},
	Booktitle = {Probabilistic Methods for Algorithmic Discrete Mathematics},
	Editor = {Habib, M. and McDiarmid, C. and Ramirez-Alfonsen, J. and Reed, B.},
	Pages = {195--248},
	Publisher = {springer},
	Title = {Concentration},
	Year = {1998}}

@article{Francois-Patrice-2019,
author = {Patrice Bertail and François Portier},
title = {{Rademacher complexity for Markov chains: Applications to kernel smoothing and Metropolis–Hastings}},
volume = {25},
journal = {Bernoulli},
number = {4B},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {3912 -- 3938},
keywords = {Concentration inequalities, kernel smoothing, Markov chains, Metropolis Hastings, Rademacher complexity},
year = {2019},
doi = {10.3150/19-BEJ1115},
}

@ARTICLE{DEvroy-79,
  author={Devroye, L. and Wagner, T.},
  journal={IEEE Transactions on Information Theory}, 
  title={Distribution-free performance bounds for potential function rules}, 
  year={1979},
  volume={25},
  number={5},
  pages={601-604},
  doi={10.1109/TIT.1979.1056087}}

@inproceedings{BousquetElisseeff-2000,
 author = {Bousquet, Olivier and Elisseeff, Andr\'{e}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Algorithmic Stability and Generalization Performance},
 volume = {13},
 year = {2001}
}

@article{bayle2020cross,
  title={Cross-validation confidence intervals for test error},
  author={Bayle, Pierre and Bayle, Alexandre and Janson, Lucas and Mackey, Lester},
  journal={arXiv preprint arXiv:2007.12671},
  year={2020}
}

@article{kearns1999algorithmic,
  title={Algorithmic stability and sanity-check bounds for leave-one-out cross-validation},
  author={Kearns, Michael and Ron, Dana},
  journal={Neural computation},
  volume={11},
  number={6},
  pages={1427--1453},
  year={1999},
  publisher={MIT Press}
}

@inproceedings{kumar2013near,
  title={Near-optimal bounds for cross-validation via loss stability},
  author={Kumar, Ravi and Lokshtanov, Daniel and Vassilvitskii, Sergei and Vattani, Andrea},
  booktitle={International Conference on Machine Learning},
  pages={27--35},
  year={2013},
  organization={PMLR}
}

@inproceedings{jalalzai2018binary,
  title={On Binary Classification in Extreme Regions.},
  author={Jalalzai, Hamid and Cl{\'e}men{\c{c}}on, Stephan and Sabourin, Anne},
  booktitle={NeurIPS},
  pages={3096--3104},
  year={2018}
}


@article{lei2020cross,
  title={Cross-validation with confidence},
  author={Lei, Jing},
  journal={Journal of the American Statistical Association},
  volume={115},
  number={532},
  pages={1978--1997},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{elisseeff2003leave,
  title={Leave-one-out error and stability of learning algorithms with applications},
  author={Elisseeff, Andr{\'e} and Pontil, Massimiliano and others},
  journal={NATO science series sub series iii computer and systems sciences},
  volume={190},
  pages={111--130},
  year={2003},
  publisher={IOS press}
}
@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@inproceedings{homrighausen2013lasso,
  title={The lasso, persistence, and cross-validation},
  author={Homrighausen, Darren and McDonald, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={1031--1039},
  year={2013},
  organization={PMLR}
}

@article{xu2020rademacher,
  title={Rademacher upper bounds for cross-validation errors with an application to the lasso},
  author={Xu, Ning and Fisher, Timothy CG and Hong, Jian},
  journal={arXiv preprint arXiv:2007.15598},
  year={2020}
}

@article{wasserman2009high,
  title={High dimensional variable selection},
  author={Wasserman, Larry and Roeder, Kathryn},
  journal={Annals of statistics},
  volume={37},
  number={5A},
  pages={2178},
  year={2009},
  publisher={NIH Public Access}
}
@article{arlot2016VFchoice,
	author  = {Sylvain Arlot and Matthieu Lerasle},
	title   = {Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation},
	journal = {Journal of Machine Learning Research},
	year    = {2016},
	volume  = {17},
	number  = {208},
	pages   = {1-50},
	url     = {http://jmlr.org/papers/v17/14-296.html}
}
@unpublished{arlot2008VFpen,
	TITLE = {{V-fold cross-validation improved: V-fold penalization}},
	AUTHOR = {Arlot, Sylvain},
	URL = {https://hal.archives-ouvertes.fr/hal-00239182},
	NOTE = {40 pages, plus a separate technical appendix.},
	YEAR = {2008},
	MONTH = Feb,
	KEYWORDS = {non-parametric statistics ; statistical learning ; resampling ; non-asymptotic ; V-fold cross-validation ; model selection ; penalization ; non-parametric regression ; adaptivity ; heteroscedastic data},
	PDF = {https://hal.archives-ouvertes.fr/hal-00239182v2/file/penVF.pdf},
	HAL_ID = {hal-00239182},
	HAL_VERSION = {v2},
}
@article{BURMAN89,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2336116},
	abstract = {Concepts of ν-fold cross-validation and repeated learning-testing methods have been introduced here. In many problems, these methods are computationally much less expensive than ordinary cross-validation and can be used in its place. A comparative study of these three methods has been carried out in detail.},
	author = {Prabir Burman},
	journal = {Biometrika},
	number = {3},
	pages = {503--514},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods},
	volume = {76},
	year = {1989}
}

@misc{kutin2012,
	title={Almost-everywhere algorithmic stability and generalization error}, 
	author={Samuel Kutin and Partha Niyogi},
	year={2012},
	eprint={1301.0579},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@book{vershynin_2018, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Probability: An Introduction with Applications in Data Science}, DOI={10.1017/9781108231596}, publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}
@inproceedings{Blum1999,
	title={Beating the hold-out: bounds for K-fold and progressive cross-validation},
	author={A. Blum and A. Kalai and J. Langford},
	booktitle={COLT '99},
	year={1999}
}
@article{Rogers1978,
	doi = {10.1214/aos/1176344196},
	url = {https://doi.org/10.1214/aos/1176344196},
	year = {1978},
	month = may,
	publisher = {Institute of Mathematical Statistics},
	volume = {6},
	number = {3},
	author = {W. H. Rogers and T. J. Wagner},
	title = {A Finite Sample Distribution-Free Performance Bound for Local Discrimination Rules},
	journal = {The Annals of Statistics}
}
@article{Laan2006,
	author = {Aad W. van der Vaart and Sandrine Dudoit and Mark J. van der Laan},
	doi = {doi:10.1524/stnd.2006.24.3.351},
	url = {https://doi.org/10.1524/stnd.2006.24.3.351},
	title = {Oracle inequalities for multi-fold cross validation: },
	journal = {Statistics and Decisions},
	number = {3},
	volume = {24},
	year = {2006},
	pages = {351--371}
}
@inproceedings{Laan2003,
	title={Unified Cross-Validation Methodology For Selection Among Estimators and a General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities and Examples},
	author={M. Laan and S. Dudoit},
	year={2003}
}
@InProceedings{Menon2013,
	title = 	 {On the Statistical Consistency of Algorithms for Binary Classification under Class Imbalance},
	author = 	 {Menon, Aditya and Narasimhan, Harikrishna and Agarwal, Shivani and Chawla, Sanjay},
	booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
	pages = 	 {603--611},
	year = 	 {2013},
	editor = 	 {Dasgupta, Sanjoy and McAllester, David},
	volume = 	 {28},
	number =       {3},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Atlanta, Georgia, USA},
	month = 	 {17--19 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v28/menon13a.pdf},
	url = 	 {https://proceedings.mlr.press/v28/menon13a.html},
	abstract = 	 {Class imbalance situations, where one class is rare compared to the other, arise frequently in machine learning applications. It is well known that the usual misclassification error is ill-suited for measuring performance in such settings. A wide range of performance measures have been proposed for this problem, in machine learning as well as in data mining, artificial intelligence, and various applied fields. However, despite the large number of studies on this problem, little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest. In this paper, we study consistency with respect to one such performance measure, namely the arithmetic mean of the true positive and true negative rates (AM), and establish that some simple methods that have been used in practice, such as applying an empirically determined threshold to a suitable class probability estimate or performing an empirically balanced form of risk minimization, are in fact consistent with respect to the AM (under mild conditions on the underlying distribution). Our results employ balanced losses that have been used recently in analyses of ranking problems (Kotlowski et al., 2011) and build on recent results on consistent surrogates for cost-sensitive losses (Scott, 2012). Experimental results confirm our consistency theorems.  }
}
@article{Elkan2001,
	author = {Elkan, Charles},
	year = {2001},
	month = {05},
	pages = {},
	title = {The Foundations of Cost-Sensitive Learning},
	volume = {1},
	journal = {Proceedings of the Seventeenth International Conference on Artificial Intelligence: 4-10 August 2001; Seattle}
}


@article{hult2006regular,
  title={Regular variation for measures on metric spaces},
  author={Hult, Henrik and Lindskog, Filip},
  journal={Publications de l'Institut Mathematique},
  volume={80},
  number={94},
  pages={121--140},
  year={2006}
}

@article{bates2021cross,
  title={Cross-validation: what does it estimate and how well does it do it?},
  author={Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  journal={arXiv preprint arXiv:2104.00673},
  year={2021}
}

@article{wager2020cross,
  title={Cross-validation, risk estimation, and model selection: Comment on a paper by Rosset and Tibshirani},
  author={Wager, Stefan},
  journal={Journal of the American Statistical Association},
  volume={115},
  number={529},
  pages={157--160},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{arlot2010survey,
  title={A survey of cross-validation procedures for model selection},
  author={Arlot, Sylvain and Celisse, Alain},
  journal={Statistics surveys},
  volume={4},
  pages={40--79},
  year={2010},
  publisher={Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the~…}
}

@article{celisse2018theoretical,
  title={Theoretical analysis of cross-validation for estimating the risk of the k-nearest neighbor classifier},
  author={Celisse, Alain and Mary-Huard, Tristan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2373--2426},
  year={2018},
  publisher={JMLR. org}
}

@book{de2006extreme,
  title={Extreme value theory: an introduction},
  author={De Haan, Laurens and Ferreira, Ana},
  volume={21},
  year={2006},
  publisher={Springer}
}

@book{resnick2013extreme,
  title={Extreme values, regular variation and point processes},
  author={Resnick, Sidney I},
  year={2013},
  publisher={Springer}
}
@book{bingham1989regular,
  title={Regular variation},
  author={Bingham, Nicholas H and Goldie, Charles M and Teugels, Jozef L and Teugels, JL},
  number={27},
  year={1989},
  publisher={Cambridge university press},
  series={Encyclopedia of Mathematics and its Applications}
}

@article{segers2020one,
  title={One-versus multi-component regular variation and extremes of Markov trees},
  author={Segers, Johan},
  journal={Advances in Applied Probability},
  volume={52},
  number={3},
  pages={855--878},
  year={2020},
  publisher={Cambridge University Press}
}

@book{beirlant2006statistics,
  title={Statistics of extremes: theory and applications},
  author={Beirlant, Jan and Goegebeur, Yuri and Segers, Johan and Teugels, Jozef L},
  year={2006},
  publisher={John Wiley \& Sons}
}

@article{basrak2009regularly,
  title={Regularly varying multivariate time series},
  author={Basrak, Bojan and Segers, Johan},
  journal={Stochastic processes and their applications},
  volume={119},
  number={4},
  pages={1055--1080},
  year={2009},
  publisher={Elsevier}
}
@article{planinic2018tail,
  title={The tail process revisited},
  author={Planini{\'c}, Hrvoje and Soulier, Philippe},
  journal={Extremes},
  volume={21},
  number={4},
  pages={551--579},
  year={2018},
  publisher={Springer}
}

@article{ferreira2014generalized,
  title={The generalized Pareto process; with a view towards application and simulation},
  author={Ferreira, Ana and De Haan, Laurens},
  journal={Bernoulli},
  volume={20},
  number={4},
  pages={1717--1737},
  year={2014},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@book{embrechts2013modelling,
  title={Modelling extremal events: for insurance and finance},
  author={Embrechts, Paul and Kl{\"u}ppelberg, Claudia and Mikosch, Thomas},
  volume={33},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{boucheron2012concentration,
  title={Concentration inequalities for order statistics},
  author={Boucheron, St{\'e}phane and Thomas, Maud},
  journal={Electronic Communications in Probability},
  volume={17},
  pages={1--12},
  year={2012},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}
@article{boucheron2015tail,
  title={Tail index estimation, concentration and adaptivity},
  author={Boucheron, St{\'e}phane and Thomas, Maud},
  journal={Electronic Journal of Statistics},
  volume={9},
  number={2},
  pages={2751--2792},
  year={2015},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@inproceedings{bousquet2003introduction,
  title={Introduction to statistical learning theory},
  author={Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi, G{\'a}bor},
  booktitle={Summer school on machine learning},
  pages={169--207},
  year={2003},
  organization={Springer}
}

@inproceedings{goix2016sparse,
  title={Sparse representation of multivariate extremes with applications to anomaly ranking},
  author={Goix, Nicolas and Sabourin, Anne and Cl{\'e}men{\c{c}}on, St{\'e}phan},
  booktitle={Artificial Intelligence and Statistics},
  pages={75--83},
  year={2016},
  organization={PMLR}
}

@article{goix2017sparse,
  title={Sparse representation of multivariate extremes with applications to anomaly detection},
  author={Goix, Nicolas and Sabourin, Anne and Cl{\'e}men{\c{c}}on, Stephan},
  journal={Journal of Multivariate Analysis},
  volume={161},
  pages={12--31},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{thomas2017anomaly,
  title={Anomaly detection in extreme regions via empirical mv-sets on the sphere},
  author={Thomas, Albert and Cl{\'e}men{\c{c}}on, Stephan and Gramfort, Alexandre and Sabourin, Anne},
  booktitle={Artificial Intelligence and Statistics},
  pages={1011--1019},
  year={2017},
  organization={PMLR}
}

@inproceedings{chiapino2016feature,
  title={Feature clustering for extreme events analysis, with application to extreme stream-flow data},
  author={Chiapino, Ma{\"e}l and Sabourin, Anne},
  booktitle={International Workshop on New Frontiers in Mining Complex Patterns},
  pages={132--147},
  year={2016},
  organization={Springer}
}

@INPROCEEDINGS{review_AD_EVT,
  author={Suboh, Syahirah and Aziz, Izzatdin Abdul},
  booktitle={2020 IEEE Conference on Big Data and Analytics (ICBDA)}, 
  title={Anomaly Detection with Machine Learning in the Presence of Extreme Value - A Review Paper}, 
  year={2020},
  volume={},
  number={},
  pages={66-72},
  doi={10.1109/ICBDA50157.2020.9289798}}

@article{engelke2021sparse,
  title={Sparse structures for multivariate extremes},
  author={Engelke, Sebastian and Ivanovs, Jevgenijs},
  journal={Annual Review of Statistics and Its Application},
  volume={8},
  pages={241--270},
  year={2021},
  publisher={Annual Reviews}
}

@article{Bhatia_Jain_Hooi_2021,
title={ExGAN: Adversarial Generation of Extreme Samples},
volume={35},
 number={8},
 journal={Proceedings of the AAAI Conference on Artificial Intelligence},
 author={Bhatia, Siddharth and Jain, Arjit and Hooi, Bryan},
 year={2021},
 month={May},
 pages={6750-6758} }

@inproceedings{jalalzai2021feature,
  title={Feature Clustering for Support Identification in Extreme Regions},
  author={Jalalzai, Hamid and Leluc, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={4733--4743},
  year={2021},
  organization={PMLR}
}

@article{jalalzai2020heavy,
  title={Heavy-tailed Representations, Text Polarity Classification \& Data Augmentation},
  author={Jalalzai, Hamid and Colombo, Pierre and Clavel, Chlo{\'e} and Gaussier, Eric and Varni, Giovanna and Vignon, Emmanuel and Sabourin, Anne},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{drees2021principal,
  title={Principal component analysis for multivariate extremes},
  author={Drees, Holger and Sabourin, Anne},
  journal={Electronic Journal of Statistics},
  volume={15},
  number={1},
  pages={908--943},
  year={2021},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@article{cornec17,
author = {Cornec, Matthieu},
title = {Concentration inequalities of the cross-validation estimator for empirical risk minimizer},
journal = {Statistics},
volume = {51},
number = {1},
pages = {43-60},
year  = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/02331888.2016.1261479},
}

@article{gine2001consistency,
  title={On consistency of kernel density estimators for randomly censored data: rates holding uniformly over adaptive intervals},
  author={Gin{\'e}, Evarist and Guillou, Armelle},
  journal={Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume={37},
  number={4},
  pages={503--522},
  year={2001}
}


@incollection{lugosi2002pattern,
  title={Pattern classification and learning theory},
  author={Lugosi, G{\'a}bor},
  booktitle={Principles of nonparametric learning},
  pages={1--56},
  year={2002},
  publisher={Springer}
}
@article{einmahl1988strong,
  title={Strong limit theorems for weighted quantile processes},
  author={Einmahl, John HJ and Mason, David M},
  journal={The Annals of Probability},
  pages={1623--1643},
  year={1988},
  publisher={JSTOR}
}

@article{lhaut2021uniform,
  title={Uniform concentration bounds for frequencies of rare events},
  author={Lhaut, St{\'e}phane and Sabourin, Anne and Segers, Johan},
  journal={arXiv preprint arXiv:2110.05826},
  year={2021}
}

@misc{clemencon2022concentration,
      title={Concentration bounds for the empirical angular measure with statistical learning applications}, 
      author={St{\'e}phan Cl{\'e}men{\c c}on and Hamid Jalalzai and St{\'e}phane Lhaut and Anne Sabourin and Johan Segers},
      year={2022},
      eprint={2104.03966},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@inproceedings{anthony1998cross,
  title={Cross-validation for binary classification by real-valued functions: theoretical analysis},
  author={Anthony, Martin and Holden, Sean B},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={218--229},
  year={1998}
}

@inproceedings{kale2011cross,
  title={Cross-validation and mean-square stability},
  author={Kale, Satyen and Kumar, Ravi and Vassilvitskii, Sergei},
  booktitle={In Proceedings of the Second Symposium on Innovations in Computer Science (ICS2011},
  year={2011},
  organization={Citeseer}
}