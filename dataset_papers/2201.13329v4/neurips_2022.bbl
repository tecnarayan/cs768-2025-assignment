\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2018{\natexlab{a}})Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{ICML}, 2018{\natexlab{a}}.

\bibitem[Athalye et~al.(2018{\natexlab{b}})Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2018synthesizing}
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Biggio and Roli(2018)]{biggio2018wild}
Battista Biggio and Fabio Roli.
\newblock Wild patterns: Ten years after the rise of adversarial machine
  learning.
\newblock \emph{Pattern Recognition}, 84:\penalty0 317--331, 2018.

\bibitem[Biggio et~al.(2012)Biggio, Nelson, and Laskov]{biggio2012poisoning}
Battista Biggio, Blaine Nelson, and Pavel Laskov.
\newblock Poisoning attacks against support vector machines.
\newblock In \emph{ICML}, 2012.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
  {\v{S}}rndi{\'c}, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{ECML-PKDD}, 2013.

\bibitem[Bojarski et~al.(2016)Bojarski, Del~Testa, Dworakowski, Firner, Flepp,
  Goyal, Jackel, Monfort, Muller, Zhang, et~al.]{bojarski2016end}
Mariusz Bojarski, Davide Del~Testa, Daniel Dworakowski, Bernhard Firner, Beat
  Flepp, Prasoon Goyal, Lawrence~D Jackel, Mathew Monfort, Urs Muller, Jiakai
  Zhang, et~al.
\newblock End to end learning for self-driving cars.
\newblock In \emph{NeurIPS Deep Learning Symposium}, 2016.

\bibitem[Borgnia et~al.(2021)Borgnia, Cherepanova, Fowl, Ghiasi, Geiping,
  Goldblum, Goldstein, and Gupta]{borgnia2021strong}
Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping,
  Micah Goldblum, Tom Goldstein, and Arjun Gupta.
\newblock Strong data augmentation sanitizes poisoning and backdoor attacks
  without an accuracy tradeoff.
\newblock In \emph{ICASSP}, 2021.

\bibitem[Carlini(2019)]{carlini2019critique}
Nicholas Carlini.
\newblock A critique of the deepsec platform for security analysis of deep
  learning models.
\newblock \emph{arXiv preprint arXiv:1905.07112}, 2019.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{S\&P}, 2017.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Croce and Hein(2020)]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Croce et~al.(2021)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{croce2021robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock In \emph{NeurIPS Datasets and Benchmarks Track}, 2021.
\newblock URL \url{https://openreview.net/forum?id=SSKZPJCt7B}.

\bibitem[Dalvi et~al.(2004)Dalvi, Domingos, Sanghai, and
  Verma]{dalvi2004adversarial}
Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, and Deepak Verma.
\newblock Adversarial classification.
\newblock In \emph{KDD}, 2004.

\bibitem[Dobriban et~al.(2020)Dobriban, Hassani, Hong, and
  Robey]{dobriban2020provable}
Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey.
\newblock Provable tradeoffs in adversarially robust classification.
\newblock \emph{arXiv preprint arXiv:2006.05161}, 2020.

\bibitem[Evtimov et~al.(2021)Evtimov, Covert, Kusupati, and
  Kohno]{evtimov2021disrupting}
Ivan Evtimov, Ian Covert, Aditya Kusupati, and Tadayoshi Kohno.
\newblock Disrupting model training with adversarial shortcuts.
\newblock In \emph{ICML 2021 Workshop}, 2021.

\bibitem[Feng et~al.(2019)Feng, Cai, and Zhou]{feng2019learning}
Ji~Feng, Qi-Zhi Cai, and Zhi-Hua Zhou.
\newblock Learning to confuse: generating training time adversarial data with
  auto-encoder.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Fowl et~al.(2021{\natexlab{a}})Fowl, Chiang, Goldblum, Geiping,
  Bansal, Czaja, and Goldstein]{fowl2021preventing}
Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek
  Czaja, and Tom Goldstein.
\newblock Preventing unauthorized use of proprietary data: Poisoning for secure
  dataset release.
\newblock \emph{arXiv preprint arXiv:2103.02683}, 2021{\natexlab{a}}.

\bibitem[Fowl et~al.(2021{\natexlab{b}})Fowl, Goldblum, Chiang, Geiping, Czaja,
  and Goldstein]{fowl2021adversarial}
Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojtek Czaja, and
  Tom Goldstein.
\newblock Adversarial examples make strong poisons.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Fu et~al.(2022)Fu, He, Liu, Shen, and Tao]{fu2022robust}
Shaopeng Fu, Fengxiang He, Yang Liu, Li~Shen, and Dacheng Tao.
\newblock Robust unlearnable examples: Protecting data privacy against
  adversarial learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=baUQQPwQiAg}.

\bibitem[Gao et~al.(2022)Gao, Wu, Zhang, Xia, Niu, and Sugiyama]{gao2022does}
Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Shu-Tao Xia, Gang Niu, and Masashi
  Sugiyama.
\newblock Does adversarial robustness really imply backdoor vulnerability?,
  2022.
\newblock URL \url{https://openreview.net/forum?id=nG4DkcHDw_}.

\bibitem[Geiping et~al.(2021)Geiping, Fowl, Huang, Czaja, Taylor, Moeller, and
  Goldstein]{geiping2020witches}
Jonas Geiping, Liam~H Fowl, W~Ronny Huang, Wojciech Czaja, Gavin Taylor,
  Michael Moeller, and Tom Goldstein.
\newblock Witches' brew: Industrial scale data poisoning via gradient matching.
\newblock In \emph{ICLR}, 2021.

\bibitem[Goldblum et~al.(2020)Goldblum, Tsipras, Xie, Chen, Schwarzschild,
  Song, Madry, Li, and Goldstein]{goldblum2020dataset}
Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
  Dawn Song, Aleksander Madry, Bo~Li, and Tom Goldstein.
\newblock Dataset security for machine learning: Data poisoning, backdoor
  attacks, and defenses.
\newblock \emph{arXiv preprint arXiv:2012.10544}, 2020.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{ICLR}, 2015.

\bibitem[Gowal et~al.(2020)Gowal, Qin, Uesato, Mann, and
  Kohli]{gowal2020uncovering}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock \emph{arXiv preprint arXiv:2010.03593}, 2020.

\bibitem[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{arXiv preprint arXiv:1708.06733}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Huang et~al.(2021)Huang, Ma, Erfani, Bailey, and
  Wang]{huang2021unlearnable}
Hanxun Huang, Xingjun Ma, Sarah~Monazam Erfani, James Bailey, and Yisen Wang.
\newblock Unlearnable examples: Making personal data unexploitable.
\newblock In \emph{ICLR}, 2021.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Javanmard and Soltanolkotabi(2020)]{javanmard2020precise}
Adel Javanmard and Mahdi Soltanolkotabi.
\newblock Precise statistical analysis of classification accuracies for
  adversarial training.
\newblock \emph{arXiv preprint arXiv:2010.11213}, 2020.

\bibitem[Kim et~al.(2021)Kim, Lee, and Ro]{kim2021distilling}
Junho Kim, Byung-Kwan Lee, and Yong~Man Ro.
\newblock Distilling robust and non-robust features in adversarial examples by
  information bottleneck.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{ICML}, 2017.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Le and Yang(2015)]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021anti}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Li, Wu, Li, He, and
  Lyu]{li2021invisible}
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In \emph{CVPR}, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2017)Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{liu2017trojaning}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock 2017.

\bibitem[Liu et~al.(2021)Liu, Zhao, Kolmus, Berns, van Laarhoven, Heskes, and
  Larson]{liu2021going}
Zhuoran Liu, Zhengyu Zhao, Alex Kolmus, Tijn Berns, Twan van Laarhoven, Tom
  Heskes, and Martha Larson.
\newblock Going grayscale: The road to understanding and improving unlearnable
  examples.
\newblock \emph{arXiv preprint arXiv:2111.13244}, 2021.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mehrabi et~al.(2021)Mehrabi, Javanmard, Rossi, Rao, and
  Mai]{mehrabi2021fundamental}
Mohammad Mehrabi, Adel Javanmard, Ryan~A Rossi, Anup Rao, and Tung Mai.
\newblock Fundamental tradeoffs in distributionally adversarial training.
\newblock In \emph{ICML}, 2021.

\bibitem[Mu{\~n}oz-Gonz{\'a}lez et~al.(2017)Mu{\~n}oz-Gonz{\'a}lez, Biggio,
  Demontis, Paudice, Wongrassamee, Lupu, and Roli]{munoz2017towards}
Luis Mu{\~n}oz-Gonz{\'a}lez, Battista Biggio, Ambra Demontis, Andrea Paudice,
  Vasin Wongrassamee, Emil~C Lupu, and Fabio Roli.
\newblock Towards poisoning of deep learning algorithms with back-gradient
  optimization.
\newblock In \emph{ACM Workshop on Artificial Intelligence and Security}, 2017.

\bibitem[Nakkiran(2019)]{nakkiran2019a}
Preetum Nakkiran.
\newblock A discussion of 'adversarial examples are not bugs, they are
  features': Adversarial examples are just bugs, too.
\newblock \emph{Distill}, 2019.
\newblock \doi{10.23915/distill.00019.5}.
\newblock https://distill.pub/2019/advex-bugs-discussion/response-5.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Newsome et~al.(2006)Newsome, Karp, and Song]{newsome2006paragraph}
James Newsome, Brad Karp, and Dawn Song.
\newblock Paragraph: Thwarting signature learning by training maliciously.
\newblock In \emph{International Workshop on Recent Advances in Intrusion
  Detection}, 2006.

\bibitem[Nguyen and Tran(2020)]{nguyen2020input}
Tuan~Anh Nguyen and Anh Tran.
\newblock Input-aware dynamic backdoor attack.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Pang et~al.(2021{\natexlab{a}})Pang, Yang, Dong, Su, and
  Zhu]{pang2020bag}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Bag of tricks for adversarial training.
\newblock In \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Pang et~al.(2021{\natexlab{b}})Pang, Yang, Dong, Su, and
  Zhu]{pang2021accumulative}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Accumulative poisoning attacks on real-time data.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Pang et~al.(2022)Pang, Lin, Yang, Zhu, and Yan]{pang2022robustness}
Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan.
\newblock Robustness and accuracy could be reconcilable by (proper) definition.
\newblock \emph{arXiv preprint arXiv:2202.10103}, 2022.

\bibitem[Raghunathan et~al.(2020)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2020understanding}
Aditi Raghunathan, Sang~Michael Xie, Fanny Yang, John Duchi, and Percy Liang.
\newblock Understanding and mitigating the tradeoff between robustness and
  accuracy.
\newblock In \emph{ICML}, 2020.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Saha et~al.(2020)Saha, Subramanya, and Pirsiavash]{saha2020hidden}
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash.
\newblock Hidden trigger backdoor attacks.
\newblock In \emph{AAAI}, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Goldblum, Gupta, Dickerson,
  and Goldstein]{schwarzschild2021just}
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John~P Dickerson, and Tom
  Goldstein.
\newblock Just how toxic is data poisoning? a unified benchmark for backdoor
  and data poisoning attacks.
\newblock In \emph{ICML}, 2021.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Najibi, Suciu, Studer, Dumitras,
  and Goldstein]{shafahi2018poison}
Ali Shafahi, W~Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
  Tudor Dumitras, and Tom Goldstein.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Shen et~al.(2019)Shen, Zhu, and Ma]{shen2019tensorclog}
Juncheng Shen, Xiaolei Zhu, and De~Ma.
\newblock Tensorclog: An imperceptible poisoning attack on deep neural network
  applications.
\newblock \emph{IEEE Access}, 2019.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Springer et~al.(2021)Springer, Mitchell, and
  Kenyon]{springer2021little}
Jacob Springer, Melanie Mitchell, and Garrett Kenyon.
\newblock A little robustness goes a long way: Leveraging robust features for
  targeted transfer attacks.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Su et~al.(2018)Su, Zhang, Chen, Yi, Chen, and Gao]{su2018robustness}
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao.
\newblock Is robustness the cost of accuracy?--a comprehensive study on the
  robustness of 18 deep image classification models.
\newblock In \emph{ECCV}, 2018.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[Tack et~al.(2021)Tack, Yu, Jeong, Kim, Hwang, and
  Shin]{tack2021consistency}
Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung~Ju Hwang, and Jinwoo
  Shin.
\newblock Consistency regularization for adversarial robustness.
\newblock In \emph{ICML 2021 Workshop on Adversarial Machine Learning}, 2021.

\bibitem[Tao et~al.(2020)Tao, Feng, Yi, and Chen]{tao2020false}
Lue Tao, Lei Feng, Jinfeng Yi, and Songcan Chen.
\newblock With false friends like these, who can notice mistakes?
\newblock \emph{arXiv preprint arXiv:2012.14738}, 2020.

\bibitem[Tao et~al.(2021)Tao, Feng, Yi, Huang, and Chen]{tao2021provable}
Lue Tao, Lei Feng, Jinfeng Yi, Sheng-Jun Huang, and Songcan Chen.
\newblock Better safe than sorry: Preventing delusive adversaries with
  adversarial training.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Tramer and Boneh(2019)]{tramer2019adversarial}
Florian Tramer and Dan Boneh.
\newblock Adversarial training and robustness for multiple perturbations.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Tram{\`e}r et~al.(2020)Tram{\`e}r, Behrmann, Carlini, Papernot, and
  Jacobsen]{tramer2020fundamental}
Florian Tram{\`e}r, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and
  J{\"o}rn-Henrik Jacobsen.
\newblock Fundamental tradeoffs between invariance and sensitivity to
  adversarial perturbations.
\newblock In \emph{ICML}, 2020.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{ICLR}, 2019.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{turner2019label}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock \emph{arXiv preprint arXiv:1912.02771}, 2019.

\bibitem[Wang et~al.(2021)Wang, Liu, Han, Liu, Gong, Niu, Zhou, and
  Sugiyama]{wang2021probabilistic}
Qizhou Wang, Feng Liu, Bo~Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan
  Zhou, and Masashi Sugiyama.
\newblock Probabilistic margins for instance reweighting in adversarial
  training.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Wang et~al.(2020)Wang, Zou, Yi, Bailey, Ma, and Gu]{wang2019improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{ICLR}, 2020.

\bibitem[Wang et~al.(2022)Wang, Wang, and Wang]{wang2022fooling}
Zhirui Wang, Yifei Wang, and Yisen Wang.
\newblock Fooling adversarial training with induction noise, 2022.
\newblock URL \url{https://openreview.net/forum?id=4o1xPXaS4X}.

\bibitem[Weng et~al.(2020)Weng, Lee, and Wu]{weng2020trade}
Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung~Brandon Wu.
\newblock On the trade-off between adversarial and backdoor robustness.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Wong and Kolter(2018)]{wong2018provable}
Eric Wong and Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{ICML}, 2018.

\bibitem[Wu et~al.(2022)Wu, Chen, Zhang, Zhu, Wei, Yuan, Shen, and
  Zha]{wu2022backdoorbench}
Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan,
  Chao Shen, and Hongyuan Zha.
\newblock Backdoorbench: A comprehensive benchmark of backdoor learning.
\newblock \emph{arXiv preprint arXiv:2206.12654}, 2022.

\bibitem[Wu and Wang(2021)]{wu2021Adversarial}
Dongxian Wu and Yisen Wang.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xiao et~al.(2015)Xiao, Biggio, Brown, Fumera, Eckert, and
  Roli]{xiao2015feature}
Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and
  Fabio Roli.
\newblock Is feature selection secure against training data poisoning?
\newblock In \emph{ICML}, 2015.

\bibitem[Xu et~al.(2021)Xu, Liu, Li, Jain, and Tang]{xu2021robust}
Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang.
\newblock To be robust or to be fair: Towards fairness in adversarial training.
\newblock In \emph{ICML}, 2021.

\bibitem[Yang et~al.(2020)Yang, Rashtchian, Zhang, Salakhutdinov, and
  Chaudhuri]{yang2020closer}
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ~R Salakhutdinov, and
  Kamalika Chaudhuri.
\newblock A closer look at accuracy vs. robustness.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Yu et~al.(2021)Yu, Zhang, Chen, Yin, and Liu]{yu2021indiscriminate}
Da~Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.
\newblock Indiscriminate poisoning attacks are shortcuts.
\newblock \emph{arXiv preprint arXiv:2111.00898}, 2021.

\bibitem[Yuan and Wu(2021)]{pmlr-v139-yuan21b}
Chia-Hung Yuan and Shan-Hung Wu.
\newblock Neural tangent generalization attacks.
\newblock In \emph{ICML}, 2021.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, Cui, Sugiyama, and
  Kankanhalli]{zhang2020attacks}
Jingfeng Zhang, Xilie Xu, Bo~Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and
  Mohan Kankanhalli.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In \emph{ICML}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Xu, Han, Liu, Niu, Cui, and
  Sugiyama]{zhang2021noilin}
Jingfeng Zhang, Xilie Xu, Bo~Han, Tongliang Liu, Gang Niu, Lizhen Cui, and
  Masashi Sugiyama.
\newblock Noilin: Do noisy labels always hurt adversarial training?
\newblock \emph{arXiv preprint arXiv:2105.14676}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Zhu, Niu, Han, Sugiyama, and
  Kankanhalli]{zhang2020geometry}
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo~Han, Masashi Sugiyama, and Mohan
  Kankanhalli.
\newblock Geometry-aware instance-reweighted adversarial training.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2019)Zhu, Huang, Li, Taylor, Studer, and
  Goldstein]{zhu2019transferable}
Chen Zhu, W~Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom
  Goldstein.
\newblock Transferable clean-label poisoning attacks on deep neural nets.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhu et~al.(2021)Zhu, Zhang, Han, Liu, Niu, Yang, Kankanhalli, and
  Sugiyama]{zhu2021understanding}
Jianing Zhu, Jingfeng Zhang, Bo~Han, Tongliang Liu, Gang Niu, Hongxia Yang,
  Mohan Kankanhalli, and Masashi Sugiyama.
\newblock Understanding the interaction of adversarial training with noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2102.03482}, 2021.

\end{thebibliography}
