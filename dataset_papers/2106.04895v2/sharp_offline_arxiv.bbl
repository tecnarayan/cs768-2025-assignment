\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
A.~Agarwal, S.~Kakade, A.~Krishnamurthy, and W.~Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{arXiv preprint arXiv:2006.10814}, 2020.

\bibitem[Agrawal and Jia(2017)]{agrawal2017optimistic}
S.~Agrawal and R.~Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 1184--1194, 2017.

\bibitem[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew,
  Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving}
I.~Akkaya, M.~Andrychowicz, M.~Chociej, M.~Litwin, B.~McGrew, A.~Petron,
  A.~Paino, M.~Plappert, G.~Powell, R.~Ribas, et~al.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
A.~Antos, C.~Szepesv{\'a}ri, and R.~Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Bai et~al.(2019)Bai, Xie, Jiang, and Wang]{bai2019provably}
Y.~Bai, T.~Xie, N.~Jiang, and Y.-X. Wang.
\newblock Provably efficient q-learning with low switching cost.
\newblock \emph{arXiv preprint arXiv:1905.12849}, 2019.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Y.~Bai, C.~Jin, and T.~Yu.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Baker et~al.(2019)Baker, Kanitscheider, Markov, Wu, Powell, McGrew,
  and Mordatch]{baker2019emergent}
B.~Baker, I.~Kanitscheider, T.~Markov, Y.~Wu, G.~Powell, B.~McGrew, and
  I.~Mordatch.
\newblock Emergent tool use from multi-agent autocurricula.
\newblock \emph{arXiv preprint arXiv:1909.07528}, 2019.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
R.~I. Brafman and M.~Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Chen and Jiang(2019)]{chen2019information}
J.~Chen and N.~Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1042--1051. PMLR, 2019.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
C.~Dann, T.~Lattimore, and E.~Brunskill.
\newblock Unifying pac and regret: uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 5717--5727, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
C.~Dann, L.~Li, W.~Wei, and E.~Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1516. PMLR, 2019.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2021episodic}
O.~D. Domingues, P.~M{\'e}nard, E.~Kaufmann, and M.~Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In \emph{Algorithmic Learning Theory}, pages 578--598. PMLR, 2021.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
S.~S. Du, S.~M. Kakade, J.~D. Lee, S.~Lovett, G.~Mahajan, W.~Sun, and R.~Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock \emph{arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[Farahmand et~al.(2010)Farahmand, Munos, and
  Szepesv{\'a}ri]{farahmand2010error}
A.~M. Farahmand, R.~Munos, and C.~Szepesv{\'a}ri.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
S.~Fujimoto, D.~Meger, and D.~Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062. PMLR, 2019.

\bibitem[Gao et~al.(2021)Gao, Xie, Du, and Yang]{gao2021provably}
M.~Gao, T.~Xie, S.~S. Du, and L.~F. Yang.
\newblock A provably efficient algorithm for linear markov decision process
  with low switching cost.
\newblock \emph{arXiv preprint arXiv:2101.00494}, 2021.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
M.~Hessel, J.~Modayil, H.~Van~Hasselt, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~Azar, and D.~Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
M.~Janner, J.~Fu, M.~Zhang, and S.~Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{arXiv preprint arXiv:1906.08253}, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
N.~Jiang, A.~Krishnamurthy, A.~Agarwal, J.~Langford, and R.~E. Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 4868--4878, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020{\natexlab{a}}.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
C.~Jin, Q.~Liu, and S.~Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, and Wang]{jin2020pessimism}
Y.~Jin, Z.~Yang, and Z.~Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock \emph{arXiv preprint arXiv:2012.15085}, 2020{\natexlab{b}}.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke,
  et~al.]{kalashnikov2018scalable}
D.~Kalashnikov, A.~Irpan, P.~Pastor, J.~Ibarz, A.~Herzog, E.~Jang, D.~Quillen,
  E.~Holly, M.~Kalakrishnan, V.~Vanhoucke, et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pages 651--673. PMLR, 2018.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
M.~Kearns and S.~Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
R.~Kidambi, A.~Rajeswaran, P.~Netrapalli, and T.~Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
A.~Kumar, J.~Fu, G.~Tucker, and S.~Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{arXiv preprint arXiv:1906.00949}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
A.~Kumar, A.~Zhou, G.~Tucker, and S.~Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
T.~Lattimore and C.~Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2020)Lee, Hwangbo, Wellhausen, Koltun, and
  Hutter]{lee2020learning}
J.~Lee, J.~Hwangbo, L.~Wellhausen, V.~Koltun, and M.~Hutter.
\newblock Learning quadrupedal locomotion over challenging terrain.
\newblock \emph{Science robotics}, 5\penalty0 (47), 2020.

\bibitem[Lehmann and Casella(2006)]{lehmann2006theory}
E.~L. Lehmann and G.~Casella.
\newblock \emph{Theory of point estimation}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Yu, Bai, and Jin]{liu2020sharp}
Q.~Liu, T.~Yu, Y.~Bai, and C.~Jin.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock \emph{arXiv preprint arXiv:2010.01604}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Y.~Liu, A.~Swaminathan, A.~Agarwal, and E.~Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020{\natexlab{b}}.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deployment}
T.~Matsushima, H.~Furuta, Y.~Matsuo, O.~Nachum, and S.~Gu.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization.
\newblock \emph{arXiv preprint arXiv:2006.03647}, 2020.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
A.~Maurer and M.~Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Munos(2003)]{munos2003error}
R.~Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{ICML}, volume~3, pages 560--567, 2003.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R.~Munos and C.~Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Osband and Roy(2014)]{osband2014model}
I.~Osband and B.~V. Roy.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In \emph{Proceedings of the 27th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 1466--1474, 2014.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
P.~Rashidinejad, B.~Zhu, C.~Ma, J.~Jiao, and S.~Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{arXiv preprint arXiv:2103.12021}, 2021.

\bibitem[Ren et~al.(2021)Ren, Li, Dai, Du, and Sanghavi]{ren2021nearly}
T.~Ren, J.~Li, B.~Dai, S.~S. Du, and S.~Sanghavi.
\newblock Nearly horizon-free offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2103.14077}, 2021.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Su et~al.(2021)Su, Lee, Mulvey, and Poor]{su2021musbo}
D.~Su, J.~D. Lee, J.~M. Mulvey, and H.~V. Poor.
\newblock Musbo: Model-based uncertainty regularized and sample efficient batch
  optimization for deployment constrained reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.11448}, 2021.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
W.~Sun, N.~Jiang, A.~Krishnamurthy, A.~Agarwal, and J.~Langford.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on Learning Theory}, pages 2898--2933. PMLR,
  2019.

\bibitem[Szepesv{\'a}ri and Munos(2005)]{szepesvari2005finite}
C.~Szepesv{\'a}ri and R.~Munos.
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 880--887, 2005.

\bibitem[Ta{\"\i}ga et~al.(2019)Ta{\"\i}ga, Fedus, Machado, Courville, and
  Bellemare]{taiga2019benchmarking}
A.~A. Ta{\"\i}ga, W.~Fedus, M.~C. Machado, A.~Courville, and M.~G. Bellemare.
\newblock Benchmarking bonus-based exploration methods on the arcade learning
  environment.
\newblock \emph{arXiv preprint arXiv:1908.02388}, 2019.

\bibitem[Taiga et~al.(2020)Taiga, Fedus, Machado, Courville, and
  Bellemare]{taiga2020on}
A.~A. Taiga, W.~Fedus, M.~C. Machado, A.~Courville, and M.~G. Bellemare.
\newblock On bonus based exploration methods in the arcade learning
  environment.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJewlyStDr}.

\bibitem[Tosatto et~al.(2017)Tosatto, Pirotta, d’Eramo, and
  Restelli]{tosatto2017boosted}
S.~Tosatto, M.~Pirotta, C.~d’Eramo, and M.~Restelli.
\newblock Boosted fitted q-iteration.
\newblock In \emph{International Conference on Machine Learning}, pages
  3434--3443. PMLR, 2017.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement}
R.~Wang, R.~R. Salakhutdinov, and L.~Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6123--6135, 2020.

\bibitem[Wang et~al.(2021)Wang, Zhou, and Gu]{wang2021provably}
T.~Wang, D.~Zhou, and Q.~Gu.
\newblock Provably efficient reinforcement learning with linear function
  approximation under adaptivity constraints.
\newblock \emph{arXiv preprint arXiv:2101.02195}, 2021.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie and Jiang(2020)]{xie2020q}
T.~Xie and N.~Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  550--559. PMLR, 2020.

\bibitem[Yang et~al.(2020)Yang, Jin, Wang, Wang, and Jordan]{yang2020bridging}
Z.~Yang, C.~Jin, Z.~Wang, M.~Wang, and M.~I. Jordan.
\newblock Bridging exploration and general function approximation in
  reinforcement learning: Provably efficient kernel and neural value
  iterations.
\newblock \emph{arXiv preprint arXiv:2011.04622}, 2020.

\bibitem[Yin et~al.(2020)Yin, Bai, and Wang]{yin2020near}
M.~Yin, Y.~Bai, and Y.-X. Wang.
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[Yin et~al.(2021)Yin, Bai, and Wang]{yin2021near}
M.~Yin, Y.~Bai, and Y.-X. Wang.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock \emph{arXiv preprint arXiv:2102.01748}, 2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
T.~Yu, G.~Thomas, L.~Yu, S.~Ermon, J.~Zou, S.~Levine, C.~Finn, and T.~Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
A.~Zanette, A.~Lazaric, M.~Kochenderfer, and E.~Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020provably}
A.~Zanette, A.~Lazaric, M.~J. Kochenderfer, and E.~Brunskill.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock \emph{arXiv preprint arXiv:2008.07737}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{zhang2020almost}
Z.~Zhang, Y.~Zhou, and X.~Ji.
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
