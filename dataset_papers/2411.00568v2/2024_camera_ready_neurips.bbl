% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Faulkner22s}
M.~F. Faulkner and S.~Livingstone, ``Sampling algorithms in statistical
  physics: a guide for statistics and machine learning,'' \emph{arXiv preprint
  arXiv:2208.04751}, 2022.

\bibitem{Schoot21b}
R.~van~de Schoot, S.~Depaoli, R.~King, B.~Kramer, K.~M{\"a}rtens, M.~G.
  Tadesse, M.~Vannucci, A.~Gelman, D.~Veen, J.~Willemsen \emph{et~al.},
  ``{B}ayesian statistics and modelling,'' \emph{Nature Reviews Methods
  Primers}, vol.~1, no.~1, p.~1, 2021.

\bibitem{Song19g}
Y.~Song and S.~Ermon, ``Generative modeling by estimating gradients of the data
  distribution,'' \emph{Advances in neural information processing systems},
  vol.~32, 2019.

\bibitem{Roberts04g}
G.~O. Roberts and J.~S. Rosenthal, ``General state space {Markov} chains and
  {MCMC} algorithms,'' \emph{Probability Surveys}, vol.~1, 2004.

\bibitem{Robert04m}
C.~Robert and G.~Casella, \emph{Monte {Carlo} statistical methods}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer Verlag, 2004.

\bibitem{Roberts96e}
G.~O. Roberts and R.~L. Tweedie, ``Exponential convergence of {Langevin}
  distributions and their discrete approximations,'' \emph{Bernoulli}, pp.
  341--363, 1996.

\bibitem{Wibisono18s}
A.~Wibisono, ``Sampling as optimization in the space of measures: The
  {Langevin} dynamics as a composite optimization problem,'' in
  \emph{Conference on Learning Theory}.\hskip 1em plus 0.5em minus 0.4em\relax
  PMLR, 2018, pp. 2093--3027.

\bibitem{Durmus19a}
A.~Durmus, S.~Majewski, and B.~Miasojedow, ``Analysis of {Langevin Monte Carlo}
  via convex optimization,'' \emph{The Journal of Machine Learning Research},
  vol.~20, no.~1, pp. 2666--2711, 2019.

\bibitem{Wang22a}
Y.~Wang and W.~Li, ``Accelerated information gradient flow,'' \emph{J. Sci.
  Comput.}, vol.~90, no.~1, 2022.

\bibitem{Lang07b}
L.~Lang, W.-s. Chen, B.~R. Bakshi, P.~K. Goel, and S.~Ungarala, ``{B}ayesian
  estimation via sequential {Monte Carlo} sampling—{Constrained} dynamic
  systems,'' \emph{Automatica}, vol.~43, no.~9, pp. 1615--1622, 2007.

\bibitem{Li15e}
Y.~Li and S.~K. Ghosh, ``Efficient sampling methods for truncated multivariate
  normal and student-t distributions subject to linear inequality
  constraints,'' \emph{Journal of Statistical Theory and Practice}, vol.~9, pp.
  712--732, 2015.

\bibitem{Hsieh18m}
Y.-P. Hsieh, A.~Kavis, P.~Rolland, and V.~Cevher, ``Mirrored {Langevin}
  dynamics,'' \emph{Advances in Neural Information Processing Systems},
  vol.~31, 2018.

\bibitem{Bubeck18s}
S.~Bubeck, R.~Eldan, and J.~Lehec, ``Sampling from a log-concave distribution
  with projected {Langevin Monte Carlo},'' \emph{Discrete \& Computational
  Geometry}, vol.~59, no.~4, pp. 757--783, 2018.

\bibitem{Salim20p}
A.~Salim and P.~Richtarik, ``Primal dual interpretation of the proximal
  stochastic gradient {L}angevin algorithm,'' in \emph{Advances in Neural
  Information Processing Systems}, 2020, pp. 3786--3796.

\bibitem{Ahn21e}
K.~Ahn and S.~Chewi, ``Efficient constrained sampling via the mirror-{L}angevin
  algorithm,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 28\,405--28\,418, 2021.

\bibitem{Kook22s}
Y.~Kook, Y.~Lee, R.~Shen, and S.~Vempala, ``Sampling with {R}iemannian
  {H}amiltonian {Monte Carlo} in a constrained space,'' in \emph{Advances in
  Neural Information Processing Systems}, A.~H. Oh, A.~Agarwal, D.~Belgrave,
  and K.~Cho, Eds., 2022.

\bibitem{Sharrock23l}
L.~Sharrock, L.~Mackey, and C.~Nemeth, ``Learning rate free {B}ayesian
  inference in constrained domains,'' in \emph{Conference on Neural Information
  Processing Systems}, 2023.

\bibitem{Noble23u}
M.~Noble, V.~De~Bortoli, and A.~Durmus, ``Unbiased constrained sampling with
  self-concordant barrier {H}amiltonian {Monte Carlo},'' in
  \emph{Thirty-seventh Conference on Neural Information Processing Systems},
  2023.

\bibitem{Madry18t}
A.~M\k{a}dry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu, ``Towards deep
  learning models resistant to adversarial attacks,'' in \emph{International
  Conference on Learning Representations}, 2018.

\bibitem{Kearns18p}
M.~Kearns, S.~Neel, A.~Roth, and Z.~S. Wu, ``Preventing fairness
  {G}errymandering: {A}uditing and learning for subgroup fairness,'' in
  \emph{International Conference on Machine Learning}, 2018, pp. 2564--2572.

\bibitem{Cotter19o}
A.~Cotter, H.~Jiang, M.~Gupta, S.~Wang, T.~Narayan, S.~You, and K.~Sridharan,
  ``Optimization with non-differentiable constraints with applications to
  fairness, recall, churn, and other goals,'' \emph{Journal of Machine Learning
  Research}, vol.~20, no. 172, pp. 1--59, 2019.

\bibitem{Chamon23c}
L.~F.~O. Chamon, S.~Paternain, M.~{Calvo-Fullana}, and A.~Ribeiro,
  ``Constrained learning with non-convex losses,'' \emph{IEEE Trans. on Inf.
  Theory}, vol. 69[3], pp. 1739--1760, 2023.

\bibitem{Gurbuzbalaban22p}
M.~G{\"u}rb{\"u}zbalaban, Y.~Hu, and L.~Zhu, ``Penalized {{Langevin}} and
  {{Hamiltonian Monte Carlo Algorithms}} for {{Constrained Sampling}},'' no.
  arXiv:2212.00570, 2022.

\bibitem{Liu21s}
X.~Liu, X.~Tong, and Q.~Liu, ``Sampling with trusthworthy constraints: A
  variational gradient framework,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 23\,557--23\,568, 2021.

\bibitem{Jordan98t}
R.~Jordan, D.~Kinderlehrer, and F.~Otto, ``The variational formulation of the
  {F}okker--{P}lanck equation,'' \emph{SIAM Journal on Mathematical Analysis},
  vol.~29, no.~1, pp. 1--17, 1998.

\bibitem{Ambrosio05g}
L.~Ambrosio, N.~Gigli, and G.~Savar{\'e}, \emph{Gradient flows: in metric
  spaces and in the space of probability measures}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer Science \& Business Media, 2005.

\bibitem{dalalyan2019user}
A.~S. Dalalyan and A.~Karagulyan, ``User-friendly guarantees for the {L}angevin
  {Monte Carlo} with inaccurate gradient,'' \emph{Stochastic Processes and
  their Applications}, vol. 129, no.~12, pp. 5278--5311, 2019.

\bibitem{dalalyan2022bounding}
A.~S. Dalalyan, A.~Karagulyan, and L.~Riou-Durand, ``Bounding the error of
  discretized {L}angevin algorithms for non-strongly log-concave targets,''
  \emph{Journal of Machine Learning Research}, vol.~23, no. 235, pp. 1--38,
  2022.

\bibitem{Vempala19r}
S.~Vempala and A.~Wibisono, ``Rapid convergence of the unadjusted {L}angevin
  algorithm: Isoperimetry suffices,'' in \emph{Advances in Neural Information
  Processing Systems (NeurIPS)}, 2019, pp. 8092--8104.

\bibitem{Bertsekas09c}
D.~P. Bertsekas, \emph{Convex Optimization Theory}.\hskip 1em plus 0.5em minus
  0.4em\relax Athena Scientific, 2009.

\bibitem{Luenberger68o}
D.~G. Luenberger, \emph{Optimization by Vector Space Methods}.\hskip 1em plus
  0.5em minus 0.4em\relax Wiley, 1968.

\bibitem{Jeyakumar92g}
V.~Jeyakumar and H.~Wolkowicz, ``Generalizations of slater's constraint
  qualification for infinite convex programs,'' \emph{Math. Program.}, vol.~57,
  no. 1–3, pp. 85--101, 1992.

\bibitem{Bonnans00p}
J.~F. Bonnans and A.~Shapiro, \emph{Perturbation Analysis of Optimization
  Problems}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2000.

\bibitem{Kurdila05c}
A.~J. Kurdila and M.~Zabarankin, \emph{Convex Functional Analysis}.\hskip 1em
  plus 0.5em minus 0.4em\relax Birkh\"{a}user Basel, 2005.

\bibitem{Villani21t}
C.~Villani, \emph{Topics in optimal transportation}.\hskip 1em plus 0.5em minus
  0.4em\relax American Mathematical Soc., 2021, vol.~58.

\bibitem{Ruszczynski06n}
A.~P. Ruszczy{\'{n}}ski, \emph{Nonlinear Optimization}.\hskip 1em plus 0.5em
  minus 0.4em\relax Princeton University Press, 2006.

\bibitem{Nisan07a}
N.~Nisan, T.~Roughgarden, \'{E}va Tardos, and V.~V. Vazirani, Eds.,
  \emph{Algorithmic Game Theory}.\hskip 1em plus 0.5em minus 0.4em\relax
  Cambridge University, 2007.

\bibitem{Bertsekas96c}
D.~P. Bertsekas, \emph{Constrained Optimization and Lagrange Multiplier Methods
  (Optimization and Neural Computation Series)}, 1st~ed.\hskip 1em plus 0.5em
  minus 0.4em\relax Athena Scientific, 1996.

\bibitem{Kloeckner12a}
B.~Kloeckner, ``\BIBforeignlanguage{en}{Approximation by finitely supported
  measures},'' \emph{\BIBforeignlanguage{en}{ESAIM: Control, Optimisation and
  Calculus of Variations}}, vol.~18, no.~2, pp. 343--359, 2012.

\bibitem{Nagurney96p}
A.~Nagurney and D.~Zhang, \emph{Projected Dynamical Systems and Variational
  Inequalities with Applications}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 1996.

\bibitem{Welling11b}
M.~Welling and Y.~W. Teh, ``{B}ayesian learning via stochastic gradient
  {L}angevin dynamics,'' in \emph{Proceedings of the 28th International
  Conference on International Conference on Machine Learning}, ser.
  ICML'11.\hskip 1em plus 0.5em minus 0.4em\relax Madison, WI, USA: Omnipress,
  2011, p. 681–688.

\bibitem{Salim20t}
A.~Salim, A.~Korba, and G.~Luise, ``The {W}asserstein proximal gradient
  algorithm,'' \emph{Advances in Neural Information Processing Systems},
  vol.~33, pp. 12\,356--12\,366, 2020.

\bibitem{Nedic09a}
A.~Nedi{\'c} and A.~Ozdaglar, ``Approximate primal solutions and rate analysis
  for dual subgradient methods,'' \emph{SIAM Journal on Optimization}, vol.~19,
  no.~4, pp. 1757--1780, 2009.

\bibitem{cherukuri2016asymptotic}
A.~Cherukuri, E.~Mallada, and J.~Cortés, ``Asymptotic convergence of
  constrained primal–dual dynamics,'' \emph{Systems \& Control Letters},
  vol.~87, pp. 10--15, 2016.

\bibitem{nemirovski2004prox}
A.~Nemirovski, ``Prox-method with rate of convergence o(1/t) for variational
  inequalities with {L}ipschitz continuous monotone operators and smooth
  convex-concave saddle point problems,'' \emph{SIAM Journal on Optimization},
  vol.~15, no.~1, pp. 229--251, 2004.

\bibitem{nesterov2007dual}
Y.~Nesterov, ``Dual extrapolation and its applications to solving variational
  inequalities and related problems,'' \emph{Math. Program.}, vol. 109, no.
  2–3, p. 319–344, 2007.

\bibitem{lin2020near}
T.~Lin, C.~Jin, and M.~I. Jordan, ``Near-optimal algorithms for minimax
  optimization,'' in \emph{Proceedings of Thirty Third Conference on Learning
  Theory}, 2020, pp. 2738--2779.

\bibitem{mokhtari2020unified}
A.~Mokhtari, A.~Ozdaglar, and S.~Pattathil, ``A unified analysis of
  extra-gradient and optimistic gradient methods for saddle point problems:
  Proximal point approach,'' in \emph{Proceedings of the Twenty Third
  International Conference on Artificial Intelligence and Statistics}, 2020,
  pp. 1497--1507.

\bibitem{Chewi22a}
S.~Chewi, M.~A. Erdogdu, M.~Li, R.~Shen, and S.~Zhang, ``Analysis of {Langevin}
  {Monte} {Carlo}: from {Poincar\'e} to {Log-Sobolev},'' in \emph{Proceedings
  of Thirty Fifth Conference on Learning Theory}, ser. Proceedings of Machine
  Learning Research, P.-L. Loh and M.~Raginsky, Eds., vol. 178.\hskip 1em plus
  0.5em minus 0.4em\relax PMLR, 2022, pp. 1--2.

\bibitem{holley1986logarithmic}
R.~Holley and D.~W. Stroock, ``Logarithmic {S}obolev inequalities and
  stochastic {I}sing models,'' \emph{Journal of Statistical Physics}, no.
  5–6, 1986.

\bibitem{Bakry14a}
D.~Bakry, I.~Gentil, M.~Ledoux \emph{et~al.}, \emph{Analysis and geometry of
  Markov diffusion operators}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2014, vol. 103.

\bibitem{Cattiaux22f}
P.~Cattiaux and A.~Guillin, ``Functional inequalities for perturbed measures
  with applications to log-concave measures and to some {B}ayesian problems,''
  \emph{Bernoulli}, vol.~28, no.~4, 2022.

\bibitem{Karimi16l}
H.~Karimi, J.~Nutini, and M.~Schmidt, ``Linear convergence of gradient and
  proximal-gradient methods under the {P}olyak-{{\L}}ojasiewicz condition,'' in
  \emph{Machine Learning and Knowledge Discovery in Databases}, P.~Frasconi,
  N.~Landwehr, G.~Manco, and J.~Vreeken, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Cham: Springer International Publishing, 2016, pp. 795--811.

\bibitem{yang2022nest}
J.~Yang, X.~Li, and N.~He, ``Nest your adaptive algorithm for
  parameter-agnostic nonconvex minimax optimization,'' in \emph{Advances in
  Neural Information Processing Systems}, 2022.

\bibitem{boroun2023accelerated}
M.~Boroun, Z.~Alizadeh, and A.~Jalilzadeh, ``Accelerated primal-dual scheme for
  a class of stochastic nonconvex-concave saddle point problems,'' in
  \emph{American Control Conference}, 2023, pp. 204--209.

\bibitem{sanjabi2018convergence}
M.~Sanjabi, J.~Ba, M.~Razaviyayn, and J.~D. Lee, ``On the convergence and
  robustness of training gans with regularized optimal transport,'' in
  \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem{yang2020global}
J.~Yang, N.~Kiyavash, and N.~He, ``Global convergence and variance reduction
  for a class of nonconvex-nonconcave minimax problems,'' in \emph{Advances in
  Neural Information Processing Systems}, 2020, pp. 1153--1165.

\bibitem{fiez2021global}
T.~Fiez, L.~Ratliff, E.~Mazumdar, E.~Faulkner, and A.~Narang, ``Global
  convergence to local minmax equilibrium in classes of nonconvex zero-sum
  games,'' in \emph{Advances in Neural Information Processing Systems}, 2021,
  pp. 29\,049--29\,063.

\bibitem{Ahn20e}
K.~Ahn and S.~Chewi, ``Efficient constrained sampling via the
  mirror-{{Langevin}} algorithm,'' in \emph{Advances in Neural Information
  Processing Systems}, vol.~34, 2021, p.~26.

\bibitem{Dua17u}
\BIBentryALTinterwordspacing
D.~Dua and C.~Graff, ``{UCI} machine learning repository,'' 2017. [Online].
  Available: \url{http://archive.ics.uci.edu/ml}
\BIBentrySTDinterwordspacing

\bibitem{Chamon20p}
L.~F.~O. Chamon and A.~Ribeiro, ``Probably approximately correct constrained
  learning,'' in \emph{Advances in Neural Information Processing}, 2020.

\bibitem{blei2003latent}
D.~M. Blei, A.~Y. Ng, and M.~I. Jordan, ``Latent {D}irichlet allocation,''
  \emph{J. Mach. Learn. Res.}, vol.~3, pp. 993--1022, 2003.

\bibitem{celeux2012regularization}
G.~Celeux, M.~E. Anbari, J.-M. Marin, and C.~P. Robert, ``{Regularization in
  Regression: Comparing Bayesian and Frequentist Methods in a Poorly
  Informative Situation},'' \emph{Bayesian Analysis}, vol.~7, no.~2, pp.
  477--502, 2012.

\bibitem{Lamperski21p}
A.~Lamperski, ``Projected stochastic gradient {L}angevin algorithms for
  constrained sampling and non-convex learning,'' in \emph{Conference on
  Learning Theory}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  2891--2937.

\bibitem{Li22s}
L.~Li, Q.~Liu, A.~Korba, M.~Yurochkin, and J.~Solomon, ``Sampling with
  mollified interaction energy descent,'' \emph{arXiv preprint
  arXiv:2210.13400}, 2022.

\bibitem{Zhang20w}
K.~S. Zhang, G.~Peyr{\'e}, J.~Fadili, and M.~Pereyra, ``{W}asserstein control
  of mirror {L}angevin {Monte Carlo},'' in \emph{Conference on Learning
  Theory}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 3814--3841.

\bibitem{Jiang21m}
Q.~Jiang, ``Mirror {L}angevin {Monte Carlo}: the case under isoperimetry,''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, pp.
  715--725, 2021.

\bibitem{Srinivasan23f}
V.~Srinivasan, A.~Wibisono, and A.~Wilson, ``Fast sampling from constrained
  spaces using the metropolis-adjusted mirror {L}angevin algorithm,''
  \emph{arXiv preprint arXiv:2312.08823}, 2023.

\bibitem{Shi22s}
J.~Shi, C.~Liu, and L.~Mackey, ``Sampling with mirrored {Stein} operators,''
  \emph{International Conference of Learning Representations}, 2022.

\bibitem{Girolami11r}
M.~Girolami and B.~Calderhead, ``{R}iemann manifold {L}angevin and
  {H}amiltonian {Monte Carlo} methods,'' \emph{Journal of the Royal Statistical
  Society Series B: Statistical Methodology}, vol.~73, no.~2, pp. 123--214,
  2011.

\bibitem{Brubaker12a}
M.~Brubaker, M.~Salzmann, and R.~Urtasun, ``A family of {MCMC} methods on
  implicitly defined manifolds,'' in \emph{Artificial intelligence and
  statistics}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2012, pp. 161--172.

\bibitem{tseng1995linear}
P.~Tseng, ``On linear convergence of iterative methods for the variational
  inequality problem,'' \emph{Journal of Computational and Applied
  Mathematics}, vol.~60, no.~1, pp. 237--252, 1995.

\bibitem{golowich2020last}
N.~Golowich, S.~Pattathil, C.~Daskalakis, and A.~Ozdaglar, ``Last iterate is
  slower than averaged iterate in smooth convex-concave saddle point
  problems,'' in \emph{Proceedings of Thirty Third Conference on Learning
  Theory}, 2020, pp. 1758--1784.

\bibitem{panaretos2020invitation}
V.~M. Panaretos and Y.~Zemel, \emph{An invitation to statistics in
  {W}asserstein space}.\hskip 1em plus 0.5em minus 0.4em\relax Springer Nature,
  2020.

\bibitem{Otto01t}
F.~Otto, ``The geometry of dissipative evolution equations: the porous medium
  equation,'' \emph{Communications in Partial Differential Equations}, vol.~26,
  no. 1-2, pp. 101--174, 2001.

\bibitem{Schwarm99c}
A.~T. Schwarm and M.~Nikolaou, ``Chance-constrained model predictive control,''
  \emph{AIChE Journal}, vol. 45[8], pp. 1743--1752, 1999.

\bibitem{Li00r}
P.~Li, M.~Wendt, and G.~Wozny, ``Robust model predictive control under chance
  constraints,'' \emph{Computers \& Chemical Engineering}, vol. 24[2-7], pp.
  829--834, 2000.

\bibitem{Borrelli17p}
F.~Borrelli, A.~Bemporad, and M.~Morari, \emph{Predictive Control for Linear
  and Hybrid Systems}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge
  University Press, 2017.

\end{thebibliography}
