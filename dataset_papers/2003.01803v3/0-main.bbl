\begin{thebibliography}{32}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abramowitz and Stegun(1965)}]{abramowitz1965handbook}
\textsc{Abramowitz, M.} and \textsc{Stegun, I.~A.} (1965).
\newblock Handbook of mathematical functions with formulas, graphs, and
  mathematical table.
\newblock In \textit{US Department of Commerce}. National Bureau of Standards
  Applied Mathematics series 55.

\bibitem[{Agrawal and Goyal(2012)}]{agrawal2012analysis}
\textsc{Agrawal, S.} and \textsc{Goyal, N.} (2012).
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock In \textit{Conference on learning theory}.

\bibitem[{Agrawal and Goyal(2013)}]{agrawal2013further}
\textsc{Agrawal, S.} and \textsc{Goyal, N.} (2013).
\newblock Further optimal regret bounds for thompson sampling.
\newblock In \textit{Artificial intelligence and statistics}.

\bibitem[{Agrawal and Goyal(2017)}]{agrawal2017near}
\textsc{Agrawal, S.} and \textsc{Goyal, N.} (2017).
\newblock Near-optimal regret bounds for thompson sampling.
\newblock \textit{Journal of the ACM (JACM)} \textbf{64} 30.

\bibitem[{Audibert and Bubeck(2009)}]{audibert2009minimax}
\textsc{Audibert, J.-Y.} and \textsc{Bubeck, S.} (2009).
\newblock Minimax policies for adversarial and stochastic bandits.
\newblock In \textit{COLT}.

\bibitem[{Auer et~al.(2002{\natexlab{a}})Auer, Cesa-Bianchi and
  Fischer}]{auer2002finite}
\textsc{Auer, P.}, \textsc{Cesa-Bianchi, N.} and \textsc{Fischer, P.}
  (2002{\natexlab{a}}).
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \textit{Machine learning} \textbf{47} 235--256.

\bibitem[{Auer et~al.(2002{\natexlab{b}})Auer, Cesa-Bianchi, Freund and
  Schapire}]{auer2002nonstochastic}
\textsc{Auer, P.}, \textsc{Cesa-Bianchi, N.}, \textsc{Freund, Y.} and
  \textsc{Schapire, R.~E.} (2002{\natexlab{b}}).
\newblock The nonstochastic multiarmed bandit problem.
\newblock \textit{SIAM journal on computing} \textbf{32} 48--77.

\bibitem[{Auer and Ortner(2010)}]{auer2010ucb}
\textsc{Auer, P.} and \textsc{Ortner, R.} (2010).
\newblock Ucb revisited: Improved regret bounds for the stochastic multi-armed
  bandit problem.
\newblock \textit{Periodica Mathematica Hungarica} \textbf{61} 55--65.

\bibitem[{Bubeck and Liu(2013)}]{bubeck2013prior}
\textsc{Bubeck, S.} and \textsc{Liu, C.-Y.} (2013).
\newblock Prior-free and prior-dependent regret bounds for thompson sampling.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Chapelle and Li(2011)}]{chapelle2011empirical}
\textsc{Chapelle, O.} and \textsc{Li, L.} (2011).
\newblock An empirical evaluation of thompson sampling.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Gao et~al.(2019)Gao, Han, Ren and Zhou}]{NIPS2019_8341}
\textsc{Gao, Z.}, \textsc{Han, Y.}, \textsc{Ren, Z.} and \textsc{Zhou, Z.}
  (2019).
\newblock Batched multi-armed bandits problem.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Garivier and Capp{\'e}(2011)}]{garivier2011kl}
\textsc{Garivier, A.} and \textsc{Capp{\'e}, O.} (2011).
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock In \textit{Proceedings of the 24th annual conference on learning
  theory}.

\bibitem[{Garivier et~al.(2016)Garivier, Lattimore and
  Kaufmann}]{garivier2016explore}
\textsc{Garivier, A.}, \textsc{Lattimore, T.} and \textsc{Kaufmann, E.} (2016).
\newblock On explore-then-commit strategies.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jin et~al.(2020)Jin, Xu, Xiao and Gu}]{jin2020double}
\textsc{Jin, T.}, \textsc{Xu, P.}, \textsc{Xiao, X.} and \textsc{Gu, Q.}
  (2020).
\newblock Double explore-then-commit: Asymptotic optimality and beyond.
\newblock \textit{arXiv preprint arXiv:2002.09174} .

\bibitem[{Katehakis and Robbins(1995)}]{katehakis1995sequential}
\textsc{Katehakis, M.~N.} and \textsc{Robbins, H.} (1995).
\newblock Sequential choice from several populations.
\newblock \textit{Proceedings of the National Academy of Sciences of the United
  States of America} \textbf{92} 8584.

\bibitem[{Kaufmann(2016)}]{kaufmann2016bayesian}
\textsc{Kaufmann, E.} (2016).
\newblock On bayesian index policies for sequential resource allocation.
\newblock \textit{arXiv preprint arXiv:1601.01190} .

\bibitem[{Kaufmann et~al.(2012)Kaufmann, Korda and
  Munos}]{kaufmann2012thompson}
\textsc{Kaufmann, E.}, \textsc{Korda, N.} and \textsc{Munos, R.} (2012).
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In \textit{International conference on algorithmic learning theory}.
  Springer.

\bibitem[{Korda et~al.(2013)Korda, Kaufmann and Munos}]{korda2013thompson}
\textsc{Korda, N.}, \textsc{Kaufmann, E.} and \textsc{Munos, R.} (2013).
\newblock Thompson sampling for 1-dimensional exponential family bandits.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Lai and Robbins(1985)}]{lai1985asymptotically}
\textsc{Lai, T.~L.} and \textsc{Robbins, H.} (1985).
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \textit{Advances in applied mathematics} \textbf{6} 4--22.

\bibitem[{Lattimore(2015)}]{lattimore2015optimally}
\textsc{Lattimore, T.} (2015).
\newblock Optimally confident ucb: Improved regret for finite-armed bandits.
\newblock \textit{arXiv preprint arXiv:1507.07880} .

\bibitem[{Lattimore(2018)}]{lattimore2018refining}
\textsc{Lattimore, T.} (2018).
\newblock Refining the confidence level for optimistic bandit strategies.
\newblock \textit{The Journal of Machine Learning Research} \textbf{19}
  765--796.

\bibitem[{Lattimore and Szepesv{\'a}ri(2020)}]{lattimore2018bandit}
\textsc{Lattimore, T.} and \textsc{Szepesv{\'a}ri, C.} (2020).
\newblock \textit{Bandit algorithms}.
\newblock Cambridge University Press.

\bibitem[{Li and Chapelle(2012)}]{li2012open}
\textsc{Li, L.} and \textsc{Chapelle, O.} (2012).
\newblock Open problem: Regret bounds for thompson sampling.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Maillard et~al.(2011)Maillard, Munos and Stoltz}]{maillard2011finite}
\textsc{Maillard, O.-A.}, \textsc{Munos, R.} and \textsc{Stoltz, G.} (2011).
\newblock A finite-time analysis of multi-armed bandits problems with
  kullback-leibler divergences.
\newblock In \textit{Proceedings of the 24th annual Conference On Learning
  Theory}.

\bibitem[{M{\'e}nard and Garivier(2017)}]{menard2017minimax}
\textsc{M{\'e}nard, P.} and \textsc{Garivier, A.} (2017).
\newblock A minimax and asymptotically optimal algorithm for stochastic
  bandits.
\newblock In \textit{International Conference on Algorithmic Learning Theory}.

\bibitem[{Perchet et~al.(2016)Perchet, Rigollet, Chassang, Snowberg
  et~al.}]{perchet2016batched}
\textsc{Perchet, V.}, \textsc{Rigollet, P.}, \textsc{Chassang, S.},
  \textsc{Snowberg, E.} \textsc{et~al.} (2016).
\newblock Batched bandit problems.
\newblock \textit{The Annals of Statistics} \textbf{44} 660--681.

\bibitem[{Pike-Burke et~al.(2018)Pike-Burke, Agrawal, Szepesvari and
  Grunewalder}]{pike2018bandits}
\textsc{Pike-Burke, C.}, \textsc{Agrawal, S.}, \textsc{Szepesvari, C.} and
  \textsc{Grunewalder, S.} (2018).
\newblock Bandits with delayed, aggregated anonymous feedback.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Russo and Van~Roy(2014)}]{russo2014learning}
\textsc{Russo, D.} and \textsc{Van~Roy, B.} (2014).
\newblock Learning to optimize via posterior sampling.
\newblock \textit{Mathematics of Operations Research} \textbf{39} 1221--1243.

\bibitem[{Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband and
  Wen}]{russo2018tutorial}
\textsc{Russo, D.~J.}, \textsc{Van~Roy, B.}, \textsc{Kazerouni, A.},
  \textsc{Osband, I.} and \textsc{Wen, Z.} (2018).
\newblock A tutorial on thompson sampling.
\newblock \textit{Foundations and Trends{\textregistered} in Machine Learning}
  \textbf{11} 1--96.

\bibitem[{Thompson(1933)}]{thompson1933likelihood}
\textsc{Thompson, W.~R.} (1933).
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \textit{Biometrika} \textbf{25} 285--294.

\bibitem[{Vaswani et~al.(2019)Vaswani, Mehrabian, Durand and
  Kveton}]{vaswani2019old}
\textsc{Vaswani, S.}, \textsc{Mehrabian, A.}, \textsc{Durand, A.} and
  \textsc{Kveton, B.} (2019).
\newblock Old dog learns new tricks: Randomized ucb for bandit problems.
\newblock \textit{arXiv preprint arXiv:1910.04928} .

\bibitem[{Wang and Chen(2018)}]{wang2018thompson}
\textsc{Wang, S.} and \textsc{Chen, W.} (2018).
\newblock Thompson sampling for combinatorial semi-bandits.
\newblock In \textit{International Conference on Machine Learning}.

\end{thebibliography}
