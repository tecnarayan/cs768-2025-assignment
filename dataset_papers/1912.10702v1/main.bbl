\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alemi et~al.(2017)Alemi, Poole, Fischer, Dillon, Saurous, and
  Murphy]{alemi2017fixing}
A.~Alemi, B.~Poole, I.~Fischer, J.~Dillon, R.~Saurous, and K.~Murphy.
\newblock Fixing a broken {ELBO}.
\newblock \emph{ar{X}iv preprint ar{X}iv:1711.00464}, 2017.

\bibitem[Bauer \& Mnih(2018)Bauer and Mnih]{bauer2018resampled}
M.~Bauer and A.~Mnih.
\newblock Resampled priors for variational autoencoders.
\newblock \emph{ar{X}iv preprint ar{X}iv:1810.11428}, 2018.

\bibitem[Bauer \& Mnih(2019)Bauer and Mnih]{bauer2019resampled}
M.~Bauer and A.~Mnih.
\newblock Resampled priors for variational autoencoders.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2019.

\bibitem[Bowman et~al.(2015)Bowman, Vilnis, Vinyals, Dai, Jozefowicz, and
  Bengio]{bowman2015generating}
S.~Bowman, L.~Vilnis, O.~Vinyals, A.~Dai, R.~Jozefowicz, and S.~Bengio.
\newblock Generating sentences from a continuous space.
\newblock \emph{ar{X}iv preprint ar{X}iv:1511.06349}, 2015.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Y.~Burda, R.~Grosse, and R.~Salakhutdinov.
\newblock Importance weighted autoencoders.
\newblock \emph{ar{X}iv preprint ar{X}iv:1509.00519}, 2015.

\bibitem[Cai et~al.(2017)Cai, Gao, and Ji]{cai2017multi}
L.~Cai, H.~Gao, and S.~Ji.
\newblock Multi-stage variational auto-encoders for coarse-to-fine image
  generation.
\newblock \emph{ar{X}iv preprint ar{X}iv:1705.07202}, 2017.

\bibitem[Cand\`{e}s et~al.(2011)Cand\`{e}s, Li, Ma, and Wright]{Candes11}
E.~Cand\`{e}s, X.~Li, Y.~Ma, and J.~Wright.
\newblock Robust principal component analysis?
\newblock \emph{J. ACM}, 58\penalty0 (2), 2011.

\bibitem[Chen et~al.(2016)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{chen2016variational}
X.~Chen, D.~Kingma, T.~Salimans, Y.~Duan, P.~Dhariwal, J.~Schulman,
  I.~Sutskever, and P.~Abbeel.
\newblock Variational lossy autoencoder.
\newblock \emph{ar{X}iv preprint ar{X}iv:1611.02731}, 2016.

\bibitem[Dai \& Wipf(2019)Dai and Wipf]{bin2019iclr}
B.~Dai and D.~Wipf.
\newblock Diagnosing and enhancing {VAE} models.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Dai et~al.(2019)Dai, Wang, Aston, Hua, and Wipf]{dai2017hidden}
B.~Dai, Y.~Wang, J.~Aston, G.~Hua, and D.~Wipf.
\newblock Hidden talents of the variational autoencoder.
\newblock \emph{ar{X}iv preprint ar{X}iv:1706.05148}, 2019.

\bibitem[Dieng et~al.(2018)Dieng, Kim, Rush, and Blei]{dieng2018avoiding}
A.~Dieng, Y.~Kim, A.~Rush, and D.~Blei.
\newblock Avoiding latent variable collapse with generative skip models.
\newblock \emph{ar{X}iv preprint ar{X}iv:1807.04863}, 2018.

\bibitem[Gregor \& LeCun(2010)Gregor and LeCun]{Gregor10}
K.~Gregor and Y.~LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock \emph{International Conference on Machine Learning}, 2010.

\bibitem[He et~al.(2019)He, Spokoyny, Neubig, and
  Berg-Kirkpatrick]{he2019lagging}
J.~He, D.~Spokoyny, G.~Neubig, and T.~Berg-Kirkpatrick.
\newblock Lagging inference networks and posterior collapse in variational
  autoencoders.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2015deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CVPR}, 2016.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, , and Lerchner]{higgins2017}
I.~Higgins, L.~Matthey, A.~Pal, C.~Burgess, X.~Glorot, M.~Botvinick,
  S.~Mohamed, , and A.~Lerchner.
\newblock $\beta$-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Huang et~al.(2018)Huang, Tan, Lacoste, and
  Courville]{huang2018improving}
C.~Huang, S.~Tan, A.~Lacoste, and A.~Courville.
\newblock Improving explorability in variational inference with annealed
  variational objectives.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Kawaguchi(2016)]{Kawaguchi2016}
K.~Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{Kingma2014}
D.~Kingma and M.~Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Li \& Nguyen(2019)Li and Nguyen]{li2019random}
P.~Li and P.M. Nguyen.
\newblock On random deep weight-tied autoencoders: Exact asymptotic analysis,
  phase transitions, and implications to training.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lucas et~al.(2019)Lucas, Tucker, Grosse, and
  Norouzi]{lucas2019understanding}
J.~Lucas, G.~Tucker, R.~Grosse, and M.~Norouzi.
\newblock Understanding posterior collapse in generative latent variable
  models.
\newblock \emph{International Conference on Learning Representations, Workshop
  Paper}, 2019.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
L.~Maal{\o}e, M.~Fraccaro, V.~Li{\'e}vin, and O.~Winther.
\newblock {BIVA}: {A} very deep hierarchy of latent variables for generative
  modeling.
\newblock \emph{ar{X}iv preprint ar{X}iv:1902.02102}, 2019.

\bibitem[Makhzani et~al.(2016)Makhzani, Shlens, Jaitly, Goodfellow, and
  Frey]{makhzani2016}
A.~Makhzani, J.~Shlens, N.~Jaitly, I.~Goodfellow, and B.~Frey.
\newblock Adversarial autoencoders.
\newblock \emph{ar{X}iv preprint ar{X}iv:1511.05644}, 2016.

\bibitem[Mattei \& Frellsen(2018)Mattei and Frellsen]{mattei2018leveraging}
P.A. Mattei and J.~Frellsen.
\newblock Leveraging the exact likelihood of deep latent variable models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Orjebin(2014)]{Orjebin14}
E.~Orjebin.
\newblock A recursive formula for the moments of a truncated univariate normal
  distribution.
\newblock 2014.
\newblock URL
  \url{https://people.smp.uq.edu.au/YoniNazarathy/teaching_projects/studentWork/EricOrjebin_TruncatedNormalMoments.pdf}.

\bibitem[Razavi et~al.(2019)Razavi, Oord, Poole, and
  Vinyals]{razavi2019preventing}
A.~Razavi, A.~Oord, B.~Poole, and O.~Vinyals.
\newblock Preventing posterior collapse with $\delta$-{VAE}s.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende2015variational}
D.~Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock \emph{ar{X}iv preprint ar{X}iv:1505.05770}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{Rezende2014}
D.~Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{International Conference on Machine Learning}, 2014.

\bibitem[Rolinek et~al.(2019)Rolinek, Zietlow, and
  Martius]{rolinek2019variational}
M.~Rolinek, D.~Zietlow, and G.~Martius.
\newblock Variational autoencoders pursue {PCA} directions (by accident).
\newblock 2019.

\bibitem[S{\o}nderby et~al.(2016)S{\o}nderby, Raiko, Maal{\o}e, S{\o}nderby,
  and Winther]{sonderby2016train}
C.~S{\o}nderby, T.~Raiko, L.~Maal{\o}e, S.~S{\o}nderby, and O.~Winther.
\newblock How to train deep variational autoencoders and probabilistic ladder
  networks.
\newblock \emph{ar{X}iv preprint ar{X}iv:1602.02282}, 2016.

\bibitem[Sprechmann et~al.(2015)Sprechmann, Bronstein, and
  Sapiro]{Sprechmann15}
P.~Sprechmann, A.M. Bronstein, and G.~Sapiro.
\newblock Learning efficient sparse and low rank models.
\newblock \emph{IEEE Trans. Pattern Analysis and Machine Intelligence},
  37\penalty0 (9), 2015.

\bibitem[Tipping \& Bishop(1999)Tipping and Bishop]{Tipping1999}
M.~Tipping and C.~Bishop.
\newblock Probabilistic principal component analysis.
\newblock \emph{J. Royal Statistical Society, Series B}, 61\penalty0
  (3):\penalty0 611--622, 1999.

\bibitem[Tomczak \& Welling(2018)Tomczak and Welling]{tomczak2018vae}
J.~Tomczak and M.~Welling.
\newblock {VAE} with a {V}amp{P}rior.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2018.

\bibitem[Van~den Oord et~al.(2016)Van~den Oord, Kalchbrenner, Espeholt,
  Vinyals, Graves, and Kavukcuoglu]{van2016conditional}
A.~Van~den Oord, N.~Kalchbrenner, L.~Espeholt, O.~Vinyals, A.~Graves, and
  K.~Kavukcuoglu.
\newblock Conditional image generation with {P}ixel{CNN} decoders.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yeung et~al.(2017)Yeung, Kannan, Dauphin, and
  Fei-Fei]{yeung2017tackling}
S.~Yeung, A.~Kannan, Y.~Dauphin, and L.~Fei-Fei.
\newblock Tackling over-pruning in variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1706.03643}, 2017.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2018small}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhao \& Yu(2006)Zhao and Yu]{zhao2006model}
P.~Zhao and B.~Yu.
\newblock On model selection consistency of {L}asso.
\newblock \emph{Journal of Machine learning research}, 7:\penalty0 2541--2563,
  2006.

\end{thebibliography}
