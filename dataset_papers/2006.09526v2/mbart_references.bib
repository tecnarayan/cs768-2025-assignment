@string{iclr = "International Conference on Learning Representations (ICLR)"}
@string{aaai = "Conference on Artificial Intelligence (AAAI)"}
@string{emnlp = "Empirical Methods in Natural Language Processing (EMNLP)"}
@string{acl = "Association for Computational Linguistics (ACL)"}
@string{acl_demo = "Association for Computational Linguistics (ACL): System Demonstrations"}
@string{aistats = "Artificial Intelligence and Statistics (AISTATS)"}
@string{nips = "Advances in Neural Information Processing Systems (NIPS)"}
@string{icml = "International Conference on Machine Learning (ICML)"}
@string{naacl = "North American Association for Computational Linguistics (NAACL)"}
@string{naacl_demo = "North American Association for Computational Linguistics (NAACL): System Demonstrations"}
@string{conll = "Computational Natural Language Learning (CoNLL)"}
@string{ijcnlp = "International Joint Conference on Natural Language Processing (IJCNLP)"}
@string{cvpr = "Conference on computer vision and pattern recognition (CVPR)"}
@string{iccv = "International Conference on Computer Vision (ICCV)"}
@string{acl_hlt = "Association for Computational Linguistics: Human Language Technologies (ACL-HLT)"}
@string{jmlr = "The Journal of Machine Learning Research (JMLR)"}
@string{tacl = "Transactions of the Association of Computational Linguistics (TACL)"}
@string{lrec = "International Conference on Language Resources and Evaluation (LREC)"}
@string{coling = "International Conference on Computational Linguistics (COLING)"}
@string{cl = "Computational Linguistics"}


@article{joshi2019spanbert,
    title={{SpanBERT}: Improving Pre-training by Representing and Predicting Spans},
    author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
    year={2019},
    journal={arXiv preprint arXiv:1907.10529}
}

@inproceedings{zhang2019ernie,
  title={{ERNIE}: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle=acl,
  year={2019}
}

@article{sun2019ernie,
  title={{ERNIE}: Enhanced Representation through Knowledge Integration},
  author={Yu Stephanie Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Xuyi Chen and Han Zhang and Xinlun Tian and Danxiang Zhu and Hao Tian and Hua Wu},
  journal={arXiv preprint arXiv:1904.09223},
  year={2019}
}

@article{dong2019unified,
  title={Unified Language Model Pre-training for Natural Language Understanding and Generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={arXiv preprint arXiv:1905.03197},
  year={2019}
}


@article{chan2019kermit,
  title={{KERMIT}: Generative Insertion-Based Modeling for Sequences},
  author={Chan, William and Kitaev, Nikita and Guu, Kelvin and Stern, Mitchell and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1906.01604},
  year={2019}
}

@inproceedings{mccann2017learned,
  title={Learned in translation: Contextualized word vectors},
  author={McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  booktitle=nips,
  pages={6297--6308},
  year={2017}
}

@inproceedings{devlin2018bert,
    title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    booktitle=naacl,
    year={2019}
}

@article{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@inproceedings{moviebook,
    title = {Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
    author = {Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
    booktitle = {arXiv preprint arXiv:1506.06724},
    year = {2015}
}

@article{baevski2019cloze,
  title={Cloze-driven pretraining of self-attention networks},
  author={Baevski, Alexei and Edunov, Sergey and Liu, Yinhan and Zettlemoyer, Luke and Auli, Michael},
  journal={arXiv preprint arXiv:1903.07785},
  year={2019}
}

@misc{gokaslan2019openwebtext,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\path{http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@misc{xlnetteam2019fair,  
	title={A Fair Comparison Study of XLNet and BERT with Large Models},
	author={\text{XLNet Team}},
	howpublished={\path{https://web.archive.org/web/20190725164152/https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0}}, 
	year={2019}
}

@article{you2019reducing,
  title={Reducing BERT Pre-Training Time from 3 Days to 76 Minutes},
  author={You, Yang and Li, Jing and Hseu, Jonathan and Song, Xiaodan and Demmel, James and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@inproceedings{loshchilov2018decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@misc{iyer2016quora,
    title={First Quora Dataset Release: Question Pairs},
    author={Shankar Iyer and Nikhil Dandekar and Kornél Csernai},
    howpublished={\path{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}},
    year={2016}
}


@misc{nagel2016ccnews,
    title={CC-NEWS},
    author={Sebastian Nagel},
    howpublished={\path{http://web.archive.org/save/http://commoncrawl.org/2016/10/news-dataset-available}},
    year={2016}
}

@InProceedings{hamborg2017newsplease,
  author     = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},
  title      = {news-please: A Generic News Crawler and Extractor},
  year       = {2017},
  booktitle  = {Proceedings of the 15th International Symposium of Information Science}
}

@article{zellers2019neuralfakenews,
    title={Defending Against Neural Fake News},
    author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
    journal={arXiv preprint arXiv:1905.12616},
    year={2019}
}

@inproceedings{wang2019glue,
     title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
     author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
     booktitle=iclr,
     year={2019}
}

@InProceedings{conneau2018xnli,
  author = "Conneau, Alexis
        and Rinott, Ruty
        and Lample, Guillaume
        and Williams, Adina
        and Bowman, Samuel R.
        and Schwenk, Holger
        and Stoyanov, Veselin",
  title = "XNLI: Evaluating Cross-lingual Sentence Representations",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods
               in Natural Language Processing",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  location = "Brussels, Belgium",
}

@article{wang2019superglue,
   title={Super{GLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
   author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
   journal={arXiv preprint 1905.00537},
   year={2019}
 }
 @inproceedings{clark2019boolq,
   title={{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
   author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
   booktitle={Proceedings of NAACL-HLT 2019},
   year={2019}
 }
 @inproceedings{demarneffe:cb,
   title={{The CommitmentBank}: Investigating projection in naturally occurring discourse},
   author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
   note={To appear in proceedings of Sinn und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/},
   year={2019}
 }
 @inproceedings{roemmele2011choice,
   title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
   author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S.},
   booktitle={2011 AAAI Spring Symposium Series},
   year={2011}
 }
 @inproceedings{khashabi2018looking,
   title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
   author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
   booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
   pages={252--262},
   year={2018}
 }
 @article{zhang2018record,
   title={{ReCoRD}: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
   author={Sheng Zhang and Xiaodong Liu and Jingjing Liu and Jianfeng Gao and Kevin Duh and Benjamin Van Durme},
   journal={arXiv preprint 1810.12885},
   year={2018}
 }
 @incollection{dagan2006pascal,
   title={The {PASCAL} recognising textual entailment challenge},
   author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
   booktitle={Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment},
   pages={177--190},
   year={2006},
   publisher={Springer}
 }
 @article{bar2006second,
   title={The second {PASCAL} recognising textual entailment challenge},
   author={Bar Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
   year={2006}
 }
 @inproceedings{giampiccolo2007third,
   title={The third {PASCAL} recognizing textual entailment challenge},
   author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},
   booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
   pages={1--9},
   year={2007},
   organization={Association for Computational Linguistics},
 }
 @article{bentivogli2009fifth,
   title={The Fifth {PASCAL} Recognizing Textual Entailment Challenge},
   author={Bentivogli, Luisa and Dagan, Ido and Dang, Hoa Trang and Giampiccolo, Danilo and Magnini, Bernardo},
   booktitle={TAC},
   year={2009}
 }
 @inproceedings{pilehvar2018wic,
   title={{WiC}: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
   author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
   booktitle={Proceedings of NAACL-HLT},
   year={2019}
 }
 @inproceedings{rudinger2018winogender,
   title={Gender Bias in Coreference Resolution},
   author={Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},
   booktitle={Proceedings of NAACL-HLT},
   year={2018}
 }
 @inproceedings{poliak2018dnc,
   title={Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation},
   author={Poliak, Adam and Haldar, Aparajita and Rudinger, Rachel and Hu, J. Edward and Pavlick, Ellie and White, Aaron Steven and {Van Durme}, Benjamin},
   booktitle={Proceedings of EMNLP},
   year={2018}
 }
 @inproceedings{levesque2011winograd,
   title={The {W}inograd schema challenge},
   author={Levesque, Hector J and Davis, Ernest and Morgenstern, Leora},
   booktitle={{AAAI} Spring Symposium: Logical Formalizations of Commonsense Reasoning},
   volume={46},
   pages={47},
   year={2011}
 }
 
@inproceedings{paszke2017automatic,
  title={Automatic Differentiation in {PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS Autodiff Workshop},
  year={2017}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle=acl,
  pages={1715--1725},
  year={2016}
}

@article{wang2019superglue,
   title={Super{GLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
   author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
   journal={arXiv preprint 1905.00537},
   year={2019}
}

@article{liu2019improving,
  title={Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1904.09482},
  year={2019}
}

@article{kocijan2019surprisingly,
  title={A Surprisingly Robust Trick for Winograd Schema Challenge},
  author={Kocijan, Vid and Cretu, Ana-Maria and Camburu, Oana-Maria and Yordanov, Yordan and Lukasiewicz, Thomas},
  journal={arXiv preprint arXiv:1905.06290},
  year={2019}
}

@article{phang2018stilts,
  title={Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},
  author={Jason Phang and Thibault Févry and Samuel R. Bowman},
  journal={arXiv preprint arXiv:1811.01088},
  year={2018}
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@inproceedings{micikevicius2018mixed,
    title={Mixed Precision Training},
    author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
    booktitle={International Conference on Learning Representations},
    year={2018},
}

@article{lewis2019bart,
    title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension},
    author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and
              Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov
              and Luke Zettlemoyer },
    journal={arXiv preprint arXiv:1910.13461},
    year = {2019},
}

@article{liu2019roberta,
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and
              Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and
              Luke Zettlemoyer and Veselin Stoyanov},
    journal={arXiv preprint arXiv:1907.11692},
    year = {2019},
}

@inproceedings{ott2019fairseq,
  title = {\textsc{fairseq}: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = naacl_demo,
  year = {2019},
}

@article{liu2019mtdnn,
  title={Multi-Task Deep Neural Networks for Natural Language Understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1901.11504},
  year={2019}
}


@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle=nips,
  pages={1731--1741},
  year={2017}
}


@article{shi2019simple,
  author    = {Peng Shi and Jimmy Lin},
  title     = {Simple {BERT} Models for Relation Extraction and Semantic Role Labeling},
  journal={arXiv preprint 1904.05255},
  year={2019}
}

@inproceedings{zhang2019ernie,
  title={{ERNIE}: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle=acl,
  year={2019}
}

@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@inproceedings{zhang2017tacred,
    title={Position-aware Attention and Supervised Data Improve Slot Filling},
    author={Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
    booktitle=emnlp,
    year={2017}
}

@inproceedings{trischler2017newsqa,
  title={NewsQA: A Machine Comprehension Dataset},
  author={Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  booktitle={2nd Workshop on Representation Learning for NLP},
  year={2017}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{zellers2018swag,
  title={Swag: A large-scale adversarial dataset for grounded commonsense inference},
  author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  journal={arXiv preprint arXiv:1808.05326},
  year={2018}
}

@inproceedings{rajpurkar2018know,
  title={Know What You Don't Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle=acl,
  year={2018}
}

@inproceedings{yang2018hotpotqa,
  title={Hotpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D},
  booktitle=emnlp,
  year={2018}
}

@article{dunn2017searchqa,
  title={Searchqa: A new q\&a dataset augmented with context from a search engine},
  author={Dunn, Matthew and Sagun, Levent and Higgins, Mike and Guney, V Ugur and Cirik, Volkan and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1704.05179},
  year={2017}
}

@inproceedings{joshi2017triviaqa,
  title={{TriviaQA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
  booktitle=acl,
  year={2017}
}

@inproceedings{lee2018higher,
    title="Higher-Order Coreference Resolution with Coarse-to-Fine Inference",
    author={Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
    booktitle = naacl,
    year={2018}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  year={2017}
}

@article{kwiatkowski2019natural,
  title	= {Natural Questions: a Benchmark for Question Answering Research},
  author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
  year	= {2019},
  journal	= tacl
}

@inproceedings{peters2018deep,
  title={Deep Contextualized Word Representations},
  author={Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle=naacl,
  year={2018}
}



@inproceedings{dagan2005pascal,
  title={The PASCAL recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  year={2005},
  organization={Springer}
}

@inproceedings{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  booktitle=emnlp,
  year={2015}
}

@article{lee2016learning,
  title={Learning recurrent span representations for extractive question answering},
  author={Lee, Kenton and Salant, Shimi and Kwiatkowski, Tom and Parikh, Ankur and Das, Dipanjan and Berant, Jonathan},
  journal={arXiv preprint arXiv:1611.01436},
  year={2016}
}
@inproceedings{lee2017end,
  title={End-to-end Neural Coreference Resolution},
  author={Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
  booktitle=emnlp,
  pages={188--197},
  year={2017}
}

@inproceedings{song2019mass,
    title={{MASS}: Masked Sequence to Sequence Pre-training for Language Generation},
    author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
    booktitle=icml,
    year={2019}
}

@inproceedings{he2018jointly,
  title={Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling},
  author={He, Luheng and Lee, Kenton and Levy, Omer and Zettlemoyer, Luke},
  booktitle={Proceedings of ACL},
  pages={364--369},
  year={2018}
}

@inproceedings{pradhan2012conll,
  title={CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes},
  author={Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Uryupina, Olga and Zhang, Yuchen},
  booktitle={Joint Conference on EMNLP and CoNLL-Shared Task},
  pages={1--40},
  year={2012}
}

@article{warstadt2018neural,
   title={Neural Network Acceptability Judgments},
   author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
   journal={arXiv preprint 1805.12471},
   year={2018}
 }

 @inproceedings{socher2013recursive,
   title={Recursive deep models for semantic compositionality over a sentiment treebank},
   author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
   booktitle=emnlp,
   year={2013}
 }

 @inproceedings{dolan2005automatically,
   title={Automatically constructing a corpus of sentential paraphrases},
   author={Dolan, William B and Brockett, Chris},
   booktitle={Proceedings of the International Workshop on Paraphrasing},
   year={2005}
 }

 @book{agirre2007semantic,
   editor    = {Agirre, Eneko and  M{\`a}rquez, Llu{\'\i}s and Wicentowski, Richard},
   title     = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
   year      = {2007}
 }
 


 @inproceedings{williams2018broad,
   title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
   author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
   booktitle=naacl,
   year={2018}
 }

 @inproceedings{rajpurkar2016squad,
   title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
   author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
   booktitle=emnlp,
   year={2016}
 }
 
 @incollection{dagan2006pascal,
   title={The {PASCAL} recognising textual entailment challenge},
   author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
   booktitle={Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment},
   year={2006}
 }

 @inproceedings{bar2006second,
    title={The second {PASCAL} recognising textual entailment challenge},
    author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
    booktitle={Proceedings of the second PASCAL challenges workshop on recognising textual entailment},
    year={2006}
}

 @inproceedings{giampiccolo2007third,
   title={The third {PASCAL} recognizing textual entailment challenge},
   author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},
   booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
   year={2007}
 }

 @article{bentivogli2009fifth,
   title={The Fifth {PASCAL} Recognizing Textual Entailment Challenge},
   author={Bentivogli, Luisa and Dagan, Ido and Dang, Hoa Trang and Giampiccolo, Danilo and Magnini, Bernardo},
   booktitle={TAC},
   year={2009}
 }

 @inproceedings{levesque2011winograd,
   title={The {W}inograd schema challenge},
   author={Levesque, Hector J and Davis, Ernest and Morgenstern, Leora},
   booktitle={{AAAI} Spring Symposium: Logical Formalizations of Commonsense Reasoning},
   year={2011}
 }

@article{lample2019cross,
  title={Cross-lingual Language Model Pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@inproceedings{mikolov2013distributed,
  title={Distributed Representations of Words and Phrases and their Compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle=nips,
  pages={3111--3119},
  year={2013}
}

@article{brown1992class,
  title={Class-Based n-gram Models of Natural Language},
  author={Peter F. Brown and Vincent J. Della Pietra and Peter V. de Souza and Jennifer C. Lai and Robert L. Mercer},
  journal={Computational Linguistics},
  year={1992},
  volume={18},
  pages={467-479}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  booktitle=iclr,
  year={2015}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = emnlp,
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@inproceedings{dai2015semi,
  title={Semi-supervised sequence learning},
  author={Dai, Andrew M and Le, Quoc V},
  booktitle=nips,
  year={2015}
}

@article{harris1954,
  author = {Harris, Zellig},
  journal = {Word},
  number = "23",
  pages = {146--162},
  title = {Distributional structure},
  volume = "10",
  year = {1954}
}

@inproceedings{turian2010word,
  title={Word Representations: A Simple and General Method for Semi-Supervised Learning},
  author={Turian, Joseph and Ratinov, Lev-Arie and Bengio, Yoshua},
  booktitle=acl,
  pages={384--394},
  year={2010}
}

@article{bojanowski2017enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal=tacl,
  volume={5},
  year={2017},
  pages={135--146}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}
@inproceedings{melamud2016context2vec,
    title = "context2vec: Learning Generic Context Embedding with Bidirectional {LSTM}",
    author = "Melamud, Oren  and
      Goldberger, Jacob  and
      Dagan, Ido",
    booktitle = "Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    pages = "51--61",
}

@inproceedings{Kiros2015SkipThoughtV,
  title={Skip-Thought Vectors},
  author={Ryan Kiros and Yukun Zhu and Ruslan R. Salakhutdinov and Richard S. Zemel and Antonio Torralba and Raquel Urtasun and Sanja Fidler},
  booktitle={NIPS},
  year={2015}
}

@techreport{radford2018gpt,
  title={Improving language understanding with unsupervised learning},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Time and Sutskever, Ilya},
  year={2018},
  institution={OpenAI}
}

@techreport{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  institution={OpenAI}
}

@article{Logeswaran2018AnEF,
  title={An efficient framework for learning sentence representations},
  author={Lajanugen Logeswaran and Honglak Lee},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.02893}
}

@article{hendrycks2016gelu,
  title={Gaussian Error Linear Units (GELUs)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{ott2018scaling,
  title = {Scaling Neural Machine Translation},
  author = {Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  booktitle = {Proceedings of the Third Conference on Machine Translation (WMT)},
  year = 2018,
}

@article{lai2017large,
    title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
    author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
    journal={arXiv preprint arXiv:1704.04683},  
    year={2017}
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}
@inproceedings{guzman-etal-2019-flores,
    title = "The {FLORES} Evaluation Datasets for Low-Resource Machine Translation: {N}epali{--}{E}nglish and {S}inhala{--}{E}nglish",
    author = "Guzm{\'a}n, Francisco  and
      Chen, Peng-Jen  and
      Ott, Myle  and
      Pino, Juan  and
      Lample, Guillaume  and
      Koehn, Philipp  and
      Chaudhary, Vishrav  and
      Ranzato, Marc{'}Aurelio",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1632",
    doi = "10.18653/v1/D19-1632",
    pages = "6097--6110",
    abstract = "For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLORES evaluation datasets for Nepali{--}English and Sinhala{--} English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores.",
}
@article{conneau2019unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@inproceedings{cnn,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in neural information processing systems},
  pages={1693--1701},
  year={2015}
}
@article{xsum,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}
@article{eli5,
  title={Eli5: Long form question answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  journal={arXiv preprint arXiv:1907.09190},
  year={2019}
}
@article{2019t5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {arXiv e-prints},
  year = {2019},
  archivePrefix = {arXiv},
  eprint = {1910.10683},
}
@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}
@article{DBLP:journals/corr/abs-1710-02855,
  author    = {Anoop Kunchukuttan and
               Pratik Mehta and
               Pushpak Bhattacharyya},
  title     = {The {IIT} Bombay English-Hindi Parallel Corpus},
  journal   = {CoRR},
  volume    = {abs/1710.02855},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.02855},
  archivePrefix = {arXiv},
  eprint    = {1710.02855},
  timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-02855},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{neubig11kftt,
	author = {Graham Neubig},
	title = {The {Kyoto} Free Translation Task},
	howpublished = {http://www.phontron.com/kftt},
	year = {2011}
}

@inproceedings{cettolo2012wit3,
  title={Wit3: Web inventory of transcribed and translated talks},
  author={Cettolo, Mauro and Girardi, Christian and Federico, Marcello},
  booktitle={Conference of European Association for Machine Translation},
  pages={261--268},
  year={2012}
}

@inproceedings{cettolo2015iwslt,
  title={The IWSLT 2015 evaluation campaign},
  author={Cettolo, Mauro and Jan, Niehues and Sebastian, St{\"u}ker and Bentivogli, Luisa and Cattoni, Roldano and Federico, Marcello},
  booktitle={International Workshop on Spoken Language Translation},
  year={2015}
}

@inproceedings{miculicich-etal-2018-document,
    title = "Document-Level Neural Machine Translation with Hierarchical Attention Networks",
    author = "Miculicich, Lesly  and
      Ram, Dhananjay  and
      Pappas, Nikolaos  and
      Henderson, James",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1325",
    doi = "10.18653/v1/D18-1325",
    pages = "2947--2954",
    abstract = "Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model{'}s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.",
}
@inproceedings{Cettolo2017OverviewOT,
  title={Overview of the IWSLT 2017 Evaluation Campaign},
  author={Mauro Cettolo and Marcello Federico and Luisa Bentivogli and Jan Niehues and Sebastian St{\"u}ker and Kumiko Sudoh and Kyotaro Yoshino and Christian Federmann},
  year={2017}
}
@article{chen2019facebook,
  title={Facebook AI's WAT19 Myanmar-English Translation Task Submission},
  author={Chen, Peng-Jen and Shen, Jiajun and Le, Matt and Chaudhary, Vishrav and El-Kishky, Ahmed and Wenzek, Guillaume and Ott, Myle and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1910.06848},
  year={2019}
}

@inproceedings{shen2004discriminative,
  title={Discriminative reranking for machine translation},
  author={Shen, Libin and Sarkar, Anoop and Och, Franz Josef},
  booktitle={Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004},
  pages={177--184},
  year={2004}
}
@article{Yu2016TheNN,
  title={The Neural Noisy Channel},
  author={Lei Yu and Phil Blunsom and Chris Dyer and Edward Grefenstette and Tom{\'a}s Kocisk{\'y}},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.02554}
}
@article{li2019pretrained,
  title={Pretrained Language Models for Document-Level Neural Machine Translation},
  author={Li, Liangyou and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:1911.03110},
  year={2019}
}
@article{artetxe2017unsupervised,
  title={Unsupervised neural machine translation},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1710.11041},
  year={2017}
}
@article{lample2018phrase,
  title={Phrase-based \& neural unsupervised machine translation},
  author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1804.07755},
  year={2018}
}
@article{johnson2017google,
  title={Google’s multilingual neural machine translation system: Enabling zero-shot translation},
  author={Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={339--351},
  year={2017},
  publisher={MIT Press}
}
@article{gu2019improved,
  title={Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations},
  author={Gu, Jiatao and Wang, Yong and Cho, Kyunghyun and Li, Victor OK},
  journal={arXiv preprint arXiv:1906.01181},
  year={2019}
}
@article{wu2019extract,
  title={Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation},
  author={Wu, Jiawei and Wang, Xin and Wang, William Yang},
  journal={arXiv preprint arXiv:1904.02331},
  year={2019}
}
@inproceedings{chen2017teacher,
  title={A Teacher-Student Framework for Zero-Resource Neural Machine Translation},
  author={Chen, Yun and Liu, Yang and Cheng, Yong and Li, Victor OK},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1925--1935},
  year={2017}
}
@article{wenzek2019ccnet,
  title={Ccnet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzman, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}
@article{he2019revisiting,
  title={Revisiting Self-Training for Neural Sequence Generation},
  author={He, Junxian and Gu, Jiatao and Shen, Jiajun and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1909.13788},
  year={2019}
}
@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}
@article{edunov2019pre,
  title={Pre-trained language model representations for language generation},
  author={Edunov, Sergey and Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1903.09722},
  year={2019}
}
@article{liu2019text,
  title={Text Summarization with Pretrained Encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}
@misc{zhang2019dialogpt,
    title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
    author={Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
    year={2019},
    eprint={1911.00536},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{shirish2019ctrl,
  title={CTRL: A Conditional Transformer Language Model for Controllable Generation},
  author={Shirish Keskar, Nitish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}
@article{DBLP:journals/corr/MikolovLS13,
  author    = {Tomas Mikolov and
               Quoc V. Le and
               Ilya Sutskever},
  title     = {Exploiting Similarities among Languages for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1309.4168},
  year      = {2013},
  url       = {http://arxiv.org/abs/1309.4168},
  archivePrefix = {arXiv},
  eprint    = {1309.4168},
  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
}
@inproceedings{chen-cardie-2018-unsupervised,
    title = "Unsupervised Multilingual Word Embeddings",
    author = "Chen, Xilun  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1024",
    doi = "10.18653/v1/D18-1024",
    pages = "261--270",
    abstract = "Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources.",
}
@inproceedings{
lample2018word,
title={Word translation without parallel data},
author={Guillaume Lample and Alexis Conneau and Marc'Aurelio Ranzato and Ludovic Denoyer and Hervé Jégou},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H196sainb},
}
@article{DBLP:wada,
  author    = {Takashi Wada and
               Tomoharu Iwata},
  title     = {Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language
               Models},
  journal   = {CoRR},
  volume    = {abs/1809.02306},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.02306},
  archivePrefix = {arXiv},
  eprint    = {1809.02306},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
}
@inproceedings{firat2016multi,
    title={Multi-Way, Multilingual Neural Machine Translation with a Shared
        Attention Mechanism},
    author={Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
    booktitle={NAACL},
    year={2016}
}
@article{wu2016google,
   author = {{Wu}, Y. and {Schuster}, M. and {Chen}, Z. and {Le}, Q.~V. and 
	{Norouzi}, M. and {Macherey}, W. and {Krikun}, M. and {Cao}, Y. and 
	{Gao}, Q. and {Macherey}, K. and {Klingner}, J. and {Shah}, A. and 
	{Johnson}, M. and {Liu}, X. and {Kaiser}, {\L}. and {Gouws}, S. and 
	{Kato}, Y. and {Kudo}, T. and {Kazawa}, H. and {Stevens}, K. and 
	{Kurian}, G. and {Patil}, N. and {Wang}, W. and {Young}, C. and 
	{Smith}, J. and {Riesa}, J. and {Rudnick}, A. and {Vinyals}, O. and 
	{Corrado}, G. and {Hughes}, M. and {Dean}, J.},
    title = "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1609.08144},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Learning},
     year = 2016,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160908144W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{viegas2016google,
  title={Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
  author={Vi{\'e}gas, Fernanda and Corrado, Greg and Dean, Jeffrey and Hughes, Macduff and Wattenberg, Martin and Krikun, Maxim and Johnson, Melvin and Schuster, Mike and Thorat, Nikhil and Le, Quoc V and others},
  year={2016}
}
@inproceedings{aharoni-etal-2019-massively,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}
@article{DBLP:journals/corr/abs-1907-05019,
  author    = {Naveen Arivazhagan and
               Ankur Bapna and
               Orhan Firat and
               Dmitry Lepikhin and
               Melvin Johnson and
               Maxim Krikun and
               Mia Xu Chen and
               Yuan Cao and
               George Foster and
               Colin Cherry and
               Wolfgang Macherey and
               Zhifeng Chen and
               Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
               and Challenges},
  journal   = {CoRR},
  volume    = {abs/1907.05019},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05019},
  archivePrefix = {arXiv},
  eprint    = {1907.05019},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
}
@inproceedings{gu-etal-2018-universal,
    title = "Universal Neural Machine Translation for Extremely Low Resource Languages",
    author = "Gu, Jiatao  and
      Hassan, Hany  and
      Devlin, Jacob  and
      Li, Victor O.K.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1032",
    doi = "10.18653/v1/N18-1032",
    pages = "344--354",
    abstract = "In this paper, we propose a new universal machine translation approach focusing on languages with a limited amount of parallel data. Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language. The lexical part is shared through a Universal Lexical Representation to support multi-lingual word-level sharing. The sentence-level sharing is represented by a model of experts from all source languages that share the source encoders with all other languages. This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages. Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multi-lingual training and back-translation. Furthermore, we show that the proposed approach can achieve almost 20 BLEU on the same dataset through fine-tuning a pre-trained multi-lingual system in a zero-shot setting.",
}
@inproceedings{wang-etal-2017-exploiting-cross,
    title = "Exploiting Cross-Sentence Context for Neural Machine Translation",
    author = "Wang, Longyue  and
      Tu, Zhaopeng  and
      Way, Andy  and
      Liu, Qun",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1301",
    doi = "10.18653/v1/D17-1301",
    pages = "2826--2831",
    abstract = "In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.",
}
@article{DBLP:journals/corr/JeanLFC17,
  author    = {S{\'{e}}bastien Jean and
               Stanislas Lauly and
               Orhan Firat and
               Kyunghyun Cho},
  title     = {Does Neural Machine Translation Benefit from Larger Context?},
  journal   = {CoRR},
  volume    = {abs/1704.05135},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05135},
  archivePrefix = {arXiv},
  eprint    = {1704.05135},
  timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JeanLFC17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tiedemann-scherrer-2017-neural,
    title = "Neural Machine Translation with Extended Context",
    author = {Tiedemann, J{\"o}rg  and
      Scherrer, Yves},
    booktitle = "Proceedings of the Third Workshop on Discourse in Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4811",
    doi = "10.18653/v1/W17-4811",
    pages = "82--92",
    abstract = "We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.",
}
@article{doi:10.1162/tacla00029,
author = {Tu, Zhaopeng and Liu, Yang and Shi, Shuming and Zhang, Tong},
title = {Learning to Remember Translation History with a Continuous Cache},
journal = {Transactions of the Association for Computational Linguistics},
volume = {6},
number = {},
pages = {407-420},
year = {2018},
doi = {10.1162/tacl\_a\_00029},
    abstract = { Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost. }
}
@inproceedings{
lample2018unsupervised,
title={Unsupervised Machine Translation Using Monolingual Corpora Only},
author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkYTTf-AZ},
}
@misc{
wu2019machine,
title={Machine Translation With Weakly Paired Bilingual Documents},
author={Lijun Wu and Jinhua Zhu and Di He and Fei Gao and Xu Tan and Tao Qin and Tie-Yan Liu},
year={2019},
url={https://openreview.net/forum?id=ryza73R9tQ},
}
@inproceedings{wu-etal-2019-extract,
    title = "Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation",
    author = "Wu, Jiawei  and
      Wang, Xin  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1120",
    doi = "10.18653/v1/N19-1120",
    pages = "1173--1183",
    abstract = "The overreliance on large parallel corpora significantly limits the applicability of machine translation systems to the majority of language pairs. Back-translation has been dominantly used in previous approaches for unsupervised neural machine translation, where pseudo sentence pairs are generated to train the models with a reconstruction loss. However, the pseudo sentences are usually of low quality as translation errors accumulate during training. To avoid this fundamental issue, we propose an alternative but more effective approach, extract-edit, to extract and then edit real sentences from the target monolingual corpora. Furthermore, we introduce a comparative translation loss to evaluate the translated target sentences and thus train the unsupervised translation systems. Experiments show that the proposed approach consistently outperforms the previous state-of-the-art unsupervised machine translation systems across two benchmarks (English-French and English-German) and two low-resource language pairs (English-Romanian and English-Russian) by more than 2 (up to 3.63) BLEU points.",
}
@misc{artetxe2019crosslingual,
    title={On the Cross-lingual Transferability of Monolingual Representations},
    author={Mikel Artetxe and Sebastian Ruder and Dani Yogatama},
    year={2019},
    eprint={1910.11856},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@inproceedings{Pourdamghani2019TranslatingTA,
  title={Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation},
  author={Nima Pourdamghani and Nada Aldarrab and Marjan Ghazvininejad and Kevin Knight and Jonathan May},
  booktitle={ACL},
  year={2019}
}
@article{ding2018nova,
	title={{NOVA}: A Feasible and Flexible Annotation System for Joint Tokenization and Part-of-Speech Tagging},
	author={Ding, Chenchen and Utiyama, Masao and Sumita, Eiichiro},
	journal={ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},
	volume={18},
	number={2},
	pages={17},
	year={2018},
	publisher={ACM}
	}
@article{ding2019towards,
        title={Towards {Burmese} ({Myanmar}) Morphological Analysis: Syllable-based Tokenization and Part-of-speech Tagging},
        author={Ding, Chenchen and {Hnin Thu Zar Aye} and {Win Pa Pa} and {Khin Thandar Nwet} and {Khin Mar Soe} and Utiyama, Masao and Sumita, Eiichiro},
        journal={ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},
        volume={19},
        number={1},
        pages={5},
        year={2019},
        publisher={ACM}
        }
@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}
@inproceedings{post-2018-call,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}
@inproceedings{sennrich2016edinburgh,
  title={Edinburgh Neural Machine Translation Systems for WMT 16},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  pages={371--376},
  year={2016}
}
@inproceedings{ramachandran2017unsupervised,
  title={Unsupervised Pretraining for Sequence to Sequence Learning},
  author={Ramachandran, Prajit and Liu, Peter J and Le, Quoc},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={383--391},
  year={2017}
}
@article{qi2018and,
  title={When and why are pre-trained word embeddings useful for neural machine translation?},
  author={Qi, Ye and Sachan, Devendra Singh and Felix, Matthieu and Padmanabhan, Sarguna Janani and Neubig, Graham},
  journal={arXiv preprint arXiv:1804.06323},
  year={2018}
}
@article{yang2019towards,
  title={Towards making the most of bert in neural machine translation},
  author={Yang, Jiacheng and Wang, Mingxuan and Zhou, Hao and Zhao, Chengqi and Yu, Yong and Zhang, Weinan and Li, Lei},
  journal={arXiv preprint arXiv:1908.05672},
  year={2019}
}
@article{zhu2020incorporating,
  title={Incorporating BERT into Neural Machine Translation},
  author={Zhu, Jinhua and Xia, Yingce and Wu, Lijun and He, Di and Qin, Tao and Zhou, Wengang and Li, Houqiang and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2002.06823},
  year={2020}
}