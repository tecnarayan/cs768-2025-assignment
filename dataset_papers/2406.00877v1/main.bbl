\begin{thebibliography}{41}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{{\tt #1}}\fi

\bibitem[von Oswald et~al.(2023{\natexlab{a}})von Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{oswald2023transformers}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\~{a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock {\em ICML}, 2023{\natexlab{a}}.

\bibitem[von Oswald et~al.(2023{\natexlab{b}})von Oswald, Niklasson, Schlegel, Kobayashi, Zucchet, Scherrer, Miller, Sandler, y~Arcas, Vladymyrov, Pascanu, and Sacramento]{oswald2023uncovering}
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise~Agüera y~Arcas, Max Vladymyrov, Razvan Pascanu, and João Sacramento.
\newblock Uncovering mesa-optimization algorithms in transformers, 2023{\natexlab{b}}, arXiv:2309.05858.

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2023what}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock {\em ICLR}, 2023.

\bibitem[Brinkmann et~al.(2024)Brinkmann, Sheshadri, Levoso, Swoboda, and Bartelt]{brinkmann2024mechanistic}
Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt.
\newblock A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task, 2024, arXiv:2402.11917.

\bibitem[Ruoss et~al.(2024)Ruoss, Delétang, Medapati, Grau-Moya, Wenliang, Catt, Reid, and Genewein]{ruoss2024grandmasterlevel}
Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li~Kevin Wenliang, Elliot Catt, John Reid, and Tim Genewein.
\newblock Grandmaster-level chess without search, 2024, arXiv:2402.04494.

\bibitem[Veit et~al.(2016)Veit, Wilber, and Belongie]{veit2016}
Andreas Veit, Michael~J Wilber, and Serge Belongie.
\newblock Residual networks behave like ensembles of relatively shallow networks.
\newblock {\em NeurIPS}, 2016.

\bibitem[nostalgebraist(2020)]{logitlens}
nostalgebraist.
\newblock Interpreting {GPT}: the logit lens, 2020.
\newblock URL \url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}.

\bibitem[Belrose et~al.(2023)Belrose, Furman, Smith, Halawi, Ostrovsky, McKinney, Biderman, and Steinhardt]{tunedlens}
Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt.
\newblock Eliciting latent predictions from transformers with the tuned lens, 2023, arXiv:2303.08112.

\bibitem[Din et~al.(2023)Din, Karidi, Choshen, and Geva]{din2023jump}
Alexander~Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva.
\newblock Jump to conclusions: Short-cutting transformers with linear transformations, 2023, arXiv:2303.09435.

\bibitem[{Leela Chess Zero team}()]{leela}
{Leela Chess Zero team}.
\newblock {Leela Chess Zero}.
\newblock URL \url{https://lczero.org/}.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hassabis]{alphazero}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.
\newblock A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play.
\newblock {\em Science}, 2018.

\bibitem[Haworth and Hernandez(2021)]{tcec}
Guy Haworth and Nelson Hernandez.
\newblock The 20th {Top} {Chess} {Engine} {Championship}, {TCEC20}.
\newblock {\em J. Int. Comput. Games Assoc.}, 2021.
\newblock URL \url{https://tcec-chess.com/articles/TCEC_20.pdf}.

\bibitem[lepned(2024)]{leela_vs_gdm}
lepned.
\newblock How well do {Lc0} networks compare to the greatest transformer network from {DeepMind}?, 2024.
\newblock URL \url{https://lczero.org/blog/2024/02/how-well-do-lc0-networks-compare-to-the-greatest-transformer-network-from-deepmind/}.

\bibitem[Hewitt and Liang(2019)]{hewitt2019designing}
John Hewitt and Percy Liang.
\newblock Designing and interpreting probes with control tasks.
\newblock {\em EMNLP}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners, 2019.
\newblock URL \url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[{Lichess team}()]{lichess_puzzles}
{Lichess team}.
\newblock Lichess tactics puzzles dataset.
\newblock URL \url{https://database.lichess.org/#puzzles}.

\bibitem[Heimersheim and Nanda(2024)]{heimersheim2024use}
Stefan Heimersheim and Neel Nanda.
\newblock How to use and interpret activation patching, 2024, arXiv:2404.15255.

\bibitem[Zhang and Nanda(2024)]{zhang2024towards}
Fred Zhang and Neel Nanda.
\newblock Towards best practices of activation patching in language models: Metrics and methods.
\newblock {\em ICLR}, 2024.

\bibitem[Vig et~al.(2020)Vig, Gehrmann, Belinkov, Qian, Nevo, Singer, and Shieber]{vig2020}
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber.
\newblock Investigating gender bias in language models using causal mediation analysis.
\newblock {\em NeurIPS}, 2020.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock {\em NeurIPS}, 2022.

\bibitem[Geiger et~al.(2021)Geiger, Lu, Icard, and Potts]{geiger2021}
Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts.
\newblock Causal abstractions of neural networks.
\newblock {\em NeurIPS}, 2021.

\bibitem[Pal et~al.(2023)Pal, Sun, Yuan, Wallace, and Bau]{futurelens}
Koyena Pal, Jiuding Sun, Andrew Yuan, Byron~C Wallace, and David Bau.
\newblock {Future Lens}: Anticipating subsequent tokens from a single hidden state.
\newblock {\em Conference on Computational Natural Language Learning (CoNLL)}, 2023.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and Conwell]{hochreiter2001}
Sepp Hochreiter, A.~Steven Younger, and Peter~R. Conwell.
\newblock Learning to learn using gradient descent.
\newblock {\em ICANN}, 2001.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and Abbeel]{duan2016rl2}
Yan Duan, John Schulman, Xi~Chen, Peter~L. Bartlett, Ilya Sutskever, and Pieter Abbeel.
\newblock {RL$^2$:} fast reinforcement learning via slow reinforcement learning, 2016, arXiv:1611.02779.

\bibitem[Wang et~al.(2016)Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos, Blundell, Kumaran, and Botvinick]{wang2016learning}
Jane~X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel~Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
\newblock Learning to reinforcement learn, 2016, arXiv:1611.05763.

\bibitem[Lee et~al.(2023)Lee, Xie, Pacchiano, Chandak, Finn, Nachum, and Brunskill]{lee2023supervised}
Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill.
\newblock Supervised pretraining can learn in-context reinforcement learning.
\newblock {\em NeurIPS}, 2023.

\bibitem[McGrath et~al.(2022)McGrath, Kapishnikov, Tomašev, Pearce, Wattenberg, Hassabis, Kim, Paquet, and Kramnik]{mcgrath2022}
Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik.
\newblock Acquisition of chess knowledge in {AlphaZero}.
\newblock {\em Proceedings of the National Academy of Sciences}, 2022.

\bibitem[Schut et~al.(2023)Schut, Tomasev, McGrath, Hassabis, Paquet, and Kim]{schut2023bridging}
Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim.
\newblock Bridging the human-{AI} knowledge gap: Concept discovery and transfer in {AlphaZero}, 2023, arXiv:2310.16410.

\bibitem[Li et~al.(2023)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and Wattenberg]{othellogpt}
Kenneth Li, Aspen~K Hopkins, David Bau, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin Wattenberg.
\newblock Emergent world representations: Exploring a sequence model trained on a synthetic task.
\newblock {\em ICLR}, 2023.

\bibitem[Nanda et~al.(2023)Nanda, Lee, and Wattenberg]{othellogpt_neel}
Neel Nanda, Andrew Lee, and Martin Wattenberg.
\newblock Emergent linear representations in world models of self-supervised sequence models, 2023, arXiv:2309.00941.

\bibitem[Karvonen(2024)]{karvonen2024emergent}
Adam Karvonen.
\newblock Emergent world models and latent variable estimation in chess-playing language models, 2024, arXiv:2403.15498.

\bibitem[Hubinger et~al.(2019)Hubinger, van Merwijk, Mikulik, Skalse, and Garrabrant]{hubinger2019risks}
Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant.
\newblock Risks from learned optimization in advanced machine learning systems, 2019, arXiv:1906.01820.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L{}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em NeurIPS}, 2017.

\bibitem[{ENOT developers} et~al.(2021){ENOT developers}, Kalgin, Yanchenko, Ivanov, and Goncharenko]{onnx2torch}
{ENOT developers}, Igor Kalgin, Arseny Yanchenko, Pyoter Ivanov, and Alexander Goncharenko.
\newblock onnx2torch, 2021.
\newblock URL \url{https://github.com/ENOT-AutoDL/onnx2torch}.

\bibitem[Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Bell, Berard, Burovski, Chauhan, Chourdia, Constable, Desmaison, DeVito, Ellison, Feng, Gong, Gschwind, Hirsh, Huang, Kalambarkar, Kirsch, Lazos, Lezcano, Liang, Liang, Lu, Luk, Maher, Pan, Puhrsch, Reso, Saroufim, Siraichi, Suk, Suo, Tillet, Wang, Wang, Wen, Zhang, Zhao, Zhou, Zou, Mathews, Chanan, Wu, and Chintala]{pytorch}
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK~Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos~Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu~Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala.
\newblock {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}.
\newblock {\em ACM International Conference on Architectural Support for Programming Languages and Operating Systems}, 2024.
\newblock URL \url{https://pytorch.org/assets/pytorch2-2.pdf}.

\bibitem[Fiotto-Kaufman()]{nnsight}
Jaden Fiotto-Kaufman.
\newblock {nnsight}: The package for interpreting and manipulating the internals of deep learned models.
\newblock URL \url{https://github.com/JadenFiotto-Kaufman/nnsight}.

\bibitem[Graffa()]{lczero_tools}
Trevor Graffa.
\newblock lczero\textunderscore{}tools.
\newblock URL \url{https://github.com/so-much-meta/lczero_tools}.

\bibitem[Fiekas()]{python_chess}
Niklas Fiekas.
\newblock {python-chess}.
\newblock URL \url{https://github.com/niklasf/python-chess}.

\bibitem[Rogozhnikov(2022)]{rogozhnikov2022einops}
Alex Rogozhnikov.
\newblock Einops: Clear and reliable tensor manipulations with {Einstein}-like notation.
\newblock {\em ICLR}, 2022.

\bibitem[{IceBerg Contributors}(2023)]{iceberg}
{IceBerg Contributors}.
\newblock {IceBerg} – compositional graphics \& diagramming, 2023.
\newblock URL \url{https://github.com/revalo/iceberg}.

\bibitem[Hunter(2007)]{matplotlib}
John~D. Hunter.
\newblock {Matplotlib}: A {2D} graphics environment.
\newblock {\em Computing in Science \& Engineering}, 2007.

\end{thebibliography}
