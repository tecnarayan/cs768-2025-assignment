\begin{thebibliography}{10}

\bibitem{abbeel2004apprenticeship}
P.~Abbeel and A.~Y. Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2004.

\bibitem{ammann2002scalable}
P.~Ammann, D.~Wijesekera, and S.~Kaushik.
\newblock Scalable, graph-based network vulnerability analysis.
\newblock In {\em ACM Conference on Computer and Communications Security},
  2002.

\bibitem{arnold2017value}
T.~Arnold, D.~Kasenberg, and M.~Scheutz.
\newblock Value alignment or misalignment -- what will keep systems
  accountable?
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2017.

\bibitem{bacsar2014stochastic}
T.~Ba{\c{s}}ar.
\newblock Stochastic differential games and intricacy of information
  structures.
\newblock In {\em Dynamic Games in Economics}, pages 23--49. Springer, 2014.

\bibitem{blackwell1965discounted}
D.~Blackwell.
\newblock Discounted dynamic programming.
\newblock {\em The Annals of Mathematical Statistics}, 36(1):226--235, 1965.

\bibitem{cardaliaguet2009stochastic}
P.~Cardaliaguet and C.~Rainer.
\newblock Stochastic differential games with asymmetric information.
\newblock {\em Applied Mathematics and Optimization}, 59(1):1--36, 2009.

\bibitem{de1996repeated}
B.~De~Meyer.
\newblock Repeated games, duality and the central limit theorem.
\newblock {\em Mathematics of Operations Research}, 21(1):237--251, 1996.

\bibitem{finn2016guided}
C.~Finn, S.~Levine, and P.~Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem{hadfield2016cooperative}
D.~Hadfield-Menell, S.~J. Russell, P.~Abbeel, and A.~Dragan.
\newblock Cooperative inverse reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{hansen2004dynamic}
E.~A. Hansen, D.~S. Bernstein, and S.~Zilberstein.
\newblock Dynamic programming for partially observable stochastic games.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2004.

\bibitem{hauskrecht2000value}
M.~Hauskrecht.
\newblock Value-function approximations for partially observable {Markov}
  decision processes.
\newblock {\em Journal of Artificial Intelligence Research}, 13:33--94, 2000.

\bibitem{hendricks2006feints}
K.~Hendricks and R.~P. McAfee.
\newblock Feints.
\newblock {\em Journal of Economics \& Management Strategy}, 15(2):431--456,
  2006.

\bibitem{horak2017heuristic}
K.~Hor{\'a}k, B.~Bosansk{\`y}, and M.~Pechoucek.
\newblock Heuristic search value iteration for one-sided partially observable
  stochastic games.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2017.

\bibitem{horner2010markov}
J.~H{\"o}rner, D.~Rosenberg, E.~Solan, and N.~Vieille.
\newblock On a {M}arkov game with one-sided information.
\newblock {\em Operations Research}, 58(4-part-2):1107--1115, 2010.

\bibitem{iannucci2016probabilistic}
S.~Iannucci and S.~Abdelwahed.
\newblock A probabilistic approach to autonomic security management.
\newblock In {\em IEEE International Conference on Autonomic Computing}, 2016.

\bibitem{ji2017rain}
Y.~Ji, S.~Lee, E.~Downing, W.~Wang, M.~Fazzini, T.~Kim, A.~Orso, and W.~Lee.
\newblock {RAIN}: Refinable attack investigation with on-demand inter-process
  information flow tracking.
\newblock In {\em ACM SIGSAC Conference on Computer and Communications
  Security}, 2017.

\bibitem{kalman1964linear}
R.~E. Kalman.
\newblock When is a linear control system optimal?
\newblock {\em Journal of Basic Engineering}, 86(1):51--60, 1964.

\bibitem{kuhn1953extensive}
H.~W. Kuhn.
\newblock {\em Extensive Games and the Problem of Information}, volume~2.
\newblock Princeton University Press, 1953.

\bibitem{lanctot2017unified}
M.~Lanctot, V.~Zambaldi, A.~Gruslys, A.~Lazaridou, K.~Tuyls, J.~P{\'e}rolat,
  D.~Silver, and T.~Graepel.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{lin2019multi}
X.~Lin, S.~C. Adams, and P.~A. Beling.
\newblock Multi-agent inverse reinforcement learning for certain general-sum
  stochastic games.
\newblock {\em Journal of Artificial Intelligence Research}, 66:473--502, 2019.

\bibitem{lin2018multiagent}
X.~Lin, P.~A. Beling, and R.~Cogill.
\newblock Multiagent inverse reinforcement learning for two-person zero-sum
  games.
\newblock {\em IEEE Transactions on Games}, 10(1):56--68, 2018.

\bibitem{littman1994markov}
M.~L. Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In {\em Machine Learning Proceedings}, pages 157--163. Elsevier,
  1994.

\bibitem{malik2018efficient}
D.~Malik, M.~Palaniappan, J.~Fisac, D.~Hadfield-Mennell, S.~Russell, and
  A.~Dragan.
\newblock An efficient, generalized {B}ellman update for cooperative inverse
  reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{marschak1972economic}
J.~Marschak and R.~Radner.
\newblock {\em Economic Theory of Teams.}
\newblock Yale University Press, 1972.

\bibitem{mertens1985formulation}
J.-F. Mertens and S.~Zamir.
\newblock Formulation of bayesian analysis for games with incomplete
  information.
\newblock {\em International Journal of Game Theory}, 14(1):1--29, 1985.

\bibitem{miehling2015optimal}
E.~Miehling, M.~Rasouli, and D.~Teneketzis.
\newblock Optimal defense policies for partially observable spreading processes
  on {B}ayesian attack graphs.
\newblock In {\em Second ACM Workshop on Moving Target Defense}, 2015.

\bibitem{miehling2018pomdp}
E.~Miehling, M.~Rasouli, and D.~Teneketzis.
\newblock A {POMDP} approach to the dynamic defense of large-scale cyber
  networks.
\newblock {\em IEEE Transactions on Information Forensics and Security},
  13(10):2490--2505, 2018.

\bibitem{nayyar2012dynamic}
A.~Nayyar and T.~Ba{\c{s}}ar.
\newblock Dynamic stochastic games with asymmetric information.
\newblock In {\em IEEE Conference on Decision and Control}, 2012.

\bibitem{nayyar2014common}
A.~Nayyar, A.~Gupta, C.~Langbort, and T.~Ba{\c{s}}ar.
\newblock Common information based {M}arkov perfect equilibria for stochastic
  games with asymmetric information: {F}inite games.
\newblock {\em IEEE Transactions on Automatic Control}, 59(3):555--570, 2014.

\bibitem{nayyar2013decentralized}
A.~Nayyar, A.~Mahajan, and D.~Teneketzis.
\newblock Decentralized stochastic control with partial history sharing: A
  common information approach.
\newblock {\em IEEE Transactions on Automatic Control}, 58(7):1644--1658, 2013.

\bibitem{ng2000algorithms}
A.~Y. Ng and S.~J. Russell.
\newblock Algorithms for inverse reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2000.

\bibitem{pineau2003point}
J.~Pineau, G.~Gordon, and S.~Thrun.
\newblock Point-based value iteration: An anytime algorithm for {POMDP}s.
\newblock In {\em International Joint Conferences on Artificial Intelligence},
  2003.

\bibitem{ratliff2006maximum}
N.~D. Ratliff, J.~A. Bagnell, and M.~A. Zinkevich.
\newblock Maximum margin planning.
\newblock In {\em International Conference on Machine Learning}, 2006.

\bibitem{renault2006value}
J.~Renault.
\newblock The value of {M}arkov chain games with lack of information on one
  side.
\newblock {\em Mathematics of Operations Research}, 31(3):490--512, 2006.

\bibitem{rosenberg1998duality}
D.~Rosenberg.
\newblock Duality and {Markovian} strategies.
\newblock {\em International Journal of Game Theory}, 27(4):577--597, 1998.

\bibitem{rosenberg2000maxmin}
D.~Rosenberg and N.~Vieille.
\newblock The maxmin of recursive games with incomplete information on one
  side.
\newblock {\em Mathematics of Operations Research}, 25(1):23--35, 2000.

\bibitem{russell2015research}
S.~Russell, D.~Dewey, and M.~Tegmark.
\newblock Research priorities for robust and beneficial artificial
  intelligence.
\newblock {\em AI Magazine}, 36(4):105--114, 2015.

\bibitem{russell2016artificial}
S.~J. Russell and P.~Norvig.
\newblock {\em Artificial Intelligence: {A} Modern Approach}.
\newblock Malaysia; Pearson Education Limited, 2016.

\bibitem{sorin2003stochastic}
S.~Sorin.
\newblock Stochastic games with incomplete information.
\newblock In {\em Stochastic Games and Applications}, pages 375--395. Springer,
  2003.

\bibitem{srinivasan2018actor}
S.~Srinivasan, M.~Lanctot, V.~Zambaldi, J.~P{\'e}rolat, K.~Tuyls, R.~Munos, and
  M.~Bowling.
\newblock Actor-critic policy optimization in partially observable multiagent
  environments.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{wang2018competitive}
X.~Wang and D.~Klabjan.
\newblock Competitive multi-agent inverse reinforcement learning with
  sub-optimal demonstrations.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{wiener1960some}
N.~Wiener.
\newblock Some moral and technical consequences of automation.
\newblock {\em Science}, 131(3410):1355--1358, 1960.

\bibitem{zhang2019policy}
K.~Zhang, Z.~Yang, and T.~Ba{\c{s}}ar.
\newblock Policy optimization provably converges to {N}ash equilibria in
  zero-sum linear quadratic games.
\newblock {\em arXiv preprint arXiv:1906.00729}, 2019.

\bibitem{zhang2018finite}
K.~Zhang, Z.~Yang, H.~Liu, T.~Zhang, and T.~Ba{\c{s}}ar.
\newblock Finite-sample analyses for fully decentralized multi-agent
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1812.02783}, 2018.

\bibitem{zhang2018fully}
K.~Zhang, Z.~Yang, H.~Liu, T.~Zhang, and T.~Ba{\c{s}}ar.
\newblock Fully decentralized multi-agent reinforcement learning with networked
  agents.
\newblock In {\em International Conference on Machine Learning}, 2018.

\end{thebibliography}
