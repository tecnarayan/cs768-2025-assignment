@article{guest_what_2017,
	title = {What the success of brain imaging implies about the neural code},
	volume = {6},
	issn = {2050-084X},
	doi = {10.7554/eLife.21397},
	journal = {eLife},
	author = {Guest, Olivia and Love, Bradley C},
	editor = {Poldrack, Russell},
	month = jan,
	year = {2017},
	note = {Publisher: eLife Sciences Publications, Ltd},
	pages = {e21397}
}

@inproceedings{chen_densified_2018,
        title = {Densified winner take all ({WTA}) hashing for sparse datasets},
        booktitle = {Uncertainty in artificial intelligence},
        author = {Chen, Beidi and Shrivastava, Anshumali},
        year = {2018},
        file = {Full Text:/home/chay/Zotero/storage/EIQ34FFQ/Chen and Shrivastava - 2018 - Densified winner take all (WTA) hashing for sparse.pdf:application/pdf;Snapshot:/home/chay/Zotero/storage/MUAL3S6K/10065989.html:text/html
        }
}



@incollection{TheerthaQuantNIPS,
title = {Multiscale Quantization for Fast Similarity Search},
author = {Wu, Xiang and Guo, Ruiqi and Suresh, Ananda Theertha and Kumar, Sanjiv and Holtmann-Rice, Daniel N and Simcha, David and Yu, Felix},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5745--5755},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7157-multiscale-quantization-for-fast-similarity-search.pdf}
}

@article{koulakov_sparse_2011,
	title = {Sparse {Incomplete} {Representations}: {A} {Potential} {Role} of {Olfactory} {Granule} {Cells}},
	volume = {72},
	issn = {0896-6273},
	shorttitle = {Sparse {Incomplete} {Representations}},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627311006891},
	doi = {10.1016/j.neuron.2011.07.031},
	abstract = {Mitral/tufted cells of the olfactory bulb receive odorant information from receptor neurons and transmit this information to the cortex. Studies in awake behaving animals have found that sustained responses of mitral cells to odorants are rare, suggesting sparse combinatorial representation of the odorants. Careful alignment of mitral cell firing with the phase of the respiration cycle revealed brief transient activity in the larger population of mitral cells, which respond to odorants during a small fraction of the respiration cycle. Responses of these cells are therefore temporally sparse. Here, we propose a mathematical model for the olfactory bulb network that can reproduce both combinatorially and temporally sparse mitral cell codes. We argue that sparse codes emerge as a result of the balance between mitral cells' excitatory inputs and inhibition provided by the granule cells. Our model suggests functional significance for the dendrodendritic synapses mediating interactions between mitral and granule cells.},
	language = {en},
	number = {1},
	urldate = {2020-01-24},
	journal = {Neuron},
	author = {Koulakov, Alexei A. and Rinberg, Dmitry},
	month = oct,
	year = {2011},
	pages = {124--136},
	file = {ScienceDirect Full Text PDF:/home/chay/Zotero/storage/LWI9J2FI/Koulakov and Rinberg - 2011 - Sparse Incomplete Representations A Potential Rol.pdf:application/pdf;ScienceDirect Snapshot:/home/chay/Zotero/storage/KYCZ5ZW3/S0896627311006891.html:text/html}
}



@article{tsne,
  title = {Visualizing Data using t-SNE},
  volume = {9},
  language = {en},
  number = {9},
  journal = {Journal of Machine Learning Research},
  author = {L.J.P. {van der Maaten} and G.E. {Hinton}},
  month = apr,
  year = {2008},
  pages = {2579--2605},
}




@inproceedings{pehlevan2017,
author={C. {Pehlevan} and A. {Genkin} and D. B. {Chklovskii}},
booktitle={2017 51st Asilomar Conference on Signals, Systems, and Computers},
title={A clustering neural network model of insect olfaction},
year={2017},
volume={},
number={},
pages={593-600},
ISSN={2576-2303},
month={Oct}}

@article{sompolinskySparsenessExpansionSensory2014,
  title = {Sparseness and {{Expansion}} in {{Sensory Representations}}},
  volume = {83},
  issn = {08966273},
  abstract = {In several sensory pathways, input stimuli project to sparsely active downstream populations that have more neurons than incoming axons. Here, we address the computational benefits of expansion and sparseness for clustered inputs, where different clusters represent behaviorally distinct stimuli and intracluster variability represents sensory or neuronal noise. Through analytical calculations and numerical simulations, we show that expansion implemented by feed-forward random synaptic weights amplifies variability in the incoming stimuli, and this noise enhancement increases with sparseness of the expanded representation. In addition, the low dimensionality of the input layer generates overlaps between the induced representations of different stimuli, limiting the benefit of expansion. Highly sparse expansive representations obtained through synapses that encode the clustered structure of the input reduce both intrastimulus variability and the excess overlaps between stimuli, enhancing the ability of downstream neurons to perform classification and recognition tasks. Implications for olfactory, cerebellar, and visual processing are discussed.},
  language = {en},
  number = {5},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2014.07.035},
  author = {Sompolinsky, Babadi},
  month = sep,
  year = {2014},
  keywords = {Sanjoy,Sparse,Expansion,Random Kernel},
  pages = {1213-1226},
  file = {/home/chay/Zotero/storage/CTIZ86TB/2014 - Sparseness and Expansion in Sensory Representation.pdf;/home/chay/Zotero/storage/HW3FTT4X/Sparseness and Expansion in Sensory Systems.pdf;/home/chay/Zotero/storage/NG9H9MIR/mmc1.pdf}
}

@article{linSparseDecorrelatedOdor2014,
  title = {Sparse, Decorrelated Odor Coding in the Mushroom Body Enhances Learned Odor Discrimination},
  volume = {17},
  issn = {1097-6256, 1546-1726},
  language = {en},
  number = {4},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.3660},
  author = {Lin, Andrew C and Bygrave, Alexei M and {de Calignon}, Alix and Lee, Tzumin and Miesenb{\"o}ck, Gero},
  month = apr,
  year = {2014},
  pages = {559-568},
  file = {/home/chay/Zotero/storage/T69U3JSF/Lin et al. - 2014 - Sparse, decorrelated odor coding in the mushroom b.pdf}
}

@article{raichleAppraisingBrainEnergy2002,
  title = {Appraising the Brain's Energy Budget},
  volume = {99},
  number = {16},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Raichle, Marcus E. and Gusnard, Debra A.},
  year = {2002},
  pages = {10237--10239},
  file = {/home/chay/Zotero/storage/ZHBTLHAQ/Raichle and Gusnard - 2002 - Appraising the brain's energy budget.pdf;/home/chay/Zotero/storage/5AZ43YCT/10237.html}
}

@article{levyEnergyEfficientNeural1996,
  title = {Energy Efficient Neural Codes},
  volume = {8},
  issn = {0899-7667},
  abstract = {In 1969 Barlow introduced the phrase "economy of impulses" to express the tendency for successive neural systems to use lower and lower levels of cell firings to produce equivalent encodings. From this viewpoint, the ultimate economy of impulses is a neural code of minimal redundancy. The hypothesis motivating our research is that energy expenditures, e.g., the metabolic cost of recovering from an action potential relative to the cost of inactivity, should also be factored into the economy of impulses. In fact, coding schemes with the largest representational capacity are not, in general, optimal when energy expenditures are taken into account. We show that for both binary and analog neurons, increased energy expenditure per neuron implies a decrease in average firing rate if energy efficient information transmission is to be maintained.},
  language = {eng},
  number = {3},
  journal = {Neural Computation},
  author = {Levy, W. B. and Baxter, R. A.},
  month = apr,
  year = {1996},
  keywords = {Action Potentials,Algorithms,Energy Metabolism,Models; Neurological,Neurons,Synaptic Transmission},
  pages = {531-543},
  pmid = {8868566}
}

@article{pehlevanHebbianAntiHebbianNetwork2014a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.00680},
  title = {A {{Hebbian}}/{{Anti}}-{{Hebbian Network Derived}} from {{Online Non}}-{{Negative Matrix Factorization Can Cluster}} and {{Discover Sparse Features}}},
  abstract = {Despite our extensive knowledge of biophysical properties of neurons, there is no commonly accepted algorithmic theory of neuronal function. Here we explore the hypothesis that single-layer neuronal networks perform online symmetric nonnegative matrix factorization (SNMF) of the similarity matrix of the streamed data. By starting with the SNMF cost function we derive an online algorithm, which can be implemented by a biologically plausible network with local learning rules. We demonstrate that such network performs soft clustering of the data as well as sparse feature discovery. The derived algorithm replicates many known aspects of sensory anatomy and biophysical properties of neurons including unipolar nature of neuronal activity and synaptic weights, local synaptic plasticity rules and the dependence of learning rate on cumulative neuronal activity. Thus, we make a step towards an algorithmic theory of neuronal function, which should facilitate large-scale neural circuit simulations and biologically inspired artificial intelligence.},
  journal = {2014 48th Asilomar Conference on Signals, Systems and Computers},
  doi = {10.1109/ACSSC.2014.7094553},
  author = {Pehlevan, Cengiz and Chklovskii, Dmitri B.},
  month = nov,
  year = {2014},
  keywords = {Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  pages = {769-775},
  file = {/home/chay/Zotero/storage/JW9EPCP7/Pehlevan and Chklovskii - 2014 - A HebbianAnti-Hebbian Network Derived from Online.pdf;/home/chay/Zotero/storage/P5LTS2PA/1503.html}
}

@article{pehlevanClusteringNeuralNetwork2018a,
  title = {A Clustering Neural Network Model of Insect Olfaction},
  abstract = {A key step in insect olfaction is the transformation of a dense representation of odors in a small population of neurons - projection neurons (PNs) of the antennal lobe - into a sparse representation in a much larger population of neurons Kenyon cells (KCs) of the mushroom body. What computational purpose does this transformation serve? We propose that the PN-KC network implements an online clustering algorithm which we derive from the k-means cost function. The vector of PN-KC synaptic weights converging onto a given KC represents the corresponding cluster centroid. KC activities represent attribution indices, i.e. the degree to which a given odor presentation is attributed to each cluster. Remarkably, such clustering view of the PN-KC circuit naturally accounts for several of its salient features. First, attribution indices are nonnegative thus rationalizing rectification in KCs. Second, the constraint on the total sum of attribution indices for each presentation is enforced by a Lagrange multiplier identified with the activity of a single inhibitory interneuron reciprocally connected with KCs. Third, the soft-clustering version of our algorithm reproduces observed sparsity and overcompleteness of the KC representation which may optimize supervised classification downstream.},
  language = {en},
  doi = {10.1101/226746},
  author = {Pehlevan, Cengiz and Genkin, Alexander and Chklovskii, Dmitri B.},
  month = jan,
  year = {2018},
  file = {/home/chay/Zotero/storage/P8GKKII6/Pehlevan et al. - 2018 - A clustering neural network model of insect olfact.pdf}
}

@article{pehlevanHebbianAntiHebbianNeural2015a,
  title = {A {{Hebbian}}/{{Anti}}-{{Hebbian Neural Network}} for {{Linear Subspace Learning}}: {{A Derivation}} from {{Multidimensional Scaling}} of {{Streaming Data}}},
  volume = {27},
  issn = {0899-7667, 1530-888X},
  shorttitle = {A {{Hebbian}}/{{Anti}}-{{Hebbian Neural Network}} for {{Linear Subspace Learning}}},
  language = {en},
  number = {7},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00745},
  author = {Pehlevan, Cengiz and Hu, Tao and Chklovskii, Dmitri B.},
  month = jul,
  year = {2015},
  pages = {1461-1495}
}

@inproceedings{torabiasrQueryingWordEmbeddings2018,
  address = {{New Orleans, Louisiana}},
  title = {Querying {{Word Embeddings}} for {{Similarity}} and {{Relatedness}}},
  abstract = {Word embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data. Skipgram generates a set of word and context embeddings, the latter typically discarded after training. We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev \& McRae, 2016). Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/N18-1062},
  author = {Torabi Asr, Fatemeh and Zinkov, Robert and Jones, Michael},
  month = jun,
  year = {2018},
  pages = {675--684},
  file = {/home/chay/Zotero/storage/5QYRAKQ8/Torabi Asr et al. - 2018 - Querying Word Embeddings for Similarity and Relate.pdf}
}

@misc{WordSimilarity353TestCollection,
  title = {The {{WordSimilarity}}-353 {{Test Collection}}},
  howpublished = {http://www.cs.technion.ac.il/\textasciitilde{}gabr/resources/data/wordsim353/},
  file = {/home/chay/Zotero/storage/UALY5AAZ/wordsim353.html}
}

@article{tangEmpiricalStudyPostprocessing2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.10971},
  primaryClass = {cs, stat},
  title = {An {{Empirical Study}} on {{Post}}-Processing {{Methods}} for {{Word Embeddings}}},
  abstract = {Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.},
  journal = {arXiv:1905.10971 [cs, stat]},
  author = {Tang, Shuai and Mousavi, Mahta and {de Sa}, Virginia R.},
  month = may,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Computation and Language},
  file = {/home/chay/Zotero/storage/BQHNWP62/Tang et al. - 2019 - An Empirical Study on Post-processing Methods for .pdf;/home/chay/Zotero/storage/R3VBL5YZ/1905.html}
}

@article{muALLBUTTHETOPSIMPLEEFFECTIVE2018,
  title = {{{ALL}}-{{BUT}}-{{THE}}-{{TOP}}: {{SIMPLE AND EFFECTIVE POST}}- {{PROCESSING FOR WORD REPRESENTATIONS}}},
  abstract = {Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a very simple, and yet counter-intuitive, postprocessing technique \textendash{} eliminate the common mean vector and a few top dominating directions from the word vectors \textendash{} that renders off-the-shelf representations even stronger. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and text classification) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.},
  language = {en},
  author = {Mu, Jiaqi and Viswanath, Pramod},
  year = {2018},
  pages = {25},
  file = {/home/chay/Zotero/storage/JAQKJX8Y/PIIS0896627317305093 (1).pdf;/home/chay/Zotero/storage/PBJKNRZ3/Mu and Viswanath - 2018 - ALL-BUT-THE-TOP SIMPLE AND EFFECTIVE POST- PROCES.pdf}
}

@article{iuzzolinoConvolutionalBipartiteAttractor2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.03504},
  primaryClass = {cs, stat},
  title = {Convolutional {{Bipartite Attractor Networks}}},
  abstract = {In human perception and cognition, the fundamental operation that brains perform is interpretation: constructing coherent neural states from noisy, incomplete, and intrinsically ambiguous evidence. The problem of interpretation is well matched to an early and often overlooked architecture, the attractor network---a recurrent neural network that performs constraint satisfaction, imputation of missing features, and clean up of noisy data via energy minimization dynamics. We revisit attractor nets in light of modern deep learning methods, and propose a convolutional bipartite architecture with a novel training loss, activation function, and connectivity constraints. We tackle problems much larger than have been previously explored with attractor nets and demonstrate their potential for image denoising, completion, and super-resolution. We argue that this architecture is better motivated than ever-deeper feedforward models and is a viable alternative to more costly sampling-based methods on a range of supervised and unsupervised tasks.},
  journal = {arXiv:1906.03504 [cs, stat]},
  author = {Iuzzolino, Michael and Singer, Yoram and Mozer, Michael C.},
  month = jun,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Machine Learning},
  file = {/home/chay/Zotero/storage/UY448JQH/Iuzzolino et al. - 2019 - Convolutional Bipartite Attractor Networks.pdf;/home/chay/Zotero/storage/WGUDZDD9/1906.html}
}

@article{huangLocalReceptiveFields2015a,
  title = {Local {{Receptive Fields Based Extreme Learning Machine}}},
  volume = {10},
  issn = {1556-603X},
  language = {en},
  number = {2},
  journal = {IEEE Computational Intelligence Magazine},
  doi = {10.1109/MCI.2015.2405316},
  author = {Huang, Guang-Bin and Bai, Zuo and Kasun, Liyanaarachchi Lekamalage Chamara and Vong, Chi Man},
  month = may,
  year = {2015},
  pages = {18-29},
  file = {/home/chay/Zotero/storage/SYSJ8AYZ/Huang et al. - 2015 - Local Receptive Fields Based Extreme Learning Mach.pdf}
}

@misc{caoMustreadPapersDeep2019,
  title = {Must-Read Papers on Deep Learning to Hash ({{DeepHash}}): Caoyue10/{{DeepHash}}-{{Papers}}},
  shorttitle = {Must-Read Papers on Deep Learning to Hash ({{DeepHash}})},
  author = {Cao, Yue},
  month = jun,
  year = {2019}
}

@article{chaLanguageModelingClustering2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01888},
  primaryClass = {cs},
  title = {Language {{Modeling}} by {{Clustering}} with {{Word Embeddings}} for {{Text Readability Assessment}}},
  abstract = {We present a clustering-based language model using word embeddings for text readability prediction. Presumably, an Euclidean semantic space hypothesis holds true for word embeddings whose training is done by observing word co-occurrences. We argue that clustering with word embeddings in the metric space should yield feature representations in a higher semantic space appropriate for text regression. Also, by representing features in terms of histograms, our approach can naturally address documents of varying lengths. An empirical evaluation using the Common Core Standards corpus reveals that the features formed on our clustering-based language model significantly improve the previously known results for the same corpus in readability prediction. We also evaluate the task of sentence matching based on semantic relatedness using the Wiki-SimpleWiki corpus and find that our features lead to superior matching performance.},
  journal = {arXiv:1709.01888 [cs]},
  author = {Cha, Miriam and Gwon, Youngjune and Kung, H. T.},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Computation and Language},
  file = {/home/chay/Zotero/storage/YESFKLZB/Cha et al. - 2017 - Language Modeling by Clustering with Word Embeddin.pdf;/home/chay/Zotero/storage/A8VZJJ3M/1709.html}
}

@article{wangSequentialProjectionLearning,
  title = {Sequential {{Projection Learning}} for {{Hashing}} with {{Compact Codes}}},
  abstract = {Hashing based Approximate Nearest Neighbor (ANN) search has attracted much attention due to its fast query time and drastically reduced storage. However, most of the hashing methods either use random projections or extract principal directions from the data to derive hash functions. The resulting embedding suffers from poor discrimination when compact codes are used. In this paper, we propose a novel data-dependent projection learning method such that each hash function is designed to correct the errors made by the previous one sequentially. The proposed method easily adapts to both unsupervised and semi-supervised scenarios and shows significant performance gains over the state-ofthe-art methods on two large datasets containing up to 1 million points.},
  language = {en},
  author = {Wang, Jun and Kumar, Sanjiv and Chang, Shih-Fu},
  pages = {8},
  file = {/home/chay/Zotero/storage/PTZLCQ2W/Wang et al. - Sequential Projection Learning for Hashing with Co.pdf}
}

@incollection{kongIsotropicHashing2012,
  title = {Isotropic {{Hashing}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Kong, Weihao and Li, Wu-jun},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1646--1654},
  file = {/home/chay/Zotero/storage/3RK4HZ8N/Kong and Li - 2012 - Isotropic Hashing.pdf;/home/chay/Zotero/storage/JIGG3FSC/4846-isotropic-hashing.html}
}

@inproceedings{linCompressedHashing2013,
  address = {{Portland, OR, USA}},
  title = {Compressed {{Hashing}}},
  isbn = {978-0-7695-4989-7},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2013.64},
  author = {Lin, Yue and Jin, Rong and Cai, Deng and Yan, Shuicheng and Li, Xuelong},
  month = jun,
  year = {2013},
  pages = {446-451}
}

@article{liuUnsupervisedLocalFeature2016,
  title = {Unsupervised {{Local Feature Hashing}} for {{Image Similarity Search}}},
  volume = {46},
  issn = {2168-2267},
  abstract = {The potential value of hashing techniques has led to it becoming one of the most active research areas in computer vision and multimedia. However, most existing hashing methods for image search and retrieval are based on global feature representations, which are susceptible to image variations such as viewpoint changes and background cluttering. Traditional global representations gather local features directly to output a single vector without the analysis of the intrinsic geometric property of local features. In this paper, we propose a novel unsupervised hashing method called unsupervised bilinear local hashing (UBLH) for projecting local feature descriptors from a high-dimensional feature space to a lower-dimensional Hamming space via compact bilinear projections rather than a single large projection matrix. UBLH takes the matrix expression of local features as input and preserves the feature-to-feature and image-to-image structures of local features simultaneously. Experimental results on challenging data sets including Caltech-256, SUN397, and Flickr 1M demonstrate the superiority of UBLH compared with state-of-the-art hashing methods.},
  number = {11},
  journal = {IEEE Transactions on Cybernetics},
  doi = {10.1109/TCYB.2015.2480966},
  author = {Liu, L. and Yu, M. and Shao, L.},
  month = nov,
  year = {2016},
  keywords = {image representation,matrix algebra,Optimization,bilinear projection,Complexity theory,feature extraction,feature representation,file organisation,Hamming space,Hashing,Histograms,image retrieval,image similarity search,Kernel,Linear programming,local feature,Matrix decomposition,matrix expression,UBLH,unsupervised bilinear local hashing,unsupervised learning,unsupervised local feature hashing,Visualization},
  pages = {2548-2558},
  file = {/home/chay/Zotero/storage/UASNJYJW/Liu et al. - 2016 - Unsupervised Local Feature Hashing for Image Simil.pdf;/home/chay/Zotero/storage/D93X776L/7297820.html}
}

@article{grossbergAdaptivePatternClassification1976,
  title = {Adaptive Pattern Classification and Universal Recoding: {{I}}. {{Parallel}} Development and Coding of Neural Feature Detectors},
  volume = {23},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Adaptive Pattern Classification and Universal Recoding},
  language = {en},
  number = {3},
  journal = {Biological Cybernetics},
  doi = {10.1007/BF00344744},
  author = {Grossberg, S.},
  year = {1976},
  pages = {121-134}
}

@article{sterneEfficientRobustAssociative2012,
  title = {Efficient and Robust Associative Memory from a Generalized {{Bloom}} Filter},
  volume = {106},
  issn = {0340-1200, 1432-0770},
  language = {en},
  number = {4-5},
  journal = {Biological Cybernetics},
  doi = {10.1007/s00422-012-0494-6},
  author = {Sterne, P.},
  month = jul,
  year = {2012},
  pages = {271-281}
}

@article{grossbergAdaptivePatternClassification1976a,
  title = {Adaptive Pattern Classification and Universal Recoding: {{I}}. {{Parallel}} Development and Coding of Neural Feature Detectors},
  volume = {23},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Adaptive Pattern Classification and Universal Recoding},
  language = {en},
  number = {3},
  journal = {Biological Cybernetics},
  doi = {10.1007/BF00344744},
  author = {Grossberg, S.},
  year = {1976},
  pages = {121-134}
}

@article{huaLocalitySensitiveBloomFilter2012,
  title = {Locality-{{Sensitive Bloom Filter}} for {{Approximate Membership Query}}},
  volume = {61},
  issn = {0018-9340},
  number = {6},
  journal = {IEEE Transactions on Computers},
  doi = {10.1109/TC.2011.108},
  author = {Hua, Yu and Xiao, Bin and Veeravalli, Bharadwaj and Feng, Dan},
  month = jun,
  year = {2012},
  pages = {817-830}
}

@article{dasguptaNeuralDataStructure2018,
  title = {A Neural Data Structure for Novelty Detection},
  volume = {115},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {51},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1814448115},
  author = {Dasgupta, Sanjoy and Sheehan, Timothy C. and Stevens, Charles F. and Navlakha, Saket},
  month = dec,
  year = {2018},
  pages = {13093-13098},
  file = {/home/chay/Zotero/storage/FZAH8QYR/Dasgupta et al. - 2018 - A neural data structure for novelty detection.pdf}
}

@article{tissierNearlosslessBinarizationWord2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09065},
  primaryClass = {cs},
  title = {Near-Lossless {{Binarization}} of {{Word Embeddings}}},
  abstract = {Word embeddings are commonly used as a starting point in many NLP models to achieve state-of-the-art performances. However, with a large vocabulary and many dimensions, these floating-point representations are expensive both in terms of memory and calculations which makes them unsuitable for use on low-resource devices. The method proposed in this paper transforms real-valued embeddings into binary embeddings while preserving semantic information, requiring only 128 or 256 bits for each vector. This leads to a small memory footprint and fast vector operations. The model is based on an autoencoder architecture, which also allows to reconstruct original vectors from the binary ones. Experimental results on semantic similarity, text classification and sentiment analysis tasks show that the binarization of word embeddings only leads to a loss of \textasciitilde{}2\% in accuracy while vector size is reduced by 97\%. Furthermore, a top-k benchmark demonstrates that using these binary vectors is 30 times faster than using real-valued vectors.},
  journal = {arXiv:1803.09065 [cs]},
  author = {Tissier, Julien and Gravier, Christophe and Habrard, Amaury},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/home/chay/Zotero/storage/VKKEMRUC/Tissier et al. - 2018 - Near-lossless Binarization of Word Embeddings.pdf;/home/chay/Zotero/storage/6CLIGEIT/1803.html}
}

@article{bennaArePlaceCells2019,
  title = {Are Place Cells Just Memory Cells? {{Memory}} Compression Leads to Spatial Tuning and History Dependence},
  shorttitle = {Are Place Cells Just Memory Cells?},
  abstract = {The observation of place cells has suggested that the hippocampus plays a special role in encoding spatial information. However, place cell responses are modulated by several non-spatial variables, and reported to be rather unstable. Here we propose a memory model of the hippocampus that provides a novel interpretation of place cells consistent with these observations. We hypothesize that the hippocampus is a memory device that takes advantage of the correlations between sensory experiences to generate compressed representations of the episodes that are stored in memory. A simple neural network model that can efficiently compress information naturally produces place cells that are similar to those observed in experiments. It predicts that the activity of these cells is variable and that the fluctuations of the place fields encode information about the recent history of sensory experiences. Place cells may simply be a consequence of a memory compression process implemented in the hippocampus.},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/624239},
  author = {Benna, Marcus K. and Fusi, Stefano},
  month = may,
  year = {2019},
  file = {/home/chay/Zotero/storage/R5TWPZED/Benna and Fusi - 2019 - Are place cells just memory cells Memory compress.pdf}
}

@article{coatesAnalysisSingleLayerNetworks,
  title = {An {{Analysis}} of {{Single}}-{{Layer Networks}} in {{Unsupervised Feature Learning}}},
  abstract = {A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several very simple factors, such as the number of hidden nodes in the model, may be as important to achieving high performance as the choice of learning algorithm or the depth of the model. Specifically, we will apply several off-the-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs and K-means clustering, Gaussian mixtures) to NORB and CIFAR datasets using only single-layer networks. We then present a detailed analysis of the effect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (``stride'') between extracted features, and the effect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are as critical to achieving high performance as the choice of algorithm itself\textemdash{}so critical, in fact, that when these parameters are pushed to their limits, we are able to achieve state-of-theart performance on both CIFAR and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyper-parameters to tune beyond the model structure itself, and is very easy implement. Despite the simplicity of our system, we achieve performance beyond all previously published results on the CIFAR-10 and NORB datasets (79.6\% and 97.0\% accuracy respectively).},
  language = {en},
  author = {Coates, Adam and Lee, Honglak and Ng, Andrew Y},
  pages = {9},
  file = {/home/chay/Zotero/storage/8UYMZCC4/Coates et al. - An Analysis of Single-Layer Networks in Unsupervis.pdf}
}

@article{serraCOMPACTEMBEDDINGBINARYCODED2017,
  title = {{{COMPACT EMBEDDING OF BINARY}}-{{CODED INPUTS AND OUTPUTS USING BLOOM FILTERS}}},
  abstract = {The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12\%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as `on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.},
  language = {en},
  author = {Serra, Joan and Karatzoglou, Alexandros},
  year = {2017},
  pages = {17},
  file = {/home/chay/Zotero/storage/U94UKHM8/Serra and Karatzoglou - 2017 - COMPACT EMBEDDING OF BINARY-CODED INPUTS AND OUTPU.pdf}
}

@incollection{agarwalHyperfeaturesMultilevelLocal2006,
  address = {{Berlin, Heidelberg}},
  title = {Hyperfeatures \textendash{} {{Multilevel Local Coding}} for {{Visual Recognition}}},
  volume = {3951},
  isbn = {978-3-540-33832-1 978-3-540-33833-8},
  abstract = {Histograms of local appearance descriptors are a popular representation for visual recognition. They are highly discriminant and have good resistance to local occlusions and to geometric and photometric variations, but they are not able to exploit spatial co-occurrence statistics at scales larger than their local input patches. We present a new multilevel visual representation, `hyperfeatures', that is designed to remedy this. The starting point is the familiar notion that to detect object parts, in practice it often suffices to detect co-occurrences of more local object fragments \textendash{} a process that can be formalized as comparison (e.g. vector quantization) of image patches against a codebook of known fragments, followed by local aggregation of the resulting codebook membership vectors to detect cooccurrences. This process converts local collections of image descriptor vectors into somewhat less local histogram vectors \textendash{} higher-level but spatially coarser descriptors. We observe that as the output is again a local descriptor vector, the process can be iterated, and that doing so captures and codes ever larger assemblies of object parts and increasingly abstract or `semantic' image properties. We formulate the hyperfeatures model and study its performance under several different image coding methods including clustering based Vector Quantization, Gaussian Mixtures, and combinations of these with Latent Dirichlet Allocation. We find that the resulting high-level features provide improved performance in several object image and texture image classification tasks.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2006},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Agarwal, Ankur and Triggs, Bill},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  pages = {30-43},
  file = {/home/chay/Zotero/storage/CIBGQB8U/Agarwal and Triggs - 2006 - Hyperfeatures – Multilevel Local Coding for Visual.pdf},
  doi = {10.1007/11744023_3}
}

@article{wadhwaLearningSparseDistributed2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.04228},
  primaryClass = {cs},
  title = {Learning {{Sparse}}, {{Distributed Representations}} Using the {{Hebbian Principle}}},
  abstract = {The "fire together, wire together" Hebbian model is a central principle for learning in neuroscience, but surprisingly, it has found limited applicability in modern machine learning. In this paper, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning (AHL). We illustrate the distributed nature of the learned representations via output entropy computations for synthetic data, and demonstrate superior performance, compared to standard alternatives such as autoencoders, in training a deep convolutional net on standard image datasets.},
  journal = {arXiv:1611.04228 [cs]},
  author = {Wadhwa, Aseem and Madhow, Upamanyu},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Machine Learning},
  file = {/home/chay/Zotero/storage/ZR2M4MEP/Wadhwa and Madhow - 2016 - Learning Sparse, Distributed Representations using.pdf;/home/chay/Zotero/storage/NTP72Z2D/1611.html}
}

@article{krotovUnsupervisedLearningCompeting2019,
  title = {Unsupervised Learning by Competing Hidden Units},
  volume = {116},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {16},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1820458116},
  author = {Krotov, Dmitry and Hopfield, John J.},
  month = apr,
  year = {2019},
  pages = {7723-7731},
  file = {/home/chay/Zotero/storage/ZHDMWD5A/Krotov and Hopfield - 2019 - Unsupervised learning by competing hidden units.pdf}
}

@article{henaffPerceptualStraighteningNatural2019,
  title = {Perceptual Straightening of Natural Videos},
  volume = {22},
  issn = {1097-6256, 1546-1726},
  language = {en},
  number = {6},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-019-0377-4},
  author = {H{\'e}naff, Olivier J. and Goris, Robbe L. T. and Simoncelli, Eero P.},
  month = jun,
  year = {2019},
  pages = {984-991},
  file = {/home/chay/Zotero/storage/TV3E8NBT/Hénaff et al. - 2019 - Perceptual straightening of natural videos.pdf}
}

@incollection{coatesEmergenceObjectSelectiveFeatures2012,
  title = {Emergence of {{Object}}-{{Selective Features}} in {{Unsupervised Feature Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Coates, Adam and Karpathy, Andrej and Ng, Andrew Y.},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {2681--2689},
  file = {/home/chay/Zotero/storage/36CUGCXF/Coates et al. - 2012 - Emergence of Object-Selective Features in Unsuperv.pdf;/home/chay/Zotero/storage/D4MXF5EB/4497-emergence-of-object-selective-features-in-unsupervised-feature-learning.html}
}

@article{brendelAPPROXIMATINGCNNSBAGOFLOCAL2019,
  title = {{{APPROXIMATING CNNS WITH BAG}}-{{OF}}-{{LOCAL}}- {{FEATURES MODELS WORKS SURPRISINGLY WELL ON IMAGENET}}},
  abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 \texttimes{} 33 px features and Alexnet performance for 17 \texttimes{} 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.},
  language = {en},
  author = {Brendel, Wieland and Bethge, Matthias},
  year = {2019},
  pages = {15},
  file = {/home/chay/Zotero/storage/KWME6ZIW/Brendel and Bethge - 2019 - APPROXIMATING CNNS WITH BAG-OF-LOCAL- FEATURES MOD.pdf}
}

@article{fardDeepMeansJointly2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.10069},
  primaryClass = {cs, stat},
  title = {Deep \$k\$-{{Means}}: {{Jointly}} Clustering with \$k\$-{{Means}} and Learning Representations},
  shorttitle = {Deep \$k\$-{{Means}}},
  abstract = {We study in this paper the problem of jointly clustering and learning representations. As several previous studies have shown, learning representations that are both faithful to the data to be clustered and adapted to the clustering algorithm can lead to better clustering performance, all the more so that the two tasks are performed jointly. We propose here such an approach for \$k\$-Means clustering based on a continuous reparametrization of the objective function that leads to a truly joint solution. The behavior of our approach is illustrated on various datasets showing its efficacy in learning representations for objects while clustering them.},
  journal = {arXiv:1806.10069 [cs, stat]},
  author = {Fard, Maziar Moradi and Thonet, Thibaut and Gaussier, Eric},
  month = jun,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/home/chay/Zotero/storage/T2IFEW4H/Fard et al. - 2018 - Deep $k$-Means Jointly clustering with $k$-Means .pdf;/home/chay/Zotero/storage/HGW4AMQK/1806.html}
}

@article{frosstAnalyzingImprovingRepresentations2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.01889},
  primaryClass = {cs, stat},
  title = {Analyzing and {{Improving Representations}} with the {{Soft Nearest Neighbor Loss}}},
  abstract = {We explore and expand the \$\textbackslash{}textit\{Soft Nearest Neighbor Loss\}\$ to measure the \$\textbackslash{}textit\{entanglement\}\$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that \$\textbackslash{}textit\{maximizing\}\$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.},
  journal = {arXiv:1902.01889 [cs, stat]},
  author = {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
  month = feb,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/home/chay/Zotero/storage/ISHR2QVK/Frosst et al. - 2019 - Analyzing and Improving Representations with the S.pdf;/home/chay/Zotero/storage/PR5F4LPR/1902.html}
}

@article{masciSparseSimilaritypreservingHashing2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5479},
  primaryClass = {cs},
  title = {Sparse Similarity-Preserving Hashing},
  abstract = {In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.},
  journal = {arXiv:1312.5479 [cs]},
  author = {Masci, Jonathan and Bronstein, Alex M. and Bronstein, Michael M. and Sprechmann, Pablo and Sapiro, Guillermo},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Data Structures and Algorithms},
  file = {/home/chay/Zotero/storage/FI6Z6U3A/Masci et al. - 2013 - Sparse similarity-preserving hashing.pdf;/home/chay/Zotero/storage/4FEU79LM/1312.html}
}

@inproceedings{gongIterativeQuantizationProcrustean2011,
  address = {{Colorado Springs, CO, USA}},
  title = {Iterative Quantization: {{A}} Procrustean Approach to Learning Binary Codes},
  isbn = {978-1-4577-0394-2},
  shorttitle = {Iterative Quantization},
  abstract = {This paper addresses the problem of learning similaritypreserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube. This method, dubbed iterative quantization (ITQ), has connections to multi-class spectral clustering and to the orthogonal Procrustes problem, and it can be used both with unsupervised data embeddings such as PCA and supervised embeddings such as canonical correlation analysis (CCA). Our experiments show that the resulting binary coding schemes decisively outperform several other state-of-the-art methods.},
  language = {en},
  booktitle = {{{CVPR}} 2011},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2011.5995432},
  author = {Gong, Yunchao and Lazebnik, Svetlana},
  month = jun,
  year = {2011},
  pages = {817-824},
  file = {/home/chay/Zotero/storage/T6NKXJYM/Gong and Lazebnik - 2011 - Iterative quantization A procrustean approach to .pdf}
}

@article{doCompactHashCode2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.02956},
  primaryClass = {cs},
  title = {Compact {{Hash Code Learning}} with {{Binary Deep Neural Network}}},
  abstract = {In this work, we firstly propose deep network models and learning algorithms for learning binary hash codes given image representations under both unsupervised and supervised manners. Then, by leveraging the powerful capacity of convolutional neural networks, we propose an end-to-end architecture which jointly learns to extract visual features and produce binary hash codes. Our novel network designs constrain one hidden layer to directly output the binary codes. This addresses a challenging issue in some previous works: optimizing nonsmooth objective functions due to binarization. Additionally, we incorporate independence and balance properties in the direct and strict forms into the learning schemes. Furthermore, we also include similarity preserving property in our objective functions. Our resulting optimizations involving these binary, independence, and balance constraints are difficult to solve. We propose to attack them with alternating optimization and careful relaxation. Experimental results on the benchmark datasets show that our proposed methods compare favorably with the state of the art.},
  journal = {arXiv:1712.02956 [cs]},
  author = {Do, Thanh-Toan and Tan, Dang-Khoa Le and Hoang, Tuan and Cheung, Ngai-Man},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/chay/Zotero/storage/2USSLTYH/Do et al. - 2017 - Compact Hash Code Learning with Binary Deep Neural.pdf;/home/chay/Zotero/storage/XXMRNJVH/1712.html}
}

@inproceedings{linDeepLearningBinary2015,
  address = {{Boston, MA, USA}},
  title = {Deep Learning of Binary Hash Codes for Fast Image Retrieval},
  isbn = {978-1-4673-6759-2},
  abstract = {Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-ofthe-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.},
  language = {en},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  publisher = {{IEEE}},
  doi = {10.1109/CVPRW.2015.7301269},
  author = {Lin, Kevin and Yang, Huei-Fang and Hsiao, Jen-Hao and Chen, Chu-Song},
  month = jun,
  year = {2015},
  pages = {27-35},
  file = {/home/chay/Zotero/storage/WB27DLNS/Lin et al. - 2015 - Deep learning of binary hash codes for fast image .pdf}
}

@article{henaffTestingMechanismTemporal,
  title = {Testing a Mechanism for Temporal Prediction in Perceptual, Neural, and Machine Representations},
  language = {en},
  author = {H{\'e}naff, Olivier J},
  pages = {120},
  file = {/home/chay/Zotero/storage/8LJ3CEJH/henaff-phd.pdf}
}

@article{cherianEfficientNearestNeighbors2014,
  title = {Efficient {{Nearest Neighbors}} via {{Robust Sparse Hashing}}},
  volume = {23},
  issn = {1057-7149, 1941-0042},
  number = {8},
  journal = {IEEE Transactions on Image Processing},
  doi = {10.1109/TIP.2014.2324280},
  author = {Cherian, Anoop and Sra, Suvrit and Morellas, Vassilios and Papanikolopoulos, Nikolaos},
  month = aug,
  year = {2014},
  pages = {3646-3655}
}

@inproceedings{yangSemanticStructurebasedUnsupervised2018,
  address = {{Stockholm, Sweden}},
  title = {Semantic {{Structure}}-Based {{Unsupervised Deep Hashing}}},
  isbn = {978-0-9992411-2-7},
  abstract = {Hashing is becoming increasingly popular for approximate nearest neighbor searching in massive databases due to its storage and search efficiency. Recent supervised hashing methods, which usually construct semantic similarity matrices to guide hash code learning using label information, have shown promising results. However, it is relatively difficult to capture and utilize the semantic relationships between points in unsupervised settings. To address this problem, we propose a novel unsupervised deep framework called Semantic Structurebased unsupervised Deep Hashing (SSDH). We first empirically study the deep feature statistics, and find that the distribution of the cosine distance for point pairs can be estimated by two half Gaussian distributions. Based on this observation, we construct the semantic structure by considering points with distances obviously smaller than the others as semantically similar and points with distances obviously larger than the others as semantically dissimilar. We then design a deep architecture and a pair-wise loss function to preserve this semantic structure in Hamming space. Extensive experiments show that SSDH significantly outperforms current state-of-the-art methods.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2018/148},
  author = {Yang, Erkun and Deng, Cheng and Liu, Tongliang and Liu, Wei and Tao, Dacheng},
  month = jul,
  year = {2018},
  pages = {1064-1070},
  file = {/home/chay/Zotero/storage/Y2MVXNUM/Yang et al. - 2018 - Semantic Structure-based Unsupervised Deep Hashing.pdf}
}

@inproceedings{linDiscriminativeDeepHashing2017,
  address = {{Melbourne, Australia}},
  title = {Discriminative {{Deep Hashing}} for {{Scalable Face Image Retrieval}}},
  isbn = {978-0-9992411-0-3},
  abstract = {With the explosive growth of images containing faces, scalable face image retrieval has attracted increasing attention. Due to the amazing effectiveness, deep hashing has become a popular hashing method recently. In this work, we propose a new Discriminative Deep Hashing (DDH) network to learn discriminative and compact hash codes for large-scale face image retrieval. The proposed network incorporates the end-to-end learning, the divide-and-encode module and the desired discrete code learning into a unified framework. Specifically, a network with a stack of convolution-pooling layers is proposed to extract multi-scale and robust features by merging the outputs of the third max pooling layer and the fourth convolutional layer. To reduce the redundancy among hash codes and the network parameters simultaneously, a divide-andencode module to generate compact hash codes. Moreover, a loss function is introduced to minimize the prediction errors of the learned hash codes, which can lead to discriminative hash codes. Extensive experiments on two datasets demonstrate that the proposed method achieves superior performance compared with some state-of-the-art hashing methods.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2017/315},
  author = {Lin, Jie and Li, Zechao and Tang, Jinhui},
  month = aug,
  year = {2017},
  pages = {2266-2272},
  file = {/home/chay/Zotero/storage/MIR2ZM5A/Lin et al. - 2017 - Discriminative Deep Hashing for Scalable Face Imag.pdf}
}

@article{yangDistillHashUnsupervisedDeep2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.03465},
  primaryClass = {cs},
  title = {{{DistillHash}}: {{Unsupervised Deep Hashing}} by {{Distilling Data Pairs}}},
  shorttitle = {{{DistillHash}}},
  abstract = {Due to the high storage and search efficiency, hashing has become prevalent for large-scale similarity search. Particularly, deep hashing methods have greatly improved the search performance under supervised scenarios. In contrast, unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable supervisory similarity signals. To address this issue, we propose a novel deep unsupervised hashing model, dubbed DistillHash, which can learn a distilled data set consisted of data pairs, which have confidence similarity signals. Specifically, we investigate the relationship between the initial noisy similarity signals learned from local structures and the semantic similarity labels assigned by a Bayes optimal classifier. We show that under a mild assumption, some data pairs, of which labels are consistent with those assigned by the Bayes optimal classifier, can be potentially distilled. Inspired by this fact, we design a simple yet effective strategy to distill data pairs automatically and further adopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the proposed DistillHash consistently accomplishes the state-of-the-art search performance.},
  journal = {arXiv:1905.03465 [cs]},
  author = {Yang, Erkun and Liu, Tongliang and Deng, Cheng and Liu, Wei and Tao, Dacheng},
  month = may,
  year = {2019},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/chay/Zotero/storage/PAWJSKEY/Yang et al. - 2019 - DistillHash Unsupervised Deep Hashing by Distilli.pdf;/home/chay/Zotero/storage/QUTESRLF/1905.html}
}

@article{jinUnsupervisedSemanticDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  primaryClass = {cs},
  title = {Unsupervised {{Semantic Deep Hashing}}},
  abstract = {In recent years, deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However, these methods are mostly supervised. In real-world application, it is a time-consuming and overloaded task for annotating a large number of images. In this paper, we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method, namely unsupervised semantic deep hashing (\textbackslash{}textbf\{USDH\}), uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model: 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy, and 4) invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have demonstrated that \textbackslash{}textbf\{USDH\} outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.},
  journal = {arXiv:1803.06911 [cs]},
  author = {Jin, Sheng},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/chay/Zotero/storage/327GWXAY/Jin - 2018 - Unsupervised Semantic Deep Hashing.pdf;/home/chay/Zotero/storage/DU8SZNTJ/1803.html}
}

@inproceedings{kalantidisLocallyOptimizedProduct2014,
  address = {{Columbus, OH, USA}},
  title = {Locally {{Optimized Product Quantization}} for {{Approximate Nearest Neighbor Search}}},
  isbn = {978-1-4799-5118-5},
  abstract = {We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate nearest neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to provide non-exhaustive search, i.e., inverted lists or a multiindex, the idea is to locally optimize an individual product quantizer (PQ) per cell and use it to encode residuals. Local optimization is over rotation and space decomposition; interestingly, we apply a parametric solution that assumes a normal distribution and is extremely fast to train. With a reasonable space and time overhead that is constant in the data size, we set a new state-of-the-art on several public datasets, including a billion-scale one.},
  language = {en},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2014.298},
  author = {Kalantidis, Yannis and Avrithis, Yannis},
  month = jun,
  year = {2014},
  pages = {2329-2336},
  file = {/home/chay/Zotero/storage/YS4S4ITG/Kalantidis and Avrithis - 2014 - Locally Optimized Product Quantization for Approxi.pdf}
}

@inproceedings{yanxiaSparseProjectionsHighdimensional2015,
  address = {{Boston, MA, USA}},
  title = {Sparse Projections for High-Dimensional Binary Codes},
  isbn = {978-1-4673-6964-0},
  abstract = {This paper addresses the problem of learning long binary codes from high-dimensional data. We observe that two key challenges arise while learning and using long binary codes: (1) lack of an effective regularizer for the learned high-dimensional mapping and (2) high computational cost for computing long codes. In this paper, we overcome both these problems by introducing a sparsity encouraging regularizer that reduces the effective number of parameters involved in the learned projection operator. This regularizer not only reduces overfitting but, due to the sparse nature of the projection matrix, also leads to a dramatic reduction in the computational cost. To evaluate the effectiveness of our method, we analyze its performance on the problems of nearest neighbour search, image retrieval and image classification. Experiments on a number of challenging datasets show that our method leads to better accuracy than dense projections (ITQ [11] and LSH [16]) with the same code lengths, and meanwhile is over an order of magnitude faster. Furthermore, our method is also more accurate and faster than other recently proposed methods for speeding up high-dimensional binary encoding.},
  language = {en},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2015.7298954},
  author = {{Yan Xia} and He, Kaiming and Kohli, Pushmeet and Sun, Jian},
  month = jun,
  year = {2015},
  pages = {3332-3339},
  file = {/home/chay/Zotero/storage/ZQRC9K37/Yan Xia et al. - 2015 - Sparse projections for high-dimensional binary cod.pdf}
}

@inproceedings{yangSemanticStructurebasedUnsupervised2018a,
  address = {{Stockholm, Sweden}},
  title = {Semantic {{Structure}}-Based {{Unsupervised Deep Hashing}}},
  isbn = {978-0-9992411-2-7},
  abstract = {Hashing is becoming increasingly popular for approximate nearest neighbor searching in massive databases due to its storage and search efficiency. Recent supervised hashing methods, which usually construct semantic similarity matrices to guide hash code learning using label information, have shown promising results. However, it is relatively difficult to capture and utilize the semantic relationships between points in unsupervised settings. To address this problem, we propose a novel unsupervised deep framework called Semantic Structurebased unsupervised Deep Hashing (SSDH). We first empirically study the deep feature statistics, and find that the distribution of the cosine distance for point pairs can be estimated by two half Gaussian distributions. Based on this observation, we construct the semantic structure by considering points with distances obviously smaller than the others as semantically similar and points with distances obviously larger than the others as semantically dissimilar. We then design a deep architecture and a pair-wise loss function to preserve this semantic structure in Hamming space. Extensive experiments show that SSDH significantly outperforms current state-of-the-art methods.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2018/148},
  author = {Yang, Erkun and Deng, Cheng and Liu, Tongliang and Liu, Wei and Tao, Dacheng},
  month = jul,
  year = {2018},
  pages = {1064-1070},
  file = {/home/chay/Zotero/storage/HTR8SUF8/Yang et al. - 2018 - Semantic Structure-based Unsupervised Deep Hashing.pdf}
}

@incollection{wuMultiscaleQuantizationFast2017a,
  title = {Multiscale {{Quantization}} for {{Fast Similarity Search}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Wu, Xiang and Guo, Ruiqi and Suresh, Ananda Theertha and Kumar, Sanjiv and {Holtmann-Rice}, Daniel N and Simcha, David and Yu, Felix},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5745--5755},
  file = {/home/chay/Zotero/storage/9SIKBAXT/Wu et al. - 2017 - Multiscale Quantization for Fast Similarity Search.pdf;/home/chay/Zotero/storage/WDCG96D5/7157-multiscale-quantization-for-fast-similarity-search.html}
}

@article{sablayrollesSPREADINGVECTORSSIMILARITY2019,
  title = {{{SPREADING VECTORS FOR SIMILARITY SEARCH}}},
  abstract = {Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko\textendash{}Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer. The code is available online1.},
  language = {en},
  author = {Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Jegou, Herve},
  year = {2019},
  pages = {13},
  file = {/home/chay/Zotero/storage/XMEVEJRB/Sablayrolles et al. - 2019 - SPREADING VECTORS FOR SIMILARITY SEARCH.pdf}
}

@inproceedings{yagnikPowerComparativeReasoning2011,
  address = {{Barcelona, Spain}},
  title = {The Power of Comparative Reasoning},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
  abstract = {Rank correlation measures are known for their resilience to perturbations in numeric values and are widely used in many evaluation metrics. Such ordinal measures have rarely been applied in treatment of numeric features as a representational transformation. We emphasize the benefits of ordinal representations of input features both theoretically and empirically. We present a family of algorithms for computing ordinal embeddings based on partial order statistics. Apart from having the stability benefits of ordinal measures, these embeddings are highly nonlinear, giving rise to sparse feature spaces highly favored by several machine learning methods. These embeddings are deterministic, data independent and by virtue of being based on partial order statistics, add another degree of resilience to noise. These machine-learning-free methods when applied to the task of fast similarity search outperform state-of-theart machine learning methods with complex optimization setups. For solving classification problems, the embeddings provide a nonlinear transformation resulting in sparse binary codes that are well-suited for a large class of machine learning algorithms. These methods show significant improvement on VOC 2010 using simple linear classifiers which can be trained quickly. Our method can be extended to the case of polynomial kernels, while permitting very efficient computation. Further, since the popular MinHash algorithm is a special case of our method, we demonstrate an efficient scheme for computing MinHash on conjunctions of binary features. The actual method can be implemented in about 10 lines of code in most languages (2 lines in MATLAB), and does not require any data-driven optimization.},
  language = {en},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV.2011.6126527},
  author = {Yagnik, Jay and Strelow, Dennis and Ross, David A. and Lin, Ruei-sung},
  month = nov,
  year = {2011},
  pages = {2431-2438},
  file = {/home/chay/Zotero/storage/RHF955GD/Yagnik et al. - 2011 - The power of comparative reasoning.pdf}
}

@inproceedings{jegouSearchingOneBillion2011,
  address = {{Prague, Czech Republic}},
  title = {Searching in One Billion Vectors: {{Re}}-Rank with Source Coding},
  isbn = {978-1-4577-0538-0},
  shorttitle = {Searching in One Billion Vectors},
  abstract = {Recent indexing techniques inspired by source coding have been shown successful to index billions of high-dimensional vectors in memory. In this paper, we propose an approach that re-ranks the neighbor hypotheses obtained by these compressed-domain indexing methods. In contrast to the usual post-verification scheme, which performs exact distance calculation on the short-list of hypotheses, the estimated distances are refined based on short quantization codes, to avoid reading the full vectors from disk.},
  language = {en},
  booktitle = {2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2011.5946540},
  author = {Jegou, Herve and Tavenard, Romain and Douze, Matthijs and Amsaleg, Laurent},
  month = may,
  year = {2011},
  pages = {861-864},
  file = {/home/chay/Zotero/storage/QRV77Y94/Jegou et al. - 2011 - Searching in one billion vectors Re-rank with sou.pdf}
}

@article{jegouProductQuantizationNearest2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  volume = {33},
  issn = {0162-8828},
  abstract = {This paper introduces a product quantization based approach for approximate nearest neighbor search. The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The Euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code.},
  language = {en},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2010.57},
  author = {J{\'e}gou, H and Douze, M and Schmid, C},
  month = jan,
  year = {2011},
  pages = {117-128},
  file = {/home/chay/Zotero/storage/AC3WKRVK/Jégou et al. - 2011 - Product Quantization for Nearest Neighbor Search.pdf}
}

@article{liModelingWinnerTakeAllCompetition2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.11959},
  primaryClass = {cs, stat},
  title = {Modeling {{Winner}}-{{Take}}-{{All Competition}} in {{Sparse Binary Projections}}},
  abstract = {Inspired by the advances in biological science, the study of sparse binary projection models has attracted considerable recent research attention. The models project dense input samples into a higher-dimensional space and output sparse binary vectors after Winner-Take-All competition, subject to the constraint that the projection matrix is also sparse and binary. Following the work along this line, we developed a supervised-WTA model under the supervised setting where training samples with both input and output representations are available, from which the projection matrix can be obtained with a simple, efficient yet effective algorithm. We further extended the model and the algorithm to an unsupervised setting where only the input representation of the samples is available. In a series of empirical evaluation on similarity search tasks, both models reported significantly improved results over the state-of-the-art methods in both search accuracy and running time. The successful results give us strong confidence that the proposed work provides a highly practical tool to real world applications.},
  journal = {arXiv:1907.11959 [cs, stat]},
  author = {Li, Wenye},
  month = jul,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/home/chay/Zotero/storage/55RUTZDN/Li - 2019 - Modeling Winner-Take-All Competition in Sparse Bin.pdf;/home/chay/Zotero/storage/S8XF97P3/1907.html}
}

@article{krotovUnsupervisedLearningCompeting2019a,
  title = {Unsupervised Learning by Competing Hidden Units},
  volume = {116},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {16},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1820458116},
  author = {Krotov, Dmitry and Hopfield, John J.},
  month = apr,
  year = {2019},
  pages = {7723-7731},
  file = {/home/chay/Zotero/storage/D8AF5NMF/Krotov and Hopfield - 2019 - Unsupervised learning by competing hidden units.pdf}
}

@incollection{liFastSimilaritySearch2018a,
  title = {Fast {{Similarity Search}} via {{Optimal Sparse Lifting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Li, Wenye and Mao, Jingwei and Zhang, Yin and Cui, Shuguang},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {176--184},
  file = {/home/chay/Zotero/storage/VNPNPYT6/Li et al. - 2018 - Fast Similarity Search via Optimal Sparse Lifting.pdf;/home/chay/Zotero/storage/VFIB4F8D/7302-fast-similarity-search-via-optimal-sparse-lifting.html}
}

@inproceedings{jae-pilheoSphericalHashing2012,
  address = {{Providence, RI}},
  title = {Spherical Hashing},
  isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
  abstract = {Many binary code encoding schemes based on hashing have been actively studied recently, since they can provide efficient similarity search, especially nearest neighbor search, and compact data representations suitable for handling large scale image databases in many computer vision problems. Existing hashing techniques encode highdimensional data points by using hyperplane-based hashing functions. In this paper we propose a novel hyperspherebased hashing function, spherical hashing, to map more spatially coherent data points into a binary code compared to hyperplane-based hashing functions. Furthermore, we propose a new binary code distance function, spherical Hamming distance, that is tailored to our hyperspherebased binary coding scheme, and design an efficient iterative optimization process to achieve balanced partitioning of data points for each hash function and independence between hashing functions. Our extensive experiments show that our spherical hashing technique significantly outperforms six state-of-the-art hashing techniques based on hyperplanes across various image benchmarks of sizes ranging from one to 75 million of GIST descriptors. The performance gains are consistent and large, up to 100\% improvements. The excellent results confirm the unique merits of the proposed idea in using hyperspheres to encode proximity regions in high-dimensional spaces. Finally, our method is intuitive and easy to implement.},
  language = {en},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2012.6248024},
  author = {{Jae-Pil Heo} and {Youngwoon Lee} and {Junfeng He} and {Shih-Fu Chang} and {Sung-Eui Yoon}},
  month = jun,
  year = {2012},
  pages = {2957-2964},
  file = {/home/chay/Zotero/storage/5XZX5KR3/Jae-Pil Heo et al. - 2012 - Spherical hashing.pdf}
}

@incollection{weissSpectralHashing2009,
  title = {Spectral {{Hashing}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 21},
  publisher = {{Curran Associates, Inc.}},
  author = {Weiss, Yair and Torralba, Antonio and Fergus, Rob},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  pages = {1753--1760},
  file = {/home/chay/Zotero/storage/J5UP2PSK/Weiss et al. - 2009 - Spectral Hashing.pdf;/home/chay/Zotero/storage/BW9FNQ2W/3383-spectral-hashing.html}
}

@inproceedings{indykApproximateNearestNeighbors1998,
  address = {{Dallas, Texas, United States}},
  title = {Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality},
  isbn = {978-0-89791-962-3},
  shorttitle = {Approximate Nearest Neighbors},
  abstract = {The nearest neighbor problem is the following: Given a set of n points P = f g p1; : : :; pn in some metric space X, preprocess P so as to e ciently answer queries which require nding the point in P closest to a query point q 2 X. We focus on the particularly interesting case of the d-dimensional Euclidean space where X = {$<$}d under some lp norm. Despite decades of e ort, the current solutions are far from satisfactory; in fact, for large d, in theory or in practice, they provide little improvement over the brute-force algorithm which compares the query point to each data point. Of late, there has been some interest in the approximate nearest neighbors problem, which is: Find a point p 2 P that is an -approximate nearest neighbor of the query q in that for all p0 2 P , d(p; q) (1 + )d(p0; q).},
  language = {en},
  booktitle = {Proceedings of the Thirtieth Annual {{ACM}} Symposium on {{Theory}} of Computing  - {{STOC}} '98},
  publisher = {{ACM Press}},
  doi = {10.1145/276698.276876},
  author = {Indyk, Piotr and Motwani, Rajeev},
  year = {1998},
  pages = {604-613},
  file = {/home/chay/Zotero/storage/PCHI8IWQ/Indyk and Motwani - 1998 - Approximate nearest neighbors towards removing th.pdf}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  address = {{Doha, Qatar}},
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/D14-1162},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532-1543},
  file = {/home/chay/Zotero/storage/I4FCZ9I4/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {0018-9219},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  number = {11},
  journal = {Proceedings of the IEEE},
  doi = {10.1109/5.726791},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  month = nov,
  year = {1998},
  keywords = {Neural networks,2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,Feature extraction,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,Machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,performance measure minimization,Principal component analysis,segmentation recognition},
  pages = {2278-2324},
  file = {/home/chay/Zotero/storage/3GXES9AN/726791.html}
}

@article{carandiniNormalizationCanonicalNeural2011,
  title = {Normalization as a Canonical Neural Computation},
  volume = {13},
  issn = {1471-003X},
  abstract = {There is increasing evidence that the brain relies on a set of canonical neural computations, repeating them across brain regions and modalities to apply similar operations to different problems. A promising candidate for such a computation is normalization, in which the responses of neurons are divided by a common factor that typically includes the summed activity of a pool of neurons. Normalization was developed to explain responses in the primary visual cortex and is now thought to operate throughout the visual system, and in many other sensory modalities and brain regions. Normalization may underlie operations such as the representation of odours, the modulatory effects of visual attention, the encoding of value and the integration of multisensory information. Its presence in such a diversity of neural systems in multiple species, from invertebrates to mammals, suggests that it serves as a canonical neural computation.},
  number = {1},
  journal = {Nature reviews. Neuroscience},
  doi = {10.1038/nrn3136},
  author = {Carandini, Matteo and Heeger, David J.},
  month = nov,
  year = {2011},
  pages = {51-62},
  file = {/home/chay/Zotero/storage/ILSYLDUV/Carandini and Heeger - 2011 - Normalization as a canonical neural computation.pdf},
  pmid = {22108672},
  pmcid = {PMC3273486}
}

@incollection{srivastavaCompeteCompute2013,
  title = {Compete to {{Compute}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  author = {Srivastava, Rupesh K and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {2310--2318},
  file = {/home/chay/Zotero/storage/QJVQHCPQ/Srivastava et al. - 2013 - Compete to Compute.pdf;/home/chay/Zotero/storage/DZHPTKPV/5059-compete-to-compute.html}
}

@article{xiaoResistingAdversarialAttacks2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.10510},
  primaryClass = {cs, stat},
  title = {Resisting {{Adversarial Attacks}} by \$k\$-{{Winners}}-{{Take}}-{{All}}},
  abstract = {We propose a simple change to the current neural network structure for defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of \$k\$-Winners-Take-All (\$k\$-WTA) activation, a \$C\^0\$ discontinuous function that purposely invalidates the neural network model's gradient at densely distributed input data points. Our proposal is theoretically rationalized. We show why the discontinuities in \$k\$-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training. This understanding is also empirically backed. Even without notoriously expensive adversarial training, the robustness performance of our networks is comparable to conventional ReLU networks optimized by adversarial training. Furthermore, after also optimized through adversarial training, our networks outperform the state-of-the-art methods under white-box attacks on various datasets that we experimented with.},
  journal = {arXiv:1905.10510 [cs, stat]},
  author = {Xiao, Chang and Zhong, Peilin and Zheng, Changxi},
  month = may,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms,Computer Science - Cryptography and Security},
  file = {/home/chay/Zotero/storage/ZUQC4J78/Xiao et al. - 2019 - Resisting Adversarial Attacks by $k$-Winners-Take-.pdf;/home/chay/Zotero/storage/YJNQ79FH/1905.html}
}

@article{barrettAnalyzingBiologicalArtificial2019,
  series = {Machine {{Learning}}, {{Big Data}}, and {{Neuroscience}}},
  title = {Analyzing Biological and Artificial Neural Networks: Challenges with Opportunities for Synergy?},
  volume = {55},
  issn = {0959-4388},
  shorttitle = {Analyzing Biological and Artificial Neural Networks},
  abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks.},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2019.01.007},
  author = {Barrett, David GT and Morcos, Ari S and Macke, Jakob H},
  month = apr,
  year = {2019},
  pages = {55-64},
  file = {/home/chay/Zotero/storage/YQ8BPSVI/Barrett et al. - 2019 - Analyzing biological and artificial neural network.pdf;/home/chay/Zotero/storage/UWX2GR9S/S0959438818301569.html}
}

@article{hassabisNeuroscienceInspiredArtificialIntelligence2017,
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  volume = {95},
  issn = {08966273},
  language = {en},
  number = {2},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2017.06.011},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  month = jul,
  year = {2017},
  pages = {245-258},
  file = {/home/chay/Zotero/storage/TJWF4JDZ/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf}
}

@article{caronRandomConvergenceOlfactory2013,
  title = {Random Convergence of Olfactory Inputs in the {{{\emph{Drosophila}}}} Mushroom Body},
  volume = {497},
  copyright = {2013 Nature Publishing Group},
  issn = {1476-4687},
  abstract = {The mushroom body in the fruitfly Drosophila melanogaster is an associative brain centre that translates odour representations into learned behavioural responses1. Kenyon cells, the intrinsic neurons of the mushroom body, integrate input from olfactory glomeruli to encode odours as sparse distributed patterns of neural activity2,3. We have developed anatomic tracing techniques to identify the glomerular origin of the inputs that converge onto 200 individual Kenyon cells. Here we show that each Kenyon cell integrates input from a different and apparently random combination of glomeruli. The glomerular inputs to individual Kenyon cells show no discernible organization with respect to their odour tuning, anatomic features or developmental origins. Moreover, different classes of Kenyon cells do not seem to preferentially integrate inputs from specific combinations of glomeruli. This organization of glomerular connections to the mushroom body could allow the fly to contextualize novel sensory experiences, a feature consistent with the role of this brain centre in mediating learned olfactory associations and behaviours.},
  language = {en},
  number = {7447},
  journal = {Nature},
  doi = {10.1038/nature12063},
  author = {Caron, Sophie J. C. and Ruta, Vanessa and Abbott, L. F. and Axel, Richard},
  month = may,
  year = {2013},
  pages = {113-117},
  file = {/home/chay/Zotero/storage/UAS2HEZI/Caron et al. - 2013 - Random convergence of olfactory inputs in the iD.pdf;/home/chay/Zotero/storage/7GWDZMXB/nature12063.html}
}

@article{stevensStatisticalPropertyFly2016,
  title = {A Statistical Property of Fly Odor Responses Is Conserved across Odors},
  volume = {113},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {24},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1606339113},
  author = {Stevens, Charles F.},
  month = jun,
  year = {2016},
  pages = {6737-6742},
  file = {/home/chay/Zotero/storage/3PQBAZIU/Stevens - 2016 - A statistical property of fly odor responses is co.pdf}
}

@article{pehlevanNeuroscienceinspiredOnlineUnsupervised2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1908.01867},
  primaryClass = {cs, q-bio},
  title = {Neuroscience-Inspired Online Unsupervised Learning Algorithms},
  abstract = {Although the currently popular deep learning networks achieve unprecedented performance on some tasks, the human brain still has a monopoly on general intelligence. Motivated by this and biological implausibility of deep learning networks, we developed a family of biologically plausible artificial neural networks (NNs) for unsupervised learning. Our approach is based on optimizing principled objective functions containing a term that matches the pairwise similarity of outputs to the similarity of inputs, hence the name - similarity-based. Gradient-based online optimization of such similarity-based objective functions can be implemented by NNs with biologically plausible local learning rules. Similarity-based cost functions and associated NNs solve unsupervised learning tasks such as linear dimensionality reduction, sparse and/or nonnegative feature extraction, blind nonnegative source separation, clustering and manifold learning.},
  journal = {arXiv:1908.01867 [cs, q-bio]},
  author = {Pehlevan, Cengiz and Chklovskii, Dmitri B.},
  month = aug,
  year = {2019},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/chay/Zotero/storage/EEKCZ2I2/Pehlevan and Chklovskii - 2019 - Neuroscience-inspired online unsupervised learning.pdf;/home/chay/Zotero/storage/CVRK4ELU/1908.html}
}

@article{olshausenSparseCodingSensory2004b,
  title = {Sparse Coding of Sensory Inputs},
  volume = {14},
  issn = {09594388},
  language = {en},
  number = {4},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2004.07.007},
  author = {Olshausen, B and Field, D},
  month = aug,
  year = {2004},
  pages = {481-487},
  file = {/home/chay/Zotero/storage/LUH2SNIA/Olshausen and Field - 2004 - Sparse coding of sensory inputs.pdf}
}

@article{valiantWhatMustGlobal2014,
  title = {What Must a Global Theory of Cortex Explain?},
  volume = {25},
  issn = {09594388},
  language = {en},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2013.10.006},
  author = {Valiant, Leslie G},
  month = apr,
  year = {2014},
  pages = {15-19},
  file = {/home/chay/Zotero/storage/LQDT5NXH/Valiant - 2014 - What must a global theory of cortex explain.pdf}
}

@article{sharmaImprovingSimilaritySearch2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  primaryClass = {cs, stat},
  title = {Improving {{Similarity Search}} with {{High}}-Dimensional {{Locality}}-Sensitive {{Hashing}}},
  abstract = {We propose a new class of data-independent locality-sensitive hashing (LSH) algorithms based on the fruit fly olfactory circuit. The fundamental difference of this approach is that, instead of assigning hashes as dense points in a low dimensional space, hashes are assigned in a high dimensional space, which enhances their separability. We show theoretically and empirically that this new family of hash functions is locality-sensitive and preserves rank similarity for inputs in any `p space. We then analyze different variations on this strategy and show empirically that they outperform existing LSH methods for nearest-neighbors search on six benchmark datasets. Finally, we propose a multi-probe version of our algorithm that achieves higher performance for the same query time, or conversely, that maintains performance of prior approaches while taking significantly less indexing time and memory. Overall, our approach leverages the advantages of separability provided by high-dimensional spaces, while still remaining computationally efficient},
  journal = {arXiv:1812.01844 [cs, stat]},
  author = {Sharma, Jaiyam and Navlakha, Saket},
  month = dec,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/home/chay/Zotero/storage/2VUL9UVJ/Sharma and Navlakha - 2018 - Improving Similarity Search with High-dimensional .pdf;/home/chay/Zotero/storage/IVCPTGQA/1812.html}
}

@article{wangHashingSimilaritySearch2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  primaryClass = {cs},
  title = {Hashing for {{Similarity Search}}: {{A Survey}}},
  shorttitle = {Hashing for {{Similarity Search}}},
  abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
  journal = {arXiv:1408.2927 [cs]},
  author = {Wang, Jingdong and Shen, Heng Tao and Song, Jingkuan and Ji, Jianqiu},
  month = aug,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Data Structures and Algorithms,Computer Science - Databases},
  file = {/home/chay/Zotero/storage/6PQ9WAER/Wang et al. - 2014 - Hashing for Similarity Search A Survey.pdf;/home/chay/Zotero/storage/RG9LXWUD/1408.html}
}

@techreport{krizhevskyLearningMultipleLayers2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  language = {en},
  author = {Krizhevsky, Alex},
  year = {2009},
  pages = {60},
  file = {/home/chay/Zotero/storage/Z4BJPB6U/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@article{dasguptaNeuralAlgorithmFundamental2017a,
  title = {A Neural Algorithm for a Fundamental Computing Problem},
  language = {en},
  journal = {Science},
  author = {Dasgupta, Sanjoy and Stevens, Charles F and Navlakha, Saket},
  year = {2017},
  pages = {5},
  file = {/home/chay/Zotero/storage/HHV7V7T9/Dasgupta et al. - 2017 - A neural algorithm for a fundamental computing pro.pdf}
}

@inproceedings{chenLearningDeepUnsupervised2018,
  address = {{Stockholm, Sweden}},
  title = {Learning {{Deep Unsupervised Binary Codes}} for {{Image Retrieval}}},
  isbn = {978-0-9992411-2-7},
  abstract = {Hashing is an efficient approximate nearest neighbor search method. It has been widely adopted for large-scale multimedia retrieval. While supervised learning is popular for the data-dependent hashing, deep unsupervised hashing methods can learn non-linear transformations for converting multimedia inputs to binary codes without label information. Most of the existing deep unsupervised hashing methods make use of a quadratic constraint for minimizing the difference between the compact representations and the target binary codes, which inevitably causes severe information loss. In this paper, we propose a novel deep unsupervised method called DeepQuan for hashing. The DeepQuan model utilizes a deep autoencoder network, where the encoder is used to learn compact representations and the decoder is for manifold preservation. To contrast with the existing unsupervised methods, DeepQuan learns the binary codes by minimizing the quantization error through product quantization. Furthermore, a weighted triplet loss is proposed to avoid trivial solutions and poor generalization. Extensive experimental results on standard datasets show that the proposed DeepQuan model outperforms the state-of-the-art unsupervised hashing methods for image retrieval tasks.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2018/85},
  author = {Chen, Junjie and Cheung, William K. and Wang, Anran},
  month = jul,
  year = {2018},
  pages = {613-619},
  file = {/home/chay/Zotero/storage/44B5NWHB/Chen et al. - 2018 - Learning Deep Unsupervised Binary Codes for Image .pdf}
}

@misc{LearningCompactBinary,
  title = {Learning {{Compact Binary Descriptors}} with {{Unsupervised Deep Neural Networks}} - {{IEEE Conference Publication}}},
  howpublished = {https://ieeexplore.ieee.org/document/7780502},
  file = {/home/chay/Zotero/storage/DLTEV8BN/7780502.html}
}

@article{tissierNearLosslessBinarizationWord2019a,
  title = {Near-{{Lossless Binarization}} of {{Word Embeddings}}},
  volume = {33},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  issn = {2374-3468},
  language = {en},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v33i01.33017104},
  author = {Tissier, Julien and Gravier, Christophe and Habrard, Amaury},
  month = jul,
  year = {2019},
  pages = {7104-7111},
  file = {/home/chay/Zotero/storage/P7NXWF3A/Tissier et al. - 2019 - Near-Lossless Binarization of Word Embeddings.pdf;/home/chay/Zotero/storage/AH5J8H5X/4692.html}
}

@article{jinUnsupervisedSemanticDeep2019,
  title = {Unsupervised Semantic Deep Hashing},
  volume = {351},
  issn = {0925-2312},
  abstract = {In recent years, deep hashing methods have been proved to be effective since it employs convolutional neural network to learn features and hashing codes simultaneously. However, these methods are mostly supervised. In real-world applications, it is a time-consuming and overloaded task for annotating a large number of images. In this paper, we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method, namely unsupervised semantic deep hashing (USDH), uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model: 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy, and 4) invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have demonstrated that USDH outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.},
  journal = {Neurocomputing},
  doi = {10.1016/j.neucom.2019.01.020},
  author = {Jin, Sheng and Yao, Hongxun and Sun, Xiaoshuai and Zhou, Shangchen},
  month = jul,
  year = {2019},
  keywords = {Deep learning,Semantic loss,Unsupervised hashing},
  pages = {19-25},
  file = {/home/chay/Zotero/storage/5NGSEREI/Jin et al. - 2019 - Unsupervised semantic deep hashing.pdf;/home/chay/Zotero/storage/738AVT6B/S0925231219300323.html}
}

@article{kornblithSimilarityNeuralNetwork,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  language = {en},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  pages = {11},
  file = {/home/chay/Zotero/storage/LLRANIYB/Kornblith et al. - Similarity of Neural Network Representations Revis.pdf}
}

@inproceedings{grettonMeasuringStatisticalDependence2005,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Measuring {{Statistical Dependence}} with {{Hilbert}}-{{Schmidt Norms}}},
  isbn = {978-3-540-31696-1},
  abstract = {We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.},
  language = {en},
  booktitle = {Algorithmic {{Learning Theory}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  editor = {Jain, Sanjay and Simon, Hans Ulrich and Tomita, Etsuji},
  year = {2005},
  keywords = {Covariance Operator,Independence Criterion,Independent Component Analysis,Reproduce Kernel Hilbert Space},
  pages = {63-77}
}

@article{cortesAlgorithmsLearningKernels,
  title = {Algorithms for {{Learning Kernels Based}} on {{Centered Alignment}}},
  abstract = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression.},
  language = {en},
  author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  pages = {34},
  file = {/home/chay/Zotero/storage/7XZUGB98/Cortes et al. - Algorithms for Learning Kernels Based on Centered .pdf}
}

@incollection{cristianiniKernelTargetAlignment2002,
  title = {On {{Kernel}}-{{Target Alignment}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  publisher = {{MIT Press}},
  author = {Cristianini, Nello and {Shawe-Taylor}, John and Elisseeff, Andr{\'e} and Kandola, Jaz S.},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  year = {2002},
  pages = {367--373},
  file = {/home/chay/Zotero/storage/U8ZR9K9R/Cristianini et al. - 2002 - On Kernel-Target Alignment.pdf;/home/chay/Zotero/storage/EZ4323BY/1946-on-kernel-target-alignment.html}
}

@article{neyshaburSymmetricAsymmetricLSHs,
  title = {On {{Symmetric}} and {{Asymmetric LSHs}} for {{Inner Product Search}}},
  abstract = {We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li (2014a) argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required.},
  language = {en},
  author = {Neyshabur, Behnam and Srebro, Nathan},
    year = {2015},
  pages = {9},
  file = {/home/chay/Zotero/storage/P275E87X/Neyshabur and Srebro - On Symmetric and Asymmetric LSHs for Inner Product.pdf}
}

@incollection{shrivastavaAsymmetricLSHALSH2014,
  title = {Asymmetric {{LSH}} ({{ALSH}}) for {{Sublinear Time Maximum Inner Product Search}} ({{MIPS}})},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Shrivastava, Anshumali and Li, Ping},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2321--2329},
  file = {/home/chay/Zotero/storage/4P73B26D/Shrivastava and Li - 2014 - Asymmetric LSH (ALSH) for Sublinear Time Maximum I.pdf;/home/chay/Zotero/storage/AZTJ3R8P/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.html}
}

@inproceedings{shrivastavaImprovedDensificationOne2914,
  address = {{Arlington, Virginia, United States}},
  series = {{{UAI}}'14},
  title = {Improved {{Densification}} of {{One Permutation Hashing}}},
  isbn = {978-0-9749039-1-0},
  abstract = {The existing work on densification of one permutation hashing [24] reduces the query processing cost of the (K, L)-parameterized Locality Sensitive Hashing (LSH) algorithm with minwise hashing, from O(dKL) to merely O(d + KL), where d is the number of nonzeros of the data vector, K is the number of hashes in each hash table, and L is the number of hash tables. While that is a substantial improvement, our analysis reveals that the existing densification scheme in [24] is sub-optimal. In particular, there is no enough randomness in that procedure, which affects its accuracy on very sparse datasets. In this paper, we provide a new densification procedure which is provably better than the existing scheme [24]. This improvement is more significant for very sparse datasets which are common over the web. The improved technique has the same cost of O(d + KL) for query processing, thereby making it strictly preferable over the existing procedure. Experimental evaluations on public datasets, in the task of hashing based near neighbor search, support our theoretical findings.},
  booktitle = {Proceedings of the {{Thirtieth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  publisher = {{AUAI Press}},
  author = {Shrivastava, Anshumali and Li, Ping},
  year = {2914},
  pages = {732--741}
}

@inproceedings{charikarSimilarityEstimationTechniques2002,
  address = {{New York, NY, USA}},
  series = {{{STOC}} '02},
  title = {Similarity {{Estimation Techniques}} from {{Rounding Algorithms}}},
  isbn = {978-1-58113-495-7},
  abstract = {(MATH) A locality sensitive hashing scheme is a distribution on a family \$\textbackslash{}F\$ of hash functions operating on a collection of objects, such that for two objects x,y, Prh\&egr;F[h(x) = h(y)] = sim(x,y), where sim(x,y) \&egr; [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure sim(A,B) = \textbackslash{}frac\{|A \&Pgr; B|\}\{|A \&Pgr B|\}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:{$<$}ol{$>$}A collection of vectors with the distance between \textrightarrow \textbackslash{}over u and \textrightarrow \textbackslash{}over v measured by {\O}(\textrightarrow \textbackslash{}over u, \textrightarrow \textbackslash{}over v)/\&pgr;, where {\O}(\textrightarrow \textbackslash{}over u, \textrightarrow \textbackslash{}over v) is the angle between \textrightarrow \textbackslash{}over u) and \textrightarrow \textbackslash{}over v). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.A collection of distributions on n points in a metric space, with distance between distributions measured by the Earth Mover Distance (EMD), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions P and Q, EMD(P,Q) \&xie; Eh\&egr;\textbackslash{}F [d(h(P),h(Q))] \&xie; O(log n log log n). EMD(P, Q).{$<$}/ol{$>$}.},
  booktitle = {Proceedings of the {{Thiry}}-Fourth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/509907.509965},
  author = {Charikar, Moses S.},
  year = {2002},
  pages = {380--388}
}

@incollection{yanNormRangingLSHMaximum2018,
  title = {Norm-{{Ranging LSH}} for {{Maximum Inner Product Search}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Yan, Xiao and Li, Jinfeng and Dai, Xinyan and Chen, Hongzhi and Cheng, James},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {2952--2961},
  file = {/home/chay/Zotero/storage/RZ5GZRXL/Yan et al. - 2018 - Norm-Ranging LSH for Maximum Inner Product Search.pdf;/home/chay/Zotero/storage/6SRF3KMI/7559-norm-ranging-lsh-for-maximum-inner-product-search.html}
}

@inproceedings{koenigsteinEfficientRetrievalRecommendations2012,
  address = {{New York, NY, USA}},
  series = {{{CIKM}} '12},
  title = {Efficient {{Retrieval}} of {{Recommendations}} in a {{Matrix Factorization Framework}}},
  isbn = {978-1-4503-1156-4},
  abstract = {Low-rank Matrix Factorization (MF) methods provide one of the simplest and most effective approaches to collaborative filtering. This paper is the first to investigate the problem of efficient retrieval of recommendations in a MF framework. We reduce the retrieval in a MF model to an apparently simple task of finding the maximum dot-product for the user vector over the set of item vectors. However, to the best of our knowledge the problem of efficiently finding the maximum dot-product in the general case has never been studied. To this end, we propose two techniques for efficient search -- (i) We index the item vectors in a binary spatial-partitioning metric tree and use a simple branch and-bound algorithm with a novel bounding scheme to efficiently obtain exact solutions. (ii) We use spherical clustering to index the users on the basis of their preferences and pre-compute recommendations only for the representative user of each cluster to obtain extremely efficient approximate solutions. We obtain a theoretical error bound which determines the quality of any approximate result and use it to control the approximation. Both these simple techniques are fairly independent of each other and hence are easily combined to further improve recommendation retrieval efficiency. We evaluate our algorithms on real-world collaborative-filtering datasets, demonstrating more than \texttimes{}7 speedup (with respect to the naive linear search) for the exact solution and over \texttimes{}250 speedup for approximate solutions by combining both techniques.},
  booktitle = {Proceedings of the 21st {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/2396761.2396831},
  author = {Koenigstein, Noam and Ram, Parikshit and Shavitt, Yuval},
  year = {2012},
  keywords = {collaborative filtering,fast retrieval,inner-product},
  pages = {535--544},
  file = {/home/chay/Zotero/storage/PZVFS32K/Koenigstein et al. - 2012 - Efficient Retrieval of Recommendations in a Matrix.pdf}
}

@article{tesauroTemporalDifferenceLearning1995,
  title = {Temporal {{Difference Learning}} and {{TD}}-{{Gammon}}},
  volume = {38},
  issn = {0001-0782},
  abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.},
  number = {3},
  journal = {Commun. ACM},
  doi = {10.1145/203330.203343},
  author = {Tesauro, Gerald},
  month = mar,
  year = {1995},
  pages = {58--68}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to {{Predict}} by the {{Methods}} of {{Temporal Differences}}},
  volume = {3},
  issn = {0885-6125},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction \textendash{} that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  number = {1},
  journal = {Mach. Learn.},
  doi = {10.1023/A:1022633531479},
  author = {Sutton, Richard S.},
  month = aug,
  year = {1988},
  keywords = {connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  pages = {9--44},
  file = {/home/chay/Zotero/storage/4UHTJUH5/Sutton - 1988 - Learning to Predict by the Methods of Temporal Dif.pdf}
}

@article{fukushimaNeocognitronSelforganizingNeural1980a,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  volume = {36},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Neocognitron},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  language = {en},
  number = {4},
  journal = {Biological Cybernetics},
  doi = {10.1007/BF00344251},
  author = {Fukushima, Kunihiko},
  month = apr,
  year = {1980},
  pages = {193-202},
  file = {/home/chay/Zotero/storage/AKDZK8BR/Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf}
}

@article{lindseyUnifiedTheoryEarly2019,
  title = {A {{Unified Theory}} of {{Early Visual Representations}} from {{Retina}} to {{Cortex}} through {{Anatomically Constrained Deep CNNs}}},
  abstract = {The vertebrate visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex (V1), typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and for the first time we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple downstream cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortical networks. This result predicts that the retinas of small vertebrates (e.g. salamander, frog) should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system.},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/511535},
  author = {Lindsey, Jack and Ocko, Samuel A. and Ganguli, Surya and Deny, Stephane},
  month = jan,
  year = {2019},
  file = {/home/chay/Zotero/storage/GF3HIGBY/Lindsey et al. - 2019 - A Unified Theory of Early Visual Representations f.pdf}
}

@article{hopfieldNeuralNetworksPhysical1982a,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  volume = {79},
  issn = {0027-8424, 1091-6490},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  language = {en},
  number = {8},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.79.8.2554},
  author = {Hopfield, J. J.},
  month = apr,
  year = {1982},
  pages = {2554-2558},
  file = {/home/chay/Zotero/storage/4G6B564C/Hopfield - 1982 - Neural networks and physical systems with emergent.pdf;/home/chay/Zotero/storage/NVCBWTEL/2554.html},
  pmid = {6953413}
}

@article{andoniNearoptimalHashingAlgorithms2008,
  title = {Near-Optimal {{Hashing Algorithms}} for {{Approximate Nearest Neighbor}} in {{High Dimensions}}},
  volume = {51},
  issn = {0001-0782},
  abstract = {In this article, we give an overview of efficient algorithms for the approximate and exact nearest neighbor problem. The goal is to preprocess a dataset of objects (e.g., images) so that later, given a new query object, one can quickly return the dataset object that is most similar to the query. The problem is of significant interest in a wide variety of areas.},
  number = {1},
  journal = {Commun. ACM},
  doi = {10.1145/1327452.1327494},
  author = {Andoni, Alexandr and Indyk, Piotr},
  month = jan,
  year = {2008},
  pages = {117--122},
  file = {/home/chay/Zotero/storage/X5LG6VWV/Andoni and Indyk - 2008 - Near-optimal Hashing Algorithms for Approximate Ne.pdf}
}

@article{grinbergLocalUnsupervisedLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  primaryClass = {cs, q-bio, stat},
  title = {Local {{Unsupervised Learning}} for {{Image Analysis}}},
  abstract = {Local Hebbian learning is believed to be inferior in performance to end-to-end training using a backpropagation algorithm. We question this popular belief by designing a local algorithm that can learn convolutional filters at scale on large image datasets. These filters combined with patch normalization and very steep non-linearities result in a good classification accuracy for shallow networks trained locally, as opposed to end-to-end. The filters learned by our algorithm contain both orientation selective units and unoriented color units, resembling the responses of pyramidal neurons located in the cytochrome oxidase 'interblob' and 'blob' regions in the primary visual cortex of primates. It is shown that convolutional networks with patch normalization significantly outperform standard convolutional networks on the task of recovering the original classes when shadows are superimposed on top of standard CIFAR-10 images. Patch normalization approximates the retinal adaptation to the mean light intensity, important for human vision. We also demonstrate a successful transfer of learned representations between CIFAR-10 and ImageNet 32x32 datasets. All these results taken together hint at the possibility that local unsupervised training might be a powerful tool for learning general representations (without specifying the task) directly from unlabeled data.},
  journal = {arXiv:1908.08993 [cs, q-bio, stat]},
  author = {Grinberg, Leopold and Hopfield, John and Krotov, Dmitry},
  month = aug,
  year = {2019},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/chay/Zotero/storage/QXQ4AKHP/Grinberg et al. - 2019 - Local Unsupervised Learning for Image Analysis.pdf;/home/chay/Zotero/storage/N6NF84TX/1908.html}
}

@article{gruntmanIntegrationOlfactoryCode2013,
  title = {Integration of the Olfactory Code across Dendritic Claws of Single Mushroom Body Neurons},
  volume = {16},
  copyright = {2013 Nature Publishing Group},
  issn = {1546-1726},
  abstract = {In the olfactory system, sensory inputs are arranged in different glomerular channels, which respond in combinatorial ensembles to the various chemical features of an odor. We investigated where and how this combinatorial code is read out deeper in the brain. We exploited the unique morphology of neurons in the Drosophila mushroom body, which receive input on large dendritic claws. Imaging odor responses of these dendritic claws revealed that input channels with distinct odor tuning converge on individual mushroom body neurons. We determined how these inputs interact to drive the cell to spike threshold using intracellular recordings to examine mushroom body responses to optogenetically controlled input. Our results provide an elegant explanation for the characteristic selectivity of mushroom body neurons: these cells receive different types of input and require those inputs to be coactive to spike. These results establish the mushroom body as an important site of integration in the fly olfactory system.},
  language = {en},
  number = {12},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.3547},
  author = {Gruntman, Eyal and Turner, Glenn C.},
  month = dec,
  year = {2013},
  pages = {1821-1829},
  file = {/home/chay/Zotero/storage/DCQEFSMG/Gruntman and Turner - 2013 - Integration of the olfactory code across dendritic.pdf;/home/chay/Zotero/storage/TDTUC2DM/nn.html}
}

@article{zhengCompleteElectronMicroscopy2018,
  title = {A {{Complete Electron Microscopy Volume}} of the {{Brain}} of {{Adult Drosophila}} Melanogaster},
  volume = {174},
  issn = {00928674},
  abstract = {Drosophila melanogaster has a rich repertoire of innate and learned behaviors. Its 100,000-neuron brain is a large but tractable target for comprehensive neural circuit mapping. Only electron microscopy (EM) enables complete, unbiased mapping of synaptic connectivity; however, the fly brain is too large for conventional EM. We developed a custom high-throughput EM platform and imaged the entire brain of an adult female fly at synaptic resolution. To validate the dataset, we traced brainspanning circuitry involving the mushroom body (MB), which has been extensively studied for its role in learning. All inputs to Kenyon cells (KCs), the intrinsic neurons of the MB, were mapped, revealing a previously unknown cell type, postsynaptic partners of KC dendrites, and unexpected clustering of olfactory projection neurons. These reconstructions show that this freely available EM volume supports mapping of brain-spanning circuits, which will significantly accelerate Drosophila neuroscience.},
  language = {en},
  number = {3},
  journal = {Cell},
  doi = {10.1016/j.cell.2018.06.019},
  author = {Zheng, Zhihao and Lauritzen, J. Scott and Perlman, Eric and Robinson, Camenzind G. and Nichols, Matthew and Milkie, Daniel and Torrens, Omar and Price, John and Fisher, Corey B. and Sharifi, Nadiya and {Calle-Schuler}, Steven A. and Kmecova, Lucia and Ali, Iqbal J. and Karsh, Bill and Trautman, Eric T. and Bogovic, John A. and Hanslovsky, Philipp and Jefferis, Gregory S.X.E. and Kazhdan, Michael and Khairy, Khaled and Saalfeld, Stephan and Fetter, Richard D. and Bock, Davi D.},
  month = jul,
  year = {2018},
  pages = {730-743.e22},
  file = {/home/chay/Zotero/storage/5RKM9PWC/Zheng et al. - 2018 - A Complete Electron Microscopy Volume of the Brain.pdf}
}

@article{ganguliCompressedSensingSparsity2012,
  title = {Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis.},
  volume = {35},
  abstract = {The curse of dimensionality poses severe challenges to both technical and conceptual progress in neuroscience. In particular, it plagues our ability to acquire, process, and model high-dimensional data sets. Moreover, neural systems must cope with the challenge of processing data in high dimensions to learn and operate successfully within a complex world. We review recent mathematical advances that provide ways to combat dimensionality in specific situations. These advances shed light on two dual questions in neuroscience. First, how can we as neuroscientists rapidly acquire high-dimensional data from the brain and subsequently extract meaningful models from limited amounts of these data? And second, how do brains themselves process information in their intrinsically high-dimensional patterns of neural activity as well as learn meaningful, generalizable models of the external world from limited experience?},
  journal = {Annual review of neuroscience},
  doi = {10.1146/annurev-neuro-062111-150410},
  author = {Ganguli, Surya and Sompolinsky, Haim},
  year = {2012},
  keywords = {Mathematics,Brain,Dual,Neuroscience discipline,Plague},
  pages = {485-508},
  file = {/home/chay/Zotero/storage/I527LK3L/Ganguli and Sompolinsky - 2012 - Compressed sensing, sparsity, and dimensionality i.pdf}
}

@inproceedings{doLearningHashBinary2016,
  title = {Learning to Hash with Binary Deep Neural Network},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  publisher = {{Springer}},
  author = {Do, Thanh-Toan and Doan, Anh-Dzung and Cheung, Ngai-Man},
  year = {2016},
  pages = {219--234},
  file = {/home/chay/Zotero/storage/3EQWMLLT/Do et al. - 2016 - Learning to hash with binary deep neural network.pdf;/home/chay/Zotero/storage/M8YGC3GY/978-3-319-46454-1_14.html}
}

@article{luDeepHashingScalable2017,
  title = {Deep Hashing for Scalable Image Search},
  volume = {26},
  number = {5},
  journal = {IEEE transactions on image processing},
  author = {Lu, Jiwen and Liong, Venice Erin and Zhou, Jie},
  year = {2017},
  pages = {2352--2367},
  file = {/home/chay/Zotero/storage/X7RNKDMI/Lu et al. - 2017 - Deep hashing for scalable image search.pdf;/home/chay/Zotero/storage/U3EK9Q5M/7870632.html}
}

@inproceedings{linLearningCompactBinary2016,
  title = {Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lin, Kevin and Lu, Jiwen and Chen, Chu-Song and Zhou, Jie},
  year = {2016},
  pages = {1183--1192},
  file = {/home/chay/Zotero/storage/XLB2D95A/Lin et al. - 2016 - Learning compact binary descriptors with unsupervi.pdf;/home/chay/Zotero/storage/KZ5GV6TB/Lin_Learning_Compact_Binary_CVPR_2016_paper.html}
}

@inproceedings{suGreedyHashFast2018,
  title = {Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in {{CNN}}},
  shorttitle = {Greedy Hash},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Su, Shupeng and Zhang, Chao and Han, Kai and Tian, Yonghong},
  year = {2018},
  pages = {798--807},
  file = {/home/chay/Zotero/storage/PU6BEYEK/Su et al. - 2018 - Greedy hash towards fast optimization for accurat.pdf;/home/chay/Zotero/storage/24H6TYW4/7360-greedy-hash-towards-fast-optimization-for-accurate-hash-coding-in-cnn.html}
}

@article{renNormalizingNormalizersComparing2016,
  title = {Normalizing the {{Normalizers}}: {{Comparing}} and {{Extending Network Normalization Schemes}}},
  shorttitle = {Normalizing the {{Normalizers}}},
  abstract = {Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to...},
  journal = {International Conference on Learning Representations},
  author = {Ren, Mengye and Liao, Renjie and Urtasun, Raquel and Sinz, Fabian H. and Zemel, Richard S.},
  year = {2016},
  file = {/home/chay/Zotero/storage/MFAJPNYM/Ren et al. - 2016 - Normalizing the Normalizers Comparing and Extendi.pdf;/home/chay/Zotero/storage/Z25HCHDW/forum.html}
}

@article{yunNOMADNonlockingStochastic2014,
  title = {{{NOMAD}}: Non-Locking, Stochastic Multi-Machine Algorithm for Asynchronous and Decentralized Matrix Completion},
  volume = {7},
  issn = {21508097},
  shorttitle = {{{NOMAD}}},
  abstract = {We develop an efficient parallel distributed algorithm for matrix completion, named NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion). NOMAD is a decentralized algorithm with non-blocking communication between processors. One of the key features of NOMAD is that the ownership of a variable is asynchronously transferred between processors in a decentralized fashion. As a consequence it is a lock-free parallel algorithm. In spite of being asynchronous, the variable updates of NOMAD are serializable, that is, there is an equivalent update ordering in a serial implementation. NOMAD outperforms synchronous algorithms which require explicit bulk synchronization after every iteration: our extensive empirical evaluation shows that not only does our algorithm perform well in distributed setting on commodity hardware, but also outperforms stateof-the-art algorithms on a HPC cluster both in multi-core and distributed memory settings.},
  language = {en},
  number = {11},
  journal = {Proceedings of the VLDB Endowment},
  doi = {10.14778/2732967.2732973},
  author = {Yun, Hyokun and Yu, Hsiang-Fu and Hsieh, Cho-Jui and Vishwanathan, S. V. N. and Dhillon, Inderjit},
  month = jul,
  year = {2014},
  pages = {975-986},
  file = {/home/chay/Zotero/storage/JA2MCIC2/Yun et al. - 2014 - NOMAD non-locking, stochastic multi-machine algor.pdf}
}

@article{mombaertsVisualizingOlfactorySensory1996,
  title = {Visualizing an {{Olfactory Sensory Map}}},
  volume = {87},
  issn = {0092-8674},
  abstract = {We have developed a genetic approach to visualize axons from olfactory sensory neurons expressing a given odorant receptor, as they project to the olfactory bulb. Neurons expressing a specific receptor project to only two topographically fixed loci among the 1800 glomeruli in the mouse olfactory bulb. Our data provide direct support for a model in which a topographic map of receptor activation encodes odor quality in the olfactory bulb. Receptor swap experiments suggest that the olfactory receptor plays an instructive role in the guidance process but cannot be the sole determinant in the establishment of this map. This genetic approach may be more broadly applied to visualize the development and plasticity of projections in the mammalian nervous system.},
  number = {4},
  journal = {Cell},
  doi = {10.1016/S0092-8674(00)81387-2},
  author = {Mombaerts, Peter and Wang, Fan and Dulac, Catherine and Chao, Steve K and Nemes, Adriana and Mendelsohn, Monica and Edmondson, James and Axel, Richard},
  month = nov,
  year = {1996},
  pages = {675-686},
  file = {/home/chay/Zotero/storage/TY869UNT/Mombaerts et al. - 1996 - Visualizing an Olfactory Sensory Map.pdf;/home/chay/Zotero/storage/BRD3ICA4/S0092867400813872.html}
}

@article{turnerOlfactoryRepresentationsDrosophila2008,
  title = {Olfactory Representations by {{Drosophila}} Mushroom Body Neurons},
  volume = {99},
  number = {2},
  journal = {Journal of neurophysiology},
  author = {Turner, Glenn C. and Bazhenov, Maxim and Laurent, Gilles},
  year = {2008},
  pages = {734--746},
  file = {/home/chay/Zotero/storage/JKQAIY9X/Turner et al. - 2008 - Olfactory representations by Drosophila mushroom b.pdf;/home/chay/Zotero/storage/A3I4G2EH/jn.01283.html}
}

@article{coverGeometricalStatisticalProperties1965,
  title = {Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition},
  number = {3},
  journal = {IEEE transactions on electronic computers},
  author = {Cover, Thomas M.},
  year = {1965},
  pages = {326--334},
  file = {/home/chay/Zotero/storage/TSAWHKQT/4038449.html}
}

@article{tsodyksEnhancedStorageCapacity1988,
  title = {The Enhanced Storage Capacity in Neural Networks with Low Activity Level},
  volume = {6},
  number = {2},
  journal = {EPL (Europhysics Letters)},
  author = {Tsodyks, Mikhail V. and Feigelman, Mikhail V.},
  year = {1988},
  pages = {101},
  file = {/home/chay/Zotero/storage/278HS4Q4/Tsodyks and Feigel'man - 1988 - The enhanced storage capacity in neural networks w.pdf;/home/chay/Zotero/storage/8JRPSYSY/meta.html}
}

@incollection{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  file = {/home/chay/Zotero/storage/MS6BC726/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/home/chay/Zotero/storage/RZ8W8QLZ/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@inproceedings{doSimultaneousFeatureAggregating2017,
  title = {Simultaneous {{Feature Aggregating}} and {{Hashing}} for {{Large}}-{{Scale Image Search}}},
  abstract = {In most state-of-the-art hashing-based visual search systems, local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work, the aggregating and the hashing processes are designed independently. In this paper, we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically, our joint optimization produces aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition, we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  doi = {10.1109/CVPR.2017.449},
  author = {Do, T. and Tan, D. L. and Pham, T. T. and Cheung, N.},
  month = jul,
  year = {2017},
  keywords = {Binary Autoencoder,binary codes,Binary codes,convolutional features,discriminative binary hash codes,feature extraction,Feature extraction,file organisation,hashing function,image coding,Image reconstruction,Image representation,image retrieval,joint optimization,large-scale image search,local image descriptors,optimisation,Optimization,Quantization (signal),SIFT,simultaneous feature aggregating,single feature vector,Training,vectors,visual databases,visual search systems},
  pages = {4217-4226},
  file = {/home/chay/Zotero/storage/3XQI2LRB/Do et al. - 2017 - Simultaneous Feature Aggregating and Hashing for L.pdf;/home/chay/Zotero/storage/UNGXYMX4/8099932.html}
}

@article{simonyanVeryDeepConvolutional2014a,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal = {arXiv preprint arXiv:1409.1556},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  file = {/home/chay/Zotero/storage/5HJCD86S/Simonyan and Zisserman - 2014 - Very deep convolutional networks for large-scale i.pdf;/home/chay/Zotero/storage/SKNHUILT/1409.html}
}

@inproceedings{dengImagenetLargescaleHierarchical2009a,
  title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {Imagenet},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond. 1.},
  booktitle = {In {{CVPR}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-jia and Li, Kai and {Fei-fei}, Li},
  year = {2009},
  file = {/home/chay/Zotero/storage/6M568PJP/Deng et al. - 2009 - Imagenet A large-scale hierarchical image databas.pdf;/home/chay/Zotero/storage/K8CFI5LU/summary.html}
}

@inproceedings{yangSemanticStructurebasedUnsupervised2018b,
  address = {{Stockholm, Sweden}},
  title = {Semantic {{Structure}}-Based {{Unsupervised Deep Hashing}}},
  isbn = {978-0-9992411-2-7},
  abstract = {Hashing is becoming increasingly popular for approximate nearest neighbor searching in massive databases due to its storage and search efficiency. Recent supervised hashing methods, which usually construct semantic similarity matrices to guide hash code learning using label information, have shown promising results. However, it is relatively difficult to capture and utilize the semantic relationships between points in unsupervised settings. To address this problem, we propose a novel unsupervised deep framework called Semantic Structurebased unsupervised Deep Hashing (SSDH). We first empirically study the deep feature statistics, and find that the distribution of the cosine distance for point pairs can be estimated by two half Gaussian distributions. Based on this observation, we construct the semantic structure by considering points with distances obviously smaller than the others as semantically similar and points with distances obviously larger than the others as semantically dissimilar. We then design a deep architecture and a pair-wise loss function to preserve this semantic structure in Hamming space. Extensive experiments show that SSDH significantly outperforms current state-of-the-art methods.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2018/148},
  author = {Yang, Erkun and Deng, Cheng and Liu, Tongliang and Liu, Wei and Tao, Dacheng},
  month = jul,
  year = {2018},
  pages = {1064-1070},
  file = {/home/chay/Zotero/storage/262HIXVI/Yang et al. - 2018 - Semantic Structure-based Unsupervised Deep Hashing.pdf}
}

@article{ioffeBatchNormalizationAccelerating2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  primaryClass = {cs},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  journal = {arXiv:1502.03167 [cs]},
  author = {Ioffe, Sergey and Szegedy, Christian},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Machine Learning},
  file = {/home/chay/Zotero/storage/LISL39RK/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/home/chay/Zotero/storage/KL2WQ9UA/1502.html}
}


@article{garg2018modeling,
  title={Modeling Psychotherapy Dialogues with Kernelized Hashcode Representations: A Nonparametric Information-Theoretic Approach},
  author={Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Goyal, Palash and Ghazarian, Sarik and Gao, Shuyang and Steeg, Greg Ver and Galstyan, Aram},
  journal={arXiv preprint arXiv:1804.10188},
  year={2018}
}

@article{dhillon2001concept,
  title={Concept decompositions for large sparse text data using clustering},
  author={Dhillon, Inderjit S and Modha, Dharmendra S},
  journal={Machine learning},
  volume={42},
  number={1-2},
  pages={143--175},
  year={2001},
  publisher={Springer}
}