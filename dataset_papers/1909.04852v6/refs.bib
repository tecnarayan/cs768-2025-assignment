% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Kumar2019-uh,
  title    = "{ArviZ} a unified library for exploratory analysis of Bayesian
              models in Python",
  author   = "Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin,
              Osvaldo",
  abstract = "Software archive",
  journal  = "The Journal of Open Source Software",
  volume   =  4,
  number   =  33,
  pages    = "1143",
  month    =  jan,
  year     =  2019,
  url      = "http://joss.theoj.org/papers/10.21105/joss.01143",
  file     = "All Papers/K/Kumar et al. 2019 - ArviZ a unified library for exploratory analysis of Bayesian models in Python.pdf",
  keywords = "momentum\_sampler"
}

@INCOLLECTION{Zhang2012-qm,
  title     = "Continuous Relaxations for Discrete Hamiltonian Monte Carlo",
  booktitle = "Advances in Neural Information Processing Systems 25",
  author    = "Zhang, Yichuan and Ghahramani, Zoubin and Storkey, Amos J and
               Sutton, Charles A",
  editor    = "Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  pages     = "3194--3202",
  year      =  2012,
  url       = "http://papers.nips.cc/paper/4652-continuous-relaxations-for-discrete-hamiltonian-monte-carlo.pdf",
  file      = "All Papers/Z/Zhang et al. 2012 - Continuous Relaxations for Discrete Hamiltonian Monte Carlo.pdf",
  keywords  = "momentum\_sampler"
}

@ARTICLE{Betancourt2017-vx,
  title         = "A Conceptual Introduction to Hamiltonian Monte Carlo",
  author        = "Betancourt, Michael",
  abstract      = "Hamiltonian Monte Carlo has proven a remarkable empirical
                   success, but only recently have we begun to develop a
                   rigorous understanding of why it performs so well on
                   difficult problems and how it is best applied in practice.
                   Unfortunately, that understanding is confined within the
                   mathematics of differential geometry which has limited its
                   dissemination, especially to the applied communities for
                   which it is particularly important. In this review I provide
                   a comprehensive conceptual account of these theoretical
                   foundations, focusing on developing a principled intuition
                   behind the method and its optimal implementations rather of
                   any exhaustive rigor. Whether a practitioner or a
                   statistician, the dedicated reader will acquire a solid
                   grasp of how Hamiltonian Monte Carlo works, when it
                   succeeds, and, perhaps most importantly, when it fails.",
  journal       =  "arXiv:1701.02434",
  month         =  jul,
  year          =  2018,
  url           = "http://arxiv.org/abs/1701.02434",
  file          = "All Papers/B/Betancourt 2017 - A Conceptual Introduction to Hamiltonian Monte Carlo.pdf",
  keywords      = "momentum\_sampler",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1701.02434"
}

@ARTICLE{Livingstone2017-zd,
  title         = "Kinetic energy choice in Hamiltonian/hybrid Monte Carlo",
  author        = "Livingstone, Samuel and Faulkner, Michael F and Roberts,
                   Gareth O",
  abstract      = "We consider how different choices of kinetic energy in
                   Hamiltonian Monte Carlo affect algorithm performance. To
                   this end, we introduce two quantities which can be easily
                   evaluated, the composite gradient and the implicit noise.
                   Results are established on integrator stability and
                   geometric convergence, and we show that choices of kinetic
                   energy that result in heavy-tailed momentum distributions
                   can exhibit an undesirable negligible moves property, which
                   we define. A general efficiency-robustness trade off is
                   outlined, and implementations which rely on approximate
                   gradients are also discussed. Two numerical studies
                   illustrate our theoretical findings, showing that the
                   standard choice which results in a Gaussian momentum
                   distribution is not always optimal in terms of either
                   robustness or efficiency.",
  journal       =  "arXiv:1706.02649",
  month         =  nov,
  year          =  2018,
  url           = "http://arxiv.org/abs/1706.02649",
  file          = "All Papers/L/Livingstone et al. 2017 - Kinetic energy choice in Hamiltonian - hybrid Monte Carlo.pdf",
  keywords      = "momentum\_sampler",
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1706.02649"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ito1994-tp,
  title     = "{Cluster} {VS} Single-spin Algorithms-- Which are more
  Efficient?",
  author    = "Ito, N and Kohring, G A",
  abstract  = "A comparison between single-cluster and single-spin algorithms
               is made for the Ising model in 2 and 3 dimensions. We compare
               the amount of computer time needed to achieve a given level of
               statistical accuracy, rather than the speed in terms of site
               updates per second or the dynamical critical exponents. Our main
               result is that the cluster algorithms become more efficient when
               the system size, Ld, exceeds, L~70?300 for d=2 and l~80?200 for
               d=3. The exact value of the crossover is dependent upon the
               computer being used. The lower end of the crossover range is
               typical of workstations while the higher end is typical of
               vector computers. Hence, even for workstations, the system sizes
               needed for efficient use of the cluster algorithm is relatively
               large.",
  journal   = "International Journal of Modern Physics C",
  publisher = "World Scientific Publishing Co.",
  volume    =  05,
  number    =  01,
  pages     = "1--14",
  month     =  feb,
  year      =  1994,
  url       = "https://doi.org/10.1142/S0129183194000027",
  file      = "All Papers/I/Ito and Kohring 1994 - CLUSTER VS. SINGLE-SPIN ALGORITHMS—WHICH ARE MORE EFFICIENT.pdf",
  keywords  = "momentum\_sampler"
}

@ARTICLE{Zanella2017-ro,
  title     = "Informed Proposals for Local {MCMC} in Discrete Spaces",
  author    = "Zanella, Giacomo",
  abstract  = "There is a lack of methodological results to design
               efficient Markov chain Monte Carlo ( MCMC) algorithms for
               statistical models with discrete-valued high-dimensional
               parameters. Motivated by this consideration, we propose a simple
               framework for the design of informed MCMC proposals (i.e.,
               Metropolis?Hastings proposal distributions that appropriately
               incorporate local information about the target) which is
               naturally applicable to discrete spaces. Using Peskun-type
               comparisons of Markov kernels, we explicitly characterize the
               class of asymptotically optimal proposal distributions under
               this framework, which we refer to as locally balanced proposals.
               The resulting algorithms are straightforward to implement in
               discrete spaces and provide orders of magnitude improvements in
               efficiency compared to alternative MCMC schemes, including
               discrete versions of Hamiltonian Monte Carlo. Simulations are
               performed with both simulated and real datasets, including a
               detailed application to Bayesian record linkage. A direct
               connection with gradient-based MCMC suggests that locally
               balanced proposals can be seen as a natural way to extend the
               latter to discrete spaces. Supplementary materials for this
               article are available online.",
  journal   = "Journal of the American Statistical Association",
  publisher = "Taylor \& Francis",
  pages     = "1--27",
  month     =  mar,
  year      =  2019,
  url       = "https://doi.org/10.1080/01621459.2019.1585255",
  file      = "All Papers/Z/Zanella 2019 - Informed Proposals for Local MCMC in Discrete Spaces.pdf"
}

@INPROCEEDINGS{Dinh2017-td,
  title     = "Probabilistic Path Hamiltonian Monte Carlo",
  booktitle = "Proceedings of the 34th International Conference on Machine
               Learning - Volume 70",
  author    = "Dinh, Vu and Bilge, Arman and Zhang, Cheng and Matsen, IV,
               Frederick A",
  publisher = "JMLR.org",
  pages     = "1009--1018",
  series    = "ICML'17",
  year      =  2017,
  url       = "http://dl.acm.org/citation.cfm?id=3305381.3305486",
  file      = "All Papers/D/Dinh et al. 2017 - Probabilistic Path Hamiltonian Monte Carlo.pdf",
  address   = "Sydney, NSW, Australia"
}

@ARTICLE{Nishimura2017-wv,
  title         = "Discontinuous Hamiltonian Monte Carlo for discrete
                   parameters and discontinuous likelihoods",
  author        = "Nishimura, Akihiko and Dunson, David and Lu, Jianfeng",
  abstract      = "Hamiltonian Monte Carlo has emerged as a standard tool for
                   posterior computation. In this article, we present an
                   extension that can efficiently explore target distributions
                   with discontinuous densities, which in turn enables
                   efficient sampling from ordinal parameters though embedding
                   of probability mass functions into continuous spaces. We
                   motivate our approach through a theory of discontinuous
                   Hamiltonian dynamics and develop a numerical solver of
                   discontinuous dynamics. The proposed numerical solver is the
                   first of its kind, with a remarkable ability to exactly
                   preserve the Hamiltonian and thus yield a type of
                   rejection-free proposals. We apply our algorithm to
                   challenging posterior inference problems to demonstrate its
                   wide applicability and competitive performance.",
  month         =  aug,
  year          =  2018,
  journal       =  "arXiv:1705.08510",
  url           = "http://arxiv.org/abs/1705.08510",
  file          = "All Papers/N/Nishimura et al. 2017 - Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods.pdf",
  keywords      = "momentum\_sampler",
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1705.08510"
}

@ARTICLE{Titsias2017-nt,
  title    = "The Hamming Ball Sampler",
  author   = "Titsias, Michalis K and Yau, Christopher",
  abstract = "We introduce the Hamming ball sampler, a novel Markov chain Monte
              Carlo algorithm, for efficient inference in statistical models
              involving high-dimensional discrete state spaces. The sampling
              scheme uses an auxiliary variable construction that adaptively
              truncates the model space allowing iterative exploration of the
              full model space. The approach generalizes conventional Gibbs
              sampling schemes for discrete spaces and provides an intuitive
              means for user-controlled balance between statistical efficiency
              and computational tractability. We illustrate the generic utility
              of our sampling algorithm through application to a range of
              statistical models. Supplementary materials for this article are
              available online.",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  112,
  number   =  520,
  pages    = "1598--1611",
  month    =  sep,
  year     =  2017,
  url      = "http://dx.doi.org/10.1080/01621459.2016.1222288",
  file     = "All Papers/T/Titsias and Yau 2017 - The Hamming Ball Sampler.pdf",
  keywords = "Bayesian; Discrete state spaces; Markov chain Monte
              Carlo;momentum\_sampler",
  language = "en"
}

@INCOLLECTION{Mohasel_Afshar2015-uh,
  title     = "Reflection, Refraction, and Hamiltonian Monte Carlo",
  booktitle = "Advances in Neural Information Processing Systems 28",
  author    = "Mohasel Afshar, Hadi and Domke, Justin",
  editor    = "Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "3007--3015",
  year      =  2015,
  file      = "All Papers/M/Mohasel Afshar and Domke 2015 - Reflection, Refraction, and Hamiltonian Monte Carlo.pdf",
  keywords  = "momentum\_sampler"
}

@INCOLLECTION{Pakman2013-gi,
  title     = "Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for
               Binary Distributions",
  booktitle = "Advances in Neural Information Processing Systems 26",
  author    = "Pakman, Ari and Paninski, Liam",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  pages     = "2490--2498",
  year      =  2013,
  file      = "All Papers/P/Pakman and Paninski 2013 - Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions.pdf",
  keywords  = "momentum\_sampler"
}

@ARTICLE{Neal2012-cp,
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis
algorithm, thereby avoiding the slow exploration of the state space that results from
the diffusive behaviour of simple random-walk proposals. Though originating in
physics, Hamiltonian dynamics can be applied to most problems with continuous
state spaces by simply introducing fictitious “momentum” variables. A key to
its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories
can thus be used to define complex mappings without the need to account for a
hard-to-compute Jacobian factor — a property that can be exactly maintained
even when the dynamics is approximated by discretizing time. In this review, I
discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present
some of its variations, including using windows of states for deciding on acceptance
or rejection, computing trajectories using fast approximations, tempering during
the course of a trajectory to handle isolated modes, and short-cut methods that
prevent useless trajectories from taking much computation time.
},
  added-at = {2013-11-20T16:22:16.000+0100},
  author = {Neal, Radford M.},
  biburl = {https://www.bibsonomy.org/bibtex/233fcb1d5a6d675e371170820f0b4dbcd/giacomo.fiumara},
  citeulike-article-id = {12118398},
  interhash = {f26b3ec7720399d06a5c271916a0ee92},
  intrahash = {33fcb1d5a6d675e371170820f0b4dbcd},
  journal = {Handbook of Markov Chain Monte Carlo},
  keywords = {Markov MonteCarlo},
  pages = {113--162},
  posted-at = {2013-03-06 23:35:00},
  priority = {2},
  timestamp = {2013-11-20T16:22:16.000+0100},
  title = {{MCMC} Using {Hamiltonian} Dynamics},
  volume = 54,
  year = 2010
}

@ARTICLE{Carpenter2017-zi,
  title    = "Stan: A Probabilistic Programming Language",
  author   = "Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew and Lee,
              Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker,
              Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen",
  abstract = "Stan is a probabilistic programming language for specifying
              statistical models. A Stan program imperatively defines a log
              probability function over parameters conditioned on specified
              data and constants. As of version 2.14.0, Stan provides full
              Bayesian inference for continuous-variable models through Markov
              chain Monte Carlo methods such as the No-U-Turn sampler, an
              adaptive form of Hamiltonian Monte Carlo sampling. Penalized
              maximum likelihood estimates are calculated using optimization
              methods such as the limited memory
              Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a
              platform for computing log densities and their gradients and
              Hessians, which can be used in alternative algorithms such as
              variational Bayes, expectation propagation, and marginal
              inference using approximate integration. To this end, Stan is set
              up so that the densities, gradients, and Hessians, along with
              intermediate quantities of the algorithm such as acceptance
              probabilities, are easily accessible. Stan can be called from the
              command line using the cmdstan package, through R using the rstan
              package, and through Python using the pystan package. All three
              interfaces support sampling and optimization-based inference with
              diagnostics and posterior analysis. rstan and pystan also provide
              access to log probabilities, gradients, Hessians, parameter
              transforms, and specialized plotting.",
  journal  = "Journal of Statistical Software, Articles",
  volume   =  76,
  number   =  1,
  pages    = "1--32",
  year     =  2017,
  url      = "https://www.jstatsoft.org/v076/i01",
  file     = "All Papers/C/Carpenter et al. 2017 - Stan - A Probabilistic Programming Language.pdf",
  keywords = "probabilistic programming; Bayesian inference; algorithmic
              differentiation; Stan"
}

@article{bingham2018pyro,
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and
            Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and
            Horsfall, Paul and Goodman, Noah D.},
  title = {{Pyro: Deep Universal Probabilistic Programming}},
  journal = {arXiv preprint arXiv:1810.09538},
  year = {2018}
}

@article{Salvatier2016,
  doi = {10.7717/peerj-cs.55},
  url = {https://doi.org/10.7717/peerj-cs.55},
  year  = {2016},
  month = {apr},
  publisher = {{PeerJ}},
  volume = {2},
  pages = {e55},
  author = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
  title = {Probabilistic programming in Python using {PyMC}3},
  journal = {{PeerJ} Computer Science}
}

@ARTICLE{Phan2019-vh,
  title         = "Composable Effects for Flexible and Accelerated
                   Probabilistic Programming in {NumPyro}",
  author        = "Phan, Du and Pradhan, Neeraj and Jankowiak, Martin",
  abstract      = "NumPyro is a lightweight library that provides an alternate
                   NumPy backend to the Pyro probabilistic programming language
                   with the same modeling interface, language primitives and
                   effect handling abstractions. Effect handlers allow Pyro's
                   modeling API to be extended to NumPyro despite its being
                   built atop a fundamentally different JAX-based functional
                   backend. In this work, we demonstrate the power of composing
                   Pyro's effect handlers with the program transformations that
                   enable hardware acceleration, automatic differentiation, and
                   vectorization in JAX. In particular, NumPyro provides an
                   iterative formulation of the No-U-Turn Sampler (NUTS) that
                   can be end-to-end JIT compiled, yielding an implementation
                   that is much faster than existing alternatives in both the
                   small and large dataset regimes.",
  month         =  dec,
  year          =  2019,
  url           = "http://arxiv.org/abs/1912.11554",
  file          = "All Papers/P/Phan et al. 2019 - Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1912.11554"
}

@inproceedings{ge2018t,
  author    = {Hong Ge and
               Kai Xu and
               Zoubin Ghahramani},
  title     = {Turing: a language for flexible probabilistic inference},
  booktitle = {International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands,
               Spain},
  pages     = {1682--1690},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v84/ge18b.html},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/GeXG18},
}


@inproceedings{Cusumano-Towner:2019:GGP:3314221.3314642,
 author = {Cusumano-Towner, Marco F. and Saad, Feras A. and Lew, Alexander K. and Mansinghka, Vikash K.},
 title = {Gen: A General-purpose Probabilistic Programming System with Programmable Inference},
 booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2019},
 year = {2019},
 isbn = {978-1-4503-6712-7},
 location = {Phoenix, AZ, USA},
 pages = {221--236},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3314221.3314642},
 doi = {10.1145/3314221.3314642},
 acmid = {3314642},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain Monte Carlo, Probabilistic programming, sequential Monte Carlo, variational inference},
} 


@ARTICLE{Dillon2017-vz,
  title         = "{TensorFlow} Distributions",
  author        = "Dillon, Joshua V and Langmore, Ian and Tran, Dustin and
                   Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and
                   Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous,
                   Rif A",
  abstract      = "The TensorFlow Distributions library implements a vision of
                   probability theory adapted to the modern deep-learning
                   paradigm of end-to-end differentiable computation. Building
                   on two basic abstractions, it offers flexible building
                   blocks for probabilistic computation. Distributions provide
                   fast, numerically stable methods for generating samples and
                   computing statistics, e.g., log density. Bijectors provide
                   composable volume-tracking transformations with automatic
                   caching. Together these enable modular construction of high
                   dimensional distributions and transformations not possible
                   with previous libraries (e.g., pixelCNNs, autoregressive
                   flows, and reversible residual networks). They are the
                   workhorse behind deep probabilistic programming systems like
                   Edward and empower fast black-box inference in probabilistic
                   models built on deep-network components. TensorFlow
                   Distributions has proven an important part of the TensorFlow
                   toolkit within Google and in the broader deep learning
                   community.",
  month         =  nov,
  year          =  2017,
  url           = "http://arxiv.org/abs/1711.10604",
  file          = "All Papers/D/Dillon et al. 2017 - TensorFlow Distributions.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1711.10604"
}

@ARTICLE{Zhou2019-uk,
  title         = "{LF-PPL}: A {Low-Level} First Order Probabilistic
                   Programming Language for {Non-Differentiable} Models",
  author        = "Zhou, Yuan and Gram-Hansen, Bradley J and Kohn, Tobias and
                   Rainforth, Tom and Yang, Hongseok and Wood, Frank",
  abstract      = "We develop a new Low-level, First-order Probabilistic
                   Programming Language (LF-PPL) suited for models containing a
                   mix of continuous, discrete, and/or piecewise-continuous
                   variables. The key success of this language and its
                   compilation scheme is in its ability to automatically
                   distinguish parameters the density function is discontinuous
                   with respect to, while further providing runtime checks for
                   boundary crossings. This enables the introduction of new
                   inference engines that are able to exploit gradient
                   information, while remaining efficient for models which are
                   not everywhere differentiable. We demonstrate this ability
                   by incorporating a discontinuous Hamiltonian Monte Carlo
                   (DHMC) inference engine that is able to deliver automated
                   and efficient inference for non-differentiable models. Our
                   system is backed up by a mathematical formalism that ensures
                   that any model expressed in this language has a density with
                   measure zero discontinuities to maintain the validity of the
                   inference engine.",
  month         =  mar,
  year          =  2019,
  url           = "http://arxiv.org/abs/1903.02482",
  file          = "All Papers/Z/Zhou et al. 2019 - LF-PPL - A Low-Level First Order Probabilistic Programming Language for Non-Differentiable Models.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1903.02482"
}

@ARTICLE{Hoffman2014-so,
  title     = "The {No-U-Turn} sampler: adaptively setting path lengths in
               Hamiltonian Monte Carlo",
  author    = "Hoffman, M D and Gelman, A",
  abstract  = "Abstract Hamiltonian Monte Carlo (HMC) is a Markov chain Monte
               Carlo (MCMC) algorithm that avoids the random walk behavior and
               sensitivity to correlated parameters that plague many MCMC
               methods by taking a series of steps informed by first-order
               gradient …",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  year      =  2014,
  url       = "http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf",
  file      = "All Papers/H/Hoffman and Gelman 2014 - The No-U-Turn sampler - adaptively setting path lengths in Hamiltonian Monte Carlo.pdf"
}

@article{arviz_2019,
	title = {{ArviZ} a unified library for exploratory analysis of {Bayesian} models in {Python}},
	author = {Kumar, Ravin and Colin, Carroll and Hartikainen, Ari and Martin, Osvaldo A.},
	journal = {The Journal of Open Source Software},
	year = {2019},
	doi = {10.21105/joss.01143},
	url = {http://joss.theoj.org/papers/10.21105/joss.01143},
}

@ARTICLE{Blei2007-tk,
  title         = "A correlated topic model of Science",
  author        = "Blei, David M and Lafferty, John D",
  abstract      = "Topic models, such as latent Dirichlet allocation (LDA), can
                   be useful tools for the statistical analysis of document
                   collections and other discrete data. The LDA model assumes
                   that the words of each document arise from a mixture of
                   topics, each of which is a distribution over the vocabulary.
                   A limitation of LDA is the inability to model topic
                   correlation even though, for example, a document about
                   genetics is more likely to also be about disease than X-ray
                   astronomy. This limitation stems from the use of the
                   Dirichlet distribution to model the variability among the
                   topic proportions. In this paper we develop the correlated
                   topic model (CTM), where the topic proportions exhibit
                   correlation via the logistic normal distribution [J. Roy.
                   Statist. Soc. Ser. B 44 (1982) 139--177]. We derive a fast
                   variational inference algorithm for approximate posterior
                   inference in this model, which is complicated by the fact
                   that the logistic normal is not conjugate to the
                   multinomial. We apply the CTM to the articles from Science
                   published from 1990--1999, a data set that comprises 57M
                   words. The CTM gives a better fit of the data than LDA, and
                   we demonstrate its use as an exploratory tool of large
                   document collections.",
  month         =  aug,
  year          =  2007,
  url           = "http://arxiv.org/abs/0708.3601",
  file          = "All Papers/B/Blei and Lafferty 2007 - A correlated topic model of Science.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "stat.AP",
  eprint        = "0708.3601"
}

@ARTICLE{Blei2003-pz,
  title    = "Latent Dirichlet Allocation",
  author   = "Blei, David M and Ng, Andrew Y and Jordan, Michael I",
  journal  = "J. Mach. Learn. Res.",
  volume   =  3,
  number   = "Jan",
  pages    = "993--1022",
  year     =  2003,
  url      = "http://www.jmlr.org/papers/v3/blei03a",
  file     = "All Papers/B/Blei et al. 2003 - Latent Dirichlet Allocation.pdf"
}

@INCOLLECTION{Chen2013-ge,
  title     = "Scalable Inference for {Logistic-Normal} Topic Models",
  booktitle = "Advances in Neural Information Processing Systems 26",
  author    = "Chen, Jianfei and Zhu, Jun and Wang, Zi and Zheng, Xun and
               Zhang, Bo",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  abstract  = "Logistic - normal topic models can effectively discover
               correlation structures among latent topics. However, their
               inference remains a challenge because of the non-conjugacy
               between the logistic - normal prior and multinomial topic mixing
               proportions. Existing …",
  publisher = "Curran Associates, Inc.",
  pages     = "2445--2453",
  year      =  2013,
  file      = "All Papers/C/Chen et al. 2013 - Scalable Inference for Logistic-Normal Topic Models.pdf"
}

@INPROCEEDINGS{Mimno2008-mi,
  title     = "Gibbs sampling for logistic normal topic models with graph-based
               priors",
  booktitle = "{NIPS} Workshop on Analyzing Graphs",
  author    = "Mimno, David and Wallach, Hanna and McCallum, Andrew",
  abstract  = "Previous work on probabilistic topic models has either focused
               on models with relatively simple conjugate priors that support
               Gibbs sampling or models with non-conjugate priors that
               typically require variational inference. Gibbs sampling is more
               accurate than variational …",
  publisher = "people.cs.umass.edu",
  volume    =  61,
  year      =  2008,
  url       = "https://people.cs.umass.edu/~wallach/publications/mimno08gibbs.pdf",
  file      = "All Papers/M/Mimno et al. 2008 - Gibbs sampling for logistic normal topic models with graph-based priors.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Harman1993-df,
  title     = "Overview of the first {TREC} conference",
  booktitle = "Proceedings of the 16th annual international {ACM} {SIGIR}
               conference on Research and development in information retrieval",
  author    = "Harman, Donna",
  abstract  = "OVERVIEW OF THE FIRST TREC CONFERENCE … First there was a desire
               to SI1OWa wide range of query construction methods by keeping
               the topic (the need statement) d~tinct from the query (the
               actual text submitted to the sys … top> ~eacb Tipster Topic
               Description -m- Number …",
  publisher = "dl.acm.org",
  pages     = "36--47",
  year      =  1993,
  url       = "https://dl.acm.org/doi/abs/10.1145/160688.160692"
}

@ARTICLE{Polson2013-zt,
  title     = "Bayesian Inference for Logistic Models Using {P{\'o}lya--Gamma}
               Latent Variables",
  author    = "Polson, Nicholas G and Scott, James G and Windle, Jesse",
  abstract  = "We propose a new data-augmentation strategy for fully Bayesian
               inference in models with binomial likelihoods. The approach
               appeals to a new class of P{\'o}lya?Gamma distributions, which
               are constructed in detail. A variety of examples are presented
               to show the versatility of the method, including logistic
               regression, negative binomial regression, nonlinear mixed-effect
               models, and spatial models for count data. In each case, our
               data-augmentation strategy leads to simple, effective methods
               for posterior inference that (1) circumvent the need for
               analytic approximations, numerical integration, or
               Metropolis?Hastings; and (2) outperform other known
               data-augmentation strategies, both in ease of use and in
               computational efficiency. All methods, including an efficient
               sampler for the P{\'o}lya?Gamma distribution, are implemented in
               the R package BayesLogit. Supplementary materials for this
               article are available online.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  108,
  number    =  504,
  pages     = "1339--1349",
  month     =  dec,
  year      =  2013,
  url       = "https://doi.org/10.1080/01621459.2013.829001",
  file      = "All Papers/P/Polson et al. 2013 - Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables.pdf"
}


@ARTICLE{Dellaportas2000-pa,
  title     = "Bayesian variable selection using the Gibbs sampler",
  author    = "Dellaportas, Petros and Forster, Jonathan J and Ntzoufras,
               Ioannis",
  abstract  = "Specification of the linear predictor for a generalized linear
               model requires determining which variables to include. We
               consider Bayesian strategies for performing this variable
               selection. In particular we focus on approaches based on the
               Gibbs sampler. Such approaches may be implemented using the
               publically available software BUGS. We illustrate the methods
               using a simple example. BUGS code is provided in an appendix.",
  journal   = "BIOSTATISTICS-BASEL-",
  publisher = "MARCEL DEKKER INC",
  volume    =  5,
  pages     = "273--286",
  year      =  2000,
  url       = "https://books.google.com/books?hl=en&lr=&id=OUxZDwAAQBAJ&oi=fnd&pg=PA273&dq=Bayesian+Variable+Selection+Using+the+Gibbs+Sampler&ots=D4mJ5rJGNj&sig=mE1wGOT6gd5iUaz2TugO_IhWLJU",
  file      = "All Papers/D/Dellaportas et al. 2000 - Bayesian variable selection using the Gibbs sampler.pdf"
}

@ARTICLE{Zucknick2014-pf,
  title         = "{MCMC} algorithms for Bayesian variable selection in the
                   logistic regression model for large-scale genomic
                   applications",
  author        = "Zucknick, Manuela and Richardson, Sylvia",
  abstract      = "In large-scale genomic applications vast numbers of
                   molecular features are scanned in order to find a small
                   number of candidates which are linked to a particular
                   disease or phenotype. This is a variable selection problem
                   in the ``large p, small n'' paradigm where many more
                   variables than samples are available. Additionally, a
                   complex dependence structure is often observed among the
                   markers/genes due to their joint involvement in biological
                   processes and pathways. Bayesian variable selection methods
                   that introduce sparseness through additional priors on the
                   model size are well suited to the problem. However, the
                   model space is very large and standard Markov chain Monte
                   Carlo (MCMC) algorithms such as a Gibbs sampler sweeping
                   over all p variables in each iteration are often
                   computationally infeasible. We propose to employ the
                   dependence structure in the data to decide which variables
                   should always be updated together and which are nearly
                   conditionally independent and hence do not need to be
                   considered together. Here, we focus on binary classification
                   applications. We follow the implementation of the Bayesian
                   probit regression model by Albert and Chib (1993) and the
                   Bayesian logistic regression model by Holmes and Held (2006)
                   which both lead to marginal Gaussian distributions. We in-
                   vestigate several MCMC samplers using the dependence
                   structure in different ways. The mixing and convergence
                   performances of the resulting Markov chains are evaluated
                   and compared to standard samplers in two simulation studies
                   and in an application to a real gene expression data set.",
  month         =  feb,
  year          =  2014,
  url           = "http://arxiv.org/abs/1402.2713",
  file          = "All Papers/Z/Zucknick and Richardson 2014 - MCMC algorithms for Bayesian variable se ...  in the logistic regression model for large-scale genomic applications.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1402.2713"
}


@ARTICLE{Carlin1995-fz,
  title     = "Bayesian Model Choice via Markov Chain Monte Carlo Methods",
  author    = "Carlin, Bradley P and Chib, Siddhartha",
  abstract  = "[Markov chain Monte Carlo (MCMC) integration methods enable the
               fitting of models of virtually unlimited complexity, and as such
               have revolutionized the practice of Bayesian data analysis.
               However, comparison across models may not proceed in a
               completely analogous fashion, owing to violations of the
               conditions sufficient to ensure convergence of the Markov chain.
               In this paper we present a framework for Bayesian model choice,
               along with an MCMC algorithm that does not suffer from
               convergence difficulties. Our algorithm applies equally well to
               problems where only one model is contemplated but its proper
               size is not known at the outset, such as problems involving
               integer-valued parameters, multiple changepoints or finite
               mixture distributions. We illustrate our approach with two
               published examples.]",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  57,
  number    =  3,
  pages     = "473--484",
  year      =  1995,
  url       = "http://www.jstor.org/stable/2346151"
}


@ARTICLE{Albert1993-ho,
  title     = "Bayesian Analysis of Binary and Polychotomous Response Data",
  author    = "Albert, James H and Chib, Siddhartha",
  abstract  = "Abstract A vast literature in statistics, biometrics, and
               econometrics is concerned with the analysis of binary and
               polychotomous response data. The classical approach fits a
               categorical response regression model using maximum likelihood,
               and inferences about the model are based on the associated
               asymptotic theory. The accuracy of classical confidence
               statements is questionable for small sample sizes. In this
               article, exact Bayesian methods for modeling categorical
               response data are developed using the idea of data augmentation.
               The general approach can be summarized as follows. The probit
               regression model for binary outcomes is seen to have an
               underlying normal regression structure on latent continuous
               data. Values of the latent data can be simulated from suitable
               truncated normal distributions. If the latent data are known,
               then the posterior distribution of the parameters can be
               computed using standard results for normal linear models. Draws
               from this posterior are used to sample new latent data, and the
               process is iterated with Gibbs sampling. This data augmentation
               approach provides a general framework for analyzing binary
               regression models. It leads to the same simplification achieved
               earlier for censored regression models. Under the proposed
               framework, the class of probit regression models can be enlarged
               by using mixtures of normal distributions to model the latent
               data. In this normal mixture class, one can investigate the
               sensitivity of the parameter estimates to the choice of ?link
               function,? which relates the linear regression estimate to the
               fitted probabilities. In addition, this approach allows one to
               easily fit Bayesian hierarchical models. One specific model
               considered here reflects the belief that the vector of
               regression coefficients lies on a smaller dimension linear
               subspace. The methods can also be generalized to multinomial
               response models with J > 2 categories. In the ordered
               multinomial model, the J categories are ordered and a model is
               written linking the cumulative response probabilities with the
               linear regression structure. In the unordered multinomial model,
               the latent variables have a multivariate normal distribution
               with unknown variance-covariance matrix. For both multinomial
               models, the data augmentation method combined with Gibbs
               sampling is outlined. This approach is especially attractive for
               the multivariate probit model, where calculating the likelihood
               can be difficult.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  88,
  number    =  422,
  pages     = "669--679",
  month     =  jun,
  year      =  1993,
  url       = "https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321"
}


@ARTICLE{Holmes2006-xe,
  title     = "Bayesian auxiliary variable models for binary and multinomial
               regression",
  author    = "Holmes, Chris C and Held, Leonhard",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bayesian Anal.",
  publisher = "International Society for Bayesian Analysis",
  volume    =  1,
  number    =  1,
  pages     = "145--168",
  month     =  mar,
  year      =  2006,
  url       = "https://projecteuclid.org/euclid.ba/1340371078",
  keywords  = "Auxiliary variables; Bayesian binary and multinomial regression;
               Markov chain Monte Carlo; Model averaging; Scale mixture of
               normals; Variable selection",
  language  = "en"
}

@ARTICLE{Liu1996-dg,
  title     = "Peskun's theorem and a modified discrete-state Gibbs sampler",
  author    = "Liu, Jun S",
  abstract  = "AbstractSUMMARY. Attention is drawn to the use of Peskun's
               theorem in improving statistical efficiency of discrete-state
               Gibbs sampling.",
  journal   = "Biometrika",
  publisher = "Narnia",
  volume    =  83,
  number    =  3,
  pages     = "681--682",
  month     =  sep,
  year      =  1996,
  url       = "https://academic.oup.com/biomet/article/83/3/681/241540"
}


@ARTICLE{Duane1987-ta,
  title     = "Hybrid Monte Carlo",
  author    = "Duane, Simon and Kennedy, A D and Pendleton, Brian J and Roweth,
               Duncan",
  abstract  = "We present a new method for the numerical simulation of lattice
               field theory. A hybrid (molecular dynamics/Langevin) algorithm
               is used to guide a Monte Carlo simulation. There are no
               discretization errors even for large step sizes. The method is
               especially efficient for systems such as quantum chromodynamics
               which contain fermionic degrees of freedom. Detailed results are
               presented for four-dimensional compact quantum electrodynamics
               including the dynamical effects of electrons.",
  journal   = "Phys. Lett. B",
  publisher = "Elsevier",
  volume    =  195,
  number    =  2,
  pages     = "216--222",
  month     =  sep,
  year      =  1987,
  url       = "http://www.sciencedirect.com/science/article/pii/037026938791197X"
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and Skye Wanderman-Milne},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.1.55},
  year = {2018},
}

@MISC{Siu_Kwan_Lam_Continuum_Analytics_Austin_Texas_undated-fa,
  title        = "Numba | Proceedings of the Second Workshop on the {LLVM}
                  Compiler Infrastructure in {HPC}",
  author       = "{Siu Kwan Lam Continuum Analytics, Austin, Texas} and
                  {Antoine Pitrou Continuum Analytics,} and {Stanley Seibert
                  Continuum Analytics,}",
  url          = "https://dl.acm.org/doi/pdf/10.1145/2833157.2833162",
  howpublished = "\url{https://dl.acm.org/doi/pdf/10.1145/2833157.2833162}",
  note         = "Accessed: 2020-2-6",
  language     = "en"
}
