\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2020)Ainslie, Ontanon, Alberti, Pham, Ravula, and
  Sanghai]{ainslie2020encoding}
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh Ravula,
  and Sumit Sanghai.
\newblock Etc: Encoding long and structured data in transformers, 2020.

\bibitem[Ba et~al.(2016)Ba, Hinton, Mnih, Leibo, and
  Ionescu]{NIPS2016_9f44e956}
Jimmy Ba, Geoffrey~E Hinton, Volodymyr Mnih, Joel~Z Leibo, and Catalin Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Burtsev et~al.(2020)Burtsev, Kuratov, Peganov, and
  Sapunov]{burtsev2020memory-transformer}
Mikhail~S Burtsev, Yuri Kuratov, Anton Peganov, and Grigory~V Sapunov.
\newblock Memory transformer.
\newblock \emph{arXiv preprint arXiv:2006.11527}, 2020.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers, 2019.

\bibitem[Cho et~al.(2014)Cho, van Merri{\"e}nboer, Bahdanau, and
  Bengio]{cho2014gru}
Kyunghyun Cho, Bart van Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder{--}decoder
  approaches.
\newblock In \emph{Proceedings of {SSST}-8, Eighth Workshop on Syntax,
  Semantics and Structure in Statistical Translation}, pages 103--111, Doha,
  Qatar, October 2014. Association for Computational Linguistics.
\newblock \doi{10.3115/v1/W14-4012}.
\newblock URL \url{https://aclanthology.org/W14-4012}.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformerxl}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V. Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {{BERT}}: {{Pre}}-training of {{Deep Bidirectional Transformers}} for
  {{Language Understanding}}.
\newblock In \emph{Proceedings of the 2019 {{Conference}} of the {{North
  American Chapter}} of the {{Association}} for {{Computational Linguistics}}:
  {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short
  Papers}})}, pages 4171--4186, 2019.
\newblock URL \url{https://aclweb.org/anthology/papers/N/N19/N19-1423/}.

\bibitem[Ding et~al.(2021)Ding, Shang, Wang, Sun, Tian, Wu, and
  Wang]{ding-etal-2021-ernie-doc}
SiYu Ding, Junyuan Shang, Shuohuan Wang, Yu~Sun, Hao Tian, Hua Wu, and Haifeng
  Wang.
\newblock {ERNIE}-{D}oc: A retrospective long-document modeling transformer.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 2914--2927,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.227}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.227}.

\bibitem[Dong et~al.(2018)Dong, Xu, and Xu]{speech-transformer}
Linhao Dong, Shuang Xu, and Bo~Xu.
\newblock Speech-transformer: A no-recurrence sequence-to-sequence model for
  speech recognition.
\newblock In \emph{2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 5884--5888, 2018.
\newblock \doi{10.1109/ICASSP.2018.8462506}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Fan et~al.(2020)Fan, Lavril, Grave, Joulin, and
  Sukhbaatar]{fan2020feedback-transformer}
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar
  Sukhbaatar.
\newblock Addressing some limitations of transformers with feedback memory.
\newblock \emph{arXiv preprint arXiv:2002.09402}, 2020.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines, 2014.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska-Barwińska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia,
  Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and
  Hassabis]{graves2016hybrid}
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
  Grabska-Barwińska, Sergio~Gómez Colmenarejo, Edward Grefenstette, Tiago
  Ramalho, John Agapiou, Adrià~Puigdomènech Badia, Karl~Moritz Hermann, Yori
  Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil
  Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538\penalty0 (7626):\penalty0 471--476, October 2016.
\newblock ISSN 00280836.
\newblock URL \url{http://dx.doi.org/10.1038/nature20101}.

\bibitem[Grefenstette et~al.(2015)Grefenstette, Hermann, Suleyman, and
  Blunsom]{grefenstette2015learning}
Edward Grefenstette, Karl~Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.
\newblock Learning to transduce with unbounded memory, 2015.

\bibitem[Gulcehre et~al.(2016)Gulcehre, Chandar, Cho, and
  Bengio]{gulcehre2016dynamic}
Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio.
\newblock Dynamic neural turing machine with soft and hard addressing schemes.
\newblock \emph{arXiv preprint arXiv:1607.00036}, 2016.

\bibitem[Gulcehre et~al.(2017)Gulcehre, Chandar, and
  Bengio]{gulcehre2017memory}
Caglar Gulcehre, Sarath Chandar, and Yoshua Bengio.
\newblock Memory augmented neural networks with wormhole connections.
\newblock \emph{arXiv preprint arXiv:1701.08718}, 2017.

\bibitem[Guo et~al.(2019)Guo, Qiu, Liu, Shao, Xue, and
  Zhang]{guo2019startransformer}
Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng
  Zhang.
\newblock Star-transformer, 2019.

\bibitem[Gupta and Berant(2020)]{gupta2020gmat}
Ankit Gupta and Jonathan Berant.
\newblock Gmat: Global memory augmentation for transformers.
\newblock \emph{arXiv preprint arXiv:2006.03274}, 2020.

\bibitem[Hochreiter and Schmidhuber(1997)]{10.1162/neco.1997.9.8.1735}
Sepp Hochreiter and J\"{u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Comput.}, 9\penalty0 (8):\penalty0 1735–1780, November
  1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.8.1735}.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.8.1735}.

\bibitem[Jaegle et~al.(2021)Jaegle, Borgeaud, Alayrac, Doersch, Ionescu, Ding,
  Koppula, Zoran, Brock, Shelhamer, et~al.]{jaegle2021perceiverio}
Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin
  Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
  Shelhamer, et~al.
\newblock Perceiver io: A general architecture for structured inputs \&
  outputs.
\newblock \emph{arXiv preprint arXiv:2107.14795}, 2021.

\bibitem[Joulin and Mikolov(2015)]{arm2015inferring}
Armand Joulin and Tomas Mikolov.
\newblock Inferring algorithmic patterns with stack-augmented recurrent nets,
  2015.

\bibitem[Ju et~al.(2021)Ju, Roller, Sukhbaatar, and Weston]{ju2021staircase}
Da~Ju, Stephen Roller, Sainbayar Sukhbaatar, and Jason Weston.
\newblock Staircase attention for recurrent processing of sequences.
\newblock \emph{arXiv preprint arXiv:2106.04279}, 2021.

\bibitem[Kiesel et~al.(2019)Kiesel, Mestre, Shukla, Vincent, Adineh, Corney,
  Stein, and Potthast]{kiesel2019semeval}
Johannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh,
  David Corney, Benno Stein, and Martin Potthast.
\newblock Semeval-2019 task 4: Hyperpartisan news detection.
\newblock In \emph{Proceedings of the 13th International Workshop on Semantic
  Evaluation}, pages 829--839, 2019.

\bibitem[Kingma and Ba(2015)]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR (Poster)}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Lample et~al.(2019)Lample, Sablayrolles, Ranzato, Denoyer, and
  Jégou]{lample2019large}
Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic
  Denoyer, and Hervé Jégou.
\newblock Large memory layers with product keys, 2019.

\bibitem[Lei et~al.(2020)Lei, Wang, Shen, Yu, Berg, and Bansal]{lei2020mart}
Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara~L. Berg, and Mohit Bansal.
\newblock Mart: Memory-augmented recurrent transformer for coherent video
  paragraph captioning, 2020.

\bibitem[Liu et~al.(2022)Liu, Liu, Chen, Lu, Feng, Feng, Sun, Tian, Wu, and
  Wang]{liu2022ernie}
Yang Liu, Jiaxiang Liu, Li~Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu~Sun,
  Hao Tian, Hua Wu, and Haifeng Wang.
\newblock Ernie-sparse: Learning hierarchical efficient transformer through
  regularized self-attention.
\newblock \emph{arXiv preprint arXiv:2203.12276}, 2022.

\bibitem[Mahoney(2006)]{enwik8}
Matt Mahoney.
\newblock Large text compression benchmark, 2006.
\newblock URL \url{http://www.mattmahoney.net/dc/text.html}.

\bibitem[Martins et~al.(2021)Martins, Marinho, and
  Martins]{martins2021infinity}
Pedro~Henrique Martins, Zita Marinho, and Andr{\'e}~FT Martins.
\newblock $\infty$-former: Infinite memory transformer.
\newblock \emph{arXiv preprint arXiv:2109.00301}, 2021.

\bibitem[McCulloch and Pitts(1943)]{mcculloch1943logical}
Warren~S McCulloch and Walter Pitts.
\newblock A logical calculus of the ideas immanent in nervous activity.
\newblock \emph{The bulletin of mathematical biophysics}, 5\penalty0
  (4):\penalty0 115--133, 1943.

\bibitem[Meng and Rumshisky(2018)]{meng2018context}
Yuanliang Meng and Anna Rumshisky.
\newblock Context-aware neural model for temporal information extraction.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 527--536, 2018.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2017wikitext}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=Byj72udxe}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{RadfordNarasimhanEtAl_2018_Improving_language_understanding_by_generative_pre-training}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.
\newblock URL
  \url{https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf}.

\bibitem[Rae et~al.(2016)Rae, Hunt, Harley, Danihelka, Senior, Wayne, Graves,
  and Lillicrap]{rae2016scaling}
Jack~W Rae, Jonathan~J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg
  Wayne, Alex Graves, and Timothy~P Lillicrap.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes, 2016.

\bibitem[Rae et~al.(2019)Rae, Potapenko, Jayakumar, and
  Lillicrap]{rae2019compressive}
Jack~W. Rae, Anna Potapenko, Siddhant~M. Jayakumar, and Timothy~P. Lillicrap.
\newblock Compressive transformers for long-range sequence modelling, 2019.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{dalle}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 8821--8831. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/ramesh21a.html}.

\bibitem[Stephen(1956)]{stephen1956kleene}
C~Stephen.
\newblock Kleene. representation of events in nerve nets and finite automata.
\newblock \emph{Automata studies}, 1956.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015endtoend}
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock {{Attention is All you Need}}.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.
\newblock URL \url{http://papers.nips.cc/paper/7181-attention-is-all-you-need}.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity, 2020.

\bibitem[Werbos(1990)]{werbos1990backpropagation}
Paul~J Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.

\bibitem[Weston et~al.(2014)Weston, Chopra, and Bordes]{weston2014memory}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks, 2014.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 conference on empirical methods in
  natural language processing: system demonstrations}, pages 38--45, 2020.

\bibitem[Wu et~al.(2020)Wu, Lan, Gu, and Yu]{wu2020memformer}
Qingyang Wu, Zhenzhong Lan, Jing Gu, and Zhou Yu.
\newblock Memformer: The memory-augmented transformer.
\newblock \emph{arXiv preprint arXiv:2010.06891}, 2020.

\bibitem[Xu et~al.(2021)Xu, Chen, Ma, Huang, and Xiang]{xu2021contrastive}
Peng Xu, Xinchi Chen, Xiaofei Ma, Zhiheng Huang, and Bing Xiang.
\newblock Contrastive document representation learning with graph attention
  networks.
\newblock \emph{arXiv preprint arXiv:2110.10778}, 2021.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{arXiv preprint arXiv:2007.14062}, 2020.

\end{thebibliography}
