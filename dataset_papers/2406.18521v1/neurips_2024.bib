
@inproceedings{alayrac2022flamingo,
  title     = {{Flamingo}: a Visual Language Model for Few-Shot Learning},
  author    = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022},
  url       = {https://openreview.net/forum?id=EbMuimAbPbs}
}

@inproceedings{dai2023instructblip,
  title     = {Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author    = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year      = {2023},
  url       = {https://openreview.net/forum?id=vvoWPYqZJA}
}

@InProceedings{liu2023improvedllava,
    author    = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
    title     = {Improved Baselines with Visual Instruction Tuning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {26296-26306}
}

@inproceedings{liu2023llava,
  author    = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  title     = {Visual Instruction Tuning},
  booktitle = {NeurIPS},
  year      = {2023}
}

@article{li2023otter,
  title={Otter: A Multi-Modal Model with In-Context Instruction Tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@article{bai2023qwenvl,
  title={{Qwen-VL}: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{openai2024gpt4,
  title={{GPT-4} Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {{MathVista}: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

@article{chen2023internvl,
      title={{InternLM-XComposer2}: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model},
      author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Xilin Wei and Songyang Zhang and Haodong Duan and Maosong Cao and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      journal={arXiv preprint arXiv:2401.16420},
      year={2024}
}

@article{chen2024far,
  title   = {How Far Are We to {GPT-4V}? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author  = {Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal = {arXiv preprint arXiv:2404.16821},
  year    = {2024}
}

@article{dong2024internlmxcomposer24khd,
      title={{InternLM-XComposer2-4KHD}: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD},
      author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Zhe Chen and Xinyue Zhang and Wei Li and Jingwen Li and Wenhai Wang and Kai Chen and Conghui He and Xingcheng Zhang and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang},
      journal={arXiv preprint arXiv:2404.06512},
      year={2024}
}

@misc{liu2024llavanext,
    title={{LLaVA-NeXT}: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@article{lu2024deepseekvl,
  title={{DeepSeek-VL}: Towards Real-world Vision-Language Understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@inproceedings{
laurençon2023obelics,
title={{OBELICS}: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
author={Hugo Lauren{\c{c}}on and Lucile Saulnier and Leo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=SKN2hflBIZ}
}

@article{gao2024sphinxx,
  title={{SPHINX-X}: Scaling Data and Parameters for a Family of Multi-modal Large Language Models},
  author={Gao, Peng and Zhang, Renrui and Liu, Chris and Qiu, Longtian and Huang, Siyuan and Lin, Weifeng and Zhao, Shitian and Geng, Shijie and Lin, Ziyi and Jin, Peng and others},
  journal={arXiv preprint arXiv:2402.05935},
  year={2024}
}

@article{li2024minigemini,
  title={{Mini-Gemini}: Mining the Potential of Multi-modality Vision Language Models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv:2403.18814},
  year={2024}
}

@article{viscpm,
  title   = {Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages},
  author  = {Jinyi Hu and Yuan Yao and Chongyi Wang and Shan Wang and Yinxu Pan and Qianyu Chen and Tianyu Yu and Hanghao Wu and Yue Zhao and Haoye Zhang and Xu Han and Yankai Lin and Jiao Xue and Dahai Li and Zhiyuan Liu and Maosong Sun},
  journal = {arXiv preprint arXiv:2308.12038},
  year    = {2023}
}

@article{rekateam2024reka,
  title={{Reka Core, Flash, and Edge}: A Series of Powerful Multimodal Language Models},
  author={Ormazabal, Aitor and Zheng, Che and d'Autume, Cyprien de Masson and Yogatama, Dani and Fu, Deyu and Ong, Donovan and Chen, Eric and Lamprecht, Eugenie and Pham, Hai and Ong, Isaac and others},
  journal={arXiv preprint arXiv:2404.12387},
  year={2024}
}

@article{team2023gemini,
  title   = {{Gemini}: a family of highly capable multimodal models},
  author  = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal = {arXiv preprint arXiv:2312.11805},
  year    = {2023}
}

@article{laurenccon2024matters,
  title   = {What matters when building vision-language models?},
  author  = {Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal = {arXiv preprint arXiv:2405.02246},
  year    = {2024}
}

@inproceedings{kafle2018dvqa,
  title     = {{DVQA}: Understanding Data Visualizations via Question Answering},
  author    = {Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {5648--5656},
  year      = {2018}
}

@inproceedings{masry2022chartqa,
  title     = {{ChartQA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author    = {Masry, Ahmed and Do, Xuan Long and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  pages     = {2263--2279},
  year      = {2022}
}

@article{kahou2017figureqa,
  title   = {{FigureQA}: An Annotated Figure Dataset for Visual Reasoning},
  author  = {Kahou, Samira Ebrahimi and Michalski, Vincent and Atkinson, Adam and K{\'a}d{\'a}r, {\'A}kos and Trischler, Adam and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1710.07300},
  year    = {2017}
}

@InProceedings{yue2023mmmu,
    author    = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
    title     = {{MMMU}: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {9556-9567}
}

@inproceedings{zhai2023sigmoid,
  title     = {Sigmoid Loss for Language Image Pre-Training},
  author    = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {11975--11986},
  year      = {2023}
}

@inproceedings{li2022blip,
  title        = {{BLIP}: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author       = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle    = {International conference on machine learning},
  pages        = {12888--12900},
  year         = {2022},
  organization = {PMLR}
}

@inproceedings{li2023blip,
  title        = {{BLIP-2}: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author       = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle    = {International conference on machine learning},
  pages        = {19730--19742},
  year         = {2023},
  organization = {PMLR}
}

@inproceedings{huang2023language,
  title     = {{Language Is Not All You Need}: Aligning Perception with Language Models},
  author    = {Shaohan Huang and Li Dong and Wenhui Wang and Yaru Hao and Saksham Singhal and Shuming Ma and Tengchao Lv and Lei Cui and Owais Khan Mohammed and Barun Patra and Qiang Liu and Kriti Aggarwal and Zewen Chi and Johan Bjorck and Vishrav Chaudhary and Subhojit Som and Xia Song and Furu Wei},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year      = {2023},
  url       = {https://openreview.net/forum?id=UpN2wfrLec}
}

@inproceedings{
chen2022pali,
title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=mWVoBz4W0u}
}

@article{chen2023pali,
  title   = {{PaLI-X}: On Scaling up a Multilingual Vision and Language Model},
  author  = {Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
  journal = {arXiv preprint arXiv:2305.18565},
  year    = {2023}
}

@inproceedings{
wei2021finetuned,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@article{touvron2023llama,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@misc{vicuna2023,
  title  = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
  url    = {https://lmsys.org/blog/2023-03-30-vicuna/},
  author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
  month  = {March},
  year   = {2023}
}

@misc{claude3,
  title  = {The Claude 3 Model Family: {Opus, Sonnet, Haiku}},
  url    = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
  author = {Anthropic},
  month  = {March},
  year   = {2024}
}

@article{young2024yi,
  title   = {Yi: Open foundation models by 01. ai},
  author  = {Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal = {arXiv preprint arXiv:2403.04652},
  year    = {2024}
}

@article{jiang2023mistral,
  title   = {Mistral 7B},
  author  = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal = {arXiv preprint arXiv:2310.06825},
  year    = {2023}
}

@inproceedings{
zhang2023llama,
title={{LL}a{MA}-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention},
author={Renrui Zhang and Jiaming Han and Chris Liu and Aojun Zhou and Pan Lu and Yu Qiao and Hongsheng Li and Peng Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=d4UiXAHN2W}
}

@article{zhu2023minigpt,
  title   = {{MiniGPT-4}: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author  = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal = {arXiv preprint arXiv:2304.10592},
  year    = {2023}
}

@article{ye2023mplug,
  title   = {{mPLUG-Owl}: Modularization empowers large language models with multimodality},
  author  = {Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal = {arXiv preprint arXiv:2304.14178},
  year    = {2023}
}

@article{li2023mimic,
  title   = {Mimic-it: Multi-modal in-context instruction tuning},
  author  = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal = {arXiv preprint arXiv:2306.05425},
  year    = {2023}
}

@misc{fuyu-8b,
  author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
  title  = {{Fuyu-8B}: A Multimodal Architecture for AI Agents},
  url    = {https://www.adept.ai/blog/fuyu-8b},
  year   = {2023}
}

@article{lin2023sphinx,
  title   = {{SPHINX}: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models},
  author  = {Lin, Ziyi and Liu, Chris and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Lin, Chen and Shao, Wenqi and Chen, Keqin and others},
  journal = {arXiv preprint arXiv:2311.07575},
  year    = {2023}
}

@article{lee2024moai,
  title   = {{MoAI}: Mixture of All Intelligence for Large Language and Vision Models},
  author  = {Lee, Byung-Kwan and Park, Beomchan and Kim, Chae Won and Ro, Yong Man},
  journal = {arXiv preprint arXiv:2403.07508},
  year    = {2024}
}

@inproceedings{antol2015vqa,
  title     = {{VQA}: Visual Question Answering},
  author    = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2425--2433},
  year      = {2015}
}

@inproceedings{goyal2017making,
  title     = {{Making the V in VQA Matter}: Elevating the Role of Image Understanding in Visual Question Answering},
  author    = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {6904--6913},
  year      = {2017}
}

@inproceedings{hudson2019gqa,
  title     = {{GQA}: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author    = {Hudson, Drew A and Manning, Christopher D},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {6700--6709},
  year      = {2019}
}

@inproceedings{singh2019towards,
  title     = {Towards VQA Models That Can Read},
  author    = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {8317--8326},
  year      = {2019}
}

@article{gao2024improving,
  title   = {Improving Language Understanding from Screenshots},
  author  = {Gao, Tianyu and Wang, Zirui and Bhaskar, Adithya and Chen, Danqi},
  journal = {arXiv preprint arXiv:2402.14073},
  year    = {2024}
}

@inproceedings{mathew2021docvqa,
  title     = {{DocVQA}: A Dataset for VQA on Document Images},
  author    = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages     = {2200--2209},
  year      = {2021}
}

@inproceedings{kembhavi2016diagram,
  title        = {A Diagram Is Worth A Dozen Images},
  author       = {Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle    = {Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages        = {235--251},
  year         = {2016},
  organization = {Springer}
}

@inproceedings{mathew2022infographicvqa,
  title     = {{InfographicVQA}},
  author    = {Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages     = {1697--1706},
  year      = {2022}
}

@inproceedings{
lu2022learn,
title={{Learn to Explain}: Multimodal Reasoning via Thought Chains for Science Question Answering},
author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=HjwK-Tc_Bc}
}

@inproceedings{marino2019ok,
  title     = {{OK-VQA}: A Visual Question Answering Benchmark Requiring External Knowledge},
  author    = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle = {Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages     = {3195--3204},
  year      = {2019}
}

@article{fu2024mme,
  title={{MME}: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{yu2023mm,
  title   = {{MM-Vet}: Evaluating Large Multimodal Models for Integrated Capabilities},
  author  = {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal = {arXiv preprint arXiv:2308.02490},
  year    = {2023}
}

@article{liu2024mmbench,
  title={{MMBench}: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@InProceedings{methani2020plotqa,
author = {Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M. and Kumar, Pratyush},
title = {{PlotQA}: Reasoning over Scientific Plots},
booktitle = {The IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@inproceedings{liu2024mmc,
    title = "{MMC}: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
    author = "Liu, Fuxiao  and
      Wang, Xiaoyang  and
      Yao, Wenlin  and
      Chen, Jianshu  and
      Song, Kaiqiang  and
      Cho, Sangwoo  and
      Yacoob, Yaser  and
      Yu, Dong",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.70",
    pages = "1287--1310",
    abstract = "With the rapid development of large language models (LLMs) and their integration into large multimodal models (LMMs), there has beenimpressive progress in zero-shot completion of user-oriented vision-language tasks. However, a gap remains in the domain of chartimage understanding due to the distinct abstract components in charts. To address this, we introduce a large-scale MultiModal ChartInstruction (MMC-Instruction) dataset comprising 600k instances supporting diverse tasks and chart types. Leveraging this data, we de-velop MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks. Recognizing the need for a comprehensive evaluation of LMM chart understanding, we also propose a MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated benchmark with nine distinct tasks evaluating reasoning capabilities over charts.Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the mostrecent GPT-4V model. Our work provides an instruction-tuning methodology and benchmark to advance multimodal understanding ofcharts. Code and data are available at https://github.com/FuxiaoLiu/MMC.",
}

@article{xu2024chartbench,
  title={{ChartBench}: A benchmark for complex visual reasoning in charts},
  author={Xu, Zhengzhuo and Du, Sinan and Qi, Yiyan and Xu, Chengjin and Yuan, Chun and Guo, Jian},
  journal={arXiv preprint arXiv:2312.15915},
  year={2023}
}

@article{xia2024chartx,
  title={{ChartX} \& {ChartVLM}: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning},
  author={Xia, Renqiu and Zhang, Bo and Ye, Hancheng and Yan, Xiangchao and Liu, Qi and Zhou, Hongbin and Chen, Zijun and Dou, Min and Shi, Botian and Yan, Junchi and others},
  journal={arXiv preprint arXiv:2402.12185},
  year={2024}
}

@article{wang2023cogvlm,
  title={{CogVLM}: Visual Expert for Pretrained Language Models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{panickssery2024llm,
  title={LLM Evaluators Recognize and Favor Their Own Generations},
  author={Panickssery, Arjun and Bowman, Samuel R and Feng, Shi},
  journal={arXiv preprint arXiv:2404.13076},
  year={2024}
}

@article{jimenez2023swe,
  title={{SWE-bench}: Can Language Models Resolve Real-World GitHub Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@misc{LabelStudio,
  title={{Label Studio}: Data labeling software},
  url={https://github.com/heartexlabs/label-studio},
  note={Open source software available from https://github.com/heartexlabs/label-studio},
  author={
    Maxim Tkachenko and
    Mikhail Malyuk and
    Andrey Holmanyuk and
    Nikolai Liubimov},
  year={2020-2022},
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{
zhang2023multimodal,
title={Multimodal Chain-of-Thought Reasoning in Language Models},
author={Zhuosheng Zhang and Aston Zhang and Mu Li and hai zhao and George Karypis and Alex Smola},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=y1pPWFVfvR},
note={}
}

@inproceedings{
schaeffer2024emergent,
title={Are Emergent Abilities of Large Language Models a Mirage?},
author={Rylan Schaeffer and Brando Miranda and Sanmi Koyejo},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=ITw9edRDlD}
}

@inproceedings{
zhou2023webarena,
title={{WebArena}: A Realistic Web Environment for Building Autonomous Agents},
author={Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=oKn9c6ytLx}
}

@inproceedings{rajpurkar2018know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@article{hartmann2023sok,
  title={{SoK}: Memorization in General-Purpose Large Language Models},
  author={Hartmann, Valentin and Suri, Anshuman and Bindschaedler, Vincent and Evans, David and Tople, Shruti and West, Robert},
  journal={arXiv preprint arXiv:2310.18362},
  year={2023}
}

@article{whitney2024real,
  title={Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention},
  author={Whitney, Cedric Deslandes and Norman, Justin},
  journal={arXiv preprint arXiv:2405.01820},
  year={2024}
}

@article{dubois2024alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2024multiplechoice,
    title = "Can Multiple-choice Questions Really Be Useful in Detecting the Abilities of {LLM}s?",
    author = "Li, Wangyue  and
      Li, Liangzhi  and
      Xiang, Tong  and
      Liu, Xiao  and
      Deng, Wei  and
      Garcia, Noa",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.251",
    pages = "2819--2834",
    abstract = "Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM{'}s capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ{'}s efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs{'} output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.",
}

@article{li2023scigraphqa,
  title={{SciGraphQA}: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs},
  author={Li, Shengzhi and Tajbakhsh, Nima},
  journal={arXiv preprint arXiv:2308.03349},
  year={2023}
}

@article{li2024multimodal,
  title={{Multimodal ArXiv}: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models},
  author={Li, Lei and Wang, Yuqi and Xu, Runxin and Wang, Peiyi and Feng, Xiachong and Kong, Lingpeng and Liu, Qi},
  journal={arXiv preprint arXiv:2403.00231},
  year={2024}
}

@article{gebru2021datasheets,
author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and III, Hal Daum\'{e} and Crawford, Kate},
title = {Datasheets for datasets},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3458723},
doi = {10.1145/3458723},
abstract = {Documentation to facilitate communication between dataset creators and consumers.},
journal = {Commun. ACM},
month = {nov},
pages = {86–92},
numpages = {7}
}

@inproceedings{hsu2021scicap,
    title = "{S}ci{C}ap: Generating Captions for Scientific Figures",
    author = "Hsu, Ting-Yao  and
      Giles, C Lee  and
      Huang, Ting-Hao",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.277",
    doi = "10.18653/v1/2021.findings-emnlp.277",
    pages = "3258--3264",
    abstract = "Researchers use figures to communicate rich, complex information in scientific papers. The captions of these figures are critical to conveying effective messages. However, low-quality figure captions commonly occur in scientific articles and may decrease understanding. In this paper, we propose an end-to-end neural framework to automatically generate informative, high-quality captions for scientific figures. To this end, we introduce SCICAP, a large-scale figure-caption dataset based on computer science arXiv papers published between 2010 and 2020. After pre-processing {--} including figure-type classification, sub-figure identification, text normalization, and caption text selection {--} SCICAP contained more than two million figures extracted from over 290,000 papers. We then established baseline models that caption graph plots, the dominant (19.2{\%}) figure type. The experimental results showed both opportunities and steep challenges of generating captions for scientific figures.",
}
