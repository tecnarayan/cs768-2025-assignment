%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Gregory Faletto at 2023-05-28 16:33:27 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{chen2022more,
	author = {Chen, Yiqun and Jewell, Sean and Witten, Daniela},
	date-added = {2023-05-28 16:33:27 -0700},
	date-modified = {2023-05-28 16:33:27 -0700},
	journal = {Journal of Computational and Graphical Statistics},
	pages = {1--11},
	publisher = {Taylor \& Francis},
	title = {More powerful selective inference for the graph fused lasso},
	year = {2022}}

@article{ko2019easily,
	author = {Ko, Seyoon and Yu, Donghyeon and Won, Joong-Ho},
	date-added = {2023-05-28 16:26:47 -0700},
	date-modified = {2023-05-28 16:26:47 -0700},
	journal = {Journal of Computational and Graphical Statistics},
	number = {4},
	pages = {821--833},
	publisher = {Taylor \& Francis},
	title = {Easily parallelizable and distributable class of algorithms for structured sparsity, with optimal acceleration},
	volume = {28},
	year = {2019}}

@article{zhu2017augmented,
	author = {Zhu, Yunzhang},
	date-added = {2023-05-28 16:22:11 -0700},
	date-modified = {2023-05-28 16:22:11 -0700},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {195--204},
	publisher = {Taylor \& Francis},
	title = {An augmented ADMM algorithm with application to the generalized lasso problem},
	volume = {26},
	year = {2017}}

@article{xin2016efficient,
	author = {Xin, Bo and Kawahara, Yoshinobu and Wang, Yizhou and Hu, Lingjing and Gao, Wen},
	date-added = {2023-05-28 16:14:07 -0700},
	date-modified = {2023-05-28 16:14:07 -0700},
	journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
	number = {4},
	pages = {1--22},
	publisher = {ACM New York, NY, USA},
	title = {Efficient generalized fused lasso and its applications},
	volume = {7},
	year = {2016}}

@article{viallon2016robustness,
	author = {Viallon, Vivian and Lambert-Lacroix, Sophie and Hoefling, H{\"o}lger and Picard, Franck},
	date-added = {2023-05-28 15:54:58 -0700},
	date-modified = {2023-05-28 15:54:58 -0700},
	journal = {Statistics and Computing},
	number = {1-2},
	pages = {285--301},
	publisher = {Springer},
	title = {On the robustness of the generalized fused lasso to prior specifications},
	volume = {26},
	year = {2016}}

@article{viallon2013adaptive,
	author = {Viallon, Vivian and Lambert-Lacroix, Sophie and H{\"o}fling, Holger and Picard, Franck},
	date-added = {2023-05-28 15:33:54 -0700},
	date-modified = {2023-05-28 15:33:54 -0700},
	title = {Adaptive generalized fused-lasso: Asymptotic properties and applications},
	year = {2013}}

@article{hofling2010coordinate,
	author = {H{\"o}fling, Holger and Binder, Harald and Schumacher, Martin},
	date-added = {2023-05-28 15:29:52 -0700},
	date-modified = {2023-05-28 15:29:52 -0700},
	journal = {arXiv preprint arXiv:1011.6409},
	title = {A coordinate-wise optimization algorithm for the Fused Lasso},
	year = {2010}}

@article{Cordeiro1991,
	abstract = {In this paper we derive general formulae for first-order biases of maximum likelihood estimates of the linear parameters, linear predictors, the dispersion parameter and fitted values in generalized linear models. These formulae may be implemented in the GLIM program to compute bias-corrected maximum likelihood estimates to order n-I, where n is the sample size, with minimal effort by means of a supplementary yeighted regression. For linear logistic models it is shown that the asymptotic bias v_ector of p is almost collinear with p. The approximate formula pp/m+ for the bias of /3 in logistic models, where p =dim(P) and m+= C mi is the sum of the binomial indices, is derived and checked numerically.},
	author = {Cordeiro, Gauss M and McCullagh, Peter},
	date-added = {2023-05-25 11:23:51 -0700},
	date-modified = {2023-05-25 11:23:51 -0700},
	doi = {10.1111/j.2517-6161.1991.tb01852.x},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Cordeiro, Mccullagh - 1991 - Bias Correction in Generalized Linear Models.pdf:pdf},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {3},
	pages = {629--643},
	title = {{Bias Correction in Generalized Linear Models}},
	volume = {53},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1111/j.2517-6161.1991.tb01852.x}}

@book{lehmann1999elements,
	author = {Lehmann, Erich Leo},
	date-added = {2023-05-25 11:04:56 -0700},
	date-modified = {2023-05-25 11:04:56 -0700},
	publisher = {Springer},
	title = {Elements of large-sample theory},
	year = {1999}}

@manual{mldatar,
	author = {Gary Hutson and Asif Laldin and Isabella Vel{\'a}squez},
	date-added = {2023-05-23 18:49:28 -0700},
	date-modified = {2023-05-23 18:49:28 -0700},
	note = {R package version 0.1.3},
	title = {{MLDataR: Collection of Machine Learning Datasets for Supervised Machine Learning}},
	url = {https://CRAN.R-project.org/package=MLDataR},
	year = {2022},
	bdsk-url-1 = {https://CRAN.R-project.org/package=MLDataR}}

@article{Arnold2016,
	abstract = {We consider efficient implementations of the generalized lasso dual path algorithm given by Tibshirani and Taylor in 2011. We first describe a generic approach that covers any penalty matrix D and any (full column rank) matrix X of predictor variables. We then describe fast implementations for the special cases of trend filtering problems, fused lasso problems, and sparse fused lasso problems, both with X = I and a general matrix X. These specialized implementations offer a considerable improvement over the generic implementation, both in terms of numerical stability and efficiency of the solution path computation. These algorithms are all available for use in the genlasso R package, which can be found in the CRAN repository.},
	archiveprefix = {arXiv},
	arxivid = {1405.3222},
	author = {Arnold, Taylor B. and Tibshirani, Ryan J.},
	date-added = {2023-05-23 18:19:32 -0700},
	date-modified = {2023-05-23 18:19:32 -0700},
	doi = {10.1080/10618600.2015.1008638},
	eprint = {1405.3222},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Arnold, Tibshirani - 2016 - Efficient Implementations of the Generalized Lasso Dual Path Algorithm.pdf:pdf},
	issn = {15372715},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Fused lasso,Laplacian linear systems,QR decomposition,Trend filtering},
	number = {1},
	pages = {1--27},
	title = {{Efficient Implementations of the Generalized Lasso Dual Path Algorithm}},
	volume = {25},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/10618600.2015.1008638}}

@inproceedings{xin2014efficient,
	author = {Xin, Bo and Kawahara, Yoshinobu and Wang, Yizhou and Gao, Wen},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	date-added = {2023-05-23 11:42:47 -0700},
	date-modified = {2023-05-23 11:42:47 -0700},
	number = {1},
	title = {Efficient generalized fused lasso and its application to the diagnosis of Alzheimer's disease},
	volume = {28},
	year = {2014}}

@book{wooldridge2010econometric,
	author = {Wooldridge, Jeffrey M},
	date-added = {2023-05-01 13:48:28 -0700},
	date-modified = {2023-05-01 13:48:28 -0700},
	publisher = {MIT press},
	title = {Econometric analysis of cross section and panel data},
	year = {2010}}

@book{ash1990information,
	author = {Ash, R.B.},
	date-added = {2023-01-24 19:56:10 -0800},
	date-modified = {2023-01-24 19:56:10 -0800},
	isbn = {9780486665214},
	lccn = {90045415},
	publisher = {Dover Publications},
	series = {Dover books on advanced mathematics},
	title = {Information Theory},
	url = {https://books.google.com/books?id=yZ1JZA6Wo6YC},
	year = {1990},
	bdsk-url-1 = {https://books.google.com/books?id=yZ1JZA6Wo6YC}}

@inbook{vershynin_2012,
	author = {Vershynin, Roman},
	booktitle = {Compressed Sensing: Theory and Applications},
	date-added = {2023-01-24 16:25:24 -0800},
	date-modified = {2023-01-24 16:25:24 -0800},
	doi = {10.1017/CBO9780511794308.006},
	editor = {Eldar, Yonina C. and Kutyniok, GittaEditors},
	pages = {210--268},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Introduction to the non-asymptotic analysis of random matrices},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1017/CBO9780511794308.006}}

@article{PeterBickel2009,
	abstract = {We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel , oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the p estimation loss for 1 ≤ p ≤ 2 in the linear model when the number of variables can be much larger than the sample size.},
	author = {Bickel, Peter J and Ritov, Ya'Acov and Tsybakov, Alexandre B},
	date-added = {2023-01-23 19:54:26 -0800},
	date-modified = {2023-01-23 19:54:40 -0800},
	doi = {10.1214/08-AOS620},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Peter Bickel, Ritov, Tsybakov - 2009 - SIMULTANEOUS ANALYSIS OF LASSO AND DANTZIG SELECTOR 1.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {DSO 607,DSO 607 Week 4,feature selection},
	mendeley-tags = {DSO 607,DSO 607 Week 4,feature selection},
	number = {4},
	pages = {1705--1732},
	title = {{Simultaneous Analysis of LASSO and Dantzig Selector}},
	url = {https://projecteuclid-org.libproxy1.usc.edu/download/pdfview_1/euclid.aos/1245332830},
	volume = {37},
	year = {2009},
	bdsk-url-1 = {https://projecteuclid-org.libproxy1.usc.edu/download/pdfview_1/euclid.aos/1245332830},
	bdsk-url-2 = {https://doi.org/10.1214/08-AOS620}}

@article{Ekvall2022,
	abstract = {We propose a unified framework for likelihood-based regression modeling when the response variable has finite support. Our work is motivated by the fact that, in practice, observed data are discrete and bounded. The proposed methods assume a model which includes models previously considered for interval-censored variables with log-concave distributions as special cases. The resulting log-likelihood is concave, which we use to establish asymptotic normality of its maximizer as the number of observations $n$ tends to infinity with the number of parameters $d$ fixed, and rates of convergence of $L_1$-regularized estimators when the true parameter vector is sparse and $d$ and $n$ both tend to infinity with $\log(d) / n \to 0$. We consider an inexact proximal Newton algorithm for computing estimates and give theoretical guarantees for its convergence. The range of possible applications is wide, including but not limited to survival analysis in discrete time, the modeling of outcomes on scored surveys and questionnaires, and, more generally, interval-censored regression. The applicability and usefulness of the proposed methods are illustrated in simulations and data examples.},
	archiveprefix = {arXiv},
	arxivid = {2203.04582},
	author = {Ekvall, K.O. and Bottai, M.},
	date-added = {2023-01-20 10:07:01 -0800},
	date-modified = {2023-01-20 10:07:01 -0800},
	doi = {10.1111/biom.13760},
	eprint = {2203.04582},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Ekvall, Bottai - 2022 - Concave likelihood‐based regression with finite‐support response variables.pdf:pdf},
	issn = {0006-341X},
	journal = {Biometrics},
	keywords = {2 division of biostatistics,florida,gainesville,institute of,of statistics,university of,usa},
	number = {March},
	pages = {1--12},
	title = {{Concave likelihood‐based regression with finite‐support response variables}},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1111/biom.13760}}

@inproceedings{zhang2014optimal,
	author = {Zhang, Weinan and Yuan, Shuai and Wang, Jun},
	booktitle = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
	date-added = {2022-12-22 19:35:36 -0800},
	date-modified = {2022-12-22 19:35:36 -0800},
	pages = {1077--1086},
	title = {Optimal real-time bidding for display advertising},
	year = {2014}}

@article{von2019predicting,
	author = {Von Wachter, Till and Bertrand, Marianne and Pollack, Harold and Rountree, Janey and Blackwell, Brian},
	date-added = {2022-12-19 14:00:56 -0800},
	date-modified = {2022-12-19 14:00:56 -0800},
	journal = {California Policy Lab and University of Chicago Poverty Lab},
	title = {Predicting and preventing homelessness in Los Angeles},
	year = {2019}}

@inproceedings{he2014practical,
	author = {He, Xinran and Pan, Junfeng and Jin, Ou and Xu, Tianbing and Liu, Bo and Xu, Tao and Shi, Yanxin and Atallah, Antoine and Herbrich, Ralf and Bowers, Stuart and others},
	booktitle = {Proceedings of the eighth international workshop on data mining for online advertising},
	date-added = {2022-12-19 14:00:42 -0800},
	date-modified = {2022-12-19 14:00:42 -0800},
	pages = {1--9},
	title = {Practical lessons from predicting clicks on ads at facebook},
	year = {2014}}

@article{10.1093/bioinformatics/bty984,
	abstract = {{Clinical decision support systems have been applied in numerous fields, ranging from cancer survival toward drug resistance prediction. Nevertheless, clinical decision support systems typically have a caveat: many of them are perceived as black-boxes by non-experts and, unfortunately, the obtained scores cannot usually be interpreted as class probability estimates. In probability-focused medical applications, it is not sufficient to perform well with regards to discrimination and, consequently, various calibration methods have been developed to enable probabilistic interpretation. The aims of this study were (i) to develop a tool for fast and comparative analysis of different calibration methods, (ii) to demonstrate their limitations for the use on clinical data and (iii) to introduce our novel method GUESS.We compared the performances of two different state-of-the-art calibration methods, namely histogram binning and Bayesian Binning in Quantiles, as well as our novel method GUESS on both, simulated and real-world datasets. GUESS demonstrated calibration performance comparable to the state-of-the-art methods and always retained accurate class discrimination. GUESS showed superior calibration performance in small datasets and therefore may be an optimal calibration method for typical clinical datasets. Moreover, we provide a framework (CalibratR) for R, which can be used to identify the most suitable calibration method for novel datasets in a timely and efficient manner. Using calibrated probability estimates instead of original classifier scores will contribute to the acceptance and dissemination of machine learning based classification models in cost-sensitive applications, such as clinical research.GUESS as part of CalibratR can be downloaded at CRAN.}},
	author = {Schwarz, Johanna and Heider, Dominik},
	date-added = {2022-12-19 13:54:37 -0800},
	date-modified = {2022-12-19 13:54:37 -0800},
	doi = {10.1093/bioinformatics/bty984},
	eprint = {https://academic.oup.com/bioinformatics/article-pdf/35/14/2458/28913220/bty984.pdf},
	issn = {1367-4803},
	journal = {Bioinformatics},
	month = {11},
	number = {14},
	pages = {2458-2465},
	title = {{GUESS: projecting machine learning scores to well-calibrated probability estimates for clinical decision-making}},
	url = {https://doi.org/10.1093/bioinformatics/bty984},
	volume = {35},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1093/bioinformatics/bty984}}

@inproceedings{naeini2015obtaining,
	author = {Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
	booktitle = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
	date-added = {2022-12-19 13:37:37 -0800},
	date-modified = {2022-12-19 13:37:37 -0800},
	title = {Obtaining well calibrated probabilities using bayesian binning},
	year = {2015}}

@article{benedetti2010scoring,
	author = {Benedetti, Riccardo},
	date-added = {2022-12-19 13:37:13 -0800},
	date-modified = {2022-12-19 13:37:13 -0800},
	journal = {Monthly Weather Review},
	number = {1},
	pages = {203--211},
	title = {Scoring rules for forecast verification},
	volume = {138},
	year = {2010}}

@misc{ordinal,
	author = {{R. H. B. Christensen}},
	date-added = {2022-12-19 13:36:51 -0800},
	date-modified = {2022-12-19 13:36:51 -0800},
	edition = {R package version 2019.12-10},
	title = {{ordinal---Regression Models for Ordinal Data}},
	url = {https://CRAN.R-project.org/package=ordinal},
	year = {2019},
	bdsk-url-1 = {https://CRAN.R-project.org/package=ordinal}}

@article{Christensen2011,
	abstract = {The A-not A protocol with sureness produce multinomial observations that are traditionally analyzed with statistical methods for contingency tables or by calculation of an R-index. In this paper it is shown that the Thurstonian model for the A-not A protocol can be written as a cumulative link model including the binormal unequal variances model. The model is extended to allow for explanatory variables and we illustrate how consumer differences can be modeled within the Thurstonian framework on a consumer study of packet soup conducted by Unilever. The extension also allows several test-product variations to be analyzed in the same model providing additional insight and reduced experimental costs. The effects of explanatory variables on the Thurstonian delta, the sensitivity (AUC), the ROC curve and the response category thresholds are discussed in detail. All statistical methods are implemented in the free R-package ordinal (http://www.cran.r-project.org/package=ordinal/). {\textcopyright} 2011 Elsevier Ltd.},
	author = {Christensen, Rune Haubo Bojesen and Cleaver, Graham and Brockhoff, Per Bruun},
	date-added = {2022-12-19 13:36:32 -0800},
	date-modified = {2022-12-19 13:36:32 -0800},
	doi = {10.1016/j.foodqual.2011.03.003},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Christensen, Cleaver, Brockhoff - 2011 - Statistical and Thurstonian models for the A-not A protocol with and without sureness.pdf:pdf},
	issn = {09503293},
	journal = {Food Quality and Preference},
	keywords = {Cumulative link models,Ordinal regression models,R-program,ROC curves,Signal detection theory,Thurstonian models},
	number = {6},
	pages = {542--549},
	publisher = {Elsevier Ltd},
	title = {{Statistical and Thurstonian models for the A-not A protocol with and without sureness}},
	url = {http://dx.doi.org/10.1016/j.foodqual.2011.03.003},
	volume = {22},
	year = {2011},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.foodqual.2011.03.003}}

@book{Wooldridge2002,
	abstract = {Textbook on the econometric analysis of cross section and panel data
    suitable for a second semester course in graduate econometrics,
    following a first course at the level of A Course in Econometrics
    by A. S. Goldberger (1991) or Econometric Analysis by W. Greene
    (1997). Covers conditional expectations and related concepts in
    econometrics; basic asymptotic theory; the single equation linear
    model and ordinary least squares (OLS) estimation; instrumental
    variables estimation of single equation linear models; additional
    single equation topics; estimating systems of equations by OLS and
    generalized least squares; system estimation by instrumental variables;
    simultaneous equations models; basic linear unobserved effects panel
    data models; more topics in unobserved effects models; approaches
    to nonlinear estimation; M-estimation; maximum likelihood methods;
    the generalized method of moments and minimum distance estimation;
    discrete response models; corner solution outcomes and censored
    regression models; sample selection, attrition, and stratified sampling;
    estimating average treatment effects; count data and related models;
    and duration analysis. Contains numerous end-of-chapter problems.
    Several problems require data sets that are included with the book.
    Wooldridge is Professor of Economics at Michigan State University.
    Index},
	added-at = {2006-09-29T17:06:41.000+0200},
	address = {Cambridge and London},
	author = {Wooldridge, Jeffrey M.},
	biburl = {https://www.bibsonomy.org/bibtex/243450720ab17540e4fce3251239ec0e8/gerhard},
	date-added = {2022-10-20 13:39:04 -0700},
	date-modified = {2022-10-20 13:39:04 -0700},
	file = {#F#},
	interhash = {7634506fe1e40831d09521e99547f88b},
	intrahash = {43450720ab17540e4fce3251239ec0e8},
	isbn = {0262232197},
	keywords = {Equation Econometrics Models Multiple/Simultaneous with Methods},
	publisher = {MIT Press},
	timestamp = {2006-09-29T17:06:41.000+0200},
	title = {Econometric analysis of cross section and panel data},
	year = 2002}

@book{hansen2022econometrics,
	author = {Hansen, B.},
	date-added = {2022-10-20 13:37:52 -0700},
	date-modified = {2022-10-20 13:37:52 -0700},
	isbn = {9780691235899},
	lccn = {2021049283},
	publisher = {Princeton University Press},
	title = {Econometrics},
	url = {https://books.google.com/books?id=Pte7zgEACAAJ},
	year = {2022},
	bdsk-url-1 = {https://books.google.com/books?id=Pte7zgEACAAJ}}

@book{horn_johnson_2012,
	author = {Horn, Roger A. and Johnson, Charles R.},
	date-added = {2022-10-20 13:10:39 -0700},
	date-modified = {2022-10-20 13:10:39 -0700},
	doi = {10.1017/9781139020411},
	edition = {2},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Matrix Analysis},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1017/9781139020411}}

@article{hyun2018exact,
	author = {Hyun, Sangwon and G'Sell, Max and Tibshirani, Ryan J},
	date-added = {2022-10-13 20:35:59 -0700},
	date-modified = {2022-10-13 20:35:59 -0700},
	journal = {Electronic Journal of Statistics},
	number = {1},
	pages = {1053--1097},
	publisher = {Institute of Mathematical Statistics and Bernoulli Society},
	title = {Exact post-selection inference for the generalized lasso path},
	volume = {12},
	year = {2018}}

@book{casella2021statistical,
	author = {Casella, George and Berger, Roger L},
	date-added = {2022-10-13 11:37:08 -0700},
	date-modified = {2022-10-13 11:37:08 -0700},
	publisher = {Cengage Learning},
	title = {Statistical inference},
	year = {2021}}

@book{cameron2005microeconometrics,
	author = {Cameron, A Colin and Trivedi, Pravin K},
	date-added = {2022-10-12 19:29:02 -0700},
	date-modified = {2022-10-12 19:29:02 -0700},
	publisher = {Cambridge university press},
	title = {Microeconometrics: methods and applications},
	year = {2005}}

@article{norris2006ordinal,
	author = {Norris, Colleen M and Ghali, William A and Saunders, L Duncan and Brant, Rollin and Galbraith, Diane and Faris, Peter and Knudtson, Merril L and Approach Investigators and others},
	date-added = {2022-10-12 17:35:04 -0700},
	date-modified = {2022-10-12 17:35:04 -0700},
	journal = {Journal of clinical epidemiology},
	number = {5},
	pages = {448--456},
	publisher = {Elsevier},
	title = {Ordinal regression model and the linear regression model were superior to the logistic regression models},
	volume = {59},
	year = {2006}}

@inproceedings{10.1145/2783258.2788578,
	abstract = {This paper shows how to learn probabilistic classifiers that model how sales prospects proceed through stages from first awareness to final success or failure. Specifically,we present two models, called DQM for direct qualification model and FFM for full funnel model, that can be used to rank initial leads based on their probability of conversion to a sales opportunity, probability of successful sale, and/or expected revenue. Training uses the large amount of historical data collected by customer relationship management or marketing automation software. The trained models can replace traditional lead scoring systems, which are hand-tuned and therefore error-prone and not probabilistic. DQM and FFM are designed to overcome the selection bias caused by available data being based on a traditional lead scoring system. Experimental results are shown on real sales data from two companies. Features in the training data include demographic and behavioral information about each lead. For both companies, both methods achieve high AUC scores. For one company, they result in a a 307% increase in number of successful sales, as well as a dramatic increase in total revenue. In addition, we describe the results of the DQM method in actual use. These results show that the method has additional benefits that include decreased time needed to qualify leads, and decreased number of calls placed to schedule a product demo. The proposed methods find high-quality leads earlier in the sales process because they focus on features that measure the fit of potential customers with the product being sold, in addition to their behavior.},
	address = {New York, NY, USA},
	author = {Duncan, Brendan Andrew and Elkan, Charles Peter},
	booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-12 17:21:16 -0700},
	date-modified = {2022-10-12 17:21:16 -0700},
	doi = {10.1145/2783258.2788578},
	isbn = {9781450336642},
	keywords = {predictive lead scoring, sales, machine learning, decision trees, marketing, gradient boosted trees},
	location = {Sydney, NSW, Australia},
	numpages = {8},
	pages = {1751--1758},
	publisher = {Association for Computing Machinery},
	series = {KDD '15},
	title = {Probabilistic Modeling of a Sales Funnel to Prioritize Leads},
	url = {https://doi.org/10.1145/2783258.2788578},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2783258.2788578}}

@misc{epub26912,
	abstract = {In regression models for ordinal response, each covariate can be equipped with either a simple, global effect or a more flexible and complex effect which is specific to the response categories. Instead of a priori assuming  one of these effect types, as is done in the majority of the literature, we argue in this paper that effect type selection shall be data-based. For this purpose, we propose a novel and general penalty framework that allows for an automatic, data-driven selection between global and category-specific effects in all types of ordinal regression models. Optimality conditions and an estimation algorithm for the resulting penalized estimator are given. We show that our approach is asymptotically consistent in both effect type and variable selection and possesses the oracle property. A detailed application further illustrates the workings of our method and demonstrates the advantages of effect type selection on real data.},
	author = {Wolfgang P{\"o}{\ss}necker and Gerhard Tutz},
	date-added = {2022-10-12 15:53:32 -0700},
	date-modified = {2022-10-12 15:54:26 -0700},
	keyword = {Effect Type Selection, Effect Type Lasso, Proportional Odds Model, Partial Proportional Odds Model, Ordinal Regression, Regularization, Penalization.},
	series = {tech},
	title = {A General Framework for the Selection of Effect Type in Ordinal Regression},
	url = {http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-26912-0},
	volume = {186},
	year = {2016},
	bdsk-url-1 = {http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-26912-0}}

@article{fernandez2018smote,
	author = {Fern{\'a}ndez, Alberto and Garcia, Salvador and Herrera, Francisco and Chawla, Nitesh V},
	date-added = {2022-10-12 15:45:38 -0700},
	date-modified = {2022-10-12 15:45:38 -0700},
	journal = {Journal of artificial intelligence research},
	pages = {863--905},
	title = {SMOTE for learning from imbalanced data: progress and challenges, marking the 15-year anniversary},
	volume = {61},
	year = {2018}}

@article{chawla2002smote,
	author = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
	date-added = {2022-10-12 15:44:48 -0700},
	date-modified = {2022-10-12 15:44:48 -0700},
	journal = {Journal of artificial intelligence research},
	pages = {321--357},
	title = {SMOTE: synthetic minority over-sampling technique},
	volume = {16},
	year = {2002}}

@article{owen2007infinitely,
	author = {Owen, Art B},
	date-added = {2022-10-12 15:42:51 -0700},
	date-modified = {2022-10-12 15:42:51 -0700},
	journal = {Journal of Machine Learning Research},
	number = {4},
	title = {Infinitely Imbalanced Logistic Regression.},
	volume = {8},
	year = {2007}}

@article{johnson2019,
	abstract = {The purpose of this study is to examine existing deep learning techniques for addressing class imbalanced data. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether. Class imbalance has been studied thoroughly over the last two decades using traditional machine learning models, i.e. non-deep learning. Despite recent advances in deep learning, along with its increasing popularity, very little empirical work in the area of deep learning with class imbalance exists. Having achieved record-breaking performance results in several complex domains, investigating the use of deep neural networks for problems containing high levels of class imbalance is of great interest. Available studies regarding class imbalance and deep learning are surveyed in order to better understand the efficacy of deep learning when applied to class imbalanced data. This survey discusses the implementation details and experimental results for each study, and offers additional insight into their strengths and weaknesses. Several areas of focus include: data complexity, architectures tested, performance interpretation, ease of use, big data application, and generalization to other domains. We have found that research in this area is very limited, that most existing work focuses on computer vision tasks with convolutional neural networks, and that the effects of big data are rarely considered. Several traditional methods for class imbalance, e.g. data sampling and cost-sensitive learning, prove to be applicable in deep learning, while more advanced methods that exploit neural network feature learning abilities show promising results. The survey concludes with a discussion that highlights various gaps in deep learning from class imbalanced data for the purpose of guiding future research.},
	author = {Johnson, Justin M. and Khoshgoftaar, Taghi M.},
	date = {2019/03/19},
	date-added = {2022-10-12 15:38:13 -0700},
	date-modified = {2022-10-12 15:38:13 -0700},
	doi = {10.1186/s40537-019-0192-5},
	id = {Johnson2019},
	isbn = {2196-1115},
	journal = {Journal of Big Data},
	number = {1},
	pages = {27},
	title = {Survey on deep learning with class imbalance},
	url = {https://doi.org/10.1186/s40537-019-0192-5},
	volume = {6},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1186/s40537-019-0192-5}}

@article{ordnet,
	author = {Michael J. Wurm and Paul J. Rathouz and Bret M. Hanlon},
	date-added = {2022-10-12 14:21:42 -0700},
	date-modified = {2022-10-12 14:22:49 -0700},
	journal = {Journal of Statistical Software},
	number = {6},
	pages = {1--42},
	title = {Regularized Ordinal Regression and the {ordinalNet} {R} Package},
	volume = {99},
	year = {2021}}

@article{Hyndman2006,
	abstract = {We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition as well as the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series. {\textcopyright} 2006 International Institute of Forecasters.},
	author = {Hyndman, Rob J. and Koehler, Anne B.},
	date-added = {2022-10-11 16:52:47 -0700},
	date-modified = {2022-10-11 16:52:47 -0700},
	doi = {10.1016/j.ijforecast.2006.03.001},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Hyndman, Koehler - 2006 - Another look at measures of forecast accuracy.pdf:pdf},
	issn = {01692070},
	journal = {International Journal of Forecasting},
	keywords = {Forecast accuracy,Forecast error measures,Forecast evaluation,M-competition,Mean absolute scaled error},
	number = {4},
	pages = {679--688},
	title = {{Another look at measures of forecast accuracy}},
	volume = {22},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1016/j.ijforecast.2006.03.001}}

@book{horn_johnson_1985,
	author = {Horn, Roger A. and Johnson, Charles R.},
	date-added = {2022-10-11 00:13:28 -0700},
	date-modified = {2022-10-11 00:13:28 -0700},
	doi = {10.1017/CBO9780511810817},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Matrix Analysis},
	year = {1985},
	bdsk-url-1 = {https://doi.org/10.1017/CBO9780511810817}}

@book{zhang2005schur,
	author = {Zhang, F.},
	date-added = {2022-10-11 00:13:11 -0700},
	date-modified = {2022-10-11 00:13:11 -0700},
	isbn = {9780387242712},
	lccn = {2005042750},
	publisher = {Springer},
	series = {Numerical Methods and Algorithms},
	title = {The Schur Complement and Its Applications},
	url = {https://books.google.com/books?id=Wjd8\_AwjiIIC},
	year = {2005},
	bdsk-url-1 = {https://books.google.com/books?id=Wjd8%5C_AwjiIIC}}

@inproceedings{NIPS2017_c86a7ee3,
	author = {Fathony, Rizal and Bashiri, Mohammad Ali and Ziebart, Brian},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2022-10-05 18:52:49 -0700},
	date-modified = {2022-10-05 18:52:49 -0700},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Adversarial Surrogate Losses for Ordinal Regression},
	url = {https://proceedings.neurips.cc/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2017/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf}}

@article{gutierrez2015ordinal,
	author = {Guti{\'e}rrez, Pedro Antonio and Perez-Ortiz, Maria and Sanchez-Monedero, Javier and Fernandez-Navarro, Francisco and Hervas-Martinez, Cesar},
	date-added = {2022-10-05 18:51:30 -0700},
	date-modified = {2022-10-05 18:51:30 -0700},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	number = {1},
	pages = {127--146},
	publisher = {IEEE},
	title = {Ordinal regression methods: survey and experimental study},
	volume = {28},
	year = {2015}}

@inproceedings{pmlr-v151-kleindessner22a,
	abstract = { We initiate the study of fairness for ordinal regression. We adapt two fairness notions previously considered in fair ranking and propose a strategy for training a predictor that is approximately fair according to either notion. Our predictor has the form of a threshold model, composed of a scoring function and a set of thresholds, and our strategy is based on a reduction to fair binary classification for learning the scoring function and local search for choosing the thresholds. We provide generalization guarantees on the error and fairness violation of our predictor, and we illustrate the effectiveness of our approach in extensive experiments. },
	author = {Kleindessner, Matth\"aus and Samadi, Samira and Bilal Zafar, Muhammad and Kenthapadi, Krishnaram and Russell, Chris},
	booktitle = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
	date-added = {2022-10-05 18:50:44 -0700},
	date-modified = {2022-10-05 18:50:44 -0700},
	editor = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
	month = {28--30 Mar},
	pages = {3381--3417},
	pdf = {https://proceedings.mlr.press/v151/kleindessner22a/kleindessner22a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Pairwise Fairness for Ordinal Regression},
	url = {https://proceedings.mlr.press/v151/kleindessner22a.html},
	volume = {151},
	year = {2022},
	bdsk-url-1 = {https://proceedings.mlr.press/v151/kleindessner22a.html}}

@inproceedings{pmlr-v108-gadd20a,
	abstract = {Mixtures of experts probabilistically divide the input space into regions, where the assumptions of each expert, or conditional model, need only hold locally. Combined with Gaussian process (GP) experts, this results in a powerful and highly flexible model. We focus on alternative mixtures of GP experts, which  model the joint distribution of the inputs and targets explicitly. We highlight issues of this approach in multi-dimensional input spaces, namely,  poor scalability and the need for an unnecessarily large number of experts, degrading the predictive performance and increasing uncertainty. We construct a novel model to address these issues through a nested partitioning scheme that automatically infers the number of components at both levels. Multiple response types are accommodated through a generalised GP framework, while multiple input types are included through a factorised exponential family structure. We show the effectiveness of our approach in estimating a parsimonious probabilistic description of both  synthetic data of increasing dimension and an Alzheimer's challenge dataset.},
	author = {Gadd, Charles and Wade, Sara and Boukouvalas, Alexis},
	booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
	date-added = {2022-10-05 18:48:59 -0700},
	date-modified = {2022-10-05 18:48:59 -0700},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	month = {26--28 Aug},
	pages = {3144--3154},
	pdf = {http://proceedings.mlr.press/v108/gadd20a/gadd20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Enriched mixtures of generalised Gaussian process experts},
	url = {https://proceedings.mlr.press/v108/gadd20a.html},
	volume = {108},
	year = {2020},
	bdsk-url-1 = {https://proceedings.mlr.press/v108/gadd20a.html}}

@article{Pratt1981,
	abstract = {For a very general regression model with an ordinal dependent variable, the log likelihood is proved concave if the derivative of the underlying response function has concave logarithm. For a binary dependent variable, a weaker condition suffices, namely, that the response function and its complement each have concave logarithm. The normal, logistic, sine, and extreme-value distributions, among others, satisfy the stronger condition, the t (including Cauchy) distributions only the weaker. Some converses and generalizations are also given. The model is that which arises from an ordinary linear regression model with a continuous dependent variable that is partly unobservable, being either grouped into intervals with unknown endpoints, or censored, or, more generally, grouped in some regions, censored in others, and observed exactly elsewhere. {\textcopyright} 1981, Taylor & Francis Group, LLC.},
	author = {Pratt, John W},
	date-added = {2022-10-05 11:18:51 -0700},
	date-modified = {2022-10-05 11:18:51 -0700},
	doi = {10.1080/01621459.1981.10477613},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Pratt - 1981 - Concavity of the Log Likelihood.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Censored dependent variables,Concavity of log likelihood,Grouped dependent variable,Ordered categories,Ordinal dependent variables,Strong unimodality},
	number = {373},
	pages = {103--106},
	title = {{Concavity of the log likelihood}},
	volume = {76},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.1981.10477613}}

@article{tallis1965plane,
	author = {Tallis, George Michael},
	date-added = {2022-05-10 11:28:08 -0700},
	date-modified = {2022-05-10 11:28:08 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {2},
	pages = {301--307},
	publisher = {Wiley Online Library},
	title = {Plane truncation in normal populations},
	volume = {27},
	year = {1965}}

@article{johnstone2001distribution,
	author = {Johnstone, Iain M},
	date-added = {2022-05-07 16:03:26 -0700},
	date-modified = {2022-05-07 16:03:26 -0700},
	journal = {The Annals of statistics},
	number = {2},
	pages = {295--327},
	publisher = {Institute of Mathematical Statistics},
	title = {On the distribution of the largest eigenvalue in principal components analysis},
	volume = {29},
	year = {2001}}

@book{serfling1980,
	added-at = {2009-08-21T09:49:34.000+0200},
	address = {New York, NY [u.a.]},
	author = {Serfling, {Robert J.}},
	biburl = {https://www.bibsonomy.org/bibtex/2fc5789fd0d355fc0adf3391a0753c972/fbw_hannover},
	date-added = {2022-05-07 15:58:35 -0700},
	date-modified = {2022-05-07 15:58:35 -0700},
	edition = {[Nachdr.]},
	interhash = {4a6682027949f152924a6c3fe52fd2bc},
	intrahash = {fc5789fd0d355fc0adf3391a0753c972},
	isbn = {0471024031},
	keywords = {Asymptotische_Statistik Mathematische_Statistik Sch{\"a}tztheorie},
	pagetotal = {XIV, 371},
	ppn_gvk = {024353353},
	publisher = {Wiley},
	series = {Wiley series in probability and mathematical statistics : Probability and mathematical statistics},
	timestamp = {2009-08-21T09:50:25.000+0200},
	title = {Approximation theorems of mathematical statistics},
	url = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+024353353&sourceid=fbw_bibsonomy},
	year = 1980,
	bdsk-url-1 = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+024353353&sourceid=fbw_bibsonomy}}

@article{cordeiro1991bias,
	author = {Cordeiro, Gauss M and McCullagh, Peter},
	date-added = {2022-05-07 15:58:22 -0700},
	date-modified = {2022-05-07 15:58:22 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {3},
	pages = {629--643},
	publisher = {Wiley Online Library},
	title = {Bias correction in generalized linear models},
	volume = {53},
	year = {1991}}

@book{wainwright2019high,
	author = {Wainwright, Martin J},
	date-added = {2022-05-06 12:46:29 -0700},
	date-modified = {2022-05-06 12:46:29 -0700},
	publisher = {Cambridge University Press},
	title = {High-dimensional statistics: A non-asymptotic viewpoint},
	volume = {48},
	year = {2019}}

@article{cox1995location,
	author = {Cox, Christopher},
	date-added = {2022-04-27 07:46:09 -0700},
	date-modified = {2022-04-27 07:46:09 -0700},
	journal = {Statistics in medicine},
	number = {11},
	pages = {1191--1203},
	publisher = {Wiley Online Library},
	title = {Location---scale cumulative odds models for ordinal data: A generalized non-linear model approach},
	volume = {14},
	year = {1995}}

@book{durrett2019probability,
	author = {Durrett, Rick},
	date-added = {2022-04-22 08:07:34 -0700},
	date-modified = {2022-04-22 08:07:34 -0700},
	publisher = {Cambridge university press},
	title = {Probability: theory and examples},
	volume = {49},
	year = {2019}}

@article{Liu2010,
	abstract = {The fused Lasso penalty enforces sparsity in both the coefficients and their successive differences, which is desirable for applications with features ordered in some meaningful way. The resulting problem is, however, challenging to solve, as the fused Lasso penalty is both non-smooth and non-separable. Existing algorithms have high computational complexity and do not scale to large-size problems. In this paper, we propose an Efficient Fused Lasso Algorithm (EFLA) for optimizing this class of problems. One key building block in the proposed EFLA is the Fused Lasso Signal Approxima-tor (FLSA). To efficiently solve FLSA, we propose to reformulate it as the problem of finding an "appropriate" subgradient of the fused penalty at the minimizer, and develop a Subgradient Finding Algorithm (SFA). We further design a restart technique to accelerate the convergence of SFA, by exploiting the special "structures" of both the original and the reformulated FLSA problems. Our empirical evaluations show that, both SFA and EFLA significantly outper-form existing solvers. We also demonstrate several applications of the fused Lasso.},
	author = {Liu, Jun and Yuan, Lei and Ye, Jieping},
	date-added = {2022-04-19 21:47:16 -0700},
	date-modified = {2022-04-19 21:47:16 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Yuan, Ye - 2010 - An Efficient Algorithm for a Class of Fused Lasso Problems.pdf:pdf},
	isbn = {9781450300551},
	keywords = {1 regularization,H28 [Database Management]: Database Applications-Data Min-ing General Terms Algorithms Keywords Fused Lasso,restart,subgradient},
	title = {{An Efficient Algorithm for a Class of Fused Lasso Problems}},
	year = {2010}}

@article{Yu2015,
	abstract = {a r t i c l e i n f o Penalized logistic regression Spectral data contain powerful information that can be used to identify unknown compounds and their chemical structures. In this paper, we study fused lasso logistic regression (FLLR) to classify the spectral data into two groups. We show that the FLLR has a grouping property on regression coefficients, which simultaneously selects a group of highly correlated variables together. Both the sparsity and the grouping property of the FLLR provide great advantages in the analysis of the spectral data. In particular, it resolves the well-known peak misalignment problem of the spectral data by providing data dependent binning, and provides a better interpretable classifier than other ' 1-regularization methods. We also analyze the gas chromatography/mass spectrometry data to classify the origin of herbal medicines, and illustrate the advantages of the FLLR over other existing ' 1-regularized methods.},
	author = {Yu, Donghyeon and Lee, Seul Ji and Lee, Won Jun and Kim, Sang Cheol and Lim, Johan and Kwon, Sung Won},
	date-added = {2022-04-19 21:46:08 -0700},
	date-modified = {2022-04-19 21:46:08 -0700},
	doi = {10.1016/j.chemolab.2015.01.006},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2015 - Classification of spectral data using fused lasso logistic regression.pdf:pdf},
	keywords = {Classification,Fused lasso regression,Mass spectral data 'ℓ 1-regularization},
	title = {{Classification of spectral data using fused lasso logistic regression}},
	url = {http://dx.doi.org/10.1016/j.chemolab.2015.01.006},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.chemolab.2015.01.006}}

@article{Lee2014,
	abstract = {We propose a fused lasso logistic regression to analyze callosal thickness profiles. The fused lasso regression imposes penalties on both the l1-norm of the model coefficients and their successive differences, and finds only a small number of non-zero coefficients which are locally constant. An iterative method of solving logistic regression with fused lasso regularization is proposed to make this a practical procedure. In this study we analyzed callosal thickness profiles sampled at 100 equal intervals between the rostrum and the splenium. The method was applied to corpora callosa of elderly normal controls (NCs) and patients with very mild or mild Alzheimer's disease (AD) from the Open Access Series of Imaging Studies (OASIS) database. We found specific locations in the genu and splenium of AD patients that are proportionally thinner than those of NCs. Callosal thickness in these regions combined with the Mini Mental State Examination scores differentiated AD from NC with 84% accuracy. {\textcopyright} 2013 Elsevier B.V.},
	author = {Lee, Sang H. and Yu, Donghyeon and Bachman, Alvin H. and Lim, Johan and Ardekani, Babak A.},
	date-added = {2022-04-19 21:45:05 -0700},
	date-modified = {2022-04-19 21:45:05 -0700},
	doi = {10.1016/j.jneumeth.2013.09.017},
	file = {:Users/gregfaletto/Desktop/1-s2.0-S0165027013003282-main.pdf:pdf},
	issn = {1872678X},
	journal = {Journal of Neuroscience Methods},
	keywords = {Alzheimer's disease,Brain,Corpus callosum,Fused lasso,Logistic regression,MRI},
	pages = {78--84},
	pmid = {24121089},
	publisher = {Elsevier B.V.},
	title = {{Application of fused lasso logistic regression to the study of corpus callosum thickness in early alzheimer's disease}},
	url = {http://dx.doi.org/10.1016/j.jneumeth.2013.09.017},
	volume = {221},
	year = {2014},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.jneumeth.2013.09.017}}

@article{burridge1981note,
	author = {Burridge, J},
	date-added = {2022-04-19 11:24:09 -0700},
	date-modified = {2022-04-19 11:24:09 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {1},
	pages = {41--45},
	publisher = {Wiley Online Library},
	title = {A note on maximum likelihood estimation for regression models using grouped data},
	volume = {43},
	year = {1981}}

@book{agresti2010analysis,
	author = {Agresti, Alan},
	date-added = {2022-04-15 13:26:16 -0700},
	date-modified = {2022-04-15 13:26:16 -0700},
	publisher = {John Wiley \& Sons},
	title = {Analysis of ordinal categorical data},
	volume = {656},
	year = {2010}}

@book{faraway2016extending,
	author = {Faraway, Julian J},
	date-added = {2022-04-15 13:18:13 -0700},
	date-modified = {2022-04-15 13:18:13 -0700},
	publisher = {Chapman and Hall/CRC},
	title = {Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models},
	year = {2016}}

@article{tutz2020ordinal,
	author = {Tutz, Gerhard},
	date-added = {2022-04-15 13:13:26 -0700},
	date-modified = {2022-04-15 13:13:26 -0700},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	pages = {e1545},
	publisher = {Wiley Online Library},
	title = {Ordinal regression: A review and a taxonomy of models},
	year = {2020}}

@article{peterson1990partial,
	author = {Peterson, Bercedis and Harrell Jr, Frank E},
	date-added = {2022-04-11 10:57:14 -0700},
	date-modified = {2022-04-11 10:57:14 -0700},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	number = {2},
	pages = {205--217},
	publisher = {Wiley Online Library},
	title = {Partial proportional odds models for ordinal response variables},
	volume = {39},
	year = {1990}}

@article{mccullagh1980regression,
	author = {McCullagh, Peter},
	date-added = {2022-04-11 10:46:16 -0700},
	date-modified = {2022-04-11 10:46:16 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {2},
	pages = {109--127},
	publisher = {Wiley Online Library},
	title = {Regression models for ordinal data},
	volume = {42},
	year = {1980}}

@article{armstrong1989ordinal,
	author = {Armstrong, Ben G and Sloan, Margaret},
	date-added = {2022-04-05 17:20:05 -0700},
	date-modified = {2022-04-05 17:20:05 -0700},
	journal = {American Journal of Epidemiology},
	number = {1},
	pages = {191--204},
	publisher = {Oxford University Press},
	title = {Ordinal regression models for epidemiologic data},
	volume = {129},
	year = {1989}}

@article{brant1990assessing,
	author = {Brant, Rollin},
	date-added = {2022-04-05 17:10:51 -0700},
	date-modified = {2022-04-05 17:10:51 -0700},
	journal = {Biometrics},
	pages = {1171--1178},
	publisher = {JSTOR},
	title = {Assessing proportionality in the proportional odds model for ordinal logistic regression},
	year = {1990}}

@book{Greene2012Econometric,
	author = {Greene, William H.},
	date-added = {2022-04-05 16:30:29 -0700},
	date-modified = {2022-04-05 16:30:29 -0700},
	edition = {7th},
	publisher = {Pearson Education},
	title = {Econometric Analysis},
	year = 2012,
	bdsk-url-1 = {http://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm}}

@article{adhikari2019high,
	author = {Adhikari, Samrachana and Lecci, Fabrizio and Becker, James T and Junker, Brian W and Kuller, Lewis H and Lopez, Oscar L and Tibshirani, Ryan J},
	date-added = {2022-04-05 15:59:09 -0700},
	date-modified = {2022-04-05 15:59:09 -0700},
	journal = {Statistics in medicine},
	number = {12},
	pages = {2184--2205},
	publisher = {Wiley Online Library},
	title = {High-dimensional longitudinal classification with the multinomial fused lasso},
	volume = {38},
	year = {2019}}

@article{tibshirani2011solution,
	author = {Tibshirani, Ryan J and Taylor, Jonathan},
	date-added = {2022-04-05 12:22:52 -0700},
	date-modified = {2022-04-05 12:22:52 -0700},
	journal = {The annals of statistics},
	number = {3},
	pages = {1335--1371},
	publisher = {Institute of Mathematical Statistics},
	title = {The solution path of the generalized lasso},
	volume = {39},
	year = {2011}}

@article{arnold2016efficient,
	author = {Arnold, Taylor B and Tibshirani, Ryan J},
	date-added = {2022-04-05 12:22:26 -0700},
	date-modified = {2022-04-05 12:22:26 -0700},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {1--27},
	publisher = {Taylor \& Francis},
	title = {Efficient implementations of the generalized lasso dual path algorithm},
	volume = {25},
	year = {2016}}

@article{hastie2015statistical,
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	date-added = {2022-04-05 12:18:01 -0700},
	date-modified = {2022-04-05 12:18:01 -0700},
	journal = {Monographs on statistics and applied probability},
	pages = {143},
	title = {Statistical learning with sparsity},
	volume = {143},
	year = {2015}}

@article{tibshirani2005sparsity,
	author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and Knight, Keith},
	date-added = {2022-04-05 11:45:02 -0700},
	date-modified = {2022-04-05 11:45:02 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {1},
	pages = {91--108},
	publisher = {Wiley Online Library},
	title = {Sparsity and smoothness via the fused lasso},
	volume = {67},
	year = {2005}}

@article{possnecker2016general,
	author = {P{\"o}{\ss}necker, Wolfgang and Tutz, Gerhard},
	date-added = {2022-04-04 13:18:47 -0700},
	date-modified = {2022-04-04 13:18:47 -0700},
	title = {A general framework for the selection of effect type in ordinal regression},
	year = {2016}}

@article{agresti2016categorical,
	author = {Agresti, Alan},
	date-added = {2022-04-04 12:43:04 -0700},
	date-modified = {2022-04-04 12:43:04 -0700},
	journal = {Statistical Modelling},
	number = {3},
	pages = {201--204},
	publisher = {SAGE Publications Sage India: New Delhi, India},
	title = {Categorical regularization: Discussion of article by Tutz and Gertheiss},
	volume = {16},
	year = {2016}}

@article{ugba2021serp,
	author = {Ugba, Ejike R},
	date-added = {2022-04-04 12:06:49 -0700},
	date-modified = {2022-04-04 12:06:49 -0700},
	journal = {Journal of Open Source Software},
	number = {66},
	pages = {3705},
	title = {serp: An R package for smoothing in ordinal regression},
	volume = {6},
	year = {2021}}

@article{ugba2021smoothing,
	author = {Ugba, Ejike R and M{\"o}rlein, Daniel and Gertheiss, Jan},
	date-added = {2022-04-04 12:04:52 -0700},
	date-modified = {2022-04-04 12:04:52 -0700},
	journal = {Stats},
	number = {3},
	pages = {616--633},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Smoothing in ordinal regression: An application to sensory data},
	volume = {4},
	year = {2021}}

@article{stokell2021modelling,
	author = {Stokell, Benjamin G and Shah, Rajen D and Tibshirani, Ryan J},
	date-added = {2022-04-04 11:49:19 -0700},
	date-modified = {2022-04-04 11:49:19 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {3},
	pages = {579--611},
	publisher = {Wiley Online Library},
	title = {Modelling high-dimensional categorical data using nonconvex fusion penalties},
	volume = {83},
	year = {2021}}

@article{wurm2017regularized,
	author = {Wurm, Michael J and Rathouz, Paul J and Hanlon, Bret M},
	date-added = {2022-04-04 11:34:41 -0700},
	date-modified = {2022-04-04 11:34:41 -0700},
	journal = {arXiv preprint arXiv:1706.05003},
	title = {Regularized ordinal regression and the ordinalNet R package},
	year = {2017}}

@article{gertheiss2010sparse,
	author = {Gertheiss, Jan and Tutz, Gerhard},
	date-added = {2022-04-04 11:24:20 -0700},
	date-modified = {2022-04-04 11:24:20 -0700},
	journal = {The Annals of Applied Statistics},
	number = {4},
	pages = {2150--2180},
	publisher = {Institute of Mathematical Statistics},
	title = {Sparse modeling of categorial explanatory variables},
	volume = {4},
	year = {2010}}

@article{tutz2014rating,
	author = {Tutz, Gerhard and Gertheiss, Jan},
	date-added = {2022-04-04 11:23:34 -0700},
	date-modified = {2022-04-04 11:23:34 -0700},
	journal = {Psychometrika},
	number = {3},
	pages = {357--376},
	publisher = {Springer},
	title = {Rating scales as predictors---The old question of scale level and some answers},
	volume = {79},
	year = {2014}}

@article{hoshiyar2021ordpens,
	author = {Hoshiyar, Aisouda},
	date-added = {2022-04-04 11:22:26 -0700},
	date-modified = {2022-04-04 11:22:26 -0700},
	journal = {Journal of Open Source Software},
	number = {68},
	pages = {3828},
	title = {``ordPens``: An R package for Selection, Smoothing and Principal Components Analysis for Ordinal Variables},
	volume = {6},
	year = {2021}}

@article{tutz2016regularized,
	author = {Tutz, Gerhard and Gertheiss, Jan},
	date-added = {2022-04-04 11:06:34 -0700},
	date-modified = {2022-04-04 11:06:34 -0700},
	journal = {Statistical Modelling},
	number = {3},
	pages = {161--200},
	publisher = {Sage Publications Sage India: New Delhi, India},
	title = {Regularized regression for categorical data},
	volume = {16},
	year = {2016}}

@article{gertheiss2009penalized,
	author = {Gertheiss, Jan and Tutz, Gerhard},
	date-added = {2022-04-04 10:47:44 -0700},
	date-modified = {2022-04-04 10:47:44 -0700},
	journal = {International Statistical Review},
	number = {3},
	pages = {345--365},
	publisher = {Wiley Online Library},
	title = {Penalized regression with ordinal predictors},
	volume = {77},
	year = {2009}}

@book{van2000asymptotic,
	author = {van der Vaart, A.W.},
	date-added = {2022-01-12 14:43:43 -0800},
	date-modified = {2022-01-12 14:43:43 -0800},
	isbn = {9780521784504},
	lccn = {98015176},
	publisher = {Cambridge University Press},
	series = {Asymptotic Statistics},
	title = {Asymptotic Statistics},
	url = {https://books.google.com/books?id=UEuQEM5RjWgC},
	year = {2000},
	bdsk-url-1 = {https://books.google.com/books?id=UEuQEM5RjWgC}}

@article{Peng2022,
	author = {Peng, Wei and Coleman, Tim and Mentch, Lucas},
	date-added = {2022-01-12 11:46:33 -0800},
	date-modified = {2022-01-12 11:47:35 -0800},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Peng, Coleman, Mentch - 2022 - Rates of convergence for random forests.pdf:pdf},
	journal = {Electronic Journal of Statistics},
	keywords = {60F05Berry-Esseen,62E17,62E20,U,and phrases,bag-,berry-esseen,random forests,u-statistics},
	number = {1},
	pages = {232--292},
	title = {{Rates of convergence for random forests via generalized U-statistics}},
	volume = {16},
	year = {2022}}

@book{romano2019multiple,
	author = {Romano, Joseph P and DiCiccio, Cyrus},
	date-added = {2022-01-11 12:04:41 -0800},
	date-modified = {2022-01-11 12:04:41 -0800},
	publisher = {Department of Statistics, Stanford University},
	title = {Multiple data splitting for testing},
	year = {2019}}

@book{vershynin2018high,
	author = {Vershynin, R.},
	date-added = {2022-01-10 18:10:06 -0800},
	date-modified = {2022-01-10 18:10:06 -0800},
	isbn = {9781108415194},
	lccn = {2018016910},
	publisher = {Cambridge University Press},
	series = {Cambridge Series in Statistical and Probabilistic Mathematics},
	title = {High-Dimensional Probability: An Introduction with Applications in Data Science},
	url = {https://books.google.com/books?id=J-VjswEACAAJ},
	year = {2018},
	bdsk-url-1 = {https://books.google.com/books?id=J-VjswEACAAJ}}

@article{Buhlmann2020,
	abstract = {We discuss recent work for causal inference and predictive robustness in a unifying way. The key idea relies on a notion of probabilistic invariance or stability: it opens up new insights for formulating causality as a certain risk minimization problem with a corresponding notion of robustness. The invariance itself can be estimated from general heterogeneous or perturbation data which frequently occur with nowadays data collection. The novel methodology is potentially useful in many applications, offering more robustness and better ``causal-oriented'' interpretation than machine learning or estimation in standard regression or classification frameworks.},
	author = {B{\"{u}}hlmann, Peter},
	date-added = {2021-12-17 13:52:20 -0800},
	date-modified = {2021-12-17 13:52:20 -0800},
	doi = {10.1214/19-STS721},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{u}}hlmann - 2020 - Invariance, Causality and Robustness 2018 Neyman Lecture.pdf:pdf},
	issn = {21688745},
	journal = {Statistical Science},
	keywords = {Anchor regression,Random Forests,causal regularization,distributional robustness,heterogeneous data,instrumental variables regression,interventional data,variable importance},
	number = {3},
	pages = {404--426},
	title = {{Invariance, Causality and Robustness: 2018 Neyman Lecture}},
	volume = {35},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1214/19-STS721}}

@article{friedberg2018local,
	author = {Friedberg, Rina and Tibshirani, Julie and Athey, Susan and Wager, Stefan},
	date-added = {2021-12-17 13:51:48 -0800},
	date-modified = {2021-12-17 13:51:48 -0800},
	journal = {arXiv preprint arXiv:1807.11408},
	title = {Local linear forests},
	year = {2018}}

@article{Imbens1994,
	abstract = {RANDOM ASSIGNMENT OF TREATMENT and concurrent data collection on treatment and control groups is the norm in medical evaluation research. In contrast, the use of random assignment to evaluate social programs remains controversial. Following criticism of parametric evaluation models (e.g., Lalonde (1986)), econometric research has been geared towards establishing conditions that guarantee nonparametric identification of treatment effects in observational studies, i.e. identification without relying on functional form restrictions or distributional assumptions. The focus has been on identification of average treatment effects in a population of interest, or on the average effect for the subpopulation that is treated. The conditions required to nonparametrically identify these parameters can be restrictive, however, and the derived identification results fragile. In particular, results in Chamberlain (1986), Manski (1990), Heckman (1990), and Angrist and Imbens (1991) require that there be some subpopulation for whom the probability of treatment is zero, at least in the limit. The purpose of this paper is to show that even when there is no subpopulation available for whom the probability of treatment is zero, we can still identify an average treatment effect of interest under mild restrictions satisfied in a wide range of models and circumstances. We call this a local average treatment effect (LATE). Examples of problems where the local average treatment effect is identified include latent index models and evaluations based on natural experiments such as those studied by Angrist (1990) and Angrist and Krueger (1991). LATE is the average treatment effect for individuals whose treatment status is influenced by changing an exogenous regressor that satisfies an exclusion restriction.},
	author = {Imbens, Guido W and Angrist, Joshua D},
	date-added = {2021-12-17 13:51:10 -0800},
	date-modified = {2021-12-17 13:51:10 -0800},
	doi = {10.2307/2951620},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Imbens, Angrist - 1994 - Identification and Estimation of Local Average Treatment Effects.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	number = {2},
	pages = {467},
	title = {{Identification and Estimation of Local Average Treatment Effects}},
	volume = {62},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.2307/2951620}}

@inproceedings{lejeune2020implicit,
	author = {LeJeune, Daniel and Javadi, Hamid and Baraniuk, Richard},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-12-17 13:39:24 -0800},
	date-modified = {2021-12-17 13:39:24 -0800},
	organization = {PMLR},
	pages = {3525--3535},
	title = {The implicit regularization of ordinary least squares ensembles},
	year = {2020}}

@article{Rubin1981,
	author = {Rubin, Donald B.},
	date-added = {2021-12-01 17:00:18 -0800},
	date-modified = {2021-12-01 17:00:18 -0800},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Rubin - 1981 - The Bayesian Bootstrap.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {business applications},
	mendeley-tags = {business applications},
	number = {1},
	pages = {130 -- 134},
	title = {{The Bayesian Bootstrap}},
	url = {https://projecteuclid-org.libproxy2.usc.edu/download/pdf_1/euclid.aos/1176345338},
	volume = {9},
	year = {1981},
	bdsk-url-1 = {https://projecteuclid-org.libproxy2.usc.edu/download/pdf_1/euclid.aos/1176345338}}

@unpublished{Bloesch2021,
	author = {Bloesch, Justin and Larsen, Birthe and Taska, Bledi},
	date-added = {2021-11-29 10:11:12 -0800},
	date-modified = {2021-11-29 10:11:12 -0800},
	file = {:Users/gregfaletto/Dropbox/Subspace Stability Selection/sims_6/211129/bloesch_jmp_larsen_taska_position_specificity_holdup_power.pdf:pdf},
	title = {{Which Workers Earn More at Productive Firms? Position Specific Skills and Individual Worker Hold-up Power}},
	url = {https://scholar.harvard.edu/files/bloesch/files/bloesch_jmp_larsen_taska_position_specificity_holdup_power.pdf},
	year = {2021},
	bdsk-url-1 = {https://scholar.harvard.edu/files/bloesch/files/bloesch_jmp_larsen_taska_position_specificity_holdup_power.pdf}}

@inproceedings{Schupbach2020,
	abstract = {Quantifying uncertainty is critically important to many applications of predictive modeling. In this paper we apply a recently developed method that uses U-statistics as a basis for estimating uncertainty in ensemble regressors to the case of neural network ensembles. U-statistics generalize the notion of a sample mean and provide distributional properties to estimates obtained by ensembles of estimators. With this method, we train neural networks on subsamples of the data and use the resulting ensemble to estimate the variance of the point estimates from the ensemble. We demonstrate that neural networks predicting a regression function exhibit the required theoretical properties for use in this ensemble method, and we then perform a coverage probability study of three simulated data sets to show that the empirical coverage probabilities match the theoretical values.},
	author = {Schupbach, Jordan and Sheppard, John W and Forrester, Tyler},
	booktitle = {Proceedings of the International Joint Conference on Neural Networks},
	date-added = {2021-11-28 17:08:44 -0800},
	date-modified = {2021-11-28 17:08:44 -0800},
	doi = {10.1109/IJCNN48605.2020.9206810},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Schupbach, Sheppard, Forrester - Unknown - Quantifying Uncertainty in Neural Network Ensembles using U-Statistics.pdf:pdf},
	isbn = {9781728169262},
	title = {{Quantifying Uncertainty in Neural Network Ensembles using U-Statistics}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/IJCNN48605.2020.9206810}}

@article{Chen2019,
	abstract = {This paper studies inference for the mean vector of a high-dimensional U-statistic. In the era of big data, the dimension d of the U-statistic and the sample size n of the observations tend to be both large, and the computation of the U-statistic is prohibitively demanding. Data-dependent inferential procedures such as the empirical bootstrap for U-statistics is even more computationally expensive. To overcome such a computational bottleneck, incomplete U-statistics obtained by sampling fewer terms of the U-statistic are attractive alternatives. In this paper, we introduce randomized incomplete U-statistics with sparse weights whose computational cost can be made independent of the order of the U-statistic.We derive nonasymptotic Gaussian approximation error bounds for the randomized incomplete U-statistics in high dimensions, namely in cases where the dimension d is possibly much larger than the sample size n, for both nondegenerate and degenerate kernels. In addition, we propose generic bootstrap methods for the incomplete U-statistics that are computationally much less demanding than existing bootstrap methods, and establish finite sample validity of the proposed bootstrap methods. Our methods are illustrated on the application to nonparametric testing for the pairwise independence of a high-dimensional random vector under weaker assumptions than those appearing in the literature.},
	archiveprefix = {arXiv},
	arxivid = {1712.00771},
	author = {Chen, Xiaohui and Kato, Kengo},
	date-added = {2021-11-28 17:08:21 -0800},
	date-modified = {2021-11-28 17:08:21 -0800},
	doi = {10.1214/18-aos1773},
	eprint = {1712.00771},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Kato - Unknown - RANDOMIZED INCOMPLETE U-STATISTICS IN HIGH DIMENSIONS.pdf:pdf},
	issn = {21688966},
	journal = {Annals of Statistics},
	keywords = {Bernoulli sampling,Bootstrap,Divide and conquer,Gaussian approximation,Incomplete U-statistics,Randomized inference,Sampling with replacement},
	number = {6},
	pages = {3127--3156},
	title = {{Randomized incomplete u-statistics in high dimensions}},
	volume = {47},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-aos1773}}

@article{Song2019,
	abstract = {We study the problem of distributional approximations to high-dimensional non-degenerate U-statistics with random kernels of diverging orders. Infinite-order U-statistics (IOUS) are a useful tool for constructing simultaneous prediction intervals that quantify the uncertainty of ensemble methods such as subbagging and random forests. A major obstacle in using the IOUS is their computational intractability when the sample size and/or order are large. In this article, we derive non-asymptotic Gaussian approximation error bounds for an incomplete version of the IOUS with a random kernel. We also study data-driven inferential methods for the incomplete IOUS via bootstraps and develop their statistical and computational guarantees.},
	author = {Song, Yanglei and Chen, Xiaohui and Kato, Kengo},
	date-added = {2021-11-28 17:07:45 -0800},
	date-modified = {2021-11-28 17:07:45 -0800},
	doi = {10.1214/19-EJS1643},
	file = {:Users/gregfaletto/Desktop/19-EJS1643.pdf:pdf},
	issn = {19357524},
	journal = {Electronic Journal of Statistics},
	keywords = {Bootstrap,Gaussian approximation,Incomplete U statistics,Infinite-order U-statistics,Random forests,Uncertainty quantification},
	number = {2},
	pages = {4794--4848},
	title = {{Approximating high-dimensional infinite-order U-statistics: Statistical and computational guarantees}},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/19-EJS1643}}

@article{Athey2019,
	abstract = {We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5--32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
	archiveprefix = {arXiv},
	arxivid = {1610.01271},
	author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
	date-added = {2021-11-26 17:45:02 -0800},
	date-modified = {2021-11-26 17:45:02 -0800},
	doi = {10.1214/18-AOS1709},
	eprint = {1610.01271},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Athey, Tibshirani, Wager - 2019 - Generalized random forests.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {And phrases,Asymptotic theory,Causal inference,Instrumental variable},
	number = {2},
	pages = {1179--1203},
	title = {{Generalized random forests}},
	volume = {47},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-AOS1709}}

@article{Wager2018,
	abstract = {Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimat- ing heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a prac- tical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowl- edge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	annote = {From presentation on 04/15 in DSO 607},
	archiveprefix = {arXiv},
	arxivid = {arXiv:1510.04342v2},
	author = {Wager, Stefan and Athey, Susan},
	date-added = {2021-11-26 17:41:42 -0800},
	date-modified = {2021-11-26 17:41:42 -0800},
	doi = {10.1080/01621459.2017.1319839},
	eprint = {arXiv:1510.04342v2},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Wager, Athey - 2015 - Estimation and Inference of Heterogeneous Treatment Effects using Random Forests ∗ arXiv 1510 . 04342v1 stat ..pdf:pdf},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	keywords = {DSO 607,DSO 607 Week 12,adaptive nearest neighbors matching,asymptotic normality,business applications,causal inference,outcomes,potential,unconfoundedness},
	mendeley-tags = {DSO 607,DSO 607 Week 12,business applications,causal inference},
	month = {jul},
	number = {523},
	pages = {1228--1242},
	title = {{Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839},
	volume = {113},
	year = {2018},
	bdsk-url-1 = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.2017.1319839}}

@article{Mentch2016,
	abstract = {This work develops formal statistical inference procedures for predictions generated by supervised learning ensembles. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real data set are provided.},
	archiveprefix = {arXiv},
	arxivid = {1404.6473},
	author = {Mentch, Lucas and Hooker, Giles},
	date-added = {2021-11-26 17:35:59 -0800},
	date-modified = {2021-11-26 17:35:59 -0800},
	eprint = {1404.6473},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Mentch, Hooker - 2016 - Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests.pdf:pdf},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Bagging,Random forests,Subbagging,Trees,U-statistics},
	pages = {1--41},
	title = {{Quantifying uncertainty in random forests via confidence intervals and hypothesis tests}},
	volume = {17},
	year = {2016}}

@article{Frees1989,
	author = {Frees, Edward W.},
	date-added = {2021-11-26 17:23:49 -0800},
	date-modified = {2021-11-26 17:23:49 -0800},
	file = {:Users/gregfaletto/Google Drive/Data Science/LaTeX/Stability Proportion Distribution/Frees Infinite order U-statistics.pdf:pdf},
	journal = {Scandinavian Journal of Statistics},
	number = {1},
	pages = {29--45},
	title = {{Infinite Order U-Statistics}},
	url = {https://www.jstor.org/stable/4616120},
	volume = {16},
	year = {1989},
	bdsk-url-1 = {https://www.jstor.org/stable/4616120}}

@article{Ravikumar2010,
	abstract = {We consider the problem of estimating the graph associated with a binary Ising Markov random field.We describe a method based on ℓ1-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an ℓ1-constraint. The method is analyzed under high-dimensional scaling in which both the number of nodes p and maximum neighborhood size d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n,p, d) and the model parameters for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. With coherence conditions imposed on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n = $\Omega$(d3 log p) with exponentially decaying error. When these same conditions are imposed directly on the sample matrices, we show that a reduced sample size of n = $\Omega$(d2 log p) suffices for the method to estimate neighborhoods consistently. Although this paper focuses on the binary graphical models, we indicate how a generalization of the method of the paper would apply to general discrete Markov random fields. {\textcopyright} 2010 Institute of Mathematical Statistics.},
	author = {Ravikumar, Pradeep and Wainwright, Martin J and Lafferty, John D},
	date-added = {2021-11-26 16:30:13 -0800},
	date-modified = {2021-11-26 16:31:16 -0800},
	doi = {10.1214/09-AOS691},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Ravikumar, Wainwright, Lafferty - 2010 - High-dimensional ising model selection using ℓ1-regularized logistic regression.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Convex risk minimization,Graphical models,High-dimensional asymptotics,Markov random fields,Model selection,Structure learning,ℓ1-regularization},
	number = {3},
	pages = {1287--1319},
	title = {{High-dimensional ising model selection using L1-regularized logistic regression}},
	volume = {38},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1214/09-AOS691}}

@book{pesaran-2015-text,
	abstract = {This book is concerned with recent developments in time series and panel data techniques for the analysis of macroeconomic and financial data. It provides a rigorous, nevertheless user-friendly, account of the time series techniques dealing with univariate and multivariate time series models, as well as panel data models. It is distinct from other time series texts in the sense that it also covers panel data models and attempts at a more coherent integration of time series, multivariate analysis, and panel data models. It builds on the author's extensive research in the areas of time series and panel data analysis and covers a wide variety of topics in one volume. Different parts of the book can be used as teaching material for a variety of courses in econometrics. It can also be used as reference manual. It begins with an overview of basic econometric and statistical techniques, and provides an account of stochastic processes, univariate and multivariate time series, tests for unit roots, cointegration, impulse response analysis, autoregressive conditional heteroskedasticity models, simultaneous equation models, vector autoregressions, causality, forecasting, multivariate volatility models, panel data models, aggregation and global vector autoregressive models (GVAR). The techniques are illustrated using Microfit 5 (Pesaran and Pesaran, 2009, OUP) with applications to real output, inflation, interest rates, exchange rates, and stock prices.},
	author = {Pesaran, M. Hashem},
	date-added = {2019-04-17 13:51:50 +0000},
	date-modified = {2019-04-17 13:52:07 +0000},
	isbn = {ARRAY(0x3bdaaf68)},
	number = {9780198759980},
	publisher = {Oxford University Press},
	series = {OUP Catalogue},
	title = {{Time Series and Panel Data Econometrics}},
	url = {https://ideas.repec.org/b/oxp/obooks/9780198759980.html},
	year = 2015,
	bdsk-url-1 = {https://ideas.repec.org/b/oxp/obooks/9780198759980.html}}

@article{Bien,
	author = {Bien, Jacob and Wegkamp, Marten},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Wegkamp - Unknown - Discussion of ``correlated variables in regression clustering and sparse estimation''.pdf:pdf},
	journal = {Journal of Statistical Planning and Inference},
	number = {11},
	pages = {1--7},
	title = {{Discussion of ``correlated variables in regression: clustering and sparse estimation''}},
	volume = {143},
	year = {2013}}

@article{Bien2011,
	abstract = {Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage-that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces.},
	author = {Bien, Jacob and Tibshirani, Robert},
	doi = {10.1198/jasa.2011.tm10183},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - Hierarchical Clustering With Prototypes via Minimax Linkage.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	pages = {1075--1084},
	title = {{Hierarchical Clustering With Prototypes via Minimax Linkage}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	volume = {106},
	year = {2011},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	bdsk-url-2 = {https://doi.org/10.1198/jasa.2011.tm10183}}

@article{Lim2016,
	abstract = {Cross-validation (CV) is often used to select the regularization parameter in high-dimensional problems. However, when applied to the sparse modeling method Lasso, CV leads to models that are unstable in high-dimensions, and consequently not suited for reliable interpretation. In this article, we propose a model-free criterion ESCV based on a new estimation stability (ES) metric and CV. Our proposed ESCV finds a smaller and locally ES-optimal model smaller than the CV choice so that it fits the data and also enjoys estimation stability property. We demonstrate that ESCV is an effective alternative to CV at a similar easily parallelizable computational cost. In particular, we compare the two approaches with respect to several performance measures when applied to the Lasso on both simulated and real datasets. For dependent predictors common in practice, our main finding is that ESCV cuts down false positive rates often by a large margin, while sacrificing little of true positive rates. ESCV usually outperforms CV in terms of parameter estimation while giving similar performance as CV in terms of prediction. For the two real datasets from neuroscience and cell biology, the models found by ESCV are less than half of the model sizes by CV, but preserves CV's predictive performance and corroborates with subject knowledge and independent work. We also discuss some regularization parameter alignment issues that come up in both approaches. Supplementary materials are available online.},
	author = {Lim, Chinghway and Yu, Bin},
	doi = {10.1080/10618600.2015.1020159},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Lim, Yu - 2016 - Estimation Stability With Cross-Validation (ESCV).pdf:pdf},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {2},
	pages = {464--492},
	title = {{Estimation Stability With Cross-Validation (ESCV)}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	volume = {25},
	year = {2016},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	bdsk-url-2 = {https://doi.org/10.1080/10618600.2015.1020159}}

@article{Xue2017,
	abstract = {Penalty-based variable selection methods are powerful in selecting relevant covariates and estimating coefficients simultaneously. However, variable selection could fail to be consistent when covariates are highly correlated. The partial correlation approach has been adopted to solve the problem with correlated covariates. Nevertheless, the restrictive range of partial correlation is not effective for capturing signal strength for relevant covariates. In this paper, we propose a new Semi-standard PArtial Covariance (SPAC) which is able to reduce correlation effects from other predictors while incorporating the magnitude of coefficients. The proposed SPAC variable selection facilitates choosing covariates which have direct association with the response variable, via utilizing dependency among covariates. We show that the proposed method with the Lasso penalty (SPAC-Lasso) enjoys strong sign consistency in both finite-dimensional and high-dimensional settings under regularity conditions. Simulation studies and the `HapMap' gene data application show that the proposed method outperforms the traditional Lasso, adaptive Lasso, SCAD, and Peter-Clark-simple (PC-simple) methods for highly correlated predictors.},
	archiveprefix = {arXiv},
	arxivid = {1709.04840},
	author = {Xue, Fei and Qu, Annie},
	eprint = {1709.04840},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Xue, Qu - 2017 - Variable Selection for Highly Correlated Predictors.pdf:pdf},
	keywords = {Read for Jacob,feature selection,irrepresentable condition,lasso,model selection consistency,partial correlation,scad},
	mendeley-tags = {feature selection,Read for Jacob},
	title = {{Variable Selection for Highly Correlated Predictors}},
	url = {http://arxiv.org/abs/1709.04840},
	volume = {61820},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1709.04840}}

@article{GSell2016,
	abstract = {We consider a multiple hypothesis testing setting where the hypotheses are ordered and one is only permitted to reject an initial contiguous block, H{\_}1,$\backslash$dots,H{\_}k, of hypotheses. A rejection rule in this setting amounts to a procedure for choosing the stopping point k. This setting is inspired by the sequential nature of many model selection problems, where choosing a stopping point or a model is equivalent to rejecting all hypotheses up to that point and none thereafter. We propose two new testing procedures, and prove that they control the false discovery rate in the ordered testing setting. We also show how the methods can be applied to model selection using recent results on p-values in sequential model selection settings.},
	archiveprefix = {arXiv},
	arxivid = {1309.5352v2},
	author = {G'Sell, Max Grazier and Wager, Stefan and Chouldechova, Alexandra and Tibshirani, Robert},
	doi = {10.1111/rssb.12122},
	eprint = {1309.5352v2},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/G'Sell et al. - 2016 - Sequential selection procedures and false discovery rate control.pdf:pdf},
	issn = {14679868},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	keywords = {False discovery rate,Multiple-hypothesis testing,Read for Jacob,Sequential testing,Stopping rule,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Sequential selection procedures and false discovery rate control}},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1111/rssb.12122}}

@inproceedings{Sanchez2007,
	abstract = {Adequate selection of features may improve accuracy and efficiency of classifier methods. There are two main approaches for feature selection: wrapper methods, in which the features are selected using the classifier, and filter methods, in which the selection of features is independent of the classifier used. Although the wrapper approach may obtain better performances, it requires greater computational resources. For this reason, lately a new paradigm, hybrid approach, that combines both filter and wrapper methods has emerged. One of its problems is to select the filter method that gives the best relevance index for each case, and this is not an easy to solve question. Different approaches to relevance evaluation lead to a large number of indices for ranking and selection. In this paper, several filter methods are applied over artificial data sets with different number of relevant features, level of noise in the output, interaction between features and increasing number of samples. The results obtained for the four filters studied (ReliefF, Correlation-based Feature Selection, Fast Correlated Based Filter and INTERACT) are compared and discussed. The final aim of this study is to select a filter to construct a hybrid method for feature selection.},
	address = {Berlin, Heidelberg},
	author = {S{\'a}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo and Tombilla-Sanrom{\'a}n, Mar{\'\i}a},
	booktitle = {Intelligent Data Engineering and Automated Learning - IDEAL 2007},
	editor = {Yin, Hujun and Tino, Peter and Corchado, Emilio and Byrne, Will and Yao, Xin},
	isbn = {978-3-540-77226-2},
	pages = {178--187},
	publisher = {Springer Berlin Heidelberg},
	title = {Filter Methods for Feature Selection -- A Comparative Study},
	year = {2007}}

@techreport{Reid2015,
	abstract = {We propose a new approach for sparse regression and marginal testing, for data with correlated features. Our procedure first clusters the features, and then chooses as the cluster prototype the most informative feature in that cluster. Then we apply either sparse regression (lasso) or marginal significance testing to these prototypes. While this kind of strategy is not entirely new, a key feature of our proposal is its use of the post-selection inference theory of Taylor et al. (2014) and Lee et al. (2014) to compute exact p-values and confidence intervals that properly account for the selection of prototypes. We also apply the recent "knockoff" idea of Barber {\&} Cand{\`{e}}s (2014) to provide exact finite sample control of the FDR of our regression procedure. We illustrate our proposals on both real and simulated data.},
	archiveprefix = {arXiv},
	arxivid = {1503.00334v2},
	author = {Reid, Stephen and Tibshirani, Robert},
	eprint = {1503.00334v2},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Reid, Tibshirani - Unknown - Sparse regression and marginal testing using cluster prototypes.pdf:pdf},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Sparse regression and marginal testing using cluster prototypes}},
	url = {https://arxiv.org/pdf/1503.00334.pdf},
	year = {2015},
	bdsk-url-1 = {https://arxiv.org/pdf/1503.00334.pdf}}

@misc{Lumley2017,
	author = {Lumley, Thomas},
	keywords = {feature selection},
	mendeley-tags = {feature selection},
	title = {{Fixing an infelicity in leaps}},
	url = {https://notstatschat.rbind.io/2017/01/09/fixing-an-infelicity-inleaps/},
	urldate = {2019-03-31},
	year = {2017},
	bdsk-url-1 = {https://notstatschat.rbind.io/2017/01/09/fixing-an-infelicity-inleaps/}}

@techreport{Jacob2009,
	abstract = {We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.},
	author = {Jacob, Laurent and Obozinski, Guillaume and {Vert JEAN-PHILIPPEVERT}, Jean-Philippe},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Jacob, Obozinski, Vert JEAN-PHILIPPEVERT - 2009 - Group Lasso with Overlap and Graph Lasso.pdf:pdf},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Group Lasso with Overlap and Graph Lasso}},
	url = {http://delivery.acm.org.libproxy2.usc.edu/10.1145/1560000/1553431/p433-jacob.pdf?ip=154.59.124.74{\&}id=1553431{\&}acc=ACTIVE SERVICE{\&}key=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1554064234{\_}9d18459636a12f46fc34ccb79ecf},
	year = {2009},
	bdsk-url-1 = {http://delivery.acm.org.libproxy2.usc.edu/10.1145/1560000/1553431/p433-jacob.pdf?ip=154.59.124.74%7B%5C&%7Did=1553431%7B%5C&%7Dacc=ACTIVE%20SERVICE%7B%5C&%7Dkey=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.4D4702B0C3E38B35%7B%5C&%7D%7B%5C_%7D%7B%5C_%7Dacm%7B%5C_%7D%7B%5C_%7D=1554064234%7B%5C_%7D9d18459636a12f46fc34ccb79ecf}}

@unpublished{Gong2019,
	abstract = {In variable selection, most existing screening methods focus on marginal effects and ignore dependence between covariates. To improve the performance of selection, we incorporate pairwise effects in covariates for screening and penalization. We achieve this by studying the asymptotic distribution of the maximal absolute pairwise sample correlation among independent covariates. The novelty of the theory is in that the convergence is with respect to the dimensionality p, and is uniform with respect to the sample size n. Moreover, we obtain an upper bound for the maximal pairwise R squared when regressing the response onto two different covariates. Based on these extreme value results, we propose a screening procedure to detect covariates pairs that are potentially correlated and associated with the response. We further combine the pairwise screening with Sure Independence Screening [Fan and Lv, 2008] and develop a new regularized variable selection procedure. Numerical studies show that our method is very competitive in terms of both prediction accuracy and variable selection accuracy.},
	archiveprefix = {arXiv},
	arxivid = {1902.03308v1},
	author = {Gong, Siliang and Zhang, Kai and Liu, Yufeng},
	date-added = {2019-02-13 20:40:16 +0000},
	date-modified = {2019-02-13 20:40:16 +0000},
	eprint = {1902.03308v1},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Gong, Zhang, Liu - Unknown - Penalized linear regression with high-dimensional pairwise screening.pdf:pdf},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Penalized linear regression with high-dimensional pairwise screening}},
	url = {https://arxiv.org/pdf/1902.03308.pdf},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/pdf/1902.03308.pdf}}

@unpublished{Dong2019,
	abstract = {Variable importance is central to scientific studies, including the social sciences and causal inference, healthcare, and in other domains. However, current notions of variable importance are often tied to a specific predictive model. This is problematic: what if there were multiple well-performing predictive models, and a specific variable is important to some of them and not to others? In that case, we may not be able to tell from a single well-performing model whether a variable is always important in predicting the outcome. Rather than depending on variable importance for a single predictive model, we would like to explore variable importance for all approximately-equally-accurate predictive models. This work introduces the concept of a variable importance cloud, which maps every variable to its importance for every good predictive model. We show properties of the variable importance cloud and draw connections other areas of statistics. We introduce variable importance diagrams as a projection of the variable importance cloud into two dimensions for visualization purposes. Experiments with criminal justice and marketing data illustrate how variables can change dramatically in importance for approximately-equally-accurate predictive models.},
	archiveprefix = {arXiv},
	arxivid = {1901.03209},
	author = {Dong, Jiayun and Rudin, Cynthia},
	date-added = {2019-02-13 20:37:32 +0000},
	date-modified = {2019-02-13 20:37:32 +0000},
	eprint = {1901.03209},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Dong, Rudin - 2019 - Variable Importance Clouds A Way to Explore Variable Importance for the Set of Good Models.pdf:pdf},
	keywords = {Read for Jacob,algorithmic fairness,feature selection},
	mendeley-tags = {Read for Jacob,algorithmic fairness,feature selection},
	title = {{Variable Importance Clouds: A Way to Explore Variable Importance for the Set of Good Models}},
	year = {2019}}

@article{Li2018,
	abstract = {Sparse models for high-dimensional linear regression and machine learning have received substantial attention over the past two decades. Model selection, or determining which features or covariates are the best explanatory variables, is critical to the interpretability of a learned model. Much of the current literature assumes that covariates are only mildly correlated. However, in modern applications ranging from functional MRI to genome-wide association studies, covariates are highly correlated and do not exhibit key properties (such as the restricted eigenvalue condition, RIP, or other related assumptions). This paper considers a high-dimensional regression setting in which a graph governs both correlations among the covariates and the similarity among regression coefficients. Using side information about the strength of correlations among features, we form a graph with edge weights corresponding to pairwise covariances. This graph is used to define a graph total variation regularizer that promotes similar weights for highly correlated features. The graph structure encapsulated by this regularizer helps precondition correlated features to yield provably accurate estimates. Using graph-based regularizers to develop theoretical guarantees for highly-correlated covariates has not been previously examined. This paper shows how our proposed graph-based regularization yields mean-squared error guarantees for a broad range of covariance graph structures and correlation strengths which in many cases are optimal by imposing additional structure on {\$}\backslashbeta{\^{}}{\{}\backslashstar{\}}{\$} which encourages $\backslash$emph{\{}alignment{\}} with the covariance graph. Our proposed approach outperforms other state-of-the-art methods for highly-correlated design in a variety of experiments on simulated and real fMRI data.},
	archiveprefix = {arXiv},
	arxivid = {1803.07658},
	author = {Li, Yuan and Mark, Benjamin and Raskutti, Garvesh and Willett, Rebecca},
	date-added = {2019-02-13 04:48:34 +0000},
	date-modified = {2019-02-13 04:48:34 +0000},
	eprint = {1803.07658},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - Graph-based regularization for regression problems with highly-correlated designs.pdf:pdf},
	isbn = {9781728112954},
	title = {{Graph-based regularization for regression problems with highly-correlated designs}},
	url = {http://arxiv.org/abs/1803.07658},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1803.07658}}

@article{She2010,
	abstract = {This paper studies a generic sparse regression problem with a customizable sparsity pattern matrix, motivated by, but not limited to, a supervised gene clustering problem in microarray data analysis. The clustered lasso method is proposed with the l 1-type penalties imposed on both the coefficients and their pairwise differences. Somewhat surprisingly, it behaves differently than the lasso or the fused lasso-the exact clustering effect expected from the l 1 penalization is rarely seen in applications. An asymp-totic study is performed to investigate the power and limitations of the l 1-penalty in sparse regression. We propose to combine data-augmentation and weights to improve the l 1 technique. To address the computational issues in high dimensions, we successfully generalize a popular iterative algorithm both in practice and in theory and propose an 'annealing' algorithm applicable to generic sparse regressions (including the fused/clustered lasso). Some effective accelerating techniques are further investigated to boost the convergence. The accelerated annealing (AA) algorithm, involving only matrix multiplications and thresholdings, can handle a large design matrix as well as a large sparsity pattern matrix.},
	author = {She, Yiyuan},
	date-added = {2019-02-13 02:57:54 +0000},
	date-modified = {2019-02-13 02:57:54 +0000},
	doi = {10.1214/10-EJS578},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/She - 2010 - Sparse regression with exact clustering.pdf:pdf},
	journal = {Electronic Journal of Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	pages = {1055--1096},
	title = {{Sparse regression with exact clustering}},
	url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ejs/1286889184},
	volume = {4},
	year = {2010},
	bdsk-url-1 = {https://projecteuclid.org/download/pdfview%7B%5C_%7D1/euclid.ejs/1286889184},
	bdsk-url-2 = {https://doi.org/10.1214/10-EJS578}}

@article{Sharma,
	abstract = {Statistical procedures for variable selection have become integral elements in any analysis. Successful procedures are characterized by high predictive accuracy, yielding interpretable models while retaining computational efficiency. Penalized methods that perform coefficient shrinkage have been shown to be successful in many cases. Models with correlated predictors are particularly challenging to tackle. We propose a penal-ization procedure that performs variable selection while clustering groups of predictors automatically. The oracle properties of this procedure, including consistency in group identification, are also studied. The proposed method compares favorably with existing selection approaches in both prediction accuracy and model discovery, while retaining its computational efficiency. Supplementary materials are available online.},
	author = {Sharma, Dhruv B and Bondell, Howard D and {Helen Zhang}, Hao},
	date-added = {2019-02-13 02:54:39 +0000},
	date-modified = {2019-02-13 02:54:39 +0000},
	doi = {10.1080/15533174.2012.707849},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Sharma et al. - Unknown - Journal of Computational and Graphical Statistics Consistent Group Identification and Variable Selection in Re.pdf:pdf},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {2},
	pages = {319--340},
	title = {{Journal of Computational and Graphical Statistics Consistent Group Identification and Variable Selection in Regression With Correlated Predictors Consistent Group Identification and Variable Selection in Regression With Correlated Predictors}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	volume = {22},
	year = {2013},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	bdsk-url-2 = {https://doi.org/10.1080/15533174.2012.707849}}

@article{Su2017,
	abstract = {In regression settings where explanatory variables have very low correlations and there are relatively few effects, each of large magnitude, we expect the Lasso to find the important variables with few errors, if any. This paper shows that in a regime of linear sparsity-meaning that the fraction of variables with a nonvanishing effect tends to a constant, however small-this cannot really be the case, even when the design variables are stochastically independent. We demonstrate that true features and null features are always interspersed on the Lasso path, and that this phenomenon occurs no matter how strong the effect sizes are. We derive a sharp asymptotic trade-off between false and true positive rates or, equivalently, between measures of type I and type II errors along the Lasso path. This trade-off states that if we ever want to achieve a type II error (false negative rate) under a critical value, then anywhere on the Lasso path the type I error (false positive rate) will need to exceed a given threshold so that we can never have both errors at a low level at the same time. Our analysis uses tools from approximate message passing (AMP) theory as well as novel elements to deal with a possibly adaptive selection of the Lasso regularizing parameter.},
	author = {Su, Weijie and Bogdan, Ma{\l}gorzata and Cand{\`{e}}s, Emmanuel},
	date-added = {2019-02-13 01:47:12 +0000},
	date-modified = {2019-02-13 01:47:12 +0000},
	doi = {10.1214/16-AOS1521},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Su, Bogdan, Cand{\`{e}}s - 2017 - FALSE DISCOVERIES OCCUR EARLY ON THE LASSO PATH.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {5},
	pages = {2133--2150},
	title = {{FALSE DISCOVERIES OCCUR EARLY ON THE LASSO PATH}},
	url = {https://projecteuclid-org.libproxy1.usc.edu/download/pdfview{\_}1/euclid.aos/1509436830},
	volume = {45},
	year = {2017},
	bdsk-url-1 = {https://projecteuclid-org.libproxy1.usc.edu/download/pdfview%7B%5C_%7D1/euclid.aos/1509436830},
	bdsk-url-2 = {https://doi.org/10.1214/16-AOS1521}}

@article{Su2018,
	abstract = {Applied statisticians use sequential regression procedures to produce a ranking of explanatory variables and, in settings of low correlations between variables and strong true effect sizes, expect that variables at the very top of this ranking are truly relevant to the response. In a regime of certain sparsity levels, however, three examples of sequential procedures-forward stepwise, the lasso, and least angle regression-are shown to include the first spurious variable unexpectedly early. We derive a rigorous, sharp prediction of the rank of the first spurious variable for these three procedures, demonstrating that the first spurious variable occurs earlier and earlier as the regression coefficients become denser. This counterintuitive phenomenon persists for statistically independent Gaussian random designs and an arbitrarily large magnitude of the true effects. We gain a better understanding of the phenomenon by identifying the underlying cause and then leverage the insights to introduce a simple visualization tool termed the "double-ranking diagram" to improve on sequential methods. As a byproduct of these findings, we obtain the first provable result certifying the exact equivalence between the lasso and least angle regression in the early stages of solution paths beyond orthogonal designs. This equivalence can seamlessly carry over many important model selection results concerning the lasso to least angle regression.},
	author = {Su, Weijie J},
	date-added = {2019-02-13 01:44:45 +0000},
	date-modified = {2019-02-13 01:44:45 +0000},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Su - 2018 - When Is the First Spurious Variable Selected by Sequential Regression Procedures.pdf:pdf},
	journal = {Biometrika},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{When Is the First Spurious Variable Selected by Sequential Regression Procedures?}},
	url = {http://stat.wharton.upenn.edu/{~}suw/paper/FPlasso.pdf},
	year = {2018},
	bdsk-url-1 = {http://stat.wharton.upenn.edu/%7B~%7Dsuw/paper/FPlasso.pdf}}

@article{Yu2013,
	abstract = {Reproducibility is imperative for any scientific discovery. More often than not, modern scientific findings rely on statistical analysis of high-dimensional data. At a minimum, reproducibility manifests itself in stability of statistical results relative to "reasonable" perturbations to data and to the model used. Jacknife, bootstrap, and cross-validation are based on perturbations to data, while robust statistics methods deal with perturbations to models. In this article, a case is made for the importance of stability in statistics. Firstly, we motivate the necessity of stability for interpretable and reliable encoding models from brain fMRI signals. Secondly, we find strong evidence in the literature to demonstrate the central role of stability in statistical inference, such as sensitivity analysis and effect detection. Thirdly, a smoothing parameter selector based on estimation stability (ES), ES-CV, is proposed for Lasso, in order to bring stability to bear on cross-validation (CV). ES-CV is then utilized in the encoding models to reduce the number of predictors by 60{\%} with almost no loss (1.3{\%}) of prediction performance across over 2,000 voxels. Last, a novel "stability" argument is seen to drive new results that shed light on the intriguing interactions between sample to sample variability and heavier tail error distribution (e.g., double-exponential) in high-dimensional regression models with p predictors and n independent samples. In particular, when p/n → $\kappa$ ∈ (0.3, 1) and the error distribution is double-exponential, the Ordinary Least Squares (OLS) is a better estimator than the Least Absolute Deviation (LAD) estimator.},
	author = {Yu, Bin},
	date-added = {2019-02-13 01:37:18 +0000},
	date-modified = {2019-02-13 01:37:18 +0000},
	doi = {10.3150/13-BEJSP14},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Yu - 2013 - Stability.pdf:pdf},
	journal = {Bernoulli},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {4},
	pages = {1484--1500},
	title = {{Stability}},
	url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.bj/1377612862},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://projecteuclid.org/download/pdfview%7B%5C_%7D1/euclid.bj/1377612862},
	bdsk-url-2 = {https://doi.org/10.3150/13-BEJSP14}}

@article{Park2007,
	author = {Park, M.Y. and Hastie, Trevor and Tibshirani, Rob},
	date-added = {2019-02-11 01:06:32 +0000},
	date-modified = {2019-02-11 01:06:32 +0000},
	journal = {Biostatistics},
	keywords = {Read for Jacob,clustering features,feature selection},
	mendeley-tags = {feature selection,Read for Jacob,clustering features},
	pages = {212--227},
	title = {{Averaged gene expressions for regression}},
	volume = {8},
	year = {2007}}

@article{Hastie2001,
	author = {Hastie, Trevor; and Tibshirani, Rob; and Botstein, R.; and Brown, P.},
	date-added = {2019-02-11 01:04:05 +0000},
	date-modified = {2019-02-11 01:04:05 +0000},
	journal = {Genome Biology},
	keywords = {Read for Jacob,clustering features,feature selection},
	mendeley-tags = {Read for Jacob,feature selection,clustering features},
	number = {1},
	pages = {3.1--3.12},
	title = {{Supervised harvesting of expression trees}},
	volume = {2},
	year = {2001}}

@article{Bondell2008,
	abstract = {Variable selection can be challenging, particularly in situations with a large number of predic-tors with possibly high correlations, such as gene expression data. In this article, a new method called the OSCAR (octagonal shrinkage and clustering algorithm for regression) is proposed to simultaneously select variables while grouping them into predictive clusters. In addition to improving prediction accuracy and interpretation, these resulting groups can then be investigated further to discover what contributes to the group having a similar behavior. The technique is based on penalized least squares with a geometrically intuitive penalty function that shrinks some coefficients to exactly zero. Additionally, this penalty yields exact equality of some coefficients, encouraging correlated predictors that have a similar effect on the response to form predictive clusters represented by a single coefficient. The proposed procedure is shown to compare favorably to the existing shrinkage and variable selection techniques in terms of both prediction error and model complexity, while yielding the additional grouping information.},
	author = {Bondell, Howard D and Reich, Brian J},
	date-added = {2019-02-11 00:59:19 +0000},
	date-modified = {2019-02-11 00:59:19 +0000},
	doi = {10.1111/j.1541-0420.2007.00843.x},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bondell, Reich - 2008 - Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR.pdf:pdf},
	journal = {Biometrics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	pages = {115--123},
	title = {{Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR}},
	url = {https://onlinelibrary-wiley-com.libproxy1.usc.edu/doi/pdf/10.1111/j.1541-0420.2007.00843.x},
	volume = {64},
	year = {2008},
	bdsk-url-1 = {https://onlinelibrary-wiley-com.libproxy1.usc.edu/doi/pdf/10.1111/j.1541-0420.2007.00843.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1541-0420.2007.00843.x}}

@article{Buhlmann2013,
	abstract = {We consider estimation in a high-dimensional linear model with strongly correlated variables. We propose to cluster the variables first and do subsequent sparse estimation such as the Lasso for cluster-representatives or the group Lasso based on the structure from the clusters. Regarding the first step, we present a novel and bottom-up agglomerative clustering algorithm based on canonical correlations, and we show that it finds an optimal solution and is statistically consistent. We also present some theoretical arguments that canonical correlation based clustering leads to a better-posed compatibility constant for the design matrix which ensures identifiability and an oracle inequality for the group Lasso. Furthermore, we discuss circumstances where cluster-representatives and using the Lasso as subsequent estimator leads to improved results for prediction and detection of variables. We complement the theoretical analysis with various empirical results. {\textcopyright} 2013 Elsevier B.V.},
	archiveprefix = {arXiv},
	arxivid = {1209.5908},
	author = {B{\"{u}}hlmann, Peter and R{\"{u}}timann, Philipp and van de Geer, Sara and Zhang, Cun Hui},
	date-added = {2019-02-11 00:55:49 +0000},
	date-modified = {2019-02-11 00:55:49 +0000},
	doi = {10.1016/j.jspi.2013.05.019},
	eprint = {1209.5908},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{u}}hlmann et al. - 2013 - Correlated variables in regression Clustering and sparse estimation.pdf:pdf},
	isbn = {03783758},
	issn = {03783758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Canonical correlation,Group Lasso,Hierarchical clustering,High-dimensional inference,Lasso,Oracle inequality,Variable screening,Variable selection},
	number = {11},
	pages = {1835--1858},
	pmid = {11974822},
	publisher = {Elsevier},
	title = {{Correlated variables in regression: Clustering and sparse estimation}},
	url = {http://dx.doi.org/10.1016/j.jspi.2013.05.019},
	volume = {143},
	year = {2013},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.jspi.2013.05.019}}

@article{Zou2005,
	abstract = {We propose the elastic net, a new regularization and variable selection method.Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p?n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths},
	annote = {Brief discussion in 2.3 about how lasso tends to choose one of highly correlated features at random; defers theory to Efron et al (2004) LARS paper.},
	author = {Zou, Hui and Hastie, Trevor},
	date-added = {2019-02-11 00:42:59 +0000},
	date-modified = {2019-02-11 00:42:59 +0000},
	doi = {10.1016/S0042-6989(99)00110-8},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf},
	isbn = {1369-7412},
	issn = {00426989},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	keywords = {Contrast constancy,Contrast perception,DSO 607,DSO 607 Week 4,Read for Jacob,Suprathreshold,Texture,Visual modelling},
	mendeley-tags = {DSO 607,DSO 607 Week 4,Read for Jacob},
	number = {2},
	pages = {301--320},
	pmid = {20713001},
	title = {{Regularization and variable selection via the elastic net}},
	volume = {67},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1016/S0042-6989(99)00110-8}}

@article{Efron2004,
	abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
	annote = {Accordng to section 2.3 of elastic net paper, this paper contains a theoretical explanation of why lasso tends to pick one variable among highly correlated ones.},
	author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
	date-added = {2019-02-11 00:42:16 +0000},
	date-modified = {2019-02-11 00:42:16 +0000},
	doi = {10.1214/009053604000000067},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Efron et al. - 2004 - Least Angle Regression.pdf:pdf},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {DSO 607,DSO 607 Week 4,Read for Jacob,and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
	mendeley-tags = {DSO 607,DSO 607 Week 4,Read for Jacob},
	number = {2},
	pages = {407--499},
	pmid = {1000198917},
	title = {{Least Angle Regression}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=25879},
	volume = {32},
	year = {2004},
	bdsk-url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=25879},
	bdsk-url-2 = {https://doi.org/10.1214/009053604000000067}}

@techreport{Org2015,
	abstract = {In this paper we introduce fuzzy forests, a novel machine learning algorithm for ranking the importance of features in high-dimensional classification and regression problems. Fuzzy forests is specifically designed to provide relatively unbiased rankings of variable importance in the presence of highly correlated features, especially when p {\textgreater}{\textgreater} n. We introduce our implementation of fuzzy forests in the R package, fuzzyforest. Fuzzy forests works by taking advantage of the network structure between features. First, the features are partitioned into separate modules such that the correlation within modules is high and the correlation between modules is low. The package fuzzyforest allows for easy use of Weighted Gene Coexpression Network Analysis (WGCNA) to form modules of features such that the modules are roughly uncorrelated. Then recursive feature elimination random forests (RFE-RFs) are used on each module, separately. From the surviving features, a final group is selected and ranked using one last round of RFE-RFs. This procedure results in a ranked variable importance list whose size is pre-specified by the user. The selected features can then be used to construct a predictive model.},
	author = {Conn, Daniel and Ngun, Tuck and Li, Gang and Ramirez, Christina M},
	date-added = {2019-02-11 00:41:24 +0000},
	date-modified = {2019-02-11 00:44:57 +0000},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Org et al. - 2015 - UCLA Research Reports Title Fuzzy Forests Extending Random Forests for Correlated, High-Dimensional Data Publication.pdf:pdf},
	keywords = {feature selection,prediction competitions},
	mendeley-tags = {feature selection,prediction competitions},
	title = {{Fuzzy Forests: Extending Random Forests for Correlated, High-Dimensional Data Publication Date Fuzzy Forests: Extending Random Forests for Correlated, High-Dimensional Data}},
	url = {https://escholarship.org/uc/item/55h4h0w7},
	year = {2015},
	bdsk-url-1 = {https://escholarship.org/uc/item/55h4h0w7}}

@article{kang_kuznetsova_choi_luca_2013,
	author = {Kang, Jun Seok and Kuznetsova, Polina and Choi, Yejin and Luca, Michael},
	date-added = {2018-10-19 10:08:32 -0700},
	date-modified = {2018-10-19 10:08:32 -0700},
	doi = {10.2139/ssrn.2293165},
	journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	month = {Oct},
	pages = {1443--1448},
	title = {Using Text Analysis to Target Government Inspections: Evidence from Restaurant Hygiene Inspections and Online Reviews},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.2139/ssrn.2293165}}

@article{rockoff_jacob_kane_staiger_2011,
	author = {Rockoff, Jonah and Jacob, Brian and Kane, Thomas and Staiger, Douglas},
	date-added = {2018-10-18 17:16:11 -0700},
	date-modified = {2018-10-18 17:16:11 -0700},
	doi = {10.3386/w14485},
	journal = {Education Finance and Policy},
	number = {1},
	pages = {43--74},
	title = {Can You Recognize an Effective Teacher When You Recruit One?},
	volume = {6},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.3386/w14485}}

@misc{miconi-fairness,
	author = {Thomas Miconi},
	date-added = {2018-10-18 17:08:42 -0700},
	date-modified = {2018-10-18 17:08:53 -0700},
	eprint = {arXiv:1707.01195},
	title = {The impossibility of "fairness": a generalized impossibility result for decisions},
	year = {2017}}

@article{arnold_dobbie_yang_2017,
	author = {Arnold, David and Dobbie, Will and Yang, Crystal},
	date-added = {2018-10-18 17:02:24 -0700},
	date-modified = {2018-10-18 17:02:24 -0700},
	doi = {10.3386/w23421},
	title = {Racial Bias in Bail Decisions},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.3386/w23421}}

@article{kamishima_akaho_asoh_sakuma_2012,
	author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
	date-added = {2018-10-18 16:48:54 -0700},
	date-modified = {2018-10-18 16:48:54 -0700},
	doi = {10.1007/978-3-642-33486-3_3},
	journal = {Machine Learning and Knowledge Discovery in Databases Lecture Notes in Computer Science},
	pages = {35--50},
	title = {Fairness-Aware Classifier with Prejudice Remover Regularizer},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-33486-3_3}}

@article{saunders_hunt_hollywood_2016,
	author = {Saunders, Jessica and Hunt, Priscillia and Hollywood, John S.},
	date-added = {2018-10-04 20:17:01 -0700},
	date-modified = {2018-10-04 20:17:01 -0700},
	doi = {10.1007/s11292-016-9272-0},
	journal = {Journal of Experimental Criminology},
	month = {Dec},
	number = {3},
	pages = {347--371},
	title = {Predictions put into practice: a quasi-experimental evaluation of Chicago's predictive policing pilot},
	volume = {12},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s11292-016-9272-0}}

@misc{tan-2017,
	author = {Sarah Tan and Rich Caruana and Giles Hooker and Yin Lou},
	date-added = {2018-10-04 20:14:37 -0700},
	date-modified = {2018-10-04 20:14:44 -0700},
	eprint = {arXiv:1710.06169},
	title = {Auditing Black-Box Models Using Transparent Model Distillation With Side Information},
	year = {2017}}

@article{ludwig-2015,
	author = {Nicole Ludwig and Stefan Feuerriegel and Dirk Neumann},
	date-added = {2018-10-04 17:11:57 -0700},
	date-modified = {2018-10-04 17:12:06 -0700},
	doi = {10.1080/12460125.2015.994290},
	eprint = {https://doi.org/10.1080/12460125.2015.994290},
	journal = {Journal of Decision Systems},
	number = {1},
	pages = {19-36},
	publisher = {Taylor & Francis},
	title = {Putting Big Data analytics to work: Feature selection for forecasting electricity prices using the LASSO and random forests},
	url = {https://doi.org/10.1080/12460125.2015.994290},
	volume = {24},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1080/12460125.2015.994290}}

@article{schnabel-2015,
	author = {Renate B Schnabel and Xiaoyan Yin and Philimon Gona and Martin G Larson and Alexa S Beiser and David D McManus and Christopher Newton-Cheh and Steven A Lubitz and Jared W Magnani and Patrick T Ellinor and Sudha Seshadri and Philip A Wolf and Ramachandran S Vasan and Emelia J Benjamin and Daniel Levy},
	date-added = {2018-10-04 17:09:18 -0700},
	date-modified = {2018-10-04 17:09:40 -0700},
	doi = {https://doi.org/10.1016/S0140-6736(14)61774-8},
	issn = {0140-6736},
	journal = {The Lancet},
	number = {9989},
	pages = {154 - 162},
	title = {50 year trends in atrial fibrillation prevalence, incidence, risk factors, and mortality in the Framingham Heart Study: a cohort study},
	url = {http://www.sciencedirect.com/science/article/pii/S0140673614617748},
	volume = {386},
	year = {2015},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/S0140673614617748},
	bdsk-url-2 = {https://doi.org/10.1016/S0140-6736(14)61774-8}}

@article{reichman-2001,
	abstract = {No abstract is available for this item.},
	author = {Reichman, Nancy E. and Teitler, Julien O. and Garfinkel, Irwin and McLanahan, Sara S.},
	date-added = {2018-10-04 16:58:27 -0700},
	date-modified = {2018-10-04 16:58:38 -0700},
	journal = {Children and Youth Services Review},
	number = {4-5},
	pages = {303-326},
	title = {{Fragile Families: sample and design}},
	url = {https://ideas.repec.org/a/eee/cysrev/v23y2001i4-5p303-326.html},
	volume = {23},
	year = 2001,
	bdsk-url-1 = {https://ideas.repec.org/a/eee/cysrev/v23y2001i4-5p303-326.html}}

@misc{lundberg-2018,
	author = {Ian Lundberg and Arvind Narayanan and Karen Levy and Matthew J. Salganik},
	date-added = {2018-10-04 16:55:55 -0700},
	date-modified = {2018-10-04 16:56:06 -0700},
	eprint = {arXiv:1809.00103},
	title = {Privacy, ethics, and data access: A case study of the Fragile Families Challenge},
	year = {2018}}

@article{sorlie_tibshirani_2003,
	author = {S{\o}rlie, Therese and Tibshirani, Robert and Parker, Joel and Hastie, Trevor and Marron, J. S. and Nobel, Andrew and Deng, Shibing and Johnsen, Hilde and Pesich, Robert and Geisler, Stephanie and et al.},
	date-added = {2018-10-04 15:57:01 -0700},
	date-modified = {2018-10-04 15:57:01 -0700},
	doi = {10.1073/pnas.0932692100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {14},
	pages = {8418--8423},
	title = {Repeated observation of breast tumor subtypes in independent gene expression data sets},
	volume = {100},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.0932692100}}

@article{bogdan_berg_sabatti_su_candes_2015,
	author = {Bogdan, Ma{\l}gorzata and Berg, Ewout Van Den and Sabatti, Chiara and Su, Weijie and Cand{\`e}s, Emmanuel J.},
	date-added = {2018-10-04 15:39:09 -0700},
	date-modified = {2018-10-04 15:39:09 -0700},
	doi = {10.1214/15-aoas842},
	journal = {The Annals of Applied Statistics},
	number = {3},
	pages = {1103--1140},
	title = {SLOPE---Adaptive variable selection via convex optimization},
	volume = {9},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1214/15-aoas842}}

@article{barber_candes_2015,
	author = {Barber, Rina Foygel and Cand{\`e}s, Emmanuel J.},
	date-added = {2018-10-04 15:35:16 -0700},
	date-modified = {2018-10-04 15:35:16 -0700},
	doi = {10.1214/15-aos1337},
	journal = {The Annals of Statistics},
	number = {5},
	pages = {2055--2085},
	title = {Controlling the false discovery rate via knockoffs},
	volume = {43},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1214/15-aos1337}}

@misc{gsell-2013,
	author = {Max Grazier G'Sell and Trevor Hastie and Robert Tibshirani},
	date-added = {2018-10-04 14:08:18 -0700},
	date-modified = {2018-10-04 14:08:40 -0700},
	eprint = {arXiv:1302.2303},
	title = {False Variable Selection Rates in Regression},
	year = {2013}}

@article{shah_samworth_2013,
	author = {Shah, Rajen D. and Samworth, Richard J.},
	date-added = {2018-10-04 14:04:46 -0700},
	date-modified = {2018-10-04 14:04:46 -0700},
	doi = {10.1016/j.jspi.2013.05.022},
	journal = {Journal of Statistical Planning and Inference},
	number = {11},
	pages = {1866--1868},
	title = {Discussion of `Correlated variables in regression: Clustering and sparse estimation' by Peter B{\"u}hlmann, Philipp R{\"u}timann, Sara van de Geer and Cun-Hui Zhang},
	volume = {143},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1016/j.jspi.2013.05.022}}

@article{shah_samworth_2012,
	author = {Shah, Rajen D. and Samworth, Richard J.},
	date-added = {2018-10-04 14:03:28 -0700},
	date-modified = {2018-10-04 14:03:28 -0700},
	doi = {10.1111/j.1467-9868.2011.01034.x},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {1},
	pages = {55--80},
	title = {Variable selection with error control: another look at stability selection},
	volume = {75},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1111/j.1467-9868.2011.01034.x}}

@article{meinshausen-2010,
	author = {Meinshausen, Nicolai and B{\"u}hlmann, Peter},
	date-added = {2018-10-04 13:59:56 -0700},
	date-modified = {2018-10-04 14:01:59 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	month = {May},
	number = {4},
	pages = {417-473},
	title = {Stability Selection},
	volume = {72},
	year = {2010}}

@article{simmons_nelson_simonsohn_2011,
	author = {Simmons, Joseph and Nelson, Leif and Simonsohn, Uri},
	date-added = {2018-08-07 17:34:12 -0700},
	date-modified = {2018-08-07 17:34:12 -0700},
	doi = {10.1037/e519702015-014},
	journal = {PsycEXTRA Dataset},
	title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1037/e519702015-014}}

@misc{gelman_2017,
	author = {Gelman, Andrew},
	date-added = {2018-08-07 17:26:43 -0700},
	date-modified = {2018-10-04 13:53:31 -0700},
	journal = {Statistical Modeling, Causal Inference, and Social Science},
	month = {Dec},
	title = {The 80 Percent Power Lie},
	url = {http://andrewgelman.com/2017/12/04/80-power-lie/},
	year = {2017},
	bdsk-url-1 = {http://andrewgelman.com/2017/12/04/80-power-lie/}}

@article{baker_2016,
	author = {Baker, Monya},
	date-added = {2018-08-07 17:20:27 -0700},
	date-modified = {2018-08-07 17:20:27 -0700},
	doi = {10.1038/533452a},
	journal = {Nature},
	number = {7604},
	pages = {452--454},
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/533452a}}

@article{gould_2009,
	author = {Gould, Elise},
	date-added = {2018-08-07 17:06:31 -0700},
	date-modified = {2018-08-07 17:06:31 -0700},
	doi = {10.1289/ehp.0800408},
	journal = {Environmental Health Perspectives},
	number = {7},
	pages = {1162--1167},
	title = {Childhood Lead Poisoning: Conservative Estimates of the Social and Economic Benefits of Lead Hazard Control},
	volume = {117},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1289/ehp.0800408}}

@article{nevin_2000,
	author = {Nevin, Rick},
	date-added = {2018-08-07 16:58:27 -0700},
	date-modified = {2018-08-07 16:58:27 -0700},
	doi = {10.1006/enrs.1999.4045},
	journal = {Environmental Research},
	number = {1},
	pages = {1--22},
	title = {How Lead Exposure Relates to Temporal Changes in IQ, Violent Crime, and Unwed Pregnancy},
	volume = {83},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1006/enrs.1999.4045}}

@article{mannar-dunn-salt,
	author = {Mannar, M. G. Venkatesh and Dunn, John Thornton},
	date-added = {2018-08-07 16:48:23 -0700},
	date-modified = {2018-08-07 16:50:02 -0700},
	journal = {International Council for Control of Iodine Deficiency Disorders},
	title = {Salt iodization for the elimination of iodine deficiency},
	year = {1995}}

@unpublished{adhvaryu_bednar_nyshadham_molina_nguyen_2018,
	author = {Adhvaryu, Achyuta and Bednar, Steven and Nyshadham, Anant and Molina, Teresa and Nguyen, Quynh},
	date-added = {2018-08-07 16:43:16 -0700},
	date-modified = {2018-08-07 16:43:50 -0700},
	doi = {10.3386/w24847},
	journal = {NBER Working Papers 24847},
	title = {When It Rains It Pours: The Long-run Economic Impacts of Salt Iodization in the United States},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3386/w24847}}

@article{feyrer_politi_weil_2017,
	author = {Feyrer, James and Politi, Dimitra and Weil, David},
	date-added = {2018-08-07 16:34:59 -0700},
	date-modified = {2018-08-07 16:34:59 -0700},
	doi = {10.1093/jeea/jvw002},
	journal = {Journal of the European Economic Association},
	month = {Apr},
	number = {2},
	pages = {355--387},
	title = {The Cognitive Effects of Micronutrient Deficiency: Evidence from Salt Iodization in the United States},
	volume = {15},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1093/jeea/jvw002}}

@article{mccarthy_fader_hardie_2017,
	author = {Mccarthy, Daniel and Fader, Peter and Hardie, Bruce},
	date-added = {2018-08-05 16:35:36 -0700},
	date-modified = {2018-08-05 16:35:36 -0700},
	doi = {10.1509/jm.15.0519},
	journal = {Journal of Marketing},
	month = {Jan},
	pages = {17--35},
	title = {Valuing Subscription-Based Businesses Using Publicly Disclosed Customer Data},
	volume = {81},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1509/jm.15.0519}}

@article{lum_isaac_2016,
	author = {Lum, Kristian and Isaac, William},
	date-added = {2018-08-05 15:47:18 -0700},
	date-modified = {2018-08-05 15:47:18 -0700},
	doi = {10.1111/j.1740-9713.2016.00960.x},
	journal = {Significance},
	number = {5},
	pages = {14--19},
	title = {To predict and serve?},
	volume = {13},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1111/j.1740-9713.2016.00960.x}}

@article{howard_zhang_horvitz_2017,
	author = {Howard, Ayanna and Zhang, Cha and Horvitz, Eric},
	date-added = {2018-08-05 15:46:29 -0700},
	date-modified = {2018-08-05 15:46:29 -0700},
	doi = {10.1109/arso.2017.8025197},
	journal = {2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)},
	title = {Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/arso.2017.8025197}}

@article{horvitz_mulligan_2015,
	author = {Horvitz, E. and Mulligan, D.},
	date-added = {2018-08-05 15:41:56 -0700},
	date-modified = {2018-08-05 15:41:56 -0700},
	doi = {10.1126/science.aac4520},
	journal = {Science},
	number = {6245},
	pages = {253--255},
	title = {Data, privacy, and the greater good},
	volume = {349},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1126/science.aac4520}}

@article{policing-ensign,
	author = {Danielle Ensign and Sorelle A Friedler and Scott Neville and Carlos Scheidegger and Suresh Venkatasubramanian},
	date-added = {2018-08-05 15:33:11 -0700},
	date-modified = {2018-08-05 15:33:50 -0700},
	journal = {arXiv preprint arXiv:1706.09847},
	title = {Runaway feedback loops in predictive policing},
	year = {2017}}

@article{friedler-fairness,
	author = {Sorelle A Friedler and Carlos Scheidegger and Suresh Venkatasubramanian},
	date-added = {2018-08-05 15:30:56 -0700},
	date-modified = {2018-08-05 15:32:27 -0700},
	journal = {arXiv preprint arXiv:1609.07236},
	title = {On the (im) possibility of fairness},
	year = {2016}}

@article{adler_falk_friedler_rybeck_scheidegger_smith_venkatasubramanian_nix_2018,
	author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A. and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh and Nix, Tionney},
	date-added = {2018-08-05 15:29:43 -0700},
	date-modified = {2018-08-05 15:29:43 -0700},
	doi = {10.1109/icdm.2016.0011},
	journal = {Knowledge and Information Systems},
	number = {1},
	pages = {95--122},
	title = {Auditing Black-Box Models for Indirect Influence},
	volume = {54},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/icdm.2016.0011}}

@article{sapiezynski_kassarnig_wilson_2017,
	author = {Sapiezynski, Piotr and Kassarnig, Valentin and Wilson, Christo},
	date-added = {2018-08-05 15:25:24 -0700},
	date-modified = {2018-08-05 15:25:24 -0700},
	doi = {10.18122/B20Q5R},
	journal = {Proceedings of FATREC Workshop on Responsible Recommendation at ACM RecSys, Como, Italy (FATREC'17)},
	month = {Aug},
	title = {Academic performance prediction in a gender-imbalanced environment},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.18122/B20Q5R}}

@article{gebru-buolamwini,
	author = {Buolamwini, Joy and Gebru, Timnit},
	date-added = {2018-08-05 15:06:47 -0700},
	date-modified = {2018-08-05 15:10:49 -0700},
	journal = {Conference on Fairness, Accountability, and Transparency},
	pages = {1-15},
	title = {Gender Shades: Intersectional Accuracy Disparities in Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	volume = {81},
	year = {2018}}

@article{wong_mulligan_wyk_pierce_chuang_2017,
	author = {Wong, Richmond Y. and Mulligan, Deirdre K. and Wyk, Ellen Van and Pierce, James and Chuang, John},
	date-added = {2018-08-05 14:54:34 -0700},
	date-modified = {2018-08-05 14:54:34 -0700},
	doi = {10.1145/3134746},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	month = {Jun},
	number = {CSCW},
	pages = {1--26},
	title = {Eliciting Values Reflections by Engaging Privacy Futures Using Design Workbooks},
	volume = {1},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3134746}}

@article{mulligan_koopman_doty_2016,
	author = {Mulligan, Deirdre K. and Koopman, Colin and Doty, Nick},
	date-added = {2018-08-05 14:45:38 -0700},
	date-modified = {2018-08-05 14:45:38 -0700},
	doi = {10.1098/rsta.2016.0118},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	number = {2083},
	pages = {20160118},
	title = {Privacy is an essentially contested concept: a multi-dimensional analytic for mapping privacy},
	volume = {374},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1098/rsta.2016.0118}}

@article{nair_misra_hornbuckle_mishra_acharya_2013,
	author = {Nair, Harikesh and Misra, Sanjog and Hornbuckle, William J. and Mishra, Ranjan and Acharya, Anand},
	date-added = {2018-05-11 22:38:48 +0000},
	date-modified = {2018-05-18 05:49:29 +0000},
	doi = {10.1287/mksc.2017.1039},
	journal = {Marketing Science},
	number = {5},
	pages = {699-725},
	title = {Big Data and Marketing Analytics in Gaming: Combining Empirical Models and Field Experimentation},
	volume = {36},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.2139/ssrn.2399676}}

@article{athey_imbens_2016,
	author = {Athey, Susan and Imbens, Guido},
	date-added = {2018-05-11 22:15:02 +0000},
	date-modified = {2018-05-11 22:15:02 +0000},
	doi = {10.1073/pnas.1510489113},
	journal = {Proceedings of the National Academy of Sciences},
	month = {May},
	number = {27},
	pages = {7353--7360},
	title = {Recursive partitioning for heterogeneous causal effects: Table 1.},
	volume = {113},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.1510489113}}

@article{hill-2012,
	author = {Jennifer L. Hill},
	date-added = {2018-05-03 23:13:59 +0000},
	date-modified = {2018-05-11 22:35:34 +0000},
	doi = {10.1198/jcgs.2010.08162},
	eprint = {https://doi.org/10.1198/jcgs.2010.08162},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {217-240},
	publisher = {Taylor & Francis},
	title = {Bayesian Nonparametric Modeling for Causal Inference},
	url = {https://doi.org/10.1198/jcgs.2010.08162},
	volume = {20},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1198/jcgs.2010.08162}}

@article{hill-carnegie-harada-2016,
	author = {Carnegie, Nicole Bohme and Harada, Masataka and Hill, Jennifer L.},
	date-added = {2018-05-03 23:11:07 +0000},
	date-modified = {2018-05-03 23:11:57 +0000},
	doi = {10.1080/19345747.2015.1078862},
	eprint = {https://doi.org/10.1080/19345747.2015.1078862},
	journal = {Journal of Research on Educational Effectiveness},
	number = {3},
	pages = {395-420},
	publisher = {Routledge},
	title = {Assessing Sensitivity to Unmeasured Confounding Using a Simulated Potential Confounder},
	url = {https://doi.org/10.1080/19345747.2015.1078862},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/19345747.2015.1078862}}

@article{VanderWeele-Arah-2011,
	abstract = {Uncontrolled confounding in observational studies gives rise to biased effect estimates. Sensitivity analysis techniques can be useful in assessing the magnitude of these biases. In this paper, we use the potential outcomes framework to derive a general class of sensitivity-analysis formulas for outcomes, treatments, and measured and unmeasured confounding variables that may be categorical or continuous. We give results for additive, risk-ratio and odds-ratio scales. We show that these results encompass a number of more specific sensitivity-analysis methods in the statistics and epidemiology literature. The applicability, usefulness, and limits of the bias-adjustment formulas are discussed. We illustrate the sensitivity-analysis techniques that follow from our results by applying them to 3 different studies. The bias formulas are particularly simple and easy to use in settings in which the unmeasured confounding variable is binary with constant effect on the outcome across treatment levels.},
	author = {Tyler J. VanderWeele and Onyebuchi A. Arah},
	date-added = {2018-05-03 23:04:41 +0000},
	date-modified = {2018-05-03 23:04:54 +0000},
	issn = {10443983},
	journal = {Epidemiology},
	number = {1},
	pages = {42--52},
	publisher = {Lippincott Williams & Wilkins},
	title = {Bias Formulas for Sensitivity Analysis of Unmeasured Confounding for General Outcomes, Treatments, and Confounders},
	url = {http://www.jstor.org/stable/29764679},
	volume = {22},
	year = {2011},
	bdsk-url-1 = {http://www.jstor.org/stable/29764679}}

@article{mccandless_somers_2017,
	author = {McCandless, Lawrence C and Somers, Julian M},
	date-added = {2018-05-03 23:00:13 +0000},
	date-modified = {2018-05-03 23:30:35 +0000},
	doi = {10.1177/0962280217729844},
	journal = {Statistical Methods in Medical Research},
	month = {Jul},
	pages = {096228021772984},
	title = {Bayesian sensitivity analysis for unmeasured confounding in causal mediation analysis},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1177/0962280217729844}}

@article{McCandless-2017,
	abstract = {Bias from unmeasured confounding is a persistent concern in observational studies, and sensitivity analysis has been proposed as a solution. In the recent years, probabilistic sensitivity analysis using either Monte Carlo sensitivity analysis (MCSA) or Bayesian sensitivity analysis (BSA) has emerged as a practical analytic strategy when there are multiple bias parameters inputs. BSA uses Bayes theorem to formally combine evidence from the prior distribution and the data. In contrast, MCSA samples bias parameters directly from the prior distribution. Intuitively, one would think that BSA and MCSA ought to give similar results. Both methods use similar models and the same (prior) probability distributions for the bias parameters. In this paper, we illustrate the surprising finding that BSA and MCSA can give very different results. Specifically, we demonstrate that MCSA can give inaccurate uncertainty assessments (e.g. 95\% intervals) that do not reflect the data's influence on uncertainty about unmeasured confounding. Using a data example from epidemiology and simulation studies, we show that certain combinations of data and prior distributions can result in dramatic prior‐to‐posterior changes in uncertainty about the bias parameters. This occurs because the application of Bayes theorem in a non‐identifiable model can sometimes rule out certain patterns of unmeasured confounding that are not compatible with the data. Consequently, the MCSA approach may give 95\% intervals that are either too wide or too narrow and that do not have 95\% frequentist coverage probability. Based on our findings, we recommend that analysts use BSA for probabilistic sensitivity analysis. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
	author = {McCandless, Lawrence C. and Gustafson, Paul},
	date-added = {2018-05-03 22:41:39 +0000},
	date-modified = {2018-05-03 23:31:03 +0000},
	doi = {10.1002/sim.7298},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7298},
	journal = {Statistics in Medicine},
	keywords = {Bayesian analysis, causal inference, non‐identifiable model, sensitivity analysis},
	number = {18},
	pages = {2887-2901},
	title = {A comparison of Bayesian and Monte Carlo sensitivity analysis for unmeasured confounding},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7298},
	volume = {36},
	year = {2017},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7298},
	bdsk-url-2 = {https://doi.org/10.1002/sim.7298}}

@article{dorie_harada_carnegie_hill_2016,
	author = {Dorie, Vincent and Harada, Masataka and Carnegie, Nicole Bohme and Hill, Jennifer},
	date-added = {2018-05-03 22:37:27 +0000},
	date-modified = {2018-05-03 22:37:27 +0000},
	doi = {10.1002/sim.6973},
	journal = {Statistics in Medicine},
	month = {Mar},
	number = {20},
	pages = {3453--3470},
	title = {A flexible, interpretable framework for assessing sensitivity to unmeasured confounding},
	volume = {35},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1002/sim.6973}}

@article{athey-yelp-2018,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180107826A},
	archiveprefix = {arXiv},
	author = {{Athey}, S. and {Blei}, D. and {Donnelly}, R. and {Ruiz}, F. and {Schmidt}, T.},
	date-added = {2018-05-03 21:16:14 +0000},
	date-modified = {2018-05-03 21:16:49 +0000},
	eprint = {1801.07826},
	journal = {ArXiv e-prints},
	keywords = {Economics - Econometrics, Computer Science - Artificial Intelligence, Statistics - Applications, Statistics - Machine Learning},
	month = jan,
	title = {{Estimating Heterogeneous Consumer Preferences for Restaurants and Travel Time Using Mobile Location Data}},
	year = 2018}

@article{peters-2016,
	author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
	date-added = {2018-05-03 19:41:49 +0000},
	date-modified = {2018-05-03 22:14:19 +0000},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	pages = {947-1012},
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	year = {2016}}

@book{pearl_2009,
	author = {Pearl, Judea},
	date-added = {2018-05-03 19:05:54 +0000},
	date-modified = {2018-05-03 22:18:32 +0000},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Causality: models, reasoning, and inference},
	year = {2009}}

@book{imbens_rubin_2015,
	author = {Imbens, Guido and Rubin, Donald B.},
	date-added = {2018-05-03 18:41:44 +0000},
	date-modified = {2018-05-03 18:41:44 +0000},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Causal inference for statistics, social, and biomedical sciences: an introduction},
	year = {2015}}

@manual{gdata,
	author = {Gregory R. Warnes and Ben Bolker and Gregor Gorjanc and Gabor Grothendieck and Ales Korosec and Thomas Lumley and Don MacQueen and Arni Magnusson and Jim Rogers and {others}},
	date-added = {2018-01-07 20:48:14 +0000},
	date-modified = {2018-01-07 20:48:18 +0000},
	note = {R package version 2.18.0},
	title = {gdata: Various R Programming Tools for Data Manipulation},
	url = {https://CRAN.R-project.org/package=gdata},
	year = {2017},
	bdsk-url-1 = {https://CRAN.R-project.org/package=gdata}}

@book{ggplot2,
	author = {Hadley Wickham},
	date-added = {2018-01-07 20:47:15 +0000},
	date-modified = {2018-01-07 20:47:21 +0000},
	isbn = {978-0-387-98140-6},
	publisher = {Springer-Verlag New York},
	title = {ggplot2: Elegant Graphics for Data Analysis},
	url = {http://ggplot2.org},
	year = {2009},
	bdsk-url-1 = {http://ggplot2.org}}

@book{DMwR,
	author = {L. Torgo},
	date-added = {2018-01-07 20:46:18 +0000},
	date-modified = {2018-01-07 20:46:25 +0000},
	publisher = {Chapman and Hall/CRC},
	title = {Data Mining with R, learning with case studies},
	url = {http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR},
	year = {2010},
	bdsk-url-1 = {http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR}}

@book{r-lattice,
	address = {New York},
	author = {Deepayan Sarkar},
	date-added = {2018-01-07 20:45:31 +0000},
	date-modified = {2018-01-07 20:45:39 +0000},
	note = {ISBN 978-0-387-75968-5},
	publisher = {Springer},
	title = {Lattice: Multivariate Data Visualization with R},
	url = {http://lmdvr.r-forge.r-project.org},
	year = {2008},
	bdsk-url-1 = {http://lmdvr.r-forge.r-project.org}}

@manual{readability,
	address = {Buffalo, New York},
	author = {Tyler W. Rinker},
	date-added = {2018-01-07 20:44:31 +0000},
	date-modified = {2018-01-07 20:44:37 +0000},
	note = {version 0.1.1},
	organization = {University at Buffalo/SUNY},
	title = {{readability}: Tools to Calculate Readability Scores},
	url = {http://github.com/trinker/readability},
	year = {2017},
	bdsk-url-1 = {http://github.com/trinker/readability}}

@manual{r-matrix,
	author = {Douglas Bates and Martin Maechler},
	date-added = {2018-01-07 20:43:40 +0000},
	date-modified = {2018-01-07 20:43:47 +0000},
	note = {R package version 1.2-11},
	title = {Matrix: Sparse and Dense Matrix Classes and Methods},
	url = {https://CRAN.R-project.org/package=Matrix},
	year = {2017},
	bdsk-url-1 = {https://CRAN.R-project.org/package=Matrix}}

@manual{stargazer,
	address = {Cambridge, USA},
	author = {Marek Hlavac},
	date-added = {2018-01-07 20:41:01 +0000},
	date-modified = {2018-01-07 20:41:07 +0000},
	note = {R package version 5.2},
	organization = {Harvard University},
	title = {stargazer: Well-Formatted Regression and Summary Statistics Tables},
	url = {http://CRAN.R-project.org/package=stargazer},
	year = {2015},
	bdsk-url-1 = {http://CRAN.R-project.org/package=stargazer}}

@article{glmnet,
	author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
	date-added = {2018-01-07 20:39:10 +0000},
	date-modified = {2018-01-07 20:39:19 +0000},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--22},
	title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
	url = {http://www.jstatsoft.org/v33/i01/},
	volume = {33},
	year = {2010},
	bdsk-url-1 = {http://www.jstatsoft.org/v33/i01/}}

@article{plyr,
	author = {Hadley Wickham},
	date-added = {2018-01-07 20:37:07 +0000},
	date-modified = {2018-01-07 20:37:52 +0000},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--29},
	title = {The Split-Apply-Combine Strategy for Data Analysis},
	url = {http://www.jstatsoft.org/v40/i01/},
	volume = {40},
	year = {2011},
	bdsk-url-1 = {http://www.jstatsoft.org/v40/i01/}}

@manual{foreach,
	author = {{Revolution Analytics} and Steve Weston},
	date-added = {2018-01-07 20:35:17 +0000},
	date-modified = {2018-01-11 20:21:53 +0000},
	note = {R package version 1.4.3},
	title = {foreach: Provides Foreach Looping Construct for R},
	url = {https://CRAN.R-project.org/package=foreach},
	year = {2015},
	bdsk-url-1 = {https://CRAN.R-project.org/package=foreach}}

@manual{r-webmining,
	author = {Mario Annau},
	date-added = {2018-01-07 20:33:49 +0000},
	date-modified = {2018-01-07 20:34:03 +0000},
	note = {R package version 1.3},
	title = {tm.plugin.webmining: Retrieve Structured, Textual Data from Various Web Sources},
	url = {https://CRAN.R-project.org/package=tm.plugin.webmining},
	year = {2015},
	bdsk-url-1 = {https://CRAN.R-project.org/package=tm.plugin.webmining}}

@manual{r-cite,
	address = {Vienna, Austria},
	author = {{R Core Team}},
	date-added = {2018-01-07 20:31:46 +0000},
	date-modified = {2018-01-07 20:32:07 +0000},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2017},
	bdsk-url-1 = {https://www.R-project.org/}}

@url{usda,
	author = {{United States Department of Agriculture Economic Research Service}},
	date-added = {2018-01-06 01:52:41 +0000},
	date-modified = {2018-01-11 20:19:16 +0000},
	lastchecked = {2017},
	title = {2013 Rural-Urban Continuum Codes},
	url = {https://www.ers.usda.gov/data-products/rural-urban-continuum-codes.aspx},
	year = {2013},
	bdsk-url-1 = {https://www.ers.usda.gov/data-products/rural-urban-continuum-codes.aspx}}

@article{tibshirani-2014,
	author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
	date-added = {2018-01-06 01:49:11 +0000},
	date-modified = {2018-01-06 01:49:26 +0000},
	fjournal = {The Annals of Statistics},
	issn = {0090-5364},
	journal = {Ann. Statist.},
	mrclass = {62J05 (62F03 62J07)},
	mrnumber = {3210970},
	number = {2},
	pages = {413--468},
	title = {A significance test for the lasso},
	url = {https://doi.org/10.1214/13-AOS1175},
	volume = {42},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1214/13-AOS1175}}

@article{tibshirani-2016,
	author = {Tibshirani, Ryan J. and Taylor, Jonathan and Lockhart, Richard and Tibshirani, Robert},
	date-added = {2018-01-06 01:47:49 +0000},
	date-modified = {2018-01-06 01:48:05 +0000},
	fjournal = {Journal of the American Statistical Association},
	issn = {0162-1459},
	journal = {J. Amer. Statist. Assoc.},
	mrclass = {62H15 (62-04 62F25 62J07 62L10)},
	mrnumber = {3538689},
	mrreviewer = {Gabriela Ciuperca},
	number = {514},
	pages = {600--620},
	title = {Exact post-selection inference for sequential regression procedures},
	url = {https://doi.org/10.1080/01621459.2015.1108848},
	volume = {111},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2015.1108848}}

@article{javanmard,
	author = {Javanmard, Adel and Montanari, Andrea},
	date-added = {2018-01-06 01:43:56 +0000},
	date-modified = {2018-01-06 01:44:11 +0000},
	fjournal = {Journal of Machine Learning Research (JMLR)},
	issn = {1532-4435},
	journal = {J. Mach. Learn. Res.},
	mrclass = {62J07 (62F12 62F25)},
	mrnumber = {3277152},
	mrreviewer = {Zaixing Li},
	pages = {2869--2909},
	title = {Confidence intervals and hypothesis testing for high-dimensional regression},
	volume = {15},
	year = {2014}}

@article{vandegeer,
	author = {van de Geer, Sara and B\"uhlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
	date-added = {2018-01-06 01:42:14 +0000},
	date-modified = {2018-01-06 01:42:25 +0000},
	fjournal = {The Annals of Statistics},
	issn = {0090-5364},
	journal = {Ann. Statist.},
	mrclass = {62J07 (62F12 62F25 62J12)},
	mrnumber = {3224285},
	mrreviewer = {Hiroto Hyakutake},
	number = {3},
	pages = {1166--1202},
	title = {On asymptotically optimal confidence regions and tests for high-dimensional models},
	url = {https://doi.org/10.1214/14-AOS1221},
	volume = {42},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1214/14-AOS1221}}

@electronic{grammakov,
	author = {{grammakov (GitHub user)}},
	date-added = {2018-01-06 01:39:47 +0000},
	date-modified = {2018-01-11 20:19:45 +0000},
	lastchecked = {2017},
	title = {US Cities, Counties and States (data set)},
	url = {https://github.com/grammakov/USA-cities- and-states},
	year = {2014},
	bdsk-url-1 = {https://github.com/grammakov/USA-cities-%20and-states}}

@book{hastie,
	added-at = {2008-05-16T16:17:42.000+0200},
	address = {New York, NY, USA},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
	date-added = {2018-01-06 01:36:13 +0000},
	date-modified = {2018-01-06 01:37:36 +0000},
	edition = {2nd ed},
	interhash = {d585aea274f2b9b228fc1629bc273644},
	intrahash = {f58afc5c9793fcc8ad8389824e57984c},
	keywords = {ml statistics},
	publisher = {Springer New York Inc.},
	series = {Springer Series in Statistics},
	timestamp = {2008-05-16T16:17:43.000+0200},
	title = {The Elements of Statistical Learning},
	year = 2017}

@unpublished{dube-misra,
	author = {Dub{\'e}, J. and Misra, S},
	date-added = {2018-01-06 01:32:02 +0000},
	date-modified = {2018-01-06 01:34:48 +0000},
	month = {August},
	note = {https://ssrn.com/abstract=2992257},
	title = {Scalable Price Targeting},
	year = {2017}}

@article{chatterjee,
	author = {Chatterjee, A. and Lahiri, S. N.},
	date-added = {2018-01-06 01:31:07 +0000},
	date-modified = {2018-01-06 01:31:15 +0000},
	fjournal = {The Annals of Statistics},
	issn = {0090-5364},
	journal = {Ann. Statist.},
	mrclass = {62J07 (62E20 62G09)},
	mrnumber = {3113809},
	number = {3},
	pages = {1232--1259},
	title = {Rates of convergence of the adaptive {LASSO} estimators to the oracle distribution and higher order refinements by the bootstrap},
	url = {https://doi.org/10.1214/13-AOS1106},
	volume = {41},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1214/13-AOS1106}}

@url{bea-regions,
	author = {{Bureau of Economic Analysis, US Department of Commerce}},
	date-added = {2018-01-06 01:28:06 +0000},
	date-modified = {2018-01-11 20:20:10 +0000},
	lastchecked = {2017},
	title = {Component state list for BEA regions},
	url = {https://www.bea.gov/regional/methods.cfm},
	year = {2017},
	bdsk-url-1 = {https://www.bea.gov/regional/methods.cfm}}

@book{angrist_mostly_2008,
	added-at = {2014-11-20T12:13:56.000+0100},
	author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
	biburl = {https://www.bibsonomy.org/bibtex/2bad3c0e83619dbdc442142ce714e5529/rlipp},
	date-added = {2018-01-06 01:25:16 +0000},
	date-modified = {2018-01-06 02:12:18 +0000},
	interhash = {6d41194af2b0685c2adb94316d607d56},
	intrahash = {bad3c0e83619dbdc442142ce714e5529},
	isbn = {0691120358},
	keywords = {imported},
	publisher = {Princeton University Press},
	shorttitle = {Mostly Harmless Econometrics},
	timestamp = {2014-11-20T12:13:56.000+0100},
	title = {Mostly Harmless Econometrics: An Empiricist's Companion},
	year = 2008}

@article{adalasso2017,
	author = {Audrino, Francesco and Camponovo, Lorenzo},
	date-added = {2018-01-06 00:47:27 +0000},
	date-modified = {2018-01-06 01:18:05 +0000},
	journal = {Journal of Time Series Analysis},
	month = {11},
	title = {Oracle Properties, Bias Correction, and Bootstrap Inference for Adaptive Lasso for Time Series M-Estimators},
	year = {2017}}

@article{adalasso,
	author = {Zou, Hui},
	date-added = {2018-01-06 00:42:43 +0000},
	date-modified = {2018-01-06 00:47:37 +0000},
	fjournal = {Journal of the American Statistical Association},
	issn = {0162-1459},
	journal = {J. Amer. Statist. Assoc.},
	mrclass = {62F07 (62F12 62F35 62J02 62J12)},
	mrnumber = {2279469},
	mrreviewer = {Alexander G. Kukush},
	number = {476},
	pages = {1418--1429},
	title = {The adaptive lasso and its oracle properties},
	url = {https://doi.org/10.1198/016214506000000735},
	volume = {101},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/016214506000000735}}
