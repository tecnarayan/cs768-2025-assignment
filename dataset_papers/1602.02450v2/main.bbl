\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Lehmann and Casella(1998)]{lcTO}
E.~L. Lehmann and G.~Casella.
\newblock \emph{Theory of point estimation}, volume~31.
\newblock Springer Science \& Business Media, 1998.

\bibitem[Kakade et~al.(2009)Kakade, Sridharan, and Tewari]{kstOT}
S.~M. Kakade, K.~Sridharan, and A.~Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In \emph{NIPS*23}, 2009.

\bibitem[du~Plessis et~al.(2015)du~Plessis, Niu, and Sugiyama]{pnsCF}
M~C. du~Plessis, G.~Niu, and M.~Sugiyama.
\newblock Convex formulation for learning from positive and unlabeled data.
\newblock In \emph{32$^{~th}$~ICML}, pages 1386--1394, 2015.

\bibitem[Garc{\i}a-Garc{\i}a and Williamson(2011)]{ggwDO}
D.~Garc{\i}a-Garc{\i}a and R.~C. Williamson.
\newblock Degrees of supervision.
\newblock In \emph{NIPS*25 workshop on Relations between machine learning
  problems}, 2011.

\bibitem[Hernandez-Gonzalez et~al.(2016)Hernandez-Gonzalez, Inza, and
  Lozano]{hgilWS}
J.~Hernandez-Gonzalez, I.~Inza, and J.A. Lozano.
\newblock Weak supervision and other non-standard classification problems: a
  taxonomy.
\newblock In \emph{Pattern Recognition Letters}, pages 49--55. Elsevier, 2016.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{ndrtLW}
N.~Natarajan, I.~S. Dhillon, P.~K. Ravikumar, and A.~Tewari.
\newblock Learning with noisy labels.
\newblock In \emph{NIPS*27}, 2013.

\bibitem[Chapelle et~al.(2006)Chapelle, Sch{\"o}lkopf, and Zien]{cbzSS}
O.~Chapelle, B.~Sch{\"o}lkopf, and A.~Zien.
\newblock \emph{Semi-supervised learning}.
\newblock MIT press Cambridge, 2006.

\bibitem[Dietterich et~al.(1997)Dietterich, Lathrop, and
  Lozano-P{\'e}rez]{dllST}
T.~G. Dietterich, R.~H. Lathrop, and T.~Lozano-P{\'e}rez.
\newblock Solving the multiple instance problem with axis-parallel rectangles.
\newblock \emph{Artificial Intelligence}, 89:\penalty0 31--71, 1997.

\bibitem[Quadrianto et~al.(2009)Quadrianto, Smola, Caetano, and Le]{qsclEL}
N.~Quadrianto, A.~J. Smola, T.~S. Caetano, and Q.~V. Le.
\newblock Estimating labels from label proportions.
\newblock \emph{JMLR}, 10:\penalty0 2349--2374, 2009.

\bibitem[Joulin and Bach(2012)]{jbAC}
A.~Joulin and F.~R. Bach.
\newblock A convex relaxation for weakly supervised classifiers.
\newblock In \emph{29$^{~th}$~ICML}, 2012.

\bibitem[Patrini et~al.(2014)Patrini, Nock, Rivera, and Caetano]{pnrcAN}
G.~Patrini, R.~Nock, P.~Rivera, and T.~Caetano.
\newblock {(Almost) no label no cry}.
\newblock In \emph{NIPS*28}, 2014.

\bibitem[van Rooyen et~al.(2015)van Rooyen, Menon, and Williamson]{rmwLW}
B.~van Rooyen, A.~K. Menon, and R.~C. Williamson.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{NIPS*29}, 2015.

\bibitem[Gao et~al.(2016)Gao, Wang, Li, and Zhou]{gwlzRM}
W.~Gao, L.~Wang, Y.-F. Li, and Z.-H Zhou.
\newblock Risk minimization in the presence of label noise.
\newblock In \emph{Proc.\ of the 30$^{th}$ AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem[Long and Servedio(2010)]{lsRC}
P.~M. Long and R.~A. Servedio.
\newblock Random classification noise defeats all convex potential boosters.
\newblock \emph{Machine learning}, 78\penalty0 (3):\penalty0 287--304, 2010.

\bibitem[Stempfel and Ralaivola(2009)]{srLS}
G.~Stempfel and L.~Ralaivola.
\newblock Learning {SVM}s from sloppily labeled data.
\newblock In \emph{Artificial Neural Networks (ICANN)}, pages 884--893.
  Springer, 2009.

\bibitem[Masnadi-Shirazi et~al.(2010)Masnadi-Shirazi, Mahadevan, and
  Vasconcelos]{mmvOT}
H.~Masnadi-Shirazi, V.~Mahadevan, and N.~Vasconcelos.
\newblock On the design of robust classifiers for computer vision.
\newblock In \emph{Proc.\ of the 23$^{rd}$ IEEE CVPR}, pages 779--786. IEEE,
  2010.

\bibitem[Ding and Vishwanathan(2010)]{dvTL}
N.~Ding and S.~V.~N. Vishwanathan.
\newblock t-logistic regression.
\newblock In \emph{NIPS*24}, pages 514--522, 2010.

\bibitem[Patrini et~al.(2015)Patrini, Nielsen, and Nock]{pnnBW}
G.~Patrini, F.~Nielsen, and R.~Nock.
\newblock Bridging weak supervision and privacy aware learning via sufficient
  statistics.
\newblock \emph{NIPS*29, Workshop on learning and privacy with incomplete data
  and weak supervision}, 2015.

\bibitem[Chaudhuri and Hsu(2011)]{khSC}
K.~Chaudhuri and D.~Hsu.
\newblock Sample complexity bounds for differentially private learning.
\newblock In \emph{JMLR}, volume 2011, page 155, 2011.

\bibitem[Reid and Williamson(2010)]{rwCB}
M.~D. Reid and R.~C. Williamson.
\newblock Composite binary losses.
\newblock \emph{JMLR}, 11:\penalty0 2387--2422, 2010.

\bibitem[Smola et~al.(2007)Smola, Gretton, Song, and Sch{\"o}lkopf]{sgssAH}
A.~Smola, A.~Gretton, L.~Song, and B.~Sch{\"o}lkopf.
\newblock A {Hilbert} space embedding for distributions.
\newblock In \emph{Algorithmic Learning Theory}, pages 13--31. Springer, 2007.

\bibitem[Nock and Nielsen(2009)]{nnBD}
R.~Nock and F.~Nielsen.
\newblock Bregman divergences and surrogates for learning.
\newblock \emph{IEEE Trans.PAMI}, 31:\penalty0 2048--2059, 2009.

\bibitem[Jaakkola and Jordan(2000)]{jjBP}
T.~S. Jaakkola and M.~I. Jordan.
\newblock Bayesian parameter estimation via variational methods.
\newblock \emph{Statistics and Computing}, 10\penalty0 (1):\penalty0 25--37,
  2000.

\bibitem[Kearns and Mansour(1996)]{kmOT}
M.~J. Kearns and Y.~Mansour.
\newblock On the boosting ability of top-down decision tree learning
  algorithms.
\newblock In \emph{28$^{~th}$ ACM STOC}, pages 459--468, 1996.

\bibitem[Masnadi-Shirazi(2011)]{msTD}
H.~Masnadi-Shirazi.
\newblock \emph{The design of bayes consistent loss functions for
  classification}.
\newblock PhD thesis, University of California at San Diego, 2011.

\bibitem[Bartlett and Mendelson(2002)]{bmRA}
P.-L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{JMLR}, 3:\penalty0 463--482, 2002.

\bibitem[Altun and Smola(2006)]{asUD}
Y.~Altun and A.~J. Smola.
\newblock Unifying divergence minimization and statistical inference via convex
  duality.
\newblock In \emph{19$^{~th}$ COLT}, 2006.

\bibitem[Shalev-Shwartz et~al.(2011)Shalev-Shwartz, Singer, Srebro, and
  Cotter]{sssscPP}
S.~Shalev-Shwartz, Y.~Singer, N.~Srebro, and A.~Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock \emph{Mathematical programming}, 127\penalty0 (1):\penalty0 3--30,
  2011.

\bibitem[Ghosh et~al.(2015)Ghosh, Manwani, and Sastry]{gmsMR}
A.~Ghosh, N.~Manwani, and P.~S. Sastry.
\newblock Making risk minimization tolerant to label noise.
\newblock \emph{Neurocomputing}, 160:\penalty0 93--107, 2015.

\bibitem[Sukhbaatar and Fergus(2014)]{sfLF}
S.~Sukhbaatar and R.~Fergus.
\newblock Learning from noisy labels with deep neural networks.
\newblock \emph{arXiv:1406.2080}, 2014.

\bibitem[Bootkrajang and Kab{\'a}n(2012)]{bkLN}
J.~Bootkrajang and A.~Kab{\'a}n.
\newblock Label-noise robust logistic regression and its applications.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases},
  pages 143--158. Springer, 2012.

\bibitem[Liu and Tao(2014)]{ltCW}
T.~Liu and D.~Tao.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{arXiv:1411.7718}, 2014.

\bibitem[Menon et~al.(2015)Menon, Rooyen, Ong, and Williamson]{mvrowLF}
A.~Menon, B.~Van Rooyen, C.~S. Ong, and B.~Williamson.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{32$^{~th}$~ICML}, 2015.

\bibitem[Scott(2015)]{sAR}
C.~Scott.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{18$^{th}$ AISTATS}, 2015.

\bibitem[Bache and Lichman(2013)]{blUM}
K.~Bache and M.~Lichman.
\newblock {UCI} machine learning repository, 2013.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Song et~al.(2009)Song, Huang, Smola, and Fukumizu]{shsfHS}
L.~Song, J.~Huang, A.~J. Smola, and K.~Fukumizu.
\newblock Hilbert space embeddings of conditional distributions with
  applications to dynamical systems.
\newblock In \emph{26$^{~th}$~ICML}. ACM, 2009.

\bibitem[Sch{\"o}lkopf and Smola(2002)]{ssLW}
B.~Sch{\"o}lkopf and A.~J. Smola.
\newblock \emph{Learning with kernels: Support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Beygelzimer et~al.(2015)Beygelzimer, III, Langford, and
  Mineiro]{bdlmLR}
A.~Beygelzimer, H.~Daum{\'e} III, J.~Langford, and P.~Mineiro.
\newblock Learning reductions that really work.
\newblock \emph{arXiv:1502.02704}, 2015.

\bibitem[Beygelzimer et~al.(2005)Beygelzimer, Dani, Hayes, Langford, and
  Zadrozny]{bdhlzEL}
A.~Beygelzimer, V.~Dani, T.~Hayes, J.~Langford, and B.~Zadrozny.
\newblock Error limiting reductions between classification tasks.
\newblock In \emph{22$^{~th}$~ICML}, pages 49--56, 2005.

\bibitem[Schmidt et~al.(2013)Schmidt, Roux, and Bach]{srbMF}
M.~Schmidt, N.~L. Roux, and F.~Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{arXiv:1309.2388}, 2013.

\bibitem[Bach et~al.(2012)Bach, Jenatton, Mairal, and Obozinski]{bjmoOW}
F.~Bach, R.~Jenatton, J.~Mairal, and G.~Obozinski.
\newblock Optimization with sparsity-inducing penalties.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (1):\penalty0 1--106, 2012.

\bibitem[McDiarmid(1998)]{mdC}
C.~McDiarmid.
\newblock Concentration.
\newblock In M.~Habib, C.~McDiarmid, J.~Ramirez-Alfonsin, and B.~Reed, editors,
  \emph{Probabilistic Methods for Algorithmic Discrete Mathematics}, pages
  1--54. Springer Verlag, 1998.

\bibitem[Manwani and Sastry(2013)]{msNT}
N.~Manwani and P.~S. Sastry.
\newblock Noise tolerance under risk minimization.
\newblock \emph{Cybernetics, IEEE Transactions on}, 43\penalty0 (3):\penalty0
  1146--1151, 2013.

\end{thebibliography}
