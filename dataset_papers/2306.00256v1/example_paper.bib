@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{you2018imagenet,
  title={Imagenet training in minutes},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  booktitle={Proceedings of the 47th International Conference on Parallel Processing},
  pages={1--10},
  year={2018}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2117--2125},
  year={2017}
}

@article{chen2019mmdetection,
  title={MMDetection: Open mmlab detection toolbox and benchmark},
  author={Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Xu, Jiarui and others},
  journal={arXiv preprint arXiv:1906.07155},
  year={2019}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  number={2},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@article{ren2016faster,
  title={Faster R-CNN: towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016},
  publisher={IEEE}
}

@inproceedings{you2019large,
  title={Large-batch training for LSTM and beyond},
  author={You, Yang and Hseu, Jonathan and Ying, Chris and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2019}
}

%% baba %% 
@inproceedings{zhang2018visual,
  title={Visual search at alibaba},
  author={Zhang, Yanhao and Pan, Pan and Zheng, Yun and Zhao, Kang and Zhang, Yingya and Ren, Xiaofeng and Jin, Rong},
  booktitle={Proceedings of the 24th International Conference on Knowledge Discovery and Data Mining (SIGKDD)},
  pages={993--1001},
  year={2018}
}

@inproceedings{zhao2019large,
  title={Large-Scale Visual Search with Binary Distributed Graph at Alibaba},
  author={Zhao, Kang and Pan, Pan and Zheng, Yun and Zhang, Yanhao and Wang, Changxu and Zhang, Yingya and Xu, Yinghui and Jin, Rong},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM)},
  pages={2567--2575},
  year={2019}
}

@inproceedings{song2020large,
  title={Large-Scale Training System for 100-Million Classification at Alibaba},
  author={Song, Liuyihan and Pan, Pan and Zhao, Kang and Yang,  Hao and Chen, Yiming and Zhang, Yingya and Xu, Yinghui and Jin,  Rong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining (SIGKDD)},
  year={2020}
}

%% local sgd %% 
@article{lin2018don,
  title={Don't Use Large Mini-Batches, Use Local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@inproceedings{stich2019local,
  title={Local SGD Converges Fast and Communicates Little},
  author={Stich, Sebastian Urban},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{yu2019parallel,
  title={Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5693--5700},
  year={2019}
}

%% Fast ImageNet Training %%

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016}
}

@article{mikami2018massively,
  title={Massively distributed SGD: ImageNet/ResNet-50 training in a flash},
  author={Mikami, Hiroaki and Suganuma, Hisahiro and Tanaka, Yoshiki and Kageyama, Yuichi and others},
  journal={arXiv preprint arXiv:1811.05233},
  year={2018}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{jia2018highly,
  title={Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes},
  author={Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and others},
  journal={arXiv preprint arXiv:1807.11205},
  year={2018}
}

@article{akiba2017extremely,
  title={Extremely large minibatch SGD: training resnet-50 on imagenet in 15 minutes},
  author={Akiba, Takuya and Suzuki, Shuji and Fukuda, Keisuke},
  journal={arXiv preprint arXiv:1711.04325},
  year={2017}
}


@article{sun2019optimizing,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Sun, Peng and Feng, Wansen and Han, Ruobing and Yan, Shengen and Wen, Yonggang},
  journal={arXiv preprint arXiv:1902.06855},
  year={2019}
}

@article{you2017scaling,
  title={Scaling sgd batch size to 32k for imagenet training},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  volume={6},
  year={2017}
}


%% communication related (method, compression, gossip) %%
@inproceedings{karimireddy2019error,
  title={Error Feedback Fixes SignSGD and other Gradient Compression Schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={3252--3261},
  year={2019}
}

@inproceedings{vogels2019powersgd,
  title={PowerSGD: Practical low-rank gradient compression for distributed optimization},
  author={Vogels, Thijs and Karimireddy, Sai Praneeth and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={14236--14245},
  year={2019}
}

@article{bernstein2018signsgd,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{aji2017sparse,
  title={Sparse Communication for Distributed Gradient Descent},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={440--445},
  year={2017}
}
@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

@article{gibiansky2017bringing,
  title={Bringing HPC techniques to deep learning},
  author={Gibiansky, Andrew},
  journal={Baidu Research, Tech. Rep.},
  year={2017}
}

@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}

%% ML %%
@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}


%% others %%
@article{patarasuk2009bandwidth,
  title={Bandwidth optimal all-reduce algorithms for clusters of workstations},
  author={Patarasuk, Pitch and Yuan, Xin},
  journal={Journal of Parallel and Distributed Computing},
  volume={69},
  number={2},
  pages={117--124},
  year={2009},
  publisher={Elsevier}
}

@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={8024--8035},
  year={2019}
}

@article{chen2015mxnet,
  title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={181--196},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{wang2019characterizing,
  title={Characterizing deep learning training workloads on alibaba-pai},
  author={Wang, Mengdi and Meng, Chen and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei and Jia, Yangqing},
  booktitle={2019 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={189--202},
  year={2019},
  organization={IEEE}
}

@inproceedings{dong2020eflops,
  title={EFLOPS: Algorithm and System Co-Design for a High Performance Distributed Training Platform},
  author={Dong, Jianbo and Cao, Zheng and Zhang, Tao and Ye, Jianxi and Wang, Shaochuang and Feng, Fei and Zhao, Li and Liu, Xiaoyong and Song, Liuyihan and Peng, Liwei and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={610--622},
  year={2020},
  organization={IEEE}
}

%% gossip %%
@inproceedings{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? {A} case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5330--5340},
  year={2017}
}

@inproceedings{lian2018asynchronous,
  title={Asynchronous decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={3043--3052},
  year={2018}
}

@inproceedings{assran2019stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={344--353},
  year={2019}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized sgd with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian U},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1--12},
  year={2020}
}

@inproceedings{luo2020prague,
  title={Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training},
  author={Luo, Qinyi and He, Jiaao and Zhuo, Youwei and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={401--416},
  year={2020}
}

@article{you2019largeb,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

%% Decentralized Optimization Algorithms %%
@article{nedic2009distributed,
  title={Distributed subgradient methods for multi-agent optimization},
  author={Nedic, Angelia and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  volume={54},
  number={1},
  pages={48--61},
  year={2009},
  publisher={IEEE}
}

@article{tsitsiklis1986distributed,
  title={Distributed asynchronous deterministic and stochastic gradient optimization algorithms},
  author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
  journal={IEEE transactions on automatic control},
  volume={31},
  number={9},
  pages={803--812},
  year={1986},
  publisher={IEEE}
}

@article{yuan2016convergence,
  title={On the convergence of decentralized gradient descent},
  author={Yuan, Kun and Ling, Qing and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={3},
  pages={1835--1854},
  year={2016},
  publisher={SIAM}
}

@article{chen2012diffusion,
  title={Diffusion adaptation strategies for distributed optimization and learning over networks},
  author={Chen, Jianshu and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={60},
  number={8},
  pages={4289--4305},
  year={2012},
  publisher={IEEE}
}

@article{duchi2011dual,
  title={Dual averaging for distributed optimization: Convergence analysis and network scaling},
  author={Duchi, John C and Agarwal, Alekh and Wainwright, Martin J},
  journal={IEEE Transactions on Automatic control},
  volume={57},
  number={3},
  pages={592--606},
  year={2011},
  publisher={IEEE}
}

@article{shi2014linear,
  title={On the linear convergence of the ADMM in decentralized consensus optimization},
  author={Shi, Wei and Ling, Qing and Yuan, Kun and Wu, Gang and Yin, Wotao},
  journal={IEEE Transactions on Signal Processing},
  volume={62},
  number={7},
  pages={1750--1761},
  year={2014},
  publisher={IEEE}
}

@article{shi2015extra,
	Author = {Shi, W. and Ling, Q. and Wu, G. and Yin, W.},
	Date-Added = {2016-04-10 02:05:10 +0000},
	Date-Modified = {2016-04-16 07:10:03 +0000},
	Journal = {SIAM Journal on Optimization},
	Number = {2},
	Pages = {944--966},
	Publisher = {SIAM},
	Title = {{EXTRA}: An exact first-order algorithm for decentralized consensus optimization},
	Volume = {25},
	Year = {2015}}
	
@article{yuan2017exact1,
	Author = {Yuan, K. and Ying, B. and Zhao, X. and Sayed, A. H.},
	Date-Added = {2017-01-31 19:33:45 +0000},
	Date-Modified = {2019-01-25 22:50:00 +0800},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {3},
	Pages = {708 -- 723},
	Title = {Exact dffusion for distributed optimization and learning -- {Part I: Algorithm development}},
	Volume = {67},
	Year = {2019}}
	
@article{li2017decentralized,
	Author = {Li, Z. and Shi, W. and Yan, M.},
	Date-Added = {2019-03-10 01:04:28 -0800},
	Date-Modified = {2019-03-10 01:04:57 -0800},
	Journal = {IEEE Transactions on Signal Processing},
	Month = {July},
	Note = {early acces. Also available on arXiv:1704.07807.},
	Title = {A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates},
	Year = {2019}}
	
@inproceedings{xu2015augmented,
	Address = {Osaka, Japan},
	Author = {Xu, J. and Zhu, S. and Soh, Y. C. and Xie, L.},
	Booktitle = {IEEE Conference on Decision and Control (CDC)},
	Date-Added = {2016-11-06 06:15:54 +0000},
	Date-Modified = {2017-02-05 23:02:41 +0000},
	Pages = {2055--2060},
	Title = {Augmented distributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes},
	Year = {2015}}
	
@article{di2016next,
	Author = {Di Lorenzo, P. and Scutari, G.},
	Date-Added = {2019-03-08 21:51:40 -0800},
	Date-Modified = {2019-03-08 21:51:58 -0800},
	Journal = {IEEE Transactions on Signal and Information Processing over Networks},
	Number = {2},
	Pages = {120--136},
	Publisher = {IEEE},
	Title = {Next: In-network nonconvex optimization},
	Volume = {2},
	Year = {2016}}
	
@article{nedic2017achieving,
	Author = {Nedic, A. and Olshevsky, A. and Shi, W.},
	Date-Added = {2019-03-13 16:13:43 -0700},
	Date-Modified = {2019-03-13 16:14:21 -0700},
	Journal = {SIAM Journal on Optimization},
	Number = {4},
	Pages = {2597--2633},
	Publisher = {SIAM},
	Title = {Achieving geometric convergence for distributed optimization over time-varying graphs},
	Volume = {27},
	Year = {2017}}
	
@article{qu2018harnessing,
	Author = {Qu, G. and Li, N.},
	Date-Added = {2019-03-10 01:15:38 -0800},
	Date-Modified = {2019-03-10 01:15:55 -0800},
	Journal = {IEEE Transactions on Control of Network Systems},
	Number = {3},
	Pages = {1245--1260},
	Publisher = {IEEE},
	Title = {Harnessing smoothness to accelerate distributed optimization},
	Volume = {5},
	Year = {2018}}
	
@inproceedings{scaman2017optimal,
  title={Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={International Conference on Machine Learning},
  pages={3027--3036},
  year={2017}
}

@inproceedings{scaman2018optimal,
  title={Optimal algorithms for non-smooth distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2740--2749},
  year={2018}
}

@article{uribe2020dual,
  title={A dual approach for optimal algorithms in distributed optimization over networks},
  author={Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  journal={Optimization Methods and Software},
  pages={1--40},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{pu2020asymptotic,
  title={Asymptotic Network Independence in Distributed Stochastic Optimization for Machine Learning: Examining Distributed and Centralized Stochastic Gradient Descent},
  author={Pu, Shi and Olshevsky, Alex and Paschalidis, Ioannis Ch},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={114--122},
  year={2020},
  publisher={IEEE}
}

@inproceedings{tang2018d,
  title={$ D^2$: Decentralized Training over Decentralized Data},
  author={Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={4848--4856},
  year={2018}
}

@article{yuan2020influence,
  title={On the influence of bias-correction on distributed stochastic optimization},
  author={Yuan, Kun and Alghunaim, Sulaiman A and Ying, Bicheng and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  year={2020},
  publisher={IEEE}
}

@article{xin2020improved,
  title={An improved convergence analysis for decentralized online stochastic non-convex optimization},
  author={Xin, Ran and Khan, Usman A and Kar, Soummya},
  journal={arXiv preprint arXiv:2008.04195},
  year={2020}
}

@inproceedings{he2018cola,
  title={Cola: Decentralized linear learning},
  author={He, Lie and Bian, An and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4536--4546},
  year={2018}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}

@inproceedings{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={6155--6165},
  year={2019},
  organization={PMLR}
}

@inproceedings{koloskova2019decentralized,
  title={Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication},
  author={Koloskova, Anastasia and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3478--3487},
  year={2019}
}

@inproceedings{koloskova2019decentralized2,
  title={Decentralized Deep Learning with Arbitrary Communication Compression},
  author={Koloskova, Anastasia and Lin, Tao and Stich, Sebastian U and Jaggi, Martin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{chen2018lag,
  title={{LAG}: Lazily aggregated gradient for communication-efficient distributed learning},
  author={Chen, Tianyi and Giannakis, Georgios and Sun, Tao and Yin, Wotao},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5050--5060},
  year={2018}
}

@article{liu2019communication,
  title={Communication-censored ADMM for decentralized consensus optimization},
  author={Liu, Yaohua and Xu, Wei and Wu, Gang and Tian, Zhi and Ling, Qing},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={10},
  pages={2565--2579},
  year={2019},
  publisher={IEEE}
}

@article{jiang2020adaptive,
  title={Adaptive Periodic Averaging: A Practical Approach to Reducing Communication in Distributed Learning},
  author={Jiang, Peng and Agrawal, Gagan},
  journal={arXiv preprint arXiv:2007.06134},
  year={2020}
}

@inproceedings{haddadpour2019local,
  title={Local SGD with periodic averaging: Tighter analysis and adaptive synchronization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11082--11094},
  year={2019}
}

@inproceedings{wang2019adaptive,
  title={Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-update SGD},
  author={Wang, Jianyu and Joshi, Gauri},
  booktitle={Systems and Machine Learning (SysML) Conference},
  year={2019}
}

@article{ben2019demystifying,
  title={Demystifying parallel and distributed deep learning: An in-depth concurrency analysis},
  author={Ben-Nun, Tal and Hoefler, Torsten},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={4},
  pages={1--43},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
  booktitle={Advances in neural information processing systems},
  pages={2595--2603},
  year={2010}
}

@article{zhang2016parallel,
  title={Parallel SGD: When does averaging help?},
  author={Zhang, Jian and De Sa, Christopher and Mitliagkas, Ioannis and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1606.07365},
  year={2016}
}

@inproceedings{bayoumi2020tighter,
  title={Tighter Theory for Local SGD on Identical and Heterogeneous Data},
  author={Bayoumi, Ahmed Khaled Ragab and Mishchenko, Konstantin and Richtarik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@inproceedings{li2019convergence,
  title={On the Convergence of FedAvg on Non-IID Data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{berahas2018balancing,
  title={Balancing communication and computation in distributed optimization},
  author={Berahas, Albert S and Bollapragada, Raghu and Keskar, Nitish Shirish and Wei, Ermin},
  journal={IEEE Transactions on Automatic Control},
  volume={64},
  number={8},
  pages={3141--3155},
  year={2018},
  publisher={IEEE}
}

@inproceedings{lu2019gnsd,
  title={GNSD: A gradient-tracking based nonconvex stochastic algorithm for decentralized optimization},
  author={Lu, Songtao and Zhang, Xinwei and Sun, Haoran and Hong, Mingyi},
  booktitle={2019 IEEE Data Science Workshop (DSW)},
  pages={315--321},
  year={2019},
  organization={IEEE}
}

@article{li2019communication,
  title={Communication efficient decentralized training with multiple local updates},
  author={Li, Xiang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1910.09126},
  year={2019}
}

@inproceedings{lin2021quasi,
  title={Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data},
  author={Lin, Tao and Karimireddy, Sai Praneeth and Stich, Sebastian U and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@inproceedings{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle={International Conference on Machine Learning},
  pages={7184--7193},
  year={2019},
  organization={PMLR}
}

@article{gao2020periodic,
  title={Periodic Stochastic Gradient Descent with Momentum for Decentralized Training},
  author={Gao, Hongchang and Huang, Heng},
  journal={arXiv preprint arXiv:2008.10435},
  year={2020}
}

@article{singh2020squarm,
  title={SQuARM-SGD: Communication-Efficient Momentum SGD for Decentralized Optimization},
  author={Singh, Navjot and Data, Deepesh and George, Jemin and Diggavi, Suhas},
  journal={arXiv preprint arXiv:2005.07041},
  year={2020}
}

@article{balu2020decentralized,
  title={Decentralized Deep Learning using Momentum-Accelerated Consensus},
  author={Balu, Aditya and Jiang, Zhanhong and Tan, Sin Yong and Hedge, Chinmay and Lee, Young M and Sarkar, Soumik},
  journal={arXiv preprint arXiv:2010.11166},
  year={2020}
}

@article{sayed2014adaptation,
  title={Adaptation, learning, and optimization over networks},
  author={Sayed, Ali H},
  journal={Foundations and Trends in Machine Learning},
  volume={7},
  number={ARTICLE},
  pages={311--801},
  year={2014},
  publisher={Now Publishers}
}

@inproceedings{kong2021consensus,
  title={Consensus Control for Decentralized Deep Learning},
  author={Kong, Lingjing and Lin, Tao and Koloskova, Anastasia and Jaggi, Martin and Stich, Sebastian U},
  booktitle={International Conference on Machine Learning},
  year={2021}
}



@article{loizou2020momentum,
  title={Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods},
  author={Loizou, Nicolas and Richt{\'a}rik, Peter},
  journal={Computational Optimization and Applications},
  volume={77},
  number={3},
  pages={653--710},
  year={2020},
  publisher={Springer}
}

@article{gitman2019understanding,
  title={Understanding the role of momentum in stochastic gradient methods},
  author={Gitman, Igor and Lang, Hunter and Zhang, Pengchuan and Xiao, Lin},
  journal={arXiv preprint arXiv:1910.13962},
  year={2019}
}

@article{yuan2016influence,
  title={On the influence of momentum acceleration on online learning},
  author={Yuan, Kun and Ying, Bicheng and Sayed, Ali H},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={6602--6667},
  year={2016},
  publisher={JMLR. org}
}

@article{liu2020improved,
  title={An improved analysis of stochastic gradient descent with momentum},
  author={Liu, Yanli and Gao, Yuan and Yin, Wotao},
  journal={arXiv preprint arXiv:2007.07989},
  year={2020}
}

@article{sebbouh2020convergence,
  title={On the convergence of the stochastic heavy ball method},
  author={Sebbouh, Othmane and Gower, Robert M and Defazio, Aaron},
  journal={arXiv preprint arXiv:2006.07867},
  year={2020}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{bluefog,
  author       = {Ying, Bicheng and Yuan, Kun and Hu, Hanbin and Chen, Yiming and Yin, Wotao },
  title        = {BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning},
  journal={arXiv preprint arXiv:2111.04287},
  year         = {2021},
}



@inproceedings{wang2019slowmo,
  title={SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum},
  author={Wang, Jianyu and Tantia, Vinayak and Ballas, Nicolas and Rabbat, Michael},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{yuan2018exact2,
  title={Exact diffusion for distributed optimization and learning—Part II: Convergence analysis},
  author={Yuan, Kun and Ying, Bicheng and Zhao, Xiaochuan and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={3},
  pages={724--739},
  year={2018},
  publisher={IEEE}
}

@article{yuan2021decentlam,
  title={{DecentLaM}: Decentralized Momentum {SGD} for Large-batch Deep Training},
  author={Yuan, Kun and Chen, Yiming and Huang, Xinmeng and Zhang, Yingya and Pan, Pan and Xu, Yinghui and Yin, Wotao},
  journal={arXiv preprint arXiv:2104.11981},
  year={2021}
}

@inproceedings{chen2021accelerating,
  title={Accelerating Gossip {SGD} with Periodic Global Averaging},
  author={Chen, Yiming and Yuan, Kun and Zhang, Yingya and Pan, Pan and Xu, Yinghui and Yin, Wotao},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{wang2019matcha,
  title={{MATCHA}: Speeding Up Decentralized {SGD} via Matching Decomposition Sampling},
  author={Wang, Jianyu and Sahu, Anit Kumar and Yang, Zhouyi and Joshi, Gauri and Kar, Soummya},
  journal={arXiv preprint arXiv:1905.09435},
  year={2019}
}


@article{gilbert1959random,
  title={Random graphs},
  author={Gilbert, Edgar N},
  journal={The Annals of Mathematical Statistics},
  volume={30},
  number={4},
  pages={1141--1144},
  year={1959},
  publisher={JSTOR}
}

@article{chung2011spectra,
  title={On the spectra of general random graphs},
  author={Chung, Fan and Radcliffe, Mary},
  journal={the electronic journal of combinatorics},
  pages={P215--P215},
  year={2011}
}

@article{mateos2010distributed,
  title={Distributed sparse linear regression},
  author={Mateos, Gonzalo and Bazerque, Juan Andr{\'e}s and Giannakis, Georgios B},
  journal={IEEE Transactions on Signal Processing},
  volume={58},
  number={10},
  pages={5262--5276},
  year={2010},
  publisher={IEEE}
}

@article{liu2020linear,
  title={Linear convergent decentralized optimization with compression},
  author={Liu, Xiaorui and Li, Yao and Wang, Rongrong and Tang, Jiliang and Yan, Ming},
  journal={arXiv preprint arXiv:2007.00232},
  year={2020}
}

@article{liu2021decentralized,
  title={Decentralized Learning With Lazy and Approximate Dual Gradients},
  author={Liu, Yanli and Sun, Yuejiao and Yin, Wotao},
  journal={IEEE Transactions on Signal Processing},
  volume={69},
  pages={1362--1377},
  year={2021},
  publisher={IEEE}
}

@article{sayed2014adaptive,
  title={Adaptive networks},
  author={Sayed, Ali H},
  journal={Proceedings of the IEEE},
  volume={102},
  number={4},
  pages={460--497},
  year={2014},
  publisher={IEEE}
}

@article{pu2019sharp,
  title={A sharp estimate on the transient time of distributed stochastic gradient descent},
  author={Pu, Shi and Olshevsky, Alex and Paschalidis, Ioannis Ch},
  journal={arXiv preprint arXiv:1906.02702},
  year={2019}
}

@article{nedic2014distributed,
  title={Distributed optimization over time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={60},
  number={3},
  pages={601--615},
  year={2014},
  publisher={IEEE}
}

@article{yuan2021removing,
  title={Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD},
  author={Yuan, Kun and Alghunaim, Sulaiman A},
  journal={arXiv preprint arXiv:2105.08023},
  year={2021}
}

@article{huang2021improving,
  title={Improving the Transient Times for Distributed Stochastic Gradient Methods},
  author={Huang, Kun and Pu, Shi},
  journal={arXiv preprint arXiv:2105.04851},
  year={2021}
}

@article{nedic2018network,
  title={Network topology and communication-computation tradeoffs in decentralized optimization},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex and Rabbat, Michael G},
  journal={Proceedings of the IEEE},
  volume={106},
  number={5},
  pages={953--976},
  year={2018},
  publisher={IEEE}
}

@inproceedings{chow2016expander,
  title={Expander graph and communication-efficient decentralized optimization},
  author={Chow, Yat-Tin and Shi, Wei and Wu, Tianyu and Yin, Wotao},
  booktitle={2016 50th Asilomar Conference on Signals, Systems and Computers},
  pages={1715--1720},
  year={2016},
  organization={IEEE}
}

@article{scutari2019distributed,
  title={Distributed nonconvex constrained optimization over time-varying digraphs},
  author={Scutari, Gesualdo and Sun, Ying},
  journal={Mathematical Programming},
  volume={176},
  number={1},
  pages={497--544},
  year={2019},
  publisher={Springer}
}

@article{gray2006toeplitz,
  title={Toeplitz and circulant matrices: A review},
  author={Gray, Robert M},
  year={2006},
  publisher={now publishers inc}
}

@article{Seneta1981nonnegative,
  title={Non-negative matrices and Markov chains (2nd ed.)},
  author={E. Seneta}, 
  year={1981} ,
  publisher={Springer Verlag, New York.}
}

@article{trevisan2017lecture,
  title={Lecture notes on graph partitioning, expanders and spectral methods},
  author={Trevisan, Luca},
  journal={University of California, Berkeley, https://people. eecs. berkeley. edu/\~{} luca/books/expanders-2016. pdf},
  year={2017}
}

@article{nachmias2008critical,
  title={Critical random graphs: diameter and mixing time},
  author={Nachmias, Asaf and Peres, Yuval},
  journal={The Annals of Probability},
  volume={36},
  number={4},
  pages={1267--1286},
  year={2008},
  publisher={Institute of Mathematical Statistics}
}

@article{benjamini2014mixing,
  title={The mixing time of the giant component of a random graph},
  author={Benjamini, Itai and Kozma, Gady and Wormald, Nicholas},
  journal={Random Structures \& Algorithms},
  volume={45},
  number={3},
  pages={383--407},
  year={2014},
  publisher={Wiley Online Library}
}

@article{beveridge2016best,
  title={The best mixing time for random walks on trees},
  author={Beveridge, Andrew and Youngblood, Jeanmarie},
  journal={Graphs and Combinatorics},
  volume={32},
  number={6},
  pages={2211--2239},
  year={2016},
  publisher={Springer}
}

@inproceedings{boyd2005mixing,
  title={Mixing Times for Random Walks on Geometric Random Graphs.},
  author={Boyd, Stephen P and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  booktitle={ALENEX/ANALCO},
  pages={240--249},
  year={2005}
}

@article{shi2015finite,
  title={Finite-time convergent gossiping},
  author={Shi, Guodong and Li, Bo and Johansson, Mikael and Johansson, Karl Henrik},
  journal={IEEE/ACM Transactions on Networking},
  volume={24},
  number={5},
  pages={2782--2794},
  year={2015},
  publisher={IEEE}
}

@article{alghunaim2021unified,
  title={A Unified and Refined Convergence Analysis for Non-Convex Decentralized Learning},
  author={Alghunaim, Sulaiman A and Yuan, Kun},
  journal={arXiv preprint arXiv:2110.09993},
  year={2021}
}

@inproceedings{lu2020decentralized,
  title={Decentralized Stochastic Non-Convex Optimization over Weakly Connected Time-Varying Digraphs},
  author={Lu, Songtao and Wu, Chai Wah},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5770--5774},
  year={2020},
  organization={IEEE}
}

@article{zhang2019decentralized,
  title={Decentralized stochastic gradient tracking for non-convex empirical risk minimization},
  author={Zhang, Jiaqi and You, Keyou},
  journal={arXiv preprint arXiv:1909.02712},
  year={2019}
}

@article{ying2021exponential,
  title={Exponential graph is provably efficient for decentralized deep training},
  author={Ying, Bicheng and Yuan, Kun and Chen, Yiming and Hu, Hanbin and Pan, Pan and Yin, Wotao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13975--13987},
  year={2021}
}

@article{lopes2008diffusion,
  title={Diffusion least-mean squares over adaptive networks: Formulation and performance analysis},
  author={Lopes, Cassio G and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={56},
  number={7},
  pages={3122--3136},
  year={2008},
  publisher={IEEE}
}

@article{dimakis2010gossip,
  title={Gossip algorithms for distributed signal processing},
  author={Dimakis, Alexandros G and Kar, Soummya and Moura, Jos{\'e} MF and Rabbat, Michael G and Scaglione, Anna},
  journal={Proceedings of the IEEE},
  volume={98},
  number={11},
  pages={1847--1864},
  year={2010},
  publisher={IEEE}
}

@article{bar1993optimal,
  title={An optimal algorithm for computing census functions in message-passing systems},
  author={Bar-Noy, Amotz and Kipnis, Shlomo and Schieber, Baruch},
  journal={Parallel Processing Letters},
  volume={3},
  number={01},
  pages={19--23},
  year={1993},
  publisher={World Scientific}
}

@article{lecun2010mnist,
  title={{MNIST} handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}


@article{song2022communication,
  title={Communication-Efficient Topologies for Decentralized Learning with $ O (1) $ Consensus Rate},
  author={Song, Zhuoqing and Li, Weijian and Jin, Kexin and Shi, Lei and Yan, Ming and Yin, Wotao and Yuan, Kun},
  journal={arXiv preprint arXiv:2210.07881},
  year={2022}
}

@inproceedings{niwa2021asynchronous,
  title={Asynchronous decentralized optimization with implicit stochastic variance reduction},
  author={Niwa, Kenta and Zhang, Guoqiang and Kleijn, W Bastiaan and Harada, Noboru and Sawada, Hiroshi and Fujino, Akinori},
  booktitle={International Conference on Machine Learning},
  pages={8195--8204},
  year={2021},
  organization={PMLR}
}


@inproceedings{rabenseifner2004optimization,
  title={Optimization of collective reduction operations},
  author={Rabenseifner, Rolf},
  booktitle={International Conference on Computational Science},
  volume={3036},
  pages={1--9},
  year={2004}
}

@inproceedings{rabenseifner2004more,
  title={More efficient reduction algorithms for non-power-of-two number of processors in message-passing parallel systems},
  author={Rabenseifner, Rolf and Tr{\"a}ff, Jesper Larsson},
  booktitle={Recent Advances in Parallel Virtual Machine and Message Passing Interface: 11th European PVM/MPI Users’ Group Meeting Budapest, Hungary, September 19-22, 2004. Proceedings 11},
  pages={36--46},
  year={2004},
  organization={Springer}
}

@inproceedings{tipparaju2003fast,
  title={Fast collective operations using shared and remote memory access protocols on clusters},
  author={Tipparaju, Vinod and Nieplocha, Jarek and Panda, Dhabaleswar},
  booktitle={Proceedings International Parallel and Distributed Processing Symposium},
  pages={10--pp},
  year={2003},
  organization={IEEE}
}

@article{verbraeken2020survey,
  title={A survey on distributed machine learning},
  author={Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S},
  journal={Acm computing surveys (csur)},
  volume={53},
  number={2},
  pages={1--33},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{jiang2020unified,
  title={A unified architecture for accelerating distributed DNN training in heterogeneous GPU/CPU clusters},
  author={Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
  booktitle={Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
  pages={463--479},
  year={2020}
}


@article{Krizhevsky09,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, A. and Hinton, G.},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009},
  publisher={Citeseer}
}

@misc{kuangliu,
  author       = "Kuang Liu",
  title        = "Train {CIFAR10} with {PyTorch}",
  howpublished = "\nolinkurl{https://github.com/kuangliu/pytorch-cifar}",
  note         = "Accessed: 2023-01",
  year = "2021"
}
