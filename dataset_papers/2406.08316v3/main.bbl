\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Lieberman(2001)]{lieberman2001your}
Henry Lieberman.
\newblock \emph{Your wish is my command: Programming by example}.
\newblock Morgan Kaufmann, 2001.

\bibitem[Gulwani(2011{\natexlab{a}})]{10.1145/1926385.1926423}
Sumit Gulwani.
\newblock Automating string processing in spreadsheets using input-output examples.
\newblock In \emph{Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}, POPL '11, page 317–330, New York, NY, USA, 2011{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9781450304900.
\newblock \doi{10.1145/1926385.1926423}.
\newblock URL \url{https://doi.org/10.1145/1926385.1926423}.

\bibitem[Gulwani(2011{\natexlab{b}})]{10.1145/1925844.1926423}
Sumit Gulwani.
\newblock Automating string processing in spreadsheets using input-output examples.
\newblock \emph{SIGPLAN Not.}, 46\penalty0 (1):\penalty0 317–330, jan 2011{\natexlab{b}}.
\newblock ISSN 0362-1340.
\newblock \doi{10.1145/1925844.1926423}.
\newblock URL \url{https://doi.org/10.1145/1925844.1926423}.

\bibitem[Polozov and Gulwani(2015)]{10.1145/2858965.2814310}
Oleksandr Polozov and Sumit Gulwani.
\newblock Flashmeta: A framework for inductive program synthesis.
\newblock \emph{SIGPLAN Not.}, 50\penalty0 (10):\penalty0 107–126, oct 2015.
\newblock ISSN 0362-1340.
\newblock \doi{10.1145/2858965.2814310}.
\newblock URL \url{https://doi.org/10.1145/2858965.2814310}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Maniatis, Singh, Sutton, Dai, Lin, and Zhou]{chen2021spreadsheetcoder}
Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou.
\newblock Spreadsheetcoder: Formula prediction from semi-structured context.
\newblock In \emph{International Conference on Machine Learning}, pages 1661--1672. PMLR, 2021{\natexlab{a}}.

\bibitem[Gulwani et~al.(2015)Gulwani, Hern{\'a}ndez-Orallo, Kitzelmann, Muggleton, Schmid, and Zorn]{gulwani2015inductive}
Sumit Gulwani, Jos{\'e} Hern{\'a}ndez-Orallo, Emanuel Kitzelmann, Stephen~H Muggleton, Ute Schmid, and Benjamin Zorn.
\newblock Inductive programming meets the real world.
\newblock \emph{Communications of the ACM}, 58\penalty0 (11):\penalty0 90--99, 2015.

\bibitem[Chollet(2019)]{chollet2019measure}
François Chollet.
\newblock On the measure of intelligence, 2019.

\bibitem[Muggleton et~al.(2018)Muggleton, Schmid, Zeller, Tamaddoni-Nezhad, and Besold]{muggleton2018ultra}
Stephen~H Muggleton, Ute Schmid, Christina Zeller, Alireza Tamaddoni-Nezhad, and Tarek Besold.
\newblock Ultra-strong machine learning: comprehensibility of programs learned with ilp.
\newblock \emph{Machine Learning}, 107\penalty0 (7):\penalty0 1119--1140, 2018.

\bibitem[Raymond(1964)]{raymond1964formal}
J~Solomono Raymond.
\newblock A formal theory of inductive inference i.
\newblock \emph{Information and Control}, 7:\penalty0 1--22, 1964.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Lago, et~al.]{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin~Dal Lago, et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Nature}, 2022.

\bibitem[Shi et~al.(2023)Shi, Dai, Li, Ellis, and Sutton]{shi2023lambdabeam}
Kensen Shi, Hanjun Dai, Wen-Ding Li, Kevin Ellis, and Charles Sutton.
\newblock Lambdabeam: Neural program search with higher-order functions and lambdas.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=qVMPXrX4FR}.

\bibitem[Feser et~al.(2015)Feser, Chaudhuri, and Dillig]{10.1145/2737924.2737977}
John~K. Feser, Swarat Chaudhuri, and Isil Dillig.
\newblock Synthesizing data structure transformations from input-output examples.
\newblock In \emph{Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}, PLDI '15, page 229–239, New York, NY, USA, 2015. Association for Computing Machinery.
\newblock ISBN 9781450334686.
\newblock \doi{10.1145/2737924.2737977}.
\newblock URL \url{https://doi.org/10.1145/2737924.2737977}.

\bibitem[Fijalkow et~al.(2022)Fijalkow, Lagarde, Matricon, Ellis, Ohlmann, and Potta]{fijalkow2022scaling}
Nathana{\"e}l Fijalkow, Guillaume Lagarde, Th{\'e}o Matricon, Kevin Ellis, Pierre Ohlmann, and Akarsh~Nayan Potta.
\newblock Scaling neural program synthesis with distribution-based search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 6623--6630, 2022.

\bibitem[Shi et~al.(2024)Shi, Hong, Deng, Yin, Zaheer, and Sutton]{shi2024exedecexecutiondecompositioncompositional}
Kensen Shi, Joey Hong, Yinlin Deng, Pengcheng Yin, Manzil Zaheer, and Charles Sutton.
\newblock Exedec: Execution decomposition for compositional generalization in neural program synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Ni et~al.(2024)Ni, Allamanis, Cohan, Deng, Shi, Sutton, and Yin]{ni2024next}
Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin.
\newblock Next: Teaching large language models to reason about code execution, 2024.

\bibitem[Rule et~al.(2024)Rule, Piantadosi, Cropper, Ellis, Nye, and Tenenbaum]{rule2024symbolic}
Joshua~S. Rule, Steven~T. Piantadosi, Andrew Cropper, Kevin Ellis, Maxwell Nye, and Joshua~B. Tenenbaum.
\newblock Symbolic metaprogram search improves learning efficiency and explains rule learning in humans.
\newblock \emph{Nature Communications}, 2024.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Kalyan et~al.(2018)Kalyan, Mohta, Polozov, Batra, Jain, and Gulwani]{kalyan2018neural}
Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani.
\newblock Neural-guided deductive search for real-time program synthesis from examples.
\newblock \emph{arXiv preprint arXiv:1804.01186}, 2018.

\bibitem[Devlin et~al.(2017)Devlin, Uesato, Bhupatiraju, Singh, Mohamed, and Kohli]{robust}
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli.
\newblock Robustfill: Neural program learning under noisy i/o.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning - Volume 70}, ICML'17, page 990–998. JMLR.org, 2017.

\bibitem[Chen et~al.(2018)Chen, Liu, and Song]{chen2018execution}
Xinyun Chen, Chang Liu, and Dawn Song.
\newblock Execution-guided neural program synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Shi et~al.(2021)Shi, Dai, Ellis, and Sutton]{shi2021crossbeam}
Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton.
\newblock Crossbeam: Learning to search in bottom-up program synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Zhang, Nguyen, Zan, Lin, Lou, and Chen]{chen2022codet}
Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.
\newblock Codet: Code generation with generated tests.
\newblock \emph{ICLR}, 2023{\natexlab{a}}.

\bibitem[Le et~al.(2022)Le, Wang, Gotmare, Savarese, and Hoi]{le2022coderl}
Hung Le, Yue Wang, Akhilesh~Deepak Gotmare, Silvio Savarese, and Steven Hoi.
\newblock Code{RL}: Mastering code generation through pretrained models and deep reinforcement learning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=WaGvb7OzySA}.

\bibitem[Zelikman et~al.(2023)Zelikman, Huang, Poesia, Goodman, and Haber]{zelikman2023parsel}
Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber.
\newblock Parsel: Algorithmic reasoning with language models by composing decompositions.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 31466--31523, 2023.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycksapps2021}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2023)Liu, Xia, Wang, and ZHANG]{liu2023is}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and LINGMING ZHANG.
\newblock Is your code generated by chat{GPT} really correct? rigorous evaluation of large language models for code generation.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=1qvx610Cu7}.

\bibitem[Qiu et~al.(2024)Qiu, Jiang, Lu, Sclar, Pyatkin, Bhagavatula, Wang, Kim, Choi, Dziri, and Ren]{qiu2023phenomenal}
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren.
\newblock Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.
\newblock \emph{ICLR}, 2024.

\bibitem[Ellis(2023)]{ellis2023human}
Kevin Ellis.
\newblock Human-like few-shot learning via bayesian reasoning over natural language.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Zelikman, Poesia, Pu, Haber, and Goodman]{wang2023hypothesis}
Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah~D. Goodman.
\newblock Hypothesis search: Inductive reasoning with language models.
\newblock \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2023selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated instructions, 2023.

\bibitem[Hinton et~al.(1995)Hinton, Dayan, Frey, and Neal]{hinton1995wake}
Geoffrey~E Hinton, Peter Dayan, Brendan~J Frey, and Radford~M Neal.
\newblock The" wake-sleep" algorithm for unsupervised neural networks.
\newblock \emph{Science}, 268\penalty0 (5214):\penalty0 1158--1161, 1995.

\bibitem[Osera and Zdancewic(2015)]{osera2015type}
Peter-Michael Osera and Steve Zdancewic.
\newblock Type-and-example-directed program synthesis.
\newblock \emph{ACM SIGPLAN Notices}, 50\penalty0 (6):\penalty0 619--630, 2015.

\bibitem[Balog et~al.(2017)Balog, Gaunt, Brockschmidt, Nowozin, and Tarlow]{balog2017deepcoder}
Matej Balog, Alexander~L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.
\newblock Deepcoder: Learning to write programs.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=ByldLrqlx}.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Lau et~al.(2003)Lau, Wolfman, Domingos, and Weld]{lau2003programming}
Tessa Lau, Steven~A Wolfman, Pedro Domingos, and Daniel~S Weld.
\newblock Programming by demonstration using version space algebra.
\newblock \emph{Machine Learning}, 53:\penalty0 111--156, 2003.

\bibitem[Cambronero et~al.(2023)Cambronero, Gulwani, Le, Perelman, Radhakrishna, Simon, and Tiwari]{cambronero2023flashfill++}
Jos{\'e} Cambronero, Sumit Gulwani, Vu~Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish Tiwari.
\newblock Flashfill++: Scaling programming by example by cutting to the chase.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 7\penalty0 (POPL):\penalty0 952--981, 2023.

\bibitem[pro(2022)]{prose}
Microsoft prose public benchmark suite, 2022.
\newblock Available at https://github.com/microsoft/prose-benchmarks.

\bibitem[Mao et~al.(2019)Mao, Zhang, Li, Freeman, Tenenbaum, and Wu]{Mao2019Program}
Jiayuan Mao, Xiuming Zhang, Yikai Li, William~T. Freeman, Joshua~B. Tenenbaum, and Jiajun Wu.
\newblock {Program-Guided Image Manipulators}.
\newblock In \emph{International Conference on Computer Vision}, 2019.

\bibitem[Ellis and Gulwani(2017)]{ellis2017learning}
Kevin Ellis and Sumit Gulwani.
\newblock Learning to learn programs from examples: Going beyond program structure.
\newblock In \emph{IJCAI}, pages 1638--1645, 2017.

\bibitem[Thornburg(1983)]{turtle}
David~D. Thornburg.
\newblock Friends of the turtle.
\newblock \emph{Compute!}, March 1983.

\bibitem[Wong et~al.(2021)Wong, Ellis, Tenenbaum, and Andreas]{Wong2021LeveragingLT}
Catherine Wong, Kevin Ellis, Joshua~B. Tenenbaum, and Jacob Andreas.
\newblock Leveraging language to learn program abstractions and search heuristics.
\newblock In \emph{ICML}, 2021.

\bibitem[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, Luo, Xiong, and Liang]{guo2024deepseekcoder}
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.~Wu, Y.~K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang.
\newblock Deepseek-coder: When the large language model meets programming -- the rise of code intelligence, 2024.

\bibitem[Ellis et~al.(2021)Ellis, Wong, Nye, Sabl\'{e}-Meyer, Morales, Hewitt, Cary, Solar-Lezama, and Tenenbaum]{10.1145/3453483.3454080}
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sabl\'{e}-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua~B. Tenenbaum.
\newblock \emph{DreamCoder: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning}, page 835–850.
\newblock Association for Computing Machinery, New York, NY, USA, 2021.
\newblock ISBN 9781450383912.
\newblock URL \url{https://doi.org/10.1145/3453483.3454080}.

\bibitem[Stengel-Eskin et~al.(2024)Stengel-Eskin, Prasad, and Bansal]{stengel2024regal}
Elias Stengel-Eskin, Archiki Prasad, and Mohit Bansal.
\newblock Regal: Refactoring programs to discover generalizable abstractions.
\newblock \emph{arXiv preprint arXiv:2401.16467}, 2024.

\bibitem[Muggleton et~al.(2015)Muggleton, Lin, and Tamaddoni-Nezhad]{10.1007/s10994-014-5471-y}
Stephen~H. Muggleton, Dianhuan Lin, and Alireza Tamaddoni-Nezhad.
\newblock Meta-interpretive learning of higher-order dyadic datalog: Predicate invention revisited.
\newblock \emph{Mach. Learn.}, 100\penalty0 (1):\penalty0 49–73, jul 2015.
\newblock ISSN 0885-6125.
\newblock \doi{10.1007/s10994-014-5471-y}.
\newblock URL \url{https://doi.org/10.1007/s10994-014-5471-y}.

\bibitem[Piantadosi(2023)]{fleet}
Steven Piantadosi.
\newblock Fleet.
\newblock \url{https://github.com/piantado/Fleet/}, 2023.
\newblock [Online GitHub repository].

\bibitem[Alur et~al.(2013)Alur, Bodik, Juniwal, Martin, Raghothaman, Seshia, Singh, Solar-Lezama, Torlak, and Udupa]{alur2013syntax}
Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo~MK Martin, Mukund Raghothaman, Sanjit~A Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa.
\newblock \emph{Syntax-guided synthesis}.
\newblock IEEE, 2013.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.
\newblock Wizardcoder: Empowering code large language models with evol-instruct, 2023.

\bibitem[Ye et~al.(2022)Ye, Gao, Li, Xu, Feng, Wu, Yu, and Kong]{ye2022zerogen}
Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.
\newblock Zerogen: Efficient zero-shot learning via dataset generation.
\newblock \emph{arXiv preprint arXiv:2202.07922}, 2022.

\bibitem[Patel et~al.(2024)Patel, Raffel, and Callison-Burch]{patel2024datadreamer}
Ajay Patel, Colin Raffel, and Chris Callison-Burch.
\newblock Datadreamer: A tool for synthetic data generation and reproducible llm workflows, 2024.

\bibitem[Meng et~al.(2022)Meng, Huang, Zhang, and Han]{meng2022generating}
Yu~Meng, Jiaxin Huang, Yu~Zhang, and Jiawei Han.
\newblock Generating training data with language models: Towards zero-shot language understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 462--477, 2022.

\bibitem[Gupta et~al.(2020)Gupta, Christensen, Chen, and Song]{10.5555/3495724.3497208}
Kavi Gupta, Peter~Ebert Christensen, Xinyun Chen, and Dawn Song.
\newblock Synthesize, execute and debug: learning to repair for neural program synthesis.
\newblock In \emph{Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Barke et~al.(2020)Barke, Peleg, and Polikarpova]{10.1145/3428295}
Shraddha Barke, Hila Peleg, and Nadia Polikarpova.
\newblock Just-in-time learning for bottom-up enumerative synthesis.
\newblock \emph{Proc. ACM Program. Lang.}, 4\penalty0 (OOPSLA), nov 2020.
\newblock \doi{10.1145/3428295}.
\newblock URL \url{https://doi.org/10.1145/3428295}.

\bibitem[Ellis et~al.(2019)Ellis, Nye, Pu, Sosa, Tenenbaum, and Solar-Lezama]{10.5555/3454287.3455109}
Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Joshua~B. Tenenbaum, and Armando Solar-Lezama.
\newblock \emph{Write, Execute, Assess: Program Synthesis with a REPL}.
\newblock Curran Associates Inc., Red Hook, NY, USA, 2019.

\bibitem[Singh and Gulwani(2015)]{singh2015predicting}
Rishabh Singh and Sumit Gulwani.
\newblock Predicting a correct program in programming by example.
\newblock In \emph{27th International Conference on Computer Aided Verification (CAV 2015)}, July 2015.
\newblock URL \url{https://www.microsoft.com/en-us/research/publication/predicting-a-correct-program-in-programming-by-example/}.

\bibitem[Liang et~al.(2010)Liang, Jordan, and Klein]{10.5555/3104322.3104404}
Percy Liang, Michael~I. Jordan, and Dan Klein.
\newblock Learning programs: a hierarchical bayesian approach.
\newblock In \emph{Proceedings of the 27th International Conference on International Conference on Machine Learning}, ICML'10, page 639–646, Madison, WI, USA, 2010. Omnipress.
\newblock ISBN 9781605589077.

\bibitem[Inala et~al.(2022)Inala, Wang, Yang, Codas, Encarnaci\'{o}n, Lahiri, Musuvathi, and Gao]{NEURIPS2022_5762c579}
Jeevana~Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnaci\'{o}n, Shuvendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao.
\newblock Fault-aware neural code rankers.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, volume~35, pages 13419--13432. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/5762c579d09811b7639be2389b3d07be-Paper-Conference.pdf}.

\bibitem[Piriyakulkij and Ellis(2024)]{piriyakulkij2024doing}
Wasu~Top Piriyakulkij and Kevin Ellis.
\newblock Doing experiments and revising rules with natural language and probabilistic reasoning.
\newblock \emph{CogSci}, 2024.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Zelikman, Poesia, Pu, Haber, and Goodman]{wang2024hypothesis}
Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman.
\newblock Hypothesis search: Inductive reasoning with language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=G7UtIGQmjm}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Lin, Sch{\"a}rli, and Zhou]{chen2023teaching}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Teaching large language models to self-debug.
\newblock \emph{arXiv preprint arXiv:2304.05128}, 2023{\natexlab{b}}.

\bibitem[Welleck et~al.(2023)Welleck, Lu, West, Brahman, Shen, Khashabi, and Choi]{welleck2022generating}
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi.
\newblock Generating sequences by learning to self-correct.
\newblock \emph{ICLR}, 2023.

\bibitem[Olausson et~al.(2023)Olausson, Inala, Wang, Gao, and Solar-Lezama]{olausson2023selfrepair}
Theo~X. Olausson, Jeevana~Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.
\newblock Is self-repair a silver bullet for code generation?, 2023.

\bibitem[Shinn et~al.(2023)Shinn, Labash, and Gopinath]{shinn2023reflexion}
Noah Shinn, Beck Labash, and Ashwin Gopinath.
\newblock Reflexion: an autonomous agent with dynamic memory and self-reflection.
\newblock \emph{arXiv preprint arXiv:2303.11366}, 2023.

\bibitem[Madaan et~al.(2024)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2024self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Tang et~al.(2024)Tang, Hu, Zhou, Zhong, Zheng, Si, and Ellis]{rex}
Hao Tang, Keya Hu, Jin~Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, and Kevin Ellis.
\newblock Code repair with llms gives an exploration-exploitation tradeoff.
\newblock \emph{arXiv}, 2024.

\bibitem[Ni et~al.(2023)Ni, Iyer, Radev, Stoyanov, Yih, Wang, and Lin]{ni2023lever}
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida~I Wang, and Xi~Victoria Lin.
\newblock Lever: Learning to verify language-to-code generation with execution.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning (ICML'23)}, 2023.

\bibitem[jin()]{jinamodel}
jinaai/jina-embeddings-v2-base-code.
\newblock \url{https://huggingface.co/jinaai/jina-embeddings-v2-base-code}.
\newblock Accessed: 2024-05-22.

\end{thebibliography}
