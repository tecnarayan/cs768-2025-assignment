\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aad et~al.(2012)Aad, Abajyan, and
  Collaboration]{Collaboration2012ObservationOA}
Aad, G., Abajyan, T., and Collaboration, T.~A.
\newblock {Observation of a new particle in the search for the Standard Model
  Higgs boson with the ATLAS detector at the LHC}.
\newblock \emph{Physics Letters B}, 716:\penalty0 1–29, 09 2012.

\bibitem[Abbott et~al.(2016)Abbott, Collaboration, and
  Collaboration]{Collaboration2016ObservationOG}
Abbott, B., Collaboration, L.~S., and Collaboration, V.
\newblock {Observation of Gravitational Waves from a Binary Black Hole Merger}.
\newblock \emph{Physical Review Letters}, 116, 02 2016.
\newblock \doi{10.1103/PhysRevLett.116.061102}.

\bibitem[Alistarh et~al.(2018)Alistarh, Allen-Zhu, and
  Li]{alistarh2018byzantine}
Alistarh, D., Allen-Zhu, Z., and Li, J.
\newblock Byzantine stochastic gradient descent.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  4618--4628, 2018.

\bibitem[Allen-Zhu et~al.(2021)Allen-Zhu, Ebrahimianghazani, Li, and
  Alistarh]{allen2020byzantine}
Allen-Zhu, Z., Ebrahimianghazani, F., Li, J., and Alistarh, D.
\newblock Byzantine-resilient non-convex stochastic gradient descent.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=PbEHqvFtcS}.

\bibitem[Anderson(2004)]{anderson2004boinc}
Anderson, D.~P.
\newblock Boinc: A system for public-resource computing and storage.
\newblock In \emph{Fifth IEEE/ACM international workshop on grid computing},
  pp.\  4--10. IEEE, 2004.

\bibitem[Atre et~al.(2021)Atre, Jha, and Rao]{atre2021distributed}
Atre, M., Jha, B., and Rao, A.
\newblock Distributed deep learning using volunteer computing-like paradigm,
  2021.

\bibitem[Balakrishnan et~al.(2003)Balakrishnan, Kaashoek, Karger, Morris, and
  Stoica]{chord}
Balakrishnan, H., Kaashoek, M.~F., Karger, D., Morris, R., and Stoica, I.
\newblock Looking up data in p2p systems.
\newblock \emph{Communications of the ACM}, 46\penalty0 (2):\penalty0 43--48,
  2003.

\bibitem[Baruch et~al.(2019)Baruch, Baruch, and Goldberg]{baruch2019little}
Baruch, G., Baruch, M., and Goldberg, Y.
\newblock A little is enough: Circumventing defenses for distributed learning.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf}.

\bibitem[Beberg et~al.(2009)Beberg, Ensign, Jayachandran, Khaliq, and
  Pande]{foldingathome}
Beberg, A.~L., Ensign, D., Jayachandran, G., Khaliq, S., and Pande, V.
\newblock Folding@home: Lessons from eight years of volunteer distributed
  computing.
\newblock \emph{2009 IEEE International Symposium on Parallel \& Distributed
  Processing}, pp.\  1--8, 2009.

\bibitem[Ben-Ameur et~al.(2013)Ben-Ameur, Bianchi, and
  Jakubowicz]{ben2013robust}
Ben-Ameur, W., Bianchi, P., and Jakubowicz, J.
\newblock Robust consensus in distributed networks using total variation.
\newblock \emph{arXiv preprint arXiv:1309.7264}, 2013.

\bibitem[Blanchard et~al.(2017)Blanchard, El~Mhamdi, Guerraoui, and
  Stainer]{blanchard2017machine}
Blanchard, P., El~Mhamdi, E.~M., Guerraoui, R., and Stainer, J.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  118--128, 2017.

\bibitem[Blum(1983)]{blum1983coin}
Blum, M.
\newblock Coin flipping by telephone a protocol for solving impossible
  problems.
\newblock \emph{ACM SIGACT News}, 15\penalty0 (1):\penalty0 23--27, 1983.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Bulusu et~al.(2020)Bulusu, Khanduri, Sharma, and
  Varshney]{bulusu2020distributed}
Bulusu, S., Khanduri, P., Sharma, P., and Varshney, P.~K.
\newblock On distributed stochastic gradient descent for nonconvex functions in
  the presence of byzantines.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  3137--3141. IEEE, 2020.

\bibitem[Chen et~al.(2018)Chen, Wang, Charles, and
  Papailiopoulos]{chen2018draco}
Chen, L., Wang, H., Charles, Z., and Papailiopoulos, D.
\newblock Draco: Byzantine-resilient distributed training via redundant
  gradients.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  903--912. PMLR, 2018.

\bibitem[Cleve(1986)]{cleve1986limits}
Cleve, R.
\newblock Limits on the security of coin flips when half the processors are
  faulty.
\newblock In \emph{Proceedings of the eighteenth annual ACM symposium on Theory
  of computing}, pp.\  364--369, 1986.

\bibitem[Damaskinos et~al.(2019)Damaskinos, El~Mhamdi, Guerraoui, Guirguis, and
  Rouault]{damaskinos2019aggregathor}
Damaskinos, G., El~Mhamdi, E.~M., Guerraoui, R., Guirguis, A. H.~A., and
  Rouault, S. L.~A.
\newblock Aggregathor: Byzantine machine learning via robust gradient
  aggregation.
\newblock In \emph{The Conference on Systems and Machine Learning (SysML),
  2019}, 2019.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, Le, and Ng]{sharded_ps_first}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M.~a.,
  Senior, A., Tucker, P., Yang, K., Le, Q., and Ng, A.
\newblock Large scale distributed deep networks.
\newblock In Pereira, F., Burges, C. J.~C., Bottou, L., and Weinberger, K.~Q.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~25,
  pp.\  1223--1231. Curran Associates, Inc., 2012.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf}.

\bibitem[Defazio \& Bottou(2019)Defazio and Bottou]{defazio2018ineffectiveness}
Defazio, A. and Bottou, L.
\newblock On the ineffectiveness of variance reduced optimization for deep
  learning.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf}.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances In Neural Information Processing Systems}, 2014.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding}.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Diskin et~al.(2021)Diskin, Bukhtiyarov, Ryabinin, Saulnier, Lhoest,
  Sinitsin, Popov, Pyrkin, Kashirin, Borzunov, del Moral, Mazur, Kobelev,
  Jernite, Wolf, and Pekhimenko]{dedloc}
Diskin, M., Bukhtiyarov, A., Ryabinin, M., Saulnier, L., Lhoest, Q., Sinitsin,
  A., Popov, D., Pyrkin, D., Kashirin, M., Borzunov, A., del Moral, A.~V.,
  Mazur, D., Kobelev, I., Jernite, Y., Wolf, T., and Pekhimenko, G.
\newblock Distributed deep learning in open collaborations.
\newblock \emph{CoRR}, abs/2106.10207, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.10207}.

\bibitem[Douceur(2002)]{sybil}
Douceur, J.~R.
\newblock The sybil attack.
\newblock In \emph{IPTPS}, 2002.

\bibitem[El~Mhamdi et~al.(2018)El~Mhamdi, Guerraoui, and
  Rouault]{mhamdi2018hidden}
El~Mhamdi, E.~M., Guerraoui, R., and Rouault, S.
\newblock The hidden vulnerability of distributed learning in {B}yzantium.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3521--3530. PMLR, 10--15 Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/mhamdi18a.html}.

\bibitem[El-Mhamdi et~al.(2020)El-Mhamdi, Guerraoui, Guirguis, Hoang, and
  Rouault]{el2020genuinely}
El-Mhamdi, E.-M., Guerraoui, R., Guirguis, A., Hoang, L.~N., and Rouault, S.
\newblock Genuinely distributed byzantine machine learning.
\newblock In \emph{Proceedings of the 39th Symposium on Principles of
  Distributed Computing}, pp.\  355--364, 2020.

\bibitem[Ghadimi \& Lan(2012)Ghadimi and Lan]{ghadimi2012optimal}
Ghadimi, S. and Lan, G.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1469--1492, 2012.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Kovalev, Makarenko, and
  Richt{\'a}rik]{gorbunov2020linearly}
Gorbunov, E., Kovalev, D., Makarenko, D., and Richt{\'a}rik, P.
\newblock Linearly converging error compensated sgd.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Gorbunov et~al.(2021)Gorbunov, Burlachenko, Li, and
  Richt{\'a}rik]{gorbunov2021marina}
Gorbunov, E., Burlachenko, K., Li, Z., and Richt{\'a}rik, P.
\newblock Marina: Faster non-convex distributed learning with compression.
\newblock \emph{arXiv preprint arXiv:2102.07845}, 2021.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017.

\bibitem[Goyal et~al.(2021)Goyal, Duval, Reizenstein, Leavitt, Xu, Lefaudeux,
  Singh, Reis, Caron, Bojanowski, Joulin, and Misra]{vissl}
Goyal, P., Duval, Q., Reizenstein, J., Leavitt, M., Xu, M., Lefaudeux, B.,
  Singh, M., Reis, V., Caron, M., Bojanowski, P., Joulin, A., and Misra, I.
\newblock Vissl, 2021.
\newblock URL \url{https://github.com/facebookresearch/vissl}.

\bibitem[Gupta \& Vaidya(2021)Gupta and Vaidya]{gupta2021byzantine2}
Gupta, N. and Vaidya, N.~H.
\newblock Byzantine fault-tolerance in peer-to-peer distributed
  gradient-descent.
\newblock \emph{arXiv preprint arXiv:2101.12316}, 2021.

\bibitem[Gupta et~al.(2021)Gupta, Doan, and Vaidya]{gupta2021byzantine}
Gupta, N., Doan, T.~T., and Vaidya, N.~H.
\newblock Byzantine fault-tolerance in decentralized optimization under
  2f-redundancy.
\newblock In \emph{2021 American Control Conference (ACC)}, pp.\  3632--3637.
  IEEE, 2021.

\bibitem[Hammurabi \& Harper(1904)Hammurabi and Harper]{hammurabi}
Hammurabi, K. o.~B. and Harper, R.~F.
\newblock \emph{The Code of Hammurabi, King of Babylon: About 2250 BC:
  Autographed Text, Transliteration, Translation, Glossary Index of Subjects,
  Lists of Proper Names, Signs, Numuerals...}
\newblock University of Chicago Press, 1904.
\newblock URL \url{https://books.google.ru/books?id=jeLz_BYUoeQC&pg=PA11}.
\newblock Page 11, \textsection 1.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2015.

\bibitem[He et~al.(2020)He, Karimireddy, and Jaggi]{he2020byzantine}
He, L., Karimireddy, S.~P., and Jaggi, M.
\newblock Byzantine-robust learning on heterogeneous datasets via resampling.
\newblock \emph{arXiv preprint arXiv:2006.09365v3}, 2020.

\bibitem[Huang et~al.(2019)Huang, Cheng, Chen, Lee, Ngiam, Le, and Chen]{gpipe}
Huang, Y., Cheng, Y., Chen, D., Lee, H., Ngiam, J., Le, Q.~V., and Chen, Z.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{ArXiv}, abs/1811.06965, 2019.

\bibitem[Jiang et~al.(2020)Jiang, Zhu, Lan, Yi, Cui, and Guo]{byteps}
Jiang, Y., Zhu, Y., Lan, C., Yi, B., Cui, Y., and Guo, C.
\newblock A unified architecture for accelerating distributed {DNN} training in
  heterogeneous gpu/cpu clusters.
\newblock In \emph{14th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} 20)}, pp.\  463--479. {USENIX} Association, November
  2020.
\newblock ISBN 978-1-939133-19-9.
\newblock URL
  \url{https://www.usenix.org/conference/osdi20/presentation/jiang}.

\bibitem[Karimireddy et~al.(2020)Karimireddy, He, and
  Jaggi]{karimireddy2020learning}
Karimireddy, S.~P., He, L., and Jaggi, M.
\newblock Learning from history for byzantine robust optimization.
\newblock \emph{arXiv preprint arXiv:2012.10333v3}, 2020.

\bibitem[Kijsipongse et~al.(2018)Kijsipongse, Piyatumrong, and
  U-ruekolan]{volunteer_dl_async}
Kijsipongse, E., Piyatumrong, A., and U-ruekolan, S.
\newblock A hybrid gpu cluster and volunteer computing platform for scalable
  deep learning.
\newblock \emph{The Journal of Supercomputing}, 04 2018.
\newblock \doi{10.1007/s11227-018-2375-9}.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{Kolesnikov2020BigT}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{ECCV}, 2020.

\bibitem[Koloskova et~al.(2020)Koloskova, Lin, Stich, and
  Jaggi]{koloskova2020decentralized}
Koloskova, A., Lin, T., Stich, S.~U., and Jaggi, M.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkgGCkrKvH}.

\bibitem[Krizhevsky et~al.()Krizhevsky, Nair, and Hinton]{cifar}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{albert}
Lan, Z.-Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{ArXiv}, abs/1909.11942, 2019.

\bibitem[Li et~al.(2019)Li, Xu, Chen, Giannakis, and Ling]{li2019rsa}
Li, L., Xu, W., Chen, T., Giannakis, G.~B., and Ling, Q.
\newblock Rsa: Byzantine-robust stochastic aggregation methods for distributed
  learning from heterogeneous datasets.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  1544--1551, 2019.

\bibitem[Li(2014)]{parameter_server_first}
Li, M.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{Proceedings of the 2014 International Conference on Big Data
  Science and Computing}, BigDataScience '14, New York, NY, USA, 2014.
  Association for Computing Machinery.
\newblock ISBN 9781450328913.
\newblock \doi{10.1145/2640087.2644155}.
\newblock URL \url{https://doi.org/10.1145/2640087.2644155}.

\bibitem[Li et~al.(2020)Li, Zhao, Varma, Salpekar, Noordhuis, Li, Paszke,
  Smith, Vaughan, Damania, and Chintala]{pytorch_distributed}
Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li, T., Paszke, A.,
  Smith, J., Vaughan, B., Damania, P., and Chintala, S.
\newblock Pytorch distributed: Experiences on accelerating data parallel
  training.
\newblock \emph{Proc. VLDB Endow.}, 13\penalty0 (12):\penalty0 3005–3018,
  August 2020.
\newblock ISSN 2150-8097.
\newblock \doi{10.14778/3415478.3415530}.
\newblock URL \url{https://doi.org/10.14778/3415478.3415530}.

\bibitem[Li et~al.(2017)Li, Davis, and Jarvis]{butterfly_arsgd}
Li, Z., Davis, J., and Jarvis, S.
\newblock An efficient task-based all-reduce for machine learning applications.
\newblock In \emph{Proceedings of the Machine Learning on HPC Environments},
  MLHPC'17, New York, NY, USA, 2017. Association for Computing Machinery.
\newblock ISBN 9781450351379.
\newblock \doi{10.1145/3146347.3146350}.
\newblock URL \url{https://doi.org/10.1145/3146347.3146350}.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and
  Dally]{deepgradientcompression}
Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, W.~J.
\newblock {Deep Gradient Compression: Reducing the communication bandwidth for
  distributed training}.
\newblock In \emph{The International Conference on Learning Representations},
  2018.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv}, abs/1907.11692, 2019.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations (ICLR)
  2017 Conference Track}, April 2017.

\bibitem[Lyu et~al.(2020)Lyu, Yu, Ma, Sun, Zhao, Yang, and Yu]{lyu2020privacy}
Lyu, L., Yu, H., Ma, X., Sun, L., Zhao, J., Yang, Q., and Yu, P.~S.
\newblock Privacy and robustness in federated learning: Attacks and defenses.
\newblock \emph{arXiv preprint arXiv:2012.06337}, 2020.

\bibitem[Maymounkov \& Mazieres(2002)Maymounkov and Mazieres]{kademlia}
Maymounkov, P. and Mazieres, D.
\newblock Kademlia: A peer-to-peer information system based on the xor metric.
\newblock In \emph{International Workshop on Peer-to-Peer Systems}, pp.\
  53--65. Springer, 2002.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{FedLearningOriginal}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1273--1282,
  2017.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{wikitext103}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{ArXiv}, abs/1609.07843, 2017.

\bibitem[Merritt(2020)]{folding_exaflop_2}
Merritt, R.
\newblock \emph{Folding@home gets 1.5+ Exaflops to Fight COVID-19}, 04 2020.
\newblock
  \url{https://blogs.nvidia.com/blog/2020/04/01/foldingathome-exaflop-coronavirus/}(accessed
  on Apr 29, 2021).

\bibitem[Mikami et~al.(2019)Mikami, Suganuma, U-chupala, Tanaka, and
  Kageyama]{mikami2019massively}
Mikami, H., Suganuma, H., U-chupala, P., Tanaka, Y., and Kageyama, Y.
\newblock Massively distributed sgd: Imagenet/resnet-50 training in a flash,
  2019.

\bibitem[Mishchenko et~al.(2019)Mishchenko, Gorbunov, Tak{\'a}{\v{c}}, and
  Richt{\'a}rik]{mishchenko2019distributed}
Mishchenko, K., Gorbunov, E., Tak{\'a}{\v{c}}, M., and Richt{\'a}rik, P.
\newblock Distributed learning with compressed gradient differences.
\newblock \emph{arXiv preprint arXiv:1901.09269}, 2019.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri,
  Devanur, Ganger, Gibbons, and Zaharia]{pipedream}
Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N.~R.,
  Ganger, G.~R., Gibbons, P.~B., and Zaharia, M.
\newblock Pipedream: Generalized pipeline parallelism for dnn training.
\newblock In \emph{Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, SOSP ’19, pp.\  1–15, New York, NY, USA, 2019. Association
  for Computing Machinery.
\newblock ISBN 9781450368735.
\newblock \doi{10.1145/3341301.3359646}.
\newblock URL \url{https://doi.org/10.1145/3341301.3359646}.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary,
  Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, et~al.]{megatron2}
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M.,
  Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B.,
  et~al.
\newblock Efficient large-scale language model training on gpu clusters.
\newblock \emph{arXiv preprint arXiv:2104.04473}, 2021.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nesterov(1983)]{nesterov}
Nesterov, Y.
\newblock A method for solving the convex programming problem with convergence
  rate $o(1/k^2)$.
\newblock \emph{Proceedings of the USSR Academy of Sciences}, 269:\penalty0
  543--547, 1983.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Patarasuk \& Yuan(2009)Patarasuk and
  Yuan]{bandwidth_optimal_allreduce}
Patarasuk, P. and Yuan, X.
\newblock Bandwidth optimal all-reduce algorithms for clusters of workstations.
\newblock \emph{J. Parallel Distrib. Comput.}, 69\penalty0 (2):\penalty0
  117–124, February 2009.
\newblock ISSN 0743-7315.
\newblock \doi{10.1016/j.jpdc.2008.09.002}.
\newblock URL \url{https://doi.org/10.1016/j.jpdc.2008.09.002}.

\bibitem[Peng et~al.(2021)Peng, Li, and Ling]{peng2021byzantine}
Peng, J., Li, W., and Ling, Q.
\newblock Byzantine-robust decentralized stochastic optimization over static
  and time-varying networks.
\newblock \emph{Signal Processing}, 183:\penalty0 108020, 2021.

\bibitem[Pillutla et~al.(2019)Pillutla, Kakade, and
  Harchaoui]{pillutla2019robust}
Pillutla, K., Kakade, S.~M., and Harchaoui, Z.
\newblock Robust aggregation for federated learning.
\newblock \emph{arXiv preprint arXiv:1912.13445}, 2019.

\bibitem[Rabin \& Ben-Or(1989)Rabin and Ben-Or]{rabin1989verifiable}
Rabin, T. and Ben-Or, M.
\newblock Verifiable secret sharing and multiparty protocols with honest
  majority.
\newblock In \emph{Proceedings of the twenty-first annual ACM symposium on
  Theory of computing}, pp.\  73--85, 1989.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He]{zero}
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock \emph{SC20: International Conference for High Performance Computing,
  Networking, Storage and Analysis}, pp.\  1--16, 2020.

\bibitem[Rajput et~al.(2019)Rajput, Wang, Charles, and
  Papailiopoulos]{rajput2019detox}
Rajput, S., Wang, H., Charles, Z., and Papailiopoulos, D.
\newblock Detox: A redundancy-based framework for faster and more robust
  gradient aggregation.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf}.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Recht, B., Re, C., Wright, S., and Niu, F.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  693--701, 2011.

\bibitem[Regatti et~al.(2020)Regatti, Chen, and Gupta]{regatti2020bygars}
Regatti, J., Chen, H., and Gupta, A.
\newblock Bygars: Byzantine sgd with arbitrary number of attackers.
\newblock \emph{arXiv preprint arXiv:2006.13421}, 2020.

\bibitem[Rivest et~al.(1978)Rivest, Shamir, and Adleman]{rivest1978method}
Rivest, R.~L., Shamir, A., and Adleman, L.
\newblock A method for obtaining digital signatures and public-key
  cryptosystems.
\newblock \emph{Communications of the ACM}, 21\penalty0 (2):\penalty0 120--126,
  1978.

\bibitem[Rodr{\'\i}guez-Barroso et~al.(2020)Rodr{\'\i}guez-Barroso,
  Mart{\'\i}nez-C{\'a}mara, Luz{\'o}n, Seco, Veganzones, and
  Herrera]{rodriguez2020dynamic}
Rodr{\'\i}guez-Barroso, N., Mart{\'\i}nez-C{\'a}mara, E., Luz{\'o}n, M., Seco,
  G.~G., Veganzones, M.~{\'A}., and Herrera, F.
\newblock Dynamic federated learning model for identifying adversarial clients.
\newblock \emph{arXiv preprint arXiv:2007.15030}, 2020.

\bibitem[Rowstron \& Druschel(2001)Rowstron and Druschel]{pastry}
Rowstron, A. and Druschel, P.
\newblock Pastry: Scalable, decentralized object location, and routing for
  large-scale peer-to-peer systems.
\newblock In \emph{IFIP/ACM International Conference on Distributed Systems
  Platforms and Open Distributed Processing}, pp.\  329--350. Springer, 2001.

\bibitem[Ruttley et~al.(2017)Ruttley, Robinson, and Gerstenmaier]{ISS}
Ruttley, T., Robinson, J., and Gerstenmaier, W.
\newblock The international space station: Collaboration, utilization, and
  commercialization*: The international space station.
\newblock \emph{Social Science Quarterly}, 98:\penalty0 1160--1174, 12 2017.
\newblock \doi{10.1111/ssqu.12469}.

\bibitem[Ryabinin \& Gusev(2020)Ryabinin and Gusev]{hivemind_dmoe}
Ryabinin, M. and Gusev, A.
\newblock Towards crowdsourced training of large neural networks using
  decentralized mixture-of-experts.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  3659--3672. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf}.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch]{sennrich-etal-2016-neural}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1715--1725,
  Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1162}.
\newblock URL \url{https://www.aclweb.org/anthology/P16-1162}.

\bibitem[Sergeev \& Balso(2018)Sergeev and Balso]{horovod}
Sergeev, A. and Balso, M.~D.
\newblock Horovod: fast and easy distributed deep learning in tensorflow, 2018.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-lm: Training multi-billion parameter language models using
  gpu model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Smith(2019)]{setihomeamdfail}
Smith, B.
\newblock Flakey amd/ati gpus, including rx 5700 xt, cross validating,
  polluting the database, 2019.
\newblock URL \url{https://setiathome.berkeley.edu/forum_thread.php?id=84508}.
\newblock Accessed: 2021-05-20.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{jft300m}
Sun, C., Shrivastava, A., Singh, S., and Gupta, A.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, Oct 2017.

\bibitem[Tolpegin et~al.(2020)Tolpegin, Truex, Gursoy, and
  Liu]{tolpegin2020data}
Tolpegin, V., Truex, S., Gursoy, M.~E., and Liu, L.
\newblock Data poisoning attacks against federated learning systems.
\newblock In \emph{ESORICS}, 2020.

\bibitem[Trifa \& Khemakhem(2014)Trifa and Khemakhem]{sybil_nodes}
Trifa, Z. and Khemakhem, M.
\newblock Sybil nodes as a mitigation strategy against sybil attack.
\newblock \emph{Procedia Computer Science}, 32:\penalty0 1135--1140, 2014.

\bibitem[Urdaneta et~al.(2011)Urdaneta, Pierre, and Steen]{urdaneta2011survey}
Urdaneta, G., Pierre, G., and Steen, M.~V.
\newblock A survey of dht security techniques.
\newblock \emph{ACM Computing Surveys (CSUR)}, 43\penalty0 (2):\penalty0 1--49,
  2011.

\bibitem[Vyzovitis et~al.(2020)Vyzovitis, Napora, McCormick, Dias, and
  Psaras]{vyzovitis2020gossipsub}
Vyzovitis, D., Napora, Y., McCormick, D., Dias, D., and Psaras, Y.
\newblock Gossip{S}ub: Attack-resilient message propagation in the {F}ilecoin
  and {ETH2.0} networks.
\newblock \emph{arXiv preprint arXiv:2007.02754}, 2020.

\bibitem[Wang \& Kangasharju(2012)Wang and Kangasharju]{sybil_attacks_dht}
Wang, L. and Kangasharju, J.
\newblock Real-world sybil attacks in bittorrent mainline dht.
\newblock In \emph{2012 IEEE Global Communications Conference (GLOBECOM)}, pp.\
   826--832. IEEE, 2012.

\bibitem[Wu et~al.(2020)Wu, Ling, Chen, and Giannakis]{wu2020federated}
Wu, Z., Ling, Q., Chen, T., and Giannakis, G.~B.
\newblock Federated variance-reduced stochastic gradient descent with
  robustness to byzantine attacks.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  4583--4596, 2020.

\bibitem[Xie et~al.(2020)Xie, Koyejo, and Gupta]{xie2020fall}
Xie, C., Koyejo, O., and Gupta, I.
\newblock Fall of empires: Breaking byzantine-tolerant sgd by inner product
  manipulation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  261--270.
  PMLR, 2020.

\bibitem[Xu \& Lyu(2020)Xu and Lyu]{xu2020towards}
Xu, X. and Lyu, L.
\newblock Towards building a robust and fair federated learning system.
\newblock \emph{arXiv preprint arXiv:2011.10464}, 2020.

\bibitem[Yang \& Bajwa(2019{\natexlab{a}})Yang and Bajwa]{yang2019bridge}
Yang, Z. and Bajwa, W.~U.
\newblock Bridge: Byzantine-resilient decentralized gradient descent.
\newblock \emph{arXiv preprint arXiv:1908.08098}, 2019{\natexlab{a}}.

\bibitem[Yang \& Bajwa(2019{\natexlab{b}})Yang and Bajwa]{yang2019byrdie}
Yang, Z. and Bajwa, W.~U.
\newblock Byrdie: Byzantine-resilient distributed coordinate descent for
  decentralized learning.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 5\penalty0 (4):\penalty0 611--627, 2019{\natexlab{b}}.

\bibitem[Yin et~al.(2018)Yin, Chen, Kannan, and Bartlett]{yin2018byzantine}
Yin, D., Chen, Y., Kannan, R., and Bartlett, P.
\newblock Byzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5650--5659. PMLR, 2018.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{lamb}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Syx4wnEtvH}.

\bibitem[Zhang et~al.(2019)Zhang, Liu, Lai, Jin, and Li]{zhang2019efficient}
Zhang, E., Liu, F.-H., Lai, Q., Jin, G., and Li, Y.
\newblock Efficient multi-party private set intersection against malicious
  adversaries.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Cloud
  Computing Security Workshop}, pp.\  93--104, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020why}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S., Kumar, S., and
  Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  15383--15393. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf}.

\bibitem[Zhao et~al.(2003)Zhao, Huang, Stribling, Rhea, Joseph, and
  Kubiatowicz]{tapestry}
Zhao, B., Huang, L., Stribling, J., Rhea, S., Joseph, A., and Kubiatowicz, J.
\newblock Tapestry: A resilient global-scale overlay for service deployment.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 22, 07 2003.
\newblock \doi{10.1109/JSAC.2003.818784}.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{localsgd_first}
Zinkevich, M., Weimer, M., Li, L., and Smola, A.
\newblock Parallelized stochastic gradient descent.
\newblock In Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and
  Culotta, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~23, pp.\  2595--2603. Curran Associates, Inc., 2010.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf}.

\end{thebibliography}
