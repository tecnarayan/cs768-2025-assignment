\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GMMM21}

\bibitem[ACCD12]{arias2012fundamental}
Ery Arias-Castro, Emmanuel~J Candes, and Mark~A Davenport.
\newblock On the fundamental limits of adaptive sensing.
\newblock {\em IEEE Transactions on Information Theory}, 59(1):472--481, 2012.

\bibitem[AFH{\etalchar{+}}11]{agarwal2011stochastic}
Alekh Agarwal, Dean~P Foster, Daniel Hsu, Sham~M Kakade, and Alexander Rakhlin.
\newblock Stochastic convex optimization with bandit feedback.
\newblock {\em arXiv preprint arXiv:1107.1744}, 2011.

\bibitem[AGH{\etalchar{+}}14]{anandkumar2014tensor}
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham~M Kakade, and Matus Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock {\em Journal of machine learning research}, 15:2773--2832, 2014.

\bibitem[AJKS19]{agarwal2019reinforcement}
Alekh Agarwal, Nan Jiang, Sham~M Kakade, and Wen Sun.
\newblock Reinforcement learning: Theory and algorithms.
\newblock {\em CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, 2019.

\bibitem[AYBM14]{abbasi2014linear}
Yasin Abbasi-Yadkori, Peter~L Bartlett, and Alan Malek.
\newblock Linear programming for large-scale {M}arkov decision problems.
\newblock {\em arXiv preprint arXiv:1402.6763}, 2014.

\bibitem[AYPS11]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em NIPS}, volume~11, pages 2312--2320, 2011.

\bibitem[AYPS12]{abbasi2012online}
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari.
\newblock Online-to-confidence-set conversions and application to sparse
  stochastic bandits.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1--9. PMLR,
  2012.

\bibitem[AZL17]{allen2017first}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock First efficient convergence for streaming k-pca: a global, gap-free,
  and near-optimal rate.
\newblock In {\em 2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 487--492. IEEE, 2017.

\bibitem[AZL19]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock {\em arXiv preprint arXiv:1905.10337}, 2019.

\bibitem[BCB12]{bubeck2012regret}
Sebastien Bubeck and Nicolo Cesa-Bianchi.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock {\em Foundations and Trends in Machine Learning}, 5(1), 2012.

\bibitem[BCR13]{bochnak2013real}
Jacek Bochnak, Michel Coste, and Marie-Fran{\c{c}}oise Roy.
\newblock {\em Real algebraic geometry}, volume~36.
\newblock Springer Science \& Business Media, 2013.

\bibitem[BDWY16]{balcan2016improved}
Maria-Florina Balcan, Simon~Shaolei Du, Yining Wang, and Adams~Wei Yu.
\newblock An improved gap-dependency analysis of the noisy power method.
\newblock In {\em Conference on Learning Theory}, pages 284--309. PMLR, 2016.

\bibitem[BL20]{bai2019beyond}
Yu~Bai and Jason~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[BLE17]{bubeck2017kernel}
S{\'e}bastien Bubeck, Yin~Tat Lee, and Ronen Eldan.
\newblock Kernel-based methods for bandit convex optimization.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 72--85, 2017.

\bibitem[CBL{\etalchar{+}}20]{chen2020towards}
Minshuo Chen, Yu~Bai, Jason~D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and
  Richard Socher.
\newblock Towards understanding hierarchical learning: Benefits of neural
  representations.
\newblock {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[CLM{\etalchar{+}}16]{cai2016optimal}
T~Tony Cai, Xiaodong Li, Zongming Ma, et~al.
\newblock Optimal rates of convergence for noisy sparse phase retrieval via
  thresholded wirtinger flow.
\newblock {\em The Annals of Statistics}, 44(5):2221--2251, 2016.

\bibitem[CLS15]{candes2015phase}
Emmanuel~J Candes, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 61(4):1985--2007,
  2015.

\bibitem[CM20]{chen2020learning}
Sitan Chen and Raghu Meka.
\newblock Learning polynomials in few relevant dimensions.
\newblock In {\em Conference on Learning Theory}, pages 1161--1227. PMLR, 2020.

\bibitem[DHK08]{dani2008stochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock {\em The 21st Annual Conference on Learning Theory}, 2008.

\bibitem[DHK{\etalchar{+}}20]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock {\em arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[DKL{\etalchar{+}}21]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen
  Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in {RL}.
\newblock {\em arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[DLL{\etalchar{+}}19]{du2019gradient}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1675--1685, 2019.

\bibitem[DML21]{damian2021label}
Alex Damian, Tengyu Ma, and Jason Lee.
\newblock Label noise sgd provably prefers flat global minimizers.
\newblock {\em arXiv preprint arXiv:2106.06530}, 2021.

\bibitem[DYM21]{dong2021provable}
Kefan Dong, Jiaqi Yang, and Tengyu Ma.
\newblock Provable model-based nonlinear bandit and reinforcement learning:
  Shelve optimism, embrace virtual curvature.
\newblock {\em arXiv preprint arXiv:2102.04168}, 2021.

\bibitem[FKM04]{flaxman2004online}
Abraham~D Flaxman, Adam~Tauman Kalai, and H~Brendan McMahan.
\newblock Online convex optimization in the bandit setting: gradient descent
  without a gradient.
\newblock {\em arXiv preprint cs/0408007}, 2004.

\bibitem[FLYZ20]{fang2020modeling}
Cong Fang, Jason~D Lee, Pengkun Yang, and Tong Zhang.
\newblock Modeling from features: a mean-field framework for over-parameterized
  deep neural networks.
\newblock {\em arXiv preprint arXiv:2007.01452}, 2020.

\bibitem[GCL{\etalchar{+}}19]{gao2019convergence}
Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, and Jason~D Lee.
\newblock Convergence of adversarial training in overparametrized networks.
\newblock {\em Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[GHM15]{garber2015online}
Dan Garber, Elad Hazan, and Tengyu Ma.
\newblock Online learning of eigenvectors.
\newblock In {\em International Conference on Machine Learning}, pages
  560--568. PMLR, 2015.

\bibitem[GLM18]{ge2018learning}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[GMMM19]{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em arXiv preprint arXiv:1904.12191}, 2019.

\bibitem[GMMM21]{ghorbani2021linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em The Annals of Statistics}, 49(2):1029--1054, 2021.

\bibitem[GMZ16]{gopalan2016low}
Aditya Gopalan, Odalric-Ambrym Maillard, and Mohammadi Zaki.
\newblock Low-rank bandits with latent mixtures.
\newblock {\em arXiv preprint arXiv:1609.01508}, 2016.

\bibitem[HCJ{\etalchar{+}}21]{hu2021nearoptimal}
Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang.
\newblock Near-optimal representation learning for linear bandits and linear
  rl, 2021.

\bibitem[HL16]{hazan2016optimal}
Elad Hazan and Yuanzhi Li.
\newblock An optimal algorithm for bandit convex optimization.
\newblock {\em arXiv preprint arXiv:1603.04350}, 2016.

\bibitem[HLSW21]{hao2021online}
Botao Hao, Tor Lattimore, Csaba Szepesv{\'a}ri, and Mengdi Wang.
\newblock Online sparse reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 316--324. PMLR, 2021.

\bibitem[HLW20]{hao2020high}
Botao Hao, Tor Lattimore, and Mengdi Wang.
\newblock High-dimensional sparse linear bandits.
\newblock {\em arXiv preprint arXiv:2011.04020}, 2020.

\bibitem[HP14]{hardt2014noisy}
Moritz Hardt and Eric Price.
\newblock The noisy power method: A meta algorithm with applications.
\newblock {\em Advances in neural information processing systems},
  27:2861--2869, 2014.

\bibitem[HWLM20]{haochen2020shape}
Jeff~Z. HaoChen, Colin Wei, Jason~D. Lee, and Tengyu Ma.
\newblock Shape matters: Understanding the implicit bias of the noise
  covariance.
\newblock {\em arXiv preprint arXiv:2006.08680}, 2020.

\bibitem[HZWS20]{hao2020low}
Botao Hao, Jie Zhou, Zheng Wen, and Will~Wei Sun.
\newblock Low-rank tensor bandits.
\newblock {\em arXiv preprint arXiv:2007.15788}, 2020.

\bibitem[JHG18]{jacot2018neural}
Arthur Jacot, Cl{\'e}ment Hongler, and Franck Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em NeurIPS}, 2018.

\bibitem[JLM21]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[JSB16]{johnson2016structured}
Nicholas Johnson, Vidyashankar Sivakumar, and Arindam Banerjee.
\newblock Structured stochastic linear bandits.
\newblock {\em arXiv preprint arXiv:1606.05693}, 2016.

\bibitem[JWWN19]{jun2019bilinear}
Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak.
\newblock Bilinear bandits with low-rank structure.
\newblock In {\em International Conference on Machine Learning}, pages
  3163--3172. PMLR, 2019.

\bibitem[KKS{\etalchar{+}}17]{katariya2017stochastic}
Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade, and Zheng
  Wen.
\newblock Stochastic rank-1 bandits.
\newblock In {\em Artificial Intelligence and Statistics}, pages 392--401.
  PMLR, 2017.

\bibitem[Kle04]{kleinberg2004nearly}
Robert Kleinberg.
\newblock Nearly tight bounds for the continuum-armed bandit problem.
\newblock {\em Advances in Neural Information Processing Systems}, 17:697--704,
  2004.

\bibitem[KN19]{kotlowski2019bandit}
Wojciech Kot{\l}owski and Gergely Neu.
\newblock Bandit principal component analysis.
\newblock In {\em Conference On Learning Theory}, pages 1994--2024. PMLR, 2019.

\bibitem[KTB19]{kileel2019expressive}
Joe Kileel, Matthew Trager, and Joan Bruna.
\newblock On the expressive power of deep polynomial neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[LAAH19]{lale2019stochastic}
Sahin Lale, Kamyar Azizzadenesheli, Anima Anandkumar, and Babak Hassibi.
\newblock Stochastic linear bandits with hidden low rank structure.
\newblock {\em arXiv preprint arXiv:1901.09490}, 2019.

\bibitem[Lat20]{lattimore2020improved}
Tor Lattimore.
\newblock Improved regret for zeroth-order adversarial bandit convex
  optimisation, 2020.

\bibitem[LCLS10]{li2010contextual}
Lihong Li, Wei Chu, John Langford, and Robert~E Schapire.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In {\em Proceedings of the 19th international conference on World
  wide web}, pages 661--670, 2010.

\bibitem[LH21]{lattimore2021bandit}
Tor Lattimore and Botao Hao.
\newblock Bandit phase retrieval.
\newblock {\em arXiv preprint arXiv:2106.01660}, 2021.

\bibitem[LL18]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem[LM13]{lecue2013minimax}
Guillaume Lecu{\'e} and Shahar Mendelson.
\newblock Minimax rate of convergence and the performance of erm in phase
  recovery.
\newblock {\em arXiv preprint arXiv:1311.5024}, 2013.

\bibitem[LMT21]{lu2021low}
Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari.
\newblock Low-rank generalized linear bandit problems.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 460--468. PMLR, 2021.

\bibitem[LS20]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[MGW{\etalchar{+}}20]{moroshko2020implicit}
Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason~D Lee, Nathan Srebro,
  and Daniel Soudry.
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Mil17]{milneAG}
James~S. Milne.
\newblock Algebraic geometry (v6.02), 2017.
\newblock Available at www.jmilne.org/math/.

\bibitem[MM15]{musco2015randomized}
Cameron Musco and Christopher Musco.
\newblock Randomized block krylov methods for stronger and faster approximate
  singular value decomposition.
\newblock {\em arXiv preprint arXiv:1504.05477}, 2015.

\bibitem[NGL{\etalchar{+}}19]{nacson2019lexicographic}
Mor~Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel
  Soudry.
\newblock Lexicographic and depth-sensitive margins in homogeneous and
  non-homogeneous deep models.
\newblock In {\em International Conference on Machine Learning}, pages
  4683--4692. PMLR, 2019.

\bibitem[RT10]{rusmevichientong2010linearly}
Paat Rusmevichientong and John~N Tsitsiklis.
\newblock Linearly parameterized bandits.
\newblock {\em Mathematics of Operations Research}, 35(2):395--411, 2010.

\bibitem[RTS18]{riquelme2018deep}
Carlos Riquelme, George Tucker, and Jasper Snoek.
\newblock Deep bayesian bandits showdown: An empirical comparison of bayesian
  deep networks for thompson sampling, 2018.

\bibitem[RVR13]{russo2013eluder}
Dan Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2256--2264, 2013.

\bibitem[SBRL19]{sanjabi2019does}
Maziar Sanjabi, Sina Baharlouei, Meisam Razaviyayn, and Jason~D Lee.
\newblock When does non-orthogonal tensor decomposition have no spurious local
  minima?
\newblock {\em arXiv preprint arXiv:1911.09815}, 2019.

\bibitem[Sha13]{shafarevich2013basic}
Igor~R Shafarevich.
\newblock {\em Basic Algebraic Geometry 1: Varieties in Projective Space}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Tro12]{tropp2012user}
Joel~A Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock {\em Foundations of computational mathematics}, 12(4):389--434, 2012.

\bibitem[VKM{\etalchar{+}}13]{valko2013finite}
Michal Valko, Nathaniel Korda, R{\'e}mi Munos, Ilias Flaounas, and Nelo
  Cristianini.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock {\em arXiv preprint arXiv:1309.6869}, 2013.

\bibitem[WA16]{wang2016online}
Yining Wang and Animashree Anandkumar.
\newblock Online and differentially-private tensor decomposition.
\newblock {\em arXiv preprint arXiv:1606.06237}, 2016.

\bibitem[WGL{\etalchar{+}}19]{woodworth2019kernel}
Blake Woodworth, Suriya Genesekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and deep regimes in overparametrized models.
\newblock In {\em Conference on Learning Theory (COLT)}, 2019.

\bibitem[WLLM19]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9709--9721, 2019.

\bibitem[WSY20]{wang2020reinforcement}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6123--6135, 2020.

\bibitem[WWL{\etalchar{+}}20]{wang2020beyond}
Xiang Wang, Chenwei Wu, Jason~D Lee, Tengyu Ma, and Rong Ge.
\newblock Beyond lazy training for over-parameterized tensor decomposition.
\newblock {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[WX19]{wang2019generalized}
Yang Wang and Zhiqiang Xu.
\newblock Generalized phase retrieval: measurement number, matrix recovery and
  beyond.
\newblock {\em Applied and Computational Harmonic Analysis}, 47(2):423--446,
  2019.

\bibitem[XWZG20]{xu2020neural}
Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu.
\newblock Neural contextual bandits with deep representation and shallow
  exploration.
\newblock {\em arXiv preprint arXiv:2012.01780}, 2020.

\bibitem[YHLD20]{yang2020provable}
Jiaqi Yang, Wei Hu, Jason~D Lee, and Simon~S Du.
\newblock Provable benefits of representation learning in linear bandits.
\newblock {\em arXiv preprint arXiv:2010.06531}, 2020.

\bibitem[ZLKB20]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent {Bellman} error.
\newblock In {\em International Conference on Machine Learning}, 2020.

\end{thebibliography}
