% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{16}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
X.~Glorot and Y.~Bengio, ``Understanding the difficulty of training deep
  feedforward neural networks,'' in \emph{Proceedings of the International
  Conference on Artificial Intelligence and Statistics}, 2010, pp. 249--256.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2013exact}
A.~M. Saxe, J.~L. McClelland, and S.~Ganguli, ``Exact solutions to the
  nonlinear dynamics of learning in deep linear neural networks,''
  \emph{Proceedings of the International Conference on Learning
  Representations}, 2014.

\bibitem[Sussillo and Abbott(2014)]{sussillo2014random}
D.~Sussillo and L.~Abbott, ``Random walk initialization for training very deep
  feedforward networks,'' \emph{arXiv preprint arXiv:1412.6558}, 2014.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Delving deep into rectifiers: Surpassing
  human-level performance on {ImageNet} classification,'' in \emph{Proceedings
  of the IEEE International Conference on Computer Vision}, 2015, pp.
  1026--1034.

\bibitem[Mishkin and Matas(2016)]{mishkin2015all}
D.~Mishkin and J.~Matas, ``All you need is a good init,'' \emph{Proceedings of
  International Conference on Learning Representations}, 2016.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011deep}
X.~Glorot, A.~Bordes, and Y.~Bengio, ``Deep sparse rectifier neural networks,''
  in \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2011, pp. 315--323.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~E. Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: a simple way to prevent neural networks from overfitting.''
  \emph{Journal of Machine Learning Research}, vol.~15, no.~1, pp. 1929--1958,
  2014.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``{ImageNet} classification with
  deep convolutional neural networks,'' in \emph{Advances in Neural Information
  Processing Systems}, 2012, pp. 1097--1105.

\bibitem[Dahl et~al.(2013)Dahl, Sainath, and Hinton]{dahl2013improving}
G.~E. Dahl, T.~N. Sainath, and G.~E. Hinton, ``Improving deep neural networks
  for {LVCSR} using rectified linear units and dropout,'' in \emph{Proceedings
  of the IEEE International Conference on Acoustics, Speech and Signal
  Processing}, 2013, pp. 8609--8613.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole2016exponential}
B.~Poole, S.~Lahiri, M.~Raghu, J.~Sohl-Dickstein, and S.~Ganguli, ``Exponential
  expressivity in deep neural networks through transient chaos,'' in
  \emph{Advances in Neural Information Processing Systems}, 2016, pp.
  3360--3368.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
S.~S. Schoenholz, J.~Gilmer, S.~Ganguli, and J.~Sohl-Dickstein, ``Deep
  information propagation,'' \emph{Proceedings of the International Conference
  on Learning Representations}, 2017.

\bibitem[Yang and Schoenholz(2017)]{yang2017mean}
G.~Yang and S.~Schoenholz, ``Mean field residual networks: On the edge of
  chaos,'' in \emph{Advances in Neural Information Processing Systems}, 2017,
  pp. 7103--7114.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
L.~Xiao, Y.~Bahri, J.~Sohl-Dickstein, S.~S. Schoenholz, and J.~Pennington,
  ``Dynamical isometry and a mean field theory of \uppercase{CNN}s: How to
  train 10,000-layer vanilla convolutional neural networks,'' \emph{Proceedings
  of the International Conference on Machine Learning}, 2018.

\bibitem[Chen et~al.(2018)Chen, Pennington, and Schoenholz]{chen2018dynamical}
M.~Chen, J.~Pennington, and S.~S. Schoenholz, ``Dynamical isometry and a mean
  field theory of \uppercase{RNN}s: Gating enables signal propagation in
  recurrent neural networks,'' \emph{Proceedings of the International
  Conference on Machine Learning}, 2018.

\bibitem[Hayou et~al.(2018)Hayou, Doucet, and Rousseau]{hayou2018selection}
S.~Hayou, A.~Doucet, and J.~Rousseau, ``On the selection of initialization and
  activation function for deep neural networks,'' \emph{arXiv preprint
  arXiv:1805.08266}, 2018.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\end{thebibliography}
