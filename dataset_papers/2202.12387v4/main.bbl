\begin{thebibliography}{10}

\bibitem{swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock {\em arXiv preprint arXiv:2006.09882}, 2020.

\bibitem{chen2021simpler}
Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung
  Chung, Yi~Xu, Belinda Zeng, Wenlian Lu, et~al.
\newblock Simpler, faster, stronger: Breaking the log-k curse on contrastive
  learners with flatnce.
\newblock {\em arXiv preprint arXiv:2107.01152}, 2021.

\bibitem{flatnce}
Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung
  Chung, Yi~Xu, Belinda Zeng, Wenlian Lu, et~al.
\newblock Simpler, faster, stronger: Breaking the log-k curse on contrastive
  learners with flatnce.
\newblock {\em arXiv preprint arXiv:2107.01152}, 2021.

\bibitem{simclrv1}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{simclrv2}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock {\em arXiv preprint arXiv:2006.10029}, 2020.

\bibitem{mocov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{DBLP:conf/cvpr/ChenH21}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition,
  {CVPR} 2021, virtual, June 19-25, 2021}, pages 15750--15758. Computer Vision
  Foundation / {IEEE}, 2021.

\bibitem{mocov3}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock {\em arXiv preprint arXiv:2104.02057}, 2021.

\bibitem{chopra2005learning}
Sumit Chopra, Raia Hadsell, and Yann LeCun.
\newblock Learning a similarity metric discriminatively, with application to
  face verification.
\newblock In {\em 2005 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition (CVPR'05)}, volume~1, pages 539--546. IEEE, 2005.

\bibitem{chuang2020debiased}
Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie
  Jegelka.
\newblock Debiased contrastive learning.
\newblock {\em Advances in neural information processing systems},
  33:8765--8775, 2020.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{Dosovitskiy2021AnII}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em ArXiv}, abs/2010.11929, 2021.

\bibitem{dwibedi2021little}
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew
  Zisserman.
\newblock With a little help from my friends: Nearest-neighbor contrastive
  learning of visual representations.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9588--9597, 2021.

\bibitem{byol}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock {\em arXiv preprint arXiv:2006.07733}, 2020.

\bibitem{guo2021stochastic}
Zhishuai Guo, Yi~Xu, Wotao Yin, Rong Jin, and Tianbao Yang.
\newblock On stochastic moving-average estimators for non-convex optimization.
\newblock {\em arXiv preprint arXiv:2104.14840}, 2021.

\bibitem{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 297--304. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{10.1109/CVPR.2006.100}
Raia Hadsell, Sumit Chopra, and Yann LeCun.
\newblock Dimensionality reduction by learning an invariant mapping.
\newblock In {\em Proceedings of the 2006 IEEE Computer Society Conference on
  Computer Vision and Pattern Recognition - Volume 2}, CVPR '06, page
  1735â€“1742, USA, 2006. IEEE Computer Society.

\bibitem{mocov1}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9729--9738, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{open_clip}
Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini, Rohan Taori, Achal Dave,
  Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali
  Farhadi, and Ludwig Schmidt.
\newblock Openclip, July 2021.
\newblock If you use this software, please cite it as below.

\bibitem{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em arXiv preprint arXiv:2004.11362}, 2020.

\bibitem{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{li2021efficient}
Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai,
  Lu~Yuan, and Jianfeng Gao.
\newblock Efficient self-supervised vision transformers for representation
  learning.
\newblock {\em arXiv preprint arXiv:2106.09785}, 2021.

\bibitem{li2020prototypical}
Junnan Li, Pan Zhou, Caiming Xiong, and Steven~CH Hoi.
\newblock Prototypical contrastive learning of unsupervised representations.
\newblock {\em arXiv preprint arXiv:2005.04966}, 2020.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock {\em arXiv preprint arXiv:1301.3781}, 2013.

\bibitem{ReLiCv1}
Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles
  Blundell.
\newblock Representation learning via invariant causal mechanisms.
\newblock {\em arXiv preprint arXiv:2010.07922}, 2020.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{qi2021stochastic}
Qi~Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang.
\newblock Stochastic optimization of areas under precision-recall curves with
  provable convergence.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{Qian2021SpatiotemporalCV}
Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, H.~Wang, Serge~J.
  Belongie, and Yin Cui.
\newblock Spatiotemporal contrastive video representation learning.
\newblock {\em 2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 6960--6970, 2021.

\bibitem{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock {\em arXiv preprint arXiv:2103.00020}, 2021.

\bibitem{sohn2016improved}
Kihyuk Sohn.
\newblock Improved deep metric learning with multi-class n-pair loss objective.
\newblock In {\em Advances in neural information processing systems}, pages
  1857--1865, 2016.

\bibitem{tian2020makes}
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
  Phillip Isola.
\newblock What makes for good views for contrastive learning?
\newblock {\em Advances in Neural Information Processing Systems},
  33:6827--6839, 2020.

\bibitem{ReLiCv2}
Nenad Tomasev, Ioana Bica, Brian McWilliams, Lars Buesing, Razvan Pascanu,
  Charles Blundell, and Jovana Mitrovic.
\newblock Pushing the limits of self-supervised resnets: Can we outperform
  supervised learning without labels on imagenet?
\newblock {\em arXiv preprint arXiv:2201.05119}, 2022.

\bibitem{wang2020understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In {\em International Conference on Machine Learning}, pages
  9929--9939. PMLR, 2020.

\bibitem{wu2019large}
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and
  Yun Fu.
\newblock Large scale incremental learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 374--382, 2019.

\bibitem{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock {\em arXiv preprint arXiv:1708.03888}, 2017.

\bibitem{pmlr-v139-zbontar21a}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and Stephane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 12310--12320. PMLR, 18--24 Jul 2021.

\bibitem{Zhai2021ScalingVT}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock {\em ArXiv}, abs/2106.04560, 2021.

\bibitem{zhang2021contrastive}
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher~D Manning, and Curtis
  Langlotz.
\newblock Contrastive learning of medical visual representations from paired
  images and text, 2021.

\bibitem{zhu2020eqco}
Benjin Zhu, Junqiang Huang, Zeming Li, Xiangyu Zhang, and Jian Sun.
\newblock Eqco: Equivalent rules for self-supervised contrastive learning.
\newblock {\em arXiv preprint arXiv:2010.01929}, 2020.

\bibitem{eqco}
Benjin Zhu, Junqiang Huang, Zeming Li, Xiangyu Zhang, and Jian Sun.
\newblock Eqco: Equivalent rules for self-supervised contrastive learning.
\newblock {\em arXiv preprint arXiv:2010.01929}, 2020.

\bibitem{zhu2020deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2010.04159}, 2020.

\end{thebibliography}
