\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,
  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{tensorflow}
Mart{\'{\i}}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
  Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
  Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore,
  Derek~Gordon Murray, Benoit Steiner, Paul~A. Tucker, Vijay Vasudevan, Pete
  Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
\newblock Tensorflow: {A} system for large-scale machine learning.
\newblock In Kimberly Keeton and Timothy Roscoe, editors, \emph{12th {USENIX}
  Symposium on Operating Systems Design and Implementation, {OSDI} 2016,
  Savannah, GA, USA, November 2-4, 2016}, pages 265--283. {USENIX} Association,
  2016.

\bibitem[Amari(1998)]{ng}
Shun{-}ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Comput.}, 10\penalty0 (2):\penalty0 251--276, 1998.
\newblock \doi{10.1162/089976698300017746}.

\bibitem[Avazu(2015)]{avazu}
Avazu.
\newblock Avazu click-through rate prediction.
\newblock \url{https://www.kaggle.com/c/avazu-ctr-prediction/data}, 2015.

\bibitem[Broyden(1970)]{bfgs1}
Charles~G. Broyden.
\newblock The convergence of a class of double-rank minimization algorithms.
\newblock \emph{Journal of the Institute of Mathematics and Its Applications},
  6:\penalty0 6–90, 1970.
\newblock \doi{10.1093/imamat/6.1.76}.

\bibitem[Byrd et~al.(1995)Byrd, Lu, Nocedal, and Zhu]{lbfgs}
Richard~H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu.
\newblock A limited memory algorithm for bound constrained optimization.
\newblock \emph{{SIAM} J. Sci. Comput.}, 16\penalty0 (5):\penalty0 1190--1208,
  1995.
\newblock \doi{10.1137/0916069}.

\bibitem[Cettolo et~al.(2014)Cettolo, Niehues, St{\"u}ker, Bentivogli, and
  Federico]{cettolo-etal-2014-report}
Mauro Cettolo, Jan Niehues, Sebastian St{\"u}ker, Luisa Bentivogli, and
  Marcello Federico.
\newblock Report on the 11th {IWSLT} evaluation campaign.
\newblock In \emph{Proceedings of the 11th International Workshop on Spoken
  Language Translation: Evaluation Campaign}, pages 2--17, Lake Tahoe,
  California, December 4-5 2014.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Zhou, Tang, Yang, Cao, and
  Gu]{Chen2020}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock pages 3239--3247, 07 2020{\natexlab{a}}.
\newblock \doi{10.24963/ijcai.2020/448}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Zhou, Tang, Yang, Cao, and
  Gu]{close_general}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In Christian Bessiere, editor, \emph{Proceedings of the Twenty-Ninth
  International Joint Conference on Artificial Intelligence, {IJCAI} 2020},
  pages 3267--3275. ijcai.org, 2020{\natexlab{b}}.
\newblock \doi{10.24963/ijcai.2020/452}.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{nonconvex_convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Cheng et~al.(2016)Cheng, Koc, Harmsen, Shaked, Chandra, Aradhye,
  Anderson, Corrado, Chai, Ispir, Anil, Haque, Hong, Jain, Liu, and Shah]{wnd}
Heng{-}Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
  Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
  Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
\newblock Wide {\&} deep learning for recommender systems.
\newblock In Alexandros Karatzoglou, Bal{\'{a}}zs Hidasi, Domonkos Tikk,
  Oren~Sar Shalom, Haggai Roitman, Bracha Shapira, and Lior Rokach, editors,
  \emph{Proceedings of the 1st Workshop on Deep Learning for Recommender
  Systems, DLRS@RecSys 2016, Boston, MA, USA, September 15, 2016}, pages 7--10.
  {ACM}, 2016.
\newblock \doi{10.1145/2988450.2988454}.

\bibitem[Criteo(2014)]{criteo}
Criteo.
\newblock Criteo display ad challenge.
\newblock
  \url{http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset},
  2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.
\newblock \doi{10.5555/1953048.2021068}.

\bibitem[Fletcher(1970)]{bfgs2}
R.~Fletcher.
\newblock A new approach to variable metric algorithms.
\newblock \emph{Comput. J.}, 13\penalty0 (3):\penalty0 317--322, 1970.
\newblock \doi{10.1093/comjnl/13.3.317}.

\bibitem[Goldfarb(1970)]{bfgs3}
Donald Goldfarb.
\newblock A family of variable metric updates derived by variational means.
\newblock \emph{Mathematics of Computation}, 24\penalty0 (109):\penalty0
  23–26, 1970.
\newblock \doi{10.1090/S0025-5718-1970-0258249-6}.

\bibitem[Graepel et~al.(2010)Graepel, Candela, Borchert, and Herbrich]{auc}
Thore Graepel, Joaquin~Qui{\~{n}}onero Candela, Thomas Borchert, and Ralf
  Herbrich.
\newblock Web-scale bayesian click-through rate prediction for sponsored search
  advertising in microsoft's bing search engine.
\newblock In Johannes F{\"{u}}rnkranz and Thorsten Joachims, editors,
  \emph{Proceedings of the 27th International Conference on Machine Learning
  (ICML-10), June 21-24, 2010, Haifa, Israel}, pages 13--20. Omnipress, 2010.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In Jennifer~G. Dy and Andreas Krause, editors, \emph{Proceedings of
  the 35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 1837--1845. {PMLR},
  2018.

\bibitem[Keskar and Socher(2017)]{adam2sgd}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to {SGD}.
\newblock \emph{CoRR}, abs/1712.07628, 2017.

\bibitem[Kingma and Ba(2015)]{adam}
Diederik~P. Kingma and Jimmy~Lei Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations}, ICLR '15, San Diego, CA, USA, 2015.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and Hinton]{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock \url{http://www.cs.toronto.edu/~kriz/cifar.html}, 2009.

\bibitem[Ling et~al.(2017)Ling, Deng, Gu, Zhou, Li, and Sun]{ensemble}
Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun.
\newblock Model ensemble for click prediction in bing search ads.
\newblock In Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy
  Gabrilovich, editors, \emph{Proceedings of the 26th International Conference
  on World Wide Web Companion, Perth, Australia, April 3-7, 2017}, pages
  689--698. {ACM}, 2017.
\newblock \doi{10.1145/3041021.3054192}.

\bibitem[Loshchilov and Hutter(2019)]{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{adabound}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{treebank}
Mitchell~P. Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of {E}nglish: {T}he {P}enn
  treebank, June 1993.

\bibitem[Moreau et~al.(2022)Moreau, Massias, Gramfort, Ablin, Bannier,
  Charlier, Dagréou, Dupré~la Tour, Durif, F.~Dantas, Klopfenstein, Larsson,
  Lai, Lefort, Malézieux, Moufad, T.~Nguyen, Rakotomamonjy, Ramzi, Salmon, and
  Vaiter]{benchopt}
Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin,
  Pierre-Antoine Bannier, Benjamin Charlier, Mathieu Dagréou, Tom Dupré~la
  Tour, Ghislain Durif, Cassio F.~Dantas, Quentin Klopfenstein, Johan Larsson,
  En~Lai, Tanguy Lefort, Benoit Malézieux, Badr Moufad, Binh T.~Nguyen, Alain
  Rakotomamonjy, Zaccharie Ramzi, Joseph Salmon, and Samuel Vaiter.
\newblock Benchopt: Reproducible, efficient and collaborative optimization
  benchmarks.
\newblock 2022.

\bibitem[Nesterov(2004)]{convex_optimization}
Yurii~E. Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization - {A} Basic
  Course}, volume~87 of \emph{Applied Optimization}.
\newblock Springer, 2004.
\newblock ISBN 978-1-4613-4691-3.
\newblock \doi{10.1007/978-1-4419-8853-9}.
\newblock URL \url{https://doi.org/10.1007/978-1-4419-8853-9}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
  Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas K{\"{o}}pf, Edward~Z. Yang, Zachary DeVito, Martin Raison,
  Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and
  Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 8024--8035, 2019.

\bibitem[Polyak(1964)]{momentum2}
Boris~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.
\newblock \doi{10.1016/0041-5553(64)90137-5}.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{amsgrad}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations}, ICLR '18, Vancouver, BC, Canada, 2018. OpenReview.net.

\bibitem[Robbins and Monro(1951)]{sgd}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Shanno(1970)]{bfgs4}
David~F. Shanno.
\newblock Conditioning of quasi-newton methods for function minimization.
\newblock \emph{Mathematics of Computation}, 24\penalty0 (111):\penalty0
  647–656, 1970.
\newblock \doi{10.1090/S0025-5718-1970-0274029-X}.

\bibitem[Wang et~al.(2017)Wang, Fu, Fu, and Wang]{dcn}
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.
\newblock Deep {\&} cross network for ad click predictions.
\newblock In \emph{Proceedings of the ADKDD'17, Halifax, NS, Canada, August 13
  - 17, 2017}, pages 12:1--12:7. {ACM}, 2017.
\newblock \doi{10.1145/3124749.3124754}.

\bibitem[Yang et~al.(2016)Yang, Lin, and Li]{momentum_convergence}
Tianbao Yang, Qihang Lin, and Zhe Li.
\newblock Unified convergence analysis of stochastic momentum methods for
  convex and non-convex optimization.
\newblock \emph{CoRR}, abs/1604.03257v2, 2016.

\bibitem[Yao et~al.(2020)Yao, Gholami, Shen, Keutzer, and Mahoney]{adahessian}
Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael~W. Mahoney.
\newblock {ADAHESSIAN:} an adaptive second order optimizer for machine
  learning.
\newblock \emph{CoRR}, abs/2006.00719, 2020.

\bibitem[Zeiler(2012)]{adadelta}
Matthew~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock \emph{CoRR}, abs/1212.5701, 2012.

\bibitem[Zheng et~al.(2017)Zheng, Meng, Wang, Chen, Yu, Ma, and Liu]{asgd}
Shuxin Zheng, Qi~Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhiming Ma, and
  Tie{-}Yan Liu.
\newblock Asynchronous stochastic gradient descent with delay compensation.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pages 4120--4129. {PMLR}, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Tang, Yang, Cao, and
  Gu]{nonconvex_convergence2}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{CoRR}, abs/1808.05671, 2018.

\bibitem[Zhu et~al.(2021)Zhu, Liu, Yang, Zhang, and He]{barsctr}
Jieming Zhu, Jinyang Liu, Shuai Yang, Qi~Zhang, and Xiuqiang He.
\newblock Open benchmarking for click-through rate prediction.
\newblock In Gianluca Demartini, Guido Zuccon, J.~Shane Culpepper, Zi~Huang,
  and Hanghang Tong, editors, \emph{{CIKM} '21: The 30th {ACM} International
  Conference on Information and Knowledge Management, Virtual Event,
  Queensland, Australia, November 1 - 5, 2021}, pages 2759--2769. {ACM}, 2021.
\newblock \doi{10.1145/3459637.3482486}.
\newblock URL \url{https://doi.org/10.1145/3459637.3482486}.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{adabelief}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar~C. Tatikonda, Nicha~C. Dvornek,
  Xenophon Papademetris, and James~S. Duncan.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\end{thebibliography}
