\begin{thebibliography}{}

\bibitem[Abbe et~al., 2023]{abbe2023generalization}
Abbe, E., Bengio, S., Lotfi, A., and Rizk, K. (2023).
\newblock Generalization on the unseen, logic reasoning and degree curriculum.
\newblock In {\em International Conference on Machine Learning}, pages 31--60.
  PMLR.

\bibitem[Agrawal et~al., 2022]{agrawal2022large}
Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., and Sontag, D. (2022).
\newblock Large language models are few-shot clinical information extractors.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 1998--2022.

\bibitem[Balcan et~al., 2004]{balcan2004co}
Balcan, M.-F., Blum, A., and Yang, K. (2004).
\newblock Co-training and expansion: Towards bridging theory and practice.
\newblock {\em Advances in neural information processing systems}, 17.

\bibitem[Ben-David et~al., 2006]{ben2006analysis}
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2006).
\newblock Analysis of representations for domain adaptation.
\newblock {\em Advances in neural information processing systems}, 19.

\bibitem[Blitzer et~al., 2007]{blitzer2007learning}
Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J. (2007).
\newblock Learning bounds for domain adaptation.
\newblock {\em Advances in neural information processing systems}, 20.

\bibitem[Blum and Mitchell, 1998]{blum1998combining}
Blum, A. and Mitchell, T. (1998).
\newblock Combining labeled and unlabeled data with co-training.
\newblock In {\em Proceedings of the eleventh annual conference on
  Computational learning theory}, pages 92--100.

\bibitem[Blum et~al., 1981]{blum1981complexity}
Blum, M., Karp, R.~M., Vornberger, O., Papadimitriu, C.~H., and Yannakakis, M.
  (1981).
\newblock The complexity of testing whether a graph is a superconcentrator.
\newblock {\em Information Processing Letters}, 13(4-5):164--167.

\bibitem[Bousquet et~al., 2003]{bousquet2003introduction}
Bousquet, O., Boucheron, S., and Lugosi, G. (2003).
\newblock Introduction to statistical learning theory.
\newblock In {\em Summer school on machine learning}, pages 169--207. Springer.

\bibitem[Buciluǎ et~al., 2006]{bucilua2006model}
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A. (2006).
\newblock Model compression.
\newblock In {\em Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541.

\bibitem[Burns et~al., 2023]{burns2023weak}
Burns, C., Izmailov, P., Kirchner, J.~H., Baker, B., Gao, L., Aschenbrenner,
  L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., et~al. (2023).
\newblock Weak-to-strong generalization: Eliciting strong capabilities with
  weak supervision.
\newblock {\em arXiv preprint arXiv:2312.09390}.

\bibitem[Cabannnes et~al., 2021]{cabannnes2021disambiguation}
Cabannnes, V.~A., Bach, F., and Rudi, A. (2021).
\newblock Disambiguation of weak supervision leading to exponential convergence
  rates.
\newblock In {\em International Conference on Machine Learning}, pages
  1147--1157. PMLR.

\bibitem[Cai et~al., 2021]{cai2021theory}
Cai, T., Gao, R., Lee, J., and Lei, Q. (2021).
\newblock A theory of label propagation for subpopulation shift.
\newblock In {\em International Conference on Machine Learning}, pages
  1170--1182. PMLR.

\bibitem[Chen et~al., 2022]{chen2022shoring}
Chen, M.~F., Fu, D.~Y., Adila, D., Zhang, M., Sala, F., Fatahalian, K., and
  R{\'e}, C. (2022).
\newblock Shoring up the foundations: Fusing model embeddings and weak
  supervision.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 357--367.
  PMLR.

\bibitem[Chen et~al., 2020]{chen2020self}
Chen, Y., Wei, C., Kumar, A., and Ma, T. (2020).
\newblock Self-training avoids using spurious features under domain shift.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21061--21071.

\bibitem[Chiang and Sugiyama, 2023]{chiang2023unified}
Chiang, C.-K. and Sugiyama, M. (2023).
\newblock Unified risk analysis for weakly supervised learning.
\newblock {\em arXiv preprint arXiv:2309.08216}.

\bibitem[Dawid and Skene, 1979]{dawid1979maximum}
Dawid, A.~P. and Skene, A.~M. (1979).
\newblock Maximum likelihood estimation of observer error-rates using the em
  algorithm.
\newblock {\em Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 28(1):20--28.

\bibitem[Ding et~al., 2023]{ding2022gpt}
Ding, B., Qin, C., Liu, L., Chia, Y.~K., Li, B., Joty, S., and Bing, L. (2023).
\newblock Is {GPT}-3 a good data annotator?
\newblock In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, {\em
  Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)}, pages 11173--11195, Toronto, Canada.
  Association for Computational Linguistics.

\bibitem[Eisenstein et~al., 2022]{eisenstein2022honest}
Eisenstein, J., Andor, D., Bohnet, B., Collins, M., and Mimno, D. (2022).
\newblock Honest students from untrusted teachers: Learning an interpretable
  question-answering pipeline from a pretrained language model.
\newblock In {\em Workshop on Trustworthy and Socially Responsible Machine
  Learning, NeurIPS 2022}.

\bibitem[Frei et~al., 2022]{frei2022self}
Frei, S., Zou, D., Chen, Z., and Gu, Q. (2022).
\newblock Self-training converts weak learners to strong learners in mixture
  models.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 8003--8021. PMLR.

\bibitem[Fries et~al., 2019]{fries2019weakly}
Fries, J.~A., Varma, P., Chen, V.~S., Xiao, K., Tejeda, H., Saha, P., Dunnmon,
  J., Chubb, H., Maskatia, S., Fiterau, M., et~al. (2019).
\newblock Weakly supervised classification of aortic valve malformations using
  unlabeled cardiac mri sequences.
\newblock {\em Nature communications}, 10(1):3111.

\bibitem[Fu et~al., 2020]{fu2020fast}
Fu, D., Chen, M., Sala, F., Hooper, S., Fatahalian, K., and R{\'e}, C. (2020).
\newblock Fast and three-rious: Speeding up weak supervision with triplet
  methods.
\newblock In {\em International Conference on Machine Learning}, pages
  3280--3291. PMLR.

\bibitem[HaoChen et~al., 2021]{haochen2021provable}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T. (2021).
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock {\em Advances in Neural Information Processing Systems},
  34:5000--5011.

\bibitem[HaoChen et~al., 2022]{haochen2022beyond}
HaoChen, J.~Z., Wei, C., Kumar, A., and Ma, T. (2022).
\newblock Beyond separability: Analyzing the linear transferability of
  contrastive representations to related subpopulations.
\newblock {\em Advances in neural information processing systems},
  35:26889--26902.

\bibitem[He et~al., 2023]{he2023annollm}
He, X., Lin, Z., Gong, Y., Jin, A., Zhang, H., Lin, C., Jiao, J., Yiu, S.~M.,
  Duan, N., Chen, W., et~al. (2023).
\newblock Annollm: Making large language models to be better crowdsourced
  annotators.
\newblock {\em arXiv preprint arXiv:2303.16854}.

\bibitem[Irvin et~al., 2019]{irvin2019chexpert}
Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C.,
  Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., et~al. (2019).
\newblock Chexpert: A large chest radiograph dataset with uncertainty labels
  and expert comparison.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 590--597.

\bibitem[Johnson et~al., 2019]{johnson2019mimic}
Johnson, A.~E., Pollard, T.~J., Berkowitz, S.~J., Greenbaum, N.~R., Lungren,
  M.~P., Deng, C.-y., Mark, R.~G., and Horng, S. (2019).
\newblock Mimic-cxr, a de-identified publicly available database of chest
  radiographs with free-text reports.
\newblock {\em Scientific data}, 6(1):317.

\bibitem[Kannan et~al., 2006]{kannan2006blocking}
Kannan, R., Lov{\'a}sz, L., and Montenegro, R. (2006).
\newblock Blocking conductance and mixing in random walks.
\newblock {\em Combinatorics, Probability and Computing}, 15(4):541--570.

\bibitem[Karamanolakis et~al., 2021]{karamanolakis2021self}
Karamanolakis, G., Mukherjee, S., Zheng, G., and Hassan, A. (2021).
\newblock Self-training with weak supervision.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 845--863.

\bibitem[Karger et~al., 2011]{karger2011iterative}
Karger, D., Oh, S., and Shah, D. (2011).
\newblock Iterative learning for reliable crowdsourcing systems.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Khetan et~al., 2018]{khetan2018learning}
Khetan, A., Lipton, Z.~C., and Anandkumar, A. (2018).
\newblock Learning from noisy singly-labeled data.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Kifer et~al., 2004]{kifer2004detecting}
Kifer, D., Ben-David, S., and Gehrke, J. (2004).
\newblock Detecting change in data streams.
\newblock In {\em VLDB}, volume~4, pages 180--191. Toronto, Canada.

\bibitem[Kumar et~al., 2020]{kumar2020understanding}
Kumar, A., Ma, T., and Liang, P. (2020).
\newblock Understanding self-training for gradual domain adaptation.
\newblock In {\em International conference on machine learning}, pages
  5468--5479. PMLR.

\bibitem[Kuzman et~al., 2023]{kuzman2023chatgpt}
Kuzman, T., Mozetic, I., and Ljube{\v{s}}ic, N. (2023).
\newblock Chatgpt: Beginning of an end of manual linguistic data annotation?
  use case of automatic genre identification.
\newblock {\em arXiv preprint arXiv:2303.03953}.

\bibitem[Kwok et~al., 2016]{kwok2016improved}
Kwok, T.~C., Lau, L.~C., and Lee, Y.~T. (2016).
\newblock Improved cheeger's inequality and analysis of local graph
  partitioning using vertex expansion and expansion profile.
\newblock In {\em Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium
  on Discrete Algorithms}, pages 1848--1861. SIAM.

\bibitem[Lang et~al., 2022a]{lang2022co}
Lang, H., Agrawal, M.~N., Kim, Y., and Sontag, D. (2022a).
\newblock Co-training improves prompt-based learning for large language models.
\newblock In {\em International Conference on Machine Learning}, pages
  11985--12003. PMLR.

\bibitem[Lang and Poon, 2021]{lang2021self}
Lang, H. and Poon, H. (2021).
\newblock Self-supervised self-supervision by combining deep learning and
  probabilistic logic.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 4978--4986.

\bibitem[Lang et~al., 2022b]{lang2022training}
Lang, H., Vijayaraghavan, A., and Sontag, D. (2022b).
\newblock Training subset selection for weak supervision.
\newblock {\em Advances in Neural Information Processing Systems},
  35:16023--16036.

\bibitem[Li et~al., 2023a]{li2023characterizing}
Li, J., Zhang, J., Schmidt, L., and Ratner, A. (2023a).
\newblock Characterizing the impacts of semi-supervised learning for weak
  supervision.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}.

\bibitem[Li et~al., 2023b]{li2023self}
Li, X., Yu, P., Zhou, C., Schick, T., Zettlemoyer, L., Levy, O., Weston, J.,
  and Lewis, M. (2023b).
\newblock Self-alignment with instruction backtranslation.
\newblock {\em arXiv preprint arXiv:2308.06259}.

\bibitem[Loshchilov and Hutter, 2018]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F. (2018).
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Maas et~al., 2011]{imdbdata}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
  (2011).
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA. Association for Computational Linguistics.

\bibitem[Makarychev et~al., 2023]{makarychev2023higher}
Makarychev, K., Makarychev, Y., Shan, L., and Vijayaraghavan, A. (2023).
\newblock Higher-order cheeger inequality for partitioning with buffers.
\newblock {\em arXiv preprint arXiv:2308.10160}.

\bibitem[Meng et~al., 2018]{meng2018weakly}
Meng, Y., Shen, J., Zhang, C., and Han, J. (2018).
\newblock Weakly-supervised neural text classification.
\newblock In {\em proceedings of the 27th ACM International Conference on
  information and knowledge management}, pages 983--992.

\bibitem[Muennighoff et~al., 2023]{muennighoff2022mteb}
Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. (2023).
\newblock {MTEB}: Massive text embedding benchmark.
\newblock In Vlachos, A. and Augenstein, I., editors, {\em Proceedings of the
  17th Conference of the European Chapter of the Association for Computational
  Linguistics}, pages 2014--2037, Dubrovnik, Croatia. Association for
  Computational Linguistics.

\bibitem[Natarajan et~al., 2013]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A. (2013).
\newblock Learning with noisy labels.
\newblock {\em Advances in neural information processing systems}, 26.

\bibitem[Northcutt et~al., 2021]{northcutt2021confident}
Northcutt, C., Jiang, L., and Chuang, I. (2021).
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock {\em Journal of Artificial Intelligence Research}, 70:1373--1411.

\bibitem[Oymak and Cihad~Gulcu, 2021]{oymak2021theoretical}
Oymak, S. and Cihad~Gulcu, T. (2021).
\newblock A theoretical characterization of semi-supervised learning with
  self-training for gaussian mixture models.
\newblock In Banerjee, A. and Fukumizu, K., editors, {\em Proceedings of The
  24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of {\em Proceedings of Machine Learning Research}, pages
  3601--3609. PMLR.

\bibitem[Pukdee et~al., 2022]{pukdee2022label}
Pukdee, R., Sam, D., Ravikumar, P.~K., and Balcan, N. (2022).
\newblock Label propagation with weak supervision.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}.

\bibitem[Ratner et~al., 2017]{ratner2017snorkel}
Ratner, A., Bach, S.~H., Ehrenberg, H., Fries, J., Wu, S., and R{\'e}, C.
  (2017).
\newblock Snorkel: Rapid training data creation with weak supervision.
\newblock In {\em Proceedings of the VLDB Endowment. International Conference
  on Very Large Data Bases}, volume~11, page 269. NIH Public Access.

\bibitem[Ratner et~al., 2019]{ratner2019training}
Ratner, A., Hancock, B., Dunnmon, J., Sala, F., Pandey, S., and R{\'e}, C.
  (2019).
\newblock Training complex models with multi-task weak supervision.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4763--4771.

\bibitem[Ratner et~al., 2016]{ratner2016data}
Ratner, A.~J., De~Sa, C.~M., Wu, S., Selsam, D., and R{\'e}, C. (2016).
\newblock Data programming: Creating large training sets, quickly.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Reimers and Gurevych, 2019]{sentencebert}
Reimers, N. and Gurevych, I. (2019).
\newblock Sentence-{BERT}: Sentence embeddings using {S}iamese {BERT}-networks.
\newblock In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, {\em
  Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing and the 9th International Joint Conference on Natural Language
  Processing (EMNLP-IJCNLP)}, pages 3982--3992, Hong Kong, China. Association
  for Computational Linguistics.

\bibitem[Robinson et~al., 2020]{robinson2020strength}
Robinson, J., Jegelka, S., and Sra, S. (2020).
\newblock Strength from weakness: Fast learning using weak supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8127--8136. PMLR.

\bibitem[R{\"u}hling~Cachay et~al., 2021]{ruhling2021end}
R{\"u}hling~Cachay, S., Boecking, B., and Dubrawski, A. (2021).
\newblock End-to-end weak supervision.
\newblock {\em Advances in Neural Information Processing Systems},
  34:1845--1857.

\bibitem[Sam and Kolter, 2023]{sam2023losses}
Sam, D. and Kolter, J.~Z. (2023).
\newblock Losses over labels: Weakly supervised learning via direct loss
  construction.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, pages 9695--9703.

\bibitem[Sauer, 1972]{sauer1972density}
Sauer, N. (1972).
\newblock On the density of families of sets.
\newblock {\em Journal of Combinatorial Theory, Series A}, 13(1):145--147.

\bibitem[Shelah, 1972]{shelah1972combinatorial}
Shelah, S. (1972).
\newblock A combinatorial problem; stability and order for models and theories
  in infinitary languages.
\newblock {\em Pacific Journal of Mathematics}, 41(1):247--261.

\bibitem[Stanton et~al., 2021]{stanton2021does}
Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A.~A., and Wilson, A.~G.
  (2021).
\newblock Does knowledge distillation really work?
\newblock {\em Advances in Neural Information Processing Systems},
  34:6906--6919.

\bibitem[Sugiyama et~al., 2022]{sugiyama2022machine}
Sugiyama, M., Bao, H., Ishida, T., Lu, N., Sakai, T., and Niu, G. (2022).
\newblock {\em Machine Learning from Weak Supervision: An Empirical Risk
  Minimization Approach}.
\newblock MIT Press.

\bibitem[Tsybakov, 2004]{tsybakov}
Tsybakov, A.~B. (2004).
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock {\em The Annals of Statistics}, 32(1):135--166.

\bibitem[Vapnik, 1971]{vapnik1971uniform}
Vapnik, V. (1971).
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock {\em Theory of Probability and its Applications}, 16(2):264--281.

\bibitem[Varma and R{\'e}, 2018]{varma2018snuba}
Varma, P. and R{\'e}, C. (2018).
\newblock Snuba: Automating weak supervision to label training data.
\newblock In {\em Proceedings of the VLDB Endowment. International Conference
  on Very Large Data Bases}, volume~12, page 223. NIH Public Access.

\bibitem[Varma et~al., 2019]{varma2019learning}
Varma, P., Sala, F., He, A., Ratner, A., and R{\'e}, C. (2019).
\newblock Learning dependency structures for weak supervision models.
\newblock In {\em International Conference on Machine Learning}, pages
  6418--6427. PMLR.

\bibitem[Veselovsky et~al., 2023]{veselovsky2023prevalence}
Veselovsky, V., Ribeiro, M.~H., Cozzolino, P., Gordon, A., Rothschild, D., and
  West, R. (2023).
\newblock Prevalence and prevention of large language model use in crowd work.

\bibitem[Wang et~al., 2021]{wang2021want}
Wang, S., Liu, Y., Xu, Y., Zhu, C., and Zeng, M. (2021).
\newblock Want to reduce labeling cost? gpt-3 can help.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 4195--4205.

\bibitem[Wang et~al., 2023]{wang-etal-2023-self-instruct}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and
  Hajishirzi, H. (2023).
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 13484--13508,
  Toronto, Canada. Association for Computational Linguistics.

\bibitem[Wang et~al., 2019]{wang2019clinical}
Wang, Y., Sohn, S., Liu, S., Shen, F., Wang, L., Atkinson, E.~J., Amin, S., and
  Liu, H. (2019).
\newblock A clinical text classification paradigm using weak supervision and
  deep representation.
\newblock {\em BMC medical informatics and decision making}, 19:1--13.

\bibitem[Wei and Ma, 2019]{wei2019improved}
Wei, C. and Ma, T. (2019).
\newblock Improved sample complexities for deep neural networks and robust
  classification via an all-layer margin.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Wei et~al., 2020]{wei2020theoretical}
Wei, C., Shen, K., Chen, Y., and Ma, T. (2020).
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Yu et~al., 2021]{yu2021fine}
Yu, Y., Zuo, S., Jiang, H., Ren, W., Zhao, T., and Zhang, C. (2021).
\newblock Fine-tuning pre-trained language model with weak supervision: A
  contrastive-regularized self-training approach.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1063--1077.

\bibitem[Zhang et~al., 2022a]{zhang2022survey}
Zhang, J., Hsieh, C.-Y., Yu, Y., Zhang, C., and Ratner, A. (2022a).
\newblock A survey on programmatic weak supervision.
\newblock {\em arXiv preprint arXiv:2202.05433}.

\bibitem[Zhang et~al., 2022b]{zhang2022understanding}
Zhang, J., Wang, H., Hsieh, C.-Y., and Ratner, A.~J. (2022b).
\newblock Understanding programmatic weak supervision via source-aware
  influence function.
\newblock {\em Advances in Neural Information Processing Systems},
  35:2862--2875.

\bibitem[Zhang et~al., 2021]{zhang2021wrench}
Zhang, J., Yu, Y., Li, Y., Wang, Y., Yang, Y., Yang, M., and Ratner, A. (2021).
\newblock {WRENCH}: A comprehensive benchmark for weak supervision.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}.

\bibitem[Zhang et~al., 2018]{zhang2018w2f}
Zhang, Y., Bai, Y., Ding, M., Li, Y., and Ghanem, B. (2018).
\newblock W2f: A weakly-supervised to fully-supervised framework for object
  detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 928--936.

\end{thebibliography}
