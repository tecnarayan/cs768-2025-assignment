\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{alon2007network}
Uri Alon.
\newblock Network motifs: theory and experimental approaches.
\newblock {\em Nature Reviews Genetics}, 8(6):450--461, 2007.

\bibitem{andreas2022language}
Jacob Andreas.
\newblock Language models as agent models.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 5769--5779, 2022.

\bibitem{DBLP:journals/corr/abs-2308-09687}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler.
\newblock Graph of thoughts: Solving elaborate problems with large language models.
\newblock In {\em AAAI}, 2024.

\bibitem{brashears2013humans}
Matthew~E Brashears.
\newblock Humans use compression heuristics to improve the recall of social networks.
\newblock {\em Scientific reports}, 3(1):1513, 2013.

\bibitem{brashears2016enemy}
Matthew~E Brashears and Laura~Aufderheide Brashears.
\newblock The enemy of my friend is easy to remember: Balance as a compression heuristic.
\newblock In {\em Advances in group processes}, volume~33, pages 1--31. Emerald Group Publishing Limited, 2016.

\bibitem{brashears2016sex}
Matthew~E Brashears, Emily Hoagland, and Eric Quintane.
\newblock Sex and network recall accuracy.
\newblock {\em Social Networks}, 44:74--84, 2016.

\bibitem{brashears2015microstructures}
Matthew~E Brashears and Eric Quintane.
\newblock The microstructures of network recall: How social networks are encoded and represented in human memory.
\newblock {\em Social Networks}, 41:113--126, 2015.

\bibitem{brewer2000forgetting}
Devon~D Brewer.
\newblock Forgetting in the recall-based elicitation of personal and social networks.
\newblock {\em Social networks}, 22(1):29--43, 2000.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{burt1998personality}
Ronald~S Burt, Joseph~E Jannotta, and James~T Mahoney.
\newblock Personality correlates of structural holes.
\newblock {\em Social networks}, 20(1):63--87, 1998.

\bibitem{chai2023graphllm}
Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang.
\newblock Graphllm: Boosting graph reasoning ability of large language model.
\newblock {\em arXiv preprint arXiv:2310.05845}, 2023.

\bibitem{chen2024llaga}
Runjin Chen, Tong Zhao, AJAY~KUMAR JAISWAL, Neil Shah, and Zhangyang Wang.
\newblock Llaga: Large language and graph assistant.
\newblock In {\em Forty-first International Conference on Machine Learning}, 2024.

\bibitem{chen2023label}
Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang.
\newblock Label-free node classification on graphs with large language models (llms).
\newblock {\em ICLR}, 2024.

\bibitem{creswell2022selection}
Antonia Creswell, Murray Shanahan, and Irina Higgins.
\newblock Selection-inference: Exploiting large language models for interpretable logical reasoning.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{croft2010reactome}
David Croft, Gavin Oâ€™kelly, Guanming Wu, Robin Haw, Marc Gillespie, Lisa Matthews, Michael Caudy, Phani Garapati, Gopal Gopinath, Bijay Jassal, et~al.
\newblock Reactome: a database of reactions, pathways and biological processes.
\newblock {\em Nucleic acids research}, 39(suppl\_1):D691--D697, 2010.

\bibitem{daneman1980individual}
Meredyth Daneman and Patricia~A Carpenter.
\newblock Individual differences in working memory and reading.
\newblock {\em Journal of verbal learning and verbal behavior}, 19(4):450--466, 1980.

\bibitem{fatemi2023talk}
Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi.
\newblock Talk like a graph: Encoding graphs for large language models.
\newblock {\em arXiv preprint arXiv:2310.04560}, 2023.

\bibitem{LLMTest_NeedleInAHaystack}
gkamradt.
\newblock Llmtest needle in a haystack - pressure testing llms.
\newblock \url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}, 2023.

\bibitem{guo2023gpt4graph}
Jiayan Guo, Lun Du, and Hengyu Liu.
\newblock Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.
\newblock {\em arXiv preprint arXiv:2305.15066}, 2023.

\bibitem{he2023explanations}
Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi.
\newblock Explanations as features: Llm-based features for text-attributed graphs.
\newblock {\em ICLR}, 2024.

\bibitem{ibarra1992homophily}
Herminia Ibarra.
\newblock Homophily and differential returns: Sex differences in network structure and access in an advertising firm.
\newblock {\em Administrative science quarterly}, pages 422--447, 1992.

\bibitem{DBLP:conf/emnlp/JiangZDYZW23}
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji{-}Rong Wen.
\newblock Structgpt: {A} general framework for large language model to reason over structured data.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em EMNLP}, 2023.

\bibitem{jin2023large}
Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han.
\newblock Large language models on graphs: A comprehensive survey.
\newblock {\em arXiv preprint arXiv:2312.02783}, 2023.

\bibitem{kilduff2008interpersonal}
Martin Kilduff and David Krackhardt.
\newblock {\em Interpersonal networks in organizations: Cognition, personality, dynamics, and culture}, volume~30.
\newblock Cambridge University Press, 2008.

\bibitem{kuratov2024search}
Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev.
\newblock In search of needles in a 10m haystack: Recurrent memory finds what llms miss.
\newblock {\em arXiv preprint arXiv:2402.10790}, 2024.

\bibitem{leskovec2012learning}
Jure Leskovec and Julian Mcauley.
\newblock Learning to discover social circles in ego networks.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{li2020distance}
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec.
\newblock Distance encoding: Design provably more powerful neural networks for graph representation learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:4465--4478, 2020.

\bibitem{liu2023one}
Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang.
\newblock One for all: Towards training one graph model for all classification tasks.
\newblock {\em arXiv preprint arXiv:2310.00149}, 2023.

\bibitem{DBLP:journals/corr/abs-2310-00149}
Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang.
\newblock One for all: Towards training one graph model for all classification tasks.
\newblock In {\em ICLR}, 2024.

\bibitem{liu2023towards}
Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip~S Yu, et~al.
\newblock Towards graph foundation models: A survey and beyond.
\newblock {\em arXiv preprint arXiv:2310.11829}, 2023.

\bibitem{madaan2022language}
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig.
\newblock Language models of code are few-shot commonsense learners.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 1384--1403, 2022.

\bibitem{mao2024graph}
Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael Galkin, and Jiliang Tang.
\newblock Graph foundation models.
\newblock {\em arXiv preprint arXiv:2402.02216}, 2024.

\bibitem{milo2002network}
Ron Milo, Shai Shen-Orr, Shalev Itzkovitz, Nadav Kashtan, Dmitri Chklovskii, and Uri Alon.
\newblock Network motifs: simple building blocks of complex networks.
\newblock {\em Science}, 298(5594):824--827, 2002.

\bibitem{omodei2017mechanistic}
Elisa Omodei, Matthew~E Brashears, and Alex Arenas.
\newblock A mechanistic model of human recall of social network structure and relationship affect.
\newblock {\em Scientific reports}, 7(1):17133, 2017.

\bibitem{DBLP:journals/corr/abs-2310-05499}
Shirui Pan, Yizhen Zheng, and Yixin Liu.
\newblock Integrating graphs with large language models: Methods and prospects.
\newblock {\em IEEE Intelligent Systems}, 2023.

\bibitem{perozzi2024let}
Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and Jonathan Halcrow.
\newblock Let your graph do the talking: Encoding structured data for llms.
\newblock {\em arXiv preprint arXiv:2402.05862}, 2024.

\bibitem{qian2023can}
Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu.
\newblock Can large language models empower molecular property prediction?
\newblock {\em arXiv preprint arXiv:2307.07443}, 2023.

\bibitem{robins2007introduction}
Garry Robins, Pip Pattison, Yuval Kalish, and Dean Lusher.
\newblock An introduction to exponential random graph (p*) models for social networks.
\newblock {\em Social networks}, 29(2):173--191, 2007.

\bibitem{romney1982predicting}
A~Kimball Romney and Katherine Faust.
\newblock Predicting the structure of a communications network from recalled data.
\newblock {\em Social Networks}, 4(4):285--304, 1982.

\bibitem{roth2021network}
Adam~R Roth, Siyun Peng, Max~E Coleman, Evan Finley, and Brea~L Perry.
\newblock Network recall among older adults with cognitive impairments.
\newblock {\em Social Networks}, 64:99--108, 2021.

\bibitem{rozado2024political}
David Rozado.
\newblock The political preferences of llms.
\newblock {\em arXiv preprint arXiv:2402.01789}, 2024.

\bibitem{sanford2024understanding}
Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni.
\newblock Understanding transformer reasoning capabilities via graph algorithms.
\newblock {\em arXiv preprint arXiv:2405.18512}, 2024.

\bibitem{simpson2011power}
Brent Simpson, Barry Markovsky, and Mike Steketee.
\newblock Power and the perception of social networks.
\newblock {\em Social Networks}, 33(2):166--171, 2011.

\bibitem{sintos2014using}
Stavros Sintos and Panayiotis Tsaparas.
\newblock Using strong triadic closure to characterize ties in social networks.
\newblock In {\em Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining}, pages 1466--1475, 2014.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wang2023can}
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.
\newblock Can language models solve graph problems in natural language?
\newblock In {\em Proceedings of the 2023 Conference on Neural Information Processing Systems}, 2023.

\bibitem{DBLP:journals/corr/abs-2305-10037}
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.
\newblock Can language models solve graph problems in natural language?
\newblock In {\em NeurIPS}, 2023.

\bibitem{wang2021inductive}
Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li.
\newblock Inductive representation learning in temporal networks via causal anonymous walks.
\newblock {\em arXiv preprint arXiv:2101.05974}, 2021.

\bibitem{wang2024graphs}
Yanbang Wang and Jon Kleinberg.
\newblock From graphs to hypergraphs: Hypergraph projection and its reconstruction.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{wang2024relationship}
Yanbang Wang and Jon Kleinberg.
\newblock On the relationship between relevance and conflict in online social link recommendations.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{wang2021tedic}
Yanbang Wang, Pan Li, Chongyang Bai, and Jure Leskovec.
\newblock Tedic: Neural modeling of behavioral patterns in dynamic social interaction networks.
\newblock In {\em Proceedings of the Web Conference 2021}, pages 693--705, 2021.

\bibitem{wang2020generic}
Yanbang Wang, Pan Li, Chongyang Bai, V~Subrahmanian, and Jure Leskovec.
\newblock Generic representation learning for dynamic social interaction.
\newblock In {\em Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining Workshop}, pages 1--9, 2020.

\bibitem{wei2023llmrec}
Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang.
\newblock Llmrec: Large language models with graph augmentation for recommendation.
\newblock In {\em WSDM}, 2024.

\bibitem{yao2023schema}
Yunzhi Yao, Shengyu Mao, Ningyu Zhang, Xiang Chen, Shumin Deng, Xi~Chen, and Huajun Chen.
\newblock Schema-aware reference as prompt improves data-efficient knowledge graph construction.
\newblock In {\em SIGIR}, 2023.

\bibitem{yin2020revisiting}
Haoteng Yin, Yanbang Wang, and Pan Li.
\newblock Revisiting graph neural networks and distance encoding from a practical view.
\newblock {\em arXiv preprint arXiv:2011.12228}, 2020.

\bibitem{zhang2023autoalign}
Rui Zhang, Yixin Su, Bayu~Distiawan Trisedya, Xiaoyan Zhao, Min Yang, Hong Cheng, and Jianzhong Qi.
\newblock Autoalign: Fully automatic and effective knowledge graph alignment enabled by large language models.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering}, 2023.

\bibitem{zhang2023large}
Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu.
\newblock Large graph models: A perspective.
\newblock {\em arXiv preprint arXiv:2308.14522}, 2023.

\bibitem{zhao2023gimlet}
Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi~Liu.
\newblock Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning.
\newblock {\em bioRxiv}, 2023.

\end{thebibliography}
