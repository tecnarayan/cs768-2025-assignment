\begin{thebibliography}{10}

\bibitem{afouras2018deepav}
Triantafyllos Afouras, Joon~Son Chung, Andrew Senior, Oriol Vinyals, and Andrew
  Zisserman.
\newblock Deep audio-visual speech recognition.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2018.

\bibitem{afouras2018deep}
Triantafyllos Afouras, Joon~Son Chung, and Andrew Zisserman.
\newblock Deep lip reading: a comparison of models and an online application.
\newblock In {\em Interspeech}, 2018.

\bibitem{arik2017deep}
Sercan~{\"O} Ar{\i}k, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew
  Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman,
  et~al.
\newblock Deep voice: Real-time neural text-to-speech.
\newblock In {\em International Conference on Machine Learning}, pages
  195--204. PMLR, 2017.

\bibitem{assael2016lipnet}
Yannis~M Assael, Brendan Shillingford, Shimon Whiteson, and Nando De~Freitas.
\newblock Lipnet: End-to-end sentence-level lipreading.
\newblock {\em arXiv preprint arXiv:1611.01599}, 2016.

\bibitem{cao2018vggface2}
Qiong Cao, Li~Shen, Weidi Xie, Omkar~M Parkhi, and Andrew Zisserman.
\newblock Vggface2: A dataset for recognising faces across pose and age.
\newblock In {\em 2018 13th IEEE international conference on automatic face \&
  gesture recognition (FG 2018)}, pages 67--74. IEEE, 2018.

\bibitem{chen2020multispeech}
Mingjian Chen, Xu~Tan, Yi~Ren, Jin Xu, Hao Sun, Sheng Zhao, Tao Qin, and
  Tie-Yan Liu.
\newblock Multispeech: Multi-speaker text to speech with transformer.
\newblock {\em arXiv preprint arXiv:2006.04664}, 2020.

\bibitem{chorowski2015attention}
Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua
  Bengio.
\newblock Attention-based models for speech recognition.
\newblock {\em arXiv preprint arXiv:1506.07503}, 2015.

\bibitem{chu2006objective}
Min Chu and Hu~Peng.
\newblock Objective measure for estimating mean opinion score of synthesized
  speech, April~4 2006.
\newblock US Patent 7,024,362.

\bibitem{chung2017you}
Joon~Son Chung, Amir Jamaludin, and Andrew Zisserman.
\newblock You said that?
\newblock In {\em British Machine Vision Conference}, 2017.

\bibitem{chung2017lip}
Joon~Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman.
\newblock Lip reading sentences in the wild.
\newblock In {\em 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 3444--3453. IEEE, 2017.

\bibitem{chung2016out}
Joon~Son Chung and Andrew Zisserman.
\newblock Out of time: automated lip sync in the wild.
\newblock In {\em Asian conference on computer vision}, pages 251--263.
  Springer, 2016.

\bibitem{Chung17a}
Joon~Son Chung and Andrew Zisserman.
\newblock Lip reading in profile.
\newblock In {\em British Machine Vision Conference}, 2017.

\bibitem{chung2020seeing}
Soo-Whan Chung, Hong~Goo Kang, and Joon~Son Chung.
\newblock Seeing voices and hearing voices: learning discriminative embeddings
  using cross-modal self-supervision.
\newblock {\em arXiv preprint arXiv:2004.14326}, 2020.

\bibitem{edwards2016jali}
Pif Edwards, Chris Landreth, Eugene Fiume, and Karan Singh.
\newblock Jali: an animator-centric viseme model for expressive lip
  synchronization.
\newblock {\em ACM Transactions on Graphics (TOG)}, 35(4):1--11, 2016.

\bibitem{ephrat2017vid2speech}
Ariel Ephrat and Shmuel Peleg.
\newblock Vid2speech: speech reconstruction from silent video.
\newblock In {\em 2017 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 5095--5099. IEEE, 2017.

\bibitem{griffin1984signal}
Daniel Griffin and Jae Lim.
\newblock Signal estimation from modified short-time fourier transform.
\newblock {\em IEEE Transactions on acoustics, speech, and signal processing},
  32(2):236--243, 1984.

\bibitem{halperin2019dynamic}
Tavi Halperin, Ariel Ephrat, and Shmuel Peleg.
\newblock Dynamic temporal alignment of speech to lips.
\newblock In {\em ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3980--3984. IEEE, 2019.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hunt1996unit}
Andrew~J Hunt and Alan~W Black.
\newblock Unit selection in a concatenative speech synthesis system using a
  large speech database.
\newblock In {\em 1996 IEEE International Conference on Acoustics, Speech, and
  Signal Processing Conference Proceedings}, pages 373--376. IEEE, 1996.

\bibitem{jia2018transfer}
Ye~Jia, Yu~Zhang, Ron~J Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen,
  Patrick Nguyen, Ruoming Pang, Ignacio~Lopez Moreno, et~al.
\newblock Transfer learning from speaker verification to multispeaker
  text-to-speech synthesis.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{karras2017audio}
Tero Karras, Timo Aila, Samuli Laine, Antti Herva, and Jaakko Lehtinen.
\newblock Audio-driven facial animation by joint end-to-end learning of pose
  and emotion.
\newblock {\em ACM Transactions on Graphics (TOG)}, 36(4):1--12, 2017.

\bibitem{kello2004neural}
Christopher~T Kello and David~C Plaut.
\newblock A neural network model of the articulatory-acoustic forward mapping
  trained on recordings of articulatory parameters.
\newblock {\em The Journal of the Acoustical Society of America},
  116(4):2354--2364, 2004.

\bibitem{kim2018learning}
Changil Kim, Hijung~Valentina Shin, Tae-Hyun Oh, Alexandre Kaspar, Mohamed
  Elgharib, and Wojciech Matusik.
\newblock On learning associations of faces and voices.
\newblock In {\em Asian Conference on Computer Vision}, pages 276--292.
  Springer, 2018.

\bibitem{kim2020glow}
Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon.
\newblock Glow-tts: A generative flow for text-to-speech via monotonic
  alignment search.
\newblock {\em arXiv preprint arXiv:2005.11129}, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kumar2019melgan}
Kundan Kumar, Rithesh Kumar, Thibault de~Boissiere, Lucas Gestin, Wei~Zhen
  Teoh, Jose Sotelo, Alexandre de~Br{\'e}bisson, Yoshua Bengio, and Aaron
  Courville.
\newblock Melgan: Generative adversarial networks for conditional waveform
  synthesis.
\newblock {\em arXiv preprint arXiv:1910.06711}, 2019.

\bibitem{kumar2019lipper}
Yaman Kumar, Rohit Jain, Khwaja~Mohd Salik, Rajiv~Ratn Shah, Yifang Yin, and
  Roger Zimmermann.
\newblock Lipper: Synthesizing thy speech using multi-view lipreading.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 2588--2595, 2019.

\bibitem{li2019neural}
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu.
\newblock Neural speech synthesis with transformer network.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 6706--6713, 2019.

\bibitem{nagrani2018learnable}
Arsha Nagrani, Samuel Albanie, and Andrew Zisserman.
\newblock Learnable pins: Cross-modal embeddings for person identity.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 71--88, 2018.

\bibitem{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
  Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock {\em arXiv preprint arXiv:1609.03499}, 2016.

\bibitem{parkhi2015deep}
Omkar~M Parkhi, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep face recognition.
\newblock 2015.

\bibitem{petridis2018end}
Stavros Petridis, Themos Stafylakis, Pingehuan Ma, Feipeng Cai, Georgios
  Tzimiropoulos, and Maja Pantic.
\newblock End-to-end audiovisual speech recognition.
\newblock In {\em 2018 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 6548--6552. IEEE, 2018.

\bibitem{ping2018clarinet}
Wei Ping, Kainan Peng, and Jitong Chen.
\newblock Clarinet: Parallel wave generation in end-to-end text-to-speech.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{ping2018deep}
Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan~O Arik, Ajay Kannan, Sharan
  Narang, Jonathan Raiman, and John Miller.
\newblock Deep voice 3: 2000-speaker neural text-to-speech.
\newblock {\em Proc. ICLR}, pages 214--217, 2018.

\bibitem{prajwal2020learning}
KR~Prajwal, Rudrabha Mukhopadhyay, Vinay~P Namboodiri, and CV~Jawahar.
\newblock Learning individual speaking styles for accurate lip to speech
  synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13796--13805, 2020.

\bibitem{prajwal2020lip}
KR~Prajwal, Rudrabha Mukhopadhyay, Vinay~P Namboodiri, and CV~Jawahar.
\newblock A lip sync expert is all you need for speech to lip generation in the
  wild.
\newblock In {\em Proceedings of the 28th ACM International Conference on
  Multimedia}, pages 484--492, 2020.

\bibitem{prenger2019waveglow}
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
\newblock Waveglow: A flow-based generative network for speech synthesis.
\newblock In {\em ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3617--3621. IEEE, 2019.

\bibitem{ren2021fastspeech}
Yi~Ren, Chenxu Hu, Xu~Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu.
\newblock Fastspeech 2: Fast and high-quality end-to-end text to speech.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{ren2019fastspeech}
Yi~Ren, Yangjun Ruan, Xu~Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu.
\newblock Fastspeech: Fast, robust and controllable text to speech.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{richard2021meshtalk}
Alexander Richard, Michael Zollhoefer, Yandong Wen, Fernando de~la Torre, and
  Yaser Sheikh.
\newblock Meshtalk: 3d face animation from speech using cross-modality
  disentanglement.
\newblock {\em arXiv preprint arXiv:2104.08223}, 2021.

\bibitem{shen2018natural}
Jonathan Shen, Ruoming Pang, Ron~J Weiss, Mike Schuster, Navdeep Jaitly,
  Zongheng Yang, Zhifeng Chen, Yu~Zhang, Yuxuan Wang, Rj~Skerrv-Ryan, et~al.
\newblock Natural tts synthesis by conditioning wavenet on mel spectrogram
  predictions.
\newblock In {\em 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4779--4783. IEEE, 2018.

\bibitem{stafylakis2017combining}
Themos Stafylakis and Georgios Tzimiropoulos.
\newblock Combining residual networks with lstms for lipreading.
\newblock {\em arXiv preprint arXiv:1703.04105}, 2017.

\bibitem{suwajanakorn2017synthesizing}
Supasorn Suwajanakorn, Steven~M Seitz, and Ira Kemelmacher-Shlizerman.
\newblock Synthesizing obama: learning lip sync from audio.
\newblock {\em ACM Transactions on Graphics (ToG)}, 36(4):1--13, 2017.

\bibitem{tachibana2018efficiently}
Hideyuki Tachibana, Katsuya Uenoyama, and Shunsuke Aihara.
\newblock Efficiently trainable text-to-speech system based on deep
  convolutional networks with guided attention.
\newblock In {\em 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4784--4788. IEEE, 2018.

\bibitem{taylor2017deep}
Sarah Taylor, Taehwan Kim, Yisong Yue, Moshe Mahler, James Krahe,
  Anastasio~Garcia Rodriguez, Jessica Hodgins, and Iain Matthews.
\newblock A deep learning approach for generalized speech animation.
\newblock {\em ACM Transactions on Graphics (TOG)}, 36(4):1--11, 2017.

\bibitem{thies2020neural}
Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias
  Nie{\ss}ner.
\newblock Neural voice puppetry: Audio-driven facial reenactment.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 716--731, 2020.

\bibitem{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of machine learning research}, 9(11), 2008.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em arXiv preprint arXiv:1706.03762}, 2017.

\bibitem{vougioukas2020realistic}
Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic.
\newblock Realistic speech-driven facial animation with gans.
\newblock {\em International Journal of Computer Vision}, 128(5):1398--1413,
  2020.

\bibitem{wan2018generalized}
Li~Wan, Quan Wang, Alan Papir, and Ignacio~Lopez Moreno.
\newblock Generalized end-to-end loss for speaker verification.
\newblock In {\em 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4879--4883. IEEE, 2018.

\bibitem{wang2017tacotron}
Yuxuan Wang, RJ~Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron~J Weiss, Navdeep
  Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et~al.
\newblock Tacotron: Towards end-to-end speech synthesis.
\newblock {\em arXiv preprint arXiv:1703.10135}, 2017.

\bibitem{wiles2018x2face}
Olivia Wiles, A~Koepke, and Andrew Zisserman.
\newblock X2face: A network for controlling face generation using images,
  audio, and pose codes.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 670--686, 2018.

\bibitem{wu2016merlin}
Zhizheng Wu, Oliver Watts, and Simon King.
\newblock Merlin: An open source neural network speech synthesis system.
\newblock In {\em SSW}, pages 202--207, 2016.

\bibitem{yamamoto2020parallel}
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim.
\newblock Parallel wavegan: A fast waveform generation model based on
  generative adversarial networks with multi-resolution spectrogram.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 6199--6203. IEEE, 2020.

\bibitem{yang2020large}
Yi~Yang, Brendan Shillingford, Yannis Assael, Miaosen Wang, Wendi Liu, Yutian
  Chen, Yu~Zhang, Eren Sezener, Luis~C Cobo, Misha Denil, et~al.
\newblock Large-scale multilingual audio visual dubbing.
\newblock {\em arXiv preprint arXiv:2011.03530}, 2020.

\bibitem{zen2019libritts}
Heiga Zen, Viet Dang, Rob Clark, Yu~Zhang, Ron~J Weiss, Ye~Jia, Zhifeng Chen,
  and Yonghui Wu.
\newblock Libritts: A corpus derived from librispeech for text-to-speech.
\newblock {\em arXiv preprint arXiv:1904.02882}, 2019.

\bibitem{zhang2017s3fd}
Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, and Stan~Z Li.
\newblock S3fd: Single shot scale-invariant face detector.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 192--201, 2017.

\bibitem{zhou2019talking}
Hang Zhou, Yu~Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang.
\newblock Talking face generation by adversarially disentangled audio-visual
  representation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 9299--9306, 2019.

\bibitem{zhou2018visemenet}
Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu Maji, and
  Karan Singh.
\newblock Visemenet: Audio-driven animator-centric speech animation.
\newblock {\em ACM Transactions on Graphics (TOG)}, 37(4):1--10, 2018.

\end{thebibliography}
