\begin{thebibliography}{10}

\bibitem{SVRG++}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved {SVRG} for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In {\em ICML 2016}, volume~48, pages 1080--1089, 2016.

\bibitem{entropy-sgd}
P.~{Chaudhari}, A.~{Choromanska}, S.~{Soatto}, Y.~{LeCun}, C.~{Baldassi},
  C.~{Borgs}, J.~{Chayes}, L.~{Sagun}, and R.~{Zecchina}.
\newblock {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}.
\newblock {\em ArXiv e-prints}, November 2016.

\bibitem{limitcycles}
P.~{Chaudhari} and S.~{Soatto}.
\newblock {Stochastic gradient descent performs variational inference,
  converges to limit cycles for deep networks}.
\newblock {\em ArXiv e-prints}, October 2017.

\bibitem{SAGA}
Aaron Defazio, Francis~R. Bach, and Simon Lacoste{-}Julien.
\newblock {SAGA:} {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em NIPS 2014}, pages 1646--1654, 2014.

\bibitem{sharpminimacangenearlizefordeepnets}
L.~{Dinh}, R.~{Pascanu}, S.~{Bengio}, and Y.~{Bengio}.
\newblock {Sharp Minima Can Generalize For Deep Nets}.
\newblock {\em ArXiv e-prints}, March 2017.

\bibitem{adagrad}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12:2121--2159, 2011.

\bibitem{Ge2015}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points - online stochastic gradient for tensor
  decomposition.
\newblock In {\em COLT 2015}, volume~40, pages 797--842, 2015.

\bibitem{qualitativelycharacterizing}
Ian~J. Goodfellow and Oriol Vinyals.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock {\em CoRR}, abs/1412.6544, 2014.

\bibitem{train-faster}
M.~{Hardt}, B.~{Recht}, and Y.~{Singer}.
\newblock {Train faster, generalize better: Stability of stochastic gradient
  descent}.
\newblock {\em ArXiv e-prints}, September 2015.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{flat_minima_2}
Sepp Hochreiter and JÃ¼rgen Schmidhuber.
\newblock Simplifying neural nets by discovering flat minima.
\newblock In {\em Advances in Neural Information Processing Systems 7}, pages
  529--536. MIT Press, 1995.

\bibitem{hoeffding}
Wassily Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock {\em Journal of the American Statistical Association},
  58(301):13--30, 1963.

\bibitem{trainlonger}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 1729--1739. Curran Associates, Inc., 2017.

\bibitem{densenet}
G.~{Huang}, Z.~{Liu}, K.~Q. {Weinberger}, and L.~{van der Maaten}.
\newblock {Densely Connected Convolutional Networks}.
\newblock {\em ArXiv e-prints}, August 2016.

\bibitem{snapshotensembles}
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John~E. Hopcroft, and Kilian~Q.
  Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock In {\em ICLR 2017}, 2017.

\bibitem{escapesaddlepointsefficiently}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M. Kakade, and Michael~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock {\em CoRR}, abs/1703.00887, 2017.

\bibitem{SVRG}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em NIPS 2013}, pages 315--323, 2013.

\bibitem{largebatchtraining}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In {\em ICLR 2017}, 2017.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{twolayerconvergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em NIPS 2017}, 2017.

\bibitem{sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR:} stochastic gradient descent with restarts.
\newblock In {\em ICLR 2017}, 2017.

\bibitem{SGDasapproximatebayesianinference}
S.~{Mandt}, M.~D. {Hoffman}, and D.~M. {Blei}.
\newblock {Stochastic Gradient Descent as Approximate Bayesian Inference}.
\newblock {\em ArXiv e-prints}, April 2017.

\bibitem{generalizationboundsofsgld}
W.~{Mou}, L.~{Wang}, X.~{Zhai}, and K.~{Zheng}.
\newblock {Generalization Bounds of SGLD for Non-convex Learning: Two
  Theoretical Viewpoints}.
\newblock {\em ArXiv e-prints}, July 2017.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, Incorporated, 1 edition, 2014.

\bibitem{theimpactoflocalgeomatry}
V.~{Patel}.
\newblock {The Impact of Local Geometry and Batch Size on the Convergence and
  Divergence of Stochastic Gradient Descent}.
\newblock {\em ArXiv e-prints}, September 2017.

\bibitem{SAG}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, pages 1--30, 2016.

\bibitem{abayesianperspectiveongeneralization}
S.~L. {Smith} and Q.~V. {Le}.
\newblock {A Bayesian Perspective on Generalization and Stochastic Gradient
  Descent}.
\newblock {\em ArXiv e-prints}, October 2017.

\bibitem{momentum}
Ilya Sutskever, James Martens, George~E. Dahl, and Geoffrey~E. Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em ICML}, pages 1139--1147, 2013.

\bibitem{hittingtime}
Yuchen Zhang, Percy Liang, and Moses Charikar.
\newblock A hitting time analysis of stochastic gradient langevin dynamics.
\newblock {\em CoRR}, abs/1702.05575, 2017.

\end{thebibliography}
