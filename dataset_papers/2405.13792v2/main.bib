@misc{yoon2024compactcompressingretrieveddocuments,
      title={CompAct: Compressing Retrieved Documents Actively for Question Answering}, 
      author={Chanwoong Yoon and Taewhoo Lee and Hyeon Hwang and Minbyul Jeong and Jaewoo Kang},
      year={2024},
      eprint={2407.09014},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09014}, 
}
@misc{liu2024leveragingpassageembeddingsefficient,
      title={Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models}, 
      author={Qi Liu and Bo Wang and Nan Wang and Jiaxin Mao},
      year={2024},
      eprint={2406.14848},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14848}, 
}
@article{beauchemin2024quebec,
  title={Quebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation},
  author={Beauchemin, David and Gagnon, Zachary and Khoury, Ricahrd},
  journal={arXiv preprint arXiv:2410.09623},
  year={2024}
}
@article{zhao2024funnelrag,
  title={FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG},
  author={Zhao, Xinping and Zhong, Yan and Sun, Zetian and Hu, Xinshuo and Liu, Zhenyu and Li, Dongfang and Hu, Baotian and Zhang, Min},
  journal={arXiv preprint arXiv:2410.10293},
  year={2024}
}
@misc{li2024promptcompressionlargelanguage,
      title={Prompt Compression for Large Language Models: A Survey}, 
      author={Zongqian Li and Yinhong Liu and Yixuan Su and Nigel Collier},
      year={2024},
      eprint={2410.12388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12388}, 
}
@misc{li2024500xcompressorgeneralizedpromptcompression,
      title={500xCompressor: Generalized Prompt Compression for Large Language Models}, 
      author={Zongqian Li and Yixuan Su and Nigel Collier},
      year={2024},
      eprint={2408.03094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.03094}, 
}
@misc{pan2024llmlingua2datadistillationefficient,
      title={LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression}, 
      author={Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Rühle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang},
      year={2024},
      eprint={2403.12968},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.12968}, 
}
@misc{izacard2021leveraging,
      title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering}, 
      author={Gautier Izacard and Edouard Grave},
      year={2021},
      eprint={2007.01282},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{karamcheti2024prismatic,
      title={Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models}, 
      author={Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh},
      year={2024},
      eprint={2402.07865},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{li2023sentence,
      title={Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence}, 
      author={Haoran Li and Mingshi Xu and Yangqiu Song},
      year={2023},
      eprint={2305.03010},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{asai-etal-2023-retrieval,
    title = "Retrieval-based Language Models and Applications",
    author = "Asai, Akari  and
      Min, Sewon  and
      Zhong, Zexuan  and
      Chen, Danqi",
    editor = "Chen, Yun-Nung (Vivian)  and
      Margot, Margot  and
      Reddy, Siva",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-tutorials.6",
    doi = "10.18653/v1/2023.acl-tutorials.6",
    pages = "41--46",
    abstract = "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
}
@misc{li2024synthetic,
      title={Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models}, 
      author={Haoran Li and Qingxiu Dong and Zhengyang Tang and Chaojun Wang and Xingxing Zhang and Haoyang Huang and Shaohan Huang and Xiaolong Huang and Zeqiang Huang and Dongdong Zhang and Yuxian Gu and Xin Cheng and Xun Wang and Si-Qing Chen and Li Dong and Wei Lu and Zhifang Sui and Benyou Wang and Wai Lam and Furu Wei},
      year={2024},
      eprint={2402.13064},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhong2022training,
      title={Training Language Models with Memory Augmentation}, 
      author={Zexuan Zhong and Tao Lei and Danqi Chen},
      year={2022},
      eprint={2205.12674},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{nguyen2024sfr,
  title={Sfr-rag: Towards contextually faithful llms},
  author={Nguyen, Xuan-Phi and Pandit, Shrey and Purushwalkam, Senthil and Xu, Austin and Chen, Hailin and Ming, Yifei and Ke, Zixuan and Savarese, Silvio and Xong, Caiming and Joty, Shafiq},
  journal={arXiv preprint arXiv:2409.09916},
  year={2024}
}
@misc{luo2024empirical,
      title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning}, 
      author={Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
      year={2024},
      eprint={2308.08747},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{yun-etal-2023-focus,
    title = "Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification",
    author = "Yun, Jungmin  and
      Kim, Mihyeon  and
      Kim, Youngbin",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.909",
    doi = "10.18653/v1/2023.findings-emnlp.909",
    pages = "13617--13628",
    abstract = "Transformer-based models have achieved dominant performance in numerous NLP tasks. Despite their remarkable successes, pre-trained transformers such as BERT suffer from a computationally expensive self-attention mechanism that interacts with all tokens, including the ones unfavorable to classification performance. To overcome these challenges, we propose integrating two strategies: token pruning and token combining. Token pruning eliminates less important tokens in the attention mechanism{'}s key and value as they pass through the layers. Additionally, we adopt fuzzy logic to handle uncertainty and alleviate potential mispruning risks arising from an imbalanced distribution of each token{'}s importance. Token combining, on the other hand, condenses input sequences into smaller sizes in order to further compress the model. By integrating these two approaches, we not only improve the model{'}s performance but also reduce its computational demands. Experiments with various datasets demonstrate superior performance compared to baseline models, especially with the best improvement over the existing BERT model, achieving +5{\%}p in accuracy and +5.6{\%}p in F1 score. Additionally, memory cost is reduced to 0.61x, and a speedup of 1.64x is achieved.",
}

@inproceedings{li-etal-2023-compressing,
    title = "Compressing Context to Enhance Inference Efficiency of Large Language Models",
    author = "Li, Yucheng  and
      Dong, Bo  and
      Guerin, Frank  and
      Lin, Chenghua",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.391",
    doi = "10.18653/v1/2023.emnlp-main.391",
    pages = "6342--6353",
    abstract = "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM{'}s fixed context length. This paper proposes a method called \textit{Selective Context} that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50{\%} reduction in context cost, resulting in a 36{\%} reduction in inference memory usage and a 32{\%} reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.",
}
@misc{luo2023sail,
      title={SAIL: Search-Augmented Instruction Learning}, 
      author={Hongyin Luo and Yung-Sung Chuang and Yuan Gong and Tianhua Zhang and Yoon Kim and Xixin Wu and Danny Fox and Helen Meng and James Glass},
      year={2023},
      eprint={2305.15225},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chevalier2023adapting,
      title={Adapting Language Models to Compress Contexts}, 
      author={Alexis Chevalier and Alexander Wettig and Anirudh Ajith and Danqi Chen},
      year={2023},
      eprint={2305.14788},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jiang2023longllmlingua,
      title={LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression}, 
      author={Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2023},
      eprint={2310.06839},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{asai2023selfrag,
      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, 
      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
      year={2023},
      eprint={2310.11511},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{feng2023synergistic,
      title={Synergistic Interplay between Search and Large Language Models for Information Retrieval}, 
      author={Jiazhan Feng and Chongyang Tao and Xiubo Geng and Tao Shen and Can Xu and Guodong Long and Dongyan Zhao and Daxin Jiang},
      year={2023},
      eprint={2305.07402},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{khattab2023demonstratesearchpredict,
      title={Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP}, 
      author={Omar Khattab and Keshav Santhanam and Xiang Lisa Li and David Hall and Percy Liang and Christopher Potts and Matei Zaharia},
      year={2023},
      eprint={2212.14024},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{gao2024retrievalaugmented,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2022training,
      title={Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data}, 
      author={Shuohang Wang and Yichong Xu and Yuwei Fang and Yang Liu and Siqi Sun and Ruochen Xu and Chenguang Zhu and Michael Zeng},
      year={2022},
      eprint={2203.08773},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{cheng2022neural,
      title={Neural Machine Translation with Contrastive Translation Memories}, 
      author={Xin Cheng and Shen Gao and Lemao Liu and Dongyan Zhao and Rui Yan},
      year={2022},
      eprint={2212.03140},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{khandelwal2021nearest,
      title={Nearest Neighbor Machine Translation}, 
      author={Urvashi Khandelwal and Angela Fan and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
      year={2021},
      eprint={2010.00710},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{cheng2023decouple,
      title={Decouple knowledge from parameters for plug-and-play language modeling}, 
      author={Xin Cheng and Yankai Lin and Xiuying Chen and Dongyan Zhao and Rui Yan},
      year={2023},
      eprint={2305.11564},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{min2023nonparametric,
      title={Nonparametric Masked Language Modeling}, 
      author={Sewon Min and Weijia Shi and Mike Lewis and Xilun Chen and Wen-tau Yih and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2023},
      eprint={2212.01349},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{khandelwal2020generalization,
      title={Generalization through Memorization: Nearest Neighbor Language Models}, 
      author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
      year={2020},
      eprint={1911.00172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lin2024radit,
      title={RA-DIT: Retrieval-Augmented Dual Instruction Tuning}, 
      author={Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Rich James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Scott Yih},
      year={2024},
      eprint={2310.01352},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{morris2023text,
      title={Text Embeddings Reveal (Almost) As Much As Text}, 
      author={John X. Morris and Volodymyr Kuleshov and Vitaly Shmatikov and Alexander M. Rush},
      year={2023},
      eprint={2310.06816},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2023learning,
      title={Learning to Filter Context for Retrieval-Augmented Generation}, 
      author={Zhiruo Wang and Jun Araki and Zhengbao Jiang and Md Rizwan Parvez and Graham Neubig},
      year={2023},
      eprint={2311.08377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{xu2023recomp,
      title={RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation}, 
      author={Fangyuan Xu and Weijia Shi and Eunsol Choi},
      year={2023},
      eprint={2310.04408},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{chen2023dense,
  title={Dense X Retrieval: What Retrieval Granularity Should We Use?},
  author={Chen, Tong and Wang, Hongwei and Chen, Sihao and Yu, Wenhao and Ma, Kaixin and Zhao, Xinran and Yu, Dong and Zhang, Hongming},
  journal={arXiv preprint arXiv:2312.06648},
  year={2023}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{gao2023enabling,
      title={Enabling Large Language Models to Generate Text with Citations}, 
      author={Tianyu Gao and Howard Yen and Jiatong Yu and Danqi Chen},
      year={2023},
      eprint={2305.14627},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{cheng2023lift,
      title={Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory}, 
      author={Xin Cheng and Di Luo and Xiuying Chen and Lemao Liu and Dongyan Zhao and Rui Yan},
      year={2023},
      eprint={2305.02437},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{borgeaud2022improving,
      title={Improving language models by retrieving from trillions of tokens}, 
      author={Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and Tom Hennigan and Saffron Huang and Loren Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and Karen Simonyan and Jack W. Rae and Erich Elsen and Laurent Sifre},
      year={2022},
      eprint={2112.04426},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{guu2020realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{cheng2024dated,
      title={Dated Data: Tracing Knowledge Cutoffs in Large Language Models}, 
      author={Jeffrey Cheng and Marc Marone and Orion Weller and Dawn Lawrie and Daniel Khashabi and Benjamin Van Durme},
      year={2024},
      eprint={2403.12958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{huang2023survey,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, 
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{asai2024reliable,
      title={Reliable, Adaptable, and Attributable Language Models with Retrieval}, 
      author={Akari Asai and Zexuan Zhong and Danqi Chen and Pang Wei Koh and Luke Zettlemoyer and Hannaneh Hajishirzi and Wen-tau Yih},
      year={2024},
      eprint={2403.03187},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yoran2023making,
      title={Making Retrieval-Augmented Language Models Robust to Irrelevant Context}, 
      author={Ori Yoran and Tomer Wolfson and Ori Ram and Jonathan Berant},
      year={2023},
      eprint={2310.01558},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{xia2024less,
      title={LESS: Selecting Influential Data for Targeted Instruction Tuning}, 
      author={Mengzhou Xia and Sadhika Malladi and Suchin Gururangan and Sanjeev Arora and Danqi Chen},
      year={2024},
      eprint={2402.04333},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wu2024continual,
      title={Continual Learning for Large Language Models: A Survey}, 
      author={Tongtong Wu and Linhao Luo and Yuan-Fang Li and Shirui Pan and Thuy-Trang Vu and Gholamreza Haffari},
      year={2024},
      eprint={2402.01364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhou2023lima,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lin2023train,
      title={How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval}, 
      author={Sheng-Chieh Lin and Akari Asai and Minghan Li and Barlas Oguz and Jimmy Lin and Yashar Mehdad and Wen-tau Yih and Xilun Chen},
      year={2023},
      eprint={2302.07452},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{xiao2023cpack,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2024text,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{deepseekai2024deepseek,
      title={DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}, 
      author={DeepSeek-AI and : and Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Zhe Fu and Huazuo Gao and Kaige Gao and Wenjun Gao and Ruiqi Ge and Kang Guan and Daya Guo and Jianzhong Guo and Guangbo Hao and Zhewen Hao and Ying He and Wenjie Hu and Panpan Huang and Erhang Li and Guowei Li and Jiashi Li and Yao Li and Y. K. Li and Wenfeng Liang and Fangyun Lin and A. X. Liu and Bo Liu and Wen Liu and Xiaodong Liu and Xin Liu and Yiyuan Liu and Haoyu Lu and Shanghao Lu and Fuli Luo and Shirong Ma and Xiaotao Nie and Tian Pei and Yishi Piao and Junjie Qiu and Hui Qu and Tongzheng Ren and Zehui Ren and Chong Ruan and Zhangli Sha and Zhihong Shao and Junxiao Song and Xuecheng Su and Jingxiang Sun and Yaofeng Sun and Minghui Tang and Bingxuan Wang and Peiyi Wang and Shiyu Wang and Yaohui Wang and Yongji Wang and Tong Wu and Y. Wu and Xin Xie and Zhenda Xie and Ziwei Xie and Yiliang Xiong and Hanwei Xu and R. X. Xu and Yanhong Xu and Dejian Yang and Yuxiang You and Shuiping Yu and Xingkai Yu and B. Zhang and Haowei Zhang and Lecong Zhang and Liyue Zhang and Mingchuan Zhang and Minghua Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Qihao Zhu and Yuheng Zou},
      year={2024},
      eprint={2401.02954},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kim2023factkg,
  title={FactKG: Fact verification via reasoning on knowledge graphs},
  author={Kim, Jiho and Park, Sungjin and Kwon, Yeonsu and Jo, Yohan and Thorne, James and Choi, Edward},
  journal={arXiv preprint arXiv:2305.06590},
  year={2023}
}
@misc{jiang2023llmlingua,
      title={LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models}, 
      author={Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2023},
      eprint={2310.05736},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{petroni2021kilt,
      title={KILT: a Benchmark for Knowledge Intensive Language Tasks}, 
      author={Fabio Petroni and Aleksandra Piktus and Angela Fan and Patrick Lewis and Majid Yazdani and Nicola De Cao and James Thorne and Yacine Jernite and Vladimir Karpukhin and Jean Maillard and Vassilis Plachouras and Tim Rocktäschel and Sebastian Riedel},
      year={2021},
      eprint={2009.02252},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{muennighoff2023mteb,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{santhanam2022colbertv2,
      title={ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction}, 
      author={Keshav Santhanam and Omar Khattab and Jon Saad-Falcon and Christopher Potts and Matei Zaharia},
      year={2022},
      eprint={2112.01488},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{yu2023generate,
      title={Generate rather than Retrieve: Large Language Models are Strong Context Generators}, 
      author={Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
      year={2023},
      eprint={2209.10063},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{khattab2021relevanceguided,
      title={Relevance-guided Supervision for OpenQA with ColBERT}, 
      author={Omar Khattab and Christopher Potts and Matei Zaharia},
      year={2021},
      eprint={2007.00814},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{izacard2022atlas,
      title={Atlas: Few-shot Learning with Retrieval Augmented Language Models}, 
      author={Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
      year={2022},
      eprint={2208.03299},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{shi2023replug,
      title={REPLUG: Retrieval-Augmented Black-Box Language Models}, 
      author={Weijia Shi and Sewon Min and Michihiro Yasunaga and Minjoon Seo and Rich James and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
      year={2023},
      eprint={2301.12652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2017reading,
      title={Reading Wikipedia to Answer Open-Domain Questions}, 
      author={Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
      year={2017},
      eprint={1704.00051},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{thorne-etal-2018-fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}
@misc{lin2022truthfulqa,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yang2018hotpotqa,
      title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}, 
      author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
      year={2018},
      eprint={1809.09600},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{berant2013semantic,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1533--1544},
  year={2013}
}
@article{DBLP:journals/corr/abs-2309-06256,
  author       = {Yong Lin and
                  Lu Tan and
                  Hangyu Lin and
                  Zeming Zheng and
                  Renjie Pi and
                  Jipeng Zhang and
                  Shizhe Diao and
                  Haoxiang Wang and
                  Han Zhao and
                  Yuan Yao and
                  Tong Zhang},
  title        = {Speciality vs Generality: An Empirical Study on Catastrophic Forgetting
                  in Fine-tuning Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2309.06256},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.06256},
  doi          = {10.48550/ARXIV.2309.06256},
  eprinttype    = {arXiv},
  eprint       = {2309.06256},
  timestamp    = {Tue, 31 Oct 2023 08:38:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-06256.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{mu2024learning,
      title={Learning to Compress Prompts with Gist Tokens}, 
      author={Jesse Mu and Xiang Lisa Li and Noah Goodman},
      year={2024},
      eprint={2304.08467},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wei2022finetuned,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2022},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chen2021dialogsum,
      title={DialogSum: A Real-Life Scenario Dialogue Summarization Dataset}, 
      author={Yulong Chen and Yang Liu and Liang Chen and Yue Zhang},
      year={2021},
      eprint={2105.06762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{Gliwa_2019,
   title={SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization},
   url={http://dx.doi.org/10.18653/v1/D19-5409},
   DOI={10.18653/v1/d19-5409},
   booktitle={Proceedings of the 2nd Workshop on New Frontiers in Summarization},
   publisher={Association for Computational Linguistics},
   author={Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
   year={2019} }

@misc{see2017point,
      title={Get To The Point: Summarization with Pointer-Generator Networks}, 
      author={Abigail See and Peter J. Liu and Christopher D. Manning},
      year={2017},
      eprint={1704.04368},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bajaj2018ms,
      title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset}, 
      author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
      year={2018},
      eprint={1611.09268},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{jiang-etal-2019-freebaseqa,
    title = "{F}reebase{QA}: A New Factoid {QA} Data Set Matching Trivia-Style Question-Answer Pairs with {F}reebase",
    author = "Jiang, Kelvin  and
      Wu, Dekun  and
      Jiang, Hui",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1028",
    doi = "10.18653/v1/N19-1028",
    pages = "318--323",
    abstract = "In this paper, we present a new data set, named FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically sophisticated questions than other existing data sets.",
}
@inproceedings{yang-etal-2015-wikiqa,
    title = "{W}iki{QA}: A Challenge Dataset for Open-Domain Question Answering",
    author = "Yang, Yi  and
      Yih, Wen-tau  and
      Meek, Christopher",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1237",
    doi = "10.18653/v1/D15-1237",
    pages = "2013--2018",
}
@misc{talmor2019commonsenseqa,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{joshi2017triviaqa,
      title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}, 
      author={Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
      year={2017},
      eprint={1705.03551},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
}
@misc{rajpurkar2018know,
      title={Know What You Don't Know: Unanswerable Questions for SQuAD}, 
      author={Pranav Rajpurkar and Robin Jia and Percy Liang},
      year={2018},
      eprint={1806.03822},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{rogers2020getting,
  title={Getting closer to AI complete question answering: A set of prerequisite real tasks},
  author={Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={8722--8731},
  year={2020}
}
@inproceedings{jin-etal-2019-pubmedqa,
    title = "{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering",
    author = "Jin, Qiao  and
      Dhingra, Bhuwan  and
      Liu, Zhengping  and
      Cohen, William  and
      Lu, Xinghua",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1259",
    doi = "10.18653/v1/D19-1259",
    pages = "2567--2577",
}
@misc{kočiský2017narrativeqa,
      title={The NarrativeQA Reading Comprehension Challenge}, 
      author={Tomáš Kočiský and Jonathan Schwarz and Phil Blunsom and Chris Dyer and Karl Moritz Hermann and Gábor Melis and Edward Grefenstette},
      year={2017},
      eprint={1712.07040},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dua2019drop,
      title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs}, 
      author={Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
      year={2019},
      eprint={1903.00161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{reddy2019coqa,
      title={CoQA: A Conversational Question Answering Challenge}, 
      author={Siva Reddy and Danqi Chen and Christopher D. Manning},
      year={2019},
      eprint={1808.07042},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{chu2023qwen,
  title={Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models},
  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.07919},
  year={2023}
}
@misc{zhang2023videollama,
      title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}, 
      author={Hang Zhang and Xin Li and Lidong Bing},
      year={2023},
      eprint={2306.02858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{snell2022learning,
      title={Learning by Distilling Context}, 
      author={Charlie Snell and Dan Klein and Ruiqi Zhong},
      year={2022},
      eprint={2209.15189},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}
@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}
@inproceedings{oh2018self,
  title={Self-imitation learning},
  author={Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  booktitle={International conference on machine learning},
  pages={3878--3887},
  year={2018},
  organization={PMLR}
}
@misc{ivison2023camels,
      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2311.10702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2023far,
      title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, 
      author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2306.04751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lin2023radit,
      title={RA-DIT: Retrieval-Augmented Dual Instruction Tuning}, 
      author={Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Rich James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Scott Yih},
      year={2023},
      eprint={2310.01352},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ge2023incontext,
      title={In-context Autoencoder for Context Compression in a Large Language Model}, 
      author={Tao Ge and Jing Hu and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
      year={2023},
      eprint={2307.06945},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lu2024deepseekvl,
      title={DeepSeek-VL: Towards Real-World Vision-Language Understanding}, 
      author={Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Hao Yang and Yaofeng Sun and Chengqi Deng and Hanwei Xu and Zhenda Xie and Chong Ruan},
      year={2024},
      eprint={2403.05525},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{li2023llamavid,
      title={LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models}, 
      author={Yanwei Li and Chengyao Wang and Jiaya Jia},
      year={2023},
      eprint={2311.17043},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhu2023minigpt4,
      title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}, 
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
      year={2023},
      eprint={2304.10592},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2023improved,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
      year={2023},
      eprint={2310.03744},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}
@article{malkov2018efficient,
  title={Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author={Malkov, Yu A and Yashunin, Dmitry A},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={4},
  pages={824--836},
  year={2018},
  publisher={IEEE}
}

@article{shrivastava2014asymmetric,
  title={Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)},
  author={Shrivastava, Anshumali and Li, Ping},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
