\begin{thebibliography}{10}

\bibitem{phi2}
Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César~Teodoro Mendes, Weizhu Chen, Allie~Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin~Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat~Singh Behl, Adam~Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi~Zhang.
\newblock Phi-2: The surprising power of small language models, 2023.

\bibitem{NEURIPS2019_c0a62e13}
Ron Banner, Yury Nahshan, and Daniel Soudry.
\newblock Post training 4-bit quantization of convolutional networks for rapid-deployment.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{beltagy2020longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer, 2020.

\bibitem{chee2023quip}
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher~De Sa.
\newblock Qu{IP}: 2-bit quantization of large language models with guarantees.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR09}, 2009.

\bibitem{dettmers2022gptint}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock {GPT}3.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{apq-vit}
Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, and Xianglong Liu.
\newblock Towards accurate post-training quantization for vision transformer.
\newblock In {\em Proceedings of the 30th ACM International Conference on Multimedia}, MM '22, page 5380–5388, New York, NY, USA, 2022. Association for Computing Machinery.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{wikidump}
Wikimedia Foundation.
\newblock Wikimedia downloads.

\bibitem{frantar2023optq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock {OPTQ}: Accurate quantization for generative pre-trained transformers.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{huffman}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem{I-BERT}
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael~W. Mahoney, and Kurt Keutzer.
\newblock I-bert: Integer-only bert quantization.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th International Conference on Machine Learning}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 5506--5518. PMLR, 18--24 Jul 2021.

\bibitem{ivit}
Zhikai Li and Qingyi Gu.
\newblock I-vit: Integer-only quantization for efficient vision transformer inference.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 17065--17075, 2023.

\bibitem{li2023repq}
Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu.
\newblock Repq-vit: Scale reparameterization for post-training quantization of vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 17227--17236, 2023.

\bibitem{lin2022fqvit}
Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou.
\newblock Fq-vit: Post-training quantization for fully quantized vision transformer.
\newblock In {\em Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, {IJCAI-22}}, pages 1173--1179, 2022.

\bibitem{llmfp4}
Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng.
\newblock {LLM}-{FP}4: 4-bit floating-point quantized transformers.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 592--605, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem{liu2023llmqat}
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
\newblock Llm-qat: Data-free quantization aware training for large language models, 2023.

\bibitem{nagel2019data}
Markus Nagel, Mart~van Baalen, Tijmen Blankevoort, and Max Welling.
\newblock Data-free quantization through weight equalization and bias correction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1325--1334, 2019.

\bibitem{Narayan2018DontGM}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.
\newblock {\em ArXiv}, abs/1808.08745, 2018.

\bibitem{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of Machine Learning Research}, 21(140):1--67, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem{wang2018training}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{wortsman2023stable}
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt.
\newblock Stable and low-precision training for large-scale vision-language models.
\newblock {\em arXiv preprint arXiv:2304.13013}, 2023.

\bibitem{wu2018training}
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi.
\newblock Training and inference with integers in deep neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock {S}mooth{Q}uant: Accurate and efficient post-training quantization for large language models.
\newblock In {\em Proceedings of the 40th International Conference on Machine Learning}, 2023.

\bibitem{PTQ4ViT_arixv2022}
Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun.
\newblock Ptq4vit: Post-training quantization framework for vision transformers with twin uniform quantization, 2022.

\bibitem{zhu2020towards}
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan.
\newblock Towards unified int8 training for convolutional neural network.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1969--1979, 2020.

\end{thebibliography}
