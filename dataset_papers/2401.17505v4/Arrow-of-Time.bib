
@misc{schaeffer_are_2023,
	title = {Are {Emergent} {Abilities} of {Large} {Language} {Models} a {Mirage}?},
	url = {http://arxiv.org/abs/2304.15004},
	abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing ﬁxed model outputs, emergent abilities appear due the researcher’s choice of metric rather than due to fundamental changes in model behavior with scale. Speciﬁcally, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and conﬁrm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and conﬁrm two predictions about metric choices in a metaanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
	language = {en},
	urldate = {2023-07-16},
	publisher = {arXiv},
	author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	month = may,
	year = {2023},
	note = {arXiv:2304.15004 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf:/Users/jcw/Zotero/storage/LDWUUACL/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf:application/pdf},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efﬁciency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2022-06-17},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.07682
arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:/Users/jcw/Zotero/storage/UYURYRH8/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf},
}

@article{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overﬁtting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sampleefﬁcient, such that optimally compute-efﬁcient training involves training very large models on a relatively modest amount of data and stopping signiﬁcantly before convergence.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08361},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	annote = {Comment: 19 pages, 15 figures},
	file = {Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:/Users/jcw/Zotero/storage/PAZJ5FMU/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf},
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2018-10-17},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: EMNLP 2014},
	file = {Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:/Users/jcw/Zotero/storage/KWEJD2YB/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf},
}

@article{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	month = jul,
	year = {2018},
	pages = {12},
	annote = {https://github.com/openai/finetune-transformer-lm
 },
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/jcw/Zotero/storage/36PWIG9D/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difﬁculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	language = {en},
	urldate = {2018-11-20},
	journal = {arXiv:1409.3215 [cs]},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 9 pages},
	file = {Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:/Users/jcw/Zotero/storage/7JLG3MMI/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	month = feb,
	year = {2019},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/jcw/Zotero/storage/Z5K8KJG6/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2018-10-08},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/Users/jcw/Zotero/storage/6I2JGL8Q/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-term {Memory}},
	volume = {9},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = dec,
	year = {1997},
	pages = {1735--80},
	file = {Hochreiter and Schmidhuber - 1997 - Long Short-term Memory.pdf:/Users/jcw/Zotero/storage/9W94WIHA/Hochreiter and Schmidhuber - 1997 - Long Short-term Memory.pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	urldate = {2020-05-29},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = may,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:/Users/jcw/Zotero/storage/RGYC8QZT/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over {\textbackslash}nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, {\textbackslash}chinchilla, that uses the same compute budget as {\textbackslash}gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. {\textbackslash}chinchilla uniformly and significantly outperforms {\textbackslash}Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that {\textbackslash}chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, {\textbackslash}chinchilla reaches a state-of-the-art average accuracy of 67.5{\textbackslash}\% on the MMLU benchmark, greater than a 7{\textbackslash}\% improvement over {\textbackslash}gopher.},
	language = {en},
	urldate = {2022-04-01},
	journal = {arXiv:2203.15556 [cs]},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.15556},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:/Users/jcw/Zotero/storage/KJURVUPV/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf},
}

@inproceedings{liu_agreement_2016,
	address = {San Diego, California},
	title = {Agreement on {Target}-bidirectional {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/N16-1046},
	doi = {10.18653/v1/N16-1046},
	abstract = {Neural machine translation (NMT) with recurrent neural networks, has proven to be an effective technique for end-to-end machine translation. However, in spite of its promising advances over traditional translation methods, it typically suffers from an issue of unbalanced outputs, that arise from both the nature of recurrent neural networks themselves, and the challenges inherent in machine translation. To overcome this issue, we propose an agreement model for neural machine translation and show its effectiveness on large-scale Japaneseto-English and Chinese-to-English translation tasks. Our results show the model can achieve improvements of up to 1.4 BLEU over the strongest baseline NMT system. With the help of an ensemble technique, this new end-to-end NMT approach ﬁnally outperformed phrasebased and hierarchical phrase-based Moses baselines by up to 5.6 BLEU points.},
	language = {en},
	urldate = {2024-01-05},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Lemao and Utiyama, Masao and Finch, Andrew and Sumita, Eiichiro},
	year = {2016},
	pages = {411--416},
	file = {Liu et al. - 2016 - Agreement on Target-bidirectional Neural Machine T.pdf:/Users/jcw/Zotero/storage/52BZI3WS/Liu et al. - 2016 - Agreement on Target-bidirectional Neural Machine T.pdf:application/pdf},
}

@misc{zhang_regularizing_2018,
	title = {Regularizing {Neural} {Machine} {Translation} by {Target}-bidirectional {Agreement}},
	url = {http://arxiv.org/abs/1808.04064},
	abstract = {Although Neural Machine Translation (NMT) has achieved remarkable progress in the past several years, most NMT systems still suffer from a fundamental shortcoming as in other sequence generation tasks: errors made early in generation process are fed as inputs to the model and can be quickly ampliﬁed, harming subsequent sequence generation. To address this issue, we propose a novel model regularization method for NMT training, which aims to improve the agreement between translations generated by left-to-right (L2R) and rightto-left (R2L) NMT decoders. This goal is achieved by introducing two Kullback-Leibler divergence regularization terms into the NMT training objective to reduce the mismatch between output probabilities of L2R and R2L models. In addition, we also employ a joint training strategy to allow L2R and R2L models to improve each other in an interactive update process. Experimental results show that our proposed method signiﬁcantly outperforms state-of-the-art baselines on Chinese-English and English-German translation tasks.},
	language = {en},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Zhang, Zhirui and Wu, Shuangzhi and Liu, Shujie and Li, Mu and Zhou, Ming and Xu, Tong},
	month = nov,
	year = {2018},
	note = {arXiv:1808.04064 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by AAAI 2019},
	file = {Zhang et al. - 2018 - Regularizing Neural Machine Translation by Target-.pdf:/Users/jcw/Zotero/storage/3T4SRW6D/Zhang et al. - 2018 - Regularizing Neural Machine Translation by Target-.pdf:application/pdf},
}

@misc{serdyuk_twin_2018,
	title = {Twin {Networks}: {Matching} the {Future} for {Sequence} {Generation}},
	shorttitle = {Twin {Networks}},
	url = {http://arxiv.org/abs/1708.06742},
	abstract = {We propose a simple technique for encouraging generative RNNs to plan ahead. We train a “backward” recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9\% relative improvement for a speech recognition task, and achieves signiﬁcant improvement on a COCO caption generation task.},
	language = {en},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Serdyuk, Dmitriy and Ke, Nan Rosemary and Sordoni, Alessandro and Trischler, Adam and Pal, Chris and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {arXiv:1708.06742 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, published at ICLR 2018},
	file = {Serdyuk et al. - 2018 - Twin Networks Matching the Future for Sequence Ge.pdf:/Users/jcw/Zotero/storage/CZXDITW8/Serdyuk et al. - 2018 - Twin Networks Matching the Future for Sequence Ge.pdf:application/pdf},
}

@misc{ogawa_iterative_2023,
	title = {Iterative {Shallow} {Fusion} of {Backward} {Language} {Model} for {End}-to-{End} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2310.11010},
	abstract = {We propose a new shallow fusion (SF) method to exploit an external backward language model (BLM) for end-to-end automatic speech recognition (ASR). The BLM has complementary characteristics with a forward language model (FLM), and the effectiveness of their combination has been conﬁrmed by rescoring ASR hypotheses as post-processing. In the proposed SF, we iteratively apply the BLM to partial ASR hypotheses in the backward direction (i.e., from the possible next token to the start symbol) during decoding, substituting the newly calculated BLM scores for the scores calculated at the last iteration. To enhance the effectiveness of this iterative SF (ISF), we train a partial sentence-aware BLM (PBLM) using reversed text data including partial sentences, considering the framework of ISF. In experiments using an attention-based encoder-decoder ASR system, we conﬁrmed that ISF using the PBLM shows comparable performance with SF using the FLM. By performing ISF, early pruning of prospective hypotheses can be prevented during decoding, and we can obtain a performance improvement compared to applying the PBLM as post-processing. Finally, we conﬁrmed that, by combining SF and ISF, further performance improvement can be obtained thanks to the complementarity of the FLM and PBLM.},
	language = {en},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Ogawa, Atsunori and Moriya, Takafumi and Kamo, Naoyuki and Tawara, Naohiro and Delcroix, Marc},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11010 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted to ICASSP 2023},
	file = {Ogawa et al. - 2023 - Iterative Shallow Fusion of Backward Language Mode.pdf:/Users/jcw/Zotero/storage/KGZRN4QT/Ogawa et al. - 2023 - Iterative Shallow Fusion of Backward Language Mode.pdf:application/pdf},
}

@misc{edunov_pre-trained_2019,
	title = {Pre-trained {Language} {Model} {Representations} for {Language} {Generation}},
	url = {http://arxiv.org/abs/1903.09722},
	abstract = {Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14\%. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.},
	language = {en},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Edunov, Sergey and Baevski, Alexei and Auli, Michael},
	month = apr,
	year = {2019},
	note = {arXiv:1903.09722 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2019},
	file = {Edunov et al. - 2019 - Pre-trained Language Model Representations for Lan.pdf:/Users/jcw/Zotero/storage/ITSPCYRH/Edunov et al. - 2019 - Pre-trained Language Model Representations for Lan.pdf:application/pdf},
}

@article{xiong_backward_2015,
	title = {Backward and trigger-based language models for statistical machine translation},
	volume = {21},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/product/identifier/S1351324913000168/type/journal_article},
	doi = {10.1017/S1351324913000168},
	abstract = {The language model is one of the most important knowledge sources for statistical machine translation. In this article, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures longdistance dependencies that go beyond the scope of standard n-gram language models. We introduce algorithms to integrate the two proposed models into two kinds of state-of-the-art phrase-based decoders. Our experimental results on Chinese/Spanish/Vietnamese-to-English show that both models are able to signiﬁcantly improve translation quality in terms of BLEU and METEOR over a competitive baseline.},
	language = {en},
	number = {2},
	urldate = {2024-01-05},
	journal = {Natural Language Engineering},
	author = {Xiong, Deyi and Zhang, Min},
	month = mar,
	year = {2015},
	pages = {201--226},
	file = {Xiong and Zhang - 2015 - Backward and trigger-based language models for sta.pdf:/Users/jcw/Zotero/storage/5KBTWEPB/Xiong and Zhang - 2015 - Backward and trigger-based language models for sta.pdf:application/pdf},
}

@inproceedings{duchateau_confidence_2002,
	title = {Confidence scoring based on backward language models},
	booktitle = {{ICASSP}},
	author = {Duchateau, Jacques and Demuynck, Kris and Wambacq, Patrick},
	year = {2002},
	pages = {221--224},
	file = {Duchateau et al. - 2002 - Confidence scoring based on backward language mode.pdf:/Users/jcw/Zotero/storage/AKBL2YIN/Duchateau et al. - 2002 - Confidence scoring based on backward language mode.pdf:application/pdf},
}

@misc{biderman_pythia_2023,
	title = {Pythia: {A} {Suite} for {Analyzing} {Large} {Language} {Models} {Across} {Training} and {Scaling}},
	shorttitle = {Pythia},
	url = {http://arxiv.org/abs/2304.01373},
	abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https: //github.com/EleutherAI/pythia.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and van der Wal, Oskar},
	month = may,
	year = {2023},
	note = {arXiv:2304.01373 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Code at https://github.com/EleutherAI/pythia},
	file = {Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf:/Users/jcw/Zotero/storage/S3FEZELB/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf:application/pdf},
}

@misc{gao_pile_2020,
	title = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
	shorttitle = {The {Pile}},
	url = {http://arxiv.org/abs/2101.00027},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	month = dec,
	year = {2020},
	note = {arXiv:2101.00027 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:/Users/jcw/Zotero/storage/LYMCWXHN/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf},
}

@article{mangal_lstm_2019,
	title = {{LSTM} vs. {GRU} vs. {Bidirectional} {RNN} for script generation},
	abstract = {Scripts are an important part of any TV series. They narrate movements, actions and expressions of characters. In this paper, a case study is presented on how different sequence to sequence deep learning models perform in the task of generating new conversations between characters as well as new scenarios on the basis of a script (previous conversations). A comprehensive comparison between these models, namely, LSTM, GRU and Bidirectional RNN is presented. All the models are designed to learn the sequence of recurring characters from the input sequence. Each input sequence will contain, say ‘n’ characters, and the corresponding targets will contain the same number of characters, except, they will be shifted one character to the right. In this manner, input and output sequences are generated and used to train the models. A closer analysis of explored models’ performance and efficiency is delineated with the help of graph plots and generated texts by taking some input string. These graphs describe both, intraneural performance and interneural model performance for each model.},
	language = {en},
	author = {Mangal, Sanidhya and Joshi, Poorva and Modak, Rahul},
	month = aug,
	year = {2019},
	file = {Mangal et al. - LSTM vs. GRU vs. Bidirectional RNN for script gene.pdf:/Users/jcw/Zotero/storage/R8LDULQ5/Mangal et al. - LSTM vs. GRU vs. Bidirectional RNN for script gene.pdf:application/pdf},
}

@article{pfau_eliciting_2023,
	title = {Eliciting {Language} {Model} {Behaviors} using {Reverse} {Language} {Models}},
	abstract = {Despite advances in fine-tuning methods, language models (LMs) continue to output toxic and harmful responses on worst-case inputs, including adversarial attacks and jailbreaks. We train an LM on tokens in reverse order—a reverse LM—as a tool for identifying such worst-case inputs. By prompting a reverse LM with a problematic string, we can sample prefixes that are likely to precede the problematic suffix. We test our reverse LM by using it to guide beam search for prefixes that have high probability of generating toxic statements when input to a forwards LM. Our 160m parameter reverse LM outperforms the existing state-ofthe-art adversarial attack method, GCG, when measuring the probability of toxic continuations from the Pythia-160m LM. We also find that the prefixes generated by our reverse LM for the Pythia model are more likely to transfer to other models, eliciting toxic responses also from Llama 2 when compared to GCG-generated attacks.},
	language = {en},
	author = {Pfau, Jacob and Infanger, Alex and Sheshadri, Abhay and Panda, Ayush and Huebner, Curtis and Michael, Julian},
	month = oct,
	year = {2023},
	file = {Pfau et al. - Eliciting Language Model Behaviors using Reverse L.pdf:/Users/jcw/Zotero/storage/LZXBWUYN/Pfau et al. - Eliciting Language Model Behaviors using Reverse L.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/jcw/Zotero/storage/NWDPGR4F/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@misc{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-speciﬁc modiﬁcations and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for ﬁne-tuning a language model. Our method signiﬁcantly outperforms the state-of-the-art on six text classiﬁcation tasks, reducing the error by 1824\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code1.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = may,
	year = {2018},
	note = {arXiv:1801.06146 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ACL 2018, fixed denominator in Equation 3, line 3},
	file = {Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf:/Users/jcw/Zotero/storage/5U9287IC/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf:application/pdf},
}

@misc{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = mar,
	year = {2018},
	note = {arXiv:1802.05365 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
	file = {Peters et al. - 2018 - Deep contextualized word representations.pdf:/Users/jcw/Zotero/storage/EBMMF5B9/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf},
}

@misc{nguyen_meet_2023,
	title = {Meet in the {Middle}: {A} {New} {Pre}-training {Paradigm}},
	shorttitle = {Meet in the {Middle}},
	url = {http://arxiv.org/abs/2303.07295},
	abstract = {Most language models (LMs) are trained and applied in an autoregressive left-to-right fashion, assuming that the next token only depends on the preceding ones. However, this assumption ignores the potential benefits of using the full sequence information during training, and the possibility of having context from both sides during inference. In this paper, we propose a new pre-training paradigm with techniques that jointly improve the training data efficiency and the capabilities of the LMs in the infilling task. The first is a training objective that aligns the predictions of a left-to-right LM with those of a right-to-left LM, trained on the same data but in reverse order. The second is a bidirectional inference procedure that enables both LMs to meet in the middle. We show the effectiveness of our pre-training paradigm with extensive experiments on both programming and natural language models, outperforming strong baselines.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Nguyen, Anh and Karampatziakis, Nikos and Chen, Weizhu},
	month = mar,
	year = {2023},
	note = {arXiv:2303.07295 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 24 pages, 2 figures},
	file = {Nguyen et al. - 2023 - Meet in the Middle A New Pre-training Paradigm.pdf:/Users/jcw/Zotero/storage/2XIX34QS/Nguyen et al. - 2023 - Meet in the Middle A New Pre-training Paradigm.pdf:application/pdf},
}

@misc{mou_backward_2016,
	title = {Backward and {Forward} {Language} {Modeling} for {Constrained} {Sentence} {Generation}},
	url = {http://arxiv.org/abs/1512.06612},
	abstract = {Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN’s hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a speciﬁc word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Mou, Lili and Yan, Rui and Li, Ge and Zhang, Lu and Jin, Zhi},
	month = jan,
	year = {2016},
	note = {arXiv:1512.06612 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Mou et al. - 2016 - Backward and Forward Language Modeling for Constra.pdf:/Users/jcw/Zotero/storage/5LKYL9KL/Mou et al. - 2016 - Backward and Forward Language Modeling for Constra.pdf:application/pdf},
}

@misc{graves_bayesian_2023,
	title = {Bayesian {Flow} {Networks}},
	url = {http://arxiv.org/abs/2308.07037},
	abstract = {This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.},
	language = {en},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Graves, Alex and Srivastava, Rupesh Kumar and Atkinson, Timothy and Gomez, Faustino},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07037 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Graves et al. - 2023 - Bayesian Flow Networks.pdf:/Users/jcw/Zotero/storage/X3IMNSSC/Graves et al. - 2023 - Bayesian Flow Networks.pdf:application/pdf},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	language = {en},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
	file = {OpenAI - 2023 - GPT-4 Technical Report.pdf:/Users/jcw/Zotero/storage/RE4EYHBT/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf},
}

@misc{shen_film_2023,
	title = {{FiLM}: {Fill}-in {Language} {Models} for {Any}-{Order} {Generation}},
	shorttitle = {{FiLM}},
	url = {http://arxiv.org/abs/2310.09930},
	abstract = {Language models have become the backbone of today's AI systems. However, their predominant left-to-right generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM), a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM. During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on left-to-right language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratch or fine-tuned from a left-to-right language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, indicating FiLM's scalability and potential as a large language model.},
	language = {en},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Shen, Tianxiao and Peng, Hao and Shen, Ruoqi and Fu, Yao and Harchaoui, Zaid and Choi, Yejin},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09930 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Shen et al. - 2023 - FiLM Fill-in Language Models for Any-Order Genera.pdf:/Users/jcw/Zotero/storage/V45S7N5U/Shen et al. - 2023 - FiLM Fill-in Language Models for Any-Order Genera.pdf:application/pdf},
}

@article{savage_elicitation_1971,
	title = {Elicitation of {Personal} {Probabilities} and {Expectations}},
	volume = {66},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482346},
	doi = {10.1080/01621459.1971.10482346},
	language = {en},
	number = {336},
	urldate = {2024-01-10},
	journal = {Journal of the American Statistical Association},
	author = {Savage, Leonard J.},
	month = dec,
	year = {1971},
	pages = {783--801},
	file = {Savage - 1971 - Elicitation of Personal Probabilities and Expectat.pdf:/Users/jcw/Zotero/storage/MM5SQBR8/Savage - 1971 - Elicitation of Personal Probabilities and Expectat.pdf:application/pdf},
}

@misc{dascoli_boolformer_2023,
	title = {Boolformer: {Symbolic} {Regression} of {Logic} {Functions} with {Transformers}},
	shorttitle = {Boolformer},
	url = {http://arxiv.org/abs/2309.12207},
	abstract = {In this work, we introduce Boolformer, the first Transformer architecture trained to perform endto-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions which were not seen during training, when provided a clean truth table. Then, we demonstrate its ability to find approximate expressions when provided incomplete and noisy observations. We evaluate the Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modelling the dynamics of gene regulatory networks. Using a recent benchmark, we show that Boolformer is competitive with state-of-the art genetic algorithms with a speedup of several orders of magnitude. Our code and models are available publicly.},
	language = {en},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {d'Ascoli, Stéphane and Bengio, Samy and Susskind, Josh and Abbé, Emmanuel},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12207 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
	file = {d'Ascoli et al. - 2023 - Boolformer Symbolic Regression of Logic Functions.pdf:/Users/jcw/Zotero/storage/DAUYYUGX/d'Ascoli et al. - 2023 - Boolformer Symbolic Regression of Logic Functions.pdf:application/pdf},
}

@article{hanson_logarithmic_2012,
	title = {{LOGARITHMIC} {MARKETS} {CORING} {RULES} {FOR} {MODULAR} {COMBINATORIAL} {INFORMATION} {AGGREGATION}},
	volume = {1},
	issn = {1750-676X, 1750-6751},
	url = {http://www.bjll.org/index.php/jpm/article/view/417},
	doi = {10.5750/jpm.v1i1.417},
	abstract = {In practice, scoring rules elicit good probability estimates from individuals, while betting markets elicit good consensus estimates from groups. Market scoring rules combine these features, eliciting estimates from individuals or groups, with groups costing no more than individuals. Regarding a bet on one event given another event, only logarithmic versions preserve the probability of the given event. Logarithmic versions also preserve the conditional probabilities of other events, and so preserve conditional independence relations. Given logarithmic rules that elicit relative probabilities of base event pairs, it costs no more to elicit estimates on all combinations of these base events.},
	language = {en},
	number = {1},
	urldate = {2024-01-10},
	journal = {The Journal of Prediction Markets},
	author = {Hanson, Robin},
	month = dec,
	year = {2012},
	pages = {3--15},
	file = {Hanson - 2012 - LOGARITHMIC MARKETS CORING RULES FOR MODULAR COMBI.pdf:/Users/jcw/Zotero/storage/2WJCZGIB/Hanson - 2012 - LOGARITHMIC MARKETS CORING RULES FOR MODULAR COMBI.pdf:application/pdf},
}

@book{duff_direct_2017,
	title = {Direct {Methods} for {Sparse} {Matrices}},
	isbn = {978-0-19-850838-0},
	url = {https://doi.org/10.1093/acprof:oso/9780198508380.001.0001},
	abstract = {Direct Methods for Sparse Matrices, second edition, is a complete rewrite of the first edition published 30 years ago. Much has changed since that time. Problems have grown greatly in size and complexity; nearly all our examples were of order less than 5,000 in the first edition, and are often more than a million in the second edition. Computer architectures are now much more complex, requiring new ways of adapting algorithms to parallel environments with memory hierarchies. Because the area is such an important one to all of computational science and engineering, a huge amount of research has been done since the first edition, some of it by the authors. This new research is integrated into the text with a clear explanation of the underlying mathematics and algorithms. New research that is described includes new techniques for scaling and error control, new orderings, new combinatorial techniques for partitioning both symmetric and unsymmetric problems, and a detailed description of the multifrontal approach to solving systems that was pioneered by the research of the authors and colleagues. This includes a discussion of techniques for exploiting parallel architectures and new work for indefinite and unsymmetric systems.},
	publisher = {Oxford University Press},
	author = {Duff, I. S. and Erisman, A. M. and Reid, J. K.},
	month = jan,
	year = {2017},
	doi = {10.1093/acprof:oso/9780198508380.001.0001},
	doi = {10.1093/acprof:oso/9780198508380.001.0001},
}

@misc{karpathy_karpathymingpt_2024,
	title = {karpathy/{minGPT}},
	copyright = {MIT},
	url = {https://github.com/karpathy/minGPT},
	abstract = {A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training},
	urldate = {2024-01-24},
	author = {Karpathy, Andrej},
	month = jan,
	year = {2024},
	note = {original-date: 2020-08-17T07:08:48Z},
}

@misc{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	language = {en},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = may,
	year = {2017},
	note = {arXiv:1608.03983 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	annote = {Comment: ICLR 2017 conference paper},
	file = {Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:/Users/jcw/Zotero/storage/EZHL2UHN/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	language = {en},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:/Users/jcw/Zotero/storage/GMH8X9CB/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf},
}

@misc{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909},
	abstract = {Neural machine translation (NMT) models typically operate with a ﬁxed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.},
	language = {en},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = jun,
	year = {2016},
	note = {arXiv:1508.07909 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: accepted at ACL 2016; new in this version: figure 3},
	file = {Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:/Users/jcw/Zotero/storage/UYMEYQ52/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@misc{conneau_unsupervised_2020,
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1911.02116},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
	language = {en},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = apr,
	year = {2020},
	note = {arXiv:1911.02116 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2020 (+ updated results)},
	file = {Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:/Users/jcw/Zotero/storage/2P9EE6WW/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:application/pdf},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2024-01-24},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	pages = {359--378},
	file = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/Users/jcw/Zotero/storage/UMRCAS43/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf},
}

@misc{shen_positional_2023,
	title = {Positional {Description} {Matters} for {Transformers} {Arithmetic}},
	url = {http://arxiv.org/abs/2311.14737},
	abstract = {Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities –which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).},
	language = {en},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Shen, Ruoqi and Bubeck, Sébastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
	month = nov,
	year = {2023},
	note = {arXiv:2311.14737 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 18 pages},
	file = {Shen et al. - 2023 - Positional Description Matters for Transformers Ar.pdf:/Users/jcw/Zotero/storage/AC2W2V4T/Shen et al. - 2023 - Positional Description Matters for Transformers Ar.pdf:application/pdf},
}

@article{shannon_prediction_1951,
	title = {Prediction and entropy of printed english.},
	journal = {Bell Systems Technical Journal},
	author = {Shannon, C. E.},
	year = {1951},
	keywords = {imported},
	pages = {50--64},
	file = {Shannon - 1951 - Prediction and entropy of printed english..pdf:/Users/jcw/Zotero/storage/S7C4F84Q/Shannon - 1951 - Prediction and entropy of printed english..pdf:application/pdf},
}

@misc{mansimov_generalized_2020,
	title = {A {Generalized} {Framework} of {Sequence} {Generation} with {Application} to {Undirected} {Sequence} {Models}},
	url = {http://arxiv.org/abs/1905.12790},
	abstract = {Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as questionanswering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs signiﬁcantly from conventional monotonic generation in directed sequence models. We investigate this problem by proposing a generalized model of sequence generation that uniﬁes decoding in directed and undirected models. The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and reﬁnementbased non-autoregressive models. This uniﬁcation enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models. We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model (Lample \& Conneau, 2019). The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-theart on WMT’14 English-German translation.},
	language = {en},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Mansimov, Elman and Wang, Alex and Welleck, Sean and Cho, Kyunghyun},
	month = feb,
	year = {2020},
	note = {arXiv:1905.12790 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mansimov et al. - 2020 - A Generalized Framework of Sequence Generation wit.pdf:/Users/jcw/Zotero/storage/A3Z4AHZC/Mansimov et al. - 2020 - A Generalized Framework of Sequence Generation wit.pdf:application/pdf},
}

@misc{emelianenko_sequence_2019,
	title = {Sequence {Modeling} with {Unconstrained} {Generation} {Order}},
	url = {http://arxiv.org/abs/1911.00176},
	abstract = {The dominant approach to sequence generation is to produce a sequence in some predefined order, e.g. left to right. In contrast, we propose a more general model that can generate the output sequence by inserting tokens in any arbitrary order. Our model learns decoding order as a result of its training procedure. Our experiments show that this model is superior to fixed order models on a number of sequence generation tasks, such as Machine Translation, Image-to-LaTeX and Image Captioning.},
	language = {en},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Emelianenko, Dmitrii and Voita, Elena and Serdyukov, Pavel},
	month = oct,
	year = {2019},
	note = {arXiv:1911.00176 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Camera-ready version for NeurIPS2019},
	file = {Emelianenko et al. - 2019 - Sequence Modeling with Unconstrained Generation Or.pdf:/Users/jcw/Zotero/storage/8BJGQLRX/Emelianenko et al. - 2019 - Sequence Modeling with Unconstrained Generation Or.pdf:application/pdf},
}

@misc{chan_empirical_2019,
	title = {An {Empirical} {Study} of {Generation} {Order} for {Machine} {Translation}},
	url = {http://arxiv.org/abs/1910.13437},
	abstract = {In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we ﬁrst introduce a soft orderreward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we ﬁnd that for the WMT’14 English → German translation task, order does not have a substantial impact on output quality, with unintuitive orderings such as alphabetical and shortestﬁrst matching the performance of a standard Transformer. This demonstrates that traditional left-to-right generation is not strictly necessary to achieve high performance. On the other hand, results on the WMT’18 English → Chinese task tend to vary more widely, suggesting that translation for less well-aligned language pairs may be more sensitive to generation order.},
	language = {en},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Chan, William and Stern, Mitchell and Kiros, Jamie and Uszkoreit, Jakob},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13437 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Chan et al. - 2019 - An Empirical Study of Generation Order for Machine.pdf:/Users/jcw/Zotero/storage/5I9R5EEW/Chan et al. - 2019 - An Empirical Study of Generation Order for Machine.pdf:application/pdf},
}

@inproceedings{wu_beyond_2018,
	address = {Brussels, Belgium},
	title = {Beyond {Error} {Propagation} in {Neural} {Machine} {Translation}: {Characteristics} of {Language} {Also} {Matter}},
	shorttitle = {Beyond {Error} {Propagation} in {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D18-1396},
	doi = {10.18653/v1/D18-1396},
	abstract = {Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting ﬁndings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are conﬁrmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.},
	language = {en},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Lijun and Tan, Xu and He, Di and Tian, Fei and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
	year = {2018},
	pages = {3602--3611},
	file = {Wu et al. - 2018 - Beyond Error Propagation in Neural Machine Transla.pdf:/Users/jcw/Zotero/storage/HLFDBYYL/Wu et al. - 2018 - Beyond Error Propagation in Neural Machine Transla.pdf:application/pdf},
}

@misc{xu_theory_2020,
	title = {A {Theory} of {Usable} {Information} {Under} {Computational} {Constraints}},
	url = {http://arxiv.org/abs/2002.10689},
	abstract = {We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon’s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefﬁcient of determination. Unlike Shannon’s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, V-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more effective than mutual information for structure learning and fair representation learning.},
	language = {en},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
	month = feb,
	year = {2020},
	note = {arXiv:2002.10689 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2020 (Talk)},
	file = {Xu et al. - 2020 - A Theory of Usable Information Under Computational.pdf:/Users/jcw/Zotero/storage/437PBDMK/Xu et al. - 2020 - A Theory of Usable Information Under Computational.pdf:application/pdf},
}

@misc{wenzek_ccnet_2019,
	title = {{CCNet}: {Extracting} {High} {Quality} {Monolingual} {Datasets} from {Web} {Crawl} {Data}},
	shorttitle = {{CCNet}},
	url = {http://arxiv.org/abs/1911.00359},
	abstract = {Pre-training text representations have led to signiﬁcant improvements in many areas of natural language processing. The quality of these models beneﬁts greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identiﬁes their language. We augment this pipeline with a ﬁltering step to select documents that are close to high quality corpora like Wikipedia.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzmán, Francisco and Joulin, Armand and Grave, Edouard},
	month = nov,
	year = {2019},
	note = {arXiv:1911.00359 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {Wenzek et al. - 2019 - CCNet Extracting High Quality Monolingual Dataset.pdf:/Users/jcw/Zotero/storage/DFSLY6DW/Wenzek et al. - 2019 - CCNet Extracting High Quality Monolingual Dataset.pdf:application/pdf},
}

@misc{vinyals_order_2016,
	title = {Order {Matters}: {Sequence} to sequence for sets},
	shorttitle = {Order {Matters}},
	url = {http://arxiv.org/abs/1511.06391},
	abstract = {Sequences have become ﬁrst class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efﬁciently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we ﬁrst show using various examples that the order in which we organize input and/or output data matters signiﬁcantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modiﬁcations to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artiﬁcial tasks – sorting numbers and estimating the joint probability of unknown graphical models.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
	month = feb,
	year = {2016},
	note = {arXiv:1511.06391 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted as a conference paper at ICLR 2015},
	file = {Vinyals et al. - 2016 - Order Matters Sequence to sequence for sets.pdf:/Users/jcw/Zotero/storage/6DK5IBR9/Vinyals et al. - 2016 - Order Matters Sequence to sequence for sets.pdf:application/pdf},
}

@misc{stern_insertion_2019,
	title = {Insertion {Transformer}: {Flexible} {Sequence} {Generation} via {Insertion} {Operations}},
	shorttitle = {Insertion {Transformer}},
	url = {http://arxiv.org/abs/1902.03249},
	abstract = {We present the Insertion Transformer, an iterative, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a ﬁxed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This ﬂexibility confers a number of advantages: for instance, not only can our model be trained to follow speciﬁc orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entropy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and partially autoregressive generation (simultaneous insertions at multiple locations). We validate our approach by analyzing its performance on the WMT 2014 EnglishGerman machine translation task under various settings for training and decoding. We ﬁnd that the Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
	month = feb,
	year = {2019},
	note = {arXiv:1902.03249 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Stern et al. - 2019 - Insertion Transformer Flexible Sequence Generatio.pdf:/Users/jcw/Zotero/storage/L3BB479J/Stern et al. - 2019 - Insertion Transformer Flexible Sequence Generatio.pdf:application/pdf},
}

@misc{gu_non-autoregressive_2018,
	title = {Non-{Autoregressive} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1711.02281},
	abstract = {Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient ﬁne-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor O. K. and Socher, Richard},
	month = mar,
	year = {2018},
	note = {arXiv:1711.02281 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted by ICLR 2018},
	file = {Gu et al. - 2018 - Non-Autoregressive Neural Machine Translation.pdf:/Users/jcw/Zotero/storage/S2GCP4XE/Gu et al. - 2018 - Non-Autoregressive Neural Machine Translation.pdf:application/pdf},
}

@misc{lee_deterministic_2018,
	title = {Deterministic {Non}-{Autoregressive} {Neural} {Sequence} {Modeling} by {Iterative} {Refinement}},
	url = {http://arxiv.org/abs/1802.06901},
	abstract = {We propose a conditional non-autoregressive neural sequence model based on iterative reﬁnement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En↔De and En↔Ro) and image caption generation, and observe that it signiﬁcantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Lee, Jason and Mansimov, Elman and Cho, Kyunghyun},
	month = aug,
	year = {2018},
	note = {arXiv:1802.06901 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to EMNLP'18},
	file = {Lee et al. - 2018 - Deterministic Non-Autoregressive Neural Sequence M.pdf:/Users/jcw/Zotero/storage/S53NR6HE/Lee et al. - 2018 - Deterministic Non-Autoregressive Neural Sequence M.pdf:application/pdf},
}

@inproceedings{papineni_bleu_2001,
	address = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	doi = {10.3115/1073083.1073135},
	language = {en},
	urldate = {2024-05-10},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2001},
	pages = {311},
	file = {Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf:/Users/jcw/Zotero/storage/A83MNSDC/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf:application/pdf},
}

@misc{chen_microsoft_2015,
	title = {Microsoft {COCO} {Captions}: {Data} {Collection} and {Evaluation} {Server}},
	shorttitle = {Microsoft {COCO} {Captions}},
	url = {http://arxiv.org/abs/1504.00325},
	abstract = {In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, ﬁve independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Dollar, Piotr and Zitnick, C. Lawrence},
	month = apr,
	year = {2015},
	note = {arXiv:1504.00325 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1411.4952},
	file = {Chen et al. - 2015 - Microsoft COCO Captions Data Collection and Evalu.pdf:/Users/jcw/Zotero/storage/P4NER9R9/Chen et al. - 2015 - Microsoft COCO Captions Data Collection and Evalu.pdf:application/pdf},
}

@misc{zaremba_recurrent_2015,
	title = {Recurrent {Neural} {Network} {Regularization}},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overﬁtting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = feb,
	year = {2015},
	note = {arXiv:1409.2329 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Zaremba et al. - 2015 - Recurrent Neural Network Regularization.pdf:/Users/jcw/Zotero/storage/YB3FXVU6/Zaremba et al. - 2015 - Recurrent Neural Network Regularization.pdf:application/pdf},
}

@misc{oord_parallel_2017,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	url = {http://arxiv.org/abs/1711.10433},
	abstract = {The recently-developed WaveNet architecture [27] is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no signiﬁcant difference in quality. The resulting system is capable of generating high-ﬁdelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10433 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf:/Users/jcw/Zotero/storage/PVH63G9C/Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf:application/pdf},
}

@misc{lee_teaching_2023,
	title = {Teaching {Arithmetic} to {Small} {Transformers}},
	url = {http://arxiv.org/abs/2307.03381},
	abstract = {Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.},
	language = {en},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D. and Lee, Kangwook and Papailiopoulos, Dimitris},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03381 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Lee et al. - 2023 - Teaching Arithmetic to Small Transformers.pdf:/Users/jcw/Zotero/storage/ABLDGP6G/Lee et al. - 2023 - Teaching Arithmetic to Small Transformers.pdf:application/pdf},
}

@misc{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1.},
	language = {en},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv:1906.08237 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Pretrained models and code are available at https://github.com/zihangdai/xlnet},
	file = {Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:/Users/jcw/Zotero/storage/96Z63EDD/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf},
}

@misc{gu_insertion-based_2019,
	title = {Insertion-based {Decoding} with automatically {Inferred} {Generation} {Order}},
	url = {http://arxiv.org/abs/1902.01370},
	doi = {10.48550/arXiv.1902.01370},
	abstract = {Conventional neural autoregressive decoding commonly assumes a fixed left-to-right generation order, which may be sub-optimal. In this work, we propose a novel decoding algorithm -- InDIGO -- which supports flexible sequence generation in arbitrary orders through insertion operations. We extend Transformer, a state-of-the-art sequence generation model, to efficiently implement the proposed approach, enabling it to be trained with either a pre-defined generation order or adaptive orders obtained from beam-search. Experiments on four real-world tasks, including word order recovery, machine translation, image caption and code generation, demonstrate that our algorithm can generate sequences following arbitrary orders, while achieving competitive or even better performance compared to the conventional left-to-right generation. The generated sequences show that InDIGO adopts adaptive generation orders based on input information.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Gu, Jiatao and Liu, Qi and Cho, Kyunghyun},
	month = oct,
	year = {2019},
	note = {arXiv:1902.01370 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Camera ready. Accepted by TACL},
	file = {arXiv.org Snapshot:/Users/jcw/Zotero/storage/NS76EBM2/1902.html:text/html;Gu et al. - 2019 - Insertion-based Decoding with automatically Inferr.pdf:/Users/jcw/Zotero/storage/I5GFXI3J/Gu et al. - 2019 - Insertion-based Decoding with automatically Inferr.pdf:application/pdf},
}

@misc{welleck_non-monotonic_2019,
	title = {Non-{Monotonic} {Sequential} {Text} {Generation}},
	url = {http://arxiv.org/abs/1902.02192},
	doi = {10.48550/arXiv.1902.02192},
	abstract = {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy's own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Welleck, Sean and Brantley, Kianté and Daumé III, Hal and Cho, Kyunghyun},
	month = oct,
	year = {2019},
	note = {arXiv:1902.02192 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2019},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/ETHYG9GY/Welleck et al. - 2019 - Non-Monotonic Sequential Text Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/BV36GR8L/1902.html:text/html},
}

@inproceedings{ford_importance_2018,
	address = {Brussels, Belgium},
	title = {The {Importance} of {Generation} {Order} in {Language} {Modeling}},
	url = {http://aclweb.org/anthology/D18-1324},
	doi = {10.18653/v1/D18-1324},
	abstract = {Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the inﬂuence of token generation order on model quality via a novel two-pass language model that produces partially-ﬁlled sentence “templates” and then ﬁlls in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We ﬁnd the most effective strategy generates function words in the ﬁrst pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of generation order for neural language models.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ford, Nicolas and Duckworth, Daniel and Norouzi, Mohammad and Dahl, George},
	year = {2018},
	pages = {2942--2946},
	file = {Ford et al. - 2018 - The Importance of Generation Order in Language Mod.pdf:/Users/jcw/Zotero/storage/6R57IZAQ/Ford et al. - 2018 - The Importance of Generation Order in Language Mod.pdf:application/pdf},
}

@misc{yogatama_learning_2016,
	title = {Learning to {Compose} {Words} into {Sentences} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.09100},
	abstract = {We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the beneﬁt of learning task-speciﬁc composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.},
	language = {en},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Yogatama, Dani and Blunsom, Phil and Dyer, Chris and Grefenstette, Edward and Ling, Wang},
	month = nov,
	year = {2016},
	note = {arXiv:1611.09100 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Yogatama et al. - 2016 - Learning to Compose Words into Sentences with Rein.pdf:/Users/jcw/Zotero/storage/UH8FSMXL/Yogatama et al. - 2016 - Learning to Compose Words into Sentences with Rein.pdf:application/pdf},
}

@misc{mehri_middle-out_2018,
	title = {Middle-{Out} {Decoding}},
	url = {http://arxiv.org/abs/1810.11735},
	doi = {10.48550/arXiv.1810.11735},
	abstract = {Despite being virtually ubiquitous, sequence-to-sequence models are challenged by their lack of diversity and inability to be externally controlled. In this paper, we speculate that a fundamental shortcoming of sequence generation models is that the decoding is done strictly from left-to-right, meaning that outputs values generated earlier have a profound effect on those generated later. To address this issue, we propose a novel middle-out decoder architecture that begins from an initial middle-word and simultaneously expands the sequence in both directions. To facilitate information flow and maintain consistent decoding, we introduce a dual self-attention mechanism that allows us to model complex dependencies between the outputs. We illustrate the performance of our model on the task of video captioning, as well as a synthetic sequence de-noising task. Our middle-out decoder achieves significant improvements on de-noising and competitive performance in the task of video captioning, while quantifiably improving the caption diversity. Furthermore, we perform a qualitative analysis that demonstrates our ability to effectively control the generation process of our decoder.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Mehri, Shikib and Sigal, Leonid},
	month = oct,
	year = {2018},
	note = {arXiv:1810.11735 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published as a conference paper at NIPS 2018},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/CSKEHB6R/Mehri and Sigal - 2018 - Middle-Out Decoding.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/Y7H9BZQL/1810.html:text/html},
}

@article{xia_deliberation_nodate,
	title = {Deliberation {Networks}: {Sequence} {Generation} {Beyond} {One}-{Pass} {Decoding}},
	abstract = {The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as ﬁnal output without further polishing. However, deliberation is a common behavior in human’s daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the ﬁrst-pass decoder generates a raw sequence and the second-pass decoder polishes and reﬁnes the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.},
	language = {en},
	author = {Xia, Yingce and Tian, Fei and Wu, Lijun and Lin, Jianxin and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
	file = {Xia et al. - Deliberation Networks Sequence Generation Beyond .pdf:/Users/jcw/Zotero/storage/5DVLUI4E/Xia et al. - Deliberation Networks Sequence Generation Beyond .pdf:application/pdf},
}

@inproceedings{geng_adaptive_2018,
	address = {Brussels, Belgium},
	title = {Adaptive {Multi}-pass {Decoder} for {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D18-1048},
	doi = {10.18653/v1/D18-1048},
	abstract = {Although end-to-end neural machine translation (NMT) has achieved remarkable progress in the recent years, the idea of adopting multipass decoding mechanism into conventional NMT is not well explored. In this paper, we propose a novel architecture called adaptive multi-pass decoder, which introduces a ﬂexible multi-pass polishing mechanism to extend the capacity of NMT via reinforcement learning. More speciﬁcally, we adopt an extra policy network to automatically choose a suitable and effective number of decoding passes, according to the complexity of source sentences and the quality of the generated translations. Extensive experiments on ChineseEnglish translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a signiﬁcant improvement about 1.55 BLEU.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Geng, Xinwei and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	year = {2018},
	pages = {523--532},
	file = {Geng et al. - 2018 - Adaptive Multi-pass Decoder for Neural Machine Tra.pdf:/Users/jcw/Zotero/storage/339BTPRM/Geng et al. - 2018 - Adaptive Multi-pass Decoder for Neural Machine Tra.pdf:application/pdf},
}

@misc{reed_parallel_2017,
	title = {Parallel {Multiscale} {Autoregressive} {Density} {Estimation}},
	url = {http://arxiv.org/abs/1703.03664},
	abstract = {PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more eﬃcient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512 × 512 images. We evaluate the model on class-conditional image generation, text-toimage synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow eﬃcient sampling.},
	language = {en},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Reed, Scott and Oord, Aäron van den and Kalchbrenner, Nal and Colmenarejo, Sergio Gómez and Wang, Ziyu and Belov, Dan and de Freitas, Nando},
	month = mar,
	year = {2017},
	note = {arXiv:1703.03664 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Reed et al. - 2017 - Parallel Multiscale Autoregressive Density Estimat.pdf:/Users/jcw/Zotero/storage/KZ4J3GYS/Reed et al. - 2017 - Parallel Multiscale Autoregressive Density Estimat.pdf:application/pdf},
}

@misc{noauthor_httpsaclanthologyorgd18-1044pdf_nodate,
	title = {https://aclanthology.org/{D18}-1044.pdf},
	url = {https://aclanthology.org/D18-1044.pdf},
	urldate = {2024-05-15},
}

@inproceedings{wang_semi-autoregressive_2018,
	address = {Brussels, Belgium},
	title = {Semi-{Autoregressive} {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/D18-1044},
	doi = {10.18653/v1/D18-1044},
	abstract = {Existing approaches to neural machine translation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decoding long sequences. In this paper, we propose a novel model for fast sequence generation — the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT'14 English-German translation, the SAT achieves 5.58{\textbackslash}mbox{\textbackslash}times speedup while maintaining 88\% translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1\% degeneration in BLEU score).},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Chunqi and Zhang, Ji and Chen, Haiqing},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {479--488},
	file = {Wang et al. - 2018 - Semi-Autoregressive Neural Machine Translation.pdf:/Users/jcw/Zotero/storage/RNT5WWMU/Wang et al. - 2018 - Semi-Autoregressive Neural Machine Translation.pdf:application/pdf},
}

@misc{sun_bidirectional_2017,
	title = {Bidirectional {Beam} {Search}: {Forward}-{Backward} {Inference} in {Neural} {Sequence} {Models} for {Fill}-in-the-{Blank} {Image} {Captioning}},
	shorttitle = {Bidirectional {Beam} {Search}},
	url = {http://arxiv.org/abs/1705.08759},
	doi = {10.48550/arXiv.1705.08759},
	abstract = {We develop the first approximate inference algorithm for 1-Best (and M-Best) decoding in bidirectional neural sequence models by extending Beam Search (BS) to reason about both forward and backward time dependencies. Beam Search (BS) is a widely used approximate inference algorithm for decoding sequences from unidirectional neural sequence models. Interestingly, approximate inference in bidirectional models remains an open problem, despite their significant advantage in modeling information from both the past and future. To enable the use of bidirectional models, we present Bidirectional Beam Search (BiBS), an efficient algorithm for approximate bidirectional inference.To evaluate our method and as an interesting problem in its own right, we introduce a novel Fill-in-the-Blank Image Captioning task which requires reasoning about both past and future sentence structure to reconstruct sensible image descriptions. We use this task as well as the Visual Madlibs dataset to demonstrate the effectiveness of our approach, consistently outperforming all baseline methods.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Sun, Qing and Lee, Stefan and Batra, Dhruv},
	month = may,
	year = {2017},
	note = {arXiv:1705.08759 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/HX29FXXH/Sun et al. - 2017 - Bidirectional Beam Search Forward-Backward Infere.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/SJRVXPEK/1705.html:text/html},
}

@article{zhou_synchronous_2019,
	title = {Synchronous {Bidirectional} {Neural} {Machine} {Translation}},
	volume = {7},
	url = {https://aclanthology.org/Q19-1006},
	doi = {10.1162/tacl_a_00256},
	abstract = {Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional–neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese–English, WMT14 English–German, and WMT18 Russian–English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese–English and English–German translation tasks.},
	urldate = {2024-05-15},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhou, Long and Zhang, Jiajun and Zong, Chengqing},
	editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {91--105},
	file = {Full Text PDF:/Users/jcw/Zotero/storage/73UUXUEP/Zhou et al. - 2019 - Synchronous Bidirectional Neural Machine Translati.pdf:application/pdf},
}

@misc{chan_kermit_2019,
	title = {{KERMIT}: {Generative} {Insertion}-{Based} {Modeling} for {Sequences}},
	shorttitle = {{KERMIT}},
	url = {http://arxiv.org/abs/1906.01604},
	abstract = {We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespeciﬁed factorization of the data distribution. During training, one can feed KERMIT paired data (x, y) to learn the joint distribution p(x, y), and optionally mix in unpaired data x or y to reﬁne the marginals p(x) or p(y). During inference, we have access to the conditionals p(x {\textbar} y) and p(y {\textbar} x) in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our uniﬁed approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-speciﬁc architectural adaptation.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Chan, William and Kitaev, Nikita and Guu, Kelvin and Stern, Mitchell and Uszkoreit, Jakob},
	month = jun,
	year = {2019},
	note = {arXiv:1906.01604 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: William Chan, Nikita Kitaev, Kelvin Guu, and Mitchell Stern contributed equally},
	file = {Chan et al. - 2019 - KERMIT Generative Insertion-Based Modeling for Se.pdf:/Users/jcw/Zotero/storage/47SA3IB9/Chan et al. - 2019 - KERMIT Generative Insertion-Based Modeling for Se.pdf:application/pdf},
}

@misc{noauthor_httpsarxivorgpdf190601604_nodate,
	title = {https://arxiv.org/pdf/1906.01604},
	url = {https://arxiv.org/pdf/1906.01604},
	urldate = {2024-05-16},
}

@misc{bavarian_efficient_2022,
	title = {Efficient {Training} of {Language} {Models} to {Fill} in the {Middle}},
	url = {http://arxiv.org/abs/2207.14255},
	abstract = {We show that autoregressive language models can learn to inﬁll text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efﬁciency of training models to ﬁll-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the inﬁll span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best inﬁlling model trained with best practices in our API, and release our inﬁlling benchmarks to aid future research.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.14255 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Bavarian et al. - 2022 - Efficient Training of Language Models to Fill in t.pdf:/Users/jcw/Zotero/storage/7LLAIW6G/Bavarian et al. - 2022 - Efficient Training of Language Models to Fill in t.pdf:application/pdf},
}

@misc{xiao_survey_2023,
	title = {A {Survey} on {Non}-{Autoregressive} {Generation} for {Neural} {Machine} {Translation} and {Beyond}},
	url = {http://arxiv.org/abs/2204.09269},
	abstract = {Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications. The web page of this survey is at https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Xiao, Yisheng and Wu, Lijun and Guo, Junliang and Li, Juntao and Zhang, Min and Qin, Tao and Liu, Tie-yan},
	month = jul,
	year = {2023},
	note = {arXiv:2204.09269 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 31 pages, 10 figures, 5 tables},
	file = {Xiao et al. - 2023 - A Survey on Non-Autoregressive Generation for Neur.pdf:/Users/jcw/Zotero/storage/YR5F6D53/Xiao et al. - 2023 - A Survey on Non-Autoregressive Generation for Neur.pdf:application/pdf},
}

@misc{gu_levenshtein_2019,
	title = {Levenshtein {Transformer}},
	url = {http://arxiv.org/abs/1905.11006},
	abstract = {Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.},
	language = {en},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Gu, Jiatao and Wang, Changhan and Zhao, Jake},
	month = oct,
	year = {2019},
	note = {arXiv:1905.11006 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 17 pages (6 pages appendix). Camera ready, accepted by NeurIPS 2019},
	file = {Gu et al. - 2019 - Levenshtein Transformer.pdf:/Users/jcw/Zotero/storage/NP3BH682/Gu et al. - 2019 - Levenshtein Transformer.pdf:application/pdf},
}

@misc{wang_reinforcement_2024,
	title = {Reinforcement {Learning} for {Edit}-{Based} {Non}-{Autoregressive} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/2405.01280},
	doi = {10.48550/arXiv.2405.01280},
	abstract = {Non-autoregressive (NAR) language models are known for their low latency in neural machine translation (NMT). However, a performance gap exists between NAR and autoregressive models due to the large decoding space and difficulty in capturing dependency between target words accurately. Compounding this, preparing appropriate training data for NAR models is a non-trivial task, often exacerbating exposure bias. To address these challenges, we apply reinforcement learning (RL) to Levenshtein Transformer, a representative edit-based NAR model, demonstrating that RL with self-generated data can enhance the performance of edit-based NAR models. We explore two RL approaches: stepwise reward maximization and episodic reward maximization. We discuss the respective pros and cons of these two approaches and empirically verify them. Moreover, we experimentally investigate the impact of temperature setting on performance, confirming the importance of proper temperature setting for NAR models' training.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Wang, Hao and Morimura, Tetsuro and Honda, Ukyo and Kawahara, Daisuke},
	month = may,
	year = {2024},
	note = {arXiv:2405.01280 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/L52ACUSM/Wang et al. - 2024 - Reinforcement Learning for Edit-Based Non-Autoregr.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/WE9KWBJW/2405.html:text/html},
}

@misc{kaiser_fast_2018,
	title = {Fast {Decoding} in {Sequence} {Models} using {Discrete} {Latent} {Variables}},
	url = {http://arxiv.org/abs/1803.03382},
	doi = {10.48550/arXiv.1803.03382},
	abstract = {Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. Inspired by [arxiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Kaiser, Łukasz and Roy, Aurko and Vaswani, Ashish and Parmar, Niki and Bengio, Samy and Uszkoreit, Jakob and Shazeer, Noam},
	month = jun,
	year = {2018},
	note = {arXiv:1803.03382 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICML 2018},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/28PQ4C9Q/Kaiser et al. - 2018 - Fast Decoding in Sequence Models using Discrete La.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/XGPSK2HC/1803.html:text/html},
}

@misc{savinov_step-unrolled_2022,
	title = {Step-unrolled {Denoising} {Autoencoders} for {Text} {Generation}},
	url = {http://arxiv.org/abs/2112.06749},
	doi = {10.48550/arXiv.2112.06749},
	abstract = {In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and Oord, Aaron van den},
	month = apr,
	year = {2022},
	note = {arXiv:2112.06749 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to ICLR 2022},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/Y9WVG2IX/Savinov et al. - 2022 - Step-unrolled Denoising Autoencoders for Text Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/W2JHQPGB/2112.html:text/html},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/B2GCJGJT/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/9MR6FKIH/1910.html:text/html},
}

@misc{donahue_enabling_2020,
	title = {Enabling {Language} {Models} to {Fill} in the {Blanks}},
	url = {http://arxiv.org/abs/2005.05339},
	doi = {10.48550/arXiv.2005.05339},
	abstract = {We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling---a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Donahue, Chris and Lee, Mina and Liang, Percy},
	month = sep,
	year = {2020},
	note = {arXiv:2005.05339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at ACL 2020},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/SPNZJ4N9/Donahue et al. - 2020 - Enabling Language Models to Fill in the Blanks.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/ECQTPASY/2005.html:text/html},
}

@misc{aghajanyan_cm3_2022,
	title = {{CM3}: {A} {Causal} {Masked} {Multimodal} {Model} of the {Internet}},
	shorttitle = {{CM3}},
	url = {http://arxiv.org/abs/2201.07520},
	doi = {10.48550/arXiv.2201.07520},
	abstract = {We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke},
	month = jan,
	year = {2022},
	note = {arXiv:2201.07520 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/AIDGWSUD/Aghajanyan et al. - 2022 - CM3 A Causal Masked Multimodal Model of the Inter.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/7IQJ5DV9/2201.html:text/html},
}

@misc{fried_incoder_2023,
	title = {{InCoder}: {A} {Generative} {Model} for {Code} {Infilling} and {Synthesis}},
	shorttitle = {{InCoder}},
	url = {http://arxiv.org/abs/2204.05999},
	doi = {10.48550/arXiv.2204.05999},
	abstract = {Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
	month = apr,
	year = {2023},
	note = {arXiv:2204.05999 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
	annote = {Comment: ICLR 2023. v3: camera-ready that includes PLBART and OpenAI baselines},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/FUGIMSWI/Fried et al. - 2023 - InCoder A Generative Model for Code Infilling and.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/3Q2KLEHM/2204.html:text/html},
}

@article{joshi_spanbert_2020,
	title = {{SpanBERT}: {Improving} {Pre}-training by {Representing} and {Predicting} {Spans}},
	volume = {8},
	shorttitle = {{SpanBERT}},
	url = {https://aclanthology.org/2020.tacl-1.5},
	doi = {10.1162/tacl_a_00300},
	abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6\% and 88.7\% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6\% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1},
	urldate = {2024-05-20},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
	editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2020},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {64--77},
	file = {Full Text PDF:/Users/jcw/Zotero/storage/JZVA7GXH/Joshi et al. - 2020 - SpanBERT Improving Pre-training by Representing a.pdf:application/pdf},
}

@misc{deng_reasonbert_2021,
	title = {{ReasonBERT}: {Pre}-trained to {Reason} with {Distant} {Supervision}},
	shorttitle = {{ReasonBERT}},
	url = {http://arxiv.org/abs/2109.04912},
	doi = {10.48550/arXiv.2109.04912},
	abstract = {We present ReasonBert, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we propose a generalized notion of distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases. We conduct a comprehensive evaluation on a variety of extractive question answering datasets ranging from single-hop to multi-hop and from text-only to table-only to hybrid that require various reasoning capabilities and show that ReasonBert achieves remarkable improvement over an array of strong baselines. Few-shot experiments further demonstrate that our pre-training method substantially improves sample efficiency.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Deng, Xiang and Su, Yu and Lees, Alyssa and Wu, You and Yu, Cong and Sun, Huan},
	month = sep,
	year = {2021},
	note = {arXiv:2109.04912 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to EMNLP'2021. Our code and pre-trained models are available at https://github.com/sunlab-osu/ReasonBERT},
	file = {arXiv Fulltext PDF:/Users/jcw/Zotero/storage/WFYCEKV8/Deng et al. - 2021 - ReasonBERT Pre-trained to Reason with Distant Sup.pdf:application/pdf;arXiv.org Snapshot:/Users/jcw/Zotero/storage/YZUPCPI2/2109.html:text/html},
}




@book{peters_elements_2017,
	title = {Elements of {Causal} {Inference}: {Foundations} and {Learning} {Algorithms}},
	isbn = {978-0-262-03731-0 978-0-262-34429-6},
	shorttitle = {Elements of {Causal} {Inference}},
	url = {https://library.oapen.org/handle/20.500.12657/26040},
	abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning.The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.},
	language = {English},
	urldate = {2024-05-27},
	publisher = {The MIT Press},
	author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2017},
	note = {Accepted: 2019-01-20 23:42:51},
	keywords = {algorithmic independence, assumptions, causal minimality, Causality, cause-effect models, computer science, conditional independence, counterfactuals, covariate shift, do-calculus, domain adaptation, episodic reinforcement learning, faithfulness, falsifiability, half-sibling regression, identifiability, interventions, machine learning, markov, multivariate causal models, potential outcomes, probability theory, SCMs, semi-supervised learning, simpson's paradox, statistical models, statistics, thema EDItEUR::U Computing and Information Technology::UM Computer programming / software engineering::UMS Mobile and handheld device programming / Apps programming, thema EDItEUR::U Computing and Information Technology::UY Computer science::UYQ Artificial intelligence::UYQM Machine learning, thema EDItEUR::U Computing and Information Technology::UY Computer science::UYQ Artificial intelligence::UYQN Neural networks and fuzzy systems},
	file = {Full Text PDF:C\:\\Users\\vassi\\Zotero\\storage\\U9RDH6QT\\Peters et al. - 2017 - Elements of Causal Inference Foundations and Lear.pdf:application/pdf},
}


@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	doi = {10.48550/arXiv.1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\vassi\\Zotero\\storage\\KKPNELUA\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vassi\\Zotero\\storage\\GS3G4S6B\\1502.html:text/html},
}


@misc{andrej_karpathymingpt_2024,
	title = {karpathy/{minGPT}},
	copyright = {MIT},
	url = {https://github.com/karpathy/minGPT},
	abstract = {A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training},
	urldate = {2024-05-27},
	author = {Karpathy, Andrej},
	month = may,
	year = {2023},
	note = {original-date: 2020-08-17T07:08:48Z},
}


@misc{thomas_ahle_thomasahle_this_2023,
	type = {Tweet},
	title = {This week {I} trained an {800K} transformer to learn 5 digit multiplication.},
	url = {https://twitter.com/thomasahle/status/1702723749798354976},
	language = {en},
	urldate = {2024-05-27},
	journal = {Twitter},
	author = {{Thomas Ahle}},
	month = sep,
	year = {2023},
	file = {Snapshot:C\:\\Users\\vassi\\Zotero\\storage\\LDEHPU6X\\1702723749798354976.html:text/html},
}


@misc{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	doi = {10.48550/arXiv.2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Scholkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv:2102.11107 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\vassi\\Zotero\\storage\\V7GXVKUC\\Schölkopf et al. - 2021 - Towards Causal Representation Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vassi\\Zotero\\storage\\53QNBFXU\\2102.html:text/html},
}
