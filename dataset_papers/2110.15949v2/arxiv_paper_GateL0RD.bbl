\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Sutton and Barto(2018)]{Sutton:2018}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement learning: {A}n introduction}.
\newblock MIT press, Cambridge, MA, second edition edition, 2018.

\bibitem[Hausknecht and Stone(2015)]{hausknecht2017deep}
Matthew~J. Hausknecht and Peter Stone.
\newblock Deep recurrent {Q}-learning for partially observable {MDPs}.
\newblock \emph{arXiv preprint arXiv:1507.06527}, 2015.
\newblock URL \url{http://arxiv.org/abs/1507.06527}.

\bibitem[Igl et~al.(2018)Igl, Zintgraf, Le, Wood, and Whiteson]{igl2018deep}
Maximilian Igl, Luisa Zintgraf, Tuan~Anh Le, Frank Wood, and Shimon Whiteson.
\newblock Deep variational reinforcement learning for {POMDPs}.
\newblock In \emph{International Conference on Machine Learning}, pages
  2117--2126. PMLR, 2018.

\bibitem[Zhu et~al.(2017)Zhu, Li, and Poupart]{zhu2018improving}
Pengfei Zhu, X.~Li, and P.~Poupart.
\newblock On improving deep reinforcement learning for {POMDP}s.
\newblock \emph{arXiv preprint arXiv:1804.06309}, 2017.
\newblock URL \url{http://arxiv.org/abs/1804.06309}.

\bibitem[Hochreiter and Schmidhuber(1997)]{LSTM1}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and Bengio]{GRU1}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.
\newblock URL \url{https://arxiv.org/abs/1412.3555}.

\bibitem[Peters et~al.(2017)Peters, Janzing, and Sch{\"o}lkopf]{schoelkopfBook}
Jonas Peters, Dominik Janzing, and Bernhard Sch{\"o}lkopf.
\newblock \emph{Elements of causal inference: {F}oundations and learning
  algorithms}.
\newblock MIT press, 2017.

\bibitem[Sch{\"o}lkopf(2019)]{schoelkopfCausality}
Bernhard Sch{\"o}lkopf.
\newblock Causality for machine learning.
\newblock \emph{arXiv preprint arXiv:1911.10500}, 2019.

\bibitem[Sch{\"o}lkopf et~al.(2021)Sch{\"o}lkopf, Locatello, Bauer, Ke,
  Kalchbrenner, Goyal, and Bengio]{schoelkopfBengio}
Bernhard Sch{\"o}lkopf, Francesco Locatello, Stefan Bauer, Nan~Rosemary Ke, Nal
  Kalchbrenner, Anirudh Goyal, and Yoshua Bengio.
\newblock Towards causal representation learning.
\newblock In \emph{Proceedings of the IEEE}, 2021.

\bibitem[Pitis and Garg(2020)]{Pitis2020}
Elliot Pitis, Silviu~Creager and Animesh Garg.
\newblock Counterfactual data augmentation using locally factored dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2020)}, 2020.

\bibitem[Seitzer et~al.(2021)Seitzer, Sch{\"o}lkopf, and
  Martius]{Seitzer2021CID}
Maximilian Seitzer, Bernhard Sch{\"o}lkopf, and Georg Martius.
\newblock Causal influence detection for improving efficiency in reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems 35
  (NeurIPS 2021)}, 2021.

\bibitem[Baldwin and Kosie(2021)]{BaldwinKosie:2021}
Dare~A. Baldwin and Jessica~E. Kosie.
\newblock How does the mind render streaming experience as events?
\newblock \emph{Topics in Cognitive Science}, 13\penalty0 (1):\penalty0
  79--105, 2021.
\newblock \doi{https://doi.org/10.1111/tops.12502}.

\bibitem[Butz et~al.(2021)Butz, Achimova, Bilkey, and Knott]{Butz:2021}
Martin~V. Butz, Asya Achimova, David Bilkey, and Alistair Knott.
\newblock Event-predictive cognition: A root for conceptual human thought.
\newblock \emph{Topics in Cognitive Science}, 13\penalty0 (1):\penalty0 10--24,
  2021.
\newblock \doi{https://doi.org/10.1111/tops.12522}.

\bibitem[Kuperberg(2021)]{Kuperberg:2021}
Gina~R. Kuperberg.
\newblock Tea with milk? {A} hierarchical generative framework of sequential
  event comprehension.
\newblock \emph{Topics in Cognitive Science}, 13:\penalty0 256--298, 2021.
\newblock \doi{10.1111/tops.12518}.

\bibitem[Radvansky and Zacks(2014)]{radvansky2014event}
Gabriel~A. Radvansky and Jeffrey~M. Zacks.
\newblock \emph{Event cognition}.
\newblock Oxford University Press, 2014.

\bibitem[Zacks et~al.(2007)Zacks, Speer, Swallow, Braver, and
  Reynolds]{Zacks:2007}
Jeffrey~M. Zacks, Nicole~K. Speer, Khena~M. Swallow, Todd~S. Braver, and
  Jeremy~R. Reynolds.
\newblock Event perception: a mind-brain perspective.
\newblock \emph{Psychological bulletin}, 133\penalty0 (2):\penalty0 273–293,
  2007.
\newblock \doi{10.1037/0033-2909.133.2.273}.

\bibitem[Butz(2016)]{Butz:2016}
Martin~V. Butz.
\newblock Towards a unified sub-symbolic computational theory of cognition.
\newblock \emph{Frontiers in Psychology}, 7\penalty0 (925), 2016.
\newblock \doi{10.3389/fpsyg.2016.00925}.

\bibitem[Butz et~al.(2019)Butz, Bilkey, Humaidan, Knott, and Otte]{Butz:2019}
Martin~V. Butz, David Bilkey, Dania Humaidan, Alistair Knott, and Sebastian
  Otte.
\newblock Learning, planning, and control in a monolithic neural event
  inference architecture.
\newblock \emph{Neural Networks}, 117:\penalty0 135--144, 2019.
\newblock \doi{10.1016/j.neunet.2019.05.001}.

\bibitem[Gumbsch et~al.(2019)Gumbsch, Butz, and Martius]{gumbsch2019autonomous}
Christian Gumbsch, Martin~V. Butz, and Georg Martius.
\newblock Autonomous identification and goal-directed invocation of
  event-predictive behavioral primitives.
\newblock \emph{IEEE Transactions on Cognitive and Developmental Systems},
  13\penalty0 (2):\penalty0 298--311, June 2019.
\newblock \doi{10.1109/TCDS.2019.2925890}.
\newblock URL \url{https://ieeexplore.ieee.org/document/8753716}.

\bibitem[Humaidan et~al.(2021)Humaidan, Otte, Gumbsch, Wu, and
  Butz]{Humaidan:2021}
Dania Humaidan, Sebastian Otte, Christian Gumbsch, Charley~M. Wu, and Martin~V.
  Butz.
\newblock Latent event-predictive encodings through counterfactual
  regularization.
\newblock \emph{Proceedings of the Annual Meeting of the Cognitive Science
  Society}, 43, 2021.
\newblock URL \url{https://escholarship.org/uc/item/5z38p85g}.

\bibitem[Schapiro et~al.(2013)Schapiro, Rogers, Cordova, Turk-Browne, and
  Botvinick]{Schapiro:2013}
Anna~C. Schapiro, Timothy~T. Rogers, Natalia~I. Cordova, Nicholas~B.
  Turk-Browne, and Matthew~M. Botvinick.
\newblock Neural representations of events arise from temporal community
  structure.
\newblock \emph{Nat Neurosci}, 16\penalty0 (4):\penalty0 486--492, April 2013.
\newblock ISSN 1097-6256.
\newblock URL \url{http://dx.doi.org/10.1038/nn.3331}.

\bibitem[Shin and DuBrow(2021)]{Shin:2021}
Yeon~Soon Shin and Sarah DuBrow.
\newblock Structuring memory through inference-based event segmentation.
\newblock \emph{Topics in Cognitive Science}, 13:\penalty0 106--127, 2021.
\newblock \doi{10.1111/tops.12505}.

\bibitem[Butz(2021)]{Butz:2021a}
Martin~V. Butz.
\newblock Towards strong {AI}.
\newblock \emph{Künstliche Intelligenz}, 35:\penalty0 91--101, 2021.
\newblock \doi{10.1007/s13218-021-00705-x}.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2018L0}
Christos Louizos, Max Welling, and Diederik~P. Kingma.
\newblock Learning sparse neural networks through {$L_0$} regularization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1Y8hhg0b}.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2017concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'17, 2017.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{GumbelSoftmax}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'17, 2017.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{StraightThroughBengio}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Williams(1992)]{REINFORCE}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Kingma and Welling(2014)]{VAE}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'14, 2014.

\bibitem[Mohajerin and Waslander(2017)]{hiddenInit}
Nima Mohajerin and Steven~L Waslander.
\newblock State initialization for recurrent neural network modeling of
  time-series data.
\newblock In \emph{2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 2330--2337. IEEE, 2017.

\bibitem[Ba et~al.(2015)Ba, Mnih, and Kavukcuoglu]{ba2014multiple}
Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu.
\newblock Multiple object recognition with visual attention.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'15, 2015.

\bibitem[Schmidhuber(1992)]{schmidhuber1992learning}
J{\"u}rgen Schmidhuber.
\newblock Learning complex, extended sequences using the principle of history
  compression.
\newblock \emph{Neural Computation}, 4\penalty0 (2):\penalty0 234--242, 1992.

\bibitem[Koutnik et~al.(2014)Koutnik, Greff, Gomez, and
  Schmidhuber]{clockworkRNN}
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber.
\newblock A {C}lockwork {RNN}.
\newblock In Eric~P. Xing and Tony Jebara, editors, \emph{Proceedings of the
  31st International Conference on Machine Learning}, volume~32 of
  \emph{Proceedings of Machine Learning Research}, pages 1863--1871, Bejing,
  China, 22--24 Jun 2014. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v32/koutnik14.html}.

\bibitem[Neil et~al.(2016)Neil, Pfeiffer, and Liu]{PhasedLSTM}
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu.
\newblock Phased lstm: Accelerating recurrent network training for long or
  event-based sequences.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  3882--3890, 2016.

\bibitem[Krueger and Memisevic(2015)]{RegHiddenStates}
David Krueger and Roland Memisevic.
\newblock Regularizing {RNN}s by stabilizing activations.
\newblock \emph{arXiv preprint arXiv:1511.08400}, 2015.
\newblock URL \url{https://arxiv.org/abs/1511.08400}.

\bibitem[Campos et~al.(2018)Campos, Jou, Gir{\'o}-i Nieto, Torres, and
  Chang]{SkipRNN}
V{\'\i}ctor Campos, Brendan Jou, Xavier Gir{\'o}-i Nieto, Jordi Torres, and
  Shih-Fu Chang.
\newblock Skip rnn: Learning to skip state updates in recurrent neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'18, 2018.

\bibitem[Li et~al.(2018)Li, He, Tian, Chen, Qin, Wang, and Liu]{BinaryLSTM}
Zhuohan Li, Di~He, Fei Tian, Wei Chen, Tao Qin, Liwei Wang, and Tieyan Liu.
\newblock Towards binary-valued gates for robust {LSTM} training.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 2995--3004. PMLR,
  10--15 Jul 2018.

\bibitem[Hartvigsen et~al.(2020)Hartvigsen, Sen, Kong, and
  Rundensteiner]{hartvigsen2020learning}
Thomas Hartvigsen, Cansu Sen, Xiangnan Kong, and Elke Rundensteiner.
\newblock Learning to selectively update state neurons in recurrent networks.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Information \& Knowledge Management}, pages 485--494, 2020.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2015neural}
Dzmitry Bahdanau, Kyung~Hyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{3rd International Conference on Learning Representations,
  ICLR 2015}, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{VaswaniEtAl2017:transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Goyal et~al.(2021)Goyal, Lamb, Hoffmann, Sodhani, Levine, Bengio, and
  Sch{\"o}lkopf]{RIMs}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine,
  Yoshua Bengio, and Bernhard Sch{\"o}lkopf.
\newblock Recurrent independent mechanisms.
\newblock In \emph{9th International Conference on Learning Representations
  (ICLR 2021)}, May 2021.
\newblock URL \url{https://openreview.net/pdf?id=mLcmdlEUxy-}.

\bibitem[Madan et~al.(2021)Madan, Ke, Goyal, Sch{\"{o}}lkopf, and
  Bengio]{MetaRIMs}
Kanika Madan, Nan~Rosemary Ke, Anirudh Goyal, Bernhard Sch{\"{o}}lkopf, and
  Yoshua Bengio.
\newblock Fast and slow learning of recurrent independent mechanisms.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'21, 2021.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu, Gulcehre,
  Jayakumar, Jaderberg, Kaufman, Clark, Noury,
  et~al.]{parisotto2020stabilizing}
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre,
  Siddhant Jayakumar, Max Jaderberg, Raphael~Lopez Kaufman, Aidan Clark, Seb
  Noury, et~al.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7487--7498. PMLR, 2020.

\bibitem[Elman(1990)]{ElmanRNN}
Jeffrey~L. Elman.
\newblock Finding structure in time.
\newblock \emph{Cognitive Science}, 14\penalty0 (2):\penalty0 179--211, 1990.
\newblock ISSN 0364-0213.
\newblock \doi{https://doi.org/10.1016/0364-0213(90)90002-E}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/036402139090002E}.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock {Adam}: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'14, 2014.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI} gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chevalier-Boisvert et~al.(2018{\natexlab{a}})Chevalier-Boisvert,
  Willems, and Pal]{MiniGrid}
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal.
\newblock Minimalistic gridworld environment for openai gym.
\newblock \url{https://github.com/maximecb/gym-minigrid}, 2018{\natexlab{a}}.

\bibitem[Bengio et~al.(2015)Bengio, Vinyals, Jaitly, and
  Shazeer]{scheduledSampling}
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf}.

\bibitem[Lamb et~al.(2016)Lamb, Zhang, Zhang, Courville, and
  Bengio]{ProfessorForcing}
Anirudh~Goyal Lamb, Alex~M, Ying Zhang, Saizheng Zhang, Aaron~C Courville, and
  Yoshua Bengio.
\newblock Professor forcing: A new algorithm for training recurrent networks.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/16026d60ff9b54410b3435b403afd226-Paper.pdf}.

\bibitem[Pinneri et~al.(2020)Pinneri, Sawant, Blaes, Achterhold, Stueckler,
  Rol{\i}nek, and Martius]{PinneriEtAl2020:iCEM}
Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg
  Stueckler, Michal Rol{\i}nek, and Georg Martius.
\newblock Sample-efficient cross-entropy method for real-time planning.
\newblock In \emph{Conference on Robot Learning 2020}, 2020.
\newblock URL \url{https://corlconf.github.io/paper_217}.

\bibitem[Chevalier-Boisvert et~al.(2018{\natexlab{b}})Chevalier-Boisvert,
  Bahdanau, Lahlou, Willems, Saharia, Nguyen, and Bengio]{BabyAI}
Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,
  Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio.
\newblock Baby{AI}: A platform to study the sample efficiency of grounded
  language learning.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'18, 2018{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and
  Singh]{Sutton1999:Options-framework}
Richard~S. Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction
  in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1):\penalty0 181--211,
  1999.
\newblock ISSN 0004-3702.
\newblock \doi{https://doi.org/10.1016/S0004-3702(99)00052-1}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0004370299000521}.

\bibitem[Barto and Mahadevan(2003)]{barto2003recent}
Andrew~G Barto and Sridhar Mahadevan.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock \emph{Discrete event dynamic systems}, 13\penalty0 (1):\penalty0
  41--77, 2003.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{gradientNormClipping}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{Proceedings of the 30th International Conference on
  International Conference on Machine Learning - Volume 28}, ICML'13, page
  III–1310–III–1318. JMLR.org, 2013.

\bibitem[Pinneri et~al.(2021)Pinneri, Sawant, Blaes, and
  Martius]{pinneri2021:strong-policies}
Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, and Georg Martius.
\newblock Extracting strong policies for robotics tasks from zero-order
  trajectory optimizers.
\newblock In \emph{International Conference on Learning Representations},
  ICLR'21, 2021.

\bibitem[Lu(2020)]{DyingRelu}
Lu~Lu.
\newblock Dying {ReLU} and initialization: Theory and numerical examples.
\newblock \emph{Communications in Computational Physics}, 28\penalty0
  (5):\penalty0 1671–1706, Jun 2020.
\newblock ISSN 1991-7120.
\newblock \doi{10.4208/cicp.oa-2020-0165}.
\newblock URL \url{http://dx.doi.org/10.4208/cicp.OA-2020-0165}.

\end{thebibliography}
