\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartlett \& Mendelson(2002)Bartlett and Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0 (Nov):\penalty0 463--482, 2002.

\bibitem[Ben~Zaken et~al.(2022)Ben~Zaken, Goldberg, and Ravfogel]{ben-zaken-etal-2022-bitfit}
Ben~Zaken, E., Goldberg, Y., and Ravfogel, S.
\newblock {B}it{F}it: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  1--9, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-short.1}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.1}.

\bibitem[Catoni(2007)]{catoni2007pac}
Catoni, O.
\newblock Pac-bayesian supervised classification: the thermodynamics of statistical learning.
\newblock \emph{arXiv preprint arXiv:0712.0248}, 2007.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2024qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and Saxe]{goodfellow2014qualitatively}
Goodfellow, I.~J., Vinyals, O., and Saxe, A.~M.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{arXiv preprint arXiv:1412.6544}, 2014.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International conference on machine learning}, pp.\  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA, 2000. Morgan Kaufmann.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Farkhoor, Liu, and Yosinski]{li2018measuring}
Li, C., Farkhoor, H., Liu, R., and Yosinski, J.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock \emph{arXiv preprint arXiv:1804.08838}, 2018{\natexlab{a}}.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Xu, Taylor, Studer, and Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018{\natexlab{b}}.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Li, X.~L. and Liang, P.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Liu et~al.(2021)Liu, Wen, Yuan, and Ya-Xiang]{CSIAM-AM-2-585}
Liu, X., Wen, Z., Yuan, and Ya-Xiang.
\newblock Subspace methods for nonlinear optimization.
\newblock \emph{CSIAM Transactions on Applied Mathematics}, 2\penalty0 (4):\penalty0 585--651, 2021.
\newblock ISSN 2708-0579.
\newblock \doi{https://doi.org/10.4208/csiam-am.SO-2021-0016}.
\newblock URL \url{http://global-sci.org/intro/article_detail/csiam-am/19986.html}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lv et~al.(2023)Lv, Yang, Liu, Gao, Guo, and Qiu]{lv2023parameter}
Lv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., and Qiu, X.
\newblock Full parameter fine-tuning for large language models with limited resources, 2023.

\bibitem[Maurer(2004)]{maurer2004note}
Maurer, A.
\newblock A note on the pac bayesian theorem.
\newblock \emph{arXiv preprint cs/0411099}, 2004.

\bibitem[McAllester(1998)]{mcallester1998some}
McAllester, D.~A.
\newblock Some pac-bayesian theorems.
\newblock In \emph{Proceedings of the eleventh annual conference on Computational learning theory}, pp.\  230--234, 1998.

\bibitem[McAllester(1999)]{mcallester1999pac}
McAllester, D.~A.
\newblock Pac-bayesian model averaging.
\newblock In \emph{Proceedings of the twelfth annual conference on Computational learning theory}, pp.\  164--170, 1999.

\bibitem[McAllester(2003)]{mcallester2003pac}
McAllester, D.~A.
\newblock Pac-bayesian stochastic model selection.
\newblock \emph{Machine Learning}, 51:\penalty0 5--21, 2003.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin, Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Pfeiffer et~al.(2020)Pfeiffer, Kamath, R{\"u}ckl{\'e}, Cho, and Gurevych]{pfeiffer2020adapterfusion}
Pfeiffer, J., Kamath, A., R{\"u}ckl{\'e}, A., Cho, K., and Gurevych, I.
\newblock Adapterfusion: Non-destructive task composition for transfer learning.
\newblock \emph{arXiv preprint arXiv:2005.00247}, 2020.

\bibitem[Smith \& Topin(2017)Smith and Topin]{Smith2017ExploringLF}
Smith, L.~N. and Topin, N.
\newblock Exploring loss function topology with cyclical learning rates.
\newblock \emph{ArXiv}, abs/1702.04283, 2017.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:18922459}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Thiemann et~al.(2017)Thiemann, Igel, Wintenberger, and Seldin]{thiemann2017strongly}
Thiemann, N., Igel, C., Wintenberger, O., and Seldin, Y.
\newblock A strongly quasiconvex pac-bayesian bound.
\newblock In \emph{International Conference on Algorithmic Learning Theory}, pp.\  466--492. PMLR, 2017.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vapnik(1991)]{vapnik1991principles}
Vapnik, V.
\newblock Principles of risk minimization for learning theory.
\newblock \emph{Advances in neural information processing systems}, 4, 1991.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai, A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Wiedemann et~al.(2020)Wiedemann, Mehari, Kepp, and Samek]{wiedemann2020dithered}
Wiedemann, S., Mehari, T., Kepp, K., and Samek, W.
\newblock Dithered backprop: A sparse and quantized backpropagation algorithm for more efficient deep neural network training.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}, pp.\  720--721, 2020.

\bibitem[Ye et~al.(2020)Ye, Dai, Luo, Guo, Qi, Yang, and Chen]{ye2020accelerating}
Ye, X., Dai, P., Luo, J., Guo, X., Qi, Y., Yang, J., and Chen, Y.
\newblock Accelerating cnn training by pruning activation gradients.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXV 16}, pp.\  322--338. Springer, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2021understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning (still) requires rethinking generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Bukharin, He, Cheng, Chen, and Zhao]{zhang2023adaptive}
Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T.
\newblock Adaptive budget allocation for parameter-efficient fine-tuning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\end{thebibliography}
