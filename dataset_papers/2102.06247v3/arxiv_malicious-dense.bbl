\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DKK{\etalchar{+}}20}

\bibitem[AB99]{anthony1999neural}
Martin Anthony and Peter~L. Bartlett.
\newblock {\em Neural Network Learning: {T}heoretical Foundations}.
\newblock Cambridge University Press, 1999.

\bibitem[ABL17]{awasthi2017power}
Pranjal Awasthi, Maria{-}Florina Balcan, and Philip~M. Long.
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock {\em Journal of the {ACM}}, 63(6):50:1--50:27, 2017.

\bibitem[AL88]{angluin1988learning}
Dana Angluin and Philip~D. Laird.
\newblock Learning from noisy examples.
\newblock {\em Machine Learning}, 2(4):343--370, 1988.

\bibitem[BBM05]{bartlett2005local}
Peter~L. Bartlett, Olivier Bousquet, and Shahar Mendelson.
\newblock {Local {R}ademacher complexities}.
\newblock {\em The Annals of Statistics}, 33(4):1497 -- 1537, 2005.

\bibitem[BBZ07]{balcan2007margin}
Maria{-}Florina Balcan, Andrei~Z. Broder, and Tong Zhang.
\newblock Margin based active learning.
\newblock In {\em Proceedings of the 20th Annual Conference on Learning
  Theory}, pages 35--50, 2007.

\bibitem[BEK02]{bshouty2002pac}
Nader~H. Bshouty, Nadav Eiron, and Eyal Kushilevitz.
\newblock {PAC} learning with nasty noise.
\newblock {\em Theoretical Computer Science}, 288(2):255--275, 2002.

\bibitem[BFKV96]{blum1996polynomial}
Avrim Blum, Alan~M. Frieze, Ravi Kannan, and Santosh~S. Vempala.
\newblock A polynomial-time algorithm for learning noisy linear threshold
  functions.
\newblock In {\em Proceedings of the 37th Annual {IEEE} Symposium on
  Foundations of Computer Science}, pages 330--338, 1996.

\bibitem[BL13]{balcan2013active}
Maria{-}Florina Balcan and Philip~M. Long.
\newblock Active and passive learning of linear separators under log-concave
  distributions.
\newblock In {\em Proceedings of the 26th Annual Conference on Learning
  Theory}, pages 288--316, 2013.

\bibitem[Bsh98]{bshouty1998new}
Nader~H. Bshouty.
\newblock A new composition theorem for learning algorithms.
\newblock In {\em Proceedings of the 30th Annual {ACM} Symposium on the Theory
  of Computing}, pages 583--589, 1998.

\bibitem[CDF{\etalchar{+}}99]{cesa1999sample}
Nicol{\`{o}} Cesa{-}Bianchi, Eli Dichterman, Paul Fischer, Eli Shamir, and
  Hans~Ulrich Simon.
\newblock Sample-efficient strategies for learning in the presence of noise.
\newblock {\em Journal of the {ACM}}, 46(5):684--719, 1999.

\bibitem[Dan15]{daniely2015ptas}
Amit Daniely.
\newblock A {PTAS} for agnostically learning halfspaces.
\newblock In {\em Proceedings of the 28th Annual Conference on Learning
  Theory}, volume~40, pages 484--502, 2015.

\bibitem[DGT19]{diakonikolas2019distribution}
Ilias Diakonikolas, Themis Gouleakis, and Christos Tzamos.
\newblock Distribution-independent {PAC} learning of halfspaces with {Massart}
  noise.
\newblock In {\em Proceedings of the 33rd Annual Conference on Neural
  Information Processing Systems}, pages 4751--4762, 2019.

\bibitem[DK19]{diakonikolas2019recent}
Ilias Diakonikolas and Daniel~M. Kane.
\newblock Recent advances in algorithmic high-dimensional robust statistics.
\newblock {\em CoRR}, abs/1911.05911, 2019.

\bibitem[DKK{\etalchar{+}}16]{diakonikolas2016robust}
Ilias Diakonikolas, Gautam Kamath, Daniel~M. Kane, Jerry Li, Ankur Moitra, and
  Alistair Stewart.
\newblock Robust estimators in high dimensions without the computational
  intractability.
\newblock In {\em Proceedings of the 57th Annual {IEEE} Symposium on
  Foundations of Computer Science}, pages 655--664, 2016.

\bibitem[DKK{\etalchar{+}}20]{diakonikolas2020polynomial}
Ilias Diakonikolas, Daniel~M. Kane, Vasilis Kontonis, Christos Tzamos, and
  Nikos Zarifis.
\newblock A polynomial time algorithm for learning halfspaces with {Tsybakov}
  noise.
\newblock {\em CoRR}, abs/2010.01705, 2020.

\bibitem[DKS18]{diakonikolas2018learning}
Ilias Diakonikolas, Daniel~M. Kane, and Alistair Stewart.
\newblock Learning geometric concepts with nasty noise.
\newblock In {\em Proceedings of the 50th Annual {ACM} Symposium on Theory of
  Computing}, pages 1061--1073, 2018.

\bibitem[DKTZ20]{diakonikolas2020learning}
Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis.
\newblock Learning halfspaces with {Massart} noise under structured
  distributions.
\newblock In {\em Proceedings of the 33rd Annual Conference on Learning
  Theory}, pages 1486--1513, 2020.

\bibitem[DKZ20]{diakonikolas2020near}
Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis.
\newblock Near-optimal {SQ} lower bounds for agnostically learning halfspaces
  and {ReLUs} under gaussian marginals.
\newblock In {\em Proceedings of the 34th Annual Conference on Neural
  Information Processing Systems}, pages 13586--13596, 2020.

\bibitem[Hau92]{haussler1992decision}
David Haussler.
\newblock Decision theoretic generalizations of the {PAC} model for neural net
  and other learning applications.
\newblock {\em Information and Computation}, 100(1):78--150, 1992.

\bibitem[KKMS05]{kalai2005agnostic}
Adam~Tauman Kalai, Adam~R. Klivans, Yishay Mansour, and Rocco~A. Servedio.
\newblock Agnostically learning halfspaces.
\newblock In {\em Proceedings of the 46th Annual {IEEE} Symposium on
  Foundations of Computer Science}, pages 11--20, 2005.

\bibitem[KL88]{kearns1988learning}
Michael~J. Kearns and Ming Li.
\newblock Learning in the presence of malicious errors.
\newblock In {\em Proceedings of the 20th Annual {ACM} Symposium on Theory of
  Computing}, pages 267--280, 1988.

\bibitem[KLS09]{klivans2009learning}
Adam~R. Klivans, Philip~M. Long, and Rocco~A. Servedio.
\newblock Learning halfspaces with malicious noise.
\newblock {\em Journal of Machine Learning Research}, 10:2715--2740, 2009.

\bibitem[KSS92]{kearns1992toward}
Michael~J. Kearns, Robert~E. Schapire, and Linda Sellie.
\newblock Toward efficient agnostic learning.
\newblock In David Haussler, editor, {\em Proceedings of the 5th Annual
  Conference on Computational Learning Theory}, pages 341--352, 1992.

\bibitem[LRV16]{lai2016agnostic}
Kevin~A. Lai, Anup~B. Rao, and Santosh~S. Vempala.
\newblock Agnostic estimation of mean and covariance.
\newblock In {\em Proceedings of the 57th Annual {IEEE} Symposium on
  Foundations of Computer Science}, pages 665--674, 2016.

\bibitem[LV07]{lovasz2007geometry}
L{\'{a}}szl{\'{o}} Lov{\'{a}}sz and Santosh~S. Vempala.
\newblock The geometry of logconcave functions and sampling algorithms.
\newblock {\em Random Structures and Algorithms}, 30(3):307--358, 2007.

\bibitem[MN06]{massart2006risk}
Pascal Massart and {\'E}lodie N{\'e}d{\'e}lec.
\newblock Risk bounds for statistical learning.
\newblock {\em The Annals of Statistics}, pages 2326--2366, 2006.

\bibitem[MT94]{maass1994fast}
Wolfgang Maass and Gy{\"o}rgy Tur{\'a}n.
\newblock How fast can a threshold gate learn?
\newblock In {\em Proceedings of a workshop on computational learning theory
  and natural learning systems (vol. 1): constraints and prospects}, pages
  381--414, 1994.

\bibitem[Ros58]{rosenblatt1958perceptron}
Frank Rosenblatt.
\newblock The {P}erceptron: {A} probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological review}, 65(6):386--408, 1958.

\bibitem[RV13]{rudelson2013hanson}
Mark Rudelson and Roman Vershynin.
\newblock {H}anson-{W}right inequality and sub-gaussian concentration.
\newblock {\em Electronic Communications in Probability}, 18:1--9, 2013.

\bibitem[Sch92]{schapire1992design}
Robert~E. Schapire.
\newblock {\em Design and analysis of efficient learning algorithms}.
\newblock {MIT} Press, Cambridge, MA, USA, 1992.

\bibitem[She20]{shen2020power}
Jie Shen.
\newblock On the power of localized {P}erceptron for label-optimal learning of
  halfspaces with adversarial noise.
\newblock {\em CoRR}, abs/2012.10793, 2020.

\bibitem[Slo88]{sloan1988types}
Robert~H. Sloan.
\newblock Types of noise in data for concept learning.
\newblock In {\em Proceedings of the First Annual Workshop on Computational
  Learning Theory}, pages 91--96, 1988.

\bibitem[SZ21]{shen2020attribute}
Jie Shen and Chicheng Zhang.
\newblock Attribute-efficient learning of halfspaces with malicious noise:
  Near-optimal label complexity and noise tolerance.
\newblock In {\em Proceedings of the 32nd International Conference on
  Algorithmic Learning Theory}, pages 1072--1113, 2021.

\bibitem[Tro12]{tropp2012user}
Joel~A. Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock {\em Foundations of Computational Mathematics}, 12(4):389--434, 2012.

\bibitem[Tsy04]{tsybakov2004optimal}
Alexander~B. Tsybakov.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock {\em The Annals of Statistics}, 32(1):135--166, 2004.

\bibitem[Val84]{valiant1984theory}
Leslie~G. Valiant.
\newblock A theory of the learnable.
\newblock {\em Communications of the {ACM}}, 27(11):1134--1142, 1984.

\bibitem[Val85]{valiant1985learning}
Leslie~G. Valiant.
\newblock Learning disjunction of conjunctions.
\newblock In {\em Proceedings of the 9th International Joint Conference on
  Artificial Intelligence}, pages 560--566, 1985.

\bibitem[Vem10]{vempala2010random}
Santosh~S. Vempala.
\newblock A random-sampling-based algorithm for learning intersections of
  halfspaces.
\newblock {\em Journal of the {ACM}}, 57(6):32:1--32:14, 2010.

\bibitem[Zha18]{zhang2018efficient}
Chicheng Zhang.
\newblock Efficient active learning of sparse halfspaces.
\newblock In {\em Proceedings of the 31st Annual Conference On Learning
  Theory}, pages 1856--1880, 2018.

\bibitem[ZSA20]{zhang2020efficient}
Chicheng Zhang, Jie Shen, and Pranjal Awasthi.
\newblock Efficient active learning of sparse halfspaces with arbitrary bounded
  noise.
\newblock In {\em Proceedings of the 34th Annual Conference on Neural
  Information Processing Systems}, pages 7184--7197, 2020.

\end{thebibliography}
