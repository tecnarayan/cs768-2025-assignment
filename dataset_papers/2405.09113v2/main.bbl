\begin{thebibliography}{10}

\bibitem{bach2022promptsource}
Stephen~H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel,
  Nihal~V Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault Fevry,
  et~al.
\newblock Promptsource: An integrated development environment and repository
  for natural language prompts.
\newblock {\em arXiv preprint arXiv:2202.01279}, 2022.

\bibitem{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock {\em arXiv preprint arXiv:2204.05862}, 2022.

\bibitem{bartolo2021improving}
Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, and
  Douwe Kiela.
\newblock Improving question answering model robustness with synthetic
  adversarial data generation.
\newblock {\em arXiv preprint arXiv:2104.08678}, 2021.

\bibitem{carlini2024aligned}
Nicholas Carlini, Milad Nasr, Christopher~A Choquette-Choo, Matthew Jagielski,
  Irena Gao, Pang Wei~W Koh, Daphne Ippolito, Florian Tramer, and Ludwig
  Schmidt.
\newblock Are aligned neural networks adversarially aligned?
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{pair}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas,
  and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock {\em arXiv preprint arXiv:2310.08419}, 2023.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock {\em See https://vicuna. lmsys. org (accessed 14 April 2023)},
  2(3):6, 2023.

\bibitem{dinan2019build}
Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston.
\newblock Build it break it fix it for dialogue safety: Robustness from
  adversarial human attack.
\newblock {\em arXiv preprint arXiv:1908.06083}, 2019.

\bibitem{ebrahimi-etal-2018-hotflip}
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.
\newblock {H}ot{F}lip: White-box adversarial examples for text classification.
\newblock In Iryna Gurevych and Yusuke Miyao, editors, {\em Proceedings of the
  56th Annual Meeting of the Association for Computational Linguistics (Volume
  2: Short Papers)}, 2018.

\bibitem{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav
  Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling
  behaviors, and lessons learned.
\newblock {\em arXiv preprint arXiv:2209.07858}, 2022.

\bibitem{gehman-etal-2020-realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {R}eal{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In Trevor Cohn, Yulan He, and Yang Liu, editors, {\em Findings of the
  Association for Computational Linguistics: EMNLP 2020}, pages 3356--3369,
  2020.

\bibitem{greshake2023more}
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten
  Holz, and Mario Fritz.
\newblock More than you've asked for: A comprehensive analysis of novel prompt
  injection threats to application-integrated large language models.
\newblock {\em arXiv e-prints}, pages arXiv--2302, 2023.

\bibitem{guo2021gradient}
Chuan Guo, Alexandre Sablayrolles, Herv{\'e} J{\'e}gou, and Douwe Kiela.
\newblock Gradient-based adversarial attacks against text transformers.
\newblock {\em arXiv preprint arXiv:2104.13733}, 2021.

\bibitem{hazell2023large}
Julian Hazell.
\newblock Large language models can be used to effectively scale spear phishing
  campaigns.
\newblock {\em arXiv preprint arXiv:2305.06972}, 2023.

\bibitem{jones2023automatically}
Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt.
\newblock Automatically auditing large language models via discrete
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  15307--15329. PMLR, 2023.

\bibitem{kang2023exploiting}
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and
  Tatsunori Hashimoto.
\newblock Exploiting programmatic behavior of llms: Dual-use through standard
  security attacks.
\newblock {\em arXiv preprint arXiv:2302.05733}, 2023.

\bibitem{karabacak2023embracing}
Mert Karabacak and Konstantinos Margetis.
\newblock Embracing large language models for medical applications:
  opportunities and challenges.
\newblock {\em Cureus}, 15(5), 2023.

\bibitem{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
  Wen-tau Yih, editors, {\em Proceedings of the 2021 Conference on Empirical
  Methods in Natural Language Processing}, 2021.

\bibitem{li-etal-2023-multi-step}
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu
  Song.
\newblock Multi-step jailbreaking privacy attacks on {C}hat{GPT}.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Findings
  of the Association for Computational Linguistics: EMNLP 2023}, 2023.

\bibitem{autodan}
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao.
\newblock Autodan: Generating stealthy jailbreak prompts on aligned large
  language models.
\newblock {\em arXiv preprint arXiv:2310.04451}, 2023.

\bibitem{lu2023instag}
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang
  Zhou, and Jingren Zhou.
\newblock \# instag: Instruction tagging for analyzing supervised fine-tuning
  of large language models.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem{lukas2023analyzing}
Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and
  Santiago Zanella-B{\'e}guelin.
\newblock Analyzing leakage of personally identifiable information in language
  models.
\newblock In {\em 2023 IEEE Symposium on Security and Privacy (SP)}, pages
  346--363. IEEE, 2023.

\bibitem{luo2022biogpt}
Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and
  Tie-Yan Liu.
\newblock Biogpt: generative pre-trained transformer for biomedical text
  generation and mining.
\newblock {\em Briefings in bioinformatics}, 23(6):bbac409, 2022.

\bibitem{maus2023black}
Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner.
\newblock Black box adversarial prompting for foundation models.
\newblock {\em arXiv preprint arXiv:2302.04237}, 2023.

\bibitem{harmbench}
Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham
  Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al.
\newblock Harmbench: A standardized evaluation framework for automated red
  teaming and robust refusal.
\newblock {\em arXiv preprint arXiv:2402.04249}, 2024.

\bibitem{tap}
Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum
  Anderson, Yaron Singer, and Amin Karbasi.
\newblock Tree of attacks: Jailbreaking black-box llms automatically.
\newblock {\em arXiv preprint arXiv:2312.02119}, 2023.

\bibitem{meyer2023chatgpt}
Jesse~G Meyer, Ryan~J Urbanowicz, Patrick~CN Martin, Karen Oâ€™Connor, Ruowang
  Li, Pei-Chen Peng, Tiffani~J Bright, Nicholas Tatonetti, Kyoung~Jae Won,
  Graciela Gonzalez-Hernandez, et~al.
\newblock Chatgpt and large language models in academia: opportunities and
  challenges.
\newblock {\em BioData Mining}, 16(1):20, 2023.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in neural information processing systems},
  35:27730--27744, 2022.

\bibitem{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock {\em arXiv preprint arXiv:2202.03286}, 2022.

\bibitem{qin2022cold}
Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi.
\newblock Cold decoding: Energy-based constrained text generation with langevin
  dynamics.
\newblock {\em Advances in Neural Information Processing Systems},
  35:9538--9551, 2022.

\bibitem{ren2024learning}
Mengjie Ren, Boxi Cao, Hongyu Lin, Liu Cao, Xianpei Han, Ke~Zeng, Guanglu Wan,
  Xunliang Cai, and Le~Sun.
\newblock Learning or self-aligning? rethinking instruction fine-tuning.
\newblock {\em arXiv preprint arXiv:2402.18243}, 2024.

\bibitem{roziere2023code}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
  Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin,
  et~al.
\newblock Code llama: Open foundation models for code.
\newblock {\em arXiv preprint arXiv:2308.12950}, 2023.

\bibitem{autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock {\em arXiv preprint arXiv:2010.15980}, 2020.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484--489, 2016.

\bibitem{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society (palms) with
  values-targeted datasets.
\newblock {\em Advances in Neural Information Processing Systems},
  34:5861--5873, 2021.

\bibitem{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{zephyr}
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul,
  Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier,
  Nathan Habib, et~al.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock {\em arXiv preprint arXiv:2310.16944}, 2023.

\bibitem{wang2022exploring}
Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi,
  Bo~Li, Anima Anandkumar, and Bryan Catanzaro.
\newblock Exploring the limits of domain-adaptive training for detoxifying
  large-scale language models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:35811--35824, 2022.

\bibitem{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock {\em arXiv preprint arXiv:2212.10560}, 2022.

\bibitem{wei2024jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does llm safety training fail?
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{welbl-etal-2021-challenges-detoxifying}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang.
\newblock Challenges in detoxifying language models.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
  Wen-tau Yih, editors, {\em Findings of the Association for Computational
  Linguistics: EMNLP 2021}, 2021.

\bibitem{wen2024hard}
Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom
  Goldstein.
\newblock Hard prompts made easy: Gradient-based discrete optimization for
  prompt tuning and discovery.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{wiggins2022opportunities}
Walter~F Wiggins and Ali~S Tejani.
\newblock On the opportunities and risks of foundation models for natural
  language processing in radiology.
\newblock {\em Radiology: Artificial Intelligence}, 4(4):e220119, 2022.

\bibitem{wolf2023fundamental}
Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua.
\newblock Fundamental limitations of alignment in large language models.
\newblock {\em arXiv preprint arXiv:2304.11082}, 2023.

\bibitem{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock {\em arXiv preprint arXiv:1909.08593}, 2019.

\bibitem{gcg}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language
  models.
\newblock {\em arXiv preprint arXiv:2307.15043}, 2023.

\end{thebibliography}
