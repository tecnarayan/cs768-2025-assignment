\begin{thebibliography}{100}

\bibitem{acar2021federated}
D.~A.~E. Acar, Y.~Zhao, R.~Matas, M.~Mattina, P.~Whatmough, and V.~Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{achille2018emergence}
A.~Achille and S.~Soatto.
\newblock Emergence of invariance and disentanglement in deep representations.
\newblock {\em The Journal of Machine Learning Research}, 2018.

\bibitem{ahuja2021invariance}
K.~Ahuja, E.~Caballero, D.~Zhang, J.-C. Gagnon-Audet, Y.~Bengio, I.~Mitliagkas, and I.~Rish.
\newblock Invariance principle meets information bottleneck for out-of-distribution generalization.
\newblock {\em Advances in Neural Information Processing Systems}, 34:3438--3450, 2021.

\bibitem{ahuja2020empirical}
K.~Ahuja, J.~Wang, A.~Dhurandhar, K.~Shanmugam, and K.~R. Varshney.
\newblock Empirical or invariant risk minimization? a sample complexity perspective.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{ainsworth2022git}
S.~Ainsworth, J.~Hayase, and S.~Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{al2020federated}
M.~Al-Shedivat, J.~Gillenwater, E.~Xing, and A.~Rostamizadeh.
\newblock Federated learning via posterior averaging: A new perspective and practical algorithms.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{alain2016understanding}
G.~Alain and Y.~Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock {\em arXiv preprint arXiv:1610.01644}, 2016.

\bibitem{arjovsky2019invariant}
M.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz.
\newblock Invariant risk minimization.
\newblock {\em arXiv preprint arXiv:1907.02893}, 2019.

\bibitem{babakniya2022federated}
S.~Babakniya, S.~Kundu, S.~Prakash, Y.~Niu, and S.~Avestimehr.
\newblock Federated sparse training: Lottery aware model compression for resource constrained edge.
\newblock In {\em Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)}, 2022.

\bibitem{bagdasaryan2020backdoor}
E.~Bagdasaryan, A.~Veit, Y.~Hua, D.~Estrin, and V.~Shmatikov.
\newblock How to backdoor federated learning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 2938--2948, 2020.

\bibitem{belghazi2018mutual}
M.~I. Belghazi, A.~Baratin, S.~Rajeshwar, S.~Ozair, Y.~Bengio, A.~Courville, and D.~Hjelm.
\newblock Mutual information neural estimation.
\newblock In {\em International conference on machine learning}, pages 531--540. PMLR, 2018.

\bibitem{bibikar2022federated}
S.~Bibikar, H.~Vikalo, Z.~Wang, and X.~Chen.
\newblock Federated dynamic sparse training: Computing less, communicating less, yet learning better.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, pages 6080--6088, 2022.

\bibitem{bistritz2020distributed}
I.~Bistritz, A.~Mann, and N.~Bambos.
\newblock Distributed distillation for on-device learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:22593--22604, 2020.

\bibitem{GPT3}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{DataSynthesisDPMRF}
K.~Cai, X.~Lei, J.~Wei, and X.~Xiao.
\newblock Data synthesis via differentially private markov random fields.
\newblock {\em Proc. VLDB Endow.}, 14(11):2190–2202, jul 2021.

\bibitem{chang2019cronus}
H.~Chang, V.~Shejwalkar, R.~Shokri, and A.~Houmansadr.
\newblock Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer.
\newblock {\em arXiv preprint arXiv:1912.11279}, 2019.

\bibitem{CompressiveLearninWithDP}
A.~Chatalic, V.~Schellekens, F.~Houssiau, Y.~A. de~Montjoye, L.~Jacques, and R.~Gribonval.
\newblock {Compressive learning with privacy guarantees}.
\newblock {\em Information and Inference: A Journal of the IMA}, 05 2021.
\newblock iaab005.

\bibitem{chen2019data}
H.~Chen, Y.~Wang, C.~Xu, Z.~Yang, C.~Liu, B.~Shi, C.~Xu, C.~Xu, and Q.~Tian.
\newblock Data-free learning of student networks.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 3514--3522, 2019.

\bibitem{aggregation2}
H.-Y. Chen and W.-L. Chao.
\newblock Fedbe: Making bayesian model ensemble applicable to federated learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{chen2021bridging}
H.-Y. Chen and W.-L. Chao.
\newblock On bridging generic and personalized federated learning for image classification.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In {\em ICML}, 2020.

\bibitem{chen2023understanding}
Y.~Chen, W.~Huang, K.~Zhou, Y.~Bian, B.~Han, and J.~Cheng.
\newblock Understanding and improving feature learning for out-of-distribution generalization.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{dai2024enhancing}
R.~Dai, Y.~Zhang, A.~Li, T.~Liu, X.~Yang, and B.~Han.
\newblock Enhancing one-shot federated learning through data and ensemble co-boosting.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{dai2023tackling}
Y.~Dai, Z.~Chen, J.~Li, S.~Heinecke, L.~Sun, and R.~Xu.
\newblock Tackling data heterogeneity in federated learning with class prototypes.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, 2023.

\bibitem{dennis2021heterogeneity}
D.~K. Dennis, T.~Li, and V.~Smith.
\newblock Heterogeneity for the win: One-shot federated clustering.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{diao2023towards}
Y.~Diao, Q.~Li, and B.~He.
\newblock Towards addressing label skews in one-shot federated learning.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{10323204}
J.~Dong, H.~Li, Y.~Cong, G.~Sun, Y.~Zhang, and L.~Van~Gool.
\newblock No one left behind: Real-world federated class-incremental learning.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 46(4):2054--2070, 2024.

\bibitem{dong2022federated_FCIL}
J.~Dong, L.~Wang, Z.~Fang, G.~Sun, S.~Xu, X.~Wang, and Q.~Zhu.
\newblock Federated class-incremental learning.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2022.

\bibitem{dong2022spherefed}
X.~Dong, S.~Q. Zhang, A.~Li, and H.~Kung.
\newblock Spherefed: Hyperspherical federated learning.
\newblock In {\em European Conference on Computer Vision}, 2022.

\bibitem{DoCoFL}
R.~Dorfman, S.~Vargaftik, Y.~Ben-Itzhak, and K.~Y. Levy.
\newblock Docofl: downlink compression for cross-device federated learning.
\newblock In {\em Proceedings of the 40th International Conference on Machine Learning}, 2023.

\bibitem{loramoe}
S.~Dou, E.~Zhou, Y.~Liu, S.~Gao, W.~Shen, L.~Xiong, Y.~Zhou, X.~Wang, Z.~Xi, X.~Fan, S.~Pu, J.~Zhu, R.~Zheng, T.~Gui, Q.~Zhang, and X.~Huang.
\newblock {L}o{RAM}o{E}: Alleviating world knowledge forgetting in large language models via {M}o{E}-style plugin.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, Aug. 2024.

\bibitem{ezzeldin2023fairfed}
Y.~H. Ezzeldin, S.~Yan, C.~He, E.~Ferrara, and A.~S. Avestimehr.
\newblock Fairfed: Enabling group fairness in federated learning.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, pages 7494--7502, 2023.

\bibitem{frankle2018lottery}
J.~Frankle and M.~Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{pmlr-v119-frankle20a}
J.~Frankle, G.~K. Dziugaite, D.~Roy, and M.~Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In {\em Proceedings of the 37th International Conference on Machine Learning}, pages 3259--3269, 2020.

\bibitem{geirhos2020shortcut}
R.~Geirhos, J.-H. Jacobsen, C.~Michaelis, R.~Zemel, W.~Brendel, M.~Bethge, and F.~A. Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock {\em Nature Machine Intelligence}, 2(11):665--673, 2020.

\bibitem{geirhos2018imagenet}
R.~Geirhos, P.~Rubisch, C.~Michaelis, M.~Bethge, F.~A. Wichmann, and W.~Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{FLviaSyntheticData}
J.~Goetz and A.~Tewari.
\newblock Federated learning via synthetic data.
\newblock {\em arXiv preprint arXiv:2008.04489}, 2020.

\bibitem{guha2019one}
N.~Guha, A.~Talwalkar, and V.~Smith.
\newblock One-shot federated learning.
\newblock {\em arXiv preprint arXiv:1902.11175}, 2019.

\bibitem{gulrajani2020search}
I.~Gulrajani and D.~Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{adv_attack}
C.~Guo, M.~Rana, M.~Cisse, and L.~van~der Maaten.
\newblock Countering adversarial images using input transformations.
\newblock In {\em ICLR}, 2020.

\bibitem{gupta2023quantization}
K.~Gupta, M.~Fournarakis, M.~Reisser, C.~Louizos, and M.~Nagel.
\newblock Quantization robust federated learning for efficient inference on heterogeneous devices.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{hamman2024demystifying}
F.~Hamman and S.~Dutta.
\newblock Demystifying local \& global fairness trade-offs in federated learning using partial information decomposition.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{hao2021towards}
W.~Hao, M.~El-Khamy, J.~Lee, J.~Zhang, K.~J. Liang, C.~Chen, and L.~C. Duke.
\newblock Towards fair federated learning with zero-shot data augmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3310--3319, 2021.

\bibitem{NIPSAsimpleDPDataRelease}
M.~Hardt, K.~Ligett, and F.~Mcsherry.
\newblock A simple and practical algorithm for differentially private data release.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger, editors, {\em Advances in Neural Information Processing Systems}, volume~25. Curran Associates, Inc., 2012.

\bibitem{5670948}
M.~Hardt and G.~N. Rothblum.
\newblock A multiplicative weights mechanism for privacy-preserving data analysis.
\newblock In {\em 2010 IEEE 51st Annual Symposium on Foundations of Computer Science}, pages 61--70, 2010.

\bibitem{FedGKT2020}
C.~He, M.~Annavaram, and S.~Avestimehr.
\newblock Group knowledge transfer: Federated learning of large cnns at the edge.
\newblock In {\em Advances in Neural Information Processing Systems 34}, 2020.

\bibitem{chaoyanghe2020fedml}
C.~He, S.~Li, J.~So, M.~Zhang, H.~Wang, X.~Wang, P.~Vepakomma, A.~Singh, H.~Qiu, L.~Shen, P.~Zhao, Y.~Kang, Y.~Liu, R.~Raskar, Q.~Yang, M.~Annavaram, and S.~Avestimehr.
\newblock Fedml: A research library and benchmark for federated machine learning.
\newblock {\em arXiv preprint arXiv:2007.13518}, 2020.

\bibitem{PipeTransformer}
C.~He, S.~Li, M.~Soltanolkotabi, and S.~Avestimehr.
\newblock Pipetransformer: Automated elastic pipelining for distributed training of large-scale models.
\newblock In {\em Proceedings of the 38th International Conference on Machine Learning}, pages 4150--4159. PMLR, 18--24 Jul 2021.

\bibitem{he2021fedcv}
C.~He, A.~D. Shah, Z.~Tang, D.~F.~N. Sivashunmugam, K.~Bhogaraju, M.~Shimpi, L.~Shen, X.~Chu, M.~Soltanolkotabi, and S.~Avestimehr.
\newblock Fedcv: A federated learning framework for diverse computer vision tasks.
\newblock {\em arXiv preprint arXiv:2111.11066}, 2021.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, 2020.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{heinbaugh2023datafree}
C.~E. Heinbaugh, E.~Luz-Ricca, and H.~Shao.
\newblock Data-free one-shot federated learning under very high statistical heterogeneity.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{hendrycks2021natural}
D.~Hendrycks, K.~Zhao, S.~Basart, J.~Steinhardt, and D.~Song.
\newblock Natural adversarial examples.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15262--15271, 2021.

\bibitem{hjelm2018learning}
R.~D. Hjelm, A.~Fedorov, S.~Lavoie-Marchildon, K.~Grewal, P.~Bachman, A.~Trischler, and Y.~Bengio.
\newblock Learning deep representations by mutual information estimation and maximization.
\newblock In {\em ICLR}, 2019.

\bibitem{ho2020contrastive}
C.-H. Ho and N.~Nvasconcelos.
\newblock Contrastive learning with adversarial examples.
\newblock {\em Advances in Neural Information Processing Systems}, 33:17081--17093, 2020.

\bibitem{LDA}
T.~Hsu, H.~Qi, and M.~Brown.
\newblock Measuring the effects of non-identical data distribution for federated visual classification.
\newblock {\em ArXiv}, abs/1909.06335, 2019.

\bibitem{realgpd}
T.-M.~H. Hsu, H.~Qi, and M.~Brown.
\newblock Federated visual classification with real-world data distribution.
\newblock In A.~Vedaldi, H.~Bischof, T.~Brox, and J.-M. Frahm, editors, {\em Computer Vision -- ECCV 2020}, pages 76--92, Cham, 2020. Springer International Publishing.

\bibitem{SixuNeuronMatch}
S.~Hu, Q.~Li, and B.~He.
\newblock Communication-efficient generalized neuron matching for federated learning.
\newblock In {\em Proceedings of the 52nd International Conference on Parallel Processing}, ICPP '23, page 254–263, New York, NY, USA, 2023. Association for Computing Machinery.

\bibitem{huang2024lorahub}
C.~Huang, Q.~Liu, B.~Y. Lin, C.~Du, T.~Pang, and M.~Lin.
\newblock Lorahub: Efficient cross-task generalization via dynamic lo{RA} composition, 2024.

\bibitem{10203389}
W.~Huang, M.~Ye, Z.~Shi, H.~Li, and B.~Du.
\newblock Rethinking federated learning with domain shift: A prototype view.
\newblock In {\em 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.

\bibitem{izmailov2022feature}
P.~Izmailov, P.~Kirichenko, N.~Gruver, and A.~G. Wilson.
\newblock On feature learning in the presence of spurious correlations.
\newblock {\em Advances in Neural Information Processing Systems}, 35:38516--38532, 2022.

\bibitem{jeong2018communication}
E.~Jeong, S.~Oh, H.~Kim, J.~Park, M.~Bennis, and S.-L. Kim.
\newblock Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data.
\newblock {\em arXiv preprint arXiv:1811.11479}, 2018.

\bibitem{jhunjhunwala2023towards}
D.~Jhunjhunwala, S.~Wang, and G.~Joshi.
\newblock Towards a theoretical and practical understanding of one-shot federated learning with fisher information.
\newblock In {\em Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities}, 2023.

\bibitem{jiang2022model}
Y.~Jiang, S.~Wang, V.~Valls, B.~J. Ko, W.-H. Lee, K.~K. Leung, and L.~Tassiulas.
\newblock Model pruning enables efficient federated learning on edge devices.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 2022.

\bibitem{jin2023dataless}
X.~Jin, X.~Ren, D.~Preotiuc-Pietro, and P.~Cheng.
\newblock Dataless knowledge fusion by merging weights of language models.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{johnson2018towards}
N.~Johnson, J.~P. Near, and D.~Song.
\newblock Towards practical differential privacy for sql queries.
\newblock {\em Proceedings of the VLDB Endowment}, 11(5):526--539, 2018.

\bibitem{kairouz2021advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji, K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, R.~G.~L. D'Oliveira, H.~Eichner, S.~E. Rouayheb, D.~Evans, J.~Gardner, Z.~Garrett, A.~Gascón, B.~Ghazi, P.~B. Gibbons, M.~Gruteser, Z.~Harchaoui, C.~He, L.~He, Z.~Huo, B.~Hutchinson, J.~Hsu, M.~Jaggi, T.~Javidi, G.~Joshi, M.~Khodak, J.~Konečný, A.~Korolova, F.~Koushanfar, S.~Koyejo, T.~Lepoint, Y.~Liu, P.~Mittal, M.~Mohri, R.~Nock, A.~Özgür, R.~Pagh, M.~Raykova, H.~Qi, D.~Ramage, R.~Raskar, D.~Song, W.~Song, S.~U. Stich, Z.~Sun, A.~T. Suresh, F.~Tramèr, P.~Vepakomma, J.~Wang, L.~Xiong, Z.~Xu, Q.~Yang, F.~X. Yu, H.~Yu, and S.~Zhao.
\newblock Advances and open problems in federated learning, 2021.

\bibitem{karimireddy2019scaffold}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and A.~T. Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock {\em arXiv preprint arXiv:1910.06378}, 2019.

\bibitem{Kim_2023_ICCV}
H.~Kim, Y.~Kwak, M.~Jung, J.~Shin, Y.~Kim, and C.~Kim.
\newblock Protofl: Unsupervised federated learning via prototypical distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, October 2023.

\bibitem{kirichenko2023last}
P.~Kirichenko, P.~Izmailov, and A.~G. Wilson.
\newblock Last layer re-training is sufficient for robustness to spurious correlations.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{PNASforget}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A. Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, D.~Hassabis, C.~Clopath, D.~Kumaran, and R.~Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 2017.

\bibitem{kone2016}
J.~{Kone{\v{c}}n{\'y}}, H.~{Brendan McMahan}, F.~X. {Yu}, P.~{Richt{\'a}rik}, A.~{Theertha Suresh}, and D.~{Bacon}.
\newblock {Federated Learning: Strategies for Improving Communication Efficiency}.
\newblock {\em arXiv e-prints}, 2016.

\bibitem{Krizhevsky09learningmultiple}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis}, 2009.

\bibitem{krizhevsky2010cifar}
A.~Krizhevsky, V.~Nair, and G.~Hinton.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock {\em URL http://www.cs.toronto.edu/kriz/cifar.html}, 2010.

\bibitem{kumar2022finetuning}
A.~Kumar, A.~Raghunathan, R.~M. Jones, T.~Ma, and P.~Liang.
\newblock Fine-tuning can distort pretrained features and underperform out-of-distribution.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{le2015tiny}
Y.~Le and X.~Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 7(7):3, 2015.

\bibitem{726791}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11), 1998.

\bibitem{9708944}
A.~Li, J.~Sun, B.~Wang, L.~Duan, S.~Li, Y.~Chen, and H.~Li.
\newblock Lotteryfl: Empower edge intelligence with personalized and communication-efficient federated learning.
\newblock In {\em 2021 IEEE/ACM Symposium on Edge Computing (SEC)}, pages 68--79, 2021.

\bibitem{li2019fedmd}
D.~Li and J.~Wang.
\newblock Fedmd: Heterogenous federated learning via model distillation.
\newblock {\em arXiv preprint arXiv:1910.03581}, 2019.

\bibitem{li2021federatedstudy}
Q.~Li, Y.~Diao, Q.~Chen, and B.~He.
\newblock Federated learning on non-iid data silos: An experimental study, 2021.

\bibitem{li2020practical}
Q.~Li, B.~He, and D.~Song.
\newblock Practical one-shot federated learning for cross-silo setting.
\newblock {\em arXiv preprint arXiv:2010.01017}, 2020.

\bibitem{li2021model}
Q.~Li, B.~He, and D.~Song.
\newblock Model-contrastive federated learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10713--10722, 2021.

\bibitem{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2024reflective}
Y.~Li, B.~Luo, Q.~Wang, N.~Chen, X.~Liu, and B.~He.
\newblock A reflective llm-based agent to guide zero-shot cryptocurrency trading.
\newblock {\em arXiv preprint arXiv:2407.09546}, 2024.

\bibitem{Li_2023_ICCV}
Z.~Li, X.~Shang, R.~He, T.~Lin, and C.~Wu.
\newblock No fear of classifier biases: Neural collapse inspired federated learning with synthetic and fixed classifier.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 5319--5329, October 2023.

\bibitem{liang2020think}
P.~P. Liang, T.~Liu, L.~Ziyin, N.~B. Allen, R.~P. Auerbach, D.~Brent, R.~Salakhutdinov, and L.-P. Morency.
\newblock Think locally, act globally: Federated learning with local and global representations.
\newblock {\em arXiv preprint arXiv:2001.01523}, 2020.

\bibitem{FedDF}
T.~Lin, L.~Kong, S.~U. Stich, and M.~Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{liu2021towards}
J.~Liu, Z.~Shen, Y.~He, X.~Zhang, R.~Xu, H.~Yu, and P.~Cui.
\newblock Towards out-of-distribution generalization: A survey.
\newblock {\em arXiv preprint arXiv:2108.13624}, 2021.

\bibitem{liu2023bayesian}
L.~Liu, X.~Jiang, F.~Zheng, H.~Chen, G.-J. Qi, H.~Huang, and L.~Shao.
\newblock A bayesian federated learning framework with online laplace approximation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2023.

\bibitem{long2021g}
Y.~Long, B.~Wang, Z.~Yang, B.~Kailkhura, A.~Zhang, C.~Gunter, and B.~Li.
\newblock G-pate: Scalable differentially private data generator via private aggregation of teacher discriminators.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{luo2023ai}
B.~Luo, Z.~Zhang, Q.~Wang, A.~Ke, S.~Lu, and B.~He.
\newblock Ai-powered fraud detection in decentralized finance: A project life cycle perspective.
\newblock {\em arXiv preprint arXiv:2308.15992}, 2023.

\bibitem{luo2021no}
M.~Luo, F.~Chen, D.~Hu, Y.~Zhang, J.~Liang, and J.~Feng.
\newblock No fear of heterogeneity: Classifier calibration for federated learning with non-{IID} data.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{DFL}
Z.~Luo, Y.~Wang, Z.~Wang, Z.~Sun, and T.~Tan.
\newblock Disentangled federated learning for tackling attributes skew via invariant aggregation and diversity transferring.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and S.~Sabato, editors, {\em Proceedings of the 39th International Conference on Machine Learning}, volume 162 of {\em Proceedings of Machine Learning Research}, pages 14527--14541. PMLR, 17--23 Jul 2022.

\bibitem{matena2022merging}
M.~S. Matena and C.~A. Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock {\em Advances in Neural Information Processing Systems}, 35:17703--17716, 2022.

\bibitem{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282, 2017.

\bibitem{melis2019exploiting}
L.~Melis, C.~Song, E.~De~Cristofaro, and V.~Shmatikov.
\newblock Exploiting unintended feature leakage in collaborative learning.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages 691--706. IEEE, 2019.

\bibitem{mugunthan2022fedltn}
V.~Mugunthan, E.~Lin, V.~Gokul, C.~Lau, L.~Kagal, and S.~Pieper.
\newblock Fedltn: Federated learning for sparse and personalized lottery ticket networks.
\newblock In {\em European Conference on Computer Vision}, pages 69--85. Springer, 2022.

\bibitem{naseer2020self}
M.~Naseer, S.~Khan, M.~Hayat, F.~S. Khan, and F.~Porikli.
\newblock A self-supervised approach for adversarial robustness.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 262--271, 2020.

\bibitem{SVHN}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}, 2011.

\bibitem{NIPS2017_10ce03a1}
B.~Neyshabur, S.~Bhojanapalli, D.~Mcallester, and N.~Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{neyshabur2020being}
B.~Neyshabur, H.~Sedghi, and C.~Zhang.
\newblock What is being transferred in transfer learning?
\newblock {\em Advances in neural information processing systems}, 2020.

\bibitem{nguyen2022begin}
J.~Nguyen, K.~Malik, M.~Sanjabi, and M.~Rabbat.
\newblock Where to begin? exploring the impact of pre-training and initialization in federated learning.
\newblock {\em arXiv preprint arXiv:2206.15387}, 2022.

\bibitem{nguyen2024towards}
N.-H. Nguyen, T.-A. Nguyen, T.~Nguyen, V.~T. Hoang, D.~D. Le, and K.-S. Wong.
\newblock Towards efficient communication federated recommendation system via low-rank training.
\newblock {\em arXiv preprint arXiv:2401.03748}, 2024.

\bibitem{oord2018representation}
A.~v.~d. Oord, Y.~Li, and O.~Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{pagliardini2023agree}
M.~Pagliardini, M.~Jaggi, F.~Fleuret, and S.~P. Karimireddy.
\newblock Agree to disagree: Diversity through disagreement for better transferability.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{papernot2018deep}
N.~Papernot and P.~McDaniel.
\newblock Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning.
\newblock {\em arXiv preprint arXiv:1803.04765}, 2018.

\bibitem{park2021few}
Y.~Park, D.-J. Han, D.-Y. Kim, J.~Seo, and J.~Moon.
\newblock Few-round learning for federated learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34:28612--28622, 2021.

\bibitem{pearl2009causalityResnet}
J.~Pearl.
\newblock {\em Causality}.
\newblock Cambridge university press, 2009.

\bibitem{peters2017elements}
J.~Peters, D.~Janzing, and B.~Sch{\"o}lkopf.
\newblock {\em Elements of causal inference: foundations and learning algorithms}.
\newblock The MIT Press, 2017.

\bibitem{qiu2022zerofl}
X.~Qiu, J.~Fernandez-Marques, P.~P. Gusmao, Y.~Gao, T.~Parcollet, and N.~D. Lane.
\newblock Zero{FL}: Efficient on-device training for federated learning with local sparsity.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{qu2022generalized}
Z.~Qu, X.~Li, R.~Duan, Y.~Liu, B.~Tang, and Z.~Lu.
\newblock Generalized federated learning via sharpness aware minimization.
\newblock In {\em International Conference on Machine Learning}, pages 18250--18280. PMLR, 2022.

\bibitem{SVCCA}
M.~Raghu, J.~Gilmer, J.~Yosinski, and J.~Sohl-Dickstein.
\newblock Svcca: singular vector canonical correlation analysis for deep learning dynamics and interpretability.
\newblock In {\em Proceedings of the 31st International Conference on Neural Information Processing Systems}, NIPS'17, 2017.

\bibitem{reddi2020adaptive}
S.~J. Reddi, Z.~Charles, M.~Zaheer, Z.~Garrett, K.~Rush, J.~Kone{\v{c}}n{\'y}, S.~Kumar, and H.~B. McMahan.
\newblock Adaptive federated optimization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{reisizadeh2020fedpaq}
A.~Reisizadeh, A.~Mokhtari, H.~Hassani, A.~Jadbabaie, and R.~Pedarsani.
\newblock Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 2021--2031. PMLR, 2020.

\bibitem{sagawa2020investigation}
S.~Sagawa, A.~Raghunathan, P.~W. Koh, and P.~Liang.
\newblock An investigation of why overparameterization exacerbates spurious correlations.
\newblock In {\em International Conference on Machine Learning}, pages 8346--8356. PMLR, 2020.

\bibitem{fedprox}
A.~K. Sahu, T.~Li, M.~Sanjabi, M.~Zaheer, A.~Talwalkar, and V.~Smith.
\newblock On the convergence of federated optimization in heterogeneous networks.
\newblock {\em ArXiv}, abs/1812.06127, 2018.

\bibitem{saxe2019information}
A.~M. Saxe, Y.~Bansal, J.~Dapello, M.~Advani, A.~Kolchinsky, B.~D. Tracey, and D.~D. Cox.
\newblock On the information bottleneck theory of deep learning.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment}, 2019(12):124020, 2019.

\bibitem{scholkopf2021toward}
B.~Sch{\"o}lkopf, F.~Locatello, S.~Bauer, N.~R. Ke, N.~Kalchbrenner, A.~Goyal, and Y.~Bengio.
\newblock Toward causal representation learning.
\newblock {\em Proceedings of the IEEE}, 109(5):612--634, 2021.

\bibitem{margin5}
V.~Sehwag, S.~Mahloujifar, T.~Handina, S.~Dai, C.~Xiang, M.~Chiang, and P.~Mittal.
\newblock Improving adversarial robustness using proxy distributions.
\newblock {\em arXiv preprint arXiv:2104.09425}, 2021.

\bibitem{shin2020xor}
M.~Shin, C.~Hwang, J.~Kim, J.~Park, M.~Bennis, and S.-L. Kim.
\newblock Xor mixup: Privacy-preserving data augmentation for one-shot federated learning.
\newblock {\em arXiv preprint arXiv:2006.05148}, 2020.

\bibitem{shwartz2017opening}
R.~Shwartz-Ziv and N.~Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock {\em arXiv preprint arXiv:1703.00810}, 2017.

\bibitem{Dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em J. Mach. Learn. Res.}, 15(1):1929–1958, jan 2014.

\bibitem{pmlr-v202-sun23h}
Y.~Sun, L.~Shen, S.~Chen, L.~Ding, and D.~Tao.
\newblock Dynamic regularized sharpness aware minimization in federated learning: Approaching global consistency and smooth landscape.
\newblock In {\em Proceedings of the 40th International Conference on Machine Learning}, 2023.

\bibitem{sun2023fedspeed}
Y.~Sun, L.~Shen, T.~Huang, L.~Ding, and D.~Tao.
\newblock Fedspeed: Larger local interval, less communication round, and higher generalization accuracy.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{9743558}
A.~Z. Tan, H.~Yu, L.~Cui, and Q.~Yang.
\newblock Towards personalized federated learning.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, pages 1--17, 2022.

\bibitem{tang2023fedml}
Z.~Tang, X.~Chu, R.~Y. Ran, S.~Lee, S.~Shi, Y.~Zhang, Y.~Wang, A.~Q. Liang, S.~Avestimehr, and C.~He.
\newblock Fedml parrot: A scalable federated learning system via heterogeneity-aware scheduling on sequential and hierarchical training.
\newblock {\em arXiv preprint arXiv:2303.01778}, 2023.

\bibitem{BCRS-OPWA}
Z.~Tang, J.~Huang, R.~Yan, Y.~Wang, Z.~Tang, S.~Shi, A.~C. Zhou, and X.~Chu.
\newblock Bandwidth-aware and overlap-weighted compression for communication-efficient federated learning.
\newblock In {\em 53rd International Conference on Parallel Processing}, Gotland, Sweden, 12--15 August 2024.

\bibitem{tang2024fusionllmdecentralizedllmtraining}
Z.~Tang, X.~Kang, Y.~Yin, X.~Pan, Y.~Wang, X.~He, Q.~Wang, R.~Zeng, K.~Zhao, S.~Shi, A.~C. Zhou, B.~Li, B.~He, and X.~Chu.
\newblock Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression, 2024.

\bibitem{tang2020communication}
Z.~Tang, S.~Shi, and X.~Chu.
\newblock Communication-efficient decentralized learning with sparsification and adaptive peer selection.
\newblock In {\em 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)}, pages 1207--1208. IEEE, 2020.

\bibitem{tang2020survey}
Z.~Tang, S.~Shi, X.~Chu, W.~Wang, and B.~Li.
\newblock Communication-efficient distributed deep learning: A comprehensive survey.
\newblock {\em arXiv preprint arXiv:2003.06307}, 2020.

\bibitem{GossipFL}
Z.~Tang, S.~Shi, B.~Li, and X.~Chu.
\newblock Gossipfl: A decentralized federated learning framework with sparsified and adaptive communication.
\newblock {\em IEEE Transactions on Parallel and Distributed Systems}, pages 1--13, 2022.

\bibitem{tang2023fusionai}
Z.~Tang, Y.~Wang, X.~He, L.~Zhang, X.~Pan, Q.~Wang, R.~Zeng, K.~Zhao, S.~Shi, B.~He, et~al.
\newblock Fusionai: Decentralized training and deploying llms with massive consumer-level gpus.
\newblock {\em The 32nd International Joint Conference on Artificial Intelligence, Symposium on Large Language Models (LLM@IJCAI 2023)}, 2023.

\bibitem{VHL}
Z.~Tang, Y.~Zhang, S.~Shi, X.~He, B.~Han, and X.~Chu.
\newblock Virtual homogeneity learning: Defending against data heterogeneity in federated learning.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and S.~Sabato, editors, {\em Proceedings of the 39th International Conference on Machine Learning}, volume 162 of {\em Proceedings of Machine Learning Research}, pages 21111--21132. PMLR, 17--23 Jul 2022.

\bibitem{tang2024fedimpro}
Z.~Tang, Y.~Zhang, S.~Shi, X.~Tian, T.~Liu, B.~Han, and X.~Chu.
\newblock Fedimpro: Measuring and improving client update in federated learning.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{thapa2020splitfed}
C.~Thapa, M.~A.~P. Chamikara, S.~Camtepe, and L.~Sun.
\newblock Splitfed: When federated learning meets split learning.
\newblock {\em arXiv preprint arXiv:2004.12088}, 2020.

\bibitem{tian2020makes}
Y.~Tian, C.~Sun, B.~Poole, D.~Krishnan, C.~Schmid, and P.~Isola.
\newblock What makes for good views for contrastive learning.
\newblock {\em arXiv preprint arXiv:2005.10243}, 2020.

\bibitem{tishby99information}
N.~Tishby, F.~C. Pereira, and W.~Bialek.
\newblock The information bottleneck method.
\newblock In {\em Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing}, pages 368--377, 1999.

\bibitem{utrera2020adversarially}
F.~Utrera, E.~Kravitz, N.~B. Erichson, R.~Khanna, and M.~W. Mahoney.
\newblock Adversarially-trained deep nets transfer better: Illustration on image classification.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{vivek2020single}
B.~Vivek and R.~V. Babu.
\newblock Single-step adversarial training with dropout scheduling.
\newblock In {\em 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 947--956. IEEE, 2020.

\bibitem{wang2020Federated}
H.~Wang, M.~Yurochkin, Y.~Sun, D.~Papailiopoulos, and Y.~Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wang2020tackling}
J.~Wang, Q.~Liu, H.~Liang, G.~Joshi, and H.~V. Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock In {\em arXiv preprint arXiv:2007.07481}, 2020.

\bibitem{wang2023datafree}
N.~Wang, W.~Feng, yuchen deng, M.~Duan, F.~Liu, and S.-K. Ng.
\newblock Data-free diversity-based ensemble selection for one-shot federated learning.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{wangex}
Q.~Wang, Z.~Zhang, Z.~Liu, S.~Lu, B.~Luo, and B.~He.
\newblock Ex-graph: A pioneering dataset bridging ethereum and x.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{wang2020revisiting}
Y.~Wang, Z.~Ni, S.~Song, L.~Yang, and G.~Huang.
\newblock Revisiting locally supervised learning: an alternative to end-to-end training.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{woodworth2020minibatch}
B.~E. Woodworth, K.~K. Patel, and N.~Srebro.
\newblock Minibatch vs local sgd for heterogeneous distributed learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:6281--6292, 2020.

\bibitem{xiao2017fashion}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{yadav2023tiesmerging}
P.~Yadav, D.~Tam, L.~Choshen, C.~Raffel, and M.~Bansal.
\newblock {TIES}-merging: Resolving interference when merging models.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{kaiwen}
K.~Yang, T.~Zhou, Y.~Zhang, X.~Tian, and D.~Tao.
\newblock Class-disentanglement and applications in adversarial detection and defense.
\newblock {\em In NeurIPS}, 2021.

\bibitem{yang2024fedfed}
Z.~Yang, Y.~Zhang, Y.~Zheng, X.~Tian, H.~Peng, T.~Liu, and B.~Han.
\newblock Fedfed: Feature distillation against data heterogeneity in federated learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{ye2021towards}
H.~Ye, C.~Xie, T.~Cai, R.~Li, Z.~Li, and L.~Wang.
\newblock Towards a theoretical framework of out-of-distribution generalization.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{OODgeneralization}
M.~Yi, L.~Hou, J.~Sun, L.~Shang, X.~Jiang, Q.~Liu, and Z.~Ma.
\newblock Improved ood generalization via adversarial training and pretraing.
\newblock In M.~Meila and T.~Zhang, editors, {\em Proceedings of the 38th International Conference on Machine Learning}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 11987--11997. PMLR, 18--24 Jul 2021.

\bibitem{yin2020dreaming}
H.~Yin, P.~Molchanov, J.~M. Alvarez, Z.~Li, A.~Mallya, D.~Hoiem, N.~K. Jha, and J.~Kautz.
\newblock Dreaming to distill: Data-free knowledge transfer via deepinversion.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8715--8724, 2020.

\bibitem{yoon2021fedmix}
T.~Yoon, S.~Shin, S.~J. Hwang, and E.~Yang.
\newblock Fedmix: Approximation of mixup under mean augmented federated learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{yu2020federated}
F.~Yu, A.~S. Rawat, A.~Menon, and S.~Kumar.
\newblock Federated learning with only positive labels.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{yuandecentralized}
B.~Yuan, Y.~He, J.~Q. Davis, T.~Zhang, T.~Dao, B.~Chen, P.~Liang, C.~Re, and C.~Zhang.
\newblock Decentralized training of foundation models in heterogeneous environments.
\newblock In {\em NeurIPS}, 2022.

\bibitem{yuan2022what}
H.~Yuan, W.~R. Morningstar, L.~Ning, and K.~Singhal.
\newblock What do we mean by generalization in federated learning?
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{zhang2021can}
D.~Zhang, K.~Ahuja, Y.~Xu, Y.~Wang, and A.~Courville.
\newblock Can subnetwork structure be the key to out-of-distribution generalization?
\newblock In {\em International Conference on Machine Learning}, pages 12356--12367. PMLR, 2021.

\bibitem{zhang2022dense}
J.~Zhang, C.~Chen, B.~Li, L.~Lyu, S.~Wu, S.~Ding, C.~Shen, and C.~Wu.
\newblock Dense: Data-free one-shot federated learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem{pmlr-v162-zhang22p}
J.~Zhang, Z.~Li, B.~Li, J.~Xu, S.~Wu, S.~Ding, and C.~Wu.
\newblock Federated learning with label distribution skew via logits calibration.
\newblock In {\em Proceedings of the 39th International Conference on Machine Learning}. PMLR, 2022.

\bibitem{zhang2021adversarial}
Y.~Zhang, M.~Gong, T.~Liu, G.~Niu, X.~Tian, B.~Han, B.~Sch{\"o}lkopf, and K.~Zhang.
\newblock Adversarial robustness through the lens of causality.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhangrobust}
Y.~Zhang, Z.~Yang, X.~Tian, N.~Wang, T.~Liu, and B.~Han.
\newblock Robust training of federated models with extremely label deficiency.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{zhao2018federated}
Y.~Zhao, M.~Li, L.~Lai, N.~Suda, D.~Civin, and V.~Chandra.
\newblock Federated learning with non-iid data.
\newblock {\em arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{zhou2020distilled}
Y.~Zhou, G.~Pu, X.~Ma, X.~Li, and D.~Wu.
\newblock Distilled one-shot federated learning.
\newblock {\em arXiv preprint arXiv:2009.07999}, 2020.

\end{thebibliography}
