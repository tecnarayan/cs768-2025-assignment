\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bafna et~al.(2018)Bafna, Murtagh, and Vyas]{bafna2018thwarting}
Bafna, M., Murtagh, J., and Vyas, N.
\newblock Thwarting adversarial examples: An $\ell_0$-robust sparse fourier
  transform.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10096--10106, 2018.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Biggio, B., Corona, I., Maiorca, D., Nelson, B., {\v{S}}rndi{\'c}, N., Laskov,
  P., Giacinto, G., and Roli, F.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  387--402. Springer, 2013.

\bibitem[Co et~al.(2018)Co, Mu{\~n}oz-Gonz{\'a}lez, de~Maupeou, and
  Lupu]{co2018procedural}
Co, K.~T., Mu{\~n}oz-Gonz{\'a}lez, L., de~Maupeou, S., and Lupu, E.~C.
\newblock Procedural noise adversarial examples for black-box attacks on deep
  convolutional networks.
\newblock \emph{arXiv preprint arXiv:1810.00470}, 2018.

\bibitem[Engstrom et~al.(2019{\natexlab{a}})Engstrom, Ilyas, Santurkar,
  Tsipras, Tran, and Madry]{engstrom2019adversarial}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A.
\newblock Adversarial robustness as a prior for learned representations,
  2019{\natexlab{a}}.

\bibitem[Engstrom et~al.(2019{\natexlab{b}})Engstrom, Tran, Tsipras, Schmidt,
  and Madry]{engstrom2019exploring}
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., and Madry, A.
\newblock Exploring the landscape of spatial robustness.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1802--1811, 2019{\natexlab{b}}.

\bibitem[Gilmer et~al.(2018)Gilmer, Adams, Goodfellow, Andersen, and
  Dahl]{GilmerMotivating2018}
Gilmer, J., Adams, R.~P., Goodfellow, I., Andersen, D., and Dahl, G.~E.
\newblock Motivating the rules of the game for adversarial example research.
\newblock \emph{arXiv preprint arXiv:1807.06732}, 2018.

\bibitem[Goodfellow \& Papernot(2017)Goodfellow and
  Papernot]{goodfellow2017attacking}
Goodfellow, I. and Papernot, N.
\newblock Is attacking machine learning easier than defending it?
\newblock \emph{Blog post on Feb}, 15:\penalty0 2017, 2017.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{harnessing_adversarial}
Goodfellow, I., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Ilyas et~al.(2017)Ilyas, Jalal, Asteri, Daskalakis, and
  Dimakis]{ilyas2017robust}
Ilyas, A., Jalal, A., Asteri, E., Daskalakis, C., and Dimakis, A.~G.
\newblock The robust manifold defense: Adversarial training using generative
  models.
\newblock \emph{arXiv preprint arXiv:1712.09196}, 2017.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  125--136, 2019.

\bibitem[Jacobsen et~al.(2019)Jacobsen, Behrmann, Zemel, and
  Bethge]{jacobsen2018excessive}
Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M.
\newblock Excessive invariance causes adversarial vulnerability.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Jo \& Bengio(2017)Jo and Bengio]{jo2017measuring}
Jo, J. and Bengio, Y.
\newblock Measuring the tendency of cnns to learn surface statistical
  regularities.
\newblock \emph{arXiv preprint arXiv:1711.11561}, 2017.

\bibitem[Kang et~al.(2019)Kang, Sun, Hendrycks, Brown, and
  Steinhardt]{kang2019testing}
Kang, D., Sun, Y., Hendrycks, D., Brown, T., and Steinhardt, J.
\newblock Testing robustness against unforeseen adversaries.
\newblock \emph{arXiv preprint arXiv:1908.08016}, 2019.

\bibitem[Kaushik et~al.(2019)Kaushik, Hovy, and Lipton]{kaushik2019learning}
Kaushik, D., Hovy, E., and Lipton, Z.~C.
\newblock Learning the difference that makes a difference with
  counterfactually-augmented data.
\newblock \emph{arXiv preprint arXiv:1909.12434}, 2019.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Mirza \& Osindero(2014)Mirza and Osindero]{mirza2014conditional}
Mirza, M. and Osindero, S.
\newblock Conditional generative adversarial nets.
\newblock \emph{arXiv preprint arXiv:1411.1784}, 2014.

\bibitem[Panda et~al.(2019)Panda, Chakraborty, and
  Roy]{panda2019discretization}
Panda, P., Chakraborty, I., and Roy, K.
\newblock Discretization based solutions for secure machine learning against
  adversarial attacks.
\newblock \emph{IEEE Access}, 7:\penalty0 70157--70168, 2019.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Raghunathan, A., Steinhardt, J., and Liang, P.
\newblock Certified defenses against adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sabour et~al.(2016)Sabour, Cao, Faghri, and
  Fleet]{sabour2015adversarial}
Sabour, S., Cao, Y., Faghri, F., and Fleet, D.~J.
\newblock Adversarial manipulation of deep representations.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Samangouei et~al.(2018)Samangouei, Kabkab, and
  Chellappa]{samangouei2018defense}
Samangouei, P., Kabkab, M., and Chellappa, R.
\newblock Defense-gan: Protecting classifiers against adversarial attacks using
  generative models.
\newblock \emph{arXiv preprint arXiv:1805.06605}, 2018.

\bibitem[Schott et~al.(2019)Schott, Rauber, Bethge, and
  Brendel]{schott2018towards}
Schott, L., Rauber, J., Bethge, M., and Brendel, W.
\newblock Towards the first adversarially robust neural network model on
  {MNIST}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Shaeiri et~al.(2020)Shaeiri, Nobahari, and Rohban]{shaeiri2020towards}
Shaeiri, A., Nobahari, R., and Rohban, M.~H.
\newblock Towards deep learning models resistant to large perturbations.
\newblock \emph{arXiv preprint arXiv:2003.13370}, 2020.

\bibitem[Sharif et~al.(2018)Sharif, Bauer, and Reiter]{sharif2018suitability}
Sharif, M., Bauer, L., and Reiter, M.~K.
\newblock On the suitability of lp-norms for creating and preventing
  adversarial examples.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pp.\  1605--1613, 2018.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Tram{\`e}r \& Boneh(2019)Tram{\`e}r and Boneh]{tramer2019adversarial}
Tram{\`e}r, F. and Boneh, D.
\newblock Adversarial training and robustness for multiple perturbations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5858--5868, 2019.

\bibitem[Tram{\`e}r et~al.(2019)Tram{\`e}r, Dupr{\'e}, Rusak, Pellegrino, and
  Boneh]{tramer2019ads}
Tram{\`e}r, F., Dupr{\'e}, P., Rusak, G., Pellegrino, G., and Boneh, D.
\newblock Adversarial: Perceptual ad blocking meets adversarial machine
  learning.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, pp.\  2005--2021, 2019.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wong \& Kolter(2018)Wong and Kolter]{kolter2017provable}
Wong, E. and Kolter, Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Yin et~al.(2019)Yin, Lopes, Shlens, Cubuk, and Gilmer]{yin2019fourier}
Yin, D., Lopes, R.~G., Shlens, J., Cubuk, E.~D., and Gilmer, J.
\newblock A fourier perspective on model robustness in computer vision.
\newblock \emph{arXiv preprint arXiv:1906.08988}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Chen, Xiao, Li, Boning, and
  Hsieh]{zhang2019towards}
Zhang, H., Chen, H., Xiao, C., Li, B., Boning, D., and Hsieh, C.-J.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock \emph{arXiv preprint arXiv:1906.06316}, 2019.

\end{thebibliography}
