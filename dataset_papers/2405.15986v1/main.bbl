\begin{thebibliography}{103}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Shih et~al.(2024)Shih, Belkhale, Ermon, Sadigh, and Anari]{shih2024parallel}
Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari.
\newblock Parallel sampling of diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Albergo et~al.(2023{\natexlab{a}})Albergo, Boffi, and Vanden-Eijnden]{albergo2023stochastic}
Michael~S Albergo, Nicholas~M Boffi, and Eric Vanden-Eijnden.
\newblock Stochastic interpolants: A unifying framework for flows and diffusions.
\newblock \emph{arXiv preprint arXiv:2303.08797}, 2023{\natexlab{a}}.

\bibitem[Albergo and Vanden-Eijnden(2022)]{albergo2022building}
Michael~S Albergo and Eric Vanden-Eijnden.
\newblock Building normalizing flows with stochastic interpolants.
\newblock \emph{arXiv preprint arXiv:2209.15571}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Lipman et~al.(2022)Lipman, Chen, Ben-Hamu, Nickel, and Le]{lipman2022flow}
Yaron Lipman, Ricky~TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
\newblock Flow matching for generative modeling.
\newblock \emph{arXiv preprint arXiv:2210.02747}, 2022.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pages 2256--2265. PMLR, 2015.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Durkan, Murray, and Ermon]{song2021maximum}
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 1415--1428, 2021{\natexlab{a}}.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Song et~al.(2020{\natexlab{a}})Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2011.13456}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2018)Zhang, E, and Wang]{zhang2018monge}
Linfeng Zhang, Weinan E, and Lei Wang.
\newblock Monge-amp\`{e}re flow for generative modeling.
\newblock \emph{arXiv preprint arXiv:1809.10188}, 2018.

\bibitem[Bar-Tal et~al.(2023)Bar-Tal, Yariv, Lipman, and Dekel]{bar2023multidiffusion}
Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
\newblock Multidiffusion: Fusing diffusion paths for controlled image generation.
\newblock In \emph{International Conference on Machine Learning}, pages 1737--1752. Pmlr, 2023.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Liu, Xie, and He]{chen2024deconstructing}
Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He.
\newblock Deconstructing denoising diffusion models for self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2401.14404}, 2024{\natexlab{a}}.

\bibitem[Ho et~al.(2022{\natexlab{a}})Ho, Saharia, Chan, Fleet, Norouzi, and Salimans]{ho2022cascaded}
Jonathan Ho, Chitwan Saharia, William Chan, David~J Fleet, Mohammad Norouzi, and Tim Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (47):\penalty0 1--33, 2022{\natexlab{a}}.

\bibitem[Ma et~al.(2024)Ma, Goldstein, Albergo, Boffi, Vanden-Eijnden, and Xie]{ma2024sit}
Nanye Ma, Mark Goldstein, Michael~S Albergo, Nicholas~M Boffi, Eric Vanden-Eijnden, and Saining Xie.
\newblock Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers.
\newblock \emph{arXiv preprint arXiv:2401.08740}, 2024.

\bibitem[Meng et~al.(2021)Meng, He, Song, Song, Wu, Zhu, and Ermon]{meng2021sdedit}
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.
\newblock Sdedit: Guided image synthesis and editing with stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2108.01073}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages 8821--8831. Pmlr, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 1\penalty0 (2):\penalty0 3, 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Shen, Xing, and Ermon]{song2021solving}
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon.
\newblock Solving inverse problems in medical imaging with score-based generative models.
\newblock \emph{arXiv preprint arXiv:2111.08005}, 2021{\natexlab{b}}.

\bibitem[Sun et~al.(2023)Sun, Wu, Chen, Feng, and Bouman]{sun2023provable}
Yu~Sun, Zihui Wu, Yifan Chen, Berthy~T Feng, and Katherine~L Bouman.
\newblock Provable probabilistic imaging using score-based generative priors.
\newblock \emph{arXiv preprint arXiv:2310.10835}, 2023.

\bibitem[Xu and Chi(2024)]{xu2024provably}
Xingyu Xu and Yuejie Chi.
\newblock Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction.
\newblock \emph{arXiv preprint arXiv:2403.17042}, 2024.

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and Van Den~Berg]{austin2021structured}
Jacob Austin, Daniel~D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den~Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17981--17993, 2021.

\bibitem[Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and Hashimoto]{li2022diffusion}
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy~S Liang, and Tatsunori~B Hashimoto.
\newblock Diffusion-lm improves controllable text generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 4328--4343, 2022.

\bibitem[Ho et~al.(2022{\natexlab{b}})Ho, Salimans, Gritsenko, Chan, Norouzi, and Fleet]{ho2022video}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David~J Fleet.
\newblock Video diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 8633--8646, 2022{\natexlab{b}}.

\bibitem[Huang et~al.(2024{\natexlab{a}})Huang, Ghatare, Liu, Hu, Zhang, Sastry, Gururani, Oore, and Yue]{huang2024symbolic}
Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli~S Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue.
\newblock Symbolic music generation with non-differentiable rule guided diffusion.
\newblock \emph{arXiv preprint arXiv:2402.14285}, 2024{\natexlab{a}}.

\bibitem[Mittal et~al.(2021)Mittal, Engel, Hawthorne, and Simon]{mittal2021symbolic}
Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon.
\newblock Symbolic music generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2103.16091}, 2021.

\bibitem[Schneider(2023)]{schneider2023archisound}
Flavio Schneider.
\newblock Archisound: Audio generation with diffusion.
\newblock \emph{arXiv preprint arXiv:2301.13267}, 2023.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Srivastava, and Mandt]{yang2023diffusion}
Ruihan Yang, Prakhar Srivastava, and Stephan Mandt.
\newblock Diffusion probabilistic modeling for video generation.
\newblock \emph{Entropy}, 25\penalty0 (10):\penalty0 1469, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Yuan, Huang, Ni, Ye, Chen, and Wang]{li2024diffusion}
Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo Chen, and Mengdi Wang.
\newblock Diffusion model for data-driven black-box optimization.
\newblock \emph{arXiv preprint arXiv:2403.13219}, 2024{\natexlab{a}}.

\bibitem[Xu et~al.(2024)Xu, Lee, Cheng, and Xie]{xu2024flow}
Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, and Yao Xie.
\newblock Flow-based distributionally robust optimization.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2024.

\bibitem[Alaoui et~al.(2023)Alaoui, Montanari, and Sellke]{alaoui2023sampling}
Ahmed~El Alaoui, Andrea Montanari, and Mark Sellke.
\newblock Sampling from mean-field gibbs measures via diffusion processes.
\newblock \emph{arXiv preprint arXiv:2310.08912}, 2023.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Kontonis, and Shah]{chen2024learning}
Sitan Chen, Vasilis Kontonis, and Kulin Shah.
\newblock Learning general gaussian mixtures with efficient score matching.
\newblock \emph{arXiv preprint arXiv:2404.18893}, 2024{\natexlab{b}}.

\bibitem[El~Alaoui et~al.(2022)El~Alaoui, Montanari, and Sellke]{el2022sampling}
Ahmed El~Alaoui, Andrea Montanari, and Mark Sellke.
\newblock Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization.
\newblock In \emph{2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)}, pages 323--334. IEEE, 2022.

\bibitem[Gatmiry et~al.(2024)Gatmiry, Kelner, and Lee]{gatmiry2024learning}
Khashayar Gatmiry, Jonathan Kelner, and Holden Lee.
\newblock Learning mixtures of gaussians using diffusion models.
\newblock \emph{arXiv preprint arXiv:2404.18869}, 2024.

\bibitem[He et~al.(2024)He, Rojas, and Tao]{he2024zeroth}
Ye~He, Kevin Rojas, and Molei Tao.
\newblock Zeroth-order sampling methods for non-log-concave distributions: Alleviating metastability by denoising diffusion.
\newblock \emph{arXiv preprint arXiv:2402.17886}, 2024.

\bibitem[Huang et~al.(2023)Huang, Dong, Yifan, Ma, and Zhang]{huang2023reverse}
Xunpeng Huang, Hanze Dong, HAO Yifan, Yian Ma, and Tong Zhang.
\newblock Reverse diffusion monte carlo.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Huang et~al.(2024{\natexlab{b}})Huang, Montanari, and Pham]{huang2024sampling}
Brice Huang, Andrea Montanari, and Huy~Tuan Pham.
\newblock Sampling from spherical spin glasses in total variation via algorithmic stochastic localization.
\newblock \emph{arXiv preprint arXiv:2404.15651}, 2024{\natexlab{b}}.

\bibitem[Mei and Wu(2023)]{mei2023deep}
Song Mei and Yuchen Wu.
\newblock Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models.
\newblock \emph{arXiv preprint arXiv:2309.11420}, 2023.

\bibitem[Montanari(2023)]{montanari2023sampling}
Andrea Montanari.
\newblock Sampling, diffusions, and stochastic localization.
\newblock \emph{arXiv preprint arXiv:2305.10690}, 2023.

\bibitem[Montanari and Wu(2023)]{montanari2023posterior}
Andrea Montanari and Yuchen Wu.
\newblock Posterior sampling from the spiked models via diffusion processes.
\newblock \emph{arXiv preprint arXiv:2304.11449}, 2023.

\bibitem[Boffi and Vanden-Eijnden(2023)]{boffi2023probability}
Nicholas~M Boffi and Eric Vanden-Eijnden.
\newblock Probability flow solution of the fokker--planck equation.
\newblock \emph{Machine Learning: Science and Technology}, 4\penalty0 (3):\penalty0 035012, 2023.

\bibitem[Huang and Wang(2024)]{huang2024score}
Yan Huang and Li~Wang.
\newblock A score-based particle method for homogeneous landau equation.
\newblock \emph{arXiv preprint arXiv:2405.05187}, 2024.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Hurault, and Solomon]{li2024self}
Lingxiao Li, Samuel Hurault, and Justin~M Solomon.
\newblock Self-consistent velocity matching of probability flows.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Lu et~al.(2024)Lu, Wu, and Xiang]{lu2024score}
Jianfeng Lu, Yue Wu, and Yang Xiang.
\newblock Score-based transport modeling for mean-field fokker-planck equations.
\newblock \emph{Journal of Computational Physics}, 503:\penalty0 112859, 2024.

\bibitem[Maoutsa et~al.(2020)Maoutsa, Reich, and Opper]{maoutsa2020interacting}
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper.
\newblock Interacting particle solutions of fokker--planck equations through gradient--log--density estimation.
\newblock \emph{Entropy}, 22\penalty0 (8):\penalty0 802, 2020.

\bibitem[Campbell et~al.(2024)Campbell, Yim, Barzilay, Rainforth, and Jaakkola]{campbell2024generative}
Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola.
\newblock Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design.
\newblock \emph{arXiv preprint arXiv:2402.04997}, 2024.

\bibitem[Cotler and Rezchikov(2023)]{cotler2023renormalizing}
Jordan Cotler and Semon Rezchikov.
\newblock Renormalizing diffusion models.
\newblock \emph{arXiv preprint arXiv:2308.12355}, 2023.

\bibitem[F{\"u}rrutter et~al.(2023)F{\"u}rrutter, Mu{\~n}oz-Gil, and Briegel]{furrutter2023quantum}
Florian F{\"u}rrutter, Gorka Mu{\~n}oz-Gil, and Hans~J Briegel.
\newblock Quantum circuit synthesis with diffusion models.
\newblock \emph{arXiv preprint arXiv:2311.02041}, 2023.

\bibitem[Guo et~al.(2024)Guo, Liu, Wang, Chen, Wang, Xu, and Cheng]{guo2024diffusion}
Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, and Jianlin Cheng.
\newblock Diffusion models in bioinformatics and computational biology.
\newblock \emph{Nature reviews bioengineering}, 2\penalty0 (2):\penalty0 136--154, 2024.

\bibitem[Sheshmani et~al.(2024)Sheshmani, You, Buyukates, Ziashahabi, and Avestimehr]{sheshmani2024renormalization}
Artan Sheshmani, Yi-Zhuang You, Baturalp Buyukates, Amir Ziashahabi, and Salman Avestimehr.
\newblock Renormalization group flow, optimal transport and diffusion-based generative model.
\newblock \emph{arXiv preprint arXiv:2402.17090}, 2024.

\bibitem[Triplett and Lu(2023)]{triplett2023diffusion}
Luke Triplett and Jianfeng Lu.
\newblock Diffusion methods for generating transition paths.
\newblock \emph{arXiv preprint arXiv:2309.10276}, 2023.

\bibitem[Wang et~al.(2023)Wang, Aarts, and Zhou]{wang2023generative}
Lingxiao Wang, Gert Aarts, and Kai Zhou.
\newblock Generative diffusion models for lattice field theory.
\newblock \emph{arXiv preprint arXiv:2311.03578}, 2023.

\bibitem[Xu et~al.(2022)Xu, Yu, Song, Shi, Ermon, and Tang]{xu2022geodiff}
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang.
\newblock Geodiff: A geometric diffusion model for molecular conformation generation.
\newblock \emph{arXiv preprint arXiv:2203.02923}, 2022.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Cho, Merchant, Abbeel, Schuurmans, Mordatch, and Cubuk]{yang2023scalable}
Mengjiao Yang, KwangHwan Cho, Amil Merchant, Pieter Abbeel, Dale Schuurmans, Igor Mordatch, and Ekin~Dogus Cubuk.
\newblock Scalable diffusion for materials generation.
\newblock \emph{arXiv preprint arXiv:2311.09235}, 2023{\natexlab{b}}.

\bibitem[Yim et~al.(2024)Yim, St{\"a}rk, Corso, Jing, Barzilay, and Jaakkola]{yim2024diffusion}
Jason Yim, Hannes St{\"a}rk, Gabriele Corso, Bowen Jing, Regina Barzilay, and Tommi~S Jaakkola.
\newblock Diffusion models in protein structure and docking.
\newblock \emph{Wiley Interdisciplinary Reviews: Computational Molecular Science}, 14\penalty0 (2):\penalty0 e1711, 2024.

\bibitem[Zhu et~al.(2024)Zhu, Chen, Theodorou, Chen, and Tao]{zhu2024quantum}
Yuchen Zhu, Tianrong Chen, Evangelos~A Theodorou, Xie Chen, and Molei Tao.
\newblock Quantum state generation with structure-preserving diffusion model.
\newblock \emph{arXiv preprint arXiv:2404.06336}, 2024.

\bibitem[Chan(2024)]{chan2024tutorial}
Stanley~H Chan.
\newblock Tutorial on diffusion models for imaging and vision.
\newblock \emph{arXiv preprint arXiv:2403.18103}, 2024.

\bibitem[Chen et~al.(2024{\natexlab{c}})Chen, Mei, Fan, and Wang]{chen2024overview}
Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang.
\newblock An overview of diffusion models: Applications, guided generation, statistical rates and optimization.
\newblock \emph{arXiv preprint arXiv:2404.07771}, 2024{\natexlab{c}}.

\bibitem[De~Bortoli et~al.(2021)De~Bortoli, Thornton, Heng, and Doucet]{de2021diffusion}
Valentin De~Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet.
\newblock Diffusion schr{\"o}dinger bridge with applications to score-based generative modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17695--17709, 2021.

\bibitem[Song et~al.(2020{\natexlab{b}})Song, Meng, and Ermon]{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock \emph{arXiv preprint arXiv:2010.02502}, 2020{\natexlab{b}}.

\bibitem[Dockhorn et~al.(2022)Dockhorn, Vahdat, and Kreis]{dockhorn2022genie}
Tim Dockhorn, Arash Vahdat, and Karsten Kreis.
\newblock Genie: Higher-order denoising diffusion solvers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30150--30166, 2022.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Huang, Efimov, Wei, Chi, and Chen]{li2024accelerating}
Gen Li, Yu~Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, and Yuxin Chen.
\newblock Accelerating convergence of score-based diffusion models, provably.
\newblock \emph{arXiv preprint arXiv:2403.03852}, 2024{\natexlab{c}}.

\bibitem[Lu et~al.(2022)Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5775--5787, 2022.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and Laine]{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26565--26577, 2022.

\bibitem[Zhang and Chen(2022)]{zhang2022fast}
Qinsheng Zhang and Yongxin Chen.
\newblock Fast sampling of diffusion models with exponential integrator.
\newblock \emph{arXiv preprint arXiv:2204.13902}, 2022.

\bibitem[Dockhorn et~al.(2021)Dockhorn, Vahdat, and Kreis]{dockhorn2021score}
Tim Dockhorn, Arash Vahdat, and Karsten Kreis.
\newblock Score-based generative modeling with critically-damped langevin diffusion.
\newblock \emph{arXiv preprint arXiv:2112.07068}, 2021.

\bibitem[Jolicoeur-Martineau et~al.(2021)Jolicoeur-Martineau, Li, Pich{\'e}-Taillefer, Kachman, and Mitliagkas]{jolicoeur2021gotta}
Alexia Jolicoeur-Martineau, Ke~Li, R{\'e}mi Pich{\'e}-Taillefer, Tal Kachman, and Ioannis Mitliagkas.
\newblock Gotta go fast when generating data with score-based models.
\newblock \emph{arXiv preprint arXiv:2105.14080}, 2021.

\bibitem[Zheng et~al.(2023)Zheng, Nie, Vahdat, Azizzadenesheli, and Anandkumar]{zheng2023fast}
Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock Fast sampling of diffusion models via operator learning.
\newblock In \emph{International Conference on Machine Learning}, pages 42390--42402. PMLR, 2023.

\bibitem[Xu et~al.(2023)Xu, Deng, Cheng, Tian, Liu, and Jaakkola]{xu2023restart}
Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola.
\newblock Restart sampling for improving generative processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 76806--76838, 2023.

\bibitem[Heek et~al.(2024)Heek, Hoogeboom, and Salimans]{heek2024multistep}
Jonathan Heek, Emiel Hoogeboom, and Tim Salimans.
\newblock Multistep consistency models.
\newblock \emph{arXiv preprint arXiv:2403.06807}, 2024.

\bibitem[Song et~al.(2023)Song, Dhariwal, Chen, and Sutskever]{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock \emph{arXiv preprint arXiv:2303.01469}, 2023.

\bibitem[Luhman and Luhman(2021)]{luhman2021knowledge}
Eric Luhman and Troy Luhman.
\newblock Knowledge distillation in iterative generative models for improved sampling speed.
\newblock \emph{arXiv preprint arXiv:2101.02388}, 2021.

\bibitem[Meng et~al.(2023)Meng, Rombach, Gao, Kingma, Ermon, Ho, and Salimans]{meng2023distillation}
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
\newblock On distillation of guided diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14297--14306, 2023.

\bibitem[Salimans and Ho(2022)]{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2202.00512}, 2022.

\bibitem[Tzen and Raginsky(2019)]{tzen2019theoretical}
Belinda Tzen and Maxim Raginsky.
\newblock Theoretical guarantees for sampling and inference in generative models with latent diffusions.
\newblock In \emph{Conference on Learning Theory}, pages 3084--3114. PMLR, 2019.

\bibitem[Benton et~al.(2023)Benton, De~Bortoli, Doucet, and Deligiannidis]{benton2023linear}
Joe Benton, Valentin De~Bortoli, Arnaud Doucet, and George Deligiannidis.
\newblock Linear convergence bounds for diffusion models via stochastic localization.
\newblock \emph{arXiv preprint arXiv:2308.03686}, 2023.

\bibitem[Chen et~al.(2023)Chen, Lee, and Lu]{chen2023improved}
Hongrui Chen, Holden Lee, and Jianfeng Lu.
\newblock Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions.
\newblock In \emph{International Conference on Machine Learning}, pages 4735--4763. PMLR, 2023.

\bibitem[Chen et~al.(2024{\natexlab{d}})Chen, Chewi, Lee, Li, Lu, and Salim]{chen2024probability}
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim.
\newblock The probability flow ode is provably fast.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{d}}.

\bibitem[Chen et~al.(2022)Chen, Chewi, Li, Li, Salim, and Zhang]{chen2022sampling}
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru~R Zhang.
\newblock Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions.
\newblock \emph{arXiv preprint arXiv:2209.11215}, 2022.

\bibitem[Gao and Zhu(2024)]{gao2024convergence}
Xuefeng Gao and Lingjiong Zhu.
\newblock Convergence analysis for general probability flow odes of diffusion models in wasserstein distances.
\newblock \emph{arXiv preprint arXiv:2401.17958}, 2024.

\bibitem[Huang et~al.(2024{\natexlab{c}})Huang, Huang, and Lin]{huang2024convergence}
Daniel~Zhengyu Huang, Jiaoyang Huang, and Zhengjiang Lin.
\newblock Convergence analysis of probability flow ode for score-based generative models.
\newblock \emph{arXiv preprint arXiv:2404.09730}, 2024{\natexlab{c}}.

\bibitem[Lee et~al.(2022)Lee, Lu, and Tan]{lee2022convergence}
Holden Lee, Jianfeng Lu, and Yixin Tan.
\newblock Convergence for score-based generative modeling with polynomial complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 22870--22882, 2022.

\bibitem[Pedrotti et~al.(2023)Pedrotti, Maas, and Mondelli]{pedrotti2023improved}
Francesco Pedrotti, Jan Maas, and Marco Mondelli.
\newblock Improved convergence of score-based diffusion models via prediction-correction.
\newblock \emph{arXiv preprint arXiv:2305.14164}, 2023.

\bibitem[Anderson(1982)]{anderson1982reverse}
Brian~DO Anderson.
\newblock Reverse-time diffusion equation models.
\newblock \emph{Stochastic Processes and their Applications}, 12\penalty0 (3):\penalty0 313--326, 1982.

\bibitem[Hyv{\"a}rinen and Dayan(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen and Peter Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (4), 2005.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674, 2011.

\bibitem[Cao et~al.(2024)Cao, Chen, Luo, and Zhou]{cao2024exploring}
Yu~Cao, Jingrun Chen, Yixin Luo, and Xiang Zhou.
\newblock Exploring the optimal choice for generative processes in diffusion models: Ordinary vs stochastic differential equations.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Deveney et~al.(2023)Deveney, Stanczuk, Kreusser, Budd, and Sch{\"o}nlieb]{deveney2023closing}
Teo Deveney, Jan Stanczuk, Lisa~Maria Kreusser, Chris Budd, and Carola-Bibiane Sch{\"o}nlieb.
\newblock Closing the ode-sde gap in score-based diffusion models through the fokker-planck equation.
\newblock \emph{arXiv preprint arXiv:2311.15996}, 2023.

\bibitem[Nie et~al.(2023)Nie, Guo, Lu, Zhou, Zheng, and Li]{nie2023blessing}
Shen Nie, Hanzhong~Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li.
\newblock The blessing of randomness: Sde beats ode in general diffusion-based image editing.
\newblock \emph{arXiv preprint arXiv:2311.01410}, 2023.

\bibitem[Geyer(1991)]{geyer1991markov}
Charles~J Geyer.
\newblock Markov chain monte carlo maximum likelihood.
\newblock In \emph{E. M. Kernamidas, editor, Computing Science and Statistics: Proc. 23rd Symposium on the Interface}, pages 156--163. Interface Foundation of North America, 1991.

\bibitem[Hukushima and Nemoto(1996)]{hukushima1996exchange}
Koji Hukushima and Koji Nemoto.
\newblock Exchange monte carlo method and application to spin glass simulations.
\newblock \emph{Journal of the Physical Society of Japan}, 65\penalty0 (6):\penalty0 1604--1608, 1996.

\bibitem[Liang(2003)]{liang2003use}
Faming Liang.
\newblock Use of sequential structure in simulation from high-dimensional systems.
\newblock \emph{Physical Review E}, 67\penalty0 (5):\penalty0 056101, 2003.

\bibitem[Anari et~al.(2023)Anari, Huang, Liu, Vuong, Xu, and Yu]{anari2023parallel}
Nima Anari, Yizhi Huang, Tianyu Liu, Thuy-Duong Vuong, Brian Xu, and Katherine Yu.
\newblock Parallel discrete sampling via continuous walks.
\newblock In \emph{Proceedings of the 55th Annual ACM Symposium on Theory of Computing}, pages 103--116, 2023.

\bibitem[Lee(2023)]{lee2023parallelising}
Holden Lee.
\newblock Parallelising glauber dynamics.
\newblock \emph{arXiv preprint arXiv:2307.07131}, 2023.

\bibitem[Yu and Dalalyana(2024)]{yu2024parallelized}
Lu~Yu and Arnak Dalalyana.
\newblock Parallelized midpoint randomization for langevin monte carlo.
\newblock \emph{arXiv preprint arXiv:2402.14434}, 2024.

\bibitem[Anari et~al.(2024)Anari, Chewi, and Vuong]{anari2024fast}
Nima Anari, Sinho Chewi, and Thuy-Duong Vuong.
\newblock Fast parallel sampling under isoperimetry.
\newblock \emph{arXiv preprint arXiv:2401.09016}, 2024.

\bibitem[Oksendal(2013)]{oksendal2013stochastic}
Bernt Oksendal.
\newblock \emph{Stochastic differential equations: an introduction with applications}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Chen and Ying(2024)]{chen2024convergence}
Hongrui Chen and Lexing Ying.
\newblock Convergence analysis of discrete diffusion model: Exact implementation through uniformization.
\newblock \emph{arXiv preprint arXiv:2402.08095}, 2024.

\bibitem[Lou et~al.(2023)Lou, Meng, and Ermon]{lou2023discrete}
Aaron Lou, Chenlin Meng, and Stefano Ermon.
\newblock Discrete diffusion language modeling by estimating the ratios of the data distribution.
\newblock \emph{arXiv preprint arXiv:2310.16834}, 2023.

\bibitem[Meng et~al.(2022)Meng, Choi, Song, and Ermon]{meng2022concrete}
Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon.
\newblock Concrete score matching: Generalized score matching for discrete data.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 34532--34545, 2022.

\bibitem[Albergo et~al.(2023{\natexlab{b}})Albergo, Boffi, Lindsey, and Vanden-Eijnden]{albergo2023multimarginal}
Michael~S Albergo, Nicholas~M Boffi, Michael Lindsey, and Eric Vanden-Eijnden.
\newblock Multimarginal generative modeling with stochastic interpolants.
\newblock \emph{arXiv preprint arXiv:2310.03695}, 2023{\natexlab{b}}.

\bibitem[Le~Gall(2016)]{le2016brownian}
Jean-Fran{\c{c}}ois Le~Gall.
\newblock \emph{Brownian motion, martingales, and stochastic calculus}.
\newblock Springer, 2016.

\bibitem[Guillin and Wang(2012)]{guillin2012degenerate}
Arnaud Guillin and Feng-Yu Wang.
\newblock Degenerate fokker--planck equations: Bismut formula, gradient estimate and harnack inequality.
\newblock \emph{Journal of Differential Equations}, 253\penalty0 (1):\penalty0 20--40, 2012.

\end{thebibliography}
