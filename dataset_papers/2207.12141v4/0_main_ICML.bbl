\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2020)Abbas, Sokota, Talvitie, and
  White]{abbas2020selective}
Abbas, Z., Sokota, S., Talvitie, E., and White, M.
\newblock Selective dyna-style planning under limited model capacity.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1--10.
  PMLR, 2020.

\bibitem[Asadi et~al.(2019)Asadi, Misra, Kim, and Littman]{asadi2019combating}
Asadi, K., Misra, D., Kim, S., and Littman, M.~L.
\newblock Combating the compounding-error problem with a multi-step model.
\newblock \emph{arXiv preprint arXiv:1905.13320}, 2019.

\bibitem[Aswani et~al.(2012)Aswani, Bouffard, and Tomlin]{aswani2012extensions}
Aswani, A., Bouffard, P., and Tomlin, C.
\newblock Extensions of learning-based model predictive control for real-time
  application to a quadrotor helicopter.
\newblock In \emph{2012 American Control Conference (ACC)}, pp.\  4661--4666.
  IEEE, 2012.

\bibitem[Brittain et~al.(2019)Brittain, Bertram, Yang, and
  Wei]{brittain2019prioritized}
Brittain, M., Bertram, J., Yang, X., and Wei, P.
\newblock Prioritized sequence experience replay.
\newblock \emph{arXiv preprint arXiv:1905.12726}, 2019.

\bibitem[Chen et~al.(2020)Chen, Wang, Zhou, and Ross]{chen2021randomized}
Chen, X., Wang, C., Zhou, Z., and Ross, K.~W.
\newblock Randomized ensembled double q-learning: Learning fast without a
  model.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and
  Rasmussen]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pp.\  465--472. Citeseer, 2011.

\bibitem[Dulac-Arnold et~al.(2021)Dulac-Arnold, Levine, Mankowitz, Li,
  Paduraru, Gowal, and Hester]{dulac2021challenges}
Dulac-Arnold, G., Levine, N., Mankowitz, D.~J., Li, J., Paduraru, C., Gowal,
  S., and Hester, T.
\newblock Challenges of real-world reinforcement learning: definitions,
  benchmarks and analysis.
\newblock \emph{Machine Learning}, pp.\  1--50, 2021.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Khazatsky, Levine, and
  Salakhutdinov]{eysenbach2021mismatched}
Eysenbach, B., Khazatsky, A., Levine, S., and Salakhutdinov, R.
\newblock Mismatched no more: Joint model-policy optimization for model-based
  rl.
\newblock \emph{arXiv preprint arXiv:2110.02758}, 2021.

\bibitem[Farahmand(2018)]{farahmand2018iterative}
Farahmand, A.-m.
\newblock Iterative value-aware model learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Farahmand et~al.(2017)Farahmand, Barreto, and
  Nikovski]{farahmand2017value}
Farahmand, A.-m., Barreto, A., and Nikovski, D.
\newblock Value-aware loss function for model-based reinforcement learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1486--1494.
  PMLR, 2017.

\bibitem[Fedus et~al.(2020)Fedus, Ramachandran, Agarwal, Bengio, Larochelle,
  Rowland, and Dabney]{fedus2020revisiting}
Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., Larochelle, H., Rowland,
  M., and Dabney, W.
\newblock Revisiting fundamentals of experience replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3061--3071. PMLR, 2020.

\bibitem[Froehlich et~al.(2022)Froehlich, Lefarov, Zeilinger, and
  Berkenkamp]{froehlich2022onpolicy}
Froehlich, L., Lefarov, M., Zeilinger, M., and Berkenkamp, F.
\newblock On-policy model errors in reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Fu et~al.(2016)Fu, Levine, and Abbeel]{fu2016one}
Fu, J., Levine, S., and Abbeel, P.
\newblock One-shot learning of manipulation skills with online dynamics
  adaptation and neural network priors.
\newblock In \emph{2016 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  4019--4026. IEEE, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2020)Fujimoto, Meger, and
  Precup]{fujimoto2020equivalence}
Fujimoto, S., Meger, D., and Precup, D.
\newblock An equivalence between loss functions and non-uniform sampling in
  experience replay.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 14219--14230, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Goyal et~al.(2018)Goyal, Brakel, Fedus, Singhal, Lillicrap, Levine,
  Larochelle, and Bengio]{goyal2018recall}
Goyal, A., Brakel, P., Fedus, W., Singhal, S., Lillicrap, T., Levine, S.,
  Larochelle, H., and Bengio, Y.
\newblock Recall traces: Backtracking models for efficient reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Grimm et~al.(2020)Grimm, Barreto, Singh, and Silver]{grimm2020value}
Grimm, C., Barreto, A., Singh, S., and Silver, D.
\newblock The value equivalence principle for model-based reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5541--5552, 2020.

\bibitem[Guo et~al.(2022)Guo, Gong, and Tao]{guo2022relational}
Guo, J., Gong, M., and Tao, D.
\newblock A relational intervention approach for unsupervised dynamics
  generalization in model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2206.04551}, 2022.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2681--2691. PMLR, 2019.

\bibitem[Hu et~al.(2021)Hu, Ye, Zhu, Ren, and Zhang]{hu2021generalizable}
Hu, H., Ye, J., Zhu, G., Ren, Z., and Zhang, C.
\newblock Generalizable episodic memory for deep reinforcement learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139, pp.\  4380--4390, 2021.

\bibitem[Huang et~al.(2021)Huang, Yin, Zhang, and Huang]{huang2021learning}
Huang, W., Yin, Q., Zhang, J., and Huang, K.
\newblock Learning to reweight imaginary transitions for model-based
  reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  7848--7856, 2021.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 12519--12530, 2019.

\bibitem[Jiang et~al.(2021)Jiang, Grefenstette, and
  Rockt{\"a}schel]{jiang2021prioritized}
Jiang, M., Grefenstette, E., and Rockt{\"a}schel, T.
\newblock Prioritized level replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4940--4950. PMLR, 2021.

\bibitem[Katharopoulos \& Fleuret(2018)Katharopoulos and
  Fleuret]{katharopoulos2018not}
Katharopoulos, A. and Fleuret, F.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In \emph{International conference on machine learning}, pp.\
  2525--2534. PMLR, 2018.

\bibitem[Kumar et~al.(2016)Kumar, Todorov, and Levine]{kumar2016optimal}
Kumar, V., Todorov, E., and Levine, S.
\newblock Optimal control with learned local models: Application to dexterous
  manipulation.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  378--383. IEEE, 2016.

\bibitem[Kurutach et~al.(2018)Kurutach, Clavera, Duan, Tamar, and
  Abbeel]{kurutach2018model}
Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P.
\newblock Model-ensemble trust-region policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lahire et~al.(2021)Lahire, Geist, and Rachelson]{lahire2021large}
Lahire, T., Geist, M., and Rachelson, E.
\newblock Large batch experience replay.
\newblock \emph{arXiv preprint arXiv:2110.01528}, 2021.

\bibitem[Lai et~al.(2020)Lai, Shen, Zhang, and Yu]{29}
Lai, H., Shen, J., Zhang, W., and Yu, Y.
\newblock Bidirectional model-based policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5618--5627. PMLR, 2020.

\bibitem[Lai et~al.(2021)Lai, Shen, Zhang, Huang, Zhang, Tang, Yu, and
  Li]{lai2021effective}
Lai, H., Shen, J., Zhang, W., Huang, Y., Zhang, X., Tang, R., Yu, Y., and Li,
  Z.
\newblock On effective scheduling of model-based reinforcement learning.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Lee et~al.(2020)Lee, Seo, Lee, Lee, and Shin]{lee2020context}
Lee, K., Seo, Y., Lee, S., Lee, H., and Shin, J.
\newblock Context-aware dynamics model for generalization in model-based
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5757--5766. PMLR, 2020.

\bibitem[Lee et~al.(2019)Lee, Sungik, and Chung]{lee2019sample}
Lee, S.~Y., Sungik, C., and Chung, S.-Y.
\newblock Sample-efficient deep reinforcement learning via episodic backward
  update.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Li et~al.(2022)Li, Wang, Chen, Liu, Ma, and Liu]{li2022gradient}
Li, C., Wang, Y., Chen, W., Liu, Y., Ma, Z.-M., and Liu, T.-Y.
\newblock Gradient information matters in policy optimization by
  back-propagating through model.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{ICLR}, 2016.

\bibitem[Liu et~al.(2021)Liu, Xue, Pang, Jiang, Xu, and Yu]{liu2021regret}
Liu, X.-H., Xue, Z., Pang, J., Jiang, S., Xu, F., and Yu, Y.
\newblock Regret minimization experience replay in off-policy reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Liu et~al.(2020)Liu, Xu, and Pan]{liu2020re}
Liu, Y., Xu, J., and Pan, Y.
\newblock [re] when to trust your model: Model-based policyoptimization.
\newblock \emph{ReScience C}, 6\penalty0 (2), 2020.
\newblock Accepted at NeurIPS 2019 Reproducibility Challenge.

\bibitem[Lovatto et~al.(2020)Lovatto, Bueno, Mau{\'a}, and
  Barros]{lovatto2020decision}
Lovatto, {\^A}.~G., Bueno, T.~P., Mau{\'a}, D.~D., and Barros, L.~N.
\newblock Decision-aware model learning for actor-critic methods: when theory
  does not meet practice.
\newblock 2020.

\bibitem[Luo et~al.(2018)Luo, Xu, Li, Tian, Darrell, and
  Ma]{luo2018algorithmic}
Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T.
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Meier \& Schaal(2016)Meier and Schaal]{meier2016drifting}
Meier, F. and Schaal, S.
\newblock Drifting gaussian processes with varying neighborhood sizes for
  online model learning.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  264--269. IEEE, 2016.

\bibitem[Meier et~al.(2016)Meier, Kappler, Ratliff, and
  Schaal]{meier2016towards}
Meier, F., Kappler, D., Ratliff, N., and Schaal, S.
\newblock Towards robust online inverse dynamics learning.
\newblock In \emph{2016 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  4034--4039. IEEE, 2016.

\bibitem[Mu et~al.(2021)Mu, Zhuang, Wang, Zhu, Liu, Chen, Luo, Li, Zhang, and
  Hao]{mu2021model}
Mu, Y., Zhuang, Y., Wang, B., Zhu, G., Liu, W., Chen, J., Luo, P., Li, S.,
  Zhang, C., and Hao, J.
\newblock Model-based reinforcement learning via imagination with derived
  memory.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 9493--9505, 2021.

\bibitem[Nagabandi et~al.(2018{\natexlab{a}})Nagabandi, Clavera, Liu, Fearing,
  Abbeel, Levine, and Finn]{nagabandi2018learning}
Nagabandi, A., Clavera, I., Liu, S., Fearing, R.~S., Abbeel, P., Levine, S.,
  and Finn, C.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Nagabandi et~al.(2018{\natexlab{b}})Nagabandi, Finn, and
  Levine]{nagabandi2018deep}
Nagabandi, A., Finn, C., and Levine, S.
\newblock Deep online learning via meta-learning: Continual adaptation for
  model-based rl.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.

\bibitem[Novati \& Koumoutsakos(2019)Novati and
  Koumoutsakos]{novati2019remember}
Novati, G. and Koumoutsakos, P.
\newblock Remember and forget for experience replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4851--4860. PMLR, 2019.

\bibitem[Oh et~al.(2022)Oh, Shin, Yang, and Hwang]{oh2022modelaugmented}
Oh, Y., Shin, J., Yang, E., and Hwang, S.~J.
\newblock Model-augmented prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Pan et~al.(2020)Pan, He, Tu, and He]{pan2020trust}
Pan, F., He, J., Tu, D., and He, Q.
\newblock Trust the model when it is confident: Masked model-based
  actor-critic.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Parr et~al.(2008)Parr, Li, Taylor, Painter-Wakefield, and
  Littman]{parr2008analysis}
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., and Littman, M.~L.
\newblock An analysis of linear models, linear value-function approximation,
  and feature selection for reinforcement learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  752--759, 2008.

\bibitem[Pastor et~al.(2011)Pastor, Righetti, Kalakrishnan, and
  Schaal]{pastor2011online}
Pastor, P., Righetti, L., Kalakrishnan, M., and Schaal, S.
\newblock Online movement adaptation based on previous sensor experiences.
\newblock In \emph{2011 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  365--371. IEEE, 2011.

\bibitem[Polydoros \& Nalpantidis(2017)Polydoros and
  Nalpantidis]{polydoros2017survey}
Polydoros, A.~S. and Nalpantidis, L.
\newblock Survey of model-based reinforcement learning: Applications on
  robotics.
\newblock \emph{Journal of Intelligent \& Robotic Systems}, 86\penalty0
  (2):\penalty0 153--173, 2017.

\bibitem[Rajeswaran et~al.(2020)Rajeswaran, Mordatch, and
  Kumar]{rajeswaran2020game}
Rajeswaran, A., Mordatch, I., and Kumar, V.
\newblock A game theoretic framework for model based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7953--7963. PMLR, 2020.

\bibitem[Rasmussen \& Kuss(2004)Rasmussen and Kuss]{rasmussen2004gaussian}
Rasmussen, C. and Kuss, M.
\newblock Gaussian processes in reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, pp.\
  751--759, 2004.

\bibitem[Sastry \& Isidori(1989)Sastry and Isidori]{sastry1989adaptive}
Sastry, S.~S. and Isidori, A.
\newblock Adaptive control of linearizable systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 34\penalty0
  (11):\penalty0 1123--1131, 1989.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2016prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{ICLR (Poster)}, 2016.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2020mastering}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Shen et~al.(2020)Shen, Zhao, Zhang, and Yu]{shen2020model}
Shen, J., Zhao, H., Zhang, W., and Yu, Y.
\newblock Model-based policy optimization with unsupervised model adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sun et~al.(2022)Sun, Zheng, Wang, Cohen, and Huang]{sun2022transfer}
Sun, Y., Zheng, R., Wang, X., Cohen, A.~E., and Huang, F.
\newblock Transfer {RL} across observation feature spaces via model-based
  regularization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=7KdAoOsI81C}.

\bibitem[Sutton(1990)]{sutton1990integrated}
Sutton, R.~S.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{Machine learning proceedings 1990}, pp.\  216--224.
  Elsevier, 1990.

\bibitem[Sutton et~al.(2008)Sutton, Szepesv{\'a}ri, Geramifard, and
  Bowling]{sutton2008dyna}
Sutton, R.~S., Szepesv{\'a}ri, C., Geramifard, A., and Bowling, M.
\newblock Dyna-style planning with linear function approximation and
  prioritized sweeping.
\newblock In \emph{Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence}, pp.\  528--536, 2008.

\bibitem[Tanaskovic et~al.(2013)Tanaskovic, Fagiano, Smith, Goulart, and
  Morari]{tanaskovic2013adaptive}
Tanaskovic, M., Fagiano, L., Smith, R., Goulart, P., and Morari, M.
\newblock Adaptive model predictive control for constrained linear systems.
\newblock In \emph{2013 European Control Conference (ECC)}, pp.\  382--387.
  IEEE, 2013.

\bibitem[{Todorov} et~al.(2012){Todorov}, {Erez}, and
  {Tassa}]{todorov2012mujoco}
{Todorov}, E., {Erez}, T., and {Tassa}, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033, 2012.

\bibitem[Voelcker et~al.(2022)Voelcker, Liao, Garg, and massoud
  Farahmand]{voelcker2022value}
Voelcker, C.~A., Liao, V., Garg, A., and massoud Farahmand, A.
\newblock Value gradient weighted model-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=4-D6CZkRXxI}.

\bibitem[Yang et~al.(2022)Yang, Zhang, Hansen, Xu, and Wang]{yang2022learning}
Yang, R., Zhang, M., Hansen, N., Xu, H., and Wang, X.
\newblock Learning vision-guided quadrupedal locomotion end-to-end with
  cross-modal transformers.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Yao et~al.(2021)Yao, Xiao, An, Zhang, and Luo]{yao2021sample}
Yao, Y., Xiao, L., An, Z., Zhang, W., and Luo, D.
\newblock Sample efficient reinforcement learning via model-ensemble
  exploration and exploitation.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  4202--4208. IEEE, 2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Rashidinejad, Jiao, Tian, Gonzalez, and
  Russell]{zhang2021made}
Zhang, T., Rashidinejad, P., Jiao, J., Tian, Y., Gonzalez, J.~E., and Russell,
  S.
\newblock Made: Exploration via maximizing deviation from explored regions.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zheng et~al.(2023)Zheng, Wang, Xu, and Huang]{zheng2023model}
Zheng, R., Wang, X., Xu, H., and Huang, F.
\newblock Is model ensemble necessary? model-based rl via a single model with
  lipschitz regularized value function.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\end{thebibliography}
