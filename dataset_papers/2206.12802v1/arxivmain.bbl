\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and Liang]{all19}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in neural information processing systems}, pages
  6155--6166, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{als19_dnn}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{ICML}, 2019{\natexlab{b}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{c}})Allen-Zhu, Li, and
  Song]{als19_rnn}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock In \emph{NeurIPS}, 2019{\natexlab{c}}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{adhlsw19}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{NeurIPS}. arXiv preprint arXiv:1904.11955,
  2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and Wang]{adhlw19}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{ICML}. arXiv preprint arXiv:1901.08584, 2019{\natexlab{b}}.

\bibitem[Bakshi et~al.(2019)Bakshi, Jayaram, and Woodruff]{bjm19}
Ainesh Bakshi, Rajesh Jayaram, and David~P Woodruff.
\newblock Learning two layer rectified neural networks in polynomial time.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 195--268. PMLR,
  2019.

\bibitem[Bernstein(1924)]{b24}
Sergei Bernstein.
\newblock On a modification of chebyshev's inequality and of the error formula
  of laplace.
\newblock \emph{Ann. Sci. Inst. Sav. Ukraine, Sect. Math}, 1\penalty0
  (4):\penalty0 38--49, 1924.

\bibitem[Blum et~al.(2020)Blum, Hopcroft, and Kannan]{BlumHK20}
Avrim Blum, John Hopcroft, and Ravi Kannan.
\newblock \emph{Foundations of Data Science}.
\newblock Cambridge University Press, 2020.

\bibitem[Brand et~al.(2021)Brand, Peng, Song, and Weinstein]{bpsw21}
Jan van~den Brand, Binghui Peng, Zhao Song, and Omri Weinstein.
\newblock Training (overparametrized) neural networks in near-linear time.
\newblock In \emph{ITCS}, 2021.

\bibitem[Bubeck et~al.(2020)Bubeck, Eldan, Lee, and Mikulincer]{belm20}
S{\'{e}}bastien Bubeck, Ronen Eldan, Yin~Tat Lee, and Dan Mikulincer.
\newblock Network size and size of the weights in memorization with two-layers
  neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Cao and Gu(2019{\natexlab{a}})]{CaoGu19}
Yuan Cao and Quanquan Gu.
\newblock A generalization theory of gradient descent for learning
  over-parameterized deep relu networks.
\newblock \emph{CoRR}, abs/1902.01384, 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1902.01384}.

\bibitem[Cao and Gu(2019{\natexlab{b}})]{cg19}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{NeurIPS}, pages 10835--10845, 2019{\natexlab{b}}.

\bibitem[Chen and Xu(2020)]{cx20}
Lin Chen and Sheng Xu.
\newblock Deep neural tangent kernel and laplace kernel have the same rkhs.
\newblock \emph{arXiv preprint arXiv:2009.10683}, 2020.

\bibitem[Chen et~al.(2020)Chen, Klivans, and Meka]{ckm20}
Sitan Chen, Adam~R. Klivans, and Raghu Meka.
\newblock Learning deep relu networks is fixed-parameter tractable.
\newblock \emph{arXiv preprint arXiv:2009.13512}, 2020.

\bibitem[Chen et~al.(2019)Chen, Cao, Zou, and Gu]{Chen19}
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu.
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?
\newblock \emph{CoRR}, abs/1911.12360, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.12360}.

\bibitem[Chernoff(1952)]{c52}
Herman Chernoff.
\newblock A measure of asymptotic efficiency for tests of a hypothesis based on
  the sum of observations.
\newblock \emph{The Annals of Mathematical Statistics}, pages 493--507, 1952.

\bibitem[Collobert et~al.(2011)Collobert, Weston, Bottou, Karlen, Kavukcuoglu,
  and Kuksa]{cwb+11}
Ronan Collobert, Jason Weston, L{\'e}on Bottou, Michael Karlen, Koray
  Kavukcuoglu, and Pavel Kuksa.
\newblock Natural language processing (almost) from scratch.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (ARTICLE):\penalty0 2493--2537, 2011.

\bibitem[Daniely(2020)]{Daniely20}
Amit Daniely.
\newblock Neural networks learning and memorization with (almost) no
  over-parameterization.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  {(NeurIPS)}}, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{dclt18}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Hou, P{\'o}czos, Salakhutdinov, Wang,
  and Xu]{dhp+19}
Simon~S Du, Kangcheng Hou, Barnab{\'a}s P{\'o}czos, Ruslan Salakhutdinov,
  Ruosong Wang, and Keyulu Xu.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock \emph{arXiv preprint arXiv:1905.13192}, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Lee, Li, Wang, and Zhai]{dllwz19}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}.
  \url{https://arxiv.org/pdf/1811.03804}, 2019{\natexlab{b}}.

\bibitem[Du et~al.(2019{\natexlab{c}})Du, Zhai, Poczos, and Singh]{dzps19}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{ICLR}. \url{https://arxiv.org/pdf/1810.02054},
  2019{\natexlab{c}}.

\bibitem[Erd\H{o}s and R\'{e}nyi(1961)]{er61}
P.~Erd\H{o}s and A.~R\'{e}nyi.
\newblock On a classical problem of probability theory.
\newblock \emph{Magyar Tud. Akad. Mat. Kutat\'{o} Int. K\"{o}zl.}, 6:\penalty0
  215--220, 1961.

\bibitem[Feller(1943)]{Feller43}
William Feller.
\newblock Generalization of a probability limit theorem of cram\'{e}r.
\newblock \emph{Trans. Am. Math. Soc.}, 54:\penalty0 361â€“372, 1943.

\bibitem[Ge et~al.(2018)Ge, Lee, and Ma]{glm18}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In \emph{ICLR}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{hzrs16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition (CVPR)}, pages 770--778, 2016.

\bibitem[Hoeffding(1963)]{h63}
Wassily Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock \emph{Journal of the American Statistical Association}, 58\penalty0
  (301):\penalty0 13--30, 1963.

\bibitem[Huang and Yau(2020)]{hy20}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  4542--4551. PMLR, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jgh18}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems (NeurIPS)}, pages 8580--8589, 2018.

\bibitem[Ji and Telgarsky(2020)]{jt20}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Kawaguchi and Huang(2019)]{kh19}
Kenji Kawaguchi and Jiaoyang Huang.
\newblock Gradient descent finds global minima for generalizable deep neural
  networks of practical sizes.
\newblock In \emph{2019 57th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 92--99. IEEE, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{ksh12}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lbbh98}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2020)Lee, Shen, Song, Wang, and Yu]{lsswy20}
Jason~D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu.
\newblock Generalized leverage score sampling for neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Li et~al.(2021)Li, Jiang, Zhang, Kamp, and Dou]{ljzkd21}
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi~Dou.
\newblock Fedbn: Federated learning on non-iid features via local batch
  normalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}. \url{https://openreview.net/forum?id=6YEQUn0QICG}, 2021.

\bibitem[Li and Liang(2018)]{ll18}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Li and Yuan(2017)]{ly17}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with {R}e{LU}
  activation.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pages 597--607, 2017.

\bibitem[Nitanda et~al.(2019)Nitanda, Chinot, and Suzuki]{nitanda2019gradient}
Atsushi Nitanda, Geoffrey Chinot, and Taiji Suzuki.
\newblock Gradient descent can learn less over-parameterized two-layer neural
  networks on classification problems.
\newblock \emph{arXiv preprint arXiv:1905.09870}, 2019.

\bibitem[Oymak and Soltanolkotabi(2020)]{os20}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{alphago16}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{alphago17}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Song and Yang(2019)]{sy19}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Song et~al.(2021)Song, Yang, and Zhang]{syz21}
Zhao Song, Shuo Yang, and Ruizhe Zhang.
\newblock Does preprocessing help training over-parameterized neural networks?
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[StEx(2011)]{stex1}
StEx StEx.
\newblock How can we sum up sin and cos series when the angles are in
  arithmetic progression?
\newblock \url{ https://math.stackexchange.com/questions/17966/}, 2011.
\newblock Accessed: 2021-05-21.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{slj+15}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem[Tropp(2015)]{t15}
Joel~A Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (1-2):\penalty0 1--230, 2015.

\bibitem[Zhang et~al.(2021)Zhang, Zhang, Hong, Sun, and Luo]{ZhangZHSL21}
Jiawei Zhang, Yushun Zhang, Mingyi Hong, Ruoyu Sun, and Zhi{-}Quan Luo.
\newblock When expressivity meets trainability: Fewer than $n$ neurons can
  work.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  {(NeurIPS)}}, pages 9167--9180, 2021.

\bibitem[Zhong et~al.(2017{\natexlab{a}})Zhong, Song, and Dhillon]{zsd17}
Kai Zhong, Zhao Song, and Inderjit~S. Dhillon.
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock \emph{arXiv preprint arXiv:1711.03440}, 2017{\natexlab{a}}.

\bibitem[Zhong et~al.(2017{\natexlab{b}})Zhong, Song, Jain, Bartlett, and
  Dhillon]{zsjbd17}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L. Bartlett, and Inderjit~S. Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{ICML}, 2017{\natexlab{b}}.

\bibitem[Zou and Gu(2019)]{zg19}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{NeurIPS}, pages 2053--2062, 2019.

\end{thebibliography}
