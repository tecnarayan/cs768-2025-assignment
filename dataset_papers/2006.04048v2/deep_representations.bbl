\begin{thebibliography}{10}

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville, {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{cybenko1989approximation}
G.~Cybenko, ``Approximation by superpositions of a sigmoidal function,'' {\em
  Mathematics of control, signals and systems}, vol.~2, no.~4, pp.~303--314,
  1989.

\bibitem{funahashi1989approximate}
K.-I. Funahashi, ``On the approximate realization of continuous mappings by
  neural networks,'' {\em Neural networks}, vol.~2, no.~3, pp.~183--192, 1989.

\bibitem{hanin2017approximating}
B.~Hanin and M.~Sellke, ``Approximating continuous functions by {ReLU} nets of
  minimal width,'' {\em arXiv preprint arXiv:1710.11278}, 2017.

\bibitem{hornik1989multilayer}
K.~Hornik, M.~Stinchcombe, and H.~White, ``Multilayer feedforward networks are
  universal approximators,'' {\em Neural networks}, vol.~2, no.~5,
  pp.~359--366, 1989.

\bibitem{lu2017expressive}
Z.~Lu, H.~Pu, F.~Wang, Z.~Hu, and L.~Wang, ``The expressive power of neural
  networks: A view from the width,'' in {\em Advances in neural information
  processing systems}, pp.~6231--6239, 2017.

\bibitem{barron1993universal}
A.~R. Barron, ``Universal approximation bounds for superpositions of a
  sigmoidal function,'' {\em IEEE Transactions on Information theory}, vol.~39,
  no.~3, pp.~930--945, 1993.

\bibitem{klusowski2018approximation}
J.~M. Klusowski and A.~R. Barron, ``Approximation by combinations of {ReLU} and
  squared {ReLU} ridge functions with $l^{1}$ and $l^{0}$ controls,'' {\em IEEE
  Transactions on Information Theory}, vol.~64, no.~12, pp.~7649--7656, 2018.

\bibitem{li2019better}
B.~Li, S.~Tang, and H.~Yu, ``Better approximations of high dimensional smooth
  functions by deep neural networks with rectified power units,'' {\em arXiv
  preprint arXiv:1903.05858}, 2019.

\bibitem{liang2016deep}
S.~Liang and R.~Srikant, ``Why deep neural networks for function
  approximation?,'' {\em arXiv preprint arXiv:1610.04161}, 2016.

\bibitem{ma2019barron}
C.~Ma and L.~Wu, ``Barron spaces and the compositional function spaces for
  neural network models,'' {\em arXiv preprint arXiv:1906.08039}, 2019.

\bibitem{safran2017depth}
I.~Safran and O.~Shamir, ``Depth-width tradeoffs in approximating natural
  functions with neural networks,'' in {\em Proceedings of the 34th
  International Conference on Machine Learning-Volume 70}, pp.~2979--2987,
  JMLR. org, 2017.

\bibitem{yarotsky2017error}
D.~Yarotsky, ``Error bounds for approximations with deep {ReLU} networks,''
  {\em Neural Networks}, vol.~94, pp.~103--114, 2017.

\bibitem{yarotsky2018optimal}
D.~Yarotsky, ``Optimal approximation of continuous functions by very deep
  {ReLU} networks,'' in {\em Conference On Learning Theory}, pp.~639--649,
  2018.

\bibitem{schmidt2019deep}
J.~Schmidt-Hieber, ``Deep {ReLU} network approximation of functions on a
  manifold,'' {\em arXiv preprint arXiv:1908.00695}, 2019.

\bibitem{bresler2020corrective}
G.~Bresler and D.~Nagaraj, ``A corrective view of neural networks:
  Representation, memorization and learning,'' {\em arXiv preprint
  arXiv:2002.00274}, 2020.

\bibitem{schmidt2017nonparametric}
J.~Schmidt-Hieber, ``Nonparametric regression using deep neural networks with
  {ReLU} activation function,'' {\em arXiv preprint arXiv:1708.06633}, 2017.

\bibitem{allen2020backward}
Z.~Allen-Zhu and Y.~Li, ``Backward feature correction: How deep learning
  performs deep learning,'' {\em arXiv preprint arXiv:2001.04413}, 2020.

\bibitem{poggio2017and}
T.~Poggio, H.~Mhaskar, L.~Rosasco, B.~Miranda, and Q.~Liao, ``Why and when can
  deep-but not shallow-networks avoid the curse of dimensionality: a review,''
  {\em International Journal of Automation and Computing}, vol.~14, no.~5,
  pp.~503--519, 2017.

\bibitem{poole2016exponential}
B.~Poole, S.~Lahiri, M.~Raghu, J.~Sohl-Dickstein, and S.~Ganguli, ``Exponential
  expressivity in deep neural networks through transient chaos,'' in {\em
  Advances in neural information processing systems}, pp.~3360--3368, 2016.

\bibitem{daniely2017depth}
A.~Daniely, ``Depth separation for neural networks,'' in {\em Conference on
  Learning Theory}, pp.~690--696, 2017.

\bibitem{delalleau2011shallow}
O.~Delalleau and Y.~Bengio, ``Shallow vs. deep sum-product networks,'' in {\em
  Advances in Neural Information Processing Systems}, pp.~666--674, 2011.

\bibitem{eldan2016power}
R.~Eldan and O.~Shamir, ``The power of depth for feedforward neural networks,''
  in {\em Conference on learning theory}, pp.~907--940, 2016.

\bibitem{telgarsky2016benefits}
M.~Telgarsky, ``benefits of depth in neural networks,'' in {\em Conference on
  Learning Theory}, pp.~1517--1539, 2016.

\bibitem{cohen2016expressive}
N.~Cohen, O.~Sharir, and A.~Shashua, ``On the expressive power of deep
  learning: A tensor analysis,'' in {\em Conference on Learning Theory},
  pp.~698--728, 2016.

\bibitem{chatziafratis2019depth}
V.~Chatziafratis, S.~G. Nagarajan, I.~Panageas, and X.~Wang, ``Depth-width
  trade-offs for relu networks via sharkovsky's theorem,'' {\em arXiv preprint
  arXiv:1912.04378}, 2019.

\bibitem{chatziafratis2020better}
V.~Chatziafratis, S.~G. Nagarajan, and I.~Panageas, ``Better depth-width
  trade-offs for neural networks through the lens of dynamical systems,'' in
  {\em International Conference on Machine Learning}, pp.~1469--1478, PMLR,
  2020.

\bibitem{martens2014expressive}
J.~Martens and V.~Medabalimi, ``On the expressive efficiency of sum product
  networks,'' {\em arXiv preprint arXiv:1411.7717}, 2014.

\bibitem{friedlander1998introduction}
F.~G. Friedlander and M.~C. Joshi, {\em Introduction to the Theory of
  Distributions}.
\newblock Cambridge University Press, 1998.

\end{thebibliography}
