\begin{thebibliography}{10}

\bibitem{ieee754}
Ieee standard for floating-point arithmetic.
\newblock {\em IEEE Std 754-2019 (Revision of IEEE 754-2008)}, pages 1--84,
  2019.

\bibitem{arpit2017memorization}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In {\em International conference on machine learning}, pages
  233--242. PMLR, 2017.

\bibitem{bird2009natural}
Steven Bird, Ewan Klein, and Edward Loper.
\newblock {\em Natural language processing with Python: analyzing text with the
  natural language toolkit}.
\newblock " O'Reilly Media, Inc.", 2009.

\bibitem{borgeaud2021retro}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George van~den Driessche, Jean-Baptiste Lespiau,
  Bogdan Damoc, Aidan Clark, et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock {\em arXiv preprint arXiv:2112.04426}, 2021.

\bibitem{bricken2021transformer}
Trenton Bricken and Cengiz Pehlevan.
\newblock Attention approximates sparse distributed memory.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{carlini2021memorization}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In {\em 30th USENIX Security Symposium (USENIX Security 21)}, pages
  2633--2650, 2021.

\bibitem{cer2018universal}
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni~St
  John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et~al.
\newblock Universal sentence encoder.
\newblock {\em arXiv preprint arXiv:1803.11175}, 2018.

\bibitem{sptag2018}
Qi~Chen, Haidong Wang, Mingqin Li, Gang Ren, Scarlett Li, Jeffery Zhu, Jason
  Li, Chuanjie Liu, Lintao Zhang, and Jingdong Wang.
\newblock {\em SPTAG: A library for fast approximate nearest neighbor search},
  2018.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{de2021mention}
Michiel de~Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William
  Cohen.
\newblock Mention memory: incorporating textual knowledge into transformers
  through entity mention attention.
\newblock {\em arXiv preprint arXiv:2110.06176}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{geva2020transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock {\em arXiv preprint arXiv:2012.14913}, 2020.

\bibitem{guo2020icml}
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and
  Sanjiv Kumar.
\newblock Accelerating large-scale inference with anisotropic vector
  quantization.
\newblock In {\em International Conference on Machine Learning}, pages
  3887--3896. PMLR, 2020.

\bibitem{guu2020realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock {\em arXiv preprint arXiv:2002.08909}, 2020.

\bibitem{henderson2017efficient}
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, L{\'a}szl{\'o}
  Luk{\'a}cs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil.
\newblock Efficient natural language response suggestion for smart reply.
\newblock {\em arXiv preprint arXiv:1705.00652}, 2017.

\bibitem{izacard2020distilling}
Gautier Izacard and Edouard Grave.
\newblock Distilling knowledge from reader to retriever for question answering.
\newblock {\em arXiv preprint arXiv:2012.04584}, 2020.

\bibitem{izacard2020fid}
Gautier Izacard and Edouard Grave.
\newblock Leveraging passage retrieval with generative models for open domain
  question answering.
\newblock {\em arXiv preprint arXiv:2007.01282}, 2020.

\bibitem{johnson2019faiss}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with gpus.
\newblock {\em IEEE Transactions on Big Data}, 2019.

\bibitem{vladimir2020emnlp}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.~H. Lewis, Ledell Wu,
  Sergey Edunov, Danqi Chen, and Wen{-}tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}.

\bibitem{khandelwal2019knnlm}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Generalization through memorization: Nearest neighbor language
  models.
\newblock {\em ICLR 2020}, 2019.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{naturalquestion}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
  Jacob Devlin, Kenton Lee, Kristina~N. Toutanova, Llion Jones, Ming-Wei Chang,
  Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock {\em Transactions of the Association of Computational Linguistics},
  2019.

\bibitem{lee2020learning}
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen.
\newblock Learning dense representations of phrases at scale.
\newblock {\em arXiv preprint arXiv:2012.12624}, 2020.

\bibitem{lewis2020pre}
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and
  Luke Zettlemoyer.
\newblock Pre-training via paraphrasing.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18470--18481, 2020.

\bibitem{lewis2020rag}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:9459--9474, 2020.

\bibitem{malkov2018efficient}
Yu~A Malkov and Dmitry~A Yashunin.
\newblock Efficient and robust approximate nearest neighbor search using
  hierarchical navigable small world graphs.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  42(4):824--836, 2018.

\bibitem{ni2021sentencet5}
Jianmo Ni, Gustavo~Hern{\'a}ndez {\'A}brego, Noah Constant, Ji~Ma, Keith~B
  Hall, Daniel Cer, and Yinfei Yang.
\newblock Sentence-t5: Scalable sentence encoders from pre-trained text-to-text
  models.
\newblock {\em arXiv preprint arXiv:2108.08877}, 2021.

\bibitem{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
  Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock {\em arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{rae2021gopher}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{raffel2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}, 2019.

\bibitem{2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of Machine Learning Research}, 21(140):1--67, 2020.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}, 2016.

\bibitem{robertson2009probabilistic}
Stephen Robertson and Hugo Zaragoza.
\newblock {\em The probabilistic relevance framework: BM25 and beyond}.
\newblock Now Publishers Inc, 2009.

\bibitem{seo2019real}
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur~P Parikh, Ali Farhadi, and
  Hannaneh Hajishirzi.
\newblock Real-time open-domain question answering with dense-sparse phrase
  index.
\newblock {\em arXiv preprint arXiv:1906.05807}, 2019.

\bibitem{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In {\em International Conference on Machine Learning}, pages
  4596--4604. PMLR, 2018.

\bibitem{singh2021end}
Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama.
\newblock End-to-end training of multi-document reader and retriever for
  open-domain question answering.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{smith2022mtnlg}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, et~al.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock {\em arXiv preprint arXiv:2201.11990}, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wu2022memorizing}
Yuhuai Wu, Markus~N Rabe, DeLesley Hutchins, and Christian Szegedy.
\newblock Memorizing transformers.
\newblock {\em arXiv preprint arXiv:2203.08913}, 2022.

\bibitem{xiong2020approximate}
Lee Xiong, Chenyan Xiong, Ye~Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,
  Junaid Ahmed, and Arnold Overwijk.
\newblock Approximate nearest neighbor negative contrastive learning for dense
  text retrieval.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  April 2021.

\bibitem{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, Online, June 2021. Association for
  Computational Linguistics.

\bibitem{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{yogatama2021spalm}
Dani Yogatama, Cyprien de~Masson~d’Autume, and Lingpeng Kong.
\newblock Adaptive semiparametric language models.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:362--373, 2021.

\end{thebibliography}
