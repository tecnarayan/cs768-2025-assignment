\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Coppola(2015)]{Coppola2015:IPM}
Greg Coppola.
\newblock \emph{Iterative parameter mixing for distributed large-margin
  training of structured predictors for natural language processing}.
\newblock PhD thesis, The University of Edinburgh, 2015.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and
  Sridharan]{cotter2011better}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems 24}, pages 1647--1655. Curran Associates, Inc., 2011.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jan):\penalty0 165--202, 2012.

\bibitem[Dieuleveut and Patel(2019)]{dieuleveut2019communication}
Aymeric Dieuleveut and Kumar~Kshitij Patel.
\newblock Communication trade-offs for local-sgd with large step size.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13579--13590, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization, ii: shrinking procedures and optimal
  algorithms.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2061--2089, 2013.

\bibitem[Godichon-Baggioni and Saadane(2017)]{godichon2017rates}
Antoine Godichon-Baggioni and Sofiane Saadane.
\newblock On the rates of convergence of parallelized averaged stochastic
  gradient algorithms.
\newblock \emph{arXiv preprint arXiv:1710.07926}, 2017.

\bibitem[Haddadpour et~al.(2019{\natexlab{a}})Haddadpour, Kamani, Mahdavi, and
  Cadambe]{haddadpour2019local}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe.
\newblock Local sgd with periodic averaging: Tighter analysis and adaptive
  synchronization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11080--11092, 2019{\natexlab{a}}.

\bibitem[Haddadpour et~al.(2019{\natexlab{b}})Haddadpour, Kamani, Mahdavi, and
  Cadambe]{haddadpour2019trading}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe.
\newblock Trading redundancy for communication: Speeding up distributed sgd for
  non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  2545--2554, 2019{\natexlab{b}}.

\bibitem[Jain et~al.(2017)Jain, Netrapalli, Kakade, Kidambi, and
  Sidford]{jain2017parallelizing}
Prateek Jain, Praneeth Netrapalli, Sham~M Kakade, Rahul Kidambi, and Aaron
  Sidford.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8258--8299, 2017.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Rouayheb, Evans, Gardner,
  Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo,
  Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konečný, Korolova,
  Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh,
  Raykova, Qi, Ramage, Raskar, Song, Song, Stich, Sun, Suresh, Tramèr,
  Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{kairouz2019advances}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, Rafael G.~L. D'Oliveira, Salim~El Rouayheb, David Evans,
  Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip~B.
  Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,
  Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
  Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi
  Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
  Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage,
  Ramesh Raskar, Dawn Song, Weikang Song, Sebastian~U. Stich, Ziteng Sun,
  Ananda~Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang,
  Li~Xiong, Zheng Xu, Qiang Yang, Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning, 2019.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J Reddi,
  Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD}: Stochastic controlled averaging for on-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2019better}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Better communication complexity for local sgd.
\newblock \emph{arXiv preprint arXiv:1909.04746}, 2019.

\bibitem[Li et~al.(2014)Li, Zhang, Chen, and Smola]{li2014efficient}
Mu~Li, Tong Zhang, Yuqiang Chen, and Alexander~J Smola.
\newblock Efficient mini-batch training for stochastic optimization.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 661--670. ACM, 2014.

\bibitem[Lin et~al.(2018)Lin, Stich, Patel, and Jaggi]{lin2018don}
Tao Lin, Sebastian~U Stich, Kumar~Kshitij Patel, and Martin Jaggi.
\newblock Don't use large mini-batches, use local sgd.
\newblock \emph{arXiv preprint arXiv:1808.07217}, 2018.

\bibitem[McMahan and Streeter(2010)]{mcmahan2010adaptive}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010}, pages 244--256, 2010.
\newblock URL
  \url{http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf\#page=252}.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovskyyudin1983}
Arkadii~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Rosenblatt and Nadler(2016)]{rosenblatt2016optimality}
Jonathan~D Rosenblatt and Boaz Nadler.
\newblock On the optimality of averaging in distributed statistical learning.
\newblock \emph{Information and Inference: A Journal of the IMA}, 5\penalty0
  (4):\penalty0 379--404, 2016.

\bibitem[Shamir and Srebro()]{shamir2014distributed}
Ohad Shamir and Nathan Srebro.
\newblock Distributed stochastic optimization and learning.
\newblock In \emph{2014 52nd Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 850--857. IEEE.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International conference on machine learning}, pages
  1000--1008, 2014.

\bibitem[Simchowitz(2018)]{simchowitz2018randomized}
Max Simchowitz.
\newblock On the randomized complexity of minimizing a convex quadratic
  function.
\newblock \emph{arXiv preprint arXiv:1807.09386}, 2018.

\bibitem[Stich(2018)]{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.
\newblock URL \url{https://arxiv.org/abs/1805.09767}.

\bibitem[Stich(2019)]{stich2019unified}
Sebastian~U Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock \emph{arXiv preprint arXiv:1907.04232}, 2019.

\bibitem[Stich and Karimireddy(2019)]{stich2019error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed communication.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2019.

\bibitem[Vandenberghe(2019)]{vandenbergheLecture}
Lieven Vandenberghe.
\newblock Lecture notes 1 for optimization methods for large-scale systems,
  2019.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{wang2017memory}
Jialei Wang, Weiran Wang, and Nathan Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch-prox.
\newblock \emph{arXiv preprint arXiv:1702.06269}, 2017.
\newblock URL \url{https://arxiv.org/abs/1702.06269}.

\bibitem[Wang and Joshi(2018)]{wang2018cooperative}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock \emph{arXiv preprint arXiv:1808.07576}, 2018.

\bibitem[Woodworth et~al.(2018)Woodworth, Wang, McMahan, and
  Srebro]{woodworth2018graph}
Blake Woodworth, Jialei Wang, Brendan McMahan, and Nathan Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock \emph{arXiv preprint arXiv:1805.10222}, 2018.
\newblock URL \url{https://arxiv.org/abs/1805.10222}.

\bibitem[Yu et~al.(2019)Yu, Yang, and Zhu]{yu2019parallel}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5693--5700, 2019.

\bibitem[Zhang et~al.(2016)Zhang, De~Sa, Mitliagkas, and
  R{\'e}]{zhang2016parallel}
Jian Zhang, Christopher De~Sa, Ioannis Mitliagkas, and Christopher R{\'e}.
\newblock Parallel sgd: When does averaging help?
\newblock \emph{arXiv preprint arXiv:1606.07365}, 2016.

\bibitem[Zhang et~al.(2012)Zhang, Wainwright, and
  Duchi]{zhang2012communication}
Yuchen Zhang, Martin~J Wainwright, and John~C Duchi.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1502--1510, 2012.

\bibitem[Zhang et~al.(2013{\natexlab{a}})Zhang, Duchi, and
  Wainwright]{zhang2013divide}
Yuchen Zhang, John Duchi, and Martin Wainwright.
\newblock Divide and conquer kernel ridge regression.
\newblock In \emph{Conference on learning theory}, pages 592--617,
  2013{\natexlab{a}}.

\bibitem[Zhang et~al.(2013{\natexlab{b}})Zhang, Duchi, and
  Wainwright]{zhang2013communication}
Yuchen Zhang, John~C Duchi, and Martin~J Wainwright.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 3321--3363, 2013{\natexlab{b}}.

\bibitem[Zhou and Cong(2018)]{Zhou2018:Kaveraging}
Fan Zhou and Guojing Cong.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence, {IJCAI-18}}, pages 3219--3227.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018.
\newblock \doi{10.24963/ijcai.2018/447}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2018/447}.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  2595--2603, 2010.

\end{thebibliography}
