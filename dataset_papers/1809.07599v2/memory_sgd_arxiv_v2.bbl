\begin{thebibliography}{10}

\bibitem{Aji2017:topk}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 440--445. Association for Computational
  Linguistics, 2017.

\bibitem{alistarh2018async}
Dan Alistarh, Christopher De~Sa, and Nikola Konstantinov.
\newblock The convergence of stochastic gradient descent in asynchronous shared
  memory.
\newblock In {\em Proceedings of the 2018 ACM Symposium on Principles of
  Distributed Computing}, PODC '18, pages 169--178, New York, NY, USA, 2018.
  ACM.

\bibitem{Alistarh2017:qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em NIPS - Advances in Neural
  Information Processing Systems 30}, pages 1709--1720. Curran Associates,
  Inc., 2017.

\bibitem{alistarh2018nips}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In {\em NIPS 2018, to appear and CoRR abs/1809.10505}, 2018.

\bibitem{Bottou2010:sgd}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In Yves Lechevallier and Gilbert Saporta, editors, {\em Proceedings
  of COMPSTAT'2010}, pages 177--186, Heidelberg, 2010. Physica-Verlag HD.

\bibitem{bottou2012stochastic}
Leon Bottou.
\newblock {\em Stochastic Gradient Descent Tricks}, volume 7700, page
  430â€“445.
\newblock Springer, January 2012.

\bibitem{Chen2018:adacomp}
Chia{-}Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang, and
  Kailash Gopalakrishnan.
\newblock Adacomp : Adaptive residual gradient compression for data-parallel
  distributed training.
\newblock In Sheila~A. McIlraith and Kilian~Q. Weinberger, editors, {\em
  Proceedings of the Thirty-Second {AAAI} Conference on Artificial
  Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018}. {AAAI} Press,
  2018.

\bibitem{Cordonnier2018thesis}
Jean-Baptiste Cordonnier.
\newblock Convex optimization using sparsified stochastic gradient descent with
  memory.
\newblock Master's thesis, EPFL, Lausanne, Switzerland, 2018.

\bibitem{Dryden2016:topk}
Nikoli Dryden, Sam~Ade Jacobs, Tim Moon, and Brian Van~Essen.
\newblock Communication quantization for data-parallel training of deep neural
  networks.
\newblock In {\em Proceedings of the Workshop on Machine Learning in High
  Performance Computing Environments}, MLHPC '16, pages 1--8, Piscataway, NJ,
  USA, 2016. IEEE Press.

\bibitem{duchi2011adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em JMLR}, 12:2121--2159, August 2011.

\bibitem{Dunner2017:gpu}
Celestine D\"{u}nner, Thomas Parnell, and Martin Jaggi.
\newblock Efficient use of limited-memory accelerators for linear learning on
  heterogeneous systems.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em NIPS - Advances in Neural
  Information Processing Systems 30}, pages 4258--4267. Curran Associates,
  Inc., 2017.

\bibitem{Goyal2017:large}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training {ImageNet} in 1 hour.
\newblock {\em CoRR}, abs/1706.02677, 2017.

\bibitem{Gupta:2015limited}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In {\em Proceedings of the 32Nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, pages
  1737--1746. JMLR.org, 2015.

\bibitem{hsieh2015passcode}
Cho-Jui Hsieh, Hsiang-Fu Yu, and Inderjit Dhillon.
\newblock Passcode: Parallel asynchronous stochastic dual co-ordinate descent.
\newblock In {\em International Conference on Machine Learning}, pages
  2370--2379, 2015.

\bibitem{scipy}
Eric Jones, Travis Oliphant, Pearu Peterson, et~al.
\newblock {SciPy}: Open source scientific tools for {Python}, 2001--.

\bibitem{Kingma2014:adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{Lacoste2012:simpler}
Simon Lacoste-Julien, Mark~W. Schmidt, and Francis~R. Bach.
\newblock A simpler approach to obtaining an {$O(1/t)$} convergence rate for
  the projected stochastic subgradient method.
\newblock {\em CoRR}, abs/1212.2002, 2012.

\bibitem{asaga}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock {ASAGA}: Asynchronous parallel {SAGA}.
\newblock In Aarti Singh and Jerry Zhu, editors, {\em Proceedings of the 20th
  International Conference on Artificial Intelligence and Statistics},
  volume~54 of {\em Proceedings of Machine Learning Research}, pages 46--54,
  Fort Lauderdale, FL, USA, 20--22 Apr 2017. PMLR.

\bibitem{Leblond:2018uv}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock Improved asynchronous parallel optimization analysis for stochastic
  incremental methods.
\newblock {\em CoRR}, abs/1801.03749, January 2018.

\bibitem{rcv01-dataset}
David~D. Lewis, Yiming Yang, Tony~G. Rose, and Fan Li.
\newblock {RCV1:} {A} new benchmark collection for text categorization
  research.
\newblock {\em Journal of Machine Learning Research}, 5:361--397, 2004.

\bibitem{Lin2018:deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In {\em ICLR 2018 - International Conference on Learning
  Representations}, 2018.

\bibitem{Mania2017:perturbed}
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
  Ramchandran, and Michael~I. Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock {\em SIAM Journal on Optimization}, 27(4):2202--2229, 2017.

\bibitem{Moulines2011:nonasymptotic}
Eric Moulines and Francis~R. Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, {\em NIPS - Advances in Neural Information
  Processing Systems 24}, pages 451--459. Curran Associates, Inc., 2011.

\bibitem{Na2017:limitedprecision}
Taesik Na, Jong~Hwan Ko, Jaeha Kung, and Saibal Mukhopadhyay.
\newblock On-chip training of recurrent neural networks with limited numerical
  precision.
\newblock {\em 2017 International Joint Conference on Neural Networks (IJCNN)},
  pages 3716--3723, 2009.

\bibitem{Niu2011:hogwild}
Feng Niu, Benjamin Recht, Christopher Re, and Stephen~J. Wright.
\newblock {HOGWILD!}: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em NIPS - Proceedings of the 24th International Conference on
  Neural Information Processing Systems}, NIPS'11, pages 693--701, USA, 2011.
  Curran Associates Inc.

\bibitem{scikit-learn}
Fabian Pedregosa, Ga{\"e}l Varoquaux, Alexandre Gramfort, Vincent Michel,
  Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
  Weiss, Vincent Dubourg, et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock {\em Journal of machine learning research}, 12(Oct):2825--2830, 2011.

\bibitem{Polyak1992:averaging}
Boris~T. Polyak and Anatoli~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM Journal on Control and Optimization}, 30(4):838--855, 1992.

\bibitem{Rakhlin2012suffix}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, ICML'12, pages 1571--1578,
  USA, 2012. Omnipress.

\bibitem{Robbins:1951sgd}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock {\em The Annals of Mathematical Statistics}, 22(3):400--407,
  September 1951.

\bibitem{Ruppert1988:efficient}
David Ruppert.
\newblock Efficient estimations from a slowly convergent {Robbins-Monro}
  process.
\newblock Technical report, Cornell University Operations Research and
  Industrial Engineering, 1988.

\bibitem{Sa2015taming}
Christopher~De Sa, Ce~Zhang, Kunle Olukotun, and Christopher R{\'e}.
\newblock Taming the wild: A unified analysis of {HOGWILD!}-style algorithms.
\newblock In {\em NIPS - Proceedings of the 28th International Conference on
  Neural Information Processing Systems}, NIPS'15, pages 2674--2682, Cambridge,
  MA, USA, 2015. MIT Press.

\bibitem{Schmidt:2017:MFS:3057222.3057251}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Math. Program.}, 162(1-2):83--112, March 2017.

\bibitem{Seide2015:1bit}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech {DNN}s.
\newblock In Haizhou Li, Helen~M. Meng, Bin Ma, Engsiong Chng, and Lei Xie,
  editors, {\em INTERSPEECH}, pages 1058--1062. ISCA, 2014.

\bibitem{Shamir2013:averaging}
Ohad Shamir and Tong Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In Sanjoy Dasgupta and David McAllester, editors, {\em Proceedings of
  the 30th International Conference on Machine Learning}, volume~28 of {\em
  Proceedings of Machine Learning Research}, pages 71--79, Atlanta, Georgia,
  USA, 17--19 Jun 2013. PMLR.

\bibitem{epsilon-dataset}
Soren Sonnenburg, Vojtvech Franc, E.~Yom-Tov, and M.~Sebag.
\newblock Pascal large scale learning challenge.
\newblock 10:1937--1953, 01 2008.

\bibitem{Stich2018:localSGD}
Sebastian~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock {\em CoRR}, abs/1805.09767, May 2018.

\bibitem{Strom2015:1bit}
Nikko Strom.
\newblock Scalable distributed {DNN} training using commodity {GPU} cloud
  computing.
\newblock In {\em INTERSPEECH}, pages 1488--1492. ISCA, 2015.

\bibitem{Sun2017:sparse}
Xu~Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang.
\newblock me{P}rop: Sparsified back propagation for accelerated deep learning
  with reduced overfitting.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 3299--3308, International
  Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem{tang2018decentralized}
Hanlin Tang, Shaoduo Gan, Ce~Zhang, and Ji~Liu.
\newblock Communication compression for decentralized training.
\newblock In {\em NIPS 2018, to appear and CoRR abs/1803.06443}, 2018.

\bibitem{Wangni2018:sparsification}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em NIPS 2018, to appear and CoRR abs/1710.09854}, 2018.

\bibitem{Wen2017:terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em NIPS - Advances in Neural
  Information Processing Systems 30}, pages 1509--1519. Curran Associates,
  Inc., 2017.

\bibitem{Wu:2018vt}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized {SGD} and its applications to large-scale
  distributed optimization.
\newblock In {\em ICML 2018 - Proceedings of the 35th International Conference
  on Machine Learning}, pages 5321--5329, July 2018.

\bibitem{You2017:scaling}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling {SGD} batch size to 32k for {ImageNet} training.
\newblock {\em CoRR}, abs/1708.03888, 2017.

\bibitem{Zhang2017:zipml}
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji~Liu, and Ce~Zhang.
\newblock {Z}ip{ML}: Training linear models with end-to-end low precision, and
  a little bit of deep learning.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 4035--4043, International
  Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem{Zhang2012:communication}
Yuchen Zhang, Martin~J Wainwright, and John~C Duchi.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em NIPS - Advances in Neural Information Processing Systems 25},
  pages 1502--1510. Curran Associates, Inc., 2012.

\bibitem{Zhao2015:importance}
Peilin Zhao and Tong Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In Francis Bach and David Blei, editors, {\em Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of {\em Proceedings
  of Machine Learning Research}, pages 1--9, Lille, France, 07--09 Jul 2015.
  PMLR.

\end{thebibliography}
