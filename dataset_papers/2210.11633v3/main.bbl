\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den
  Berg]{austin2021structured}
Austin, J., Johnson, D.~D., Ho, J., Tarlow, D., and van~den Berg, R.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17981--17993, 2021.

\bibitem[Baydin et~al.(2017)Baydin, Pearlmutter, Radul, and
  Siskind]{baydinAutomaticDifferentiationMachine2017}
Baydin, A.~G., Pearlmutter, B.~A., Radul, A.~A., and Siskind, J.~M.
\newblock Automatic {{Differentiation}} in {{Machine Learning}}: A {{Survey}}.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0
  153:1--153:43, 2017.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and
  Cohan]{beltagyLongformerLongDocumentTransformer2020}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: {{The Long-Document Transformer}}, December 2020.

\bibitem[{Bloem-Reddy} \& Teh(2020){Bloem-Reddy} and
  Teh]{bloem-reddyProbabilisticSymmetriesInvariant2020}
{Bloem-Reddy}, B. and Teh, Y.~W.
\newblock Probabilistic {{Symmetries}} and {{Invariant Neural Networks}}.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (90):\penalty0 1--61, 2020.
\newblock ISSN 1533-7928.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, {Herbert-Voss}, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brownLanguageModelsAre2020}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  {Herbert-Voss}, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language {{Models}} are {{Few-Shot Learners}}.
\newblock \emph{arXiv:2005.14165 [cs]}, July 2020.

\bibitem[Chaudhuri et~al.(2021)Chaudhuri, Ellis, Polozov, Singh,
  {Solar-Lezama}, and Yue]{chaudhuriNeurosymbolicProgramming2021}
Chaudhuri, S., Ellis, K., Polozov, O., Singh, R., {Solar-Lezama}, A., and Yue,
  Y.
\newblock Neurosymbolic {{Programming}}.
\newblock \emph{Foundations and Trends\textregistered{} in Programming
  Languages}, 7\penalty0 (3):\penalty0 158--243, December 2021.
\newblock ISSN 2325-1107, 2325-1131.
\newblock \doi{10.1561/2500000049}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin,
  Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such,
  Cummings, Plappert, Chantzis, Barnes, {Herbert-Voss}, Guss, Nichol, Paino,
  Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam,
  Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew,
  Amodei, McCandlish, Sutskever, and Zaremba]{chenEvaluatingLargeLanguage2021}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J.,
  Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger,
  G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S.,
  Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C.,
  Tillet, P., Such, F.~P., Cummings, D., Plappert, M., Chantzis, F., Barnes,
  E., {Herbert-Voss}, A., Guss, W.~H., Nichol, A., Paino, A., Tezak, N., Tang,
  J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,
  A.~N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight,
  M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei,
  D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating {{Large Language Models Trained}} on {{Code}}, July 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{childGeneratingLongSequences2019}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating {{Long Sequences}} with {{Sparse Transformers}}, April
  2019.

\bibitem[Cormen(2009)]{cormenIntroductionAlgorithms2009}
Cormen, T.~H. (ed.).
\newblock \emph{Introduction to Algorithms}.
\newblock {MIT Press}, {Cambridge, Mass}, 3rd ed edition, 2009.
\newblock ISBN 978-0-262-03384-8 978-0-262-53305-8.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{daiTransformerXLAttentiveLanguage2019}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-{{XL}}: {{Attentive Language Models Beyond}} a
  {{Fixed-Length Context}}, June 2019.

\bibitem[Freivalds \& Kozlovics(2022)Freivalds and
  Kozlovics]{freivalds2022denoising}
Freivalds, K. and Kozlovics, S.
\newblock Denoising diffusion for sampling sat solutions.
\newblock \emph{arXiv preprint arXiv:2212.00121}, 2022.

\bibitem[Gershman \& Goodman(2014)Gershman and
  Goodman]{gershmanAmortizedInferenceProbabilistic2014}
Gershman, S. and Goodman, N.
\newblock Amortized inference in probabilistic reasoning.
\newblock In \emph{Proceedings of the {{Cognitive Science Society}}},
  volume~36, 2014.

\bibitem[Harvey et~al.(2022)Harvey, Naderiparizi, Masrani, Weilbach, and
  Wood]{harveyFlexibleDiffusionModeling2022}
Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., and Wood, F.
\newblock Flexible {{Diffusion Modeling}} of {{Long Videos}}, May 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{heDeepResidualLearning2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep {{Residual Learning}} for {{Image Recognition}}.
\newblock In \emph{2016 {{IEEE Conference}} on {{Computer Vision}} and
  {{Pattern Recognition}}, {{CVPR}} 2016, {{Las Vegas}}, {{NV}}, {{USA}},
  {{June}} 27-30, 2016}, pp.\  770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hendrycks \& Gimpel(2020)Hendrycks and
  Gimpel]{hendrycksGaussianErrorLinear2020}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian {{Error Linear Units}} ({{GELUs}}), July 2020.

\bibitem[Ho et~al.(2019)Ho, Chen, Srinivas, Duan, and Abbeel]{ho2019flow++}
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2722--2730. PMLR, 2019.

\bibitem[Ho et~al.(2020)Ho, Jain, and
  Abbeel]{hoDenoisingDiffusionProbabilistic2020a}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  6840--6851. {Curran Associates, Inc.}, 2020.

\bibitem[Ho et~al.(2022)Ho, Salimans, Gritsenko, Chan, Norouzi, and
  Fleet]{hoVideoDiffusionModels2022}
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D.~J.
\newblock Video {{Diffusion Models}}.
\newblock \emph{arXiv:2204.03458 [cs]}, April 2022.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Gritsenko, Bastings, Poole, Berg, and
  Salimans]{hoogeboom2021autoregressive}
Hoogeboom, E., Gritsenko, A.~A., Bastings, J., Poole, B., Berg, R. v.~d., and
  Salimans, T.
\newblock Autoregressive diffusion models.
\newblock \emph{arXiv preprint arXiv:2110.02037}, 2021.

\bibitem[Hoogeboom et~al.(2022)Hoogeboom, Satorras, Vignac, and
  Welling]{hoogeboom2022equivariant}
Hoogeboom, E., Satorras, V.~G., Vignac, C., and Welling, M.
\newblock Equivariant diffusion for molecule generation in 3d.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8867--8887. PMLR, 2022.

\bibitem[Hutter et~al.(2018)Hutter, Kotthoff, and
  Vanschoren]{hutterAutomaticMachineLearning2018}
Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.).
\newblock \emph{Automatic {{Machine Learning}}: {{Methods}}, {{Systems}},
  {{Challenges}}}.
\newblock {Springer}, 2018.

\bibitem[Ivanov et~al.(2019)Ivanov, Figurnov, and
  Vetrov]{ivanovVariationalAutoencoderArbitrary2019}
Ivanov, O., Figurnov, M., and Vetrov, D.
\newblock Variational {{Autoencoder}} with {{Arbitrary Conditioning}}.
\newblock \emph{arXiv:1806.02382 [cs, stat]}, June 2019.

\bibitem[Karpathy(2017)]{karpathy_2017}
Karpathy, A., Nov 2017.
\newblock URL \url{https://karpathy.medium.com/software-2-0-a64152b37c35}.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and
  Laine]{karras2022elucidating}
Karras, T., Aittala, M., Aila, T., and Laine, S.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock \emph{arXiv preprint arXiv:2206.00364}, 2022.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingmaAdamMethodStochastic2015}
Kingma, D.~P. and Ba, J.
\newblock Adam: {{A}} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {{ICLR}} 2015, San Diego, {{CA}},
  {{USA}}, May 7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and
  Levskaya]{kitaevReformerEfficientTransformer2020}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: {{The Efficient Transformer}}.
\newblock \emph{arXiv:2001.04451 [cs, stat]}, February 2020.

\bibitem[Koller \& Friedman(2009)Koller and
  Friedman]{kollerProbabilisticGraphicalModels2009a}
Koller, D. and Friedman, N.
\newblock \emph{Probabilistic Graphical Models: Principles and Techniques}.
\newblock Adaptive Computation and Machine Learning. {MIT Press}, {Cambridge,
  MA}, 2009.
\newblock ISBN 978-0-262-01319-2.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'e}tourneau, and
  Tossou]{kreuzer2021rethinking}
Kreuzer, D., Beaini, D., Hamilton, W., L{\'e}tourneau, V., and Tossou, P.
\newblock Rethinking graph transformers with spectral attention.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21618--21629, 2021.

\bibitem[Le et~al.(2017{\natexlab{a}})Le, Baydin, and
  Wood]{leInferenceCompilationUniversal2017}
Le, T.~A., Baydin, A.~G., and Wood, F.
\newblock Inference {{Compilation}} and {{Universal Probabilistic
  Programming}}.
\newblock In \emph{Proceedings of the 20th {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})}, volume~54 of
  \emph{Proceedings of {{Machine Learning Research}}}, pp.\  1338--1348, {Fort
  Lauderdale, FL, USA}, 2017{\natexlab{a}}. {PMLR}.

\bibitem[Le et~al.(2017{\natexlab{b}})Le, Baydin, Zinkov, and
  Wood]{le2017using}
Le, T.~A., Baydin, A.~G., Zinkov, R., and Wood, F.
\newblock Using synthetic data to train neural networks is model-based
  reasoning.
\newblock In \emph{2017 international joint conference on neural networks
  (IJCNN)}, pp.\  3514--3521. IEEE, 2017{\natexlab{b}}.

\bibitem[Marsland(2009)]{marslandMachineLearningAlgorithmic2009}
Marsland, S.
\newblock \emph{Machine Learning - an Algorithmic Perspective}.
\newblock Chapman and {{Hall}} / {{CRC}} Machine Learning and Pattern
  Recognition Series. {CRC Press}, 2009.
\newblock ISBN 978-1-4200-6718-7.

\bibitem[OpenAI(2021)]{chatgpt}
OpenAI.
\newblock Chatgpt.
\newblock \url{https://chat.openai.com}, 2021.
\newblock Accessed on March 17, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P.,
  Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Palm et~al.(2018)Palm, Paquet, and
  Winther]{palmRecurrentRelationalNetworks2018}
Palm, R., Paquet, U., and Winther, O.
\newblock Recurrent {{Relational Networks}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~31. {Curran Associates, Inc.}, 2018.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.]{pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{the Journal of machine Learning research}, 12:\penalty0
  2825--2830, 2011.

\bibitem[Ritchie et~al.(2016)Ritchie, Horsfall, and
  Goodman]{ritchieDeepAmortizedInference2016}
Ritchie, D., Horsfall, P., and Goodman, N.~D.
\newblock Deep {{Amortized Inference}} for {{Probabilistic Programs}}.
\newblock \emph{arXiv:1610.05735 [cs, stat]}, October 2016.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and
  Grangier]{royEfficientContentBasedSparse2020}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient {{Content-Based Sparse Attention}} with {{Routing
  Transformers}}, October 2020.

\bibitem[Russell \& Norvig(2010)Russell and
  Norvig]{stuartrussellArtificialIntelligenceModern2010}
Russell, S. and Norvig, P.
\newblock \emph{Artificial Intelligence: {{A Modern Approach}}}.
\newblock Prentice Hall Series in Artificial Intelligence. {Prentice Hall}, 3rd
  edition, 2010.
\newblock ISBN 0-13-604259-7 978-0-13-604259-4.

\bibitem[Salimans \& Ho(2022)Salimans and Ho]{salimans2022progressive}
Salimans, T. and Ho, J.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2202.00512}, 2022.

\bibitem[Selsam et~al.(2019)Selsam, Lamm, B{\"u}nz, Liang, {de Moura}, and
  Dill]{selsamLearningSATSolver2019}
Selsam, D., Lamm, M., B{\"u}nz, B., Liang, P., {de Moura}, L., and Dill, D.~L.
\newblock Learning a {{SAT Solver}} from {{Single-Bit Supervision}}.
\newblock \emph{arXiv:1802.03685 [cs]}, March 2019.

\bibitem[{Sohl-Dickstein} et~al.(2015){Sohl-Dickstein}, Weiss, Maheswaranathan,
  and Ganguli]{sohl-dicksteinDeepUnsupervisedLearning2015}
{Sohl-Dickstein}, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.
\newblock Deep {{Unsupervised Learning}} using {{Nonequilibrium
  Thermodynamics}}.
\newblock In \emph{Proceedings of the 32nd {{International Conference}} on
  {{Machine Learning}}}, pp.\  2256--2265. {PMLR}, June 2015.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Meng, and
  Ermon]{songDenoisingDiffusionImplicit2021}
Song, J., Meng, C., and Ermon, S.
\newblock Denoising {{Diffusion Implicit Models}}.
\newblock \emph{arXiv:2010.02502 [cs]}, November 2021{\natexlab{a}}.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Durkan, Murray, and
  Ermon]{songMaximumLikelihoodTraining2021a}
Song, Y., Durkan, C., Murray, I., and Ermon, S.
\newblock Maximum {{Likelihood Training}} of {{Score-Based Diffusion Models}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~34, pp.\  1415--1428. {Curran Associates, Inc.}, 2021{\natexlab{b}}.

\bibitem[Song et~al.(2021{\natexlab{c}})Song, {Sohl-Dickstein}, Kingma, Kumar,
  Ermon, and Poole]{songScoreBasedGenerativeModeling2021}
Song, Y., {Sohl-Dickstein}, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole,
  B.
\newblock Score-{{Based Generative Modeling}} through {{Stochastic Differential
  Equations}}.
\newblock \emph{arXiv:2011.13456 [cs, stat]}, February 2021{\natexlab{c}}.

\bibitem[Tashiro et~al.(2021)Tashiro, Song, Song, and Ermon]{tashiro2021csdi}
Tashiro, Y., Song, J., Song, Y., and Ermon, S.
\newblock {{CSDI}}: {{Conditional}} score-based diffusion models for
  probabilistic time series imputation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and
  Metzler]{tayEfficientTransformersSurvey2022}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient {{Transformers}}: {{A Survey}}, March 2022.

\bibitem[T{\"o}nshoff et~al.(2022)T{\"o}nshoff, Kisin, Lindner, and
  Grohe]{tonshoffOneModelAny2022}
T{\"o}nshoff, J., Kisin, B., Lindner, J., and Grohe, M.
\newblock One {{Model}}, {{Any CSP}}: {{Graph Neural Networks}} as {{Fast
  Global Search Heuristics}} for {{Constraint Satisfaction}}.
\newblock August 2022.
\newblock \doi{10.48550/arXiv.2208.10227}.

\bibitem[{van de Meent} et~al.(2018){van de Meent}, Paige, Yang, and
  Wood]{vandemeentIntroductionProbabilisticProgramming2018}
{van de Meent}, J.-W., Paige, B., Yang, H., and Wood, F.
\newblock An {{Introduction}} to {{Probabilistic Programming}}.
\newblock \emph{arXiv:1809.10756 [cs, stat]}, September 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswaniAttentionAllYou2017a}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is {{All}} you {{Need}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~30. {Curran Associates, Inc.}, 2017.

\bibitem[Wang et~al.(2019)Wang, Donti, Wilder, and
  Kolter]{wangSATNetBridgingDeep2019}
Wang, P.-W., Donti, P.~L., Wilder, B., and Kolter, Z.
\newblock {{SATNet}}: {{Bridging}} deep learning and logical reasoning using a
  differentiable satisfiability solver.
\newblock \emph{arXiv:1905.12149 [cs, stat]}, May 2019.

\bibitem[Webb et~al.(2017)Webb, Golinski, Zinkov, Siddharth, Rainforth, Teh,
  and Wood]{webbFaithfulInversionGenerative2017}
Webb, S., Golinski, A., Zinkov, R., Siddharth, N., Rainforth, T., Teh, Y.~W.,
  and Wood, F.
\newblock Faithful {{Inversion}} of {{Generative Models}} for {{Effective
  Amortized Inference}}.
\newblock December 2017.

\bibitem[Weilbach et~al.(2020)Weilbach, Beronov, Wood, and
  Harvey]{weilbachStructuredConditionalContinuous2020}
Weilbach, C., Beronov, B., Wood, F., and Harvey, W.
\newblock Structured conditional continuous normalizing flows for efficient
  amortized inference in graphical models.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  4441--4451. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/weilbach20a.html}.

\bibitem[Williamson \& Shmoys(2011)Williamson and
  Shmoys]{williamsonDesignApproximationAlgorithms2011a}
Williamson, D.~P. and Shmoys, D.~B.
\newblock \emph{The {{Design}} of {{Approximation Algorithms}}}.
\newblock {Cambridge University Press}, {Cambridge}, 2011.
\newblock ISBN 978-0-511-92173-5.
\newblock \doi{10.1017/CBO9780511921735}.

\bibitem[Wingate et~al.(2011)Wingate, Stuhlm{\"u}ller, and
  Goodman]{wingateLightweightImplementationsProbabilistic}
Wingate, D., Stuhlm{\"u}ller, A., and Goodman, N.~D.
\newblock Lightweight {{Implementations}} of {{Probabilistic Programming
  Languages Via Transformational Compilation}}.
\newblock pp.\ ~9, 2011.

\bibitem[Zaheer et~al.(2021)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed]{zaheerBigBirdTransformers2021}
Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S.,
  Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A.
\newblock Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}, January
  2021.

\bibitem[Zhou et~al.(2020)Zhou, Cui, Hu, Zhang, Yang, Liu, Wang, Li, and
  Sun]{zhou2020graph}
Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and
  Sun, M.
\newblock Graph neural networks: A review of methods and applications.
\newblock \emph{AI open}, 1:\penalty0 57--81, 2020.

\end{thebibliography}
