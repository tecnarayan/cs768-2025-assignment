\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop(1995)]{bishop1995neural}
Christopher~M Bishop.
\newblock \emph{Neural Networks for Pattern Recognition}.
\newblock Oxford university press, 1995.

\bibitem[Bishop(2006)]{bishop}
Christopher~M Bishop.
\newblock \emph{Pattern Recognition and Machine Learning.}
\newblock Springer-Verlag New York, 2006.

\bibitem[Bridle(1990{\natexlab{a}})]{bridle1990training}
John~S Bridle.
\newblock Training stochastic model recognition algorithms as networks can lead
  to maximum mutual information estimation of parameters.
\newblock In \emph{Proc.\ NIPS}, pages 211--217, 1990{\natexlab{a}}.

\bibitem[Bridle(1990{\natexlab{b}})]{softmax}
John~S Bridle.
\newblock Probabilistic interpretation of feedforward classification network
  outputs, with relationships to statistical pattern recognition.
\newblock In \emph{Neurocomputing}, pages 227--236. Springer,
  1990{\natexlab{b}}.

\bibitem[Chen et~al.(2017)Chen, Deng, and Du]{chen2017noisy}
Binghui Chen, Weihong Deng, and Junping Du.
\newblock Noisy softmax: Improving the generalization ability of dcnn via
  postponing the early softmax saturation.
\newblock pages 5372--5381, 2017.

\bibitem[Cho et~al.(2014)Cho, Van~Merri\"{e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho-al-emnlp14}
Kyunghyun Cho, Bart Van~Merri\"{e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder--decoder for
  statistical machine translation.
\newblock In \emph{Proc.\ EMNLP}, pages 1724--1734. ACL, 2014.

\bibitem[de~Br{\'e}bisson and Vincent(2016)]{de2015exploration}
Alexandre de~Br{\'e}bisson and Pascal Vincent.
\newblock An exploration of softmax alternatives belonging to the spherical
  loss family.
\newblock In \emph{Proc.\ ICLR}, 2016.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proc.\ AISTATS}, pages 249--256, 2010.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Grave et~al.(2017)Grave, Joulin, Ciss{\'e}, Grangier, and
  J{\'e}gou]{joulin2017efficient}
{\'E}douard Grave, Armand Joulin, Moustapha Ciss{\'e}, David Grangier, and
  Herv{\'e} J{\'e}gou.
\newblock Efficient softmax approximation for {GPU}s.
\newblock In \emph{Proc.\ ICML}, pages 1302--1310, 2017.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{Proc.\ ICASSP}, pages 6645--6649. IEEE, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock pages 770--778, 2016.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proc.\ ICML}, pages 448--456, 2015.

\bibitem[Krause et~al.(2017)Krause, Kahembwe, Murray, and
  Renals]{krause2017dynamic}
Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals.
\newblock Dynamic evaluation of neural sequence models.
\newblock \emph{arXiv preprint arXiv:1709.07432}, 2017.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Proc.\ NIPS}, pages 1097--1105, 2012.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock \emph{Computational linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Martins and Astudillo(2016)]{martins2016softmax}
Andre Martins and Ramon Astudillo.
\newblock From softmax to sparsemax: A sparse model of attention and
  multi-label classification.
\newblock In \emph{Proc.\ ICML}, pages 1614--1623, 2016.

\bibitem[Memisevic et~al.(2010)Memisevic, Zach, Pollefeys, and
  Hinton]{memisevic2010gated}
Roland Memisevic, Christopher Zach, Marc Pollefeys, and Geoffrey~E Hinton.
\newblock Gated softmax classification.
\newblock In \emph{Proc.\ NIPS}, pages 1603--1611, 2010.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{WT2_2017}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{Proc.\ ICLR}, 2017.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock In \emph{Proc.\ ICLR}, 2018.

\bibitem[Mikolov(2012)]{tomas}
Tomas Mikolov.
\newblock Statistical language models based on neural networks.
\newblock \emph{PhD thesis, Brno University of Technology}, 2012.

\bibitem[Mohassel and Zhang(2017)]{mohassel2017secureml}
Payman Mohassel and Yupeng Zhang.
\newblock Secureml: A system for scalable privacy-preserving machine learning.
\newblock In \emph{Security and Privacy (SP), 2017 IEEE Symposium on}, pages
  19--38. IEEE, 2017.

\bibitem[Nair and Hinton(2010)]{relu}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{Proc.\ ICML}, pages 807--814. Omnipress, 2010.

\bibitem[Ollivier(2013)]{ollivier2013riemannian}
Yann Ollivier.
\newblock Riemannian metrics for neural networks i: feedforward networks.
\newblock \emph{arXiv preprint arXiv:1303.0818}, 2013.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{Proc.\ ICML}, pages 1310--1318, 2013.

\bibitem[Press et~al.(2007)Press, Teukolsky, Vetterling, and
  Flannery]{press2007numerical}
William~H Press, Saul~A Teukolsky, William~T Vetterling, and Brian~P Flannery.
\newblock \emph{Numerical Recipes 3rd Edition: The Art of Scientific
  Computing}.
\newblock Cambridge University Press, 2007.

\bibitem[Shim et~al.(2017)Shim, Lee, Choi, Boo, and Sung]{shim2017svd}
Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo, and Wonyong Sung.
\newblock {SVD}-softmax: Fast softmax approximation on large vocabulary neural
  networks.
\newblock In \emph{Proc.\ NIPS}, pages 5469--5479, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{NIPS2014_5346}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Proc.\ NIPS}, pages 3104--3112. 2014.

\bibitem[Titsias(2016)]{aueb2016one}
Michalis~K. Titsias.
\newblock One-vs-each approximation to softmax for scalable estimation of
  probabilities.
\newblock In \emph{Proc.\ NIPS}, pages 4161--4169, 2016.

\bibitem[Yang et~al.(2018)Yang, Dai, Salakhutdinov, and Cohen]{softmaxbottle}
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W Cohen.
\newblock Breaking the softmax bottleneck: a high-rank rnn language model.
\newblock In \emph{Proc.\ ICLR}, 2018.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\end{thebibliography}
