@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}

@article{nijkamp2022conversational,
  title={A Conversational Paradigm for Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{xu2022systematic,
  title={A Systematic Evaluation of Large Language Models of Code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J},
  journal={arXiv preprint arXiv:2202.13169},
  year={2022}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{shen2021generate,
  title={Generate \& rank: A multi-task framework for math word problems},
  author={Shen, Jianhao and Yin, Yichun and Li, Lin and Shang, Lifeng and Jiang, Xin and Zhang, Ming and Liu, Qun},
  journal={arXiv preprint arXiv:2109.03034},
  year={2021}
  }
  
 @article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{black2022gpt,
  title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}
@article{chandel2022training,
  title={Training and Evaluating a Jupyter Notebook Data Science Assistant},
  author={Chandel, Shubham and Clement, Colin B and Serrato, Guillermo and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2201.12901},
  year={2022}
}
@article{barone2017parallel,
  title={A parallel corpus of python functions and documentation strings for automated code documentation and code generation},
  author={Barone, Antonio Valerio Miceli and Sennrich, Rico},
  journal={arXiv preprint arXiv:1707.02275},
  year={2017}
}
@inproceedings{mou2016convolutional,
  title={Convolutional neural networks over tree structures for programming language processing},
  author={Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}
@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}
@article{ahmad2021unified,
  title={Unified pre-training for program understanding and generation},
  author={Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2103.06333},
  year={2021}
}
@inproceedings{nguyen2015divide,
  title={Divide-and-conquer approach for multi-phase statistical migration for source code (t)},
  author={Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N},
  booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={585--596},
  year={2015},
  organization={IEEE}
}
@article{kulal2019spoc,
  title={Spoc: Search-based pseudocode to code},
  author={Kulal, Sumith and Pasupat, Panupong and Chandra, Kartik and Lee, Mina and Padon, Oded and Aiken, Alex and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{chen2018tree,
  title={Tree-to-tree neural networks for program translation},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{svajlenko2014towards,
  title={Towards a big data curated benchmark of inter-project code clones},
  author={Svajlenko, Jeffrey and Islam, Judith F and Keivanloo, Iman and Roy, Chanchal K and Mia, Mohammad Mamun},
  booktitle={2014 IEEE International Conference on Software Maintenance and Evolution},
  pages={476--480},
  year={2014},
  organization={IEEE}
}
@inproceedings{gu2018deep,
  title={Deep code search},
  author={Gu, Xiaodong and Zhang, Hongyu and Kim, Sunghun},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  pages={933--944},
  year={2018},
  organization={IEEE}
}
@inproceedings{cambronero2019deep,
  title={When deep learning met code search},
  author={Cambronero, Jose and Li, Hongyu and Kim, Seohyun and Sen, Koushik and Chandra, Satish},
  booktitle={Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={964--974},
  year={2019}
}
@article{schuster2021programming,
  title={Programming Puzzles},
  author={Schuster, Tal and Kalyan, Ashwin and Polozov, Oleksandr and Kalai, Adam Tauman},
  journal={arXiv preprint arXiv:2106.05784},
  year={2021}
}

@article{fried2022incoder,
  title={InCoder: A Generative Model for Code Infilling and Synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}
@article{ellis2019write,
  title={Write, execute, assess: Program synthesis with a repl},
  author={Ellis, Kevin and Nye, Maxwell and Pu, Yewen and Sosa, Felix and Tenenbaum, Josh and Solar-Lezama, Armando},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{chen2018execution,
  title={Execution-guided neural program synthesis},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{chen2021latent,
  title={Latent execution for neural program synthesis beyond domain-specific languages},
  author={Chen, Xinyun and Song, Dawn and Tian, Yuandong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{nye2021show,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{mukherjee2021neural,
  title={Neural program generation modulo static analysis},
  author={Mukherjee, Rohan and Wen, Yeming and Chaudhari, Dipak and Reps, Thomas and Chaudhuri, Swarat and Jermaine, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhou2019devign,
  title={Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
  author={Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{chakraborty2021deep,
  title={Deep learning based vulnerability detection: Are we there yet},
  author={Chakraborty, Saikat and Krishna, Rahul and Ding, Yangruibo and Ray, Baishakhi},
  journal={IEEE Transactions on Software Engineering},
  year={2021},
  publisher={IEEE}
}

@article{cheng2021deepwukong,
  title={DeepWukong: Statically detecting software vulnerabilities using deep graph neural network},
  author={Cheng, Xiao and Wang, Haoyu and Hua, Jiayi and Xu, Guoai and Sui, Yulei},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={30},
  number={3},
  pages={1--33},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{bilgin2020vulnerability,
  title={Vulnerability prediction from source code using machine learning},
  author={Bilgin, Zeki and Ersoy, Mehmet Akif and Soykan, Elif Ustundag and Tomur, Emrah and {\c{C}}omak, Pinar and Kara{\c{c}}ay, Leyli},
  journal={IEEE Access},
  volume={8},
  pages={150672--150684},
  year={2020},
  publisher={IEEE}
}

@article{kharkar2022learning,
  title={Learning to Reduce False Positives in Analytic Bug Detectors},
  author={Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Jin, Matthew and Liu, Xiaoyu and Shi, Xin and Clement, Colin and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2203.09907},
  year={2022}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@MISC{copilot,
author = {},
title = {GitHub Copilot: Your AI pair programmer},
year = {2021},
howpublished={\url{https://copilot.github.com/}}
}