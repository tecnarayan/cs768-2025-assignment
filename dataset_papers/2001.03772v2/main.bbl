\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angluin \& Laird(1988)Angluin and Laird]{Angluin1988}
Angluin, D. and Laird, P.
\newblock Learning from noisy examples.
\newblock \emph{Machine Learning}, 2\penalty0 (4):\penalty0 343--370, 1988.

\bibitem[Arachie \& Huang(2019)Arachie and Huang]{arachie2019adversarial}
Arachie, C. and Huang, B.
\newblock Adversarial label learning.
\newblock In \emph{AAAI}, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Bartlett, P.~L., Jordan, M.~I., and McAuliffe, J.~D.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Bootkrajang \& Chaijaruwanich(2018)Bootkrajang and
  Chaijaruwanich]{bootkrajang_towards_2018}
Bootkrajang, J. and Chaijaruwanich, J.
\newblock Towards instance-dependent label noise-tolerant classification: a
  probabilistic approach.
\newblock \emph{Pattern Analysis and Applications}, pp.\  1--17, 2018.

\bibitem[Branson et~al.(2017)Branson, Van~Horn, and Perona]{branson2017lean}
Branson, S., Van~Horn, G., and Perona, P.
\newblock Lean crowdsourcing: Combining humans and machines in an online
  system.
\newblock In \emph{CVPR}, 2017.

\bibitem[Charoenphakdee et~al.(2019)Charoenphakdee, Lee, and
  Sugiyama]{charoenphakdee_symmetric_2019}
Charoenphakdee, N., Lee, J., and Sugiyama, M.
\newblock On {Symmetric} {Losses} for {Learning} from {Corrupted} {Labels}.
\newblock \emph{ICML}, 2019.

\bibitem[Cheng et~al.(2020{\natexlab{a}})Cheng, Zhu, Li, Gong, Sun, and
  Liu]{cheng2020sieve}
Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., and Liu, Y.
\newblock Learning with instance-dependent label noise: A sample sieve
  approach.
\newblock \emph{arXiv preprint arXiv:2010.02347}, 2020{\natexlab{a}}.

\bibitem[Cheng et~al.(2020{\natexlab{b}})Cheng, Liu, Ramamohanarao, and
  Tao]{cheng2020learning}
Cheng, J., Liu, T., Ramamohanarao, K., and Tao, D.
\newblock Learning with bounded instance and label-dependent label noise.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Du \& Cai(2015)Du and Cai]{du2015modelling}
Du, J. and Cai, Z.
\newblock Modelling class noise with symmetric and asymmetric distributions.
\newblock In \emph{AAAI}, 2015.

\bibitem[Ghosh et~al.(2014)Ghosh, Manwani, and S.~Sastry]{Ghosh_2014}
Ghosh, A., Manwani, N., and S.~Sastry, P.
\newblock Making risk minimization tolerant to label noise.
\newblock \emph{Neurocomputing}, 160, 2014.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Ghosh, A., Kumar, H., and Sastry, P.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{AAAI}, 2017.

\bibitem[Gneiting \& Raftery(2007)Gneiting and Raftery]{gneiting2007strictly}
Gneiting, T. and Raftery, A.~E.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102\penalty0
  (477):\penalty0 359--378, 2007.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2016training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Guan et~al.(2018)Guan, Gulshan, Dai, and Hinton]{guan2018said}
Guan, M.~Y., Gulshan, V., Dai, A.~M., and Hinton, G.~E.
\newblock Who said what: Modeling individual labelers improves classification.
\newblock In \emph{AAAI}, 2018.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han_co-teaching:_2018}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Ishida et~al.(2018)Ishida, Niu, and Sugiyama]{ishida2018binary}
Ishida, T., Niu, G., and Sugiyama, M.
\newblock Binary classification from positive-confidence data.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang_mentornet:_2017}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Kearns(1993)]{kearns_1993}
Kearns, M.
\newblock Efficient noise-tolerant learning from statistical queries.
\newblock \emph{Proceedings of the twenty-fifth annual ACM symposium on Theory
  of computing - STOC 93}, 1993.

\bibitem[Khetan et~al.(2018)Khetan, Lipton, and Anandkumar]{khetan2018learning}
Khetan, A., Lipton, Z.~C., and Anandkumar, A.
\newblock Learning from noisy singly-labeled data.
\newblock In \emph{ICLR}, 2018.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Krizhevsky, A. et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Laine \& Aila(2017)Laine and Aila]{laine_temporal_2016}
Laine, S. and Aila, T.
\newblock Temporal {Ensembling} for {Semi}-{Supervised} {Learning}.
\newblock \emph{ICLR}, 2017.

\bibitem[Liu \& Tao(2015)Liu and Tao]{liu2015classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2015.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Ma, X., Wang, Y., Houle, M.~E., Zhou, S., Erfani, S., Xia, S., Wijewickrema,
  S., and Bailey, J.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Manwani \& Sastry(2013)Manwani and Sastry]{Manwani2013NoiseTU}
Manwani, N. and Sastry, P.~S.
\newblock Noise tolerance under risk minimization.
\newblock \emph{IEEE Transactions on Cybernetics}, 43:\penalty0 1146--1151,
  2013.

\bibitem[Masnadi-shirazi \& Vasconcelos(2009)Masnadi-shirazi and
  Vasconcelos]{masnadi-shirazi_design_2009}
Masnadi-shirazi, H. and Vasconcelos, N.
\newblock On the {Design} of {Loss} {Functions} for {Classification}: theory,
  robustness to outliers, and {SavageBoost}.
\newblock In \emph{NeurIPS}. 2009.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Menon, A., Van~Rooyen, B., Ong, C.~S., and Williamson, B.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{ICML}, pp.\  125--134, 2015.

\bibitem[Menon et~al.(2016)Menon, Van~Rooyen, and Natarajan]{menon2016learning}
Menon, A.~K., Van~Rooyen, B., and Natarajan, N.
\newblock Learning from binary labels with instance-dependent corruption.
\newblock \emph{arXiv preprint arXiv:1605.00751}, 2016.

\bibitem[Menon et~al.(2018)Menon, van Rooyen, and
  Natarajan]{menon_learning_2018}
Menon, A.~K., van Rooyen, B., and Natarajan, N.
\newblock Learning from binary labels with instance-dependent noise.
\newblock \emph{Machine Learning}, 107\penalty0 (8-10):\penalty0 1561--1595,
  September 2018.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Miyato, T., Maeda, S.-i., Koyama, M., and Ishii, S.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan_learning_2013}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with {Noisy} {Labels}.
\newblock In \emph{NeurIPS}. 2013.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Nock \& Nielsen(2009)Nock and Nielsen]{nock2009efficient}
Nock, R. and Nielsen, F.
\newblock On the efficient minimization of classification calibrated
  surrogates.
\newblock In \emph{NeurIPS}, 2009.

\bibitem[Patrini et~al.(2016)Patrini, Nielsen, Nock, and
  Carioni]{pmlr-v48-patrini16}
Patrini, G., Nielsen, F., Nock, R., and Carioni, M.
\newblock Loss factorization, weakly supervised learning and label noise
  robustness.
\newblock In \emph{ICML}, 2016.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{CVPR}, 2017.

\bibitem[Ratner et~al.(2020)Ratner, Bach, Ehrenberg, Fries, Wu, and
  R{\'e}]{ratner2020snorkel}
Ratner, A., Bach, S.~H., Ehrenberg, H., Fries, J., Wu, S., and R{\'e}, C.
\newblock Snorkel: Rapid training data creation with weak supervision.
\newblock \emph{The VLDB Journal}, 29\penalty0 (2):\penalty0 709--730, 2020.

\bibitem[Ratner et~al.(2016)Ratner, De~Sa, Wu, Selsam, and
  R{\'e}]{ratner2016data}
Ratner, A.~J., De~Sa, C.~M., Wu, S., Selsam, D., and R{\'e}, C.
\newblock Data programming: Creating large training sets, quickly.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Raykar et~al.(2009)Raykar, Yu, Zhao, Jerebko, Florin, Valadez, Bogoni,
  and Moy]{raykar2009supervised}
Raykar, V.~C., Yu, S., Zhao, L.~H., Jerebko, A., Florin, C., Valadez, G.~H.,
  Bogoni, L., and Moy, L.
\newblock Supervised learning from multiple experts: whom to trust when
  everyone lies a bit.
\newblock In \emph{ICML}, 2009.

\bibitem[Reed et~al.(2015)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{ICLR}, 2015.

\bibitem[Reid \& Williamson(2010)Reid and Williamson]{reid2010composite}
Reid, M.~D. and Williamson, R.~C.
\newblock Composite binary losses.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Sep):\penalty0 2387--2422, 2010.

\bibitem[Scott(2019)]{scott2018generalized}
Scott, C.
\newblock A generalized neyman-pearson criterion for optimal domain adaptation.
\newblock In \emph{ALT}, 2019.

\bibitem[Scott et~al.(2013)Scott, Blanchard, and
  Handy]{scott2013classification}
Scott, C., Blanchard, G., and Handy, G.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In \emph{COLT}, pp.\  489--511, 2013.

\bibitem[Shen \& Sanghavi(2019)Shen and Sanghavi]{shen2019learning}
Shen, Y. and Sanghavi, S.
\newblock Learning with bad training data via iterative trimmed loss
  minimization.
\newblock In \emph{ICML}, 2019.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Shimodaira, H.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Snow et~al.(2008)Snow, O{'}Connor, Jurafsky, and
  Ng]{snow-etal-2008-cheap}
Snow, R., O{'}Connor, B., Jurafsky, D., and Ng, A.
\newblock Cheap and fast {--} but is it good? evaluating non-expert annotations
  for natural language tasks.
\newblock In \emph{EMNLP}, 2008.

\bibitem[Stempfel \& Ralaivola(2009)Stempfel and
  Ralaivola]{stempfel_learning_2009}
Stempfel, G. and Ralaivola, L.
\newblock Learning {SVMs} from sloppily labeled data.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  884--893, 2009.

\bibitem[Sugiyama \& Kawanabe(2012)Sugiyama and
  Kawanabe]{book:Sugiyama+Kawanabe:2012}
Sugiyama, M. and Kawanabe, M.
\newblock \emph{Machine Learning in Non-Stationary Environments: {I}ntroduction
  to Covariate Shift Adaptation}.
\newblock MIT Press, Cambridge, Massachusetts, USA, 2012.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Krauledat, and
  M\"uller]{sugiyama2007covariate}
Sugiyama, M., Krauledat, M., and M\"uller, K.-R.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (May):\penalty0 985--1005, 2007.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Bruna, Paluri, Bourdev, and
  Fergus]{sukhbaatar2014training}
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fergus, R.
\newblock Training convolutional networks with noisy labels.
\newblock \emph{ICLR workshop}, 2015.

\bibitem[Tanaka et~al.(2018)Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018joint}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{CVPR}, 2018.

\bibitem[Tarvainen \& Valpola(2017)Tarvainen and Valpola]{tarvainen2017mean}
Tarvainen, A. and Valpola, H.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[{Tong Xiao} et~al.(2015){Tong Xiao}, {Tian Xia}, {Yi Yang}, {Chang
  Huang}, and {Xiaogang Wang}]{tong_xiao_learning_2015}
{Tong Xiao}, {Tian Xia}, {Yi Yang}, {Chang Huang}, and {Xiaogang Wang}.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{CVPR}, 2015.

\bibitem[{van Rooyen} \& {Williamson}(2015){van Rooyen} and
  {Williamson}]{van_Rooyen_2015}
{van Rooyen}, B. and {Williamson}, R.~C.
\newblock {Learning in the Presence of Corruption}.
\newblock \emph{arXiv e-prints}, art. arXiv:1504.00091, Mar 2015.

\bibitem[Yan et~al.(2010)Yan, Rosales, Fung, Schmidt, Hermosillo, Bogoni, Moy,
  and Dy]{yan2010modeling}
Yan, Y., Rosales, R., Fung, G., Schmidt, M., Hermosillo, G., Bogoni, L., Moy,
  L., and Dy, J.
\newblock Modeling annotator expertise: Learning when everybody knows a bit of
  something.
\newblock In \emph{AISTATS}, 2010.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{ZhangBHRV16}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2004)]{zhang2004statistical}
Zhang, T. et~al.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 56--85,
  2004.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang_generalized_2018}
Zhang, Z. and Sabuncu, M.
\newblock Generalized {Cross} {Entropy} {Loss} for {Training} {Deep} {Neural}
  {Networks} with {Noisy} {Labels}.
\newblock In \emph{NeurIPS}. 2018.

\end{thebibliography}
