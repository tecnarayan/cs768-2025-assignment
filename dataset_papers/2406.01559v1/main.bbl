\begin{thebibliography}{133}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balakrishnan et~al.(2017)Balakrishnan, Wainwright, and
  Yu]{balakrishnan2017statistical}
Balakrishnan, S., Wainwright, M.~J., and Yu, B.
\newblock Statistical guarantees for the em algorithm: From population to
  sample-based analysis.
\newblock \emph{Annals of Statistics}, 2017.

\bibitem[Bertinetto et~al.(2016)Bertinetto, Valmadre, Henriques, Vedaldi, and
  Torr]{bertinetto2016fully}
Bertinetto, L., Valmadre, J., Henriques, J.~F., Vedaldi, A., and Torr, P.~H.
\newblock Fully-convolutional siamese networks for object tracking.
\newblock In \emph{ECCV}, 2016.

\bibitem[Bhat et~al.(2020)Bhat, Danelljan, Van~Gool, and Timofte]{bhat2020know}
Bhat, G., Danelljan, M., Van~Gool, L., and Timofte, R.
\newblock Know your surroundings: Exploiting scene information for object
  tracking.
\newblock In \emph{ECCV}, 2020.

\bibitem[Bhat et~al.(2021)Bhat, Alhashim, and Wonka]{bhat2021adabins}
Bhat, S.~F., Alhashim, I., and Wonka, P.
\newblock Adabins: Depth estimation using adaptive bins.
\newblock In \emph{CVPR}, 2021.

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L.
\newblock Random forests.
\newblock \emph{Machine learning}, 45:\penalty0 5--32, 2001.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Butler et~al.(2012{\natexlab{a}})Butler, Wulff, Stanley, and
  Black]{Butler2012}
Butler, D.~J., Wulff, J., Stanley, G.~B., and Black, M.~J.
\newblock A naturalistic open source movie for optical flow evaluation.
\newblock In \emph{ECCV}, 2012{\natexlab{a}}.

\bibitem[Butler et~al.(2012{\natexlab{b}})Butler, Wulff, Stanley, and
  Black]{butler2012naturalistic}
Butler, D.~J., Wulff, J., Stanley, G.~B., and Black, M.~J.
\newblock A naturalistic open source movie for optical flow evaluation.
\newblock In \emph{ECCV}, 2012{\natexlab{b}}.

\bibitem[Cabon et~al.(2020)Cabon, Murray, and Humenberger]{cabon2020virtual}
Cabon, Y., Murray, N., and Humenberger, M.
\newblock Virtual kitti 2.
\newblock \emph{arXiv preprint arXiv:2001.10773}, 2020.

\bibitem[Cai et~al.(2022)Cai, Lin, Hu, Wang, Yuan, Zhang, Timofte, and
  Van~Gool]{cai2022coarse}
Cai, Y., Lin, J., Hu, X., Wang, H., Yuan, X., Zhang, Y., Timofte, R., and
  Van~Gool, L.
\newblock Coarse-to-fine sparse transformer for hyperspectral image
  reconstruction.
\newblock In \emph{ECCV}, 2022.

\bibitem[Chen et~al.(2021)Chen, Yan, Zhu, Wang, Yang, and
  Lu]{chen2021transformer}
Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., and Lu, H.
\newblock Transformer tracking.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chen et~al.(2023)Chen, Peng, Wang, Lu, and Hu]{chen2023seqtrack}
Chen, X., Peng, H., Wang, D., Lu, H., and Hu, H.
\newblock Seqtrack: Sequence to sequence learning for visual object tracking.
\newblock In \emph{CVPR}, 2023.

\bibitem[Cheng et~al.(2022)Cheng, Liang, Choi, Tao, Cao, Liu, and
  Zhang]{cheng2022physical}
Cheng, Z., Liang, J., Choi, H., Tao, G., Cao, Z., Liu, D., and Zhang, X.
\newblock Physical attack on monocular depth estimation with optimal
  adversarial patches.
\newblock In \emph{ECCV}, 2022.

\bibitem[Cheng et~al.(2023{\natexlab{a}})Cheng, Choi, Feng, Liang, Tao, Liu,
  Zuzak, and Zhang]{cheng2023fusion}
Cheng, Z., Choi, H., Feng, S., Liang, J.~C., Tao, G., Liu, D., Zuzak, M., and
  Zhang, X.
\newblock Fusion is not enough: Single modal attack on fusion models for 3d
  object detection.
\newblock In \emph{ICLR}, 2023{\natexlab{a}}.

\bibitem[Cheng et~al.(2023{\natexlab{b}})Cheng, Choi, Liang, Feng, Tao, Liu,
  Zuzak, and Zhang]{cheng2024fusion}
Cheng, Z., Choi, H., Liang, J., Feng, S., Tao, G., Liu, D., Zuzak, M., and
  Zhang, X.
\newblock Fusion is not enough: single-modal attacks to compromise fusion
  models in autonomous driving.
\newblock \emph{arXiv preprint arXiv:2304.14614}, 2023{\natexlab{b}}.

\bibitem[Cheng et~al.(2023{\natexlab{c}})Cheng, Liang, Tao, Liu, and
  Zhang]{cheng2023adversarial}
Cheng, Z., Liang, J., Tao, G., Liu, D., and Zhang, X.
\newblock Adversarial training of self-supervised monocular depth estimation
  against physical-world attacks.
\newblock \emph{arXiv preprint arXiv:2301.13487}, 2023{\natexlab{c}}.

\bibitem[Chu et~al.(2021)Chu, Tian, Wang, Zhang, Ren, Wei, Xia, and
  Shen]{chu2021twins}
Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., and Shen, C.
\newblock Twins: Revisiting spatial attention design in vision transformers.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Cortes \& Vapnik(1995)Cortes and Vapnik]{cortes1995support}
Cortes, C. and Vapnik, V.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20:\penalty0 273--297, 1995.

\bibitem[Cui et~al.(2024)Cui, Han, and Liu]{cui2024collaborative}
Cui, Y., Han, C., and Liu, D.
\newblock Collaborative multi-task learning for multi-object tracking and
  segmentation.
\newblock \emph{Journal on Autonomous Transportation Systems}, 1\penalty0
  (2):\penalty0 1--23, 2024.

\bibitem[Danelljan et~al.(2017)Danelljan, Bhat, Shahbaz~Khan, and
  Felsberg]{danelljan2017eco}
Danelljan, M., Bhat, G., Shahbaz~Khan, F., and Felsberg, M.
\newblock Eco: Efficient convolution operators for tracking.
\newblock In \emph{CVPR}, 2017.

\bibitem[Dev et~al.(1997)Dev, Krose, and Groen]{dev1997navigation}
Dev, A., Krose, B., and Groen, F.
\newblock Navigation of a mobile robot on the temporal development of the optic
  flow.
\newblock In \emph{IROS}, 1997.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT}, 2018.

\bibitem[Dong \& Xing(2018)Dong and Xing]{dong2018few}
Dong, N. and Xing, E.~P.
\newblock Few-shot semantic segmentation with prototype learning.
\newblock In \emph{BMVC}, 2018.

\bibitem[Dong et~al.(2023)Dong, Cao, and Fu]{dong2023rethinking}
Dong, Q., Cao, C., and Fu, Y.
\newblock Rethinking optical flow from geometric matching consistent
  perspective.
\newblock In \emph{CVPR}, 2023.

\bibitem[Dosovitskiy et~al.(2015)Dosovitskiy, Fischer, Ilg, Hausser, Hazirbas,
  Golkov, Van Der~Smagt, Cremers, and Brox]{dosovitskiy2015flownet}
Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V.,
  Van Der~Smagt, P., Cremers, D., and Brox, T.
\newblock Flownet: Learning optical flow with convolutional networks.
\newblock In \emph{ICCV}, 2015.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{ICLR}, 2021.

\bibitem[Eigen et~al.(2014)Eigen, Puhrsch, and Fergus]{eigen2014depth}
Eigen, D., Puhrsch, C., and Fergus, R.
\newblock Depth map prediction from a single image using a multi-scale deep
  network.
\newblock \emph{NeurIPS}, 2014.

\bibitem[Fan et~al.(2019)Fan, Lin, Yang, Chu, Deng, Yu, Bai, Xu, Liao, and
  Ling]{fan2019lasot}
Fan, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Bai, H., Xu, Y., Liao,
  C., and Ling, H.
\newblock Lasot: A high-quality benchmark for large-scale single object
  tracking.
\newblock In \emph{CVPR}, 2019.

\bibitem[Farhangi et~al.(2022)Farhangi, Sui, Hua, Bai, Huang, and
  Guo]{farhangi2022protoformer}
Farhangi, A., Sui, N., Hua, N., Bai, H., Huang, A., and Guo, Z.
\newblock Protoformer: Embedding prototypes for transformers.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining}, pp.\  447--458. Springer, 2022.

\bibitem[Fu et~al.(2018)Fu, Gong, Wang, Batmanghelich, and Tao]{fu2018deep}
Fu, H., Gong, M., Wang, C., Batmanghelich, K., and Tao, D.
\newblock Deep ordinal regression network for monocular depth estimation.
\newblock In \emph{CVPR}, 2018.

\bibitem[Gao et~al.(2022)Gao, Cui, Ye, Li, Zhao, and Zhu]{gao2022structure}
Gao, H., Cui, J., Ye, M., Li, S., Zhao, Y., and Zhu, X.
\newblock Structure-preserving motion estimation for learned video compression.
\newblock In \emph{ACMMM}, 2022.

\bibitem[Geiger et~al.(2013)Geiger, Lenz, Stiller, and
  Urtasun]{geiger2013vision}
Geiger, A., Lenz, P., Stiller, C., and Urtasun, R.
\newblock Vision meets robotics: The kitti dataset.
\newblock \emph{The International Journal of Robotics Research}, 2013.

\bibitem[Giese \& Poggio(2003)Giese and Poggio]{giese2003neural}
Giese, M.~A. and Poggio, T.
\newblock Neural mechanisms for the recognition of biological movements.
\newblock \emph{Nature Reviews Neuroscience}, 2003.

\bibitem[Godard et~al.(2017)Godard, Mac~Aodha, and
  Brostow]{godard2017unsupervised}
Godard, C., Mac~Aodha, O., and Brostow, G.~J.
\newblock Unsupervised monocular depth estimation with left-right consistency.
\newblock In \emph{CVPR}, 2017.

\bibitem[Han et~al.(2023{\natexlab{a}})Han, Wang, Cui, Cao, Wang, Qi, and
  Liu]{han20232vpt}
Han, C., Wang, Q., Cui, Y., Cao, Z., Wang, W., Qi, S., and Liu, D.
\newblock E\^{}2vpt: An effective and efficient approach for visual prompt
  tuning.
\newblock In \emph{ICCV}, 2023{\natexlab{a}}.

\bibitem[Han et~al.(2024)Han, Wang, Cui, Wang, Huang, Qi, and
  Liu]{han2024facing}
Han, C., Wang, Q., Cui, Y., Wang, W., Huang, L., Qi, S., and Liu, D.
\newblock Facing the elephant in the room: Visual prompt tuning or full
  finetuning?
\newblock In \emph{ICLR}, 2024.

\bibitem[Han et~al.(2023{\natexlab{b}})Han, Pan, Han, Song, and
  Huang]{han2023flatten}
Han, D., Pan, X., Han, Y., Song, S., and Huang, G.
\newblock Flatten transformer: Vision transformer using focused linear
  attention.
\newblock In \emph{ICCV}, 2023{\natexlab{b}}.

\bibitem[Han et~al.(2022)Han, Wang, Chen, Chen, Guo, Liu, Tang, Xiao, Xu, Xu,
  et~al.]{han2022survey}
Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A.,
  Xu, C., Xu, Y., et~al.
\newblock A survey on vision transformer.
\newblock \emph{IEEE TPAMI}, 2022.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, Friedman, and
  Friedman]{hastie2009elements}
Hastie, T., Tibshirani, R., Friedman, J.~H., and Friedman, J.~H.
\newblock \emph{The elements of statistical learning: data mining, inference,
  and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[Hawkins \& Blakeslee(2004)Hawkins and
  Blakeslee]{hawkins2004intelligence}
Hawkins, J. and Blakeslee, S.
\newblock \emph{On intelligence}.
\newblock Macmillan, 2004.

\bibitem[Hu et~al.(2021)Hu, Lu, and Xu]{hu2021fvc}
Hu, Z., Lu, G., and Xu, D.
\newblock Fvc: A new framework towards deep video compression in feature space.
\newblock In \emph{CVPR}, 2021.

\bibitem[Huang et~al.(2019)Huang, Wang, Huang, Huang, Wei, and
  Liu]{huang2019ccnet}
Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W.
\newblock Ccnet: Criss-cross attention for semantic segmentation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Huang et~al.(2022)Huang, Shi, Zhang, Wang, Cheung, Qin, Dai, and
  Li]{huang2022flowformer}
Huang, Z., Shi, X., Zhang, C., Wang, Q., Cheung, K.~C., Qin, H., Dai, J., and
  Li, H.
\newblock Flowformer: A transformer architecture for optical flow.
\newblock In \emph{ECCV}, 2022.

\bibitem[Ikotun et~al.(2023)Ikotun, Ezugwu, Abualigah, Abuhaija, and
  Heming]{ikotun2023k}
Ikotun, A.~M., Ezugwu, A.~E., Abualigah, L., Abuhaija, B., and Heming, J.
\newblock K-means clustering algorithms: A comprehensive review, variants
  analysis, and advances in the era of big data.
\newblock \emph{Information Sciences}, 2023.

\bibitem[Jaegle et~al.(2022)Jaegle, Borgeaud, Alayrac, Doersch, Ionescu, Ding,
  Koppula, Zoran, Brock, Shelhamer, et~al.]{jaegle2021perceiver}
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D.,
  Koppula, S., Zoran, D., Brock, A., Shelhamer, E., et~al.
\newblock Perceiver io: A general architecture for structured inputs \&
  outputs.
\newblock \emph{ICLR}, 2022.

\bibitem[Jetley et~al.(2015)Jetley, Romera-Paredes, Jayasumana, and
  Torr]{jetley2015prototypical}
Jetley, S., Romera-Paredes, B., Jayasumana, S., and Torr, P.
\newblock Prototypical priors: From improving classification to zero-shot
  learning.
\newblock \emph{arXiv preprint arXiv:1512.01192}, 2015.

\bibitem[Jiang et~al.(2021)Jiang, Campbell, Lu, Li, and
  Hartley]{jiang2021learning}
Jiang, S., Campbell, D., Lu, Y., Li, H., and Hartley, R.
\newblock Learning to estimate hidden motions with global motion aggregation.
\newblock In \emph{ICCV}, 2021.

\bibitem[Jiang et~al.(2012)Jiang, Lin, and Davis]{jiang2012recognizing}
Jiang, Z., Lin, Z., and Davis, L.
\newblock Recognizing human actions by learning and matching shape-motion
  prototype trees.
\newblock \emph{IEEE TPAMI}, 2012.

\bibitem[Kepes(1995)]{kepes1995language}
Kepes, G.
\newblock \emph{Language of vision}.
\newblock Courier Corporation, 1995.

\bibitem[Khalifa et~al.(2020)Khalifa, Alouani, Mahjoub, and
  Amara]{khalifa2020pedestrian}
Khalifa, A.~B., Alouani, I., Mahjoub, M.~A., and Amara, N. E.~B.
\newblock Pedestrian detection using a moving camera: A novel framework for
  foreground detection.
\newblock \emph{Cognitive Systems Research}, 2020.

\bibitem[Khan et~al.(2022)Khan, Naseer, Hayat, Zamir, Khan, and
  Shah]{khan2022transformers}
Khan, S., Naseer, M., Hayat, M., Zamir, S.~W., Khan, F.~S., and Shah, M.
\newblock Transformers in vision: A survey.
\newblock \emph{ACM Computing Surveys}, 54\penalty0 (10s):\penalty0 1--41,
  2022.

\bibitem[Kim et~al.(2021)Kim, Son, and Kim]{kim2021vilt}
Kim, W., Son, B., and Kim, I.
\newblock Vilt: Vision-and-language transformer without convolution or region
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Kondermann et~al.(2016)Kondermann, Nair, Honauer, Krispin, Andrulis,
  Brock, Gussefeld, Rahimimoghaddam, Hofmann, Brenner,
  et~al.]{kondermann2016hci}
Kondermann, D., Nair, R., Honauer, K., Krispin, K., Andrulis, J., Brock, A.,
  Gussefeld, B., Rahimimoghaddam, M., Hofmann, S., Brenner, C., et~al.
\newblock The hci benchmark suite: Stereo and flow ground truth with
  uncertainties for urban autonomous driving.
\newblock In \emph{CVPR Workshops}, 2016.

\bibitem[Lee et~al.(2023)Lee, Cho, Lee, Park, and Lee]{lee2023unsupervised}
Lee, M., Cho, S., Lee, S., Park, C., and Lee, S.
\newblock Unsupervised video object segmentation via prototype memory network.
\newblock In \emph{WACV}, 2023.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Jampani, Sevilla-Lara, Sun, Kim, and
  Kim]{li2021adaptive}
Li, G., Jampani, V., Sevilla-Lara, L., Sun, D., Kim, J., and Kim, J.
\newblock Adaptive prototype learning and allocation for few-shot segmentation.
\newblock In \emph{CVPR}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Huang, Du, Zhen, Chen, and
  Shao]{li2021variational}
Li, J., Huang, Q., Du, Y., Zhen, X., Chen, S., and Shao, L.
\newblock Variational abnormal behavior detection with motion consistency.
\newblock \emph{IEEE TIP}, 2021{\natexlab{b}}.

\bibitem[Liang et~al.(2022{\natexlab{a}})Liang, Wang, Miao, and
  Yang]{liang2022gmmseg}
Liang, C., Wang, W., Miao, J., and Yang, Y.
\newblock Gmmseg: Gaussian mixture based generative semantic segmentation
  models.
\newblock \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Liang et~al.(2022{\natexlab{b}})Liang, Wang, Chen, Yang, and
  Liu]{liang2022triangulation}
Liang, J., Wang, Y., Chen, Y., Yang, B., and Liu, D.
\newblock A triangulation-based visual localization for field robots.
\newblock \emph{IEEE/CAA Journal of Automatica Sinica}, 9\penalty0
  (6):\penalty0 1083--1086, 2022{\natexlab{b}}.

\bibitem[Liang et~al.(2023)Liang, Zhou, and Liu]{liang2023clustseg}
Liang, J., Zhou, T., and Liu, D.
\newblock Clustseg: Clustering for universal segmentation.
\newblock In \emph{ICML}, 2023.

\bibitem[Liang et~al.(2024)Liang, Cui, Wang, Geng, Wang, and
  Liu]{liang2024clusterfomer}
Liang, J., Cui, Y., Wang, Q., Geng, T., Wang, W., and Liu, D.
\newblock Clusterfomer: Clustering as a universal visual learner.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Lin et~al.(2022)Lin, Wang, Liu, and Qiu]{lin2022survey}
Lin, T., Wang, Y., Liu, X., and Qiu, X.
\newblock A survey of transformers.
\newblock \emph{AI Open}, 2022.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy,
  Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhang, Zhang, and He]{liu2020part}
Liu, Y., Zhang, X., Zhang, S., and He, X.
\newblock Part-aware prototype network for few-shot semantic segmentation.
\newblock In \emph{ECCV}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{ICCV}, 2021.

\bibitem[Liu et~al.(2022)Liu, Hu, Lin, Yao, Xie, Wei, Ning, Cao, Zhang, Dong,
  et~al.]{liu2022swin}
Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang,
  Z., Dong, L., et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In \emph{CVPR}, 2022.

\bibitem[Lookingbill et~al.(2007)Lookingbill, Rogers, Lieb, Curry, and
  Thrun]{lookingbill2007reverse}
Lookingbill, A., Rogers, J., Lieb, D., Curry, J., and Thrun, S.
\newblock Reverse optical flow for self-supervised adaptive autonomous robot
  navigation.
\newblock \emph{IJCV}, 74:\penalty0 287--302, 2007.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{ICLR}, 2019.

\bibitem[Lu et~al.(2019)Lu, Ouyang, Xu, Zhang, Cai, and Gao]{lu2019dvc}
Lu, G., Ouyang, W., Xu, D., Zhang, X., Cai, C., and Gao, Z.
\newblock Dvc: An end-to-end deep video compression framework.
\newblock In \emph{CVPR}, 2019.

\bibitem[Lu et~al.(2023)Lu, Wang, Ma, Geng, Chen, Chen, and
  Liu]{lu2023transflow}
Lu, Y., Wang, Q., Ma, S., Geng, T., Chen, Y.~V., Chen, H., and Liu, D.
\newblock Transflow: Transformer as flow learner.
\newblock In \emph{CVPR}, 2023.

\bibitem[Lu et~al.(2024)Lu, Liu, Wang, Han, Cui, Cao, Zhang, Chen, and
  Fan]{lu2024promotion}
Lu, Y., Liu, D., Wang, Q., Han, C., Cui, Y., Cao, Z., Zhang, X., Chen, Y.~V.,
  and Fan, H.
\newblock Promotion: Prototypes as motion learners.
\newblock In \emph{CVPR}, 2024.

\bibitem[Luo et~al.(2022{\natexlab{a}})Luo, Yang, Li, and Liu]{kpaflow}
Luo, A., Yang, F., Li, X., and Liu, S.
\newblock Learning optical flow with kernel patch attention.
\newblock In \emph{CVPR}, 2022{\natexlab{a}}.

\bibitem[Luo et~al.(2022{\natexlab{b}})Luo, Yang, Luo, Li, Fan, and
  Liu]{agflow}
Luo, A., Yang, F., Luo, K., Li, X., Fan, H., and Liu, S.
\newblock Learning optical flow with adaptive graph reasoning.
\newblock In \emph{AAAI}, 2022{\natexlab{b}}.

\bibitem[Ma et~al.(2022)Ma, Shou, Zhu, Fan, Xu, Yang, and Yan]{ma2022unified}
Ma, F., Shou, M.~Z., Zhu, L., Fan, H., Xu, Y., Yang, Y., and Yan, Z.
\newblock Unified transformer tracker for object tracking.
\newblock In \emph{CVPR}, 2022.

\bibitem[Ma et~al.(2023)Ma, Zhou, Wang, Qin, Sun, Liu, and Fu]{ma2023image}
Ma, X., Zhou, Y., Wang, H., Qin, C., Sun, B., Liu, C., and Fu, Y.
\newblock Image as set of points.
\newblock In \emph{ICLR}, 2023.
\newblock URL \url{https://openreview.net/forum?id=awnvqZja69}.

\bibitem[Mantini et~al.(2012)Mantini, Corbetta, Romani, Orban, and
  Vanduffel]{mantini2012data}
Mantini, D., Corbetta, M., Romani, G.~L., Orban, G.~A., and Vanduffel, W.
\newblock Data-driven analysis of analogous brain networks in monkeys and
  humans during natural vision.
\newblock \emph{Neuroimage}, 2012.

\bibitem[Marathe et~al.(2021)Marathe, Walambe, and
  Kotecha]{marathe2021evaluating}
Marathe, A., Walambe, R., and Kotecha, K.
\newblock Evaluating the performance of ensemble methods and voting strategies
  for dense 2d pedestrian detection in the wild.
\newblock In \emph{ICCV}, 2021.

\bibitem[Mayer et~al.(2016)Mayer, Ilg, Hausser, Fischer, Cremers, Dosovitskiy,
  and Brox]{mayer2016large}
Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., and
  Brox, T.
\newblock A large dataset to train convolutional networks for disparity,
  optical flow, and scene flow estimation.
\newblock In \emph{CVPR}, 2016.

\bibitem[Meyer et~al.(2004)Meyer, Neto, Jones, and
  Hindman]{meyer2004intensified}
Meyer, G.~E., Neto, J.~C., Jones, D.~D., and Hindman, T.~W.
\newblock Intensified fuzzy clusters for classifying plant, soil, and residue
  regions of interest from color images.
\newblock \emph{Computers and Electronics in Agriculture}, 2004.

\bibitem[Muller et~al.(2018)Muller, Bibi, Giancola, Alsubaihi, and
  Ghanem]{muller2018trackingnet}
Muller, M., Bibi, A., Giancola, S., Alsubaihi, S., and Ghanem, B.
\newblock Trackingnet: A large-scale dataset and benchmark for object tracking
  in the wild.
\newblock In \emph{ECCV}, 2018.

\bibitem[Nam \& Han(2016)Nam and Han]{nam2016learning}
Nam, H. and Han, B.
\newblock Learning multi-domain convolutional neural networks for visual
  tracking.
\newblock In \emph{CVPR}, 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Patil et~al.(2022)Patil, Sakaridis, Liniger, and
  Van~Gool]{patil2022p3depth}
Patil, V., Sakaridis, C., Liniger, A., and Van~Gool, L.
\newblock P3depth: Monocular depth estimation with a piecewise planarity prior.
\newblock In \emph{CVPR}, 2022.

\bibitem[Plato(402 BC)]{plato402cratylus}
Plato.
\newblock \emph{Cratylus}.
\newblock Plato, 402 BC.

\bibitem[Prakash et~al.(2021)Prakash, Chitta, and Geiger]{prakash2021multi}
Prakash, A., Chitta, K., and Geiger, A.
\newblock Multi-modal fusion transformer for end-to-end autonomous driving.
\newblock In \emph{CVPR}, 2021.

\bibitem[Qin et~al.(2023)Qin, Han, Wang, Nie, Yin, and Xiankai]{qin2023unified}
Qin, Z., Han, C., Wang, Q., Nie, X., Yin, Y., and Xiankai, L.
\newblock Unified 3d segmenter as prototypical classifiers.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 2020.

\bibitem[Ranjan \& Black(2017)Ranjan and Black]{ranjan2017optical}
Ranjan, A. and Black, M.~J.
\newblock Optical flow estimation using a spatial pyramid network.
\newblock In \emph{CVPR}, 2017.

\bibitem[Rudin et~al.(2022)Rudin, Chen, Chen, Huang, Semenova, and
  Zhong]{rudin2022interpretable}
Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., and Zhong, C.
\newblock Interpretable machine learning: Fundamental principles and 10 grand
  challenges.
\newblock \emph{Statistic Surveys}, 16:\penalty0 1--85, 2022.

\bibitem[Shao et~al.(2023)Shao, Wang, Chen, Li, and Liu]{shao2023safety}
Shao, H., Wang, L., Chen, R., Li, H., and Liu, Y.
\newblock Safety-enhanced autonomous driving using interpretable sensor fusion
  transformer.
\newblock In \emph{Conference on Robot Learning}, 2023.

\bibitem[Shen et~al.(2023)Shen, Kerofsky, and Yogamani]{shen2023optical}
Shen, S., Kerofsky, L., and Yogamani, S.
\newblock Optical flow for autonomous driving: Applications, challenges and
  improvements.
\newblock \emph{arXiv preprint arXiv:2301.04422}, 2023.

\bibitem[Shi et~al.(2023)Shi, Huang, Li, Zhang, Cheung, See, Qin, Dai, and
  Li]{shi2023flowformer++}
Shi, X., Huang, Z., Li, D., Zhang, M., Cheung, K.~C., See, S., Qin, H., Dai,
  J., and Li, H.
\newblock Flowformer++: Masked cost volume autoencoding for pretraining optical
  flow estimation.
\newblock In \emph{CVPR}, 2023.

\bibitem[Simon \& Newell(1971)Simon and Newell]{newell1972human}
Simon, H.~A. and Newell, A.
\newblock Human problem solving: The state of the theory in 1970.
\newblock \emph{American psychologist}, 26\penalty0 (2):\penalty0 145, 1971.

\bibitem[Smith \& Minda(2002)Smith and Minda]{smith2002distinguishing}
Smith, J.~D. and Minda, J.~P.
\newblock Distinguishing prototype-based and exemplar-based processes in
  dot-pattern category learning.
\newblock \emph{Journal of Experimental Psychology: Learning, Memory, and
  Cognition}, 2002.

\bibitem[Song et~al.(2022)Song, Wen, Liu, and Yu]{song2022deep}
Song, Y., Wen, J., Liu, D., and Yu, C.
\newblock Deep robotic grasping prediction with hierarchical rgb-d fusion.
\newblock \emph{IJCAS}, 20\penalty0 (1):\penalty0 243--254, 2022.

\bibitem[Strudel et~al.(2021)Strudel, Garcia, Laptev, and
  Schmid]{strudel2021segmenter}
Strudel, R., Garcia, R., Laptev, I., and Schmid, C.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In \emph{ICCV}, 2021.

\bibitem[Sui et~al.(2022)Sui, Li, Geng, Wu, Xu, Liu, Goh, and
  Zhu]{sui2022craft}
Sui, X., Li, S., Geng, X., Wu, Y., Xu, X., Liu, Y., Goh, R., and Zhu, H.
\newblock Craft: Cross-attentional flow transformer for robust optical flow.
\newblock In \emph{CVPR}, 2022.

\bibitem[Sun et~al.(2018)Sun, Yang, Liu, and Kautz]{sun2018pwc}
Sun, D., Yang, X., Liu, M.-Y., and Kautz, J.
\newblock Pwc-net: Cnns for optical flow using pyramid, warping, and cost
  volume.
\newblock In \emph{CVPR}, 2018.

\bibitem[Sun et~al.(2021)Sun, Vlasic, Herrmann, Jampani, Krainin, Chang, Zabih,
  Freeman, and Liu]{RAFT-A}
Sun, D., Vlasic, D., Herrmann, C., Jampani, V., Krainin, M., Chang, H., Zabih,
  R., Freeman, W.~T., and Liu, C.
\newblock Autoflow: Learning a better training set for optical flow.
\newblock In \emph{CVPR}, 2021.

\bibitem[Teed \& Deng(2020)Teed and Deng]{teed2020raft}
Teed, Z. and Deng, J.
\newblock Raft: Recurrent all-pairs field transforms for optical flow.
\newblock In \emph{ECCV}, 2020.

\bibitem[Tudor~Ionescu et~al.(2017)Tudor~Ionescu, Smeureanu, Alexe, and
  Popescu]{tudor2017unmasking}
Tudor~Ionescu, R., Smeureanu, S., Alexe, B., and Popescu, M.
\newblock Unmasking the abnormal events in video.
\newblock In \emph{ICCV}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Vattani(2009)]{vattani2009k}
Vattani, A.
\newblock K-means requires exponentially many iterations even in the plane.
\newblock In \emph{Annual Symposium on Computational Geometry}, 2009.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Zhu, Adam, Yuille, and
  Chen]{wang2021max}
Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C.
\newblock Max-deeplab: End-to-end panoptic segmentation with mask transformers.
\newblock In \emph{CVPR}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2019)Wang, Liew, Zou, Zhou, and Feng]{wang2019panet}
Wang, K., Liew, J.~H., Zou, Y., Zhou, D., and Feng, J.
\newblock Panet: Few-shot image semantic segmentation with prototype alignment.
\newblock In \emph{ICCV}, 2019.

\bibitem[Wang et~al.(2018)Wang, Yang, Lin, Zhang, Shamir, Lu, and
  Hu]{wang2018deep}
Wang, M., Yang, G.-Y., Lin, J.-K., Zhang, S.-H., Shamir, A., Lu, S.-P., and Hu,
  S.-M.
\newblock Deep online video stabilization with multi-grid warping
  transformation learning.
\newblock \emph{IEEE TIP}, 2018.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Zhou, Wang, and
  Li]{wang2021transformer}
Wang, N., Zhou, W., Wang, J., and Li, H.
\newblock Transformer meets tracker: Exploiting temporal context for robust
  visual tracking.
\newblock In \emph{CVPR}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2021{\natexlab{c}})Wang, Xie, Li, Fan, Song, Liang, Lu,
  Luo, and Shao]{wang2021pyramid}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In \emph{ICCV}, 2021{\natexlab{c}}.

\bibitem[Wang et~al.(2022)Wang, Liang, and Liu]{wang2022learning}
Wang, W., Liang, J., and Liu, D.
\newblock Learning equivariant segmentation with instance-unique querying.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Wang et~al.(2023)Wang, Han, Zhou, and Liu]{wang2022visual}
Wang, W., Han, C., Zhou, T., and Liu, D.
\newblock Visual recognition with deep nearest centroids.
\newblock In \emph{ICLR}, 2023.

\bibitem[Wang et~al.(2021{\natexlab{d}})Wang, Xu, Wang, Shen, Cheng, Shen, and
  Xia]{wang2021end}
Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H.
\newblock End-to-end video instance segmentation with transformers.
\newblock In \emph{CVPR}, 2021{\natexlab{d}}.

\bibitem[Wang et~al.(2021{\natexlab{e}})Wang, Zhao, Li, Wang, Torr, and
  Bertinetto]{wang2021different}
Wang, Z., Zhao, H., Li, Y.-L., Wang, S., Torr, P., and Bertinetto, L.
\newblock Do different tracking tasks require different appearance models?
\newblock \emph{NeurIPS}, 2021{\natexlab{e}}.

\bibitem[Xiong et~al.(2021)Xiong, Zhang, Zhong, Ji, Liu, and
  Xiong]{xiong2021self}
Xiong, M., Zhang, Z., Zhong, W., Ji, J., Liu, J., and Xiong, H.
\newblock Self-supervised monocular depth and visual odometry learning with
  scale-consistent geometric constraints.
\newblock In \emph{IJCAI}, 2021.

\bibitem[Xu et~al.(2022{\natexlab{a}})Xu, Guo, Nedjah, Zhang, and
  Li]{xu2022vehicle}
Xu, H., Guo, M., Nedjah, N., Zhang, J., and Li, P.
\newblock Vehicle and pedestrian detection algorithm based on lightweight
  yolov3-promote and semi-precision acceleration.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  2022{\natexlab{a}}.

\bibitem[Xu et~al.(2022{\natexlab{b}})Xu, Zhang, Cai, Rezatofighi, and
  Tao]{xu2022gmflow}
Xu, H., Zhang, J., Cai, J., Rezatofighi, H., and Tao, D.
\newblock Gmflow: Learning optical flow via global matching.
\newblock In \emph{CVPR}, 2022{\natexlab{b}}.

\bibitem[Xu et~al.(2020)Xu, Xian, Wang, Schiele, and Akata]{xu2020attribute}
Xu, W., Xian, Y., Wang, J., Schiele, B., and Akata, Z.
\newblock Attribute prototype network for zero-shot learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Yang et~al.(2020)Yang, Liu, Li, Jiao, and Ye]{yang2020prototype}
Yang, B., Liu, C., Li, B., Jiao, J., and Ye, Q.
\newblock Prototype mixture models for few-shot semantic segmentation.
\newblock In \emph{ECCV}, 2020.

\bibitem[Yang et~al.(2023)Yang, Liu, Xu, and Huang]{yang2023tvt}
Yang, J., Liu, J., Xu, N., and Huang, J.
\newblock Tvt: Transferable vision transformer for unsupervised domain
  adaptation.
\newblock In \emph{WACV}, 2023.

\bibitem[Yin et~al.(2019)Yin, Liu, Shen, and Yan]{yin2019enforcing}
Yin, W., Liu, Y., Shen, C., and Yan, Y.
\newblock Enforcing geometric constraints of virtual normal for depth
  prediction.
\newblock In \emph{ICCV}, 2019.

\bibitem[Yu et~al.(2022)Yu, Wang, Qiao, Collins, Zhu, Adam, Yuille, and
  Chen]{yu2022k}
Yu, Q., Wang, H., Qiao, S., Collins, M., Zhu, Y., Adam, H., Yuille, A., and
  Chen, L.-C.
\newblock k-means mask transformer.
\newblock \emph{ECCV}, 2022.

\bibitem[Zhai et~al.(2022)Zhai, Xiang, Lv, Ali, and Saddik]{Zhai2019SKFlowOF}
Zhai, M., Xiang, X., Lv, N., Ali, S.~M., and Saddik, A.~E.
\newblock Skflow: Optical flow estimation using selective kernel networks.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Woodford, Prisacariu, and Torr]{Separable}
Zhang, F., Woodford, O.~J., Prisacariu, V.~A., and Torr, P.~H.
\newblock Separable flow: Learning motion cost volumes for optical flow
  estimation.
\newblock In \emph{ICCV}, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Nex, Vosselman, and Kerle]{zhang2023lite}
Zhang, N., Nex, F., Vosselman, G., and Kerle, N.
\newblock Lite-mono: A lightweight cnn and transformer architecture for
  self-supervised monocular depth estimation.
\newblock In \emph{CVPR}, 2023.

\bibitem[Zhang et~al.(2020)Zhang, Peng, Fu, Li, and Hu]{zhang2020ocean}
Zhang, Z., Peng, H., Fu, J., Li, B., and Hu, W.
\newblock Ocean: Object-aware anchor-free tracking.
\newblock In \emph{ECCV}, 2020.

\bibitem[Zhao \& Ling(2020)Zhao and Ling]{zhao2020pwstablenet}
Zhao, M. and Ling, Q.
\newblock Pwstablenet: Learning pixel-wise warping maps for video
  stabilization.
\newblock \emph{IEEE TIP}, 2020.

\bibitem[Zhao et~al.(2022)Zhao, Zhao, Zhang, Zhou, and Metaxas]{gmflownet}
Zhao, S., Zhao, L., Zhang, Z., Zhou, E., and Metaxas, D.
\newblock Global matching with overlapping attention for optical flow
  estimation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhao et~al.(2020)Zhao, Liu, Shu, and Liu]{zhao2020towards}
Zhao, W., Liu, S., Shu, Y., and Liu, Y.-J.
\newblock Towards better generalization: Joint depth-pose learning without
  posenet.
\newblock In \emph{CVPR}, 2020.

\bibitem[Zheng et~al.(2021)Zheng, Lu, Zhao, Zhu, Luo, Wang, Fu, Feng, Xiang,
  Torr, et~al.]{zheng2021rethinking}
Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J.,
  Xiang, T., Torr, P.~H., et~al.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zheng et~al.(2022)Zheng, Nie, Ling, Xiong, Liu, Wang, and Li]{dip}
Zheng, Z., Nie, N., Ling, Z., Xiong, P., Liu, J., Wang, H., and Li, J.
\newblock Dip: Deep inverse patchmatch for high-resolution optical flow.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhou et~al.(2021{\natexlab{a}})Zhou, Kang, Jin, Yang, Lian, Jiang,
  Hou, and Feng]{zhou2021deepvit}
Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q., and Feng,
  J.
\newblock Deepvit: Towards deeper vision transformer.
\newblock \emph{arXiv preprint arXiv:2103.11886}, 2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2019)Zhou, Du, Zhu, Peng, Liu, and
  Goh]{zhou2019anomalynet}
Zhou, J.~T., Du, J., Zhu, H., Peng, X., Liu, Y., and Goh, R. S.~M.
\newblock Anomalynet: An anomaly detection network for video surveillance.
\newblock \emph{IEEE Transactions on Information Forensics and Security}, 2019.

\bibitem[Zhou et~al.(2022)Zhou, Wang, Konukoglu, and
  Van~Gool]{zhou2022rethinking}
Zhou, T., Wang, W., Konukoglu, E., and Van~Gool, L.
\newblock Rethinking semantic segmentation: A prototype view.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhou et~al.(2021{\natexlab{b}})Zhou, Fan, Shi, and Xin]{zhou2021r}
Zhou, Z., Fan, X., Shi, P., and Xin, Y.
\newblock R-msfm: Recurrent multi-scale feature modulation for monocular depth
  estimating.
\newblock In \emph{ICCV}, 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2021)Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and
  Catanzaro]{zhu2021long}
Zhu, C., Ping, W., Xiao, C., Shoeybi, M., Goldstein, T., Anandkumar, A., and
  Catanzaro, B.
\newblock Long-short transformer: Efficient transformers for language and
  vision.
\newblock \emph{NeurIPS}, 2021.

\end{thebibliography}
