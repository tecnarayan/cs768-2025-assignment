\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi~Yadkori et~al.(2013)Abbasi~Yadkori, Bartlett, Kanade, Seldin, and Szepesvari]{NIPS2013_Abbasi}
Yasin Abbasi~Yadkori, Peter~L Bartlett, Varun Kanade, Yevgeny Seldin, and Csaba Szepesvari.
\newblock Online learning in markov decision processes with adversarially chosen transition probability distributions.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2013.

\bibitem[Agarwal et~al.(2017)Agarwal, Luo, Neyshabur, and Schapire]{agarwal2017corralling}
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert~E Schapire.
\newblock Corralling a band of bandit algorithms.
\newblock In \emph{Proceedings of the International Conference on Computational Learning Theory (COLT)}, 2017.

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{auer2008near}
Peter Auer, Thomas Jaksch, and Ronald Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2008.

\bibitem[Chen et~al.(2021)Chen, Du, and Jamieson]{chen2021improved}
Yifang Chen, Simon Du, and Kevin Jamieson.
\newblock Improved corruption robust algorithms for episodic reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Cheung et~al.(2023)Cheung, Simchi-Levi, and Zhu]{cheung2023nonstationary}
Wang~Chi Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Nonstationary reinforcement learning: The blessing of (more) optimism.
\newblock \emph{Management Science}, 2023.

\bibitem[Dann et~al.(2023)Dann, Wei, and Zimmert]{dann2023best}
Christoph Dann, Chen-Yu Wei, and Julian Zimmert.
\newblock Best of both worlds policy optimization.
\newblock \emph{arXiv preprint arXiv:2302.09408}, 2023.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Eyal Even-Dar, Sham~M Kakade, and Yishay Mansour.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 2009.

\bibitem[Foster et~al.(2016)Foster, Li, Lykouris, Sridharan, and Tardos]{foster2016learning}
Dylan~J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos.
\newblock Learning in games: Robustness of fast convergence.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem[Foster et~al.(2020)Foster, Gentile, Mohri, and Zimmert]{foster2020adapting}
Dylan~J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert.
\newblock Adapting to misspecification in contextual bandits.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Gajane et~al.(2018)Gajane, Ortner, and Auer]{gajane2018sliding}
Pratik Gajane, Ronald Ortner, and Peter Auer.
\newblock A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions.
\newblock \emph{arXiv preprint arXiv:1805.10066}, 2018.

\bibitem[Ito(2021)]{ito21a}
Shinji Ito.
\newblock Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds.
\newblock In \emph{Proceedings of the International Conference on Computational Learning Theory (COLT)}, 2021.

\bibitem[Jin et~al.(2020)Jin, Jin, Luo, Sra, and Yu]{jin2019learning}
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu.
\newblock Learning adversarial markov decision processes with bandit feedback and unknown transition.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Jin and Luo(2020)]{jin2020simultaneously}
Tiancheng Jin and Haipeng Luo.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with known transition.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Jin et~al.(2021)Jin, Huang, and Luo]{jin2021best}
Tiancheng Jin, Longbo Huang, and Haipeng Luo.
\newblock The best of both worlds: stochastic and adversarial episodic mdps with unknown transition.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Kakade(2003)]{kakade2003sample}
Sham~Machandranath Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University College London, 2003.

\bibitem[Lee et~al.(2020)Lee, Luo, Wei, and Zhang]{lee2020biasnomore}
Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang.
\newblock Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Haipeng Luo, Chen-Yu Wei, and Chung-Wei Lee.
\newblock Policy optimization in adversarial mdps: Improved exploration via dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Luo et~al.(2022)Luo, Zhang, Zhao, and Zhou]{luo2022corralling}
Haipeng Luo, Mengxiao Zhang, Peng Zhao, and Zhi-Hua Zhou.
\newblock Corralling a larger band of bandits: A case study on switching regret for linear bandits.
\newblock In \emph{Proceedings of the International Conference on Computational Learning Theory (COLT)}, 2022.

\bibitem[Lykouris et~al.(2018)Lykouris, Mirrokni, and Paes~Leme]{lykouris2018stochastic}
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes~Leme.
\newblock Stochastic bandits robust to adversarial corruptions.
\newblock In \emph{Proceedings of the Annual ACM SIGACT Symposium on Theory of Computing}, 2018.

\bibitem[Lykouris et~al.(2019)Lykouris, Simchowitz, Slivkins, and Sun]{lykouris2019corruption}
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun.
\newblock Corruption robust exploration in episodic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.08689}, 2019.

\bibitem[Neu et~al.(2010{\natexlab{a}})Neu, Antos, Gy\"{o}rgy, and Szepesv\'{a}ri]{neu2010online}
Gergely Neu, Andras Antos, Andr\'{a}s Gy\"{o}rgy, and Csaba Szepesv\'{a}ri.
\newblock Online markov decision processes under bandit feedback.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2010{\natexlab{a}}.

\bibitem[Neu et~al.(2010{\natexlab{b}})Neu, Gy{\"{o}}rgy, and Szepesv{\'{a}}ri]{neu2010loopfree}
Gergely Neu, Andr{\'{a}}s Gy{\"{o}}rgy, and Csaba Szepesv{\'{a}}ri.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{Proceedings of the International Conference on Computational Learning Theory (COLT)}, 2010{\natexlab{b}}.

\bibitem[Pacchiano et~al.(2022)Pacchiano, Dann, and Gentile]{pacchianobest2022}
Aldo Pacchiano, Christoph Dann, and Claudio Gentile.
\newblock Best of both worlds model selection.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Rosenberg and Mansour(2019{\natexlab{a}})]{rosenberg2019online}
Aviv Rosenberg and Yishay Mansour.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2019{\natexlab{a}}.

\bibitem[Rosenberg and Mansour(2019{\natexlab{b}})]{rosenberg2019onlineb}
Aviv Rosenberg and Yishay Mansour.
\newblock Online stochastic shortest path with bandit feedback and unknown transition function.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2019{\natexlab{b}}.

\bibitem[Tian et~al.(2021)Tian, Wang, Yu, and Sra]{tian2021online}
Yi~Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra.
\newblock Online learning in unknown markov games.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Wei and Luo(2018)]{wei2018more}
Chen-Yu Wei and Haipeng Luo.
\newblock More adaptive algorithms for adversarial bandits.
\newblock In \emph{Proceedings of the International Conference on Computational Learning Theory (COLT)}, 2018.

\bibitem[Wei and Luo(2021)]{wei2021non}
Chen-Yu Wei and Haipeng Luo.
\newblock Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach.
\newblock In \emph{Proceedings of the International Conference on Computational Learning Theory (COLT)}, 2021.

\bibitem[Wei et~al.(2022)Wei, Dann, and Zimmert]{wei2022model}
Chen-Yu Wei, Christoph Dann, and Julian Zimmert.
\newblock A model selection approach for corruption robust reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Chen, Zhu, and Sun]{zhang2021robust}
Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun.
\newblock Robust policy gradient against strong data corruption.
\newblock In \emph{International Conference on Machine Learning}, pages 12391--12401. PMLR, 2021.

\bibitem[Zimin and Neu(2013)]{zimin2013online}
Alexander Zimin and Gergely Neu.
\newblock Online learning in episodic markovian decision processes by relative entropy policy search.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2013.

\bibitem[Zimmert and Seldin(2019)]{zimmert2019optimal}
Julian Zimmert and Yevgeny Seldin.
\newblock An optimal algorithm for stochastic and adversarial bandits.
\newblock In \emph{Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)}, 2019.

\end{thebibliography}
