\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azadi \& Sra(2014)Azadi and Sra]{azadi2014towards}
Azadi, S. and Sra, S.
\newblock Towards an optimal stochastic alternating direction method of
  multipliers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  620--628. PMLR, 2014.

\bibitem[Bach \& Levy(2019)Bach and Levy]{bach2019universal}
Bach, F. and Levy, K.~Y.
\newblock A universal algorithm for variational inequalities adaptive to
  smoothness and noise.
\newblock In \emph{Conference on Learning Theory}, pp.\  164--194. PMLR, 2019.

\bibitem[Barbero \& Sra(2018)Barbero and Sra]{barbero2018modular}
Barbero, A. and Sra, S.
\newblock Modular proximal optimization for multidimensional total-variation
  regularization.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2232--2313, 2018.

\bibitem[Bauschke et~al.(2011)Bauschke, Combettes, et~al.]{bauschke2011convex}
Bauschke, H.~H., Combettes, P.~L., et~al.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}, volume 408.
\newblock Springer, 2011.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Blalock, D., Ortiz, J. J.~G., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Brice{\~n}o-Arias(2015)]{briceno2015forward}
Brice{\~n}o-Arias, L.~M.
\newblock Forward-douglas--rachford splitting and forward-partial inverse
  method for solving monotone inclusions.
\newblock \emph{Optimization}, 64\penalty0 (5):\penalty0 1239--1261, 2015.

\bibitem[Cevher et~al.(2018)Cevher, V{\~u}, and
  Yurtsever]{cevher2018stochastic}
Cevher, V., V{\~u}, B.~C., and Yurtsever, A.
\newblock Stochastic forward douglas-rachford splitting method for monotone
  inclusions.
\newblock In \emph{Large-Scale and Distributed Optimization}, pp.\  149--179.
  Springer, 2018.

\bibitem[Chambolle \& Pock(2016)Chambolle and Pock]{chambolle2016ergodic}
Chambolle, A. and Pock, T.
\newblock On the ergodic convergence rates of a first-order primal--dual
  algorithm.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1):\penalty0 253--287,
  2016.

\bibitem[Chang \& Lin(2011)Chang and Lin]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock Libsvm: A library for support vector machines.
\newblock \emph{ACM transactions on intelligent systems and technology (TIST)},
  2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Condat(2013)]{condat2013primal}
Condat, L.
\newblock A primal--dual splitting method for convex optimization involving
  lipschitzian, proximable and linear composite terms.
\newblock \emph{Journal of optimization theory and applications}, 158\penalty0
  (2):\penalty0 460--479, 2013.

\bibitem[Cutkosky(2019)]{cutkosky2019anytime}
Cutkosky, A.
\newblock Anytime online-to-batch, optimism and acceleration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1446--1454. PMLR, 2019.

\bibitem[Davis \& Yin(2017)Davis and Yin]{davis2017three}
Davis, D. and Yin, W.
\newblock A three-operator splitting scheme and its optimization applications.
\newblock \emph{Set-valued and variational analysis}, 25\penalty0 (4):\penalty0
  829--858, 2017.

\bibitem[Ding et~al.(2019)Ding, Yurtsever, Cevher, Tropp, and
  Udell]{ding2019optimal}
Ding, L., Yurtsever, A., Cevher, V., Tropp, J.~A., and Udell, M.
\newblock An optimal-storage approach to semidefinite programming using
  approximate complementarity.
\newblock \emph{arXiv preprint arXiv:1902.03373}, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[El~Halabi \& Cevher(2015)El~Halabi and Cevher]{el2015totally}
El~Halabi, M. and Cevher, V.
\newblock A totally unimodular view of structured sparsity.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  223--231.
  PMLR, 2015.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011deep}
Glorot, X., Bordes, A., and Bengio, Y.
\newblock Deep sparse rectifier neural networks.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  315--323. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Grant \& Boyd(2014)Grant and Boyd]{grant2014cvx}
Grant, M. and Boyd, S.
\newblock {CVX}: {M}atlab software for disciplined convex programming, version
  2.1, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Higham \& Strabi{\'c}(2016)Higham and Strabi{\'c}]{higham2016anderson}
Higham, N.~J. and Strabi{\'c}, N.
\newblock Anderson acceleration of the alternating projections method for
  computing the nearest correlation matrix.
\newblock \emph{Numerical Algorithms}, 72\penalty0 (4):\penalty0 1021--1042,
  2016.

\bibitem[Hoffmann(1992)]{hoffmann1992distance}
Hoffmann, A.
\newblock The distance to the intersection of two convex sets expressed by the
  distances to each of them.
\newblock \emph{Mathematische Nachrichten}, 157\penalty0 (1):\penalty0 81--98,
  1992.

\bibitem[Kavis et~al.(2019)Kavis, Levy, Bach, and Cevher]{kavis2019unixgrad}
Kavis, A., Levy, K.~Y., Bach, F., and Cevher, V.
\newblock Unixgrad: A universal, adaptive algorithm with optimal guarantees for
  constrained optimization.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, 2019.

\bibitem[Kundu et~al.(2018)Kundu, Bach, and Bhattacharya]{kundu2018convex}
Kundu, A., Bach, F., and Bhattacharya, C.
\newblock Convex optimization over intersection of simple sets: improved
  convergence rate guarantees via an exact penalty approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  958--967. PMLR, 2018.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Levy(2017)]{levy2017online}
Levy, K.~Y.
\newblock Online to offline conversions, universality and adaptive minibatch
  sizes.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, 2017.

\bibitem[Levy et~al.(2018)Levy, Yurtsever, and Cevher]{levy2018online}
Levy, K.~Y., Yurtsever, A., and Cevher, V.
\newblock Online adaptive methods, universality and acceleration.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, 2018.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Russell-Rose, and Li]{lewis2004rcv1}
Lewis, D.~D., Yang, Y., Russell-Rose, T., and Li, F.
\newblock Rcv1: A new benchmark collection for text categorization research.
\newblock \emph{Journal of machine learning research}, 5\penalty0
  (Apr):\penalty0 361--397, 2004.

\bibitem[Malitsky \& Pock(2018)Malitsky and Pock]{malitsky2018first}
Malitsky, Y. and Pock, T.
\newblock A first-order primal-dual algorithm with linesearch.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  411--432, 2018.

\bibitem[Mishchenko \& Richt{\'a}rik(2019)Mishchenko and
  Richt{\'a}rik]{mishchenko2019stochastic}
Mishchenko, K. and Richt{\'a}rik, P.
\newblock A stochastic decoupling method for minimizing the sum of smooth and
  non-smooth functions.
\newblock \emph{arXiv preprint arXiv:1905.11535}, 2019.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov(2015)]{nesterov2015universal}
Nesterov, Y.
\newblock Universal gradient methods for convex optimization problems.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1):\penalty0 381--404,
  2015.

\bibitem[Ouyang et~al.(2013)Ouyang, He, Tran, and Gray]{ouyang2013stochastic}
Ouyang, H., He, N., Tran, L., and Gray, A.
\newblock Stochastic alternating direction method of multipliers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  80--88.
  PMLR, 2013.

\bibitem[Pedregosa(2016)]{pedregosa2016convergence}
Pedregosa, F.
\newblock On the convergence rate of the three operator splitting scheme.
\newblock \emph{arXiv preprint arXiv:1610.07830}, 2016.

\bibitem[Pedregosa \& Gidel(2018)Pedregosa and Gidel]{pedregosa2018adaptive}
Pedregosa, F. and Gidel, G.
\newblock Adaptive three operator splitting.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4085--4094, 2018.

\bibitem[Pedregosa et~al.(2019)Pedregosa, Fatras, and
  Casotto]{pedregosa2019proximal}
Pedregosa, F., Fatras, K., and Casotto, M.
\newblock Proximal splitting meets variance reduction.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1--10. PMLR, 2019.

\bibitem[Pedregosa et~al.(2020)Pedregosa, Negiar, and Dresdner]{copt}
Pedregosa, F., Negiar, G., and Dresdner, G.
\newblock copt: composite optimization in python.
\newblock 2020.
\newblock \doi{10.5281/zenodo.1283339}.
\newblock URL \url{http://openo.pt/copt/}.

\bibitem[Raguet et~al.(2013)Raguet, Fadili, and
  Peyr{\'e}]{raguet2013generalized}
Raguet, H., Fadili, J., and Peyr{\'e}, G.
\newblock A generalized forward-backward splitting.
\newblock \emph{SIAM Journal on Imaging Sciences}, 6\penalty0 (3):\penalty0
  1199--1226, 2013.

\bibitem[Rakhlin \& Sridharan(2013)Rakhlin and
  Sridharan]{rakhlin2013optimization}
Rakhlin, A. and Sridharan, K.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In \emph{Proceedings of the 26th International Conference on Neural
  Information Processing Systems-Volume 2}, pp.\  3066--3074, 2013.

\bibitem[Salim et~al.(2020)Salim, Condat, Mishchenko, and
  Richt{\'a}rik]{salim2020dualize}
Salim, A., Condat, L., Mishchenko, K., and Richt{\'a}rik, P.
\newblock Dualize, split, randomize: Fast nonsmooth optimization algorithms.
\newblock \emph{arXiv preprint arXiv:2004.02635}, 2020.

\bibitem[Scardapane et~al.(2017)Scardapane, Comminiello, Hussain, and
  Uncini]{scardapane2017group}
Scardapane, S., Comminiello, D., Hussain, A., and Uncini, A.
\newblock Group sparse regularization for deep neural networks.
\newblock \emph{Neurocomputing}, 241:\penalty0 81--89, 2017.

\bibitem[Shivanna et~al.(2015)Shivanna, Chatterjee, Sankaran, Bhattacharyya,
  and Bach]{shivanna2015spectral}
Shivanna, R., Chatterjee, B., Sankaran, R., Bhattacharyya, C., and Bach, F.
\newblock Spectral norm regularization of orthonormal representations for graph
  transduction.
\newblock In \emph{Neural Information Processing Systems}, 2015.

\bibitem[Tibshirani et~al.(2011)Tibshirani, Hoefling, and
  Tibshirani]{tibshirani2011nearly}
Tibshirani, R.~J., Hoefling, H., and Tibshirani, R.
\newblock Nearly-isotonic regression.
\newblock \emph{Technometrics}, 53\penalty0 (1):\penalty0 54--61, 2011.

\bibitem[Toh et~al.(1999)Toh, Todd, and T{\"u}t{\"u}nc{\"u}]{toh1999sdpt3}
Toh, K.-C., Todd, M.~J., and T{\"u}t{\"u}nc{\"u}, R.~H.
\newblock {SDPT3}—a {MATLAB} software package for semidefinite programming,
  version 1.3.
\newblock \emph{Optimization methods and software}, 11\penalty0 (1-4):\penalty0
  545--581, 1999.

\bibitem[V{\~u}(2013)]{vu2013splitting}
V{\~u}, B.~C.
\newblock A splitting algorithm for dual monotone inclusions involving
  cocoercive operators.
\newblock \emph{Advances in Computational Mathematics}, 38\penalty0
  (3):\penalty0 667--681, 2013.

\bibitem[Yan(2018)]{yan2018new}
Yan, M.
\newblock A new primal--dual algorithm for minimizing the sum of three
  functions with a linear operator.
\newblock \emph{Journal of Scientific Computing}, 76\penalty0 (3):\penalty0
  1698--1717, 2018.

\bibitem[Yuan et~al.(2011)Yuan, Liu, and Ye]{yuan2011efficient}
Yuan, L., Liu, J., and Ye, J.
\newblock Efficient methods for overlapping group lasso.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 352--360, 2011.

\bibitem[Yuan \& Lin(2006)Yuan and Lin]{yuan2006model}
Yuan, M. and Lin, Y.
\newblock Model selection and estimation in regression with grouped variables.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 68\penalty0 (1):\penalty0 49--67, 2006.

\bibitem[Yurtsever et~al.(2016)Yurtsever, V{\~u}, and
  Cevher]{yurtsever2016stochastic}
Yurtsever, A., V{\~u}, B.~C., and Cevher, V.
\newblock Stochastic three-composite convex minimization.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  4329--4337, 2016.

\bibitem[Yurtsever et~al.(2018)Yurtsever, Fercoq, Locatello, and
  Cevher]{yurtsever2018conditional}
Yurtsever, A., Fercoq, O., Locatello, F., and Cevher, V.
\newblock A conditional gradient framework for composite convex minimization
  with applications to semidefinite programming.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5727--5736. PMLR, 2018.

\bibitem[Yurtsever et~al.(2021)Yurtsever, Mangalick, and
  Sra]{yurtsever2021three}
Yurtsever, A., Mangalick, V., and Sra, S.
\newblock Three operator splitting with a nonconvex loss function.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12267--12277. PMLR, 2021.

\bibitem[Zeng \& So(2018)Zeng and So]{Zeng2018}
Zeng, W.-J. and So, H.~C.
\newblock Outlier--robust matrix completion via $\ell_p$-minimization.
\newblock \emph{{IEEE} Trans. on Sig. Process}, 66\penalty0 (5):\penalty0
  1125--1140, 2018.

\bibitem[Zhao \& Cevher(2018)Zhao and Cevher]{zhao2018stochastic}
Zhao, R. and Cevher, V.
\newblock Stochastic three-composite convex minimization with a linear
  operator.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  765--774. PMLR, 2018.

\bibitem[Zhao et~al.(2019)Zhao, Haskell, and Tan]{zhao2019optimal}
Zhao, R., Haskell, W.~B., and Tan, V.~Y.
\newblock An optimal algorithm for stochastic three-composite optimization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  428--437. PMLR, 2019.

\end{thebibliography}
