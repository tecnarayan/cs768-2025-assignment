\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angerd et~al.(2020)Angerd, Balasubramanian, and
  Annavaram]{Angerd/etal/2020}
Angerd, A., Balasubramanian, K., and Annavaram, M.
\newblock Distributed training of graph convolutional networks using subgraph
  approximation.
\newblock \emph{ICLR submission}, 2020.

\bibitem[Bottoue \& Bousquet(2007)Bottoue and Bousquet]{Bottou/Bousquet/2007}
Bottoue, L. and Bousquet, O.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{NIPS}, 2007.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Ma, and Xiao]{Chen/etal/2018b}
Chen, J., Ma, T., and Xiao, C.
\newblock {FastGCN}: Fast learning with graph convolutional networks via
  importance sampling.
\newblock In \emph{ICLR}, 2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Zhu, and Song]{Chen/etal/2018}
Chen, J., Zhu, J., and Song, L.
\newblock Stochastic training of graph convolutional networks with variance
  reduction.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Wei, Ding, Li, Yuan, Du, and
  Wen]{Chen/etal/2020b}
Chen, M., Wei, Z., Ding, B., Li, Y., Yuan, Y., Du, X., and Wen, J.~R.
\newblock Scalable graph neural networks via bidirectional propagation.
\newblock In \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Wei, Huang, Ding, and
  Li]{Chen/etal/2020}
Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y.
\newblock Simple and deep graph convolutional networks.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Chiang et~al.(2019)Chiang, Liu, Si, Li, Bengio, and
  Hsieh]{Chiang/etal/2019}
Chiang, W.~L., Liu, X., Si, S., Li, Y., Bengio, S., and Hsieh, C.~J.
\newblock {Cluster-GCN}: An efficient algorithm for training deep and large
  graph convolutional networks.
\newblock In \emph{KDD}, 2019.

\bibitem[Cong et~al.(2020)Cong, Forsati, Kandemir, and Mahdavi]{Cong/etal/2020}
Cong, W., Forsati, R., Kandemir, M., and Mahdavi, M.
\newblock Minimal variance sampling with provable guarantees for fast training
  of graph neural networks.
\newblock In \emph{KDD}, 2020.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`{o}}, and
  Veli{\v{c}}kovi{\'{c}}]{Corso/etal/2020}
Corso, G., Cavalleri, L., Beaini, D., Li{\`{o}}, P., and
  Veli{\v{c}}kovi{\'{c}}, P.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Dhillon et~al.(2007)Dhillon, Guan, and Kulis]{Dhillon/etal/2007}
Dhillon, I.~S., Guan, Y., and Kulis, B.
\newblock Weighted graph cuts without eigenvectors: A multilevel approach.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 29\penalty0 (11):\penalty0 1944--1957, 2007.

\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
  Bresson]{Dwivedi/etal/2020}
Dwivedi, V.~P., Joshi, C.~K., Laurent, T., Bengio, Y., and Bresson, X.
\newblock Benchmarking graph neural networks.
\newblock \emph{CoRR}, abs/2003.00982, 2020.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{Fey/Lenssen/2019}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR-W}, 2019.

\bibitem[Frasca et~al.(2020)Frasca, Rossi, Eynard, Chamberlain, Bronstein, and
  Monti]{Frasca/etal/2020}
Frasca, F., Rossi, E., Eynard, D., Chamberlain, B., Bronstein, M.~M., and
  Monti, F.
\newblock {SIGN}: Scalable inception graph neural networks.
\newblock In \emph{ICML-W}, 2020.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{Gilmer/etal/2017}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{ICML}, 2017.

\bibitem[Hamilton(2020)]{Hamilton/2020}
Hamilton, W.~L.
\newblock Graph representation learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 14\penalty0 (3):\penalty0 1--159, 2020.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{Hamilton/etal/2017}
Hamilton, W.~L., Ying, R., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{NIPS}, 2017.

\bibitem[Hornik(1991)]{Hornik/1991}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{Hornik/etal/1989}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{Hu/etal/2020}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock {Open Graph Benchmark}: Datasets for machine learning on graphs.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Huang et~al.(2021)Huang, He, Singh, Lim, and Benson]{Huang/etal/2021}
Huang, Q., He, H., Singh, A., Lim, S.~N., and Benson, A.~R.
\newblock Combining label propagation and simple models out-performs graph
  neural networks.
\newblock In \emph{ICLR}, 2021.

\bibitem[Huang et~al.(2018)Huang, Zhang, Rong, and Huang]{Huang/etal/2018}
Huang, W., Zhang, T., Rong, Y., and Huang, J.
\newblock Adaptive sampling towards fast graph representation learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Karypis \& Kumar(1998)Karypis and Kumar]{Karypis/Kumar/1998}
Karypis, G. and Kumar, V.
\newblock A fast and high quality multilevel scheme for partitioning irregular
  graphs.
\newblock \emph{SIAM Journal on Scientific Computing}, 20\penalty0
  (1):\penalty0 359–--392, 1998.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{Kipf/Welling/2017}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{ICLR}, 2017.

\bibitem[Klicpera et~al.(2019{\natexlab{a}})Klicpera, Bojchevski, and
  G{\"u}nnemann]{Klicpera/etal/2019a}
Klicpera, J., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Predict then propagate: Graph neural networks meet personalized
  {PageRank}.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Klicpera et~al.(2019{\natexlab{b}})Klicpera, Weißenberger, and
  G{\"u}nnemann]{Klicpera/etal/2019b}
Klicpera, J., Weißenberger, S., and G{\"u}nnemann, S.
\newblock Diffusion improves graph learning.
\newblock In \emph{NeurIPS}, 2019{\natexlab{b}}.

\bibitem[Ma et~al.(2019)Ma, Yang, Miao, Xue, Wu, Zhou, and Dai]{Ma/etal/2019}
Ma, L., Yang, Z., Miao, Y., Xue, J., Wu, M., Zhou, L., and Dai, Y.
\newblock {NeuGraph}: Parallel deep neural network computation on large graphs.
\newblock In \emph{USENIX Annual Technical Conference}, 2019.

\bibitem[Ma \& Tang(2020)Ma and Tang]{Ma/Yang/2020}
Ma, Y. and Tang, J.
\newblock \emph{Deep Learning on Graphs}.
\newblock Cambridge University Press, 2020.

\bibitem[Markowitz et~al.(2021)Markowitz, Balasubramanian, Mirtaheri,
  Abu-El-Haija, Perozzi, Ver~Steeg, and Galstyan]{Markowitz/etal/2021}
Markowitz, E., Balasubramanian, K., Mirtaheri, M., Abu-El-Haija, S., Perozzi,
  B., Ver~Steeg, G., and Galstyan, A.
\newblock Graph traversal with tensor functionals: A meta-algorithm for
  scalable learning.
\newblock In \emph{ICLR}, 2021.

\bibitem[Maron et~al.(2019)Maron, Ben-Hamu, Serviansky, and
  Lipman]{Maron/etal/2019}
Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y.
\newblock Provably powerful graph networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Mernyei \& Cangea(2020)Mernyei and Cangea]{Mernyei/Cangea/2020}
Mernyei, P. and Cangea, C.
\newblock {Wiki-CS}: A wikipedia-based benchmark for graph neural networks.
\newblock In \emph{ICML-W}, 2020.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{Morris/etal/2019}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G.,
  and Grohe, M.
\newblock {W}eisfeiler and {L}eman go neural: Higher-order graph neural
  networks.
\newblock In \emph{AAAI}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, {DeVito}, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{Paszke/etal/2019}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  {DeVito}, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
  L., Bai, J., and Chintala, S.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Qi et~al.(2017)Qi, Yi, Su, and Guibas]{Qi/etal/2017}
Qi, C.~R., Yi, L., Su, H., and Guibas, L.~J.
\newblock {PointNet++}: Deep hierarchical feature learning on point sets in a
  metric space.
\newblock In \emph{NIPS}, 2017.

\bibitem[Rong et~al.(2020)Rong, Huang, Xu, and Huang]{Rong/etal/2020}
Rong, Y., Huang, W., Xu, T., and Huang, J.
\newblock {DropEdge}: Towards deep graph convolutional networks on node
  classification.
\newblock In \emph{ICLR}, 2020.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, and Getoor]{Sen/etal/2008}
Sen, G., Namata, G., Bilgic, M., and Getoor, L.
\newblock Collective classification in network data.
\newblock \emph{AI Magazine}, 29, 2008.

\bibitem[Seo et~al.(2019)Seo, Loukas, and Perraudin]{Seo/etal/2019}
Seo, Y., Loukas, A., and Perraudin, N.
\newblock Discriminative structural graph classification.
\newblock \emph{CoRR}, abs/1905.13422, 2019.

\bibitem[Shchur et~al.(2018)Shchur, Mumme, Bojchevski, and
  G{\"u}nnemann]{Shchur/etal/2018}
Shchur, O., Mumme, M., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Pitfalls of graph neural network evaluation.
\newblock In \emph{NeurIPS-W}, 2018.

\bibitem[Shi et~al.(2020)Shi, Huang, Wang, Zhong, Feng, and Sun]{Shi/etal/2020}
Shi, Y., Huang, Z., Wang, W., Zhong, H., Feng, S., and Sun, Y.
\newblock Masked label prediction: Unified message passing model for
  semi-supervised classification.
\newblock \emph{CoRR}, abs/2009.03509, 2020.

\bibitem[Tripathy et~al.(2020)Tripathy, Yelick, and Buluc]{Tripathy/etal/2020}
Tripathy, A., Yelick, K., and Buluc, A.
\newblock Reducing communcation in graph neural network training.
\newblock \emph{CoRR}, abs/2005.03300, 2020.

\bibitem[Usama \& Chang(2018)Usama and Chang]{Usama/Chang/2018}
Usama, M. and Chang, D.~E.
\newblock Towards robust neural networks with lipschitz continuity.
\newblock \emph{CoRR}, abs/1811.09008, 2018.

\bibitem[Valsesia et~al.(2020)Valsesia, Fracastoro, and
  Magli]{Valsesia/etal/2020}
Valsesia, D., Fracastoro, G., and Magli, E.
\newblock Don't stack layers in graph neural networks, wire them randomly.
\newblock \emph{ICLR submission}, 2020.

\bibitem[Veli{\v{c}}kovi{\'{c}} et~al.(2018)Veli{\v{c}}kovi{\'{c}}, Cucurull,
  Casanova, Romero, Li{\`{o}}, and Bengio]{Velickovic/etal/2018}
Veli{\v{c}}kovi{\'{c}}, P., Cucurull, G., Casanova, A., Romero, A., Li{\`{o}},
  P., and Bengio, Y.
\newblock Graph attention networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Wan et~al.(2020)Wan, Li, Kim, and Lin]{Wan/etal/2020}
Wan, C., Li, Y., Kim, N.~S., and Lin, Y.
\newblock {BDS-GCN}: Efficient full-graph training of graph convolutional nets
  with partition-parallelism and boundary sampling.
\newblock \emph{ICLR submission}, 2020.

\bibitem[Wang et~al.(2019)Wang, Sun, Liu, Sarma, Bronstein, and
  Solomon]{Wang/etal/2019}
Wang, Y., Sun, Y., Liu, Z., Sarma, S.~E., Bronstein, M.~M., and Solomon, J.~M.
\newblock Dynamic graph {CNN} for learning on point clouds.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 2019.

\bibitem[Weisfeiler \& Lehman(1968)Weisfeiler and
  Lehman]{Weisfeiler/Lehman/1968}
Weisfeiler, B. and Lehman, A.~A.
\newblock A reduction of a graph to a canonical form and an algebra arising
  during this reduction.
\newblock \emph{Nauchno-Technicheskaya Informatsia}, 2\penalty0 (9), 1968.

\bibitem[Wu et~al.(2019)Wu, Zhang, de~Souza~Jr., Fifty, Yu, and
  Weinberger]{Wu/etal/2019}
Wu, F., Zhang, T., de~Souza~Jr., A.~H., Fifty, C., Yu, T., and Weinberger,
  K.~Q.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Xu et~al.(2018)Xu, Li, Tian, Sonobe, Kawarabayashi, and
  Jegelka]{Xu/etal/2018}
Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K., and Jegelka, S.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{Xu/etal/2019}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{ICLR}, 2019.

\bibitem[Yang et~al.(2016)Yang, Cohen, and Salakhutdinov]{Yang/etal/2016}
Yang, Z., Cohen, W., and Salakhutdinov, R.
\newblock Revisiting semi-supervised learning with graph embeddings.
\newblock In \emph{ICML}, 2016.

\bibitem[You et~al.(2020)You, Chen, Wang, and Shen]{You/etal/2020}
You, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock {L}$^2$-{GCN}: Layer-wise and learned efficient training of graph
  convolutional networks.
\newblock In \emph{CVPR}, 2020.

\bibitem[Yu et~al.(2020)Yu, Shen, Li, and Lerer]{Yu/etal/2020}
Yu, L., Shen, J., Li, J., and Lerer, A.
\newblock Scalable graph neural networks for heterogeneous graphs.
\newblock \emph{CoRR}, abs/2011.09679, 2020.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbhakhsh, P{\'o}czos,
  Salakhutdinov, and Smola]{Zaheer/etal/2017}
Zaheer, M., Kottur, S., Ravanbhakhsh, S., P{\'o}czos, B., Salakhutdinov, R.,
  and Smola, A.~J.
\newblock Deep sets.
\newblock In \emph{NIPS}, 2017.

\bibitem[Zeng et~al.(2020{\natexlab{a}})Zeng, Zhang, Xia, Srivastava, Kannan,
  Prasanna, Jin, Malevich, and Chen]{Zeng/etal/2020b}
Zeng, H., Zhang, M., Xia, Y., Srivastava, A., Kannan, R., Prasanna, V., Jin,
  L., Malevich, A., and Chen, R.
\newblock Deep graph neural networks with shallow subgraph samplers.
\newblock \emph{CoRR}, abs/2012.01.380, 2020{\natexlab{a}}.

\bibitem[Zeng et~al.(2020{\natexlab{b}})Zeng, Zhou, Srivastava, Kannan, and
  Prasanna]{Zeng/etal/2020a}
Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V.
\newblock {GraphSAINT}: Graph sampling based inductive learning method.
\newblock In \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Zheng et~al.(2020)Zheng, Ma, Wang, Zhou, Su, Song, Gan, Zhang, and
  Karypis]{Zheng/etal/2020}
Zheng, D., Ma, C., Wang, M., Zhou, J., Su, Q., Song, X., Gan, Q., Zhang, Z.,
  and Karypis, G.
\newblock {DistDGL}: Distributed graph neural network for training for
  billion-scale graphs.
\newblock \emph{CoRR}, abs/2010.05337, 2020.

\bibitem[Zhu et~al.(2019)Zhu, Zhao, Yang, Lin, Zhou, Ai, Li, and
  Zhou]{Zhu/etal/2019}
Zhu, R., Zhao, K., Yang, H., Lin, W., Zhou, C., Ai, B., Li, Y., and Zhou, J.
\newblock {AliGraph}: A comprehensive graph neural network platform.
\newblock In \emph{KDD}, 2019.

\bibitem[Zhu et~al.(2016)Zhu, Chen, Zheng, and Ma]{Zhu/etal/2016}
Zhu, X., Chen, W., Zheng, W., and Ma, X.
\newblock Gemini: A computation-centric distributed graph processing system.
\newblock In \emph{USENIX Symposium on Operating Systems Designand
  Implementation}, 2016.

\bibitem[Zou et~al.(2019)Zou, Hu, Wang, Jiang, Sun, and Gu]{Zou/etal/2019}
Zou, D., Hu, Z., Wang, Y., Jiang, S., Sun, Y., and Gu, Q.
\newblock Layer-dependent importance sampling for training deep and large graph
  convolutional networks.
\newblock In \emph{NeurIPS}, 2019.

\end{thebibliography}
