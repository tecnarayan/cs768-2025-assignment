@book{sutton,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{kakade,
  author    = {Sham M. Kakade and
               John Langford},
  title     = {Approximately Optimal Approximate Reinforcement Learning},
  booktitle = {Proceedings of the 19th International Conference on Machine Learning (ICML'02)},
  pages     = {267--274},
  address   = {Sydney, Australia},
  year      = {2002},
}




paper

@inproceedings{diff,
  author    = {Angelos Katharopoulos and
               Fran{\c{c}}ois Fleuret},
  title     = {Not All Samples Are Created Equal: Deep Learning with Importance Sampling},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML'18)},
  pages     = {2530--2539},
  address   = {Stockholmsm{\"{a}}ssan, Sweden},
  year      = {2018}
}

@article{er,
  author    = {Long Ji Lin},
  title     = {Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching},
  journal   = {Journal of Machine Learning Research},
  volume    = {8},
  pages     = {293--321},
  year      = {1992}
}

@inproceedings{sweep1,
  author    = {David Andre and
               Nir Friedman and
               Ronald Parr},
  title     = {Generalized Prioritized Sweeping},
  booktitle = {proceedings of the 10th conference on Neural Information Processing Systems (NeurIPS'97)},
  address   = {Denver, CO},
  pages     = {1001--1007},
  year      = {1997}
}

@article{sweep2,
  author    = {Andrew W. Moore and
               Christopher G. Atkeson},
  title     = {Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time},
  journal   = {Journal of Machine Learning Research},
  volume    = {13},
  pages     = {103--130},
  year      = {1993}
}

@inproceedings{sweep3,
  author    = {Harm van Seijen and
               Richard S. Sutton},
  title     = {Planning by Prioritized Sweeping with Small Backups},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML'13)},
  pages     = {361--369},
  year      = {2013}
}





RL algorithm
@inproceedings{trpo,
  author    = {John Schulman and
               Sergey Levine and
               Pieter Abbeel and
               Michael I. Jordan and
               Philipp Moritz},
  title     = {Trust Region Policy Optimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML'15)},
  pages     = {1889--1897},
  address   = {Lille, France},
  year      = {2015}

}

@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017}
}

@inproceedings{a3c,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning (ICML'16)},
  pages     = {1928--1937},
  year      = {2016}
}

@inproceedings{c51,
  author    = {Marc G. Bellemare and
               Will Dabney and
               R{\'{e}}mi Munos},
  title     = {A Distributional Perspective on Reinforcement Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML'17)},
  pages     = {449--458},
  address   = {Sydney, Australia},
  year      = {2017}
}

@inproceedings{dpg,
  author    = {David Silver and
               Guy Lever and
               Nicolas Heess and
               Thomas Degris and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Deterministic Policy Gradient Algorithms},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning (ICML'14)},
  address   = {Beijing, China},
  pages     = {387--395},
  year      = {2014}
}

@inproceedings{sac,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML'18)},
  address   = {Stockholmsm{\"{a}}ssan, Sweden},
  pages     = {1856--1865},
  year      = {2018},
}

@article{dqn,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Andrei A. Rusu and
               Joel Veness and
               Marc G. Bellemare and
               Alex Graves and
               Martin A. Riedmiller and
               Andreas Fidjeland and
               Georg Ostrovski and
               Stig Petersen and
               Charles Beattie and
               Amir Sadik and
               Ioannis Antonoglou and
               Helen King and
               Dharshan Kumaran and
               Daan Wierstra and
               Shane Legg and
               Demis Hassabis},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015}
}

@inproceedings{td3,
  author    = {Scott Fujimoto and
               Herke van Hoof and
               David Meger},
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML'18)},
  address   = {Stockholmsm{\"{a}}ssan, Sweden},
  pages     = {1582--1591},
  year      = {2018}
}

@inproceedings{rainbow,
  author    = {Matteo Hessel and
               Joseph Modayil and
               Hado van Hasselt and
               Tom Schaul and
               Georg Ostrovski and
               Will Dabney and
               Dan Horgan and
               Bilal Piot and
               Mohammad Gheshlaghi Azar and
               David Silver},
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  booktitle = {Proceedings of the 32nd Conference on Artificial Intelligence (AAAI'18)},
  address   = {New Orleans, LA},
  pages     = {3215--3222},
  year      = {2018}
}



Replay Buffer
@inproceedings{per,
  author    = {Tom Schaul and
               John Quan and
               Ioannis Antonoglou and
               David Silver},
  title     = {Prioritized Experience Replay},
  booktitle = {Proceedings of the 4th International Conference on Learning Representations (ICLR'16)},
  address   = {San Juan, Puerto Rico},
  year      = {2016},
}

@article{deeper,
  author    = {Shangtong Zhang and
               Richard S. Sutton},
  title     = {A Deeper Look at Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1712.01275},
  year      = {2017}
}

@inproceedings{revisit,
  author    = {William Fedus and
               Prajit Ramachandran and
               Rishabh Agarwal and
               Yoshua Bengio and
               Hugo Larochelle and
               Mark Rowland and
               Will Dabney},
  title     = {Revisiting Fundamentals of Experience Replay},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML'20)},
  address   = {virtual event},
  pages     = {3061--3071},
  year = {2020}
}

@article{sequence,
  author    = {Marc Brittain and
               Joshua R. Bertram and
               Xuxi Yang and
               Peng Wei},
  title     = {Prioritized Sequence Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1905.12726},
  year      = {2019}
}

@inproceedings{bottleneck,
  author    = {Justin Fu and
               Aviral Kumar and
               Matthew Soh and
               Sergey Levine},
  title     = {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML'19)},
  address   = {Long Beach, CA},
  pages     = {2021--2030},
  year      = {2019},
}

@inproceedings{equivalence,
  author    = {Scott Fujimoto and
               David Meger and
               Doina Precup},
  title     = {An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay},
  booktitle = {Proceedings of the 33rd Annual Conference
               on Neural Information Processing Systems (NeurIPS'20)},
  addressm  = {virtual event},
  year      = {2020},
}

@inproceedings{ere,
  author    = {Che Wang and
               Yanqiu Wu and
               Quan Vuong and
               Keith Ross},
  title     = {Striving for Simplicity and Performance in Off-Policy {DRL:} Output Normalization and Non-Uniform Sampling},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML'20)},
  pages     = {10070--10080},
  address   = {virtual event},
  year      = {2020}
}


@inproceedings{discor,
  author    = {Aviral Kumar and
               Abhishek Gupta and
               Sergey Levine},
  title     = {DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction},
  booktitle = {Proceedings of 33rd conference on Neural Information Processing Systems (NeurIPS'20)},
  address   = {virtual event},
  year      = {2020},
}

@article{gan,
  author    = {Samarth Sinha and
               Jiaming Song and
               Animesh Garg and
               Stefano Ermon},
  title     = {Experience Replay with Likelihood-free Importance Weights},
  journal   = {CoRR},
  volume    = {abs/2006.13169},
  year      = {2020}
}




Model Based
@inproceedings{SLBO,
  author    = {Yuping Luo and
               Huazhe Xu and
               Yuanzhi Li and
               Yuandong Tian and
               Trevor Darrell and
               Tengyu Ma},
  title     = {Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees},
  booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR'19)},
  address   = { New Orleans, LA},
  year      = {2019}
}

@inproceedings{MBPO,
  author    = {Michael Janner and
               Justin Fu and
               Marvin Zhang and
               Sergey Levine},
  title     = {When to Trust Your Model: {M}odel-Based Policy Optimization},
  booktitle = {Proceedings of the 32nd Neural Information Processing Systems (NeurIPS'19)},
  address   = {Vancouver, Canada},
  pages     = {12498--12509},
  year      = {2019}
}




offline RL
@inproceedings{bcq,
  author    = {Scott Fujimoto and
               David Meger and
               Doina Precup},
  title     = {Off-Policy Deep Reinforcement Learning without Exploration},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML'19)},
  pages     = {2052--2062},
  address   = {Long Beach, CA},
  year      = {2019}
}



Off policy Evaluation
@inproceedings{is,
  author    = {Doina Precup and
               Richard S. Sutton and
               Satinder P. Singh},
  title     = {Eligibility Traces for Off-Policy Policy Evaluation},
  booktitle = {Proceedings of the 17th International Conference on Machine Learning (ICML'00)},
  pages     = {759--766},
  address   = {Stanford, CA},
  year      = {2000}
}

@article{MWLMQL,
  author    = {Masatoshi Uehara and
               Nan Jiang},
  title     = {Minimax Weight and Q-Function Learning for Off-Policy Evaluation},
  journal   = {CoRR},
  volume    = {abs/1910.12809},
  year      = {2019}
}

@inproceedings{dice,
  author    = {Qiang Liu and
               Lihong Li and
               Ziyang Tang and
               Dengyong Zhou},
  title     = {Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation},
  booktitle = {Proceedings of the 31st Neural Information Processing Systems (NeurIPS'18)},
  pages     = {5361--5371},
  address   = {Montr{\'{e}}al, Canada},
  year      = {2018}
}


@inproceedings{gendice,
  author    = {Ruiyi Zhang and
               Bo Dai and
               Lihong Li and
               Dale Schuurmans},
  title     = {GenDICE: Generalized Offline Estimation of Stationary Values},
  booktitle = {Proceedings of the 8th International Conference on Learning Representations (ICLR'20)},
  address   = {Addis Ababa, Ethiopia},
  year      = {2020}
}

@inproceedings{dualdice,
  author    = {Ofir Nachum and
               Yinlam Chow and
               Bo Dai and
               Lihong Li},
  title     = {DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections},
  booktitle = {Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS'19)},
  pages     = {2315--2325},
  address   = {Vancouver, Canada},
  year      = {2019}
}

@inproceedings{DR,
  author    = {Nan Jiang and
               Lihong Li},
  editor    = {Maria{-}Florina Balcan and
               Kilian Q. Weinberger},
  title     = {Doubly Robust Off-policy Value Evaluation for Reinforcement Learning},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  pages     = {652--661},
  publisher = {JMLR.org},
  year      = {2016},
}

@inproceedings{MAGIC,
  author    = {Philip S. Thomas and
               Emma Brunskill},
  title     = {Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML'16)},
  pages     = {2139--2148},
  address   = {New York City, NY},
  year      = {2016}
}

@inproceedings{xutian,
  author    = {Tian Xu and
               Ziniu Li and
               Yang Yu},
  title     = {Error Bounds of Imitating Policies and Environments},
  booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'20)},
  address   = {virtual event},
  year      = {2020}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}}
}


@misc{tianshou,
  author = {Jiayi Weng and Huayu Chen and Alexis Duburcq and Kaichao You and Minghao Zhang and Dong Yan and Hang Su and Jun Zhu},
  title = {Tianshou},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/thu-ml/tianshou}}
}

@inproceedings{mujoco,
  author    = {Emanuel Todorov and
               Tom Erez and
               Yuval Tassa},
  title     = {MuJoCo: {A} physics engine for model-based control},
  booktitle = {Proceedings of 24th International Conference on Intelligent Robots and Systems (IROS'12)},
  pages     = {5026--5033},
  address   = {Vilamoura, Portugal},
  year      = {2012}
}

@inproceedings{metaworld,
  title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Karol Hausman and Chelsea Finn and Sergey Levine},
  booktitle={Proceedings of the 3rd Conference on Robot Learning (CoRL'19)},
  address = {Osaka, Japan},
  year={2019}
}

@misc{tassa2020dmcontrol,
    title={dm-control: Software and Tasks for Continuous Control},
    author={Yuval Tassa and Saran Tunyasuvunakool and Alistair Muldal and
            Yotam Doron and Siqi Liu and Steven Bohez and Josh Merel and
            Tom Erez and Timothy Lillicrap and Nicolas Heess},
    year={2020},
    eprint={2006.12983},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@inproceedings{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle={Proceedings of the 4th International Conference on Learning Representations (ICLR'16)},
  year={2016}
}

@article{redq,
  author    = {Xinyue Chen and
               Che Wang and
               Zijian Zhou and
               Keith Ross},
  title     = {Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},
  journal   = {CoRR},
  volume    = {abs/2101.05982},
  year      = {2021}
}

@inproceedings{lee2020sunrise,
  author    = {Kimin Lee and
               Michael Laskin and
               Aravind Srinivas and
               Pieter Abbeel},
  title     = {{SUNRISE:} {A} Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML'21)},
  address   = {virtual event},
  pages     = {6131--6141},
  year      = {2021}
}


@inproceedings{tradeoff,
  author    = {L{\'{e}}on Bottou and
               Olivier Bousquet},
  title     = {The Tradeoffs of Large Scale Learning},
  booktitle = {Proceedings of the 20th Conference on Neural Information Processing (NeurIPS'07)},
  pages     = {161--168},
  address   = {Vancouver, Canada},
  year      = {2007}
}

@inproceedings{svrg,
  author    = {Rie Johnson and
               Tong Zhang},
  title     = {Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
  booktitle = {Proceedings of the 26th Conference on Neural Information Processing (NeurIPS'13)},
  pages     = {315--323},
  address   = {Lake Tahoe, NV},
  year      = {2013},
}

@article{aggregate,
  author    = {Shi Dong and
               Benjamin Van Roy and
               Zhengyuan Zhou},
  title     = {Provably Efficient Reinforcement Learning with Aggregated States},
  journal   = {CoRR},
  volume    = {abs/1912.06366},
  year      = {2019}
}

@inproceedings{abstract,
  author    = {Nan Jiang and
               Alex Kulesza and
               Satinder P. Singh},
  title     = {Abstraction Selection in Model-based Reinforcement Learning},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML'15)},
  pages     = {179--188},
  address   = {Lille, France},
  year      = {2015}
}
