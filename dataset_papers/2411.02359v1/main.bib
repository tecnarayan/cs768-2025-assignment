@article{saycan,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={JMLR},
  year={2023}
}

@article{palme,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={ICML},
  year={2023}
}

@article{rt-1,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={Proceedings of Robotics: Science and Systems},
  year={2024}
}


@article{rt-2,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={CoRL},
  year={2023}
}

@article{rt-x,
  title={Open x-embodiment: Robotic learning datasets and rt-x models},
  author={Padalkar, Abhishek and Pooley, Acorn and Jain, Ajinkya and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Singh, Anikait and Brohan, Anthony and others},
  journal={arXiv preprint arXiv:2310.08864},
  year={2023}
}

@article{team2023octo,
  title={Octo: An open-source generalist robot policy},
  author={Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Xu, Charles and Luo, Jianlan and others},
    journal={First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024},
  year={2024}
}

@article{roboflamingo,
  title={Vision-language foundation models as effective robot imitators},
  author={Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and others},
  journal={ICLR},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

# finetune


# model
@inproceedings{vit,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle={ICCV},
  year={2021}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{achiam2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@online{mpt,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-7B: A New Standard for Open-Source,
    Commercially Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-05-05},
    urldate   = {2023-05-05}
}

@article{liu2024llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  year={2024}
}

@article{chen2023towards,
  title={Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond},
  author={Chen, Liang and Zhang, Yichi and Ren, Shuhuai and Zhao, Haozhe and Cai, Zefan and Wang, Yuchi and Wang, Peiyi and Liu, Tianyu and Chang, Baobao},
  journal={arXiv preprint arXiv:2310.02071},
  year={2023}
}

@article{mu2024embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={NeurIPS},
  year={2023}
}

@article{graves2012long,
  title={Long short-term memory},
  author={Graves, Alex and Graves, Alex},
  journal={Supervised sequence labelling with recurrent neural networks},
  pages={37--45},
  year={2012},
  publisher={Springer}
}

@article{ba2016layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{seem,
  title={Understanding, predicting and better resolving q-value divergence in offline-rl},
  author={Yue, Yang and Lu, Rui and Kang, Bingyi and Song, Shiji and Huang, Gao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mees2022calvin,
  title={Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks},
  author={Mees, Oier and Hermann, Lukas and Rosete-Beas, Erick and Burgard, Wolfram},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={3},
  pages={7327--7334},
  year={2022},
  publisher={IEEE}
}

@article{HULC,
  title={What matters in language conditioned robotic imitation learning over unstructured data},
  author={Mees, Oier and Hermann, Lukas and Burgard, Wolfram},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={4},
  pages={11205--11212},
  year={2022},
  publisher={IEEE}
}

@article{MCIL,
  title={Language conditioned imitation learning over unstructured data},
  author={Lynch, Corey and Sermanet, Pierre},
  journal={arXiv preprint arXiv:2005.07648},
  year={2020}
}

# efficient inference
@inproceedings{samsi2023words,
  title={From words to watts: Benchmarking the energy costs of large language model inference},
  author={Samsi, Siddharth and Zhao, Dan and McDonald, Joseph and Li, Baolin and Michaleas, Adam and Jones, Michael and Bergeron, William and Kepner, Jeremy and Tiwari, Devesh and Gadepally, Vijay},
  booktitle={2023 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--9},
  year={2023},
  organization={IEEE}
}

@article{zhu2023survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}

@article{liu2024mobilellm,
  title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
  journal={arXiv preprint arXiv:2402.14905},
  year={2024}
}

@article{xia2023flashllm,
  title={Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity},
  author={Xia, Haojun and Zheng, Zhen and Li, Yuchao and Zhuang, Donglin and Zhou, Zhongzhu and Qiu, Xiafei and Li, Yong and Lin, Wei and Song, Shuaiwen Leon},
  journal={arXiv preprint arXiv:2309.10285},
  year={2023}
}

@article{raposo2024mixture,
  title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models},
  author={Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  journal={arXiv preprint arXiv:2404.02258},
  year={2024}
}

@inproceedings{shim2021layer,
  title={Layer-wise pruning of transformer attention heads for efficient language modeling},
  author={Shim, Kyuhong and Choi, Iksoo and Sung, Wonyong and Choi, Jungwook},
  booktitle={2021 18th International SoC Design Conference (ISOCC)},
  pages={357--358},
  year={2021},
  organization={IEEE}
}

@article{elbayad2019depth,
  title={Depth-adaptive transformer},
  author={Elbayad, Maha and Gu, Jiatao and Grave, Edouard and Auli, Michael},
  journal={ICLR},
  year={2020}
}

# dynamic network and early-exit
@article{han2021dynamic,
  title={Dynamic neural networks: A survey},
  author={Han, Yizeng and Huang, Gao and Song, Shiji and Yang, Le and Wang, Honghui and Wang, Yulin},
  journal={TPAMI},
  volume={44},
  number={11},
  pages={7436--7456},
  year={2021},
  publisher={IEEE}
}

@article{voita2019bottom,
  title={The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1909.01380},
  year={2019}
}

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald},
  journal={NeurIPS},
  year={2022}
}

# CNN EE
@inproceedings{panda2016conditional,
  title={Conditional deep learning for energy-efficient and enhanced pattern recognition},
  author={Panda, Priyadarshini and Sengupta, Abhronil and Roy, Kaushik},
  year={2016},
}

@article{teerapittayanon2016branchynet,
  title={Branchynet: Fast inference via early exiting from deep neural networks},
  author={Teerapittayanon, Surat and McDanel, Bradley and Kung, Hsiang-Tsung},
journal={ICPR},
  year={2016},
}

@article{msdnet,
  title={Multi-scale dense networks for resource efficient image classification},
  author={Huang, Gao and Chen, Danlu and Li, Tianhong and Wu, Felix and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  journal={ICLR},
  year={2018}
}

@inproceedings{bolukbasi2017adaptive,
  title={Adaptive neural networks for efficient inference},
  author={Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
  booktitle={ICML},
  year={2017},
}

# LM EE
# encoder
@inproceedings{xin2021berxit,
  title={BERxiT: Early exiting for BERT with better fine-tuning and extension to regression},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={ACL},  
year={2021}
}

@article{zhou2020patience,
  title={Bert loses patience: Fast and robust inference with early exit},
  author={Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
  journal={NeurIPS},
  year={2020}
}

@article{xin2020deebert,
  title={DeeBERT: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}

@article{liu2020fastbert,
  title={Fastbert: a self-distilling bert with adaptive inference time},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Deng, Haotang and Ju, Qi},
  journal={arXiv preprint arXiv:2004.02178},
  year={2020}
}

@inproceedings{mangrulkar2022be3r,
  title={BE3R: BERT based Early-Exit Using Expert Routing},
  author={Mangrulkar, Sourab and MS, Ankith and Sembium, Vivek},
  booktitle={KDD},
  year={2022}
}

@article{schuster2021consistent,
  title={Consistent accelerated inference via confident adaptive transformers},
  author={Schuster, Tal and Fisch, Adam and Jaakkola, Tommi and Barzilay, Regina},
  journal={arXiv preprint arXiv:2104.08803},
  year={2021}
}

# decoder
@article{del2023skipdecode,
  title={Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference},
  author={Del Corro, Luciano and Del Giorno, Allie and Agarwal, Sahaj and Yu, Bin and Awadallah, Ahmed and Mukherjee, Subhabrata},
  journal={arXiv preprint arXiv:2307.02628},
  year={2023}
}

@inproceedings{fei2022deecap,
  title={Deecap: Dynamic early exiting for efficient image captioning},
  author={Fei, Zhengcong and Yan, Xu and Wang, Shuhui and Tian, Qi},
  booktitle={CVPR},
  pages={12216--12226},
  year={2022}
}

# llm
@article{elhoushi2024layerskip,
  title={Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding},
  author={Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and others},
  journal={arXiv preprint arXiv:2404.16710},
  year={2024}
}

# vit and vision-language model
@inproceedings{xu2023lgvit,
  title={Lgvit: Dynamic early exiting for accelerating vision transformer},
  author={Xu, Guanyu and Hao, Jiawei and Shen, Li and Hu, Han and Luo, Yong and Lin, Hui and Shen, Jialie},
  booktitle={ACM MM},
  year={2023}
}

@inproceedings{tang2023you,
  title={You need multiple exiting: Dynamic early exiting for accelerating unified vision language model},
  author={Tang, Shengkun and Wang, Yaqing and Kong, Zhenglun and Zhang, Tianchi and Li, Yao and Ding, Caiwen and Wang, Yanzhi and Liang, Yi and Xu, Dongkuan},
  booktitle={CVPR},
  year={2023}
}

# video
@inproceedings{ghodrati2021frameexit,
  title={Frameexit: Conditional early exiting for efficient video recognition},
  author={Ghodrati, Amir and Bejnordi, Babak Ehteshami and Habibian, Amirhossein},
  booktitle={CVPR},
  year={2021}
}

# robotics
@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={ICML},
  year={2019},
}


@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={ICCV},
  year={2019}
}

@article{li2018iot,
  title={Learning IoT in edge: Deep learning for the Internet of Things with edge computing},
  author={Li, He and Ota, Kaoru and Dong, Mianxiong},
  journal={IEEE network},
  year={2018},
}



@article{hossain2019deep,
  title={Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices},
  author={Hossain, Sabir and Lee, Deok-jin},
  year={2019},
}

@inproceedings{slimming,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2736--2744},
  year={2017}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={CVPR},
  year={2018}
}

@article{wake2023gpt4-V,
  title={GPT-4V (ision) for robotics: Multimodal task planning from human demonstration},
  author={Wake, Naoki and Kanehira, Atsushi and Sasabuchi, Kazuhiro and Takamatsu, Jun and Ikeuchi, Katsushi},
  journal={arXiv preprint arXiv:2311.12015},
  year={2023}
}

@misc{wu2023unleashing,
      title={Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation}, 
      author={Hongtao Wu and Ya Jing and Chilam Cheang and Guangzeng Chen and Jiafeng Xu and Xinghang Li and Minghuan Liu and Hang Li and Tao Kong},
      year={2023},
      eprint={2312.13139},
      archivePrefix={arXiv},
      primaryClass={cs.RO}}


@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and others},
  journal={arXiv preprint arXiv:2312.03863},
  volume={1},
  year={2023}
}
@article{zhou2024survey,
  title={A Survey on Efficient Inference for Large Language Models},
  author={Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others},
  journal={arXiv preprint arXiv:2404.14294},
  year={2024}
}
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={EMNLP},
  year={2023}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@article{peng2023rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={Findings of EMNLP},
  year={2023}
}
@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}
@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}
@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  pages={10323--10337},
  year={2023},
}
@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}
@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={NeurIPS},
  volume={36},
  pages={21702--21720},
  year={2023}
}
@inproceedings{park2023lut,
  title={LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models},
  author={Park, Gunho and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo and others},
  booktitle={ICLR},
  year={2023}
}
@article{chee2023quip,
  title={Quip: 2-bit quantization of large language models with guarantees},
  author={Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M},
  journal={NeurIPS},
  year={2023}
}
@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={ICML},
  pages={38087--38099},
  year={2023}
}
@inproceedings{yang2020resolution,
  title={Resolution adaptive networks for efficient inference},
  author={Yang, Le and Han, Yizeng and Chen, Xi and Song, Shiji and Dai, Jifeng and Huang, Gao},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{han2023dynamic,
  title={Dynamic perceiver for efficient visual recognition},
  author={Han, Yizeng and Han, Dongchen and Liu, Zeyu and Wang, Yulin and Pan, Xuran and Pu, Yifan and Deng, Chao and Feng, Junlan and Song, Shiji and Huang, Gao},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yue2023value,
  title={Value-consistent representation learning for data-efficient reinforcement learning},
  author={Yue, Yang and Kang, Bingyi and Xu, Zhongwen and Huang, Gao and Yan, Shuicheng},
  booktitle={AAAI},
  year={2023}
}

@article{dosovitskiy2020vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2021}
}

@article{smallwood1973pomdp,
  title={The optimal control of partially observable Markov processes over a finite horizon},
  author={Smallwood, Richard D and Sondik, Edward J},
  journal={Operations research},
  year={1973},
}

@article{lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  year={1997},
}

@inproceedings{brooks2023instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle={CVPR},
  year={2023}
}
@article{susie,
  title={Zero-shot robotic manipulation with pretrained image-editing diffusion models},
  author={Black, Kevin and Nakamoto, Mitsuhiko and Atreya, Pranav and Walke, Homer and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  journal={ICLR},
  year={2024}
}

@inproceedings{mees2023hulc++,
  title={Grounding language with visual affordances over unstructured data},
  author={Mees, Oier and Borja-Diaz, Jessica and Burgard, Wolfram},
  booktitle={ICRA},
  year={2023},
}

@article{zhang2022lcd,
  title={Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks},
  author={Zhang, Edwin and Lu, Yujie and Wang, William and Zhang, Amy},
  journal={arXiv preprint arXiv:2210.15629},
  year={2022}
}

@article{zhou2023spil,
  title={Language-conditioned imitation learning with base skill priors under unstructured data},
  author={Zhou, Hongkuan and Bing, Zhenshan and Yao, Xiangtong and Su, Xiaojie and Yang, Chenguang and Huang, Kai and Knoll, Alois},
  journal={ICML},
  year={2024}
}

@article{tellex2020robots,
  title={Robots that use language},
  author={Tellex, Stefanie and Gopalan, Nakul and Kress-Gazit, Hadas and Matuszek, Cynthia},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  year={2020},
  publisher={Annual Reviews}
}

@article{chen2023ee,
  title={Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism},
  author={Chen, Yanxi and Pan, Xuchen and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  journal={arXiv preprint arXiv:2312.04916},
  year={2023}
}

@article{shahriari2015bayes,
  title={Taking the human out of the loop: A review of Bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  year={2015},
  publisher={IEEE}
}

@article{Sutton1998RLbook,
  title={Reinforcement Learning: An Introduction},
  author={Richard S. Sutton and Andrew G. Barto},
  year={1998},
}

@article{yang2023dawn,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  year={2023}
}

@article{li2024evaluating,
  title={Evaluating quantized large language models},
  author={Li, Shiyao and Ning, Xuefei and Wang, Luning and Liu, Tengxuan and Shi, Xiangsheng and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  journal={ICML},
  year={2024}
}

@inproceedings{li2023llm,
  title={Llm-mq: Mixed-precision quantization for efficient llm deployment},
  author={Li, Shiyao and Ning, Xuefei and Hong, Ke and Liu, Tengxuan and Wang, Luning and Li, Xiuhong and Zhong, Kai and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  booktitle={The Efficient Natural Language and Speech Processing Workshop with NeurIPS},
  volume={9},
  year={2023}
}

@article{wang2021not,
  title={Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition},
  author={Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11960--11973},
  year={2021}
}

@article{wang2020glance,
  title={Glance and focus: a dynamic approach to reducing spatial redundancy in image classification},
  author={Wang, Yulin and Lv, Kangchen and Huang, Rui and Song, Shiji and Yang, Le and Huang, Gao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2432--2444},
  year={2020}
}

@ARTICLE{10508473,
  author={Han, Yizeng and Liu, Zeyu and Yuan, Zhihang and Pu, Yifan and Wang, Chaofei and Song, Shiji and Huang, Gao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Latency-aware Unified Dynamic Networks for Efficient Image Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TPAMI.2024.3393530}}

@inproceedings{yang2021condensenet,
  title={Condensenet v2: Sparse feature reactivation for deep networks},
  author={Yang, Le and Jiang, Haojun and Cai, Ruojin and Wang, Yulin and Song, Shiji and Huang, Gao and Tian, Qi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{Ni2024AdaNAT,
  title={AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation.},
  author={Ni, Zanlin and Wang, Yulin and Zhou, Renping and Lu, Rui and Guo, Jiayi and Hu, Jinyi and Liu, Zhiyuan and Yao, Yuan and Huang, Gao},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{han2023flatten,
  title={Flatten transformer: Vision transformer using focused linear attention},
  author={Han, Dongchen and Pan, Xuran and Han, Yizeng and Song, Shiji and Huang, Gao},
  booktitle={ICCV},
  year={2023}
}

@article{wang2024model,
  title={Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing},
  author={Wang, Huanqian and Yue, Yang and Lu, Rui and Shi, Jingxin and Zhao, Andrew and Wang, Shenzhi and Song, Shiji and Huang, Gao},
  journal={arXiv preprint arXiv:2407.08770},
  year={2024}
}

@article{fang2024real,
  title={Real-world Image Dehazing with Coherence-based Label Generator and Cooperative Unfolding Network},
  author={Fang, Chengyu and He, Chunming and Xiao, Fengyang and Zhang, Yulun and Tang, Longxiang and Zhang, Yuelin and Li, Kai and Li, Xiu},
  journal={arXiv preprint arXiv:2406.07966},
  year={2024}
}