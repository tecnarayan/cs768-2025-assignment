\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2017)Aljundi, Chakravarty, and Tuytelaars]{aljundi2017expert}
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  3366--3375, 2017.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International conference on machine learning}, pp.\  233--242. PMLR, 2017.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock2021deep}
Robert John~Nicholas Baldock, Hartmut Maennel, and Behnam Neyshabur.
\newblock Deep learning through the lens of example difficulty.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=fmgYOUahK9}.

\bibitem[Barlow(1972)]{Barlow1972}
H.~B. Barlow.
\newblock Single units and sensation: A neuron doctrine for perceptual psychology.
\newblock \emph{Perception}, 1:\penalty0 371--394, 1972.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (48):\penalty0 30063--30070, 2020.

\bibitem[Carlini et~al.(2019)Carlini, Erlingsson, and Papernot]{carlini2019distribution}
Nicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot.
\newblock Distribution density, tails, and outliers in machine learning: Metrics and applications.
\newblock \emph{arXiv preprint arXiv:1910.13427}, 2019.

\bibitem[Carlini et~al.(2021)Carlini, Tram{\`e}r, Wallace, Jagielski, Herbert-Voss, Lee, Roberts, Brown, Song, Erlingsson, Oprea, and Raffel]{carlini2021extraction}
Nicholas Carlini, Florian Tram{\`e}r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, {\'U}lfar Erlingsson, Alina Oprea, and Colin Raffel.
\newblock Extracting training data from large language models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pp.\  2633--2650. USENIX Association, August 2021.
\newblock ISBN 978-1-939133-24-3.
\newblock URL \url{https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting}.

\bibitem[Chatterjee(2020)]{Chatterjee2020Coherent}
Satrajit Chatterjee.
\newblock Coherent gradients: An approach to understanding generalization in gradient descent-based optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ryeFY0EFwS}.

\bibitem[Cheung et~al.(2019)Cheung, Terekhov, Chen, Agrawal, and Olshausen]{cheung2019superposition}
Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen.
\newblock Superposition of many models into one.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Deng(2012)]{deng2012mnist}
Li~Deng.
\newblock The mnist database of handwritten digit images for machine learning research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Feldman(2020)]{feldman2020does}
Vitaly Feldman.
\newblock Does learning require memorization? a short tale about a long tail.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing}, pp.\  954--959, 2020.

\bibitem[Feldman \& Zhang(2020)Feldman and Zhang]{feldman2020neural}
Vitaly Feldman and Chiyuan Zhang.
\newblock What neural networks memorize and why: Discovering the long tail via influence estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2881--2891, 2020.

\bibitem[Frankle et~al.(2020)Frankle, Schwab, and Morcos]{Frankle2020The}
Jonathan Frankle, David~J. Schwab, and Ari~S. Morcos.
\newblock The early phase of neural network training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Hkl1iRNFwS}.

\bibitem[Haim et~al.(2022)Haim, Vardi, Yehudai, Shamir, and Irani]{haim2022reconstructing}
Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani.
\newblock Reconstructing training data from trained neural networks.
\newblock \emph{arXiv preprint arXiv:2206.07758}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Jiang et~al.(2021)Jiang, Zhang, Talwar, and Mozer]{jiang2021}
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael~C Mozer.
\newblock Characterizing structural regularities of labeled data in overparameterized models.
\newblock In Marina Meila and Tong Zhang (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  5034--5044. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/jiang21k.html}.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Computer Science Department, University of Toronto, 2009.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and Fernandez-Granda]{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 20331--20342, 2020.

\bibitem[Liu et~al.(2022)Liu, Zhu, Qu, and You]{pmlr-v162-liu22w}
Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You.
\newblock Robust training under label noise by over-parameterization.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  14153--14172. PMLR, 17--23 Jul 2022.

\bibitem[Maini et~al.(2022)Maini, Garg, Lipton, and Kolter]{maini2022characterizing}
Pratyush Maini, Saurabh Garg, Zachary~Chase Lipton, and J~Zico Kolter.
\newblock Characterizing datapoints via second-split forgetting.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=yKDKNzjHg8N}.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2022.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{netzer2011svhn}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature Learning}, 2011.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and Netrapalli]{shah2020pitfalls}
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9573--9585, 2020.

\bibitem[Sinitsin et~al.(2020)Sinitsin, Plokhotnyuk, Pyrkin, Popov, and Babenko]{Sinitsin2020Editable}
Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko.
\newblock Editable neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJedXaEtvS}.

\bibitem[Smith(2017)]{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE winter conference on applications of computer vision (WACV)}, pp.\  464--472. IEEE, 2017.

\bibitem[Smith et~al.(2021)Smith, Dherin, Barrett, and De]{smith2021on}
Samuel~L Smith, Benoit Dherin, David Barrett, and Soham De.
\newblock On the origin of implicit regularization in stochastic gradient descent.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=rq_Qr0c1Hyo}.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0 (1):\penalty0 2822--2878, 2018.

\bibitem[Stephenson et~al.(2021)Stephenson, Padhy, Ganesh, Hui, Tang, and Chung]{stephenson21geometry}
Cory Stephenson, Suchismita Padhy, Abhinav Ganesh, Yue Hui, Hanlin Tang, and SueYeon Chung.
\newblock On the geometry of generalization and memorization in deep neural networks, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.14602}.

\bibitem[Teterwak et~al.(2021)Teterwak, Zhang, Krishnan, and Mozer]{teterwak2021understanding}
Piotr Teterwak, Chiyuan Zhang, Dilip Krishnan, and Michael~C Mozer.
\newblock Understanding invariance via feedforward inversion of discriminatively trained classifiers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10225--10235. PMLR, 2021.

\bibitem[Toneva et~al.(2019)Toneva, Sordoni, des Combes, Trischler, Bengio, and Gordon]{toneva2018an}
Mariya Toneva, Alessandro Sordoni, Remi~Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey~J. Gordon.
\newblock An empirical study of example forgetting during deep neural network learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=BJlxm30cKm}.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3987--3995. PMLR, 2017.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{The International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2022)Zhang, Bengio, and Singer]{zhang2019all}
Chiyuan Zhang, Samy Bengio, and Yoram Singer.
\newblock Are all layers created equal?
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (67):\penalty0 1--28, 2022.
\newblock URL \url{http://jmlr.org/papers/v23/20-069.html}.

\bibitem[Zhu et~al.(2021)Zhu, Rawat, Zaheer, Bhojanapalli, Li, Yu, and Kumar]{zhu2021modifying}
Chen Zhu, Ankit~Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.
\newblock Modifying memories in transformer models, 2021.
\newblock URL \url{https://openreview.net/forum?id=KubHAaKdSr7}.

\end{thebibliography}
