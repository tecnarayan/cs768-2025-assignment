@PhdThesis{Watkins89,
	author =       "Watkins, Christopher John Cornish Hellaby",
	title =        "Learning from Delayed Rewards",
	school =       "King's College, University of Cambridge",
	year =         "1989",
	address =   "Cambridge, UK",
	month =     "May",
	url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
	bib2html_rescat = "Parameter",
}

@inproceedings{Schmitt23,
	author = {Schmitt, Simon and Shawe-Taylor, John and van Hasselt, Hado},
	title = {Exploration via epistemic value estimation},
	year = {2023},
	isbn = {978-1-57735-880-0},
	publisher = {AAAI Press},
	url = {https://doi.org/10.1609/aaai.v37i8.26164},
	doi = {10.1609/aaai.v37i8.26164},
	abstract = {How to efficiently explore in reinforcement learning is an open problem. Many exploration algorithms employ the epistemic uncertainty of their own value predictions - for instance to compute an exploration bonus or upper confidence bound. Unfortunately the required uncertainty is difficult to estimate in general with function approximation.We propose epistemic value estimation (EVE): a recipe that is compatible with sequential decision making and with neural network function approximators. It equips agents with a tractable posterior over all their parameters from which epistemic value uncertainty can be computed efficiently.We use the recipe to derive an epistemic Q-Learning agent and observe competitive performance on a series of benchmarks. Experiments confirm that the EVE recipe facilitates efficient exploration in hard exploration tasks.},
	booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
	articleno = {1095},
	numpages = {10},
	series = {AAAI'23/IAAI'23/EAAI'23}
}


@inproceedings{Kuss03,
	author = {Kuss, Malte and Rasmussen, Carl},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
	pages = {},
	publisher = {MIT Press},
	title = {Gaussian Processes in Reinforcement Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7993e11204b215b27694b6f139e34ce8-Paper.pdf},
	volume = {16},
	year = {2003}
}


@article{Beck23,
	author = {Beck, Jacob and Vuorio, Risto and Liu, Evan and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
	year = {2023},
	month = {01},
	pages = {},
	title = {A Survey of Meta-Reinforcement Learning},
	doi = {10.48550/arXiv.2301.08028}
}

@article{Papamakarios21,
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	title = {Normalizing flows for probabilistic modeling and inference},
	year = {2021},
	issue_date = {January 2021},
	publisher = {JMLR.org},
	volume = {22},
	number = {1},
	issn = {1532-4435},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	journal = {J. Mach. Learn. Res.},
	month = {jan},
	articleno = {57},
	numpages = {64},
	keywords = {normalizing flows, invertible neural networks, probabilistic modeling, probabilistic inference, generative models}
}

@book{Bellman61,
	url = {https://doi.org/10.1515/9781400874668},
	title = {Adaptive Control Processes},
	title = {A Guided Tour},
	author = {Richard E. Bellman},
	publisher = {Princeton University Press},
	address = {Princeton},
	doi = {doi:10.1515/9781400874668},
	isbn = {9781400874668},
	year = {1961},
	lastchecked = {2024-01-26}
}


@article{Hallak2015,
	title={Contextual Markov Decision Processes},
	author={Assaf Hallak and Dotan Di Castro and Shie Mannor},
	journal={ArXiv},
	year={2015},
	volume={abs/1502.02259},
	url={https://api.semanticscholar.org/CorpusID:14616648}
}


@book{villani2008,
	title={Optimal Transport: Old and New},
	author={Villani, C.},
	isbn={9783540710509},
	lccn={2008932183},
	series={Grundlehren der mathematischen Wissenschaften},
	url={https://books.google.co.uk/books?id=hV8o5R7_5tkC},
	year={2008},
	publisher={Springer Berlin Heidelberg}
}

@book{federer1969geometric,
	title={Geometric Measure Theory},
	author={Federer, H.},
	isbn={9783540045052},
	lccn={69016846},
	series={Grundlehren der mathematischen Wissenschaften},
	url={https://books.google.co.uk/books?id=QslkQgAACAAJ},
	year={1969},
	publisher={Springer Berlin Heidelberg}
}



@incollection{Littman95,
	title = {Learning policies for partially observable environments: Scaling up},
	editor = {Armand Prieditis and Stuart Russell},
	booktitle = {Machine Learning Proceedings 1995},
	publisher = {Morgan Kaufmann},
	address = {San Francisco (CA)},
	pages = {362-370},
	year = {1995},
	isbn = {978-1-55860-377-6},
	doi = {https://doi.org/10.1016/B978-1-55860-377-6.50052-9},
	url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500529},
	author = {Michael L. Littman and Anthony R. Cassandra and Leslie Pack Kaelbling},
	abstract = {Partially observable Markov decision processes (POMDP's) model decision problems in which an agent tries to maximize its reward in the face of limited and/or noisy sensor feedback. While the study of POMDP's is motivated by a need to address realistic problems, existing techniques for finding optimal behavior do not appear to scale well and have been unable to find satisfactory policies for problems with more than a dozen states. After a brief review of POMDP's, this paper discusses several simple solution methods and shows that all are capable of finding near- optimal policies for a selection of extremely small POMDP'S taken from the learning literature. In contrast, we show that none are able to solve a slightly larger and noisier problem based on robot navigation. We find that a combination of two novel approaches performs well on these problems and suggest methods for scaling to even larger and more complicated domains.}
}

@book{Bogachev07,
	author = {Bogachev, Vladimir I.},
	abstract = {Measure theory is a classical area of mathematics that continues intensive development and has fruitful connections with most other fields of mathematics as well as important applications in physics. This book gives a systematic presentation of modern measure theory as it has developed over the past century and offers three levels of presentation: a standard university graduate course, an advanced study containing some complements to the basic course (the material of this level corresponds to a variety of special courses), and, finally, more specialized topics partly covered by more than 850 exercises. Bibliographical and historical comments and an extensive bibliography with 2000 works covering more than a century are provided. Volume 1 is devoted to the classical theory of measure and integral. Whereas the first volume presents the ideas that go back mainly to Lebesgue, the second volume is to a large extent the result of the later development up to the recent years. The central subjects of Volume 2 are: transformations of measures, conditional measures, and weak convergence of measures. These topics are closely interwoven and form the heart of modern measure theory. The target readership includes graduate students interested in deeper knowledge of measure theory, instructors of courses in measure and integration theory, and researchers in all fields of mathematics. The book may serve as a source for many advanced courses or as a reference.},
	address = {Berlin, Heidelberg},
	edition = {1st ed. 2007.},
	isbn = {1-280-74570-3},
	keywords = {Mathematical analysis},
	language = {eng},
	publisher = {Springer Berlin Heidelberg},
	series = {Measure Theory ; 1},
	title = {Measure Theory},
	year = {2007},
}


@article{dinh2016density,
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	title = {Density estimation using Real NVP},
	url = {http://arxiv.org/abs/1605.08803},
		journal = {ICLR},
	year = {2017},
}

@misc{
	duan2017rl,
	title={{RL}{\textasciicircum}2: Fast Reinforcement Learning via Slow Reinforcement Learning},
	author={Yan Duan and John Schulman and Xi Chen and Peter L. Bartlett and Ilya Sutskever and Pieter Abbeel},
	year={2017},
	url={https://openreview.net/forum?id=HkLXCE9lx}
}



@InProceedings{zintgraf21a,
	title = 	 {Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning},
	author =       {Zintgraf, Luisa M and Feng, Leo and Lu, Cong and Igl, Maximilian and Hartikainen, Kristian and Hofmann, Katja and Whiteson, Shimon},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {12991--13001},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {18--24 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/zintgraf21a/zintgraf21a.pdf},
	url = 	 {https://proceedings.mlr.press/v139/zintgraf21a.html},
	abstract = 	 {To rapidly learn a new task, it is often essential for agents to explore efficiently - especially when performance matters from the first timestep. One way to learn such behaviour is via meta-learning. Many existing methods however rely on dense rewards for meta-training, and can fail catastrophically if the rewards are sparse. 	Without a suitable reward signal, the need for exploration during meta-training is exacerbated. To address this, we propose HyperX, which uses novel reward bonuses for meta-training to explore in approximate hyper-state space (where hyper-states represent the environment state and the agent’s task belief). We show empirically that HyperX meta-learns better task-exploration and adapts more successfully to new tasks than existing methods.}
}


@InProceedings{Lee21,
	title = 	 {Improving Generalization in Meta-learning via Task Augmentation},
	author =       {Yao, Huaxiu and Huang, Long-Kai and Zhang, Linjun and Wei, Ying and Tian, Li and Zou, James and Huang, Junzhou and Li, Zhenhui ()},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {11887--11897},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {18--24 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/yao21b/yao21b.pdf},
	url = 	 {https://proceedings.mlr.press/v139/yao21b.html},
	abstract = 	 {Meta-learning has proven to be a powerful paradigm for transferring the knowledge from previous tasks to facilitate the learning of a novel task. Current dominant algorithms train a well-generalized model initialization which is adapted to each task via the support set. The crux lies in optimizing the generalization capability of the initialization, which is measured by the performance of the adapted model on the query set of each task. Unfortunately, this generalization measure, evidenced by empirical results, pushes the initialization to overfit the meta-training tasks, which significantly impairs the generalization and adaptation to novel tasks. To address this issue, we actively augment a meta-training task with “more data” when evaluating the generalization. Concretely, we propose two task augmentation methods, including MetaMix and Channel Shuffle. MetaMix linearly combines features and labels of samples from both the support and query sets. For each class of samples, Channel Shuffle randomly replaces a subset of their channels with the corresponding ones from a different class. Theoretical studies show how task augmentation improves the generalization of meta-learning. Moreover, both MetaMix and Channel Shuffle outperform state-of-the-art results by a large margin across many datasets and are compatible with existing meta-learning algorithms.}
}



@inproceedings{Nielsen20,
	author       = {Didrik Nielsen and
	Priyank Jaini and
	Emiel Hoogeboom and
	Ole Winther and
	Max Welling},
	editor       = {Hugo Larochelle and
	Marc'Aurelio Ranzato and
	Raia Hadsell and
	Maria{-}Florina Balcan and
	Hsuan{-}Tien Lin},
	title        = {SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows},
	booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
	on Neural Information Processing Systems 2020, NeurIPS 2020, December
	6-12, 2020, virtual},
	year         = {2020},
	url          = {https://proceedings.neurips.cc/paper/2020/hash/9578a63fbe545bd82cc5bbe749636af1-Abstract.html},
	timestamp    = {Tue, 19 Jan 2021 15:57:03 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/NielsenJHWW20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{Shwartz14,
	series = {Cambridge core},
	booktitle = {Understanding machine learning : from theory to algorithms},
	isbn = {9781107298019},
	year = {2014},
	title = {Understanding machine learning : from theory to algorithms [electronic resource]},
	language = {eng},
	address = {New York, NY, USA},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	keywords = {Machine learning; Algorithms},
}

@book{Ledoux91,
	series = {Ergebnisse der Mathematik und ihrer Grenzgebiete ; 3. Folge, Band 23},
	booktitle = {Probability in Banach spaces : isoperimetry and processes},
	isbn = {9783642202124},
	year = {1991},
	title = {Probability in Banach spaces : isoperimetry and processes [electronic resource]},
	language = {eng},
	address = {Berlin},
	author = {Ledoux, Michel and Talagrand, Michel},
	keywords = {Probabilities; Banach spaces},
}



@inbook{Mendelson03, author = {Mendelson, Shahar}, title = {A Few Notes on Statistical Learning Theory}, year = {2003}, isbn = {3540005293}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, booktitle = {Advanced Lectures on Machine Learning}, pages = {1–40}, numpages = {40} }

@article{Gine84,
	author = {Evarist Gine and Joel Zinn},
	title = {{Some Limit Theorems for Empirical Processes}},
	volume = {12},
	journal = {The Annals of Probability},
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	pages = {929 -- 989},
	keywords = {central limit theorems, Empirical processes, functional Donsker classes, Gaussian processes, laws of large numbers, Metric entropy},
	year = {1984},
	doi = {10.1214/aop/1176993138},
	URL = {https://doi.org/10.1214/aop/1176993138}
}


@book{Bernardo00,
	author        = "Bernardo, Jose M and Smith, Adrian",
	title         = "{Bayesian Theory}",
	publisher     = "John Wiley \& Sons Ltd.",
	address       = "Chichester",
	series        = "Wiley series in probability and statistics",
	year          = "2000",
	url           = "https://cds.cern.ch/record/1319894",
}

@article{ciosek2019better,
	title={Better Exploration with Optimistic Actor-Critic},
	author={Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja},
	journal={arXiv preprint arXiv:1910.12807},
	year={2019}
}

@article{Dayan97,
	author = { Peter Dayan  and  Geoffrey E. Hinton },
	title = {Using Expectation-Maximization for Reinforcement Learning},
	journal = {Neural Computation},
	volume = {9},
	number = {2},
	pages = {271-278},
	year = {1997},
	doi = {10.1162/neco.1997.9.2.271},
	URL = { 
	https://doi.org/10.1162/neco.1997.9.2.271
	},
	eprint = { 
	https://doi.org/10.1162/neco.1997.9.2.271},
	abstract = { We discuss Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster, Laird, and Rubin (1977). }
}


@InProceedings{Hachiya09,
	author="Hachiya, Hirotaka
	and Peters, Jan
	and Sugiyama, Masashi",
	editor="Buntine, Wray
	and Grobelnik, Marko
	and Mladeni{\'{c}}, Dunja
	and Shawe-Taylor, John",
	title="Efficient Sample Reuse in EM-Based Policy Search",
	booktitle="Machine Learning and Knowledge Discovery in Databases",
	year="2009",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="469--484",
	abstract="Direct policy search is a promising reinforcement learning framework in particular for controlling in continuous, high-dimensional systems such as anthropomorphic robots. Policy search often requires a large number of samples for obtaining a stable policy update estimator due to its high flexibility. However, this is prohibitive when the sampling cost is expensive. In this paper, we extend an EM-based policy search method so that previously collected samples can be efficiently reused. The usefulness of the proposed method, called Reward-weighted Regression with sample Reuse (R3), is demonstrated through a robot learning experiment.",
	isbn="978-3-642-04180-8"
}

@inproceedings{Neumann11,
	author = {Neumann, Gerhard},
	title = {Variational Inference for Policy Search in Changing Situations},
	booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	series = {ICML'11},
	year = {2011},
	isbn = {978-1-4503-0619-5},
	location = {Bellevue, Washington, USA},
	pages = {817--824},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3104482.3104585},
	acmid = {3104585},
	publisher = {Omnipress},
	address = {USA},
}

@inproceedings{Peters07,
	author = {Peters, Jan and Schaal, Stefan},
	title = {Reinforcement Learning by Reward-weighted Regression for Operational Space Control},
	booktitle = {Proceedings of the 24th International Conference on Machine Learning},
	series = {ICML '07},
	year = {2007},
	isbn = {978-1-59593-793-3},
	location = {Corvalis, Oregon, USA},
	pages = {745--750},
	numpages = {6},
	url = {http://doi.acm.org/10.1145/1273496.1273590},
	doi = {10.1145/1273496.1273590},
	acmid = {1273590},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@inproceedings{Ziebart08,
	author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
	title = {Maximum Entropy Inverse Reinforcement Learning},
	booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
	series = {AAAI'08},
	year = {2008},
	isbn = {978-1-57735-368-3},
	location = {Chicago, Illinois},
	pages = {1433--1438},
	numpages = {6},
	url = {http://dl.acm.org/citation.cfm?id=1620270.1620297},
	acmid = {1620297},
	publisher = {AAAI Press},
}

@inproceedings{Ziebart10a,
	author = {Ziebart, Brian D. and Bagnell, J. Andrew and Dey, Anind K.},
	title = {Modeling Interaction via the Principle of Maximum Causal Entropy},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	series = {ICML'10},
	year = {2010},
	isbn = {978-1-60558-907-7},
	location = {Haifa, Israel},
	pages = {1255--1262},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104481},
	acmid = {3104481},
	publisher = {Omnipress},
	address = {USA},
}

@inproceedings{Fox16,
	author = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
	title = {Taming the Noise in Reinforcement Learning via Soft Updates},
	booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
	series = {UAI'16},
	year = {2016},
	isbn = {978-0-9966431-1-5},
	location = {Jersey City, New Jersey, USA},
	pages = {202--211},
	numpages = {10},
	url = {http://dl.acm.org/citation.cfm?id=3020948.3020970},
	acmid = {3020970},
	publisher = {AUAI Press},
	address = {Arlington, Virginia, United States},
} 

@inproceedings{Kingma14,
	author       = {Diederik P. Kingma and
	Max Welling},
	editor       = {Yoshua Bengio and
	Yann LeCun},
	title        = {Auto-Encoding Variational Bayes},
	booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
	Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	year         = {2014},
	url          = {http://arxiv.org/abs/1312.6114},
	timestamp    = {Thu, 04 Apr 2019 13:20:07 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{Buening23,
	title = 	 {Minimax-Bayes Reinforcement Learning},
	author =       {Buening, Thomas Kleine and Dimitrakakis, Christos and Eriksson, Hannes and Grover, Divya and Jorge, Emilio},
	booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {7511--7527},
	year = 	 {2023},
	editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
	volume = 	 {206},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {25--27 Apr},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v206/buening23a/buening23a.pdf},
	url = 	 {https://proceedings.mlr.press/v206/buening23a.html},
	abstract = 	 {While the Bayesian decision-theoretic framework offers an elegant solution to the problem of decision making under uncertainty, one question is how to appropriately select the prior distribution. One idea is to employ a worst-case prior. However, this is not as easy to specify in sequential decision making as in simple statistical estimation problems. This paper studies (sometimes approximate) minimax-Bayes solutions for various reinforcement learning problems to gain insights into the properties of the corresponding priors and policies. We find that while the worst-case prior depends on the setting, the corresponding minimax policies are more robust than those that assume a standard (i.e. uniform) prior.}
}

@inproceedings{Ziegler19,
	title={Latent Normalizing Flows for Discrete Sequences},
	author={Zachary M. Ziegler and Alexander M. Rush},
	booktitle={International Conference on Machine Learning},
	year={2019}
}

@article{Luis23,
	title={Model-Based Uncertainty in Value Functions},
	author={Carlos E. Luis and Alessandro G. Bottero and Julia Vinogradska and Felix Berkenkamp and Jan Peters},
	journal={AISTATS 2023},
	year={2023},
	volume={abs/2302.12526}
}

@article{Lai85,
	added-at = {2007-07-05T16:17:35.000+0200},
	author = {Lai, T.L. and Robbins, H.},
	biburl = {https://www.bibsonomy.org/bibtex/243d5e28aa6ae3446e548319c7f964b7f/jleny},
	description = {bandit problems},
	interhash = {c33edf59c35ee99dbaa6f1ce8835b782},
	intrahash = {43d5e28aa6ae3446e548319c7f964b7f},
	journal = {Advances in Applied Mathematics},
	keywords = {imported},
	pages = {4--22},
	timestamp = {2007-07-05T16:17:37.000+0200},
	title = {Asymptotically Efficient Adaptive Allocation Rules},
	volume = 6,
	year = 1985
}




@article{Kearns02,
	abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
	author = {Kearns, Michael and Singh, Satinder},
	da = {2002/11/01},
	date-added = {2023-05-12 17:34:23 +0100},
	date-modified = {2023-05-12 17:34:23 +0100},
	doi = {10.1023/A:1017984413808},
	id = {Kearns2002},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {2},
	pages = {209--232},
	title = {Near-Optimal Reinforcement Learning in Polynomial Time},
	ty = {JOUR},
	url = {https://doi.org/10.1023/A:1017984413808},
	volume = {49},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1017984413808}}


@inproceedings{Chi18,
	author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Is Q-Learning Provably Efficient?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf},
	volume = {31},
	year = {2018}
}

@inbook{Ciosek19,
	author = {Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja},
	title = {Better Exploration with Optimistic Actor-Critic},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Actor-critic methods, a type of model-free Reinforcement Learning, have been successfully applied to challenging tasks in continuous control, often achieving state-of-the art performance. However, wide-scale adoption of these methods in real-world domains is made difficult by their poor sample efficiency. We address this problem both theoretically and empirically. On the theoretical side, we identify two phenomena preventing efficient exploration in existing state-of-the-art algorithms such as Soft Actor Critic. First, combining a greedy actor update with a pessimistic estimate of the critic leads to the avoidance of actions that the agent does not know about, a phenomenon we call pessimistic underexploration. Second, current algorithms are directionally uninformed, sampling actions with equal probability in opposite directions from the current mean. This is wasteful, since we typically need actions taken along certain directions much more than others. To address both of these phenomena, we introduce a new algorithm, Optimistic Actor Critic, which approximates a lower and upper confidence bound on the state-action value function. This allows us to apply the principle of optimism in the face of uncertainty to perform directed exploration using the upper bound while still using the lower bound to avoid overestimation. We evaluate OAC in several challenging continuous control tasks, achieving state-of the art sample efficiency.},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	articleno = {160},
	numpages = {12}
}

@inproceedings{Durkan19,
	author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Neural Spline Flows},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/7ac71d433f282034e088473244df8c02-Paper.pdf},
	volume = {32},
	year = {2019}
}


@inproceedings{Kolter11,
	author = {Kolter, J.},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {The Fixed Points of Off-Policy TD},
	url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf},
	volume = {24},
	year = {2011}
}


@article{Hausknecht15,
	author       = {Matthew J. Hausknecht and
	Peter Stone},
	title        = {Deep Recurrent Q-Learning for Partially Observable MDPs},
	journal      = {AAAI},
	volume       = {abs/1507.06527},
	year         = {2015},
	url          = {https://aaai.org/papers/11673-deep-recurrent-q-learning-for-partially-observable-mdps/},
	eprint       = {1507.06527},
	biburl       = {https://dblp.org/rec/journals/corr/HausknechtS15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{
	schlegel2023,
	title={Investigating Action Encodings in Recurrent Neural Networks in Reinforcement Learning},
	author={Matthew Kyle Schlegel and Volodymyr Tkachuk and Adam M White and Martha White},
	journal={Transactions on Machine Learning Research},
	issn={2835-8856},
	year={2023},
	url={https://openreview.net/forum?id=K6g4MbAC1r},
	note={}
}


@InProceedings{Buening23a,
	title = 	 {Minimax-Bayes Reinforcement Learning},
	author =       {Buening, Thomas Kleine and Dimitrakakis, Christos and Eriksson, Hannes and Grover, Divya and Jorge, Emilio},
	booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {7511--7527},
	year = 	 {2023},
	editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
	volume = 	 {206},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {25--27 Apr},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v206/buening23a/buening23a.pdf},
	url = 	 {https://proceedings.mlr.press/v206/buening23a.html},
	abstract = 	 {While the Bayesian decision-theoretic framework offers an elegant solution to the problem of decision making under uncertainty, one question is how to appropriately select the prior distribution. One idea is to employ a worst-case prior. However, this is not as easy to specify in sequential decision making as in simple statistical estimation problems. This paper studies (sometimes approximate) minimax-Bayes solutions for various reinforcement learning problems to gain insights into the properties of the corresponding priors and policies. We find that while the worst-case prior depends on the setting, the corresponding minimax policies are more robust than those that assume a standard (i.e. uniform) prior.}
}


@inproceedings{Kingma16,
	author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Improved Variational Inference with Inverse Autoregressive Flow},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
	volume = {29},
	year = {2016}
}


@inproceedings{Papamakarios17,
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Masked Autoregressive Flow for Density Estimation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
	volume = {30},
	year = {2017}
}


@article{Kaelbling98,
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	title = {Planning and Acting in Partially Observable Stochastic Domains},
	year = {1998},
	issue_date = {May, 1998},
	publisher = {Elsevier Science Publishers Ltd.},
	address = {GBR},
	volume = {101},
	number = {1–2},
	issn = {0004-3702},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.},
	journal = {Artif. Intell.},
	month = {may},
	pages = {99–134},
	numpages = {36},
	keywords = {Planning, Partially observable Markov decision processes, Uncertainty}
}


@Book{Martin67,
	author = { Martin, J. J. },
	title = { Bayesian decision problems and Markov chains [by] J. J. Martin },
	publisher = { Wiley New York },
	pages = { xii, 202 p. },
	year = { 1967 },
	type = { Book },
	language = { English },
	subjects = { Markov processes.; Bayesian statistical decision theory. },
	life-dates = { 1967 -  },
	catalogue-url = { https://nla.gov.au/nla.cat-vn539998 },
}


@inproceedings{Rawlik12,
	title={On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference},
	author={Konrad Rawlik and Marc Toussaint and Sethu Vijayakumar},
	booktitle={Robotics: Science and Systems},
	year={2012}
}

@article{Rawlik10,
	author    = {Konrad Rawlik and
	Marc Toussaint and
	Sethu Vijayakumar},
	title     = {Approximate Inference and Stochastic Optimal Control},
	journal   = {CoRR},
	volume    = {abs/1009.3958},
	year      = {2010},
	url       = {http://arxiv.org/abs/1009.3958},
	archivePrefix = {arXiv},
	eprint    = {1009.3958},
	timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1009-3958},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}




@InProceedings{fujimoto2018addressing,
	title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
	author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {1587--1596},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
	url = 	 {http://proceedings.mlr.press/v80/fujimoto18a.html},
}

@article{lillicrap2015continuous,
	Author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	Date-Added = {2018-02-01 17:17:28 +0000},
	Date-Modified = {2018-02-01 17:17:28 +0000},
	Journal = {arXiv preprint arXiv:1509.02971},
	Title = {Continuous control with deep reinforcement learning},
	Year = {2015}}

@article{Chen21,
	title={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},
	author={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},
	journal={The Ninth International Conference on Learning Representations (ICLR)},
	year={2021},
	volume={abs/2101.05982}
}

@phdthesis{Ziebart10b,
	author = {Ziebart, Brian D},
	title = {Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
	url = {http://www.cs.cmu.edu/{~}bziebart/publications/thesis-bziebart.pdf},
	year = {2010}
}

@inproceedings{Toussaint09a,
	abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, re-produces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.},
	author = {Toussaint, Marc},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
	doi = {10.1145/1553374.1553508},
	file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Toussaint - Unknown - Robot Trajectory Optimization using Approximate Inference.pdf:pdf},
	isbn = {9781605585161},
	pages = {1--8},
	title = {{Robot trajectory optimization using approximate inference}},
	url = {https://homes.cs.washington.edu/{~}todorov/courses/amath579/reading/Toussaint.pdf http://portal.acm.org/citation.cfm?doid=1553374.1553508},
	year = {2009}
}

@article{Toussaint09b,
	author = {Toussaint, Marc},
	year = {2009},
	month = {01},
	pages = {},
	title = {Probabilistic inference as a model of planned behavior},
	volume = {3},
	journal = {Kunstliche Intelligenz}
}

@ARTICLE{Dempster77maximumlikelihood,
	author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
	title = {Maximum likelihood from incomplete data via the EM algorithm},
	journal = {Journal of the Royal Statistical Society, Series B},
	year = {1977},
	volume = {39},
	number = {1},
	pages = {1--38}
}

@inproceedings{neumann2011,
	title={Variational inference for policy search in changing situations},
	author={Neumann, Gerhard and others},
	booktitle={Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
	pages={817--824},
	year={2011}
}

@article{virl_review,
	abstract = {The framework of reinforcement learning or optimal control provides a mathe-matical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learn-ing and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: for-malizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy rein-forcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynam-ics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.00909v3},
	author = {Levine, Sergey},
	eprint = {arXiv:1805.00909v3},
	file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Levine - 2018 - Reinforcement Learning and Control as Probabilistic Inference Tutorial and Review.pdf:pdf},
	title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
	url = {https://arxiv.org/pdf/1805.00909.pdf},
	year = {2018}
}

@article{Wu83,
	abstract = {Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incomplete-data) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maxi-mum likelihood estimate. A list of key properties of the algorithm is included. 1. Introduction. Dempster, Laird and Rubin (1977) (henceforth abbreviated DLR) introduced the EM algorithm for computing maximum likelihood estimates from incom-plete data. The essential ideas underlying the EM algorithm have been presented in special cases by many authors; see DLR for a detailed account. Among them we mention Baum et al. (1970), Hartley and Hocking (1971), Orchard and Woodbury (1972), Sundberg (1974). The DLR paper has made three significant contributions: (i) it recognizes the expectation step (E-step) and the maximization step (M-step) in their general forms, (ii) it gives some theoretical properties of the algorithm, and (iii) it recognizes and gives a wide range of applications in statistics. However, the proof of convergence of EM sequences in DLR contains an error. The implication from (3.13) to (3.14) in their Theorem 2 fails due to an incorrect use of the triangle inequality. Additional comments on this proof are given in Section 2.2. Therefore the convergence of EM sequence as proved in their Theorems 2 and 3 is cast in doubt. Other results on the monotonicity of likelihood sequence and the convergence rate of EM sequence (Theorems 1 and 4 of DLR) remain valid. Despite its slow numerical convergence, the EM algorithm has become a very popular computational method in statistics. Contrary to the general experience in numerical optimization, the implementation of the E-step and M-step is easy for many statistical problems, thanks to the nice form of the complete-data likelihood function. Solutions of the M-step often exist in closed form. In many cases the M-step can be performed with a standard statistical package, thus saving programming time. Another reason for statisti-cians to prefer EM is that it does not require large storage space. These two features are especially attractive to those with free access to small computers. In this paper, instead of patching up the original proof of DLR, we study more broadly two convergence aspects of the EM algorithm. Our approach is to view EM as a special optimization algorithm and to utilize existing results in the optimization literature. Formally we have two sample spaces . and cN and a many-to-one mapping from . to OY. Instead of observing the "complete data" x in ?, we observe the "incomplete data" y},
	author = {Wu, C F Jeff},
	file = {::},
	journal = {Source: The Annals of Statistics The Annals of Statistics},
	number = {1},
	pages = {95--103},
	title = {{On the Convergence Properties of the EM Algorithm'}},
	volume = {11},
	year = {1983}
}

@inproceedings{Abdolmaleki18,
	title={Maximum a Posteriori Policy Optimisation},
	author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=S1ANxQW0b},
}

@article{Toussaint06b,
	abstract = {Inference in Markov Decision Processes has recently received interest as a means to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing inference technique in DBNs now becomes available for answering behavioral questionsincluding those on continuous, factorial, or hierarchical state representations. Here we present an Expectation Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continuous stochastic optimal control problems.},
	author = {Toussaint, Marc and Storkey, Amos},
	doi = {10.1145/1143844.1143963},
	isbn = {1595933832},
	journal = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
	pages = {945--952},
	title = {{Probabilistic inference for solving discrete and continuous state Markov Decision Processes}},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143963},
	year = {2006}
}

@phdthesis{Levine14,
	author = {Levine, Sergey},
	file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Levine - 2014 - MOTOR SKILL LEARNING WITH LOCAL TRAJECTORY METHODS.pdf:pdf},
	title = {Motor Skill Learning with Trajectory Methods},
	url = {https://people.eecs.berkeley.edu/{~}svlevine/papers/thesis.pdf},
	year = {2014}
}


@InProceedings{Haarnoja17,
	title = 	 {Reinforcement Learning with Deep Energy-Based Policies},
	author = 	 {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {1352--1361},
	year = 	 {2017},
	editor = 	 {Doina Precup and Yee Whye Teh},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {International Convention Centre, Sydney, Australia},
	month = 	 {06--11 Aug},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
	url = 	 {http://proceedings.mlr.press/v70/haarnoja17a.html},
	abstract = 	 {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.}
}


@article{Haarnoja18c,
	author    = {Tuomas Haarnoja and
	Aurick Zhou and
	Kristian Hartikainen and
	George Tucker and
	Sehoon Ha and
	Jie Tan and
	Vikash Kumar and
	Henry Zhu and
	Abhishek Gupta and
	Pieter Abbeel and
	Sergey Levine},
	title     = {Soft Actor-Critic Algorithms and Applications},
	journal   = {CoRR},
	volume    = {abs/1812.05905},
	year      = {2018},
	archivePrefix = {arXiv},
	eprint    = {1812.05905},
	timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Watkins92,
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	title = {Technical Note: \cal Q -Learning},
	year = {1992},
	issue_date = {May 1992},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {8},
	number = {3–4},
	issn = {0885-6125},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = { cal Q -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally
	in controlled Markovian domains. It amounts to an incremental method for dynamic programming
	which imposes limited computational demands. It works by successively improving its
	evaluations of the quality of particular actions at particular states.This paper presents
	and proves in detail a convergence theorem for cal Q -learning based on that outlined
	in Watkins (1989). We show that cal Q -learning converges to the optimum action-values
	with probability 1 so long as all actions are repeatedly sampled in all states and
	the action-values are represented discretely. We also sketch extensions to the cases
	of non-discounted, but absorbing, Markov environments, and where many cal Q values
	can be changed each iteration, rather than just one.},
	journal = {Mach. Learn.},
	month = may,
	pages = {279–292},
	numpages = {14},
	keywords = {cal Q -learning, asynchronous dynamic programming, reinforcement learning, temporal differences}
}




@article{Haarnoja2019LearningTW,
	title={Learning to Walk via Deep Reinforcement Learning},
	author={T. Haarnoja and Aurick Zhou and Sehoon Ha and J. Tan and G. Tucker and S. Levine},
	journal={ArXiv Preprint},
	year={2019},
	volume={abs/1812.11103}
}

@article{Moerland20,
	author    = {Thomas M. Moerland and
	Joost Broekens and
	Catholijn M. Jonker},
	title     = {Model-based Reinforcement Learning: {A} Survey},
	journal   = {CoRR},
	volume    = {abs/2006.16712},
	year      = {2020},
	url       = {https://arxiv.org/abs/2006.16712},
	archivePrefix = {arXiv},
	eprint    = {2006.16712},
	timestamp = {Thu, 02 Jul 2020 14:42:48 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16712.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hochbaum2007,
	title={Complexity and algorithms for nonlinear optimization problems},
	author={D. Hochbaum},
	journal={Annals of Operations Research},
	year={2007},
	volume={153},
	pages={257-296}
}

@article{Shapiro87,
	ISSN = {00029939, 10886826},
	URL = {http://www.jstor.org/stable/2046282},
	abstract = {This paper is concerned with metric projections onto a closed subset S of a finite-dimensional normed space. Necessary and in a sense sufficient conditions for directional differentiability of a metric projection at a boundary point of S are given in terms of approximating cones. It is shown that if S is defined by a number of inequality constraints and a constraint qualification holds, then the approximating cone exists.},
	author = {Alexander Shapiro},
	journal = {Proceedings of the American Mathematical Society},
	number = {1},
	pages = {123--128},
	publisher = {American Mathematical Society},
	title = {On Differentiability of Metric Projections in Rn, 1: Boundary Case},
	volume = {99},
	year = {1987}
}


@article{Shapiro88,
	title = {Directional differentiability of metric projections onto moving sets at boundary points},
	journal = {Journal of Mathematical Analysis and Applications},
	volume = {131},
	number = {2},
	pages = {392-403},
	year = {1988},
	issn = {0022-247X},
	doi = {https://doi.org/10.1016/0022-247X(88)90213-2},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X88902132},
	author = {Alexander Shapiro}
}

@incollection{ZARANTONELLO71,
	title = {Projections on Convex Sets in Hilbert Space and Spectral Theory: Part I. Projections on Convex Sets: Part II. Spectral Theory},
	editor = {Eduardo H. Zarantonello},
	booktitle = {Contributions to Nonlinear Functional Analysis},
	publisher = {Academic Press},
	pages = {237-424},
	year = {1971},
	isbn = {978-0-12-775850-3},
	doi = {https://doi.org/10.1016/B978-0-12-775850-3.50013-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780127758503500133},
	author = {Eduardo H Zarantonello},
	abstract = {Publisher Summary
	This chapter discusses projections on convex sets in Hilbert Space and Spectral Theory. It consists of two separate parts of which the second is an outgrowth of the first and, to a certain extent, its motivation. A general study of projections is presented on convex sets in Hilbert space without any particular aim in mind. Projections appear as one of the simplest instances of nonlinear mappings that can be defined in abstract terms. It was in the study of the algebra of projections that the similarities with the linear counterpart became obvious, especially after the specialization to projections on convex cones. All obstacles were removed at once and the way toward a spectral theory was opened. The notion of a spectral resolution made up of projections on convex cones makes sense; from it, a spectral measure can be built and a spectral integration theory of real valued functions can be developed. The resulting integrals are operators in Hilbert space, nonlinear in general, generalizing linear selfadjoint ones whose properties they mimic.}
}

@InProceedings{Du17, title = {Stochastic Variance Reduction Methods for Policy Evaluation}, author = {Simon S. Du and Jianshu Chen and Lihong Li and Lin Xiao and Dengyong Zhou}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1049--1058}, year = {2017}, editor = {Doina Precup and Yee Whye Teh}, volume = {70}, series = {Proceedings of Machine Learning Research}, address = {International Convention Centre, Sydney, Australia}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/du17a/du17a.pdf}, abstract = {Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy. It is a crucial step in many reinforcement-learning algorithms. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.} }

@article{mnih2015humanlevel,
	added-at = {2015-08-26T14:46:40.000+0200},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	description = {Human-level control through deep reinforcement learning - nature14236.pdf},
	interhash = {eac59980357d99db87b341b61ef6645f},
	intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
	issn = {00280836},
	journal = {Nature},
	keywords = {deep learning toread},
	month = feb,
	number = 7540,
	pages = {529--533},
	publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	timestamp = {2015-08-26T14:46:40.000+0200},
	title = {Human-level control through deep reinforcement learning},
	url = {http://dx.doi.org/10.1038/nature14236},
	volume = 518,
	year = 2015
}



@InProceedings{vanseijen15, title = {A Deeper Look at Planning as Learning from Replay}, author = {Vanseijen, Harm and Sutton, Rich}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {2314--2322}, year = {2015}, editor = {Bach, Francis and Blei, David}, volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/vanseijen15.pdf}, url = { http://proceedings.mlr.press/v37/vanseijen15.html }, abstract = {In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation.} } 


@InProceedings{schulman15trpo,
	title = 	 {Trust Region Policy Optimization},
	author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {1889--1897},
	year = 	 {2015},
	editor = 	 {Bach, Francis and Blei, David},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
	url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
	abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@MISC{Gershman12,
	author = {Samuel J. Gershman and Matthew D. Hoffman and David M. Blei},
	title = {Nonparametric Variational Inference},
	year = {}
}

@article{Birnbaum62,
	author = { Allan   Birnbaum },
	title = {On the Foundations of Statistical Inference},
	journal = {Journal of the American Statistical Association},
	volume = {57},
	number = {298},
	pages = {269-306},
	year  = {1962},
	publisher = {Taylor & Francis},
	doi = {10.1080/01621459.1962.10480660},
	
	URL = { 
	https://www.tandfonline.com/doi/abs/10.1080/01621459.1962.10480660
	
	},
	eprint = { 
	https://www.tandfonline.com/doi/pdf/10.1080/01621459.1962.10480660
	
	}
	
}

@article{Gandenberger15,
	author = {Gandenberger, Greg},
	title = {A New Proof of the Likelihood Principle},
	journal = {The British Journal for the Philosophy of Science},
	volume = {66},
	number = {3},
	pages = {475-503},
	year = {2015},
	doi = {10.1093/bjps/axt039},
	
	URL = { 
	https://doi.org/10.1093/bjps/axt039
	
	},
	eprint = { 
	https://doi.org/10.1093/bjps/axt039
	
	}
	,
	abstract = { AbstractI present a new proof of the likelihood principle that avoids two responses to a well-known proof due to Birnbaum ([1962]). I also respond to arguments that Birnbaum’s proof is fallacious, which if correct could be adapted to this new proof. On the other hand, I urge caution in interpreting proofs of the likelihood principle as arguments against the use of frequentist statistical methods. 1 Introduction2 The New Proof3 How the New Proof Addresses Proposals to Restrict Birnbaum’s Premises4 A Response to Arguments that the Proofs Are Fallacious5 Conclusion }
}

@article{schulman2017ppo,
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
	added-at = {2019-12-16T18:31:56.000+0100},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	biburl = {https://www.bibsonomy.org/bibtex/24bbcce6aa1c42ae7f61ef8cf5475aa85/lanteunis},
	ee = {http://arxiv.org/abs/1707.06347},
	interhash = {f57ff463a90dbafb77d55a25aea8355c},
	intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
	journal = {CoRR},
	keywords = {DRLAlgoComparison ppo reinforcement_learning},
	timestamp = {2019-12-18T21:15:59.000+0100},
	title = {Proximal Policy Optimization Algorithms.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17},
	volume = {abs/1707.06347},
	year = 2017
}


@article{Lin92,
	author    = {Long Ji Lin},
	title     = {Self-Improving Reactive Agents Based On Reinforcement Learning, Planning
	and Teaching},
	journal   = {Mach. Learn.},
	volume    = {8},
	pages     = {293--321},
	year      = {1992},
	url       = {https://doi.org/10.1007/BF00992699},
	doi       = {10.1007/BF00992699},
	timestamp = {Mon, 02 Mar 2020 16:30:02 +0100},
	biburl    = {https://dblp.org/rec/journals/ml/Lin92.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Song13,
	author={L. {Song} and K. {Fukumizu} and A. {Gretton}},
	journal={IEEE Signal Processing Magazine}, 
	title={Kernel Embeddings of Conditional Distributions: A Unified Kernel Framework for Nonparametric Inference in Graphical Models}, 
	year={2013},
	volume={30},
	number={4},
	pages={98-111},
	doi={10.1109/MSP.2013.2252713}}

@article{Swiechowski21,
	author = {Świechowski, Maciej and Godlewski, Konrad and Sawicki, Bartosz and Mańdziuk, Jacek},
	year = {2021},
	month = {03},
	pages = {},
	title = {Monte Carlo Tree Search: A Review on Recent Modifications and Applications},
	journal = {arXiv Preprint}
}

@inproceedings{Nota20,
	author = {Nota, Chris and Thomas, Philip S.},
	title = {Is the Policy Gradient a Gradient?},
	year = {2020},
	isbn = {9781450375184},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	address = {Richland, SC},
	abstract = {The policy gradient theorem describes the gradient of the expected discounted return
	with respect to an agent's policy parameters. However, most policy gradient methods
	drop the discount factor from the state distribution and therefore do not optimize
	the discounted objective. What do they optimize instead? This has been an open question
	for several years, and this lack of theoretical clarity has lead to an abundance of
	misstatements in the literature. We answer this question by proving that the update
	direction approximated by most methods is not the gradient of any function. Further,
	we argue that algorithms that follow this direction are not guaranteed to converge
	to a "reasonable'' fixed point by constructing a counterexample wherein the fixed
	point is globally pessimal with respect to both the discounted and undiscounted objectives.
	We motivate this work by surveying the literature and showing that there remains a
	widespread misunderstanding regarding discounted policy gradient methods, with errors
	present even in highly-cited papers published at top conferences.},
	booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
	pages = {939–947},
	numpages = {9},
	keywords = {policy gradients, reinforcement learning},
	location = {Auckland, New Zealand},
	series = {AAMAS '20}
}

@InProceedings{mnih16,
	title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
	author = 	 {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {1928--1937},
	year = 	 {2016},
	editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
	url = 	 {https://proceedings.mlr.press/v48/mniha16.html},
	abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}


@ARTICLE{Browne12,
	author = {Cameron Browne and Edward Powley and Daniel Whitehouse and Simon Lucas and Peter I. Cowling and Stephen Tavener and Diego Perez and Spyridon Samothrakis and Simon Colton and et al.},
	title = {A survey of Monte Carlo tree search methods},
	journal = {IEEE TRANSACTIONS ON COMPUTATIONAL INTELLIGENCE AND AI},
	year = {2012}
}

@inproceedings{
	abdolmaleki2018maximum,
	title={Maximum a Posteriori Policy Optimisation},
	author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=S1ANxQW0b},
}


@book{Sutton18,
	added-at = {2019-07-13T10:11:53.000+0200},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
	edition = {Second},
	publisher = {The MIT Press},
	timestamp = {2019-07-13T10:11:53.000+0200},
	title = {Reinforcement Learning: An Introduction},
	url = {http://incompleteideas.net/book/the-book-2nd.html},
	year = {2018 }
}

@book{puterman2014,
	added-at = {2017-04-07T12:13:11.000+0200},
	author = {Puterman, Martin L},
	biburl = {https://www.bibsonomy.org/bibtex/22e7ac99cd30c4892171e5a7cef1bc7a7/becker},
	interhash = {6cec8f775a265d8741171d17e4a4e7d0},
	intrahash = {2e7ac99cd30c4892171e5a7cef1bc7a7},
	keywords = {inthesis diss markov chain decision process citedby:scholar:count:9594 citedby:scholar:timestamp:2017-4-7},
	publisher = {John Wiley \& Sons},
	timestamp = {2017-04-07T12:13:11.000+0200},
	title = {Markov decision processes: discrete stochastic dynamic programming},
	year = 2014
}

@inproceedings{Gal2017ConcreteB,
	author = {Yarin Gal and Jiri Hron and Alex Kendall},
	title = "{Concrete Dropout}",
	booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
	year = 2017,
}

@inbook{vaart98, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={Bayes Procedures}, DOI={10.1017/CBO9780511802256.011}, booktitle={Asymptotic Statistics}, publisher={Cambridge University Press}, author={Vaart, A. W. van der}, year={1998}, pages={138–152}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@inproceedings{WangB19,
	author    = {Yixin Wang and
	David M. Blei},
	editor    = {Hanna M. Wallach and
	Hugo Larochelle and
	Alina Beygelzimer and
	Florence d'Alch{\'{e}}{-}Buc and
	Emily B. Fox and
	Roman Garnett},
	title     = {Variational Bayes under Model Misspecification},
	booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
	on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
	December 2019, Vancouver, BC, Canada},
	pages     = {13357--13367},
	year      = {2019},
	url       = {http://papers.nips.cc/paper/9492-variational-bayes-under-model-misspecification},
	timestamp = {Fri, 06 Mar 2020 16:59:09 +0100},
	biburl    = {https://dblp.org/rec/conf/nips/WangB19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Osband16DQN,
	title = {Deep Exploration via Bootstrapped DQN},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {4026--4034},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf}
}


@inproceedings{Chung19,
	author    = {Wesley Chung and
	Somjit Nath and
	Ajin Joseph and
	Martha White},
	title     = {Two-Timescale Networks for Nonlinear Value Function Approximation},
	booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
	New Orleans, LA, USA, May 6-9, 2019},
	publisher = {OpenReview.net},
	year      = {2019},
	url       = {https://openreview.net/forum?id=rJleN20qK7},
	timestamp = {Thu, 25 Jul 2019 14:25:47 +0200},
	biburl    = {https://dblp.org/rec/conf/iclr/ChungNJW19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang17,
	author = {Wang, Yixin and Blei, David},
	year = {2017},
	month = {05},
	pages = {},
	title = {Frequentist Consistency of Variational Bayes},
	journal = {Journal of the American Statistical Association},
	doi = {10.1080/01621459.2018.1473776}
}

@book{Savage1954,
	year = {1954},
	publisher = {Wiley Publications in Statistics},
	title = {The Foundations of Statistics},
	author = {Leonard J. Savage}
}


@inproceedings{Hasselt16,
	author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
	title = {Deep Reinforcement Learning with Double Q-Learning},
	year = {2016},
	publisher = {AAAI Press},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain
	conditions. It was not previously known whether, in practice, such overestimations
	are common, whether they harm performance, and whether they can generally be prevented.
	In this paper, we answer all these questions affirmatively. In particular, we first
	show that the recent DQN algorithm, which combines Q-learning with a deep neural network,
	suffers from substantial overestimations in some games in the Atari 2600 domain. We
	then show that the idea behind the Double Q-learning algorithm, which was introduced
	in a tabular setting, can be generalized to work with large-scale function approximation.
	We propose a specific adaptation to the DQN algorithm and show that the resulting
	algorithm not only reduces the observed overestimations, as hypothesized, but that
	this also leads to much better performance on several games.},
	booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
	pages = {2094–2100},
	numpages = {7},
	location = {Phoenix, Arizona},
	series = {AAAI'16}
}





@article{kleijn2012,
	author = "Kleijn, B.J.K. and van der Vaart, A.W.",
	doi = "10.1214/12-EJS675",
	fjournal = "Electronic Journal of Statistics",
	journal = "Electron. J. Statist.",
	pages = "354--381",
	publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
	title = "The Bernstein-Von-Mises theorem under misspecification",
	url = "https://doi.org/10.1214/12-EJS675",
	volume = "6",
	year = "2012"
}


@book{papoulis02,
	added-at = {2015-10-23T15:50:51.000+0200},
	address = {Boston},
	author = {Papoulis, Athanasios and Pillai, S. Unnikrishna},
	biburl = {https://www.bibsonomy.org/bibtex/22a938c6a45a64bd0f43a8ebbf1f3e368/ytyoun},
	edition = {Fourth},
	interhash = {da558c64a1da0ca6035c9f1766b0c15b},
	intrahash = {2a938c6a45a64bd0f43a8ebbf1f3e368},
	isbn = {0071122567 9780071122566 0073660116 9780073660110 0071226613 9780071226615},
	keywords = {probability textbook},
	publisher = {McGraw Hill},
	refid = {611571290},
	timestamp = {2015-10-23T15:50:51.000+0200},
	title = {Probability, Random Variables, and Stochastic Processes},
	url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=0071226613},
	year = 2002
}



@inproceedings{Spooner18,
	author = {Spooner, Thomas and Savani, Rahul and Fearnley, John and Koukorinis, Andreas},
	year = {2018},
	booktitle = {17th International Conference on Autonomous Agents and Multiagent Systems},
	month = {07},
	pages = {},
	title = {Market Making via Reinforcement Learning}
}

@article{Mehrabi19,
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	year = {2019},
	month = {08},
	pages = {},
	title = {A Survey on Bias and Fairness in Machine Learning}
}

}

@Book{Ross97,
	Title                    = {Introduction to Probability Models},
	Author                   = {Sheldon M. Ross},
	Publisher                = {Academic Press},
	Year                     = {1997},
	
	Address                  = {San Diego, CA, USA},
	Edition                  = {Sixth}
}

@InProceedings{odonoghue18a,
	title = 	 {The Uncertainty {B}ellman Equation and Exploration},
	author = 	 {O'Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Vlad},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {3839--3848},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/odonoghue18a/odonoghue18a.pdf},
	url = 	 {http://proceedings.mlr.press/v80/odonoghue18a.html},
	abstract = 	 {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for $\epsilon$-greedy improves DQN performance on 51 out of 57 games in the Atari suite.}
}

@article{Mokkadem06,
	author = {Mokkadem, Abdelkader and Pelletier, Mariane},
	year = {2006},
	month = {11},
	pages = {},
	title = {Convergence rate and averaging of nonlinear two-time-scale stochastic approximation algorithms},
	volume = {16},
	journal = {Ann. Appl. Probab.},
	doi = {10.1214/105051606000000448}
}

@inproceedings{Gal16a, author = {Gal, Yarin and Ghahramani, Zoubin}, title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}, year = {2016}, publisher = {JMLR.org}, booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48}, pages = {1050–1059}, numpages = {10}, location = {New York, NY, USA}, series = {ICML’16} }

@inproceedings{Touati19,
	author = {Touati, Ahmed and Satija, Harsh and Romoff, Joshua and Pineau, Joelle and Vincent, Pascal},
	biburl = {https://www.bibsonomy.org/bibtex/2ba69b1ef8d28daf9d193d5ef1abe5f64/dblp},
	booktitle = {UAI},
	editor = {Globerson, Amir and Silva, Ricardo},
	ee = {http://auai.org/uai2019/proceedings/papers/156.pdf},
	interhash = {e8d41554882d285df7afcb23f3dc9858},
	intrahash = {ba69b1ef8d28daf9d193d5ef1abe5f64},
	keywords = {dblp},
	pages = 156,
	publisher = {AUAI Press},
	timestamp = {2020-03-13T12:41:16.000+0100},
	title = {Randomized Value Functions via Multiplicative Normalizing Flows.},
	year = 2019
}

@article{Lipton18,
	author = {Lipton, Zachary and Li, Xiujun and Gao, Jianfeng and Li, Lihong and Ahmed, Faisal and Deng, li},
	year = {2018},
	month = {11},
	pages = {},
	booktitle = {AAAI},
	journal = {Association for the Advancement of Artificial Intelligence},
	title = {BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems}
}

@inproceedings{Fortunato18,
	title	= {Noisy Networks for Exploration},
	author	= {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alexander Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
	year	= {2018},
	booktitle	= {Proceedings of the International Conference on Representation Learning (ICLR 2018)},
	address	= {Vancouver (Canada)}
}




@article{Engel16, author = {Ghavamzadeh, Mohammad and Engel, Yaakov and Valko, Michal}, title = {Bayesian Policy Gradient and Actor-Critic Algorithms}, year = {2016}, issue_date = {January 2016}, publisher = {JMLR.org}, volume = {17}, number = {1}, issn = {1532-4435}, journal = {J. Mach. Learn. Res.}, month = jan, pages = {2319–2371}, numpages = {53}, keywords = {reinforcement learning, Gaussian processes, policy gradient methods, Bayesian inference, actor-critic algorithms} }


@book{Jordan99,
	editor = {Jordan, Michael I.},
	title = {Learning in Graphical Models},
	year = {1999},
	isbn = {0-262-60032-3},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA}
} 

@phdthesis{Beal03,
	author = {Beal, Matthew James},
	year = {2003},
	title = {Variational algorithms for approximate Bayesian inference},
	booktitle = {PhD thesis}
}

@book{Chung01,
	title={A Course in Probability Theory},
	author={Chung, K.L. and Zhong, K.},
	isbn={9780121741518},
	lccn={00106712},
	url={https://books.google.co.uk/books?id=zu80w7wd40UC},
	year={2001},
	publisher={Elsevier Science}
}

@book{Rachev91,
	title={Probability Metrics and the Stability of Stochastic Models},
	author={Rachev, S.T.},
	isbn={9780471928775},
	year={1991}},
publisher={Wiley},
}

@book{Meyn09, place={Cambridge}, edition={2}, series={Cambridge Mathematical Library}, title={Markov Chains and Stochastic Stability}, DOI={10.1017/CBO9780511626630}, publisher={Cambridge University Press}, author={Meyn, Sean and Tweedie, Richard L. and Glynn, Peter W.}, year={2009}, collection={Cambridge Mathematical Library}}

@inproceedings{Fellows23,
	author = {Fellows, Mattie and Smith, Matthew and Whiteson, Shimon},
	booktitle = {ICML},
	title = {Why Target Networks Stabilise Temporal Difference Methods},
	year = {2023}
}

@book{Billing09,
	added-at = {2009-04-24T23:33:01.000+0200},
	address = {New York},
	author = {Billingsley, Patrick},
	biburl = {https://www.bibsonomy.org/bibtex/2657f92e619abe605188197d74b27f572/peter.ralph},
	description = {q-paper},
	edition = {Second},
	interhash = {555ad867bdd2f4e0824bffe13fa1b9f9},
	intrahash = {657f92e619abe605188197d74b27f572},
	isbn = {0-471-19745-9},
	keywords = {probability_theory reference},
	mrclass = {60B10 (28A33 60F17)},
	mrnumber = {MR1700749 (2000e:60008)},
	note = {A Wiley-Interscience Publication},
	pages = {x+277},
	publisher = {John Wiley \& Sons Inc.},
	series = {Wiley Series in Probability and Statistics: Probability and
	Statistics},
	timestamp = {2009-04-24T23:44:01.000+0200},
	title = {Convergence of probability measures},
	year = 1999
}

@inproceedings{Asmuth11,
	author = {Asmuth, John and Littman, Michael},
	title = {Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search},
	year = {2011},
	isbn = {9780974903972},
	publisher = {AUAI Press},
	address = {Arlington, Virginia, USA},
	abstract = {Bayes-optimal behavior, while well-defined, is often difficult to achieve. Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it is possible to act near-optimally in Markov Decision Processes (MDPs) with very large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is equivalent to optimal behavior in the known belief-space MDP, although the size of this belief-space MDP grows exponentially with the amount of history retained, and is potentially infinite. We show how an agent can use one particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an efficient way to act nearly Bayes-optimally for all but a polynomial number of steps, assuming that FSSS can be used to act efficiently in any possible underlying MDP.},
	booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
	pages = {19–26},
	numpages = {8},
	location = {Barcelona, Spain},
	series = {UAI'11}
}


@incollection{Feng19,
	title = {A Kernel Loss for Solving the Bellman Equation},
	author = {Feng, Yihao and Li, Lihong and Liu, Qiang},
	booktitle = {Advances in Neural Information Processing Systems 32},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {15456--15467},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/9679-a-kernel-loss-for-solving-the-bellman-equation.pdf}
}



@article{Antos08,
	Abstract = {In this paper we consider the problem of finding a near-optimal policy in a continuous space, discounted Markovian Decision Problem (MDP) by employing value-function-based methods when only a single trajectory of a fixed policy is available as the input. We study a policy-iteration algorithm where the iterates are obtained via empirical risk minimization with a risk function that penalizes high magnitudes of the Bellman-residual. Our main result is a finite-sample, high-probability bound on the performance of the computed policy that depends on the mixing rate of the trajectory, the capacity of the function set as measured by a novel capacity concept (the VC-crossing dimension), the approximation power of the function set and the controllability properties of the MDP. Moreover, we prove that when a linear parameterization is used the new algorithm is equivalent to Least-Squares Policy Iteration. To the best of our knowledge this is the first theoretical result for off-policy control learning over continuous state-spaces using a single trajectory.},
	Author = {Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
	Da = {2008/04/01},
	Date-Added = {2020-04-15 11:57:22 +0100},
	Date-Modified = {2020-04-15 11:57:22 +0100},
	Doi = {10.1007/s10994-007-5038-2},
	Id = {Antos2008},
	Isbn = {1573-0565},
	Journal = {Machine Learning},
	Number = {1},
	Pages = {89--129},
	Title = {Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/s10994-007-5038-2},
	Volume = {71},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10994-007-5038-2}}


@inproceedings{Brandfonbrener2019,
	title={Geometric Insights into the Convergence of Nonlinear TD Learning},
	author={David Brandfonbrener and Joan Bruna},
	booktitle={ICLR 2020},
	year={2019}
}

@INPROCEEDINGS{Brunskill12,
	author = {Emma Brunskill},
	title = {Bayes-optimal reinforcement learning for discrete uncertainty domains},
	booktitle = {In AAMAS ’12},
	year = {2012}
}

@inproceedings{Zintgraf19,
	title={VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning},
	author={Luisa Zintgraf and Kyriacos Shiarlis and Maximilian Igl and Sebastian Schulze and Yarin Gal and Katja Hofmann and Shimon Whiteson},
	year={2020},
	primaryClass={cs.LG},
	booktitle = {International Conference on Learning Representations},
	location = {Addis Ababa, Ethiopia },
	url ={https://openreview.net/pdf?id=Hkl9JlBYvr}
}

@inproceedings{Dearden99,
	author = {Dearden, Richard and Friedman, Nir and Andre, David},
	title = {Model Based Bayesian Exploration},
	year = {1999},
	isbn = {1558606149},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	abstract = {Reinforcement learning systems are often concerned with balancing exploration of untested
	actions against exploitation of actions that are known to be good. The benefit of
	exploration can be estimated using the classical notion of Value of Information -
	the expected improvement in future decision quality arising from the information acquired
	by exploration. Estimating this quantity requires an assessment of the agent's uncertainty
	about its current value estimates for states.In this paper we investigate ways to
	represent and reason about this uncertainty in algorithms where the system attempts
	to learn a model of its environment. We explicitly represent uncertainty about the
	parameters of the model and build probability distributions over Q-values based on
	these. These distributions are used to compute a myopic approximation to the value
	of information for each action and hence to select the action that best balances exploration
	and exploitation},
	booktitle = {Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence},
	pages = {150–159},
	numpages = {10},
	location = {Stockholm, Sweden},
	series = {UAI'99}
}

@inproceedings{Deisenroth11,
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},
	year = {2011},
	isbn = {9781450306195},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy
	search method. PILCO reduces model bias, one of the key problems of model-based reinforcement
	learning, in a principled way. By learning a probabilistic dynamics model and explicitly
	incorporating model uncertainty into long-term planning, PILCO can cope with very
	little data and facilitates learning from scratch in only a few trials. Policy evaluation
	is performed in closed form using state-of-the-art approximate inference. Furthermore,
	policy gradients are computed analytically for policy improvement. We report unprecedented
	learning efficiency on challenging and high-dimensional control tasks.},
	booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	pages = {465–472},
	numpages = {8},
	location = {Bellevue, Washington, USA},
	series = {ICML'11}
}



@incollection{Sutton00,
	title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
	author = {Sutton, Richard S and David A. McAllester and Satinder P. Singh and Mansour, Yishay},
	booktitle = {Advances in Neural Information Processing Systems 12},
	editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
	pages = {1057--1063},
	year = {2000},
	publisher = {MIT Press},
	url = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}
}

@article{Karmakar18, author = {Karmakar, Prasenjit and Bhatnagar, Shalabh}, title = {Two Time-Scale Stochastic Approximation with Controlled Markov Noise and Off-Policy Temporal-Difference Learning}, year = {2018}, issue_date = {February 2018}, publisher = {INFORMS}, address = {Linthicum, MD, USA}, volume = {43}, number = {1}, issn = {0364-765X}, url = {https://doi.org/10.1287/moor.2017.0855}, doi = {10.1287/moor.2017.0855}, journal = {Math. Oper. Res.}, month = feb, pages = {130–151}, numpages = {22}, keywords = {temporal-difference learning, asymptotic convergence, Markov noise, two time-scale stochastic approximation} }

@article{Vieillard2020,
	title={Leverage the Average: an Analysis of Regularization in RL},
	author={Nino Vieillard and Tadashi Kozuno and B. Scherrer and O. Pietquin and R{\'e}mi Munos and M. Geist},
	journal = {Advances in Neural Information Processing Systems},
	year={2020},
	volume={33}
}


@ARTICLE{Andrews92,
	title = {Generic Uniform Convergence},
	author = {Andrews, Donald},
	year = {1992},
	journal = {Econometric Theory},
	volume = {8},
	number = {2},
	pages = {241-257},
	abstract = {This paper presents several generic uniform convergence results that include generic uniform laws of large numbers. These results provide conditions under which pointwise convergence almost surely or in probability can be strengthened to uniform convergence. The results are useful for establishing asymptotic properties of estimators and test statistics.The results given here have the following attributes, (1) they extendresults of Newey to cover convergence almost surely as well as convergence in probability, (2) they apply to totally bounded parameter spaces (rather than just to compact parameter spaces), (3) they introduce a set of conditions for a generic uniform law of large numbers that has the attribute of giving the weakest conditions available for i.i.d. contexts, but which apply in some dependent nonidentically distributed contexts as well, and (4) they incorporate and extend themain results in the literature in a parsimonious fashion.},
}

@article{tassa2018deepmind,
	title={Deepmind control suite},
	author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
	journal={arXiv preprint arXiv:1801.00690},
	year={2018}
}

@article{brockman2016openai,
	title={Openai gym},
	author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	journal={arXiv preprint arXiv:1606.01540},
	year={2016}
}

@incollection{Konda00,
	title = {Actor-Critic Algorithms},
	author = {Vijay R. Konda and John N. Tsitsiklis},
	booktitle = {Advances in Neural Information Processing Systems 12},
	editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
	pages = {1008--1014},
	year = {2000},
	publisher = {MIT Press},
	url = {http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf}
}

xport Citations

@article{Dayan92,
	author = {Dayan, Peter},
	title = {The Convergence of TD($\lambda$) for General $\lambda$},
	year = {1992},
	issue_date = {May 1992},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {8},
	number = {3–4},
	issn = {0885-6125},
	url = {https://doi.org/10.1007/BF00992701},
	doi = {10.1007/BF00992701},
	abstract = {The method of temporal differences (TD) is one way of making consistent predictions
	about the future. This paper uses some analysis of Watkins (1989) to extend a convergence
	theorem due to Sutton (1988) from the case which only uses information from adjacent
	time steps to that involving information from arbitrary ones.It also considers how
	this version of TD behaves in the face of linearly dependent representations for statesdemonstrating
	that it still converges, but to a different answer from the least mean squares algorithm.
	Finally it adapts Watkins' theorem that cal Q -learning, his closely related prediction
	and action learning method, converges with probability one, to demonstrate this strong
	form of convergence for a slightly modified version of TD.},
	journal = {Mach. Learn.},
	month = may,
	pages = {341–362},
	numpages = {22},
	keywords = {asynchronous dynamic programming, Reinforcement learning, temporal differences}
}




@inproceedings{Boyan95,
	author = {Boyan, Justin and Moore, Andrew},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {G. Tesauro and D. Touretzky and T. Leen},
	pages = {},
	publisher = {MIT Press},
	title = {Generalization in Reinforcement Learning: Safely Approximating the Value Function},
	url = {https://proceedings.neurips.cc/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf},
	volume = {7},
	year = {1995}
}


@article{Shalizi09,
	author = {Shalizi, Cosma},
	year = {2009},
	month = {01},
	pages = {},
	title = {Dynamics of Bayesian Updating with Dependent Data and Misspecified Models},
	volume = {3},
	journal = {Electron. J. Statist.},
	doi = {10.1214/09-EJS485}
}

@article{Borkar00, author = {Borkar, V. S. and Meyn, S. P.}, title = {The O.D. E. Method for Convergence of Stochastic Approximation and Reinforcement Learning}, year = {2000}, issue_date = {January 2000}, publisher = {Society for Industrial and Applied Mathematics}, address = {USA}, volume = {38}, number = {2}, issn = {0363-0129}, url = {https://doi.org/10.1137/S0363012997331639}, doi = {10.1137/S0363012997331639}, journal = {SIAM J. Control Optim.}, month = jan, pages = {447–469}, numpages = {23}, keywords = {asynchronous algorithms, ODE method, reinforcement learning, stability, stochastic approximation} }


@book{Williams91,
	added-at = {2011-04-08T00:00:00.000+0200},
	author = {Williams, David},
	biburl = {https://www.bibsonomy.org/bibtex/21a8684715189d2ed5eaf74c929ca2aec/dblp},
	interhash = {dcea750d06f38bfcdc6dded321f8727a},
	intrahash = {1a8684715189d2ed5eaf74c929ca2aec},
	isbn = {978-0-521-40605-5},
	keywords = {dblp},
	pages = {I-XV, 1-251},
	publisher = {Cambridge University Press},
	series = {Cambridge mathematical textbooks},
	timestamp = {2011-04-29T15:26:35.000+0200},
	title = {Probability with Martingales.},
	year = 1991
}


@article{Prasenjit, author = {Karmakar, Prasenjit and Bhatnagar, Shalabh}, title = {Two Time-Scale Stochastic Approximation with Controlled Markov Noise and Off-Policy Temporal-Difference Learning}, year = {2018}, issue_date = {February 2018}, publisher = {INFORMS}, address = {Linthicum, MD, USA}, volume = {43}, number = {1}, issn = {0364-765X}, url = {https://doi.org/10.1287/moor.2017.0855}, doi = {10.1287/moor.2017.0855}, journal = {Math. Oper. Res.}, month = feb, pages = {130–151}, numpages = {22}, keywords = {temporal-difference learning, asymptotic convergence, Markov noise, two time-scale stochastic approximation} }

@inproceedings{Yang2018ConvergentRL,
	title={Convergent Reinforcement Learning with Function Approximation: A Bilevel Optimization Perspective},
	author={Zhuoran Yang and Zuyue Fu and Kaiqing Zhang and Zhaoran Wang},
	year={2018},
	journal = {Manuscript},
}

@incollection{Heusel17,
	title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {6626--6637},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf}
}


@inproceedings{kingma14,
	added-at = {2019-12-16T11:21:55.000+0100},
	author = {Kingma, Diederik P. and Welling, Max},
	biburl = {https://www.bibsonomy.org/bibtex/27762d5bd68aba369026cb397aa011bcf/annakrause},
	booktitle = {ICLR},
	editor = {Bengio, Yoshua and LeCun, Yann},
	ee = {http://arxiv.org/abs/1312.6114},
	interhash = {a626a9d77a123c52405a08da983203cb},
	intrahash = {7762d5bd68aba369026cb397aa011bcf},
	keywords = {autoencoder bayesianlearning machinelearning},
	timestamp = {2019-12-16T11:22:29.000+0100},
	title = {Auto-Encoding Variational Bayes.},
	url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2014.html\#KingmaW13},
	year = 2014
}


@article{Borkar97,
	author = {Borkar, Vivek S.},
	title = {Stochastic Approximation with Two Time Scales},
	year = {1997},
	issue_date = {February 1997},
	publisher = {Elsevier Science Publishers B. V.},
	address = {NLD},
	volume = {29},
	number = {5},
	issn = {0167-6911},
	url = {https://doi.org/10.1016/S0167-6911(97)90015-3},
	doi = {10.1016/S0167-6911(97)90015-3},
	journal = {Syst. Control Lett.},
	month = feb,
	pages = {291–294},
	numpages = {4},
	keywords = {stochastic approximation, two time scales, o.d.e. limit, singular differential equations}
}





@book{Borkar08,
	author = {Borkar, Vivek},
	year = {2008},
	month = {01},
	pages = {},
	title = {Stochastic Approximation: A Dynamical Systems Viewpoint},
	isbn = {978-81-85931-85-2},
	doi = {10.1007/978-93-86279-38-5},
	publisher = {Hindustan Book Agency Gurgaon}
}

@article{Bard91,
	author = {Bard, J. F.},
	title = {Some Properties of the Bilevel Programming Problem},
	year = {1991},
	issue_date = {February 1991},
	publisher = {Plenum Press},
	address = {USA},
	volume = {68},
	number = {2},
	issn = {0022-3239},
	journal = {J. Optim. Theory Appl.},
	month = feb,
	pages = {371–378},
	numpages = {8},
	keywords = {computational complexity, Bilevel programming, Stackelberg games}
}

@inproceedings{Williams2014,
	title={Multiplying matrices in O(n 2.373 ) time},
	author={Virginia Vassilevska Williams},
	year={2014}
}

@article{Bengio00,
	author = {Bengio, Yoshua},
	title = {Gradient-Based Optimization of Hyperparameters},
	year = {2000},
	issue_date = {August 2000},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	volume = {12},
	number = {8},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976600300015187},
	doi = {10.1162/089976600300015187},
	journal = {Neural Comput.},
	month = aug,
	pages = {1889–1900},
	numpages = {12}
}





@inproceedings{Larsen98,
	author = {Larsen, Jan and Svarer, Claus and Andersen, Lars Nonboe and Hansen, Lars Kai},
	title = {Adaptive Regularization in Neural Network Modeling},
	year = {1998},
	isbn = {3540653112},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	booktitle = {Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop},
	pages = {113–132},
	numpages = {20}
}

@InProceedings{Pedregosa16,
	title = 	 {Hyperparameter optimization with approximate gradient},
	author = 	 {Fabian Pedregosa},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {737--746},
	year = 	 {2016},
	editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v48/pedregosa16.pdf},
	url = 	 {http://proceedings.mlr.press/v48/pedregosa16.html},
	abstract = 	 {Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.}
}
@book{degroot04,
	author = {DeGroot, Morris H.},
	publisher = {John Wiley and Sons, Ltd},
	isbn = {9780471729006},
	title = {Conjugate Prior Distributions},
	booktitle = {Optimal Statistical Decisions},	
	year = {2004}
}


@misc{kiran20,
	title={Deep Reinforcement Learning for Autonomous Driving: A Survey},
	author={B Ravi Kiran and Ibrahim Sobh and Victor Talpaert and Patrick Mannion and Ahmad A. Al Sallab and Senthil Yogamani and Patrick Pérez},
	year={2020},
	eprint={2002.00444},
	booktitle={arXiv preprint},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@article{Acemoglu20,
	Author = {Acemoglu, Daron and Restrepo, Pascual},
	Title = {Unpacking Skill Bias: Automation and New Tasks},
	Journal = {AEA Papers and Proceedings},
	Volume = {110},
	Year = {2020},
	Month = {May},
	Pages = {356-61},
	DOI = {10.1257/pandp.20201063},
	URL = {https://www.aeaweb.org/articles?id=10.1257/pandp.20201063}}

@INCOLLECTION{Acemoglu18,
	title = {Artificial Intelligence, Automation, and Work},
	author = {Acemoglu, Daron and Restrepo, Pascual},
	year = {2018},
	pages = {197-236},
	booktitle = {The Economics of Artificial Intelligence: An Agenda},
	publisher = {National Bureau of Economic Research, Inc},
	url = {https://EconPapers.repec.org/RePEc:nbr:nberch:14027}
}

@book{Srnicek15, author = {Srnicek, Nick and Williams, Alex}, title = {Inventing the future: postcapitalism and a world without work}, year = {2015}, isbn = {9781784780968}, publisher = {Verso} }



@book{Greenfield18, author = {Greenfield, Adam}, title = {Radical Technologies: The Design of Everyday Life}, year = {2018}, isbn = {1784780456}, publisher = {Verso} }


@article{Smith17,
	author = {Smith, Adam and Anderson, Janna},
	year = {2017},
	booktitle = {Pew Research Center},
	pages = {},
	title = {AI, Robotics, and the Future of Jobs}
}



@article{yu2019reinforcement,
	title={Reinforcement learning in healthcare: a survey},
	author={Yu, Chao and Liu, Jiming and Nemati, Shamim},
	journal={arXiv preprint arXiv:1908.08796},
	year={2019}
}

@article{Jiang17,
	author = {Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
	year = {2017},
	month = {06},
	pages = {},
	title = {A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem}
}

@ARTICLE{Deng17, 
	author={Y. {Deng} and F. {Bao} and Y. {Kong} and Z. {Ren} and Q. {Dai}},  journal={IEEE Transactions on Neural Networks and Learning Systems},   title={Deep Direct Reinforcement Learning for Financial Signal Representation and Trading},   year={2017},  volume={28},  number={3},  pages={653-664},}


@article{Edwards63,
	author="Edwards, Ward
	and Lindman, Harold
	and Savage, Leonard J.",
	editor="Kotz, Samuel
	and Johnson, Norman L.",
	title="Bayesian Statistical Inference for Psychological Research",
	year="1963",
	journal =" Psychological Review",
	volume = "70",
	issue = "3",
	pages="193–-242",
	abstract="Bayesian statistics, a currently controversial viewpoint concerning statistical inference, is based on a definition of probability as a particular measure of the opinions of ideally consistent people. Statistical inference is modification of these opinions in the light of evidence, and Bayes' theorem specifies how such modifications should be made. The tools of Bayesian statistics include the theory of specific distributions and the principle of stable estimation, which specifies when actual prior opinions may be satisfactorily approximated by a uniform distribution. A common feature of many classical significance tests is that a sharp null hypothesis is compared with a diffuse alternative hypothesis. Often evidence which, for a Bayesian statistician, strikingly supports the null hypothesis leads to rejection of that hypothesis by standard classical procedures. The likelihood principle emphasized in Bayesian statistics implies, among other things, that the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience.",
	
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@Book{Rockafellar98,
	Title                    = {Variational Analysis},
	Author                   = {{R. Tyrrell} Rockafellar and Roger J.-B. Wets},
	Publisher                = {Springer Verlag},
	Year                     = {1998},
	
	Address                  = {Heidelberg, Berlin, New York}
}

@article{Lindley76,
	ISSN = {00031305},
	URL = {http://www.jstor.org/stable/2683855},
	author = {D. V. Lindley and L. D. Phillips},
	journal = {The American Statistician},
	number = {3},
	pages = {112--119},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Inference for a Bernoulli Process (A Bayesian View)},
	volume = {30},
	year = {1976}
}



@article{Tang17,
	author = {Tang, Yunhao and Kucukelbir, Alp},
	year = {2017},
	month = {11},
	pages = {},
	journal   = {ArXiv Preprint},
	title = {Variational Deep Q Network}
}

@article{Gould16,
	author    = {Stephen Gould and
	Basura Fernando and
	Anoop Cherian and
	Peter Anderson and
	Rodrigo Santa Cruz and
	Edison Guo},
	title     = {On Differentiating Parameterized Argmin and Argmax Problems with Application
	to Bi-level Optimization},
	journal   = {CoRR},
	volume    = {abs/1607.05447},
	year      = {2016},
	url       = {http://arxiv.org/abs/1607.05447},
	archivePrefix = {arXiv},
	eprint    = {1607.05447},
	timestamp = {Tue, 20 Nov 2018 12:24:39 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/GouldFCACG16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Samuel2009,
	title={Learning optimized MAP estimates in continuously-valued MRF models},
	author={Kegan G. G. Samuel and Marshall F. Tappen},
	journal={2009 IEEE Conference on Computer Vision and Pattern Recognition},
	year={2009},
	pages={477-484}
}

@article{Hasselt11,
	author = {Van Hasselt, Hado},
	year = {2011},
	month = {01},
	pages = {},
	title = {Insights in reinforcement learning : formal analysis and empirical evaluation of temporal-difference learning algorithms}
}






@article{Fujimoto18,
	title={Addressing Function Approximation Error in Actor-Critic Methods},
	author={Scott Fujimoto and Herke van Hoof and David Meger},
	journal={International Conference on Machine Learning},
	year={2018},
	volume={abs/1802.09477}
}

@article{Bertsekas15,
	author = {Bertsekas, Dimitri},
	year = {2015},
	month = {07},
	pages = {},
	title = {Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey},
	volume = {2010},
	journal = {Optimization}
}

@article{Nedic99,
	author = {Nedic, Angelia and Bertsekas, Dimitri},
	year = {1999},
	month = {01},
	pages = {},
	title = {Incremental Subgradient Methods for Nondifferentiable Optimization},
	volume = {12},
	journal = {Siam Journal on Optimization - SIAMJO},
	doi = {10.1137/S1052623499362111}
}

@ARTICLE{Lagoudakis03,
	author = {Michail G. Lagoudakis and Ronald Parr},
	title = {Least-Squares Policy Iteration},
	journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
	year = {2003},
	volume = {4},
	pages = {1107--1149}
}

@InProceedings{Dai18,
	title = 	 {{SBEED}: Convergent Reinforcement Learning with Nonlinear Function Approximation},
	author = 	 {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {1125--1134},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/dai18c/dai18c.pdf},
	url = 	 {http://proceedings.mlr.press/v80/dai18c.html},
	abstract = 	 {When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov’s smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm’s sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.}
}

@article{Robbins51,
	Author = {Herbert Robbins and Sutton Monro},
	Doi = {10.1214/aoms/1177729586},
	Journal = {The Annals of Mathematical Statistics},
	Number = {3},
	Pages = {400 -- 407},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{A Stochastic Approximation Method}},
	Url = {https://doi.org/10.1214/aoms/1177729586},
	Volume = {22},
	Year = {1951},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177729586}}


@INPROCEEDINGS{Azizzadenesheli18,
	
	author={Azizzadenesheli, Kamyar and Brunskill, Emma and Anandkumar, Animashree},
	
	booktitle={2018 Information Theory and Applications Workshop (ITA)}, 
	
	title={Efficient Exploration Through Bayesian Deep Q-Networks}, 
	
	year={2018},
	
	volume={},
	
	number={},
	
	pages={1-9},
	
	doi={10.1109/ITA.2018.8503252}}


@article{Ghavamzadeh16AC,
	author  = {Mohammad Ghavamzadeh and Yaakov Engel and Michal Valko},
	title   = {Bayesian Policy Gradient and Actor-Critic Algorithms},
	journal = {Journal of Machine Learning Research},
	year    = {2016},
	volume  = {17},
	number  = {66},
	pages   = {1-53},
	url     = {http://jmlr.org/papers/v17/10-245.html}
}



@inproceedings{Rezende15,
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	title = {Variational Inference with Normalizing Flows},
	year = {2015},
	publisher = {JMLR},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational
	inference. Most applications of variational inference employ simple families of posterior
	approximations in order to allow for efficient inference, focusing on mean-field or
	other simple structured approximations. This restriction has a significant impact
	on the quality of inferences made using variational methods. We introduce a new approach
	for specifying flexible, arbitrarily complex and scalable approximate posterior distributions.
	Our approximations are distributions constructed through a normalizing flow, whereby
	a simple initial density is transformed into a more complex one by applying a sequence
	of invertible transformations until a desired level of complexity is attained. We
	use this view of normalizing flows to develop categories of finite and infinitesimal
	flows and provide a unified view of approaches for constructing rich posterior approximations.
	We demonstrate that the theoretical advantages of having posteriors that better match
	the true posterior, combined with the scalability of amortized variational approaches,
	provides a clear improvement in performance and applicability of variational inference.},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {1530–1538},
	numpages = {9},
	location = {Lille, France},
	series = {ICML'15}
}





@inproceedings{Maei10,
	author = {Maei, Hamid Reza and Szepesv\'{a}ri, Csaba and Bhatnagar, Shalabh and Sutton, Richard S.},
	title = {Toward Off-Policy Learning Control with Function Approximation},
	year = {2010},
	isbn = {9781605589077},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	pages = {719–726},
	numpages = {8},
	location = {Haifa, Israel},
	series = {ICML’10}
}


@inbook{Harris56,
	author = {T. E. Harris},
	editor = {Jerzy Neyman},
	doi = {doi:10.1525/9780520350670-011},
	url = {https://doi.org/10.1525/9780520350670-011},
	title = {The Existence of Stationary Measures for Certain Markov Processes},
	booktitle = {Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Contributions to Probability Theory},
	year = {1956},
	publisher = {University of California Press},
	pages = {113--124}
}


@article{Baxendale11,
	author = {Peter Baxendale},
	title = {{T. E. Harris’s contributions to recurrent Markov processes and stochastic flows}},
	volume = {39},
	journal = {The Annals of Probability},
	number = {2},
	publisher = {Institute of Mathematical Statistics},
	pages = {417 -- 428},
	keywords = {Coalescence, Harris recurrence, Recurrent Markov processes, stirring processes, Stochastic flows},
	year = {2011},
	doi = {10.1214/10-AOP594},
	URL = {https://doi.org/10.1214/10-AOP594}
}


@InProceedings{Ciosek2020,
	author = {Ciosek, Kamil and Fortuin, Vincent and Tomioka, Ryota and Hofmann, Katja and Turner, Richard },
	title = {Conservative Uncertainty Estimation By Fitting Prior Networks},
	booktitle = {Eighth International Conference on Learning Representations},
	year = {2020},
	month = {04},
	abstract = {Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, we theoretically justify a scheme for estimating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never underestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. We also show concentration, implying that the uncertainty estimates converge to zero as we get more data. Uncertainty estimates obtained from random priors can be adapted to any deep network architecture and trained using standard supervised learning pipelines. We provide experimental evaluation of random priors on calibration and out-of-distribution detection on typical computer vision tasks, demonstrating that they outperform deep ensembles in practice.},
}

@incollection{Lakshminarayanan17,
	title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {6402--6413},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf}
}


@incollection{Osband18,
	title = {Randomized Prior Functions for Deep Reinforcement Learning},
	author = {Osband, Ian and Aslanides, John and Cassirer, Albin},
	booktitle = {Advances in Neural Information Processing Systems 31},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {8617--8629},
	year = {2018},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/8080-randomized-prior-functions-for-deep-reinforcement-learning.pdf}
}


@article{Pearce19,
	title={Uncertainty in Neural Networks: Bayesian Ensembling},
	author={Tim Pearce and Mohamed Zaki and Alexandra Brintrup and Andy Neely},
	journal={ArXiv Preprint},
	year={2019},
	month={10},
	volume={abs/1810.05546}
}

@PhdThesis{Gal2016Uncertainty,
	title={Uncertainty in Deep Learning},
	author={Gal, Yarin},
	year={2016},
	school={University of Cambridge}
}

@inproceedings{Gal16,
	author = {Gal, Yarin and Ghahramani, Zoubin},
	title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	year = {2016},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	pages = {1050–1059},
	numpages = {10},
	location = {New York, NY, USA},
	series = {ICML’16}
}





@article{Srivastava14a,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@MISC{Petersen12,
	author       = "K. B. Petersen and M. S. Pedersen",
	title        = "The Matrix Cookbook",
	year         = "2012",
	month        = "nov",
	keywords     = "Matrix identity, matrix relations, inverse, matrix derivative",
	publisher    = "Technical University of Denmark",
	address      = "",
	note         = "Version 20121115",
	url          = "http://localhost/pubdb/p.php?3274",
	abstract     = "Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices."
}


@book{Baker97,
	series = {Monographs on numerical analysis},
	publisher = {Clarendon Press},
	isbn = {9780198534068},
	year = {1977},
	title = {The numerical treatment of integral equations},
	language = {eng},
	address = {Oxford},
	author = {Baker, Christopher T. H},
	keywords = {Integral equations -- Numerical solutions},
	lccn = {LC},
}



@article{Osband19,
	author  = {Ian Osband and Benjamin Van Roy and Daniel J. Russo and Zheng Wen},
	title   = {Deep Exploration via Randomized Value Functions},
	journal = {Journal of Machine Learning Research},
	year    = {2019},
	volume  = {20},
	number  = {124},
	pages   = {1-62},
	url     = {http://jmlr.org/papers/v20/18-339.html}
}

@misc{Yang19,
	title={A Theoretical Analysis of Deep Q-Learning},
	author={Zhuora Yang and Yuchen Xie and Zhaoran Wang},
	year={2019},
	eprint={1901.00137},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@InProceedings{Riedmiller05,
	author="Riedmiller, Martin",
	editor="Gama, Jo{\~a}o
	and Camacho, Rui
	and Brazdil, Pavel B.
	and Jorge, Al{\'i}pio M{\'a}rio
	and Torgo, Lu{\'i}s",
	title="Neural Fitted Q Iteration -- First Experiences with a Data Efficient Neural Reinforcement Learning Method",
	booktitle="Machine Learning: ECML 2005",
	year="2005",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="317--328",
	abstract="This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.",
	isbn="978-3-540-31692-3"
}


@inproceedings{Antos07,
	author = {Antos, Andr\'{a}s and Munos, R\'{e}mi and Szepesv\'{a}ri, Csaba},
	title = {Fitted Q-Iteration in Continuous Action-Space MDPs},
	year = {2007},
	isbn = {9781605603520},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
	pages = {9–16},
	numpages = {8},
	location = {Vancouver, British Columbia, Canada},
	series = {NIPS’07}
}

@inproceedings{Song10,
	author = {Song, Le and Boots, Byron and Siddiqi, Sajid M. and Gordon, Geoffrey and Smola, Alex},
	title = {Hilbert Space Embeddings of Hidden Markov Models},
	year = {2010},
	isbn = {9781605589077},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	pages = {991–998},
	numpages = {8},
	location = {Haifa, Israel},
	series = {ICML’10}
}
@INPROCEEDINGS{Tsitsiklis94,
	author = {John N. Tsitsiklis and Benjamin Van Roy},
	title = {Feature-Based Methods For Large Scale Dynamic Programming},
	booktitle = {Machine Learning},
	year = {1994},
	pages = {59--94}
}

@incollection{Gordan95,
	title = {Stable Function Approximation in Dynamic Programming},
	editor = {Armand Prieditis and Stuart Russell},
	booktitle = {Machine Learning Proceedings 1995},
	publisher = {Morgan Kaufmann},
	address = {San Francisco (CA)},
	pages = {261-268},
	year = {1995},
	isbn = {978-1-55860-377-6},
	doi = {https://doi.org/10.1016/B978-1-55860-377-6.50040-2},
	url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500402},
	author = {Geoffrey J. Gordon},
	abstract = {The success of reinforcement learning in practical problems depends on the ability to combine function approximation with temporal difference methods such as value iteration. Experiments in this area have produced mixed results; there have been both notable successes and notable disappointments. Theory has been scarce, mostly due to the difficulty of reasoning about function approximators that generalize beyond the observed data. We provide a proof of convergence for a wide class of temporal difference methods involving function approximators such as k-nearest-neighbor, and show experimentally that these methods can be useful. The proof is based on a view of function approximators as expansion or contraction mappings. In addition, we present a novel view of fitted value iteration: an approximate algorithm for one environment turns out to be an exact algorithm for a different environment.}
}


@inproceedings{Grunewalder12,
	author = {Gr\"{u}new\"{a}lder, Steffen and Lever, Guy and Baldassarre, Luca and Pontil, Massimilano and Gretton, Arthur},
	title = {Modelling Transition Dynamics in MDPs with RKHS Embeddings},
	year = {2012},
	isbn = {9781450312851},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
	pages = {1603–1610},
	numpages = {8},
	location = {Edinburgh, Scotland},
	series = {ICML’12}
}



@book{Wasserman06,
	author = {Wasserman, Larry},
	title = {All of Nonparametric Statistics (Springer Texts in Statistics)},
	year = {2006},
	isbn = {0387251456},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg}
}

@BOOK{Ghavamzadeh15,
	author={M. {Ghavamzadeh} and S. {Mannor} and J. {Pineau} and A. {Tamar}},
	booktitle={Bayesian Reinforcement Learning: A Survey},
	title={Bayesian Reinforcement Learning: A Survey},
	year={2015},
	volume={},
	number={},
	pages={},
	keywords={Reinforcement learning},
	doi={},
	ISSN={null},
	publisher={now},
	isbn={null},
	url={https://ieeexplore.ieee.org/document/8187142},}

@article{Lerma92,
	title={Discrete-time Markov control processes with discounted unbounded costs: Optimality criteria},
	author={O. Hern{\'a}ndez-Lerma and Myriam Mu{\~n}oz de Ozak},
	journal={Kybernetika},
	year={1992},
	volume={28},
	pages={191-212}
}

@book{Fristedt97,
	series = {Probability and its applications},
	publisher = {Birkhäuser},
	isbn = {9780817638078},
	year = {1997},
	title = {A modern approach to probability theory},
	language = {eng},
	address = {Boston},
	author = {Fristedt, Bert and Gray, Lawrence F},
	keywords = {Probabilities},
	lccn = {96005687},
}




@article{Szepesvari10,
	Author = {Szepesv{\'{a}}ri, Csaba},
	Doi = {10.2200/S00268ED1V01Y201005AIM009},
	Isbn = {9781608454921},
	Issn = {1939-4608},
	Journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	Number = {1},
	Pages = {1--103},
	Title = {{Algorithms for Reinforcement Learning}},
	Url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
	Volume = {4},
	Year = {2010},
	Bdsk-Url-1 = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
	Bdsk-Url-2 = {https://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009}}

@book{Puterman94,
	author = {Puterman, Martin L.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994},
	isbn = {0471619779},
	publisher = {John Wiley \& Sons, Inc.},
	address = {USA},
	edition = {1st}
}

@book{Bertsekas96,
	title={Constrained Optimization and Lagrange Multiplier Methods},
	author={Bertsekas, D.P.},
	isbn={9781886529045},
	lccn={96079307},
	series={Athena scientific series in optimization and neural computation},
	url={http://web.mit.edu/dimitrib/www/Constrained-Opt.pdf},
	year={1996},
	publisher={Athena Scientific}
}

@book{Ghosal17, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={Fundamentals of Nonparametric Bayesian Inference}, DOI={10.1017/9781139029834}, publisher={Cambridge University Press}, author={Ghosal, Subhashis and van der Vaart, Aad}, year={2017}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@article{Bunke98,
	author = "Bunke, Olaf and Milhaud, Xavier",
	doi = "10.1214/aos/1028144851",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "04",
	number = "2",
	pages = "617--644",
	publisher = "The Institute of Mathematical Statistics",
	title = "Asymptotic behavior of Bayes estimates under possibly incorrect models",
	url = "https://doi.org/10.1214/aos/1028144851",
	volume = "26",
	year = "1998"
}



@article{Berk70,
	author = "Berk, Robert H.",
	doi = "10.1214/aoms/1177696967",
	fjournal = "The Annals of Mathematical Statistics",
	journal = "Ann. Math. Statist.",
	month = "06",
	number = "3",
	pages = "894--906",
	publisher = "The Institute of Mathematical Statistics",
	title = "Consistency a Posteriori",
	url = "https://doi.org/10.1214/aoms/1177696967",
	volume = "41",
	year = "1970"
}

@article{Berk66,
	author = "Berk, Robert H.",
	doi = "10.1214/aoms/1177699597",
	fjournal = "The Annals of Mathematical Statistics",
	journal = "Ann. Math. Statist.",
	month = "02",
	number = "1",
	pages = "51--58",
	publisher = "The Institute of Mathematical Statistics",
	title = "Limiting Behavior of Posterior Distributions when the Model is Incorrect",
	url = "https://doi.org/10.1214/aoms/1177699597",
	volume = "37",
	year = "1966"
}



@inproceedings{LeCam53,
	title={On some asymptotic properties of maximum likelihood estimates and related Bayes' estimates},
	author={Le Cam, Lucien },
	year={1953},
	journal={University of California publications in statistics},
	Volume={1}, 
	pages = 	 {277--300},
}

@article{Murphy07,
	author = {Murphy, Kevin},
	year = {2007},
	month = {11},
	pages = {},
	title = {Conjugate Bayesian analysis of the Gaussian distribution}
}


@inproceedings{Chowdhary13,
	author = {Chowdhary, Girish and Liu, Miao and Grande, Robert and Walsh, Thomas and How, Jonathan},
	year = {2013},
	month = {01},
	pages = {},
	title = {Off-Policy Reinforcement Learning with Gaussian Processes},
	volume = {1},
	journal = {IEEE/CAA Journal of Automatica Sinica},
	doi = {10.1109/JAS.2014.7004680}
}

@inproceedings{Engel05, author = {Engel, Yaakov and Mannor, Shie and Meir, Ron}, title = {Reinforcement Learning with Gaussian Processes}, year = {2005}, isbn = {1595931805}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1102351.1102377}, doi = {10.1145/1102351.1102377}, booktitle = {Proceedings of the 22nd International Conference on Machine Learning}, pages = {201–208}, numpages = {8}, location = {Bonn, Germany}, series = {ICML ’05} }


@article{Levine18,
	abstract = {The framework of reinforcement learning or optimal control provides a mathe-matical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learn-ing and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: for-malizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy rein-forcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynam-ics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.00909v3},
	author = {Levine, Sergey},
	eprint = {arXiv:1805.00909v3},
	file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Levine - 2018 - Reinforcement Learning and Control as Probabilistic Inference Tutorial and Review.pdf:pdf},
	title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
	url = {https://arxiv.org/pdf/1805.00909.pdf},
	year = {2018}
}


@InProceedings{Haarnoja18,
	title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author = 	 {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {1861--1870},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
	url = 	 {http://proceedings.mlr.press/v80/haarnoja18b.html},
	abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}


@incollection{Fellows19,
	title = {VIREL: A Variational Inference Framework for Reinforcement Learning},
	author = {Fellows, Matthew and Mahajan, Anuj and Rudner, Tim G. J. and Whiteson, Shimon},
	booktitle = {Advances in Neural Information Processing Systems 32},
	pages = {7120--7134},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/8934-virel-a-variational-inference-framework-for-reinforcement-learning.pdf}
}


@incollection{Fellows21,
	title = {Bayesian Bellman Operators},
	author = {Fellows, Matthew and Hartikainen, Kristian and Whiteson, Shimon},
	booktitle = {Advances in Neural Information Processing Systems 34},
	year = {2021},
	publisher = {Curran Associates, Inc.},
}

@article{Tsitsiklis97,
	author={J. N. {Tsitsiklis} and B. {Van Roy}},
	journal={IEEE Transactions on Automatic Control},
	title={An analysis of temporal-difference learning with function approximation},
	year={1997},
	volume={42},
	number={5},
	pages={674-690},
	keywords={learning (artificial intelligence);function approximation;Markov processes;convergence;temporal-difference learning;cost-to-go function;infinite-horizon discounted Markov chain;linear function approximation;irreducible aperiodic Markov chain;finite state space;infinite state space;convergence;nonlinear function approximators;Function approximation;Convergence;Cost function;Algorithm design and analysis;Dynamic programming;Linear approximation;State-space methods;Approximation error;Approximation algorithms;Error analysis},
	doi={10.1109/9.580874},
	ISSN={2334-3303},
	month={May},}


@article{Kaelbling94,
	author = {Kaelbling, Leslie Pack},
	title = {Associative Reinforcement Learning: Functions in k-DNF},
	year = {1994},
	issue_date = {June 1994},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {15},
	number = {3},
	issn = {0885-6125},
	url = {https://doi.org/10.1023/A:1022689909846},
	doi = {10.1023/A:1022689909846},
	abstract = {An agent that must learn to act in the world by trial and error faces the reinforcement
	learning problem, which is quite different from standard concept learning. Although
	good algorithms exist for this problem in the general case, they are often quite inefficient
	and do not exhibit generalization. One strategy is to find restricted classes of action
	policies that can be learned more efficiently. This paper pursues that strategy by
	developing algorithms that can efficiently learn action maps that are expressible
	in k-DNF. The algorithms are compared with existing methods in empirical trials and
	are shown to have very good performance.},
	journal = {Mach. Learn.},
	month = jun,
	pages = {279–298},
	numpages = {20},
	keywords = {generalization, reinforcement learning, k-DNF}
}





@inproceedings{Wang05,
	author = {Wang, Tao and Lizotte, Daniel and Bowling, Michael and Schuurmans, Dale},
	title = {Bayesian Sparse Sampling for On-Line Reward Optimization},
	year = {2005},
	isbn = {1595931805},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1102351.1102472},
	doi = {10.1145/1102351.1102472},
	abstract = {We present an efficient "sparse sampling" technique for approximating Bayes optimal
	decision making in reinforcement learning, addressing the well known exploration versus
	exploitation tradeoff. Our approach combines sparse sampling with Bayesian exploration
	to achieve improved decision making while controlling computational cost. The idea
	is to grow a sparse lookahead tree, intelligently, by exploiting information in a
	Bayesian posterior---rather than enumerate action branches (standard sparse sampling)
	or compensate myopically (value of perfect information). The outcome is a flexible,
	practical technique for improving action selection in simple reinforcement learning
	scenarios.},
	booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
	pages = {956–963},
	numpages = {8},
	location = {Bonn, Germany},
	series = {ICML '05}
}


@InProceedings{Bellemare17,
	title = 	 {A Distributional Perspective on Reinforcement Learning},
	author =       {Marc G. Bellemare and Will Dabney and R{\'e}mi Munos},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {449--458},
	year = 	 {2017},
	editor = 	 {Precup, Doina and Teh, Yee Whye},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06--11 Aug},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
	url = 	 {https://proceedings.mlr.press/v70/bellemare17a.html},
	abstract = 	 {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.}
}


@ARTICLE{Kobyzev19,
	author={Kobyzev, Ivan and Prince, Simon J.D. and Brubaker, Marcus A.},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={Normalizing Flows: An Introduction and Review of Current Methods}, 
	year={2021},
	volume={43},
	number={11},
	pages={3964-3979},
	doi={10.1109/TPAMI.2020.2992934}}


@InProceedings{Huang18,
	title = 	 {Neural Autoregressive Flows},
	author =       {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {2078--2087},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/huang18d/huang18d.pdf},
	url = 	 {https://proceedings.mlr.press/v80/huang18d.html},
	abstract = 	 {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.}
}


@inproceedings{Janz19,
	author = {Janz, David and Hron, Jiri and Mazur, Przemys\l{}aw and Hofmann, Katja and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Tschiatschek, Sebastian},
	title = {Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing
	exploration and exploitation in reinforcement learning. Randomised value functions
	(RVF) can be viewed as a promising approach to scaling PSRL. However, we show that
	most contemporary algorithms combining RVF with neural network function approximation
	do not possess the properties which make PSRL effective, and provably fail in sparse
	reward problems. Moreover, we find that propagation of uncertainty, a property of
	PSRL previously thought important for exploration, does not preclude this failure.
	We use these insights to design Successor Uncertainties (SU), a cheap and easy to
	implement RVF algorithm that retains key properties of PSRL. SU is highly effective
	on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it
	surpasses human performance on 38 of 49 games tested (achieving a median human normalised
	score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36
	of those.},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	articleno = {405},
	numpages = {10}
}


@InProceedings{Osband17a,
	title = 	 {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},
	author =       {Ian Osband and Van Roy, Benjamin},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {2701--2710},
	year = 	 {2017},
	editor = 	 {Precup, Doina and Teh, Yee Whye},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06--11 Aug},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/osband17a/osband17a.pdf},
	url = 	 {https://proceedings.mlr.press/v70/osband17a.html},
	abstract = 	 {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.}
}


@inproceedings{Agrawal17,
	author = {Agrawal, Shipra and Jia, Randy},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
	url = {https://proceedings.neurips.cc/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},
	volume = {30},
	year = {2017}
}



@inproceedings{Ciosek19,
	author = {Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Better Exploration with Optimistic Actor Critic},
	url = {https://proceedings.neurips.cc/paper/2019/file/a34bacf839b923770b2c360eefa26748-Paper.pdf},
	volume = {32},
	year = {2019}
}


@INPROCEEDINGS{Strens00,
	author = {Malcolm Strens},
	title = {A Bayesian framework for reinforcement learning},
	booktitle = {In Proceedings of the Seventeenth International Conference on Machine Learning},
	year = {2000},
	pages = {943--950},
	publisher = {ICML}
}

@inproceedings{Osband13,
	author = {Osband, Ian and Van Roy, Benjamin and Russo,Daniel},
	title = {(More) Efficient Reinforcement Learning via Posterior Sampling},
	year = {2013},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Most provably-efficient reinforcement learning algorithms introduce optimism about
	poorly-understood states and actions to encourage exploration. We study an alternative
	approach for efficient exploration: posterior sampling for reinforcement learning
	(PSRL). This algorithm proceeds in repeated episodes of known duration. At the start
	of each episode, PSRL updates a prior distribution over Markov decision processes
	and takes one sample from this posterior. PSRL then follows the policy that is optimal
	for this sample during the episode. The algorithm is conceptually simple, computationally
	efficient and allows an agent to encode prior knowledge in a natural way. We establish
	an \~{O}(τS/√AT) bound on expected regret, where T is time, τ is the episode length and
	S and A are the cardinalities of the state and action spaces. This bound is one of
	the first for an algorithm not based on optimism, and close to the state of the art
	for any reinforcement learning algorithm. We show through simulation that PSRL significantly
	outperforms existing algorithms with similar regret bounds.},
	booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
	pages = {3003–3011},
	numpages = {9},
	location = {Lake Tahoe, Nevada},
	series = {NIPS'13}
}

@inproceedings{Kolter09,
	author = {Kolter, J. and Ng, Andrew},
	year = {2009},
	month = {01},
	pages = {65},
	title = {Near-Bayesian exploration in polynomial time},
	volume = {382},
	journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
	doi = {10.1145/1553374.1553441}
}

@article{Thompson33,
	author = {Thomson, William R},
	title = "{On the likelihood that one unknown probability exceeds another in view of the evidence of two samples}",
	journal = {Biometrika},
	volume = {25},
	number = {3-4},
	pages = {285-294},
	year = {1933},
	month = {12},
	issn = {0006-3444},
	doi = {10.1093/biomet/25.3-4.285},
	url = {https://doi.org/10.1093/biomet/25.3-4.285},
	eprint = {http://oup.prod.sis.lan/biomet/article-pdf/25/3-4/285/513725/25-3-4-285.pdf},
}



@thesis{Kakade03,
	author = {Kakade, Sham},
	year = {2003},
	month = {01},
	title = {On the Sample Complexity of Reinforcement Learning}
}

@article{Terenin2017,
	title={A Noninformative Prior on a Space of Distribution Functions},
	author={Alexander Terenin and David Draper},
	journal={Entropy},
	year={2017},
	volume={19},
	pages={391}
}

@Inbook{Teh10,
	author="Teh, Yee Whye",
	editor="Sammut, Claude
	and Webb, Geoffrey I.",
	title="Dirichlet Process",
	bookTitle="Encyclopedia of Machine Learning",
	year="2010",
	publisher="Springer US",
	address="Boston, MA",
	pages="280--287",
	isbn="978-0-387-30164-8",
	doi="10.1007/978-0-387-30164-8_219",
	url="https://doi.org/10.1007/978-0-387-30164-8_219"
}

@article{Bellman56,
	ISSN = {00364452},
	URL = {http://www.jstor.org/stable/25048278},
	author = {Richard Bellman},
	journal = {Sankhyā: The Indian Journal of Statistics (1933-1960)},
	number = {3/4},
	pages = {221--229},
	publisher = {Springer},
	title = {A Problem in the Sequential Design of Experiments},
	volume = {16},
	year = {1956}
}

@inproceedings{Guez12,
	author = {Guez, Arthur and Silver, David and Dayan, Peter},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search},
	url = {https://proceedings.neurips.cc/paper/2012/file/35051070e572e47d2c26c241ab88307f-Paper.pdf},
	volume = {25},
	year = {2012}
}

@inproceedings{Poupart06,
	author = {Poupart, Pascal and Vlassis, Nikos and Hoey, Jesse and Regan, Kevin},
	title = {An Analytic Solution to Discrete Bayesian Reinforcement Learning},
	year = {2006},
	isbn = {1595933832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1143844.1143932},
	doi = {10.1145/1143844.1143932},
	abstract = {Reinforcement learning (RL) was originally proposed as a framework to allow agents
	to learn in an online fashion as they interact with their environment. Existing RL
	algorithms come short of achieving this goal because the amount of exploration required
	is often too costly and/or too time consuming for online learning. As a result, RL
	is mostly used for offline learning in simulated environments. We propose a new algorithm,
	called BEETLE, for effective online learning that is computationally efficient while
	minimizing the amount of exploration. We take a Bayesian model-based approach, framing
	RL as a partially observable Markov decision process. Our two main contributions are
	the analytical derivation that the optimal value function is the upper envelope of
	a set of multivariate polynomials, and an efficient point-based value iteration algorithm
	that exploits this simple parameterization.},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	pages = {697–704},
	numpages = {8},
	location = {Pittsburgh, Pennsylvania, USA},
	series = {ICML '06}
}



@article{Guez13,
	author = {Guez, Arthur and Silver, David and Dayan, Peter},
	year = {2013},
	month = {10},
	pages = {841-883},
	title = {Scalable and Efficient Bayes-Adaptive Reinforcement Learning Based on Monte-Carlo Tree Search},
	volume = {48},
	journal = {Journal of Artificial Intelligence Research},
	doi = {10.1613/jair.4117}
}

@article{Ferguson73,
	author = {Thomas S. Ferguson},
	title = {{A Bayesian Analysis of Some Nonparametric Problems}},
	volume = {1},
	journal = {The Annals of Statistics},
	number = {2},
	publisher = {Institute of Mathematical Statistics},
	pages = {209 -- 230},
	year = {1973},
	doi = {10.1214/aos/1176342360},
	URL = {https://doi.org/10.1214/aos/1176342360}
}




@book{Wald50,
	series = {Wiley publications in statistics},
	publisher = {John Wiley \& Sons},
	year = {1950},
	title = {Statistical decision functions},
	language = {eng},
	address = {New York : London},
	author = {Wald, Abraham},
	keywords = {Mathematical statistics, Statistical decision},
	lccn = {50009735},
}



@article{Wald47,
	author = {Abraham Wald},
	title = {{An Essentially Complete Class of Admissible Decision Functions}},
	volume = {18},
	journal = {The Annals of Mathematical Statistics},
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	pages = {549 -- 555},
	year = {1947},
	doi = {10.1214/aoms/1177730345},
	URL = {https://doi.org/10.1214/aoms/1177730345}
}

@phdthesis{Duff02,
	author = {Duff, Michael O’Gordon},
	title = {Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes},
	year = {2002},
	isbn = {0493525734},
	publisher = {University of Massachusetts Amherst},
	note = {AAI3039353},
	school = {Department of Computer Science, University of Massachusetts Amherst	}
}


@Inbook{Vlassis2012,
	author="Vlassis, Nikos
	and Ghavamzadeh, Mohammad
	and Mannor, Shie
	and Poupart, Pascal",
	editor="Wiering, Marco
	and van Otterlo, Martijn",
	title="Bayesian Reinforcement Learning",
	bookTitle="Reinforcement Learning: State-of-the-Art",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="359--386",
	abstract="This chapter surveys recent lines of work that use Bayesian techniques for reinforcement learning. In Bayesian learning, uncertainty is expressed by a prior distribution over unknown parameters and learning is achieved by computing a posterior distribution based on the data observed. Hence, Bayesian reinforcement learning distinguishes itself from other forms of reinforcement learning by explicitly maintaining a distribution over various quantities such as the parameters of the model, the value function, the policy or its gradient. This yields several benefits: a) domain knowledge can be naturally encoded in the prior distribution to speed up learning; b) the exploration/exploitation tradeoff can be naturally optimized; and c) notions of risk can be naturally taken into account to obtain robust policies.",
	isbn="978-3-642-27645-3",
	doi="10.1007/978-3-642-27645-3_11",
	url="https://doi.org/10.1007/978-3-642-27645-3_11"
}

@inproceedings{Scholkopf01,
	author = {Sch\"{o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
	title = {A Generalized Representer Theorem},
	booktitle = {Proceedings of the 14th Annual Conference on Computational Learning Theory and and 5th European Conference on Computational Learning Theory},
	series = {COLT '01/EuroCOLT '01},
	year = {2001},
	isbn = {3-540-42343-5},
	pages = {416--426},
	numpages = {11},
	url = {http://dl.acm.org/citation.cfm?id=648300.755324},
	acmid = {755324},
	publisher = {Springer-Verlag},
	address = {London, UK, UK},
} 


@article{mercer1909,
	author = {Mercer, J.},
	biburl = {https://www.bibsonomy.org/bibtex/2e58f01606a71fae2cda3de0a0358d62f/naufraghi},
	interhash = {bdc820a36362be8a416e89783780f939},
	intrahash = {e58f01606a71fae2cda3de0a0358d62f},
	journal = {Philosophical Transactions of the Royal Society, London},
	keywords = {MachineLearning intelligenza-artificiale imported svm},
	pages = {415--446},
	timestamp = {2006-09-15T00:20:38.000+0200},
	title = {Functions of positive and negative type, and their connection with the theory of integral equations},
	volume = 209,
	year = 1909
}

@inbook{gilks96,
	author = {Gilks, W.R. and Richardson, S. and Spiegelhalter, D.},
	isbn = {9780412055515},
	lccn = {98033429},
	publisher = {Taylor \& Francis},
	chapter = {Introduction to General State-Space Markov Chain Theory},
	series = {Chapman \& Hall/CRC Interdisciplinary Statistics},
	title = {Markov Chain Monte Carlo in Practice},
	year = {1995},
}

@inproceedings{wu2017acktr,
	abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at this https URL},
	added-at = {2019-12-16T18:30:30.000+0100},
	author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B. and Liao, Shun and Ba, Jimmy},
	biburl = {https://www.bibsonomy.org/bibtex/2e1da457c9a3a8004025dbc4730291c33/lanteunis},
	booktitle = {NIPS},
	editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	ee = {http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation},
	interhash = {a90966c72041a33ba6f8a3b1bf6b4468},
	intrahash = {e1da457c9a3a8004025dbc4730291c33},
	keywords = {DRLAlgoComparison acktr reinforcement_learning},
	pages = {5279-5288},
	timestamp = {2019-12-29T16:29:37.000+0100},
	title = {Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
	url = {http://dblp.uni-trier.de/db/conf/nips/nips2017.html#WuMGLB17},
	year = 2017
}



@article{wang2016acer,
	abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method. },
	added-at = {2019-12-16T18:28:00.000+0100},
	author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Rémi and Kavukcuoglu, Koray and de Freitas, Nando},
	biburl = {https://www.bibsonomy.org/bibtex/2f6fb87ed0695c5aa692d83fc7cab7794/lanteunis},
	ee = {http://arxiv.org/abs/1611.01224},
	interhash = {ce284493dd67acc99e0c89e834a7161c},
	intrahash = {f6fb87ed0695c5aa692d83fc7cab7794},
	journal = {CoRR},
	keywords = {DRLAlgoComparison acer reinforcement_learning},
	timestamp = {2019-12-16T21:10:37.000+0100},
	title = {Sample Efficient Actor-Critic with Experience Replay.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1611.html#WangBHMMKF16},
	volume = {abs/1611.01224},
	year = 2016
}



@article{Aronszajn50,
	author = {Aronszajn, N.},
	biburl = {https://www.bibsonomy.org/bibtex/2024c71f807cbf95a8fb6b934c01f4919/sb3000},
	description = {CiteULike: Theory of reproducing kernels},
	interhash = {5f0e5e40a1512aa0b21f287a39b81b31},
	intrahash = {024c71f807cbf95a8fb6b934c01f4919},
	journal = {Transactions of the American Mathematical Society},
	keywords = {kernel},
	number = 3,
	pages = {337--404},
	timestamp = {2010-10-07T14:13:58.000+0200},
	title = {Theory of Reproducing Kernels},
	url = {http://dx.doi.org/10.2307/1990404},
	volume = 68,
	year = 1950
}

@MISC{Gretton15o,
	author = {Arthur Gretton},
	title = {Introduction to RKHS, and some simple kernel algorithms},
	year = {2015}
}

@book{Rasmussen05,
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
	year = {2005},
	isbn = {026218253X},
	publisher = {The MIT Press},
} 


@inbook{Bass13,
	title={Real Analysis for Graduate Students},
	author={Bass, R.F.},
	isbn={9781481869140},
	url={https://books.google.co.uk/books?id=s6mVlgEACAAJ},
	year={2013},
	chapter = {21},
	publisher={Createspace Ind Pub}
}


@TECHREPORT{Minka99,
	author = {Thomas P. Minka},
	title = {Bayesian Linear Regression},
	institution = {MIT},
	year = {1999}
}

@inbook{Murphy12,
	author = {Murphy, Kevin P.},
	title = {Machine Learning: A Probabilistic Perspective},
	year = {2012},
	isbn = {0262018020, 9780262018029},
	publisher = {The MIT Press},
	chapter = {7},
} 

@inbook{Chipman01,
	address = "Beachwood, OH",
	author = "Chipman, Hugh and George, Edward I. and McCulloch, Robert E.",
	booktitle = "Model selection",
	doi = "10.1214/lnms/1215540964",
	editor = "Lahiri, P.",
	pages = "65--116",
	publisher = "Institute of Mathematical Statistics",
	series = "Lecture Notes--Monograph Series",
	title = "The Practical Implementation of Bayesian Model Selection",
	url = "https://doi.org/10.1214/lnms/1215540964",
	volume = "Volume 38",
	year = "2001"
}

@article{Bertsekas95,
	author = {Bertsekas, Dimitri P.},
	title = {A Counterexample to Temporal Differences Learning},
	journal = {Neural Comput.},
	issue_date = {March 1995},
	volume = {7},
	number = {2},
	month = mar,
	year = {1995},
	issn = {0899-7667},
	pages = {270--279},
	numpages = {10},
	url = {http://dx.doi.org/10.1162/neco.1995.7.2.270},
	doi = {10.1162/neco.1995.7.2.270},
	acmid = {206098},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@MISC{Williams93,
	author = {Ronald J. Williams and Leemon C. Baird and III},
	title = {Analysis of Some Incremental Variants of Policy Iteration: First Steps Toward Understanding Actor-Critic Learning Systems},
	year = {1993}
}

@article{Pearlmutter94,
	author = {Barak A. Pearlmutter},
	title = {Fast Exact Multiplication by the Hessian},
	journal = {Neural Computation},
	year = {1994},
	volume = {6},
	pages = {147--160}
}

@incollection{Sutton09a,
	title = {A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation},
	author = {Sutton, Richard S and Hamid R. Maei and Csaba Szepesv\'{a}ri},
	booktitle = {Advances in Neural Information Processing Systems 21},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	pages = {1609--1616},
	year = {2009},
	publisher = {Curran Associates, Inc.}
}


@inproceedings{Sutton09b,
	author = {Sutton, Richard S. and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv\'{a}ri, Csaba and Wiewiora, Eric},
	title = {Fast Gradient-descent Methods for Temporal-difference Learning with Linear Function Approximation},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	series = {ICML '09},
	year = {2009},
	isbn = {978-1-60558-516-1},
	location = {Montreal, Quebec, Canada},
	pages = {993--1000},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1553374.1553501},
	doi = {10.1145/1553374.1553501},
	acmid = {1553501},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@incollection{Maei09,
	title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
	author = {Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S and Hamid R. Maei and Csaba Szepesv\'{a}ri},
	booktitle = {Advances in Neural Information Processing Systems 22},
	editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	pages = {1204--1212},
	year = {2009},
	publisher = {Curran Associates, Inc.},
}

@Article{Sutton88,
	author="Sutton, Richard S.",
	title="Learning to predict by the methods of temporal differences",
	journal="Machine Learning",
	year="1988",
	month="Aug",
	day="01",
	volume="3",
	number="1",
	pages="9--44",
	abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
	issn="1573-0565",
	doi="10.1007/BF00115009",
	url="https://doi.org/10.1007/BF00115009"
}

@incollection{Baird95,
	abstract = {A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basisfunction system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Qlearning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties.},
	author = {Baird, Leemon},
	doi = {10.1.1.48.3256},
	issn = {00043702},
	journal = {International Conference of Machine Learning},
	number = {July},
	pages = {30--37},
	title = {{Residual algorithms: Reinforcement learning with function approximation}},
	year = {1995}
}


@article{dann2014policy,
	title={Policy evaluation with temporal differences: A survey and comparison},
	author={Dann, Christoph and Neumann, Gerhard and Peters, Jan and others},
	journal={Journal of Machine Learning Research},
	volume={15},
	pages={809--883},
	year={2014},
	publisher={Massachusetts Institute of Technology Press (MIT Press)/Microtome Publishing}
}

@book{kolmogorov1956,
	title={Foundations of the Theory of Probability},
	author={Kolmogorov, A.N. and Morrison, N. and Bharucha-Reid, A.T.},
	lccn={56011512},
	series={AMS Chelsea Publishing Series},
	url={https://books.google.co.uk/books?id=EmetAAAAMAAJ},
	year={1956},
	origdate = {1933},
	publisher={Chelsea Publishing Company}
}
