\begin{thebibliography}{10}

\bibitem{low-level}
Y.~M. Asano, C.~Rupprecht, and A.~Vedaldi.
\newblock A critical analysis of self-supervision, or what we can learn from a
  single image.
\newblock In {\em {ICLR}}, 2020.

\bibitem{cite35}
L.~Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected crfs.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 40(4):834--848,
  2018.

\bibitem{ZOO}
P.~Chen, H.~Zhang, Y.~Sharma, J.~Yi, and C.~Hsieh.
\newblock {ZOO:} zeroth order optimization based black-box attacks to deep
  neural networks without training substitute models.
\newblock In B.~M. Thuraisingham, B.~Biggio, D.~M. Freeman, B.~Miller, and
  A.~Sinha, editors, {\em Proceedings of the 10th {ACM} Workshop on Artificial
  Intelligence and Security}, pages 15--26, 2017.

\bibitem{l1}
P.-Y. Chen, Y.~Sharma, H.~Zhang, J.~Yi, and C.-J. Hsieh.
\newblock Ead: elastic-net attacks to deep neural networks via adversarial
  examples.
\newblock In {\em AAAI}, 2018.

\bibitem{SimCLR}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~E. Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em {ICML}}, 2020.

\bibitem{black-cite4}
Y.~Chen, H.~Yang, Y.~Zhang, K.~Ma, T.~Liu, B.~Han, and J.~Cheng.
\newblock Understanding and improving graph injection attack by promoting
  unnoticeability.
\newblock In {\em {ICLR}}, 2022.

\bibitem{robustbench}
F.~Croce, M.~Andriushchenko, V.~Sehwag, E.~Debenedetti, N.~Flammarion,
  M.~Chiang, P.~Mittal, and M.~Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem{autoattack}
F.~Croce and M.~Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In {\em {ICML}}, 2020.

\bibitem{matrix1}
E.~L. Denton, W.~Zaremba, J.~Bruna, Y.~LeCun, and R.~Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock {\em NIPS}, 2014.

\bibitem{MI-FGSM}
Y.~Dong, F.~Liao, T.~Pang, H.~Su, J.~Zhu, X.~Hu, and J.~Li.
\newblock Boosting adversarial attacks with momentum.
\newblock In {\em {CVPR}}, 2018.

\bibitem{TI-FSGM}
Y.~Dong, T.~Pang, H.~Su, and J.~Zhu.
\newblock Evading defenses to transferable adversarial examples by
  translation-invariant attacks.
\newblock In {\em {CVPR}}, 2019.

\bibitem{robustness}
L.~Engstrom, A.~Ilyas, H.~Salman, S.~Santurkar, and D.~Tsipras.
\newblock Robustness (python library), 2019.

\bibitem{black-box-cite3}
L.~Gao, Q.~Zhang, J.~Song, X.~Liu, and H.~T. Shen.
\newblock Patch-wise attack for fooling deep neural network.
\newblock In A.~Vedaldi, H.~Bischof, T.~Brox, and J.~Frahm, editors, {\em
  {ECCV}}, 2020.

\bibitem{black-mmd}
R.~Gao, F.~Liu, J.~Zhang, B.~Han, T.~Liu, G.~Niu, and M.~Sugiyama.
\newblock Maximum mean discrepancy test is aware of adversarial attacks.
\newblock In M.~Meila and T.~Zhang, editors, {\em {ICML}}, 2021.

\bibitem{Robustness-cite}
R.~Gao, J.~Wang, K.~Zhou, F.~Liu, B.~Xie, G.~Niu, B.~Han, and J.~Cheng.
\newblock Fast and reliable evaluation of adversarial robustness with
  minimum-margin attack.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesv{\'{a}}ri, G.~Niu,
  and S.~Sabato, editors, {\em {ICML}}, 2022.

\bibitem{Rotations}
S.~Gidaris, P.~Singh, and N.~Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In {\em {ICLR}}, 2018.

\bibitem{cite37}
R.~B. Girshick.
\newblock Fast {R-CNN}.
\newblock In {\em {ICCV}}, 2015.

\bibitem{FGSM}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em {ICLR}}, 2015.

\bibitem{Gowal2021}
S.~Gowal, S.~Rebuffi, O.~Wiles, F.~Stimberg, D.~A. Calian, and T.~A. Mann.
\newblock Improving robustness using generated data.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~N. Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, {\em { NeurIPS }}, 2021.

\bibitem{ResNet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {CVPR}}, 2016.

\bibitem{SENet}
J.~Hu, L.~Shen, and G.~Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em {CVPR}}, 2018.

\bibitem{DenseNet}
G.~Huang, Z.~Liu, L.~van~der Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em {CVPR}}, 2017.

\bibitem{ILA}
Q.~Huang, I.~Katsman, Z.~Gu, H.~He, S.~J. Belongie, and S.~Lim.
\newblock Enhancing adversarial example transferability with an intermediate
  level attack.
\newblock In {\em {ICCV}}, 2019.

\bibitem{nes}
A.~Ilyas, L.~Engstrom, A.~Athalye, and J.~Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In {\em {ICML}}, 2018.

\bibitem{pc}
A.~Ilyas, L.~Engstrom, and A.~Madry.
\newblock Prior convictions: Black-box adversarial attacks with bandits and
  priors.
\newblock In {\em { ICLR}}, 2018.

\bibitem{FDA_2020NIPS}
N.~Inkawhich, K.~Liang, B.~Wang, M.~Inkawhich, L.~Carin, and Y.~Chen.
\newblock Perturbing across the feature hierarchy to improve standard and
  strict blackbox attack transferability.
\newblock {\em NeurIPS}, 2020.

\bibitem{MSE-LOW}
N.~Inkawhich, W.~Wen, H.~H. Li, and Y.~Chen.
\newblock Feature space perturbations yield more transferable adversarial
  examples.
\newblock In {\em {CVPR}}, 2019.

\bibitem{Kang2021}
Q.~Kang, Y.~Song, Q.~Ding, and W.~P. Tay.
\newblock Stable neural {ODE} with lyapunov-stable equilibrium points for
  defending against adversarial attacks.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~N. Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, {\em { NeurIPS }}, 2021.

\bibitem{matrix3}
Y.-D. Kim, E.~Park, S.~Yoo, T.~Choi, L.~Yang, and D.~Shin.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock {\em ICLR}, 2016.

\bibitem{scale}
A.~Kurakin, I.~Goodfellow, and S.~Bengio.
\newblock Adversarial machine learning at scale.
\newblock In {\em {ICLR}}, 2017.

\bibitem{BIM}
A.~Kurakin, I.~J. Goodfellow, and S.~Bengio.
\newblock Adversarial examples in the physical world.
\newblock In {\em {ICLR}}, 2017.

\bibitem{detector2}
K.~Lee, K.~Lee, H.~Lee, and J.~Shin.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock {\em NeurIPS}, 31, 2018.

\bibitem{Mahalanobis}
K.~Lee, K.~Lee, H.~Lee, and J.~Shin.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock In S.~Bengio, H.~M. Wallach, H.~Larochelle, K.~Grauman,
  N.~Cesa{-}Bianchi, and R.~Garnett, editors, {\em {NeurIPS}}, 2018.

\bibitem{nobox}
Q.~Li, Y.~Guo, and H.~Chen.
\newblock Practical no-box adversarial attacks against dnns.
\newblock In {\em {NeurIPS }}, 2020.

\bibitem{security2018automatic}
K.~J. Liang, G.~Heilmann, C.~Gregory, S.~O. Diallo, D.~Carlson, G.~P. Spell,
  J.~B. Sigman, K.~Roe, and L.~Carin.
\newblock Automatic threat recognition of prohibited items at aviation
  checkpoint with x-ray imaging: a deep learning approach.
\newblock In {\em Anomaly Detection and Imaging with X-Rays (ADIX) III}, volume
  10632, page 1063203. International Society for Optics and Photonics, 2018.

\bibitem{cite36}
J.~Long, E.~Shelhamer, and T.~Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em {CVPR}}, 2015.

\bibitem{black-box-cite2}
Y.~Long, Q.~Zhang, B.~Zeng, L.~Gao, X.~Liu, J.~Zhang, and J.~Song.
\newblock Frequency domain model augmentation for adversarial attack.
\newblock {\em CoRR}, abs/2207.05382, 2022.

\bibitem{STL-10}
D.~G. Lowe.
\newblock Distinctive image features from scale-invariant keypoints.
\newblock {\em Int. J. Comput. Vis.}, 60(2):91--110, 2004.

\bibitem{detector1}
X.~Ma, B.~Li, Y.~Wang, S.~M. Erfani, S.~Wijewickrema, G.~Schoenebeck, D.~Song,
  M.~E. Houle, and J.~Bailey.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock In {\em {ICLR}}, 2018.

\bibitem{LID}
X.~Ma, B.~Li, Y.~Wang, S.~M. Erfani, S.~N.~R. Wijewickrema, G.~Schoenebeck,
  D.~Song, M.~E. Houle, and J.~Bailey.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock In {\em {ICLR}}, 2018.

\bibitem{PGD}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em {ICLR}}, 2018.

\bibitem{deepfool}
S.-M. Moosavi-Dezfooli, A.~Fawzi, and P.~Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In {\em {CVPR}}, 2016.

\bibitem{Pang2022}
T.~Pang, M.~Lin, X.~Yang, J.~Zhu, and S.~Yan.
\newblock Robustness and accuracy could be reconcilable by (proper) definition.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesv{\'{a}}ri, G.~Niu,
  and S.~Sabato, editors, {\em {ICML}}, 2022.

\bibitem{transform}
H.~Petzka, M.~Kamp, L.~Adilova, C.~Sminchisescu, and M.~Boley.
\newblock Relative flatness and generalization.
\newblock {\em NeurIPS}, 2021.

\bibitem{cite39}
J.~Redmon, S.~K. Divvala, R.~B. Girshick, and A.~Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em {CVPR}}, 2016.

\bibitem{cite38}
S.~Ren, K.~He, R.~B. Girshick, and J.~Sun.
\newblock Faster {R-CNN:} towards real-time object detection with region
  proposal networks.
\newblock In {\em NeurIPS}, 2015.

\bibitem{survey}
M.~Rigaki and S.~Garcia.
\newblock A survey of privacy attacks in machine learning.
\newblock {\em CoRR}, abs/2007.07646, 2020.

\bibitem{ImageNet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~S. Bernstein, A.~C. Berg, and L.~Fei{-}Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em Int. J. Comput. Vis.}, 115(3):211--252, 2015.

\bibitem{Intermediate_layer_ICLR2016}
S.~Sabour, Y.~Cao, F.~Faghri, and D.~J. Fleet.
\newblock Adversarial manipulation of deep representations.
\newblock In Y.~Bengio and Y.~LeCun, editors, {\em {ICLR}}, 2016.

\bibitem{MobileNet}
M.~Sandler, A.~G. Howard, M.~Zhu, A.~Zhmoginov, and L.~Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em {CVPR}}, 2018.

\bibitem{Sehwag2021}
V.~Sehwag, S.~Mahloujifar, T.~Handina, S.~Dai, C.~Xiang, M.~Chiang, and
  P.~Mittal.
\newblock Robust learning meets generative models: Can proxy distributions
  improve adversarial robustness?
\newblock {\em arXiv preprint arXiv:2104.09425}, 2021.

\bibitem{VGG}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Y.~Bengio and Y.~LeCun, editors, {\em {ICLR}}, 2015.

\bibitem{l0}
J.~Su, D.~V. Vargas, and K.~Sakurai.
\newblock One pixel attack for fooling deep neural networks.
\newblock {\em IEEE Transactions on Evolutionary Computation}, 23(5):828--841,
  2019.

\bibitem{Inception}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em {CVPR}}, 2016.

\bibitem{auto}
C.~E. Tuncali, G.~Fainekos, H.~Ito, and J.~Kapinski.
\newblock Simulation-based adversarial test generation for autonomous vehicles
  with machine learning components.
\newblock In {\em 2018 IEEE Intelligent Vehicles Symposium (IV)}. IEEE, 2018.

\bibitem{awp}
D.~Wu, S.-T. Xia, and Y.~Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock {\em NIPS}, 2020.

\bibitem{DI-FSGM}
C.~Xie, Z.~Zhang, Y.~Zhou, S.~Bai, J.~Wang, Z.~Ren, and A.~L. Yuille.
\newblock Improving transferability of adversarial examples with input
  diversity.
\newblock In {\em {CVPR}}, 2019.

\bibitem{detector3}
K.~Yang, T.~Zhou, Y.~Zhang, X.~Tian, and D.~Tao.
\newblock Class-disentanglement and applications in adversarial detection and
  defense.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~N. Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, {\em {NeurIPS }}, 2021.

\bibitem{low_level-first_few_layers}
J.~Yosinski, J.~Clune, Y.~Bengio, and H.~Lipson.
\newblock How transferable are features in deep neural networks?
\newblock In {\em NIPS}, 2014.

\bibitem{WRN}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In R.~C. Wilson, E.~R. Hancock, and W.~A.~P. Smith, editors, {\em
  {BMVC}}, 2016.

\bibitem{black-box-cite}
Q.~Zhang, X.~Li, Y.~Chen, J.~Song, L.~Gao, Y.~He, and H.~Xue'.
\newblock Beyond imagenet attack: Towards crafting adversarial examples for
  black-box domains.
\newblock In {\em {ICLR}}, 2022.

\bibitem{linear_probes}
R.~Zhang, P.~Isola, and A.~A. Efros.
\newblock Split-brain autoencoders: Unsupervised learning by cross-channel
  prediction.
\newblock In {\em {CVPR}}, 2017.

\bibitem{matrix2}
X.~Zhang, J.~Zou, K.~He, and J.~Sun.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 38(10):1943--1955,
  2015.

\bibitem{causal-gang}
Y.~Zhang, M.~Gong, T.~Liu, G.~Niu, X.~Tian, B.~Han, B.~Sch{\"{o}}lkopf, and
  K.~Zhang.
\newblock Adversarial robustness through the lens of causality.
\newblock In {\em {ICLR}}, 2022.

\bibitem{black-gang}
Y.~Zhang, X.~Tian, Y.~Li, X.~Wang, and D.~Tao.
\newblock Principal component adversarial example.
\newblock {\em {IEEE} Trans. Image Process.}, 29:4804--4815, 2020.

\end{thebibliography}
