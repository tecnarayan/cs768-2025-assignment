\providecommand{\noopsort}[1]{}
\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adachi et~al.(2022)Adachi, Hayakawa, J{\o}rgensen, Oberhauser, and
  Osborne]{ada22}
Adachi, M., Hayakawa, S., J{\o}rgensen, M., Oberhauser, H., and Osborne, M.~A.
\newblock Fast {B}ayesian inference with batch {B}ayesian quadrature via kernel
  recombination.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  16533--16547, 2022.

\bibitem[Bach(2017)]{bac17}
Bach, F.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 714--751, 2017.

\bibitem[Bach et~al.(2012)Bach, Lacoste-Julien, and Obozinski]{bac12}
Bach, F., Lacoste-Julien, S., and Obozinski, G.
\newblock On the equivalence between herding and conditional gradient
  algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1355--1362, 2012.

\bibitem[Belhadji(2021)]{bel21}
Belhadji, A.
\newblock An analysis of {E}rmakov--{Z}olotukhin quadrature using kernels.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Belhadji et~al.(2019)Belhadji, Bardenet, and Chainais]{bel19}
Belhadji, A., Bardenet, R., and Chainais, P.
\newblock Kernel quadrature with {DPP}s.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  12907--12917, 2019.

\bibitem[Belhadji et~al.(2020)Belhadji, Bardenet, and Chainais]{bel20}
Belhadji, A., Bardenet, R., and Chainais, P.
\newblock Kernel interpolation with continuous volume sampling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  725--735. PMLR, 2020.

\bibitem[Chatalic et~al.(2022)Chatalic, Schreuder, Rosasco, and Rudi]{cha22}
Chatalic, A., Schreuder, N., Rosasco, L., and Rudi, A.
\newblock Nystr{\"o}m kernel mean embeddings.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3006--3024. PMLR, 2022.

\bibitem[Chen et~al.(2010)Chen, Welling, and Smola]{che10}
Chen, Y., Welling, M., and Smola, A.
\newblock Super-samples from kernel herding.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\
  109--116, 2010.

\bibitem[Cohen \& Migliorati(2017)Cohen and Migliorati]{coh17}
Cohen, A. and Migliorati, G.
\newblock Optimal weighted least-squares methods.
\newblock \emph{The SMAI journal of computational mathematics}, 3:\penalty0
  181--203, 2017.

\bibitem[Cosentino et~al.(2020)Cosentino, Oberhauser, and Abate]{cos20}
Cosentino, F., Oberhauser, H., and Abate, A.
\newblock A randomized algorithm to reduce the support of discrete measures.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  15100--15110, 2020.

\bibitem[De~Marchi(2003)]{dem03}
De~Marchi, S.
\newblock On optimal center locations for radial basis function interpolation:
  computational aspects.
\newblock \emph{Rendiconti del Seminario Matematico}, 61\penalty0 (3):\penalty0
  343--358, 2003.

\bibitem[Drineas et~al.(2005)Drineas, Mahoney, and Cristianini]{dri05}
Drineas, P., Mahoney, M.~W., and Cristianini, N.
\newblock On the {N}ystr{\"o}m method for approximating a {G}ram matrix for
  improved kernel-based learning.
\newblock \emph{The Journal of Machine Learning Research}, 6\penalty0
  (12):\penalty0 2153--2175, 2005.

\bibitem[Dwivedi \& Mackey(2021)Dwivedi and Mackey]{dwi21}
Dwivedi, R. and Mackey, L.
\newblock Kernel thinning.
\newblock In \emph{Conference on Learning Theory}, pp.\  1753--1753. PMLR,
  2021.

\bibitem[Dwivedi \& Mackey(2022)Dwivedi and Mackey]{dwi22}
Dwivedi, R. and Mackey, L.
\newblock Generalized kernel thinning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gauthier(2021)]{gau21}
Gauthier, B.
\newblock Nystr{\"o}m approximation and reproducing kernels: embeddings,
  projections and squared-kernel discrepancy.
\newblock \emph{preprint}, 2021.
\newblock URL \url{https://hal.archives-ouvertes.fr/hal-03207443}.

\bibitem[Gin{\'e} \& Koltchinskii(2006)Gin{\'e} and
  Koltchinskii]{gine2006concentration}
Gin{\'e}, E. and Koltchinskii, V.
\newblock Concentration inequalities and asymptotic results for ratio type
  empirical processes.
\newblock \emph{The Annals of Probability}, 34\penalty0 (3):\penalty0
  1143--1216, 2006.

\bibitem[Gittens \& Mahoney(2016)Gittens and Mahoney]{git16}
Gittens, A. and Mahoney, M.~W.
\newblock Revisiting the {N}ystr{\"o}m method for improved large-scale machine
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 3977--4041, 2016.

\bibitem[Gretton et~al.(2006)Gretton, Borgwardt, Rasch, Sch\"{o}lkopf, and
  Smola]{gre06}
Gretton, A., Borgwardt, K., Rasch, M., Sch\"{o}lkopf, B., and Smola, A.
\newblock A kernel method for the two-sample-problem.
\newblock In Sch\"{o}lkopf, B., Platt, J., and Hoffman, T. (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~19. MIT
  Press, 2006.

\bibitem[Gy{\"o}rfi et~al.(2006)Gy{\"o}rfi, Kohler, Krzyzak, and
  Walk]{gyorfi2006distribution}
Gy{\"o}rfi, L., Kohler, M., Krzyzak, A., and Walk, H.
\newblock \emph{A distribution-free theory of nonparametric regression}.
\newblock Springer, 2006.

\bibitem[Halton(1960)]{hal60}
Halton, J.~H.
\newblock On the efficiency of certain quasi-random sequences of points in
  evaluating multi-dimensional integrals.
\newblock \emph{Numerische Mathematik}, 2\penalty0 (1):\penalty0 84--90, 1960.

\bibitem[Hayakawa \& Suzuki(2020)Hayakawa and Suzuki]{hayakawa-suzuki}
Hayakawa, S. and Suzuki, T.
\newblock On the minimax optimality and superiority of deep neural network
  learning over sparse parameter spaces.
\newblock \emph{Neural Networks}, 123:\penalty0 343--361, 2020.

\bibitem[Hayakawa et~al.(2022)Hayakawa, Oberhauser, and Lyons]{hayakawa21b}
Hayakawa, S., Oberhauser, H., and Lyons, T.
\newblock {\noopsort{a}}{P}ositively weighted kernel quadrature via
  subsampling.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  6886--6900, 2022.

\bibitem[Hayakawa et~al.(2023{\natexlab{a}})Hayakawa, Lyons, and
  Oberhauser]{hayakawa21a}
Hayakawa, S., Lyons, T., and Oberhauser, H.
\newblock Estimating the probability that a given vector is in the convex hull
  of a random sample.
\newblock \emph{Probability Theory and Related Fields}, 185:\penalty0 705--746,
  2023{\natexlab{a}}.

\bibitem[Hayakawa et~al.(2023{\natexlab{b}})Hayakawa, Oberhauser, and
  Lyons]{hayakawa22hyper}
Hayakawa, S., Oberhauser, H., and Lyons, T.
\newblock Hypercontractivity meets random convex hulls: analysis of randomized
  multivariate cubatures.
\newblock \emph{Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 479\penalty0 (2273):\penalty0 20220725,
  2023{\natexlab{b}}.

\bibitem[Husz{\'a}r \& Duvenaud(2012)Husz{\'a}r and Duvenaud]{hus12}
Husz{\'a}r, F. and Duvenaud, D.
\newblock Optimally-weighted herding is {B}ayesian quadrature.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\
  377--386, 2012.

\bibitem[Jin et~al.(2013)Jin, Yang, Mahdavi, Li, and Zhou]{jin13}
Jin, R., Yang, T., Mahdavi, M., Li, Y.-F., and Zhou, Z.-H.
\newblock Improved bounds for the {N}ystr{\"o}m method with application to
  kernel classification.
\newblock \emph{IEEE Transactions on Information Theory}, 59\penalty0
  (10):\penalty0 6939--6949, 2013.

\bibitem[Kanagawa et~al.(2016)Kanagawa, Sriperumbudur, and Fukumizu]{kan16}
Kanagawa, M., Sriperumbudur, B.~K., and Fukumizu, K.
\newblock Convergence guarantees for kernel-based quadrature rules in
  misspecified settings.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29, pp.\  3296--3304, 2016.

\bibitem[Karvonen et~al.(2021)Karvonen, S{\"a}rkk{\"a}, and Tanaka]{kar21}
Karvonen, T., S{\"a}rkk{\"a}, S., and Tanaka, K.
\newblock Kernel-based interpolation at approximate {F}ekete points.
\newblock \emph{Numerical Algorithms}, 87\penalty0 (1):\penalty0 445--468,
  2021.

\bibitem[Koltchinskii(2006)]{koltchinskii2006local}
Koltchinskii, V.
\newblock Local rademacher complexities and oracle inequalities in risk
  minimization.
\newblock \emph{The Annals of Statistics}, 34\penalty0 (6):\penalty0
  2593--2656, 2006.

\bibitem[Kumar et~al.(2012)Kumar, Mohri, and Talwalkar]{kum12}
Kumar, S., Mohri, M., and Talwalkar, A.
\newblock Sampling methods for the {N}ystr{\"o}m method.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 981--1006, 2012.

\bibitem[Li et~al.(2016)Li, Jegelka, and Sra]{li16}
Li, C., Jegelka, S., and Sra, S.
\newblock Fast {DPP} sampling for {N}ystr{\"o}m with application to kernel
  methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2061--2070. PMLR, 2016.

\bibitem[Li et~al.(2015)Li, Bi, Kwok, and Lu]{li15}
Li, M., Bi, W., Kwok, J.~T., and Lu, B.-L.
\newblock Large-scale {N}ystr{\"o}m kernel matrix approximation using
  randomized {SVD}.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  1\penalty0 (26):\penalty0 152--164, 2015.

\bibitem[Litterer \& Lyons(2012)Litterer and Lyons]{lit12}
Litterer, C. and Lyons, T.
\newblock High order recombination and an application to cubature on {W}iener
  space.
\newblock \emph{The Annals of Applied Probability}, 22\penalty0 (4):\penalty0
  1301--1327, 2012.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and Talwalkar]{moh18}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of Machine Learning}.
\newblock Second edition, 2018.

\bibitem[Oglic \& G{\"a}rtner(2017)Oglic and G{\"a}rtner]{ogl17}
Oglic, D. and G{\"a}rtner, T.
\newblock Nystr{\"o}m method with kernel {k}-means++ samples as landmarks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2652--2660. PMLR, 2017.

\bibitem[Owen(2017)]{owe17}
Owen, A.~B.
\newblock A randomized {H}alton algorithm in {R}.
\newblock \emph{arXiv preprint arXiv:1706.02808}, 2017.

\bibitem[Penrose(1955)]{pen55}
Penrose, R.
\newblock A generalized inverse for matrices.
\newblock \emph{Mathematical Proceedings of the Cambridge Philosophical
  Society}, 51\penalty0 (3):\penalty0 406–413, 1955.

\bibitem[Santin \& Haasdonk(2017)Santin and Haasdonk]{san17}
Santin, G. and Haasdonk, B.
\newblock Convergence rate of the data-independent {P}-greedy algorithm in
  kernel-based approximation.
\newblock \emph{Dolomites Research Notes on Approximation}, 10, 2017.

\bibitem[Santin \& Schaback(2016)Santin and Schaback]{san16}
Santin, G. and Schaback, R.
\newblock Approximation of eigenfunctions in kernel-based spaces.
\newblock \emph{Advances in Computational Mathematics}, 42\penalty0
  (4):\penalty0 973--993, 2016.

\bibitem[Schmidt-Hieber(2020)]{sch20}
Schmidt-Hieber, J.
\newblock Nonparametric regression using deep neural networks with {ReLU}
  activation function.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (4):\penalty0
  1875--1897, 2020.

\bibitem[Shetty et~al.(2022)Shetty, Dwivedi, and Mackey]{she22}
Shetty, A., Dwivedi, R., and Mackey, L.
\newblock Distribution compression in near-linear time.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Shinozaki et~al.(1972)Shinozaki, Sibuya, and Tanabe]{shi72}
Shinozaki, N., Sibuya, M., and Tanabe, K.
\newblock Numerical algorithms for the {M}oore--{P}enrose inverse of a matrix:
  iterative methods.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  24\penalty0 (1):\penalty0 621--629, 1972.

\bibitem[Srebro \& Sridharan(2010)Srebro and Sridharan]{sre10+}
Srebro, N. and Sridharan, K.
\newblock Note on refined {D}udley integral covering number bound.
\newblock \emph{Unpublished results}, 2010.
\newblock URL \url{https://www.cs.cornell.edu/~sridharan/dudley.pdf}.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and Tewari]{sre10}
Srebro, N., Sridharan, K., and Tewari, A.
\newblock Smoothness, low noise and fast rates.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~23, 2010.

\bibitem[Steinwart \& Scovel(2012)Steinwart and Scovel]{ste12}
Steinwart, I. and Scovel, C.
\newblock Mercer’s theorem on general domains: On the interaction between
  measures, kernels, and {RKHS}s.
\newblock \emph{Constructive Approximation}, 35\penalty0 (3):\penalty0
  363--417, 2012.

\bibitem[Tchernychova(2016)]{tch15}
Tchernychova, M.
\newblock \emph{Carath{\'e}odory cubature measures}.
\newblock PhD thesis, University of Oxford, 2016.

\bibitem[Tropp et~al.(2017)Tropp, Yurtsever, Udell, and Cevher]{tro17}
Tropp, J.~A., Yurtsever, A., Udell, M., and Cevher, V.
\newblock Fixed-rank approximation of a positive-semidefinite matrix from
  streaming data.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Tsuji et~al.(2022)Tsuji, Tanaka, and Pokutta]{tsu22}
Tsuji, K., Tanaka, K., and Pokutta, S.
\newblock Pairwise conditional gradients without swap steps and sparser kernel
  herding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  21864--21883. PMLR, 2022.

\bibitem[Wahba(1990)]{wah90}
Wahba, G.
\newblock \emph{Spline Models for Observational Data}.
\newblock Society for Industrial and Applied Mathematics, 1990.

\bibitem[Wainwright(2019)]{hds-book}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2019)Wang, Gittens, and Mahoney]{wan19}
Wang, S., Gittens, A., and Mahoney, M.~W.
\newblock Scalable kernel $k$-means clustering with {N}ystr{\"o}m
  approximation: relative-error bounds.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 431--479, 2019.

\bibitem[Yang et~al.(2012)Yang, Li, Mahdavi, Jin, and Zhou]{yan12}
Yang, T., Li, Y.-F., Mahdavi, M., Jin, R., and Zhou, Z.-H.
\newblock Nystr{\"o}m method vs random {F}ourier features: A theoretical and
  empirical comparison.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~25, 2012.

\end{thebibliography}
