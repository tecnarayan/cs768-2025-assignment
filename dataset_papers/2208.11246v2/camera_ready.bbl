\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Rendle et~al.(2012)Rendle, Freudenthaler, Gantner, and
  Schmidt-Thieme]{rendle2012bpr}
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
\newblock Bpr: Bayesian personalized ranking from implicit feedback.
\newblock \emph{arXiv preprint arXiv:1205.2618}, 2012.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Zheng and Lafferty(2016)]{zheng2016convergence}
Qinqing Zheng and John Lafferty.
\newblock Convergence analysis for rectangular matrix completion using
  burer-monteiro factorization and gradient descent.
\newblock \emph{arXiv preprint arXiv:1605.07051}, 2016.

\bibitem[Tong et~al.(2022)Tong, Ma, Prater-Bennette, Tripp, and
  Chi]{tong2022scaling}
Tian Tong, Cong Ma, Ashley Prater-Bennette, Erin Tripp, and Yuejie Chi.
\newblock Scaling and scalability: Provable nonconvex low-rank tensor
  completion.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2607--2617. PMLR, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Fattahi, and Zhang]{zhang2021preconditioned}
Jialun Zhang, Salar Fattahi, and Richard~Y Zhang.
\newblock Preconditioned gradient descent for over-parameterized nonconvex
  matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5985--5996, 2021.

\bibitem[Zhuo et~al.(2021)Zhuo, Kwon, Ho, and Caramanis]{zhuo2021computational}
Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis.
\newblock On the computational and statistical complexity of over-parameterized
  matrix sensing.
\newblock \emph{arXiv preprint arXiv:2102.02756}, 2021.

\bibitem[Kosinski et~al.(2013)Kosinski, Stillwell, and
  Graepel]{kosinski2013private}
Michal Kosinski, David Stillwell, and Thore Graepel.
\newblock Private traits and attributes are predictable from digital records of
  human behavior.
\newblock \emph{Proceedings of the national academy of sciences}, 110\penalty0
  (15):\penalty0 5802--5805, 2013.

\bibitem[Cloninger et~al.(2014)Cloninger, Czaja, Bai, and
  Basser]{cloninger2014solving}
Alexander Cloninger, Wojciech Czaja, Ruiliang Bai, and Peter~J Basser.
\newblock Solving 2d fredholm integral from incomplete measurements using
  compressive sensing.
\newblock \emph{SIAM journal on imaging sciences}, 7\penalty0 (3):\penalty0
  1775--1798, 2014.

\bibitem[Tong et~al.(2021{\natexlab{a}})Tong, Ma, and
  Chi]{tong2021accelerating}
Tian Tong, Cong Ma, and Yuejie Chi.
\newblock Accelerating ill-conditioned low-rank matrix estimation via scaled
  gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (150):\penalty0 1--63, 2021{\natexlab{a}}.

\bibitem[Jin et~al.(2016)Jin, Kakade, and Netrapalli]{jin2016provable}
Chi Jin, Sham~M Kakade, and Praneeth Netrapalli.
\newblock Provable efficient online matrix completion via non-convex stochastic
  gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Candes and Plan(2010)]{candes2010matrix}
Emmanuel~J Candes and Yaniv Plan.
\newblock Matrix completion with noise.
\newblock \emph{Proceedings of the IEEE}, 98\penalty0 (6):\penalty0 925--936,
  2010.

\bibitem[Cand{\`e}s and Tao(2010)]{candes2010power}
Emmanuel~J Cand{\`e}s and Terence Tao.
\newblock The power of convex relaxation: Near-optimal matrix completion.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (5):\penalty0 2053--2080, 2010.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Benjamin Recht, Maryam Fazel, and Pablo~A Parrilo.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Srebro and Shraibman(2005)]{srebro2005rank}
Nathan Srebro and Adi Shraibman.
\newblock Rank, trace-norm and max-norm.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 545--560. Springer, 2005.

\bibitem[Negahban and Wainwright(2012)]{negahban2012restricted}
Sahand Negahban and Martin~J Wainwright.
\newblock Restricted strong convexity and weighted matrix completion: Optimal
  bounds with noise.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 1665--1697, 2012.

\bibitem[Burer and Monteiro(2003)]{burer2003nonlinear}
Samuel Burer and Renato~DC Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock \emph{Mathematical Programming}, 95\penalty0 (2):\penalty0 329--357,
  2003.

\bibitem[Jain et~al.(2013)Jain, Netrapalli, and Sanghavi]{jain2013low}
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.
\newblock Low-rank matrix completion using alternating minimization.
\newblock In \emph{Proceedings of the forty-fifth annual ACM symposium on
  Theory of computing}, pages 665--674, 2013.

\bibitem[Hardt and Wootters(2014)]{hardt2014fast}
Moritz Hardt and Mary Wootters.
\newblock Fast matrix completion without the condition number.
\newblock In \emph{Conference on learning theory}, pages 638--678. PMLR, 2014.

\bibitem[Hardt(2014)]{hardt2014understanding}
Moritz Hardt.
\newblock Understanding alternating minimization for matrix completion.
\newblock In \emph{2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, pages 651--660. IEEE, 2014.

\bibitem[Sun and Luo(2016)]{sun2016guaranteed}
Ruoyu Sun and Zhi-Quan Luo.
\newblock Guaranteed matrix completion via non-convex factorization.
\newblock \emph{IEEE Transactions on Information Theory}, 62\penalty0
  (11):\penalty0 6535--6579, 2016.

\bibitem[Chen and Wainwright(2015)]{chen2015fast}
Yudong Chen and Martin~J Wainwright.
\newblock Fast low-rank estimation by projected gradient descent: General
  statistical and algorithmic guarantees.
\newblock \emph{arXiv preprint arXiv:1509.03025}, 2015.

\bibitem[Jain and Netrapalli(2015)]{jain2015fast}
Prateek Jain and Praneeth Netrapalli.
\newblock Fast exact matrix completion with finite samples.
\newblock In \emph{Conference on Learning Theory}, pages 1007--1034. PMLR,
  2015.

\bibitem[Tu et~al.(2016)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2016low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock In \emph{International Conference on Machine Learning}, pages
  964--973. PMLR, 2016.

\bibitem[Bhojanapalli et~al.(2016{\natexlab{a}})Bhojanapalli, Kyrillidis, and
  Sanghavi]{bhojanapalli2016dropping}
Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi.
\newblock Dropping convexity for faster semi-definite optimization.
\newblock In \emph{Conference on Learning Theory}, pages 530--582. PMLR,
  2016{\natexlab{a}}.

\bibitem[Candes et~al.(2015)Candes, Li, and Soltanolkotabi]{candes2015phase}
Emmanuel~J Candes, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (4):\penalty0 1985--2007, 2015.

\bibitem[Ma and Fattahi(2021)]{ma2021implicit}
Jianhao Ma and Salar Fattahi.
\newblock Implicit regularization of sub-gradient method in robust matrix
  recovery: Don't be afraid of outliers.
\newblock \emph{arXiv preprint arXiv:2102.02969}, 2021.

\bibitem[Bhojanapalli et~al.(2016{\natexlab{b}})Bhojanapalli, Neyshabur, and
  Srebro]{bhojanapalli2016global}
Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock \emph{arXiv preprint arXiv:1605.07221}, 2016{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Zhu, and Tang]{li2019non}
Qiuwei Li, Zhihui Zhu, and Gongguo Tang.
\newblock The non-convex geometry of low-rank matrix optimization.
\newblock \emph{Information and Inference: A Journal of the IMA}, 8\penalty0
  (1):\penalty0 51--96, 2019.

\bibitem[Sun et~al.(2018)Sun, Qu, and Wright]{sun2018geometric}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock \emph{Foundations of Computational Mathematics}, 18\penalty0
  (5):\penalty0 1131--1198, 2018.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock \emph{arXiv preprint arXiv:1605.07272}, 2016.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In \emph{International Conference on Machine Learning}, pages
  1233--1242. PMLR, 2017.

\bibitem[Chen and Li(2017)]{chen2017memory}
Ji~Chen and Xiaodong Li.
\newblock Memory-efficient kernel pca via partial matrix sampling and nonconvex
  optimization: a model-free analysis of local minima.
\newblock \emph{arXiv preprint arXiv:1711.01742}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Sojoudi, and Lavaei]{zhang2019sharp}
Richard~Y Zhang, Somayeh Sojoudi, and Javad Lavaei.
\newblock Sharp restricted isometry bounds for the inexistence of spurious
  local minima in nonconvex matrix recovery.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (114):\penalty0 1--34, 2019.

\bibitem[Zhang(2021)]{zhang2021sharp}
Richard~Y Zhang.
\newblock Sharp global guarantees for nonconvex low-rank matrix recovery in the
  overparameterized regime.
\newblock \emph{arXiv preprint arXiv:2104.10790}, 2021.

\bibitem[Josz and Lai(2021)]{josz2021nonsmooth}
C{\'e}dric Josz and Lexiao Lai.
\newblock Nonsmooth rank-one matrix factorization landscape.
\newblock \emph{Optimization Letters}, pages 1--21, 2021.

\bibitem[Bassily et~al.(2018)Bassily, Belkin, and Ma]{bassily2018exponential}
Raef Bassily, Mikhail Belkin, and Siyuan Ma.
\newblock On exponential convergence of sgd in non-convex over-parametrized
  learning.
\newblock \emph{arXiv preprint arXiv:1811.02564}, 2018.

\bibitem[Vaswani et~al.(2019)Vaswani, Bach, and Schmidt]{vaswani2019fast}
Sharan Vaswani, Francis Bach, and Mark Schmidt.
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1195--1204. PMLR, 2019.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richt{\'a}rik.
\newblock Sgd: General analysis and improved rates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5200--5209. PMLR, 2019.

\bibitem[Xie et~al.(2020)Xie, Wu, and Ward]{xie2020linear}
Yuege Xie, Xiaoxia Wu, and Rachel Ward.
\newblock Linear convergence of adaptive stochastic gradient descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1475--1485. PMLR, 2020.

\bibitem[Tong et~al.(2021{\natexlab{b}})Tong, Ma, and Chi]{tong2021low}
Tian Tong, Cong Ma, and Yuejie Chi.
\newblock Low-rank matrix recovery with scaled subgradient methods: Fast and
  robust convergence without the condition number.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0
  2396--2409, 2021{\natexlab{b}}.

\bibitem[K{\"u}mmerle and Verdun(2021)]{kummerle2021scalable}
Christian K{\"u}mmerle and Claudio~M Verdun.
\newblock A scalable second order method for ill-conditioned matrix completion
  from few samples.
\newblock In \emph{International Conference on Machine Learning}, pages
  5872--5883. PMLR, 2021.

\bibitem[Cand{\`e}s and Recht(2009)]{candes2009exact}
Emmanuel~J Cand{\`e}s and Benjamin Recht.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational mathematics}, 9\penalty0
  (6):\penalty0 717--772, 2009.

\bibitem[Recht(2011)]{recht2011simpler}
Benjamin Recht.
\newblock A simpler approach to matrix completion.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (12), 2011.

\bibitem[Zheng and Lafferty(2015)]{zheng2015convergent}
Qinqing Zheng and John Lafferty.
\newblock A convergent gradient descent algorithm for rank minimization and
  semidefinite programming from random linear measurements.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Chi et~al.(2019)Chi, Lu, and Chen]{chi2019nonconvex}
Yuejie Chi, Yue~M Lu, and Yuxin Chen.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (20):\penalty0 5239--5269, 2019.

\bibitem[Ma et~al.(2018)Ma, Wang, Chi, and Chen]{ma2018implicit}
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval and matrix completion.
\newblock In \emph{International Conference on Machine Learning}, pages
  3345--3354. PMLR, 2018.

\bibitem[Lee et~al.(2019)Lee, Panageas, Piliouras, Simchowitz, Jordan, and
  Recht]{lee2019first}
Jason~D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael~I
  Jordan, and Benjamin Recht.
\newblock First-order methods almost always avoid strict saddle points.
\newblock \emph{Mathematical programming}, 176\penalty0 (1):\penalty0 311--337,
  2019.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and
  Jordan]{jin2021nonconvex}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M Kakade, and Michael~I Jordan.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (2):\penalty0 1--29,
  2021.

\bibitem[Harper and Konstan(2015)]{movielens}
F.~Maxwell Harper and Joseph~A. Konstan.
\newblock The movielens datasets: History and context.
\newblock \emph{ACM Trans. Interact. Intell. Syst.}, 5\penalty0 (4), dec 2015.
\newblock ISSN 2160-6455.
\newblock \doi{10.1145/2827872}.
\newblock URL \url{https://doi.org/10.1145/2827872}.

\bibitem[Dokmanic et~al.(2015)Dokmanic, Parhizkar, Ranieri, and
  Vetterli]{dokmanic2015euclidean}
Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli.
\newblock Euclidean distance matrices: essential theory, algorithms, and
  applications.
\newblock \emph{IEEE Signal Processing Magazine}, 32\penalty0 (6):\penalty0
  12--30, 2015.

\bibitem[Davidson et~al.(2010)Davidson, Liebald, Liu, Nandy, Van~Vleet, Gargi,
  Gupta, He, Lambert, Livingston, et~al.]{davidson2010youtube}
James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van~Vleet,
  Ullas Gargi, Sujoy Gupta, Yu~He, Mike Lambert, Blake Livingston, et~al.
\newblock The youtube video recommendation system.
\newblock In \emph{Proceedings of the fourth ACM conference on Recommender
  systems}, pages 293--296, 2010.

\bibitem[Linden et~al.(2003)Linden, Smith, and York]{linden2003amazon}
Greg Linden, Brent Smith, and Jeremy York.
\newblock Amazon. com recommendations: Item-to-item collaborative filtering.
\newblock \emph{IEEE Internet computing}, 7\penalty0 (1):\penalty0 76--80,
  2003.

\bibitem[Smith and Linden(2017)]{smith2017two}
Brent Smith and Greg Linden.
\newblock Two decades of recommender systems at amazon. com.
\newblock \emph{Ieee internet computing}, 21\penalty0 (3):\penalty0 12--18,
  2017.

\bibitem[Davenport et~al.(2014)Davenport, Plan, Van Den~Berg, and
  Wootters]{davenport20141}
Mark~A Davenport, Yaniv Plan, Ewout Van Den~Berg, and Mary Wootters.
\newblock 1-bit matrix completion.
\newblock \emph{Information and Inference: A Journal of the IMA}, 3\penalty0
  (3):\penalty0 189--223, 2014.

\end{thebibliography}
