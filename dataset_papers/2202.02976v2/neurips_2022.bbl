\begin{thebibliography}{45}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Ba and Caruana(2014)}]{ba2014deep}
Lei~Jimmy Ba and Rich Caruana. 2014.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Proceedings of the 27th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 2654--2662.

\bibitem[{Biesialska et~al.(2020)Biesialska, Biesialska, and
  Costa-juss{\`a}}]{biesialska-etal-2020-continual}
Magdalena Biesialska, Katarzyna Biesialska, and Marta~R. Costa-juss{\`a}. 2020.
\newblock Continual lifelong learning in natural language processing: A survey.
\newblock In \emph{Proceedings of the 28th International Conference on
  Computational Linguistics}, pages 6523--6541.

\bibitem[{Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil}]{bucilua2006model}
Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil. 2006.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541.

\bibitem[{Chen and Liu(2018)}]{chen2018lifelong}
Zhiyuan Chen and Bing Liu. 2018.
\newblock Lifelong machine learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 12(3):1--207.

\bibitem[{Collins and Koo(2005)}]{collins2005discriminative}
Michael Collins and Terry Koo. 2005.
\newblock Discriminative reranking for natural language parsing.
\newblock \emph{Computational Linguistics}, 31(1):25--70.

\bibitem[{Do and Rehbein(2020)}]{do-rehbein-2020-neural}
Bich-Ngoc Do and Ines Rehbein. 2020.
\newblock Neural reranking for dependency parsing: An evaluation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 4123--4133.

\bibitem[{Dozat and Manning(2017)}]{dozat2016deep}
Timothy Dozat and Christopher~D. Manning. 2017.
\newblock Deep biaffine attention for neural dependency parsing.
\newblock In \emph{International Conference on Learning Representations, 2017}.

\bibitem[{Einolghozati et~al.(2019)Einolghozati, Pasupat, Gupta, Shah, Mohit,
  Lewis, and Zettlemoyer}]{einolghozati2019improving}
Arash Einolghozati, Panupong Pasupat, Sonal Gupta, Rushin Shah, Mrinal Mohit,
  Mike Lewis, and Luke Zettlemoyer. 2019.
\newblock Improving semantic parsing for task oriented dialog.
\newblock \emph{arXiv preprint arXiv:1902.06000}.

\bibitem[{Falke et~al.(2019)Falke, Ribeiro, Utama, Dagan, and
  Gurevych}]{falke-etal-2019-ranking}
Tobias Falke, Leonardo F.~R. Ribeiro, Prasetya~Ajie Utama, Ido Dagan, and Iryna
  Gurevych. 2019.
\newblock Ranking generated summaries by correctness: An interesting but
  challenging application for natural language inference.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2214--2220.

\bibitem[{Fan et~al.(2018)Fan, Lewis, and Dauphin}]{fan-etal-2018-hierarchical}
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
\newblock Hierarchical neural story generation.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 889--898.

\bibitem[{Gepperth and Hammer(2016)}]{gepperth2016incremental}
Alexander Gepperth and Barbara Hammer. 2016.
\newblock Incremental learning algorithms and applications.
\newblock In \emph{European symposium on artificial neural networks}.

\bibitem[{Geyik et~al.(2019)Geyik, Ambler, and Kenthapadi}]{geyik2019fairness}
Sahin~Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. 2019.
\newblock Fairness-aware ranking in search \& recommendation systems with
  application to linkedin talent search.
\newblock In \emph{Proceedings of the 25th acm sigkdd international conference
  on knowledge discovery \& data mining}, pages 2221--2231.

\bibitem[{Gupta et~al.(2018)Gupta, Shah, Mohit, Kumar, and
  Lewis}]{gupta-etal-2018-semantic-parsing}
Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and Mike Lewis. 2018.
\newblock Semantic parsing for task oriented dialog using hierarchical
  representations.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2787--2792.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem[{Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi}]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 2019.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations, 2019}.

\bibitem[{Kim and Rush(2016)}]{kim-rush-2016-sequence}
Yoon Kim and Alexander~M. Rush. 2016.
\newblock Sequence-level knowledge distillation.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 1317--1327.

\bibitem[{Kriz et~al.(2019)Kriz, Sedoc, Apidianaki, Zheng, Kumar, Miltsakaki,
  and Callison-Burch}]{kriz-etal-2019-complexity}
Reno Kriz, Jo{\~a}o Sedoc, Marianna Apidianaki, Carolina Zheng, Gaurav Kumar,
  Eleni Miltsakaki, and Chris Callison-Burch. 2019.
\newblock Complexity-weighted loss and diverse reranking for sentence
  simplification.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 3137--3147.

\bibitem[{K{\"u}bler et~al.(2009)K{\"u}bler, McDonald, and
  Nivre}]{kubler2009dependency}
Sandra K{\"u}bler, Ryan McDonald, and Joakim Nivre. 2009.
\newblock Dependency parsing.
\newblock \emph{Synthesis lectures on human language technologies},
  1(1):1--127.

\bibitem[{Le and Zuidema(2014)}]{le-zuidema-2014-inside}
Phong Le and Willem Zuidema. 2014.
\newblock The inside-outside recursive neural network model for dependency
  parsing.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pages 729--739.

\bibitem[{Li et~al.(2016)Li, Galley, Brockett, Gao, and
  Dolan}]{li-etal-2016-diversity}
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016.
\newblock A diversity-promoting objective function for neural conversation
  models.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 110--119.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Ma et~al.(2018)Ma, Hu, Liu, Peng, Neubig, and Hovy}]{ma2018stack}
Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng, Graham Neubig, and Eduard
  Hovy. 2018.
\newblock Stack-pointer networks for dependency parsing.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1403--1414.

\bibitem[{Masana et~al.(2020)Masana, Liu, Twardowski, Menta, Bagdanov, and
  Weijer}]{Masana2020ClassincrementalLS}
Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew~D.
  Bagdanov, and J.~Weijer. 2020.
\newblock Class-incremental learning: survey and performance evaluation.
\newblock \emph{ArXiv}, abs/2010.15277.

\bibitem[{Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli}]{ott-etal-2019-fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli. 2019.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages 48--53.

\bibitem[{Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter}]{Parisi2019ContinualLL}
G.~I. Parisi, Ronald Kemker, Jose~L. Part, Christopher Kanan, and S.~Wermter.
  2019.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural networks : the official journal of the International
  Neural Network Society}, 113:54--71.

\bibitem[{Petrov et~al.(2012)Petrov, Das, and
  McDonald}]{petrov-etal-2012-universal}
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
\newblock A universal part-of-speech tagset.
\newblock In \emph{Proceedings of the Eighth International Conference on
  Language Resources and Evaluation}, pages 2089--2096.

\bibitem[{Polikar et~al.(2001)Polikar, Upda, Upda, and
  Honavar}]{polikar2001learn++}
Robi Polikar, Lalita Upda, Satish~S Upda, and Vasant Honavar. 2001.
\newblock Learn++: An incremental learning algorithm for supervised neural
  networks.
\newblock \emph{IEEE transactions on systems, man, and cybernetics, part C
  (applications and reviews)}, 31(4):497--508.

\bibitem[{Qi et~al.(2020)Qi, Zhang, Zhang, Bolton, and
  Manning}]{qi-etal-2020-stanza}
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher~D. Manning.
  2020.
\newblock {S}tanza: A python natural language processing toolkit for many human
  languages.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}, pages 101--108.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8):9.

\bibitem[{Rongali et~al.(2020)Rongali, Soldaini, Monti, and
  Hamza}]{rongali2020don}
Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza. 2020.
\newblock Don’t parse, generate! a sequence to sequence architecture for
  task-oriented semantic parsing.
\newblock In \emph{Proceedings of The Web Conference 2020}, pages 2962--2968.

\bibitem[{Salazar et~al.(2019)Salazar, Liang, Nguyen, and
  Kirchhoff}]{salazar2019masked}
Julian Salazar, Davis Liang, Toan~Q Nguyen, and Katrin Kirchhoff. 2019.
\newblock Masked language model scoring.
\newblock \emph{arXiv preprint arXiv:1910.14659}.

\bibitem[{Shen et~al.(2004)Shen, Sarkar, and Och}]{shen2004discriminative}
Libin Shen, Anoop Sarkar, and Franz~Josef Och. 2004.
\newblock Discriminative reranking for machine translation.
\newblock In \emph{Proceedings of the Human Language Technology Conference of
  the North American Chapter of the Association for Computational Linguistics:
  HLT-NAACL 2004}, pages 177--184.

\bibitem[{Shen et~al.(2020)Shen, Xiong, Xia, and Soatto}]{shen2020towards}
Yantao Shen, Yuanjun Xiong, Wei Xia, and Stefano Soatto. 2020.
\newblock Towards backward-compatible representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6368--6377.

\bibitem[{Smith(2011)}]{smith2011linguistic}
Noah~A Smith. 2011.
\newblock Linguistic structure prediction.
\newblock \emph{Synthesis lectures on human language technologies},
  4(2):1--274.

\bibitem[{Socher et~al.(2013)Socher, Bauer, Manning, and
  Ng}]{socher-etal-2013-parsing}
Richard Socher, John Bauer, Christopher~D. Manning, and Andrew~Y. Ng. 2013.
\newblock Parsing with compositional vector grammars.
\newblock In \emph{Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 455--465.

\bibitem[{Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov}]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov. 2014.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15(1):1929--1958.

\bibitem[{Tr{\"{a}}uble et~al.(2021)Tr{\"{a}}uble, von K{\"{u}}gelgen,
  Kleindessner, Locatello, Sch{\"{o}}lkopf, and
  Gehler}]{trauble2021backwardcompatible}
Frederik Tr{\"{a}}uble, Julius von K{\"{u}}gelgen, Matth{\"{a}}us Kleindessner,
  Francesco Locatello, Bernhard Sch{\"{o}}lkopf, and Peter~V. Gehler. 2021.
\newblock Backward-compatible prediction updates: {A} probabilistic approach.
\newblock \emph{arXiv preprint arXiv:2107.01057}.

\bibitem[{Wang et~al.(2021)Wang, Jiang, Yan, Jia, Bach, Wang, Huang, Huang, and
  Tu}]{wang-etal-2021-structural}
Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang,
  Zhongqiang Huang, Fei Huang, and Kewei Tu. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.46} {Structural
  knowledge distillation: Tractably distilling information for structured
  predictor}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 550--564, Online.
  Association for Computational Linguistics.

\bibitem[{Xie et~al.(2021)Xie, Lai, Xiong, Zhang, and
  Soatto}]{xie-etal-2021-regression}
Yuqing Xie, Yi-An Lai, Yuanjun Xiong, Yi~Zhang, and Stefano Soatto. 2021.
\newblock Regression bugs are in your model! measuring, reducing and analyzing
  regressions in {NLP} model updates.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 6589--6602.

\bibitem[{Yan et~al.(2021)Yan, Xiong, Kundu, Yang, Deng, Wang, Xia, and
  Soatto}]{yan2021positive}
Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng Wang, Wei
  Xia, and Stefano Soatto. 2021.
\newblock Positive-congruent training: Towards regression-free model updates.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 14299--14308.

\bibitem[{Yee et~al.(2019)Yee, Dauphin, and Auli}]{yee-etal-2019-simple}
Kyra Yee, Yann Dauphin, and Michael Auli. 2019.
\newblock Simple and effective noisy channel modeling for neural machine
  translation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 5696--5701.

\bibitem[{Yin and Neubig(2019)}]{yin-neubig-2019-reranking}
Pengcheng Yin and Graham Neubig. 2019.
\newblock Reranking for neural semantic parsing.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 4553--4559.

\bibitem[{Zmigrod et~al.(2020)Zmigrod, Vieira, and
  Cotterell}]{zmigrod-etal-2020-please}
Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2020.
\newblock Please mind the root: {D}ecoding arborescences for dependency
  parsing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4809--4819.

\bibitem[{Zmigrod et~al.(2021{\natexlab{a}})Zmigrod, Vieira, and
  Cotterell}]{zmigrod-etal-2021-efficient-computation}
Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2021{\natexlab{a}}.
\newblock \href {https://doi.org/10.1162/tacl_a_00391} {Efficient computation
  of expectations under spanning tree distributions}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:675--690.

\bibitem[{Zmigrod et~al.(2021{\natexlab{b}})Zmigrod, Vieira, and
  Cotterell}]{zmigrod-etal-2021-finding}
Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2021{\natexlab{b}}.
\newblock On finding the k-best non-projective dependency trees.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1324--1337.

\end{thebibliography}
