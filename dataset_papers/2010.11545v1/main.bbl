\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Adel \bgroup \em et al.\egroup
  }{2019}]{adel2019continual}
Tameem Adel, Han Zhao, and Richard~E Turner.
\newblock Continual learning with adaptive weights (claw).
\newblock {\em arXiv preprint arXiv:1911.09514}, 2019.

\bibitem[\protect\citeauthoryear{Al-Shedivat \bgroup \em et al.\egroup
  }{2017}]{al2017continuous}
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch,
  and Pieter Abbeel.
\newblock Continuous adaptation via meta-learning in nonstationary and
  competitive environments.
\newblock {\em arXiv preprint arXiv:1710.03641}, 2017.

\bibitem[\protect\citeauthoryear{Alet \bgroup \em et al.\egroup
  }{2018}]{alet2018modular}
Ferran Alet, Tomas Lozano-Perez, and Leslie~P Kaelbling.
\newblock Modular meta-learning.
\newblock In {\em Conference on Robot Learning}, pages 856--868, 2018.

\bibitem[\protect\citeauthoryear{Chaudhry \bgroup \em et al.\egroup
  }{2018}]{chaudhry2018riemannian}
Arslan Chaudhry, Puneet~K Dokania, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 532--547, 2018.

\bibitem[\protect\citeauthoryear{Chaudhry \bgroup \em et al.\egroup
  }{2019}]{chaudhry2018efficient}
Arslan Chaudhry, Marcâ€™Aurelio Ranzato, Marcus Rohrbach, and Mohamed
  Elhoseiny.
\newblock Efficient lifelong learning with a-{GEM}.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[\protect\citeauthoryear{Fernando \bgroup \em et al.\egroup
  }{2017}]{fernando2017pathnet}
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha,
  Andrei~A Rusu, Alexander Pritzel, and Daan Wierstra.
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock {\em arXiv preprint arXiv:1701.08734}, 2017.

\bibitem[\protect\citeauthoryear{Finn and Levine}{2017}]{finn2017meta}
Chelsea Finn and Sergey Levine.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock {\em arXiv preprint arXiv:1710.11622}, 2017.

\bibitem[\protect\citeauthoryear{Finn \bgroup \em et al.\egroup
  }{2017}]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem[\protect\citeauthoryear{Finn \bgroup \em et al.\egroup
  }{2018}]{finn2018probabilistic}
Chelsea Finn, Kelvin Xu, and Sergey Levine.
\newblock Probabilistic model-agnostic meta-learning.
\newblock {\em arXiv preprint arXiv:1806.02817}, 2018.

\bibitem[\protect\citeauthoryear{Finn \bgroup \em et al.\egroup
  }{2019}]{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1920--1930, 2019.

\bibitem[\protect\citeauthoryear{Flennerhag \bgroup \em et al.\egroup
  }{2019}]{flennerhag2019meta}
Sebastian Flennerhag, Andrei~A Rusu, Razvan Pascanu, Hujun Yin, and Raia
  Hadsell.
\newblock Meta-learning with warped gradient descent.
\newblock {\em arXiv preprint arXiv:1909.00025}, 2019.

\bibitem[\protect\citeauthoryear{Garcia and Bruna}{2017}]{garcia2017few}
Victor Garcia and Joan Bruna.
\newblock Few-shot learning with graph neural networks.
\newblock {\em arXiv preprint arXiv:1711.04043}, 2017.

\bibitem[\protect\citeauthoryear{Grant \bgroup \em et al.\egroup
  }{2018}]{grant2018recasting}
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock {\em arXiv preprint arXiv:1801.08930}, 2018.

\bibitem[\protect\citeauthoryear{Gu \bgroup \em et al.\egroup
  }{2018}]{gu2018meta}
Jiatao Gu, Yong Wang, Yun Chen, Victor~OK Li, and Kyunghyun Cho.
\newblock Meta-learning for low-resource neural machine translation.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 3622--3631, 2018.

\bibitem[\protect\citeauthoryear{Jerfel \bgroup \em et al.\egroup
  }{2019}]{jerfel2019reconciling}
Ghassen Jerfel, Erin Grant, Tom Griffiths, and Katherine~A Heller.
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9119--9130, 2019.

\bibitem[\protect\citeauthoryear{Kirkpatrick \bgroup \em et al.\egroup
  }{2017}]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526, 2017.

\bibitem[\protect\citeauthoryear{Lee and Choi}{2018}]{lee2018gradient}
Yoonho Lee and Seungjin Choi.
\newblock Gradient-based meta-learning with learned layerwise metric and
  subspace.
\newblock In {\em International Conference on Machine Learning}, pages
  2927--2936, 2018.

\bibitem[\protect\citeauthoryear{Lee \bgroup \em et al.\egroup
  }{2017}]{lee2017lifelong}
Jeongtae Lee, Jaehong Yun, Sungju Hwang, and Eunho Yang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock {\em arXiv preprint arXiv:1708.01547}, 2017.

\bibitem[\protect\citeauthoryear{Li \bgroup \em et al.\egroup
  }{2017}]{li2017meta}
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li.
\newblock Meta-sgd: Learning to learn quickly for few shot learning.
\newblock {\em arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[\protect\citeauthoryear{Li \bgroup \em et al.\egroup
  }{2019}]{li2019learn}
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong.
\newblock Learn to grow: A continual structure learning framework for
  overcoming catastrophic forgetting.
\newblock {\em arXiv preprint arXiv:1904.00310}, 2019.

\bibitem[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup
  }{2018a}]{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock {\em arXiv preprint arXiv:1806.09055}, 2018.

\bibitem[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup
  }{2018b}]{liu2018transductive}
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi~Yang.
\newblock Transductive propagation network for few-shot learning.
\newblock {\em arXiv preprint arXiv:1805.10002}, 2018.

\bibitem[\protect\citeauthoryear{Lopez-Paz and
  Ranzato}{2017}]{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem[\protect\citeauthoryear{Mishra \bgroup \em et al.\egroup
  }{2017}]{mishra2018simple}
Nikhil Mishra, Mostafa Rohaninejad, Xi~Chen, and Pieter Abbeel.
\newblock A simple neural attentive meta-learner.
\newblock {\em arXiv preprint arXiv:1707.03141}, 2017.

\bibitem[\protect\citeauthoryear{Nagabandi \bgroup \em et al.\egroup
  }{2018}]{nagabandi2018deep}
Anusha Nagabandi, Chelsea Finn, and Sergey Levine.
\newblock Deep online learning via meta-learning: Continual adaptation for
  model-based rl.
\newblock {\em arXiv preprint arXiv:1812.07671}, 2018.

\bibitem[\protect\citeauthoryear{Nichol and Schulman}{2018}]{nichol2018reptile}
Alex Nichol and John Schulman.
\newblock Reptile: a scalable metalearning algorithm.
\newblock {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[\protect\citeauthoryear{Oreshkin \bgroup \em et al.\egroup
  }{2018}]{oreshkin2018tadam}
Boris Oreshkin, Pau~Rodr{\'\i}guez L{\'o}pez, and Alexandre Lacoste.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  721--731, 2018.

\bibitem[\protect\citeauthoryear{Rajeswaran \bgroup \em et al.\egroup
  }{2019}]{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  113--124, 2019.

\bibitem[\protect\citeauthoryear{Rebuffi \bgroup \em et al.\egroup
  }{2017}]{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem[\protect\citeauthoryear{Rusu \bgroup \em et al.\egroup
  }{2016}]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[\protect\citeauthoryear{Rusu \bgroup \em et al.\egroup
  }{2018}]{rusu2018meta}
Andrei~A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,
  Simon Osindero, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock {\em arXiv preprint arXiv:1807.05960}, 2018.

\bibitem[\protect\citeauthoryear{Schwarz \bgroup \em et al.\egroup
  }{2018}]{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4528--4537, 2018.

\bibitem[\protect\citeauthoryear{Shin \bgroup \em et al.\egroup
  }{2017}]{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2990--2999, 2017.

\bibitem[\protect\citeauthoryear{Snell \bgroup \em et al.\egroup
  }{2017}]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In {\em Advances in neural information processing systems}, pages
  4077--4087, 2017.

\bibitem[\protect\citeauthoryear{Sung \bgroup \em et al.\egroup
  }{2018}]{yang2018learning}
Flood Sung, Yongxin Yang, Li~Zhang, Tao Xiang, Philip~HS Torr, and Timothy~M
  Hospedales.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1199--1208, 2018.

\bibitem[\protect\citeauthoryear{Tan \bgroup \em et al.\egroup
  }{2019}]{tan2019out}
Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang, and
  Mo~Yu.
\newblock Out-of-domain detection for low-resource text classification tasks.
\newblock {\em arXiv preprint arXiv:1909.05357}, 2019.

\bibitem[\protect\citeauthoryear{Triantafillou \bgroup \em et al.\egroup
  }{2019}]{triantafillou2019meta}
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu,
  Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and
  Hugo Larochelle.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few
  examples.
\newblock {\em arXiv preprint arXiv:1903.03096}, 2019.

\bibitem[\protect\citeauthoryear{Vinyals \bgroup \em et al.\egroup
  }{2016}]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In {\em Advances in neural information processing systems}, pages
  3630--3638, 2016.

\bibitem[\protect\citeauthoryear{Vuorio \bgroup \em et al.\egroup
  }{2019}]{vuorio2019multimodal}
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph~J Lim.
\newblock Multimodal model-agnostic meta-learning via task-aware modulation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--12, 2019.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup
  }{2020}]{wang2020frustratingly}
Xin Wang, Thomas~E Huang, Trevor Darrell, Joseph~E Gonzalez, and Fisher Yu.
\newblock Frustratingly simple few-shot object detection.
\newblock {\em arXiv preprint arXiv:2003.06957}, 2020.

\bibitem[\protect\citeauthoryear{Yao \bgroup \em et al.\egroup
  }{2019}]{yao2019hierarchically}
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li.
\newblock Hierarchically structured meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7045--7054, 2019.

\bibitem[\protect\citeauthoryear{Yao \bgroup \em et al.\egroup
  }{2020}]{yao2020automated}
Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, and
  Zhenhui Li.
\newblock Automated relational meta-learning.
\newblock {\em arXiv preprint arXiv:2001.00745}, 2020.

\bibitem[\protect\citeauthoryear{Yoon \bgroup \em et al.\egroup
  }{2019}]{yoon2019tapnet}
Sung~Whan Yoon, Jun Seo, and Jaekyun Moon.
\newblock Tapnet: Neural network augmented with task-adaptive projection for
  few-shot learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7115--7123, 2019.

\bibitem[\protect\citeauthoryear{Zenke \bgroup \em et al.\egroup
  }{2017}]{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3987--3995. JMLR. org, 2017.

\end{thebibliography}
