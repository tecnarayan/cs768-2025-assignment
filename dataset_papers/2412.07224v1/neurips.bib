
### Generalization
@article{fort2019stiffness,
  title={Stiffness: A new perspective on generalization in neural networks},
  author={Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Jastrzebski, Stanislaw and Narayanan, Srini},
  journal={arXiv preprint arXiv:1901.09491},
  year={2019}
}



### Global loss landscape
@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}

@article{fort2019large,
  title={Large scale structure of neural network loss landscapes},
  author={Fort, Stanislav and Jastrzebski, Stanislaw},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{lucas2021analyzing,
  title={Analyzing monotonic linear interpolation in neural network loss landscapes},
  author={Lucas, James and Bae, Juhan and Zhang, Michael R and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  journal={arXiv preprint arXiv:2104.11044},
  year={2021}
}


## Local curvature and Hessian
@InProceedings{jastrzebski21a,
  title = 	 {Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization},
  author =       {Jastrzebski, Stanislaw and Arpit, Devansh and Astrand, Oliver and Kerg, Giancarlo B and Wang, Huan and Xiong, Caiming and Socher, Richard and Cho, Kyunghyun and Geras, Krzysztof J},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4772--4784},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jastrzebski21a/jastrzebski21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jastrzebski21a.html}}

@article{liu2020bad,
  title={Bad global minima exist and sgd can reach them},
  author={Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8543--8552},
  year={2020}
}

### Optimization trajectories and optimizers
@article{jastrzebski2020break,
  title={The break-even point on optimization trajectories of deep neural networks},
  author={Jastrzebski, Stanislaw and Szymczak, Maciej and Fort, Stanislav and Arpit, Devansh and Tabor, Jacek and Cho, Kyunghyun and Geras, Krzysztof},
  journal={arXiv preprint arXiv:2002.09572},
  year={2020}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{lyu2022understanding,
  title={Understanding the generalization benefit of normalization layers: Sharpness reduction},
  author={Lyu, Kaifeng and Li, Zhiyuan and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34689--34708},
  year={2022}
}


### Forgetting and plasticity loss

@article{dohare2021continual,
  title={Continual backprop: Stochastic gradient descent with persistent randomness},
  author={Dohare, Shibhansh and Sutton, Richard S and Mahmood, A Rupam},
  journal={arXiv preprint arXiv:2108.06325},
  year={2021}
}

@article{ash2020warm,
  title={On warm-starting neural network training},
  author={Ash, Jordan and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3884--3894},
  year={2020}
}

@inproceedings{mirzadeh2022wide,
  title={Wide neural networks forget less catastrophically},
  author={Mirzadeh, Seyed Iman and Chaudhry, Arslan and Yin, Dong and Hu, Huiyi and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  booktitle={International Conference on Machine Learning},
  pages={15699--15717},
  year={2022},
  organization={PMLR}
}
@article{mirzadeh2020understanding,
  title={Understanding the role of training regimes in continual learning},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7308--7320},
  year={2020}
}

@inproceedings{mirzadeh2022wide,
  title={Wide neural networks forget less catastrophically},
  author={Mirzadeh, Seyed Iman and Chaudhry, Arslan and Yin, Dong and Hu, Huiyi and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  booktitle={International Conference on Machine Learning},
  pages={15699--15717},
  year={2022},
  organization={PMLR}
}

@article{lyle2022understanding,
  title={Understanding and preventing capacity loss in reinforcement learning},
  author={Lyle, Clare and Rowland, Mark and Dabney, Will},
  journal={arXiv preprint arXiv:2204.09560},
  year={2022}
}

@article{lyle2023understanding,
  title={Understanding plasticity in neural networks},
  author={Lyle, Clare and Zheng, Zeyu and Nikishin, Evgenii and Pires, Bernardo Avila and Pascanu, Razvan and Dabney, Will},
  journal={arXiv preprint arXiv:2303.01486},
  year={2023}
}

@article{sokar2023dormant,
  title={The dormant neuron phenomenon in deep reinforcement learning},
  author={Sokar, Ghada and Agarwal, Rishabh and Castro, Pablo Samuel and Evci, Utku},
  journal={arXiv preprint arXiv:2302.12902},
  year={2023}
}
@article{abbas2023loss,
  title={Loss of plasticity in continual deep reinforcement learning},
  author={Abbas, Zaheer and Zhao, Rosie and Modayil, Joseph and White, Adam and Machado, Marlos C},
  journal={arXiv preprint arXiv:2303.07507},
  year={2023}
}

@article{khetarpal2020towards,
  title={Towards continual reinforcement learning: A review and perspectives. arXiv},
  author={Khetarpal, K and Riemer, M and Rish, I and Precup, D},
  journal={arXiv preprint arXiv:2012.13490},
  year={2020}
}

### Applications and benchmarks
## Curriculum learning
@article{lee2020learning,
  title={Learning quadrupedal locomotion over challenging terrain},
  author={Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
  journal={Science robotics},
  volume={5},
  number={47},
  pages={eabc5986},
  year={2020},
  publisher={American Association for the Advancement of Science}
}
@article{dennis2020emergent,
  title={Emergent complexity and zero-shot transfer via unsupervised environment design},
  author={Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={13049--13061},
  year={2020}
}
@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={Pmlr}
}
@article{wu2020curricula,
  title={When do curricula work?},
  author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2012.03107},
  year={2020}
}
@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={7382--7431},
  year={2020},
  publisher={JMLRORG}
}

### Continual learning
@article{caccia2022task,
  title={Task-agnostic continual reinforcement learning: In praise of a simple baseline},
  author={Caccia, Massimo and Mueller, Jonas and Kim, Taesup and Charlin, Laurent and Fakoor, Rasool},
  journal={arXiv preprint arXiv:2205.14495},
  year={2022}
}

@inproceedings{mallya2018packnet,
  title={Packnet: Adding multiple tasks to a single network by iterative pruning},
  author={Mallya, Arun and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={7765--7773},
  year={2018}
}

### Parseval reg
@inproceedings{cisse2017parseval,
  title={Parseval networks: Improving robustness to adversarial examples},
  author={Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
  booktitle={International conference on machine learning},
  pages={854--863},
  year={2017},
  organization={PMLR}
}


### Initializations and exploding/vanishing gradients
@article{hanin2018neural,
  title={Which neural net architectures give rise to exploding and vanishing gradients?},
  author={Hanin, Boris},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{hanin2018start,
  title={How to start training: The effect of initialization and architecture},
  author={Hanin, Boris and Rolnick, David},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}
@article{pennington2017resurrecting,
  title={Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@inproceedings{balduzzi2017shattered,
  title={The shattered gradients problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  booktitle={International Conference on Machine Learning},
  pages={342--350},
  year={2017},
  organization={PMLR}
}
@inproceedings{blumenfeld2020beyond,
  title={Beyond signal propagation: is feature diversity necessary in deep neural network initialization?},
  author={Blumenfeld, Yaniv and Gilboa, Dar and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={960--969},
  year={2020},
  organization={PMLR}
}
@article{dauphin2019metainit,
  title={Metainit: Initializing learning by learning to initialize},
  author={Dauphin, Yann N and Schoenholz, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{zhu2021gradinit,
  title={Gradinit: Learning to initialize neural networks for stable and efficient training},
  author={Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W Ronny and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16410--16422},
  year={2021}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

### Other
@article{zhang2019all,
  title={Are all layers created equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@article{smith2021origin,
  title={On the origin of implicit regularization in stochastic gradient descent},
  author={Smith, Samuel L and Dherin, Benoit and Barrett, David GT and De, Soham},
  journal={arXiv preprint arXiv:2101.12176},
  year={2021}
}

@article{ainsworth2022git,
  title={Git re-basin: Merging models modulo permutation symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  journal={arXiv preprint arXiv:2209.04836},
  year={2022}
}

@article{goodfellow2013empirical,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}
@article{nikishin2023deep,
  title={Deep Reinforcement Learning with Plasticity Injection},
  author={Nikishin, Evgenii and Oh, Junhyuk and Ostrovski, Georg and Lyle, Clare and Pascanu, Razvan and Dabney, Will and Barreto, Andr{\'e}},
  journal={arXiv preprint arXiv:2305.15555},
  year={2023}
}
@article{huang202237,
  title={The 37 implementation details of proximal policy optimization},
  author={Huang, Shengyi and Dossa, Rousslan Fernand Julien and Raffin, Antonin and Kanervisto, Anssi and Wang, Weixun},
  journal={The ICLR Blog Track 2023},
  year={2022}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{hansen2023td,
  title={Td-mpc2: Scalable, robust world models for continuous control},
  author={Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  journal={arXiv preprint arXiv:2310.16828},
  year={2023}
}
@inproceedings{zaidi2023does,
  title={When Does Re-initialization Work?},
  author={Zaidi, Sheheryar and Berariu, Tudor and Kim, Hyunjik and Bornschein, Jorg and Clopath, Claudia and Teh, Yee Whye and Pascanu, Razvan},
  booktitle={Proceedings on},
  pages={12--26},
  year={2023},
  organization={PMLR}
}

@article{caccia2022task,
  title={Task-agnostic continual reinforcement learning: In praise of a simple baseline},
  author={Caccia, Massimo and Mueller, Jonas and Kim, Taesup and Charlin, Laurent and Fakoor, Rasool},
  journal={arXiv preprint arXiv:2205.14495},
  year={2022}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@inproceedings{yu2020meta,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on robot learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}
@article{huang2022cleanrl,
  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and João G.M. Araújo},
  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {274},
  pages   = {1--18},
  url     = {http://jmlr.org/papers/v23/21-1342.html}
}
@article{rahman2022robust,
  title={Robust Policy Optimization in Deep Reinforcement Learning},
  author={Rahman, Md Masudur and Xue, Yexiang},
  journal={arXiv preprint arXiv:2212.07536},
  year={2022}
}
@article{kumar2020implicit,
  title={Implicit under-parameterization inhibits data-efficient deep reinforcement learning},
  author={Kumar, Aviral and Agarwal, Rishabh and Ghosh, Dibya and Levine, Sergey},
  journal={arXiv preprint arXiv:2010.14498},
  year={2020}
}
@article{gulcehre2022empirical,
  title={An empirical study of implicit regularization in deep offline rl},
  author={Gulcehre, Caglar and Srinivasan, Srivatsan and Sygnowski, Jakub and Ostrovski, Georg and Farajtabar, Mehrdad and Hoffman, Matt and Pascanu, Razvan and Doucet, Arnaud},
  journal={arXiv preprint arXiv:2207.02099},
  year={2022}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}
@inproceedings{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1924--1932},
  year={2018},
  organization={PMLR}
}
@inproceedings{ahmed2019understanding,
  title={Understanding the impact of entropy on policy optimization},
  author={Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle={International conference on machine learning},
  pages={151--160},
  year={2019},
  organization={PMLR}
}
@inproceedings{anil2019sorting,
  title={Sorting out Lipschitz function approximation},
  author={Anil, Cem and Lucas, James and Grosse, Roger},
  booktitle={International Conference on Machine Learning},
  pages={291--301},
  year={2019},
  organization={PMLR}
}
@article{elfwing2017sigmoid,
  title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. ArXiv e-prints (2017)},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={arXiv preprint arXiv:1702.03118},
  volume={1702},
  year={2017}
}
@article{ramachandran2017swish,
  title={Swish: a self-gated activation function},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  volume={7},
  number={1},
  pages={5},
  year={2017},
  publisher={Technical report}
}
@article{misra2019mish,
  title={Mish: A self regularized non-monotonic activation function},
  author={Misra, Diganta},
  journal={arXiv preprint arXiv:1908.08681},
  year={2019}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{hu2020provable,
  title={Provable benefit of orthogonal initialization in optimizing deep linear networks},
  author={Hu, Wei and Xiao, Lechao and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:2001.05992},
  year={2020}
}
@inproceedings{xiao2018dynamical,
  title={Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={5393--5402},
  year={2018},
  organization={PMLR}
}

@inproceedings{nikishin2022primacy,
  title={The primacy bias in deep reinforcement learning},
  author={Nikishin, Evgenii and Schwarzer, Max and D’Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  booktitle={International conference on machine learning},
  pages={16828--16847},
  year={2022},
  organization={PMLR}
}
@article{team2023human,
  title={Human-timescale adaptation in an open-ended task space},
  author={Team, Adaptive Agent and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and others},
  journal={arXiv preprint arXiv:2301.07608},
  year={2023}
}
@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={Pmlr}
}
@misc{hochreiter2001gradient,
  title={Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  author={Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
  year={2001},
  publisher={A field guide to dynamical recurrent neural networks. IEEE Press In}
}
@inproceedings{saxe2022neural,
  title={The neural race reduction: Dynamics of abstraction in gated networks},
  author={Saxe, Andrew and Sodhani, Shagun and Lewallen, Sam Jay},
  booktitle={International Conference on Machine Learning},
  pages={19287--19309},
  year={2022},
  organization={PMLR}
}
@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}
@article{achille2017critical,
  title={Critical learning periods in deep neural networks},
  author={Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
  journal={arXiv preprint arXiv:1711.08856},
  year={2017}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}
@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}
@article{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{kumar2023maintaining,
  title={Maintaining plasticity via regenerative regularization},
  author={Kumar, Saurabh and Marklund, Henrik and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2308.11958},
  year={2023}
}
@article{lewandowski2023curvature,
  title={Curvature explains loss of plasticity},
  author={Lewandowski, Alex and Tanaka, Haruto and Schuurmans, Dale and Machado, Marlos C},
  journal={arXiv preprint arXiv:2312.00246},
  year={2023}
}



@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{singla2021skew,
  title={Skew orthogonal convolutions},
  author={Singla, Sahil and Feizi, Soheil},
  booktitle={International Conference on Machine Learning},
  pages={9756--9766},
  year={2021},
  organization={PMLR}
}
@inproceedings{prach2022almost,
  title={Almost-orthogonal layers for efficient general-purpose lipschitz networks},
  author={Prach, Bernd and Lampert, Christoph H},
  booktitle={European Conference on Computer Vision},
  pages={350--365},
  year={2022},
  organization={Springer}
}
@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29304--29320},
  year={2021}
}
@inproceedings{shang2016understanding,
  title={Understanding and improving convolutional neural networks via concatenated rectified linear units},
  author={Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
  booktitle={international conference on machine learning},
  pages={2217--2225},
  year={2016},
  organization={PMLR}
}

@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@article{masarczyk2024tunnel,
  title={The tunnel effect: Building data representations in deep neural networks},
  author={Masarczyk, Wojciech and Ostaszewski, Mateusz and Imani, Ehsan and Pascanu, Razvan and Mi{\l}o{\'s}, Piotr and Trzcinski, Tomasz},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{benjamins2021carl,
  title={Carl: A benchmark for contextual and adaptive reinforcement learning},
  author={Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Biedenkapp, Andr{\'e} and Rosenhahn, Bodo and Hutter, Frank and Lindauer, Marius},
  journal={arXiv preprint arXiv:2110.02102},
  year={2021}
}
@inproceedings{d2022sample,
  title={Sample-efficient reinforcement learning by breaking the replay ratio barrier},
  author={D'Oro, Pierluca and Schwarzer, Max and Nikishin, Evgenii and Bacon, Pierre-Luc and Bellemare, Marc G and Courville, Aaron},
  booktitle={Deep Reinforcement Learning Workshop NeurIPS 2022},
  year={2022}
}

@inproceedings{bachlechner2021rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1352--1361},
  year={2021},
  organization={PMLR}
}
@article{zhang2019fixup,
  title={Fixup initialization: Residual learning without normalization},
  author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal={arXiv preprint arXiv:1901.09321},
  year={2019}
}

@article{verwimp2023continual,
  title={Continual learning: Applications and the road forward},
  author={Verwimp, Eli and Ben-David, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L and H{\"u}llermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and Lampert, Christoph H and others},
  journal={arXiv preprint arXiv:2311.11908},
  year={2023}
}