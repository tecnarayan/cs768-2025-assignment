\begin{thebibliography}{}

\bibitem[Ahmad et~al., 2020]{zaheen}
Ahmad, Z., Lelis, L., and Bowling, M. (2020).
\newblock Marginal utility for planning in continuous or large discrete action
  spaces.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Bensoussan, 2011]{bensoussan2011dynamic}
Bensoussan, A. (2011).
\newblock {\em Dynamic Programming and Inventory Control}, volume~3.
\newblock IOS Press.

\bibitem[Bertsekas and Tsitsiklis, 1996]{neuro}
Bertsekas, D.~P. and Tsitsiklis, J.~N. (1996).
\newblock {\em Neuro-Dynamic Programming}.
\newblock Athena Scientific.

\bibitem[Chen and Wang, 2017]{chenwang2017}
Chen, Y. and Wang, M. (2017).
\newblock Lower bound on the computational complexity of discounted {Markov}
  decision problems.
\newblock {\em arXiv preprint arXiv:1705.07312}.

\bibitem[Danihelka et~al., 2022]{danihelka2022policy}
Danihelka, I., Guez, A., Schrittwieser, J., and Silver, D. (2022).
\newblock Policy improvement by planning with {Gumbel}.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations}.

\bibitem[Hubert et~al., 2021]{Hubert2021LearningAP}
Hubert, T., Schrittwieser, J., Antonoglou, I., Barekatain, M., Schmitt, S., and
  Silver, D. (2021).
\newblock Learning and planning in complex action spaces.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}.

\bibitem[Kearns et~al., 2002]{kearns2002sparse}
Kearns, M., Mansour, Y., and Ng, A.~Y. (2002).
\newblock A sparse sampling algorithm for near-optimal planning in large
  {Markov} decision processes.
\newblock {\em Machine learning}, 49(2):193--208.

\bibitem[Levorato et~al., 2012]{wireless1}
Levorato, M., Narang, S., Mitra, U., and Ortega, A. (2012).
\newblock Reduced dimension policy iteration for wireless network control via
  multiscale analysis.
\newblock In {\em 2012 IEEE Global Communications Conference (GLOBECOM)}.

\bibitem[Littman et~al., 1995]{vi_bound}
Littman, M.~L., Dean, T.~L., and Kaelbling, L.~P. (1995).
\newblock On the complexity of solving {Markov} decision problems.
\newblock In {\em Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence}.

\bibitem[Liu et~al., 2017]{wireless2}
Liu, L., Chattopadhyay, A., and Mitra, U. (2017).
\newblock On exploiting spectral properties for solving {MDP} with large state
  space.
\newblock In {\em 55th Annual Allerton Conference on Communication, Control,
  and Computing}.

\bibitem[Liu et~al., 2019]{wireless3}
Liu, L., Chattopadhyay, A., and Mitra, U. (2019).
\newblock On solving {MDPs} with large state space: Exploitation of policy
  structures and spectral properties.
\newblock {\em IEEE Transactions on Communications}, 67(6):4151--4165.

\bibitem[Powell et~al., 2002]{powell2002adaptive}
Powell, W.~B., Shapiro, J.~A., and Sim{\~a}o, H.~P. (2002).
\newblock An adaptive dynamic programming algorithm for the heterogeneous
  resource allocation problem.
\newblock {\em Transportation Science}, 36(2):231--249.

\bibitem[Puterman, 1994]{mdp}
Puterman, M.~L. (1994).
\newblock {\em Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc.

\bibitem[Szepesv{\'a}ri, 2010]{szepesvari2010algorithms}
Szepesv{\'a}ri, C. (2010).
\newblock {\em Algorithms for Reinforcement Learning}.
\newblock Morgan \& Claypool Publishers.

\bibitem[Szepesv{\'a}ri and Littman, 1996]{szepesvari1996generalized}
Szepesv{\'a}ri, C. and Littman, M.~L. (1996).
\newblock Generalized {Markov} decision processes: Dynamic-programming and
  reinforcement-learning algorithms.
\newblock Technical report, CS-96-11, Brown University, Providence, RI.

\bibitem[Van~Seijen and Sutton, 2013]{harm}
Van~Seijen, H. and Sutton, R. (2013).
\newblock Planning by prioritized sweeping with small backups.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}.

\bibitem[Williams and Baird~III, 1993]{Williams93analysisof}
Williams, R.~J. and Baird~III, L.~C. (1993).
\newblock Analysis of some incremental variants of policy iteration: First
  steps toward understanding actor-critic learning systems.
\newblock Technical report, NU-CCS-93-11, Northeastern University, College of
  Computer Science, Boston, MA.

\bibitem[Zeng et~al., 2020]{zeng2020asyncqvi}
Zeng, Y., Feng, F., and Yin, W. (2020).
\newblock Async{QVI}: Asynchronous-parallel {Q}-value iteration for discounted
  {Markov} decision processes with near-optimal sample complexity.
\newblock In {\em Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}.

\end{thebibliography}
