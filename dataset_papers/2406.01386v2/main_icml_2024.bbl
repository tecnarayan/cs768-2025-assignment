\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Jiang, Kakade, and Sun]{agarwal2019reinforcement}
Agarwal, A., Jiang, N., Kakade, S.~M., and Sun, W.
\newblock Reinforcement learning: Theory and algorithms.
\newblock \emph{CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, 32, 2019.

\bibitem[Alkan et~al.(1991)Alkan, Demange, and Gale]{alkan1991fair}
Alkan, A., Demange, G., and Gale, D.
\newblock Fair allocation of indivisible goods and criteria of justice.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pp.\  1023--1039, 1991.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2-3):\penalty0 235--256, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  263--272. PMLR, 2017.

\bibitem[Chen et~al.(2013{\natexlab{a}})Chen, Lakshmanan, and Castillo]{chen2013information}
Chen, W., Lakshmanan, L.~V., and Castillo, C.
\newblock Information and influence propagation in social networks.
\newblock \emph{Synthesis Lectures on Data Management}, 5\penalty0 (4):\penalty0 1--177, 2013{\natexlab{a}}.

\bibitem[Chen et~al.(2013{\natexlab{b}})Chen, Wang, and Yuan]{chen2013combinatorial}
Chen, W., Wang, Y., and Yuan, Y.
\newblock Combinatorial multi-armed bandit: General framework and applications.
\newblock In \emph{International Conference on Machine Learning}, pp.\  151--159. PMLR, 2013{\natexlab{b}}.

\bibitem[Chen et~al.(2016)Chen, Wang, Yuan, and Wang]{chen2016combinatorial}
Chen, W., Wang, Y., Yuan, Y., and Wang, Q.
\newblock Combinatorial multi-armed bandit and its extension to probabilistically triggered arms.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0 (1):\penalty0 1746--1778, 2016.

\bibitem[Chevaleyre et~al.(2017)Chevaleyre, Endriss, and Maudet]{chevaleyre2017distributed}
Chevaleyre, Y., Endriss, U., and Maudet, N.
\newblock Distributed fair allocation of indivisible goods.
\newblock \emph{Artificial Intelligence}, 242:\penalty0 1--22, 2017.

\bibitem[Combes et~al.(2015)Combes, Talebi Mazraeh~Shahi, Proutiere, et~al.]{combes2015combinatorial}
Combes, R., Talebi Mazraeh~Shahi, M.~S., Proutiere, A., et~al.
\newblock Combinatorial bandits revisited.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Dann et~al.(2021)Dann, Marinov, Mohri, and Zimmert]{dann2021beyond}
Dann, C., Marinov, T.~V., Mohri, M., and Zimmert, J.
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 1--12, 2021.

\bibitem[Degenne \& Perchet(2016)Degenne and Perchet]{degenne2016combinatorial}
Degenne, R. and Perchet, V.
\newblock Combinatorial semi-bandit with known covariance.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  2972--2980, 2016.

\bibitem[Demirel \& Tekin(2021)Demirel and Tekin]{demirel2021combinatorial}
Demirel, I. and Tekin, C.
\newblock Combinatorial gaussian process bandits with probabilistically triggered arms.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3844--3852. PMLR, 2021.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and Wang]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R.
\newblock Bilinear classes: A structural framework for provable generalization in rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2826--2836. PMLR, 2021.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and Rakhlin]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Gai et~al.(2012)Gai, Krishnamachari, and Jain]{gai2012combinatorial}
Gai, Y., Krishnamachari, B., and Jain, R.
\newblock Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations.
\newblock \emph{IEEE/ACM Transactions on Networking (TON)}, 20\penalty0 (5):\penalty0 1466--1478, 2012.

\bibitem[Huyuk \& Tekin(2019)Huyuk and Tekin]{huyuk2019analysis}
Huyuk, A. and Tekin, C.
\newblock Analysis of thompson sampling for combinatorial multi-armed bandit with probabilistically triggered arms.
\newblock In \emph{The 22nd international conference on artificial intelligence and statistics}, pp.\  1322--1330. PMLR, 2019.

\bibitem[Hwang et~al.(2023)Hwang, Chai, and Oh]{hwang2023combinatorial}
Hwang, T., Chai, K., and Oh, M.-h.
\newblock Combinatorial neural bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\  14203--14236. PMLR, 2023.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1563--1600, 2010.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and Schapire]{jiang@2017}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low {B}ellman rank are {PAC}-learnable.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\  1704--1713. PMLR, 06--11 Aug 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR, 2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 13406--13418, 2021.

\bibitem[Kveton et~al.(2015{\natexlab{a}})Kveton, Szepesvari, Wen, and Ashkan]{kveton2015cascading}
Kveton, B., Szepesvari, C., Wen, Z., and Ashkan, A.
\newblock Cascading bandits: Learning to rank in the cascade model.
\newblock In \emph{International Conference on Machine Learning}, pp.\  767--776. PMLR, 2015{\natexlab{a}}.

\bibitem[Kveton et~al.(2015{\natexlab{b}})Kveton, Wen, Ashkan, and Szepesv{\'a}ri]{kveton2015combinatorial}
Kveton, B., Wen, Z., Ashkan, A., and Szepesv{\'a}ri, C.
\newblock Combinatorial cascading bandits.
\newblock In \emph{Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1}, pp.\  1450--1458, 2015{\natexlab{b}}.

\bibitem[Kveton et~al.(2015{\natexlab{c}})Kveton, Wen, Ashkan, and Szepesvari]{kveton2015tight}
Kveton, B., Wen, Z., Ashkan, A., and Szepesvari, C.
\newblock Tight regret bounds for stochastic combinatorial semi-bandits.
\newblock In \emph{AISTATS}, 2015{\natexlab{c}}.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Li et~al.(2021)Li, Shi, Chen, Gu, and Chi]{li2021breaking}
Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y.
\newblock Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17762--17776, 2021.

\bibitem[Li et~al.(2016)Li, Wang, Zhang, and Chen]{li2016contextual}
Li, S., Wang, B., Zhang, S., and Chen, W.
\newblock Contextual combinatorial cascading bandits.
\newblock In \emph{International conference on machine learning}, pp.\  1245--1253. PMLR, 2016.

\bibitem[Liu et~al.(2021)Liu, Zuo, Chen, Chen, and Lui]{liu2021multi}
Liu, X., Zuo, J., Chen, X., Chen, W., and Lui, J.~C.
\newblock Multi-layered network exploration via random walks: From offline optimization to online learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7057--7066. PMLR, 2021.

\bibitem[Liu et~al.(2022)Liu, Zuo, Wang, Joe-Wong, Lui, and Chen]{liu2022batch}
Liu, X., Zuo, J., Wang, S., Joe-Wong, C., Lui, J., and Chen, W.
\newblock Batch-size independent regret bounds for combinatorial semi-bandits with probabilistically triggered arms or independent arms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Zuo, Wang, Lui, Hajiesmaili, Wierman, and Chen]{liu2023contextual}
Liu, X., Zuo, J., Wang, S., Lui, J.~C., Hajiesmaili, M., Wierman, A., and Chen, W.
\newblock Contextual combinatorial bandits with probabilistically triggered arms.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22559--22593. PMLR, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Zuo, Xie, Joe-Wong, and Lui]{liu2023variance}
Liu, X., Zuo, J., Xie, H., Joe-Wong, C., and Lui, J.~C.
\newblock Variance-adaptive algorithm for probabilistic maximum coverage bandits with general feedback.
\newblock In \emph{IEEE INFOCOM 2023-IEEE Conference on Computer Communications}, pp.\  1--10. IEEE, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2024)Liu, Lu, Xiong, Zhong, Hu, Zhang, Zheng, Yang, and Wang]{liu2024maximize}
Liu, Z., Lu, M., Xiong, W., Zhong, H., Hu, H., Zhang, S., Zheng, S., Yang, Z., and Wang, Z.
\newblock Maximize to explore: One objective function fusing estimation, planning, and exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[M{\'e}nard et~al.(2021)M{\'e}nard, Domingues, Shang, and Valko]{menard2021ucb}
M{\'e}nard, P., Domingues, O.~D., Shang, X., and Valko, M.
\newblock Ucb momentum q-learning: Correcting the bias without forgetting.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7609--7618. PMLR, 2021.

\bibitem[Merlis \& Mannor(2019)Merlis and Mannor]{merlis2019batch}
Merlis, N. and Mannor, S.
\newblock Batch-size independent regret bounds for the combinatorial multi-armed bandit problem.
\newblock In \emph{Conference on Learning Theory}, pp.\  2465--2489. PMLR, 2019.

\bibitem[Neu \& Pike-Burke(2020)Neu and Pike-Burke]{neu2020unifying}
Neu, G. and Pike-Burke, C.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1392--1403, 2020.

\bibitem[Nie et~al.(2023)Nie, Nadew, Zhu, Aggarwal, and Quinn]{nie2023framework}
Nie, G., Nadew, Y.~Y., Zhu, Y., Aggarwal, V., and Quinn, C.~J.
\newblock A framework for adapting offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\  26166--26198. PMLR, 2023.

\bibitem[Nika et~al.(2020)Nika, Elahi, and Tekin]{nika2020contextual}
Nika, A., Elahi, S., and Tekin, C.
\newblock Contextual combinatorial volatile multi-armed bandit with adaptive discretization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  1486--1496. PMLR, 2020.

\bibitem[Perrault(2022)]{perrault2022combinatorial}
Perrault, P.
\newblock When combinatorial thompson sampling meets approximation regret.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17639--17651, 2022.

\bibitem[Qin et~al.(2014)Qin, Chen, and Zhu]{qin2014contextual}
Qin, L., Chen, S., and Zhu, X.
\newblock Contextual combinatorial bandit and its application on diversified online recommendation.
\newblock In \emph{Proceedings of the 2014 SIAM International Conference on Data Mining}, pp.\  461--469. SIAM, 2014.

\bibitem[Robbins(1952)]{robbins1952some}
Robbins, H.
\newblock Some aspects of the sequential design of experiments.
\newblock \emph{Bulletin of the American Mathematical Society}, 58\penalty0 (5):\penalty0 527--535, 1952.

\bibitem[Saha \& Gopalan(2019)Saha and Gopalan]{saha2019combinatorial}
Saha, A. and Gopalan, A.
\newblock Combinatorial bandits with relative feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and Jamieson]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Slivkins et~al.(2019)]{slivkins2019introduction}
Slivkins, A. et~al.
\newblock Introduction to multi-armed bandits.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 12\penalty0 (1-2):\penalty0 1--286, 2019.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and Langford]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pp.\  2898--2933. PMLR, 2019.

\bibitem[Tsuchiya et~al.(2023)Tsuchiya, Ito, and Honda]{tsuchiya2023further}
Tsuchiya, T., Ito, S., and Honda, J.
\newblock Further adaptive best-of-both-worlds algorithm for combinatorial semi-bandits.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  8117--8144. PMLR, 2023.

\bibitem[Wan et~al.(2023)Wan, Zhang, Chen, Sun, and Zhang]{wan2023bandit}
Wan, Z., Zhang, J., Chen, W., Sun, X., and Zhang, Z.
\newblock Bandit multi-linear dr-submodular maximization and its applications on adversarial submodular bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\  35491--35524. PMLR, 2023.

\bibitem[Wang \& Chen(2017)Wang and Chen]{wang2017improving}
Wang, Q. and Chen, W.
\newblock Improving regret bounds for combinatorial semi-bandits with probabilistically triggered arms and its applications.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  1161--1171, 2017.

\bibitem[Wang \& Chen(2018)Wang and Chen]{wang2018thompson}
Wang, S. and Chen, W.
\newblock Thompson sampling for combinatorial semi-bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5114--5122, 2018.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and Weinberger]{weissman2003inequalities}
Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and Weinberger, M.~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\bibitem[Wen et~al.(2017)Wen, Kveton, Valko, and Vaswani]{wen2017online}
Wen, Z., Kveton, B., Valko, M., and Vaswani, S.
\newblock Online influence maximization under independent cascade model with semi-bandit feedback.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wu et~al.(2022)Wu, Yang, Zhong, Wang, Du, and Jiao]{wu2022nearly}
Wu, T., Yang, Y., Zhong, H., Wang, L., Du, S., and Jiao, J.
\newblock Nearly optimal policy optimization with stable at any time guarantee.
\newblock In \emph{International Conference on Machine Learning}, pp.\  24243--24265. PMLR, 2022.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7304--7312. PMLR, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Ji, and Du]{zhang2021isreinforcement}
Zhang, Z., Ji, X., and Du, S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon.
\newblock In \emph{Conference on Learning Theory}, pp.\  4528--4531. PMLR, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Lee, and Du]{zhang2023settling}
Zhang, Z., Chen, Y., Lee, J.~D., and Du, S.~S.
\newblock Settling the sample complexity of online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2307.13586}, 2023.

\bibitem[Zhong et~al.(2022)Zhong, Xiong, Zheng, Wang, Wang, Yang, and Zhang]{zhong2022gec}
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T.
\newblock Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond.
\newblock \emph{arXiv preprint arXiv:2211.01962}, 2022.

\bibitem[Zimmert et~al.(2019)Zimmert, Luo, and Wei]{zimmert2019beating}
Zimmert, J., Luo, H., and Wei, C.-Y.
\newblock Beating stochastic and adversarial semi-bandits optimally and simultaneously.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7683--7692. PMLR, 2019.

\bibitem[Zuo \& Joe-Wong(2021)Zuo and Joe-Wong]{zuo2021combinatorial}
Zuo, J. and Joe-Wong, C.
\newblock Combinatorial multi-armed bandits for resource allocation.
\newblock In \emph{2021 55th Annual Conference on Information Sciences and Systems (CISS)}, pp.\  1--4. IEEE, 2021.

\bibitem[Zuo et~al.(2022)Zuo, Liu, Joe-Wong, Lui, and Chen]{zuo2022online}
Zuo, J., Liu, X., Joe-Wong, C., Lui, J.~C., and Chen, W.
\newblock Online competitive influence maximization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  11472--11502. PMLR, 2022.

\end{thebibliography}
