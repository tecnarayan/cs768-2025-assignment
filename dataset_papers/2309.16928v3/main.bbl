\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gusukuma et~al.(2018)Gusukuma, Bart, Kafura, and Ernst]{gusukuma2018misconception}
Luke Gusukuma, Austin~Cory Bart, Dennis Kafura, and Jeremy Ernst.
\newblock Misconception-driven feedback: Results from an experimental study.
\newblock In \emph{Proceedings of the 2018 ACM Conference on International Computing Education Research}, pages 160--168, 2018.

\bibitem[Webb(1995)]{webb1995group}
Noreen~M Webb.
\newblock Group collaboration in assessment: Multiple objectives, processes, and outcomes.
\newblock \emph{Educational Evaluation and Policy Analysis}, 17\penalty0 (2):\penalty0 239--261, 1995.

\bibitem[Doan et~al.(2010)Doan, Ramakrishnan, and Halevy]{doan2010mass}
AnHai Doan, Raghu Ramakrishnan, and Alon~Y Halevy.
\newblock Mass collaboration systems on the world-wide web.
\newblock \emph{Communications of the ACM}, 54\penalty0 (4):\penalty0 86--96, 2010.

\bibitem[Koh et~al.(2020)Koh, Nguyen, Tang, Mussmann, Pierson, Kim, and Liang]{cbm}
Pang~Wei Koh, Thao Nguyen, Yew~Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang.
\newblock Concept bottleneck models.
\newblock In \emph{International Conference on Machine Learning}, pages 5338--5348. PMLR, 2020.

\bibitem[Espinosa~Zarlenga et~al.(2022)Espinosa~Zarlenga, Pietro, Gabriele, Giuseppe, Giannini, Diligenti, Zohreh, Frederic, Melacci, Adrian, et~al.]{cem}
Mateo Espinosa~Zarlenga, Barbiero Pietro, Ciravegna Gabriele, Marra Giuseppe, Francesco Giannini, Michelangelo Diligenti, Shams Zohreh, Precioso Frederic, Stefano Melacci, Weller Adrian, et~al.
\newblock Concept embedding models: Beyond the accuracy-explainability trade-off.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 21400--21413. Curran Associates, Inc., 2022.

\bibitem[Havasi et~al.(2022)Havasi, Parbhoo, and Doshi-Velez]{havasi2022addressing}
Marton Havasi, Sonali Parbhoo, and Finale Doshi-Velez.
\newblock Addressing leakage in concept bottleneck models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Magister et~al.(2022)Magister, Barbiero, Kazhdan, Siciliano, Ciravegna, Silvestri, Jamnik, and Lio]{magister2022encoding}
Lucie~Charlotte Magister, Pietro Barbiero, Dmitry Kazhdan, Federico Siciliano, Gabriele Ciravegna, Fabrizio Silvestri, Mateja Jamnik, and Pietro Lio.
\newblock Encoding concepts in graph neural networks.
\newblock \emph{arXiv preprint arXiv:2207.13586}, 2022.

\bibitem[Shin et~al.(2023)Shin, Jo, Ahn, and Lee]{closer_look_at_interventions}
Sungbin Shin, Yohan Jo, Sungsoo Ahn, and Namhoon Lee.
\newblock A closer look at the intervention procedure of concept bottleneck models.
\newblock \emph{arXiv preprint arXiv:2302.14260}, 2023.

\bibitem[Chauhan et~al.(2022)Chauhan, Tiwari, Freyberg, Shenoy, and Dvijotham]{coop}
Kushal Chauhan, Rishabh Tiwari, Jan Freyberg, Pradeep Shenoy, and Krishnamurthy Dvijotham.
\newblock Interactive concept bottleneck models.
\newblock \emph{arXiv preprint arXiv:2212.07430}, 2022.

\bibitem[Espinosa~Zarlenga et~al.(2023)Espinosa~Zarlenga, Barbiero, Shams, Kazhdan, Bhatt, Weller, and Jamnik]{metric_paper}
Mateo Espinosa~Zarlenga, Pietro Barbiero, Zohreh Shams, Dmitry Kazhdan, Umang Bhatt, Adrian Weller, and Mateja Jamnik.
\newblock Towards robust metrics for concept representation evaluation.
\newblock \emph{arXiv e-prints}, pages arXiv--2301, 2023.

\bibitem[Bau et~al.(2017)Bau, Zhou, Khosla, Oliva, and Torralba]{network_dissection}
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Network dissection: Quantifying interpretability of deep visual representations.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6541--6549, 2017.

\bibitem[Fong and Vedaldi(2018)]{net2vec}
Ruth Fong and Andrea Vedaldi.
\newblock Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 8730--8738, 2018.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas, et~al.]{tcav}
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et~al.
\newblock {Interpretability Beyond Feature Attribution: Quantitative Testing With Concept Activation Vectors (TCAV)}.
\newblock In \emph{International conference on machine learning}, pages 2668--2677. PMLR, 2018.

\bibitem[Goyal et~al.(2019{\natexlab{a}})Goyal, Feder, Shalit, and Kim]{cace}
Yash Goyal, Amir Feder, Uri Shalit, and Been Kim.
\newblock {Explaining classifiers with causal concept effect (CaCE)}.
\newblock \emph{arXiv preprint arXiv:1907.07165}, 2019{\natexlab{a}}.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Wexler, Zou, and Kim]{ace}
Amirata Ghorbani, James Wexler, James Zou, and Been Kim.
\newblock Towards automatic concept-based explanations.
\newblock \emph{arXiv preprint arXiv:1902.03129}, 2019.

\bibitem[Kazhdan et~al.(2020)Kazhdan, Dimanov, Jamnik, Li{\`o}, and Weller]{cme}
Dmitry Kazhdan, Botty Dimanov, Mateja Jamnik, Pietro Li{\`o}, and Adrian Weller.
\newblock Now you see me (cme): concept-based model extraction.
\newblock \emph{arXiv preprint arXiv:2010.13233}, 2020.

\bibitem[Yeh et~al.(2020)Yeh, Kim, Arik, Li, Pfister, and Ravikumar]{concept_completeness}
Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar.
\newblock On completeness-aware concept-based explanations in deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 20554--20565, 2020.

\bibitem[Pearl et~al.(2000)]{pearl2000models}
Judea Pearl et~al.
\newblock Models, reasoning and inference.
\newblock \emph{Cambridge, UK: CambridgeUniversityPress}, 19\penalty0 (2):\penalty0 3, 2000.

\bibitem[Goyal et~al.(2019{\natexlab{b}})Goyal, Feder, Shalit, and Kim]{goyal2019explaining}
Yash Goyal, Amir Feder, Uri Shalit, and Been Kim.
\newblock Explaining classifiers with causal concept effect (cace).
\newblock \emph{arXiv preprint arXiv:1907.07165}, 2019{\natexlab{b}}.

\bibitem[O'Shaughnessy et~al.(2020)O'Shaughnessy, Canal, Connor, Rozell, and Davenport]{o2020generative}
Matthew O'Shaughnessy, Gregory Canal, Marissa Connor, Christopher Rozell, and Mark Davenport.
\newblock Generative causal explanations of black-box classifiers.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 5453--5467, 2020.

\bibitem[Shim et~al.(2018)Shim, Hwang, and Yang]{shim2018joint}
Hajin Shim, Sung~Ju Hwang, and Eunho Yang.
\newblock Joint active feature acquisition and classification with variable-size set encoding.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li and Oliva(2021)]{li2021active}
Yang Li and Junier Oliva.
\newblock Active feature acquisition with generative surrogate models.
\newblock In \emph{International Conference on Machine Learning}, pages 6450--6459. PMLR, 2021.

\bibitem[Strauss and Oliva(2022)]{strauss2022posterior}
Ryan Strauss and Junier~B Oliva.
\newblock Posterior matching for arbitrary conditioning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 18088--18099, 2022.

\bibitem[Barker et~al.(2023)Barker, Collins, Dvijotham, Weller, and Bhatt]{barker2023selective}
Matthew Barker, Katherine~M Collins, Krishnamurthy Dvijotham, Adrian Weller, and Umang Bhatt.
\newblock Selective concept models: Permitting stakeholder customisation at test-time.
\newblock \emph{AAAI HCOMP}, 2023.

\bibitem[Sheth et~al.(2022)Sheth, Rahman, Sevyeri, Havaei, and Kahou]{uncertain_concepts_via_interventions}
Ivaxi Sheth, Aamer~Abdul Rahman, Laya~Rafiee Sevyeri, Mohammad Havaei, and Samira~Ebrahimi Kahou.
\newblock Learning from uncertain concepts via test time interventions.
\newblock In \emph{Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022}, 2022.

\bibitem[Bain and Sammut(1995)]{behavioural_cloning}
Michael Bain and Claude Sammut.
\newblock A framework for behavioural cloning.
\newblock In \emph{Machine Intelligence 15}, pages 103--129, 1995.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pages 627--635. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{gumbel_softmax}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Collins et~al.(2023)Collins, Barker, Zarlenga, Raman, Bhatt, Jamnik, Sucholutsky, Weller, and Dvijotham]{collins2023human}
Katherine~M. Collins, Matthew Barker, Mateo~Espinosa Zarlenga, Naveen Raman, Umang Bhatt, Mateja Jamnik, Ilia Sucholutsky, Adrian Weller, and Krishnamurthy Dvijotham.
\newblock Human uncertainty in concept-based ai systems.
\newblock \emph{AIES}, 2023.

\bibitem[Deng(2012)]{mnist}
Li~Deng.
\newblock {The MNIST database of handwritten digit images for machine learning research}.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{cub}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.

\bibitem[Liu et~al.(2018)Liu, Luo, Wang, and Tang]{celeba}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Large-scale celebfaces attributes (celeba) dataset.
\newblock \emph{Retrieved August}, 15\penalty0 (2018):\penalty0 11, 2018.

\bibitem[Marconato et~al.(2022)Marconato, Passerini, and Teso]{glancenets}
Emanuele Marconato, Andrea Passerini, and Stefano Teso.
\newblock Glancenets: Interpretable, leak-proof concept-based models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21212--21227, 2022.

\bibitem[Alvarez-Melis and Jaakkola(2018)]{senn}
David Alvarez-Melis and Tommi~S Jaakkola.
\newblock Towards robust interpretability with self-explaining neural networks.
\newblock \emph{arXiv preprint arXiv:1806.07538}, 2018.

\bibitem[Chen et~al.(2020)Chen, Bei, and Rudin]{concept_whitening}
Zhi Chen, Yijie Bei, and Cynthia Rudin.
\newblock Concept whitening for interpretable image recognition.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (12):\penalty0 772--782, 2020.

\bibitem[Mahinpei et~al.(2021)Mahinpei, Clark, Lage, Doshi-Velez, and Pan]{promises_and_pitfalls}
Anita Mahinpei, Justin Clark, Isaac Lage, Finale Doshi-Velez, and Weiwei Pan.
\newblock Promises and pitfalls of black-box concept learning models.
\newblock \emph{arXiv preprint arXiv:2106.13314}, 2021.

\bibitem[Yuksekgonul et~al.(2023)Yuksekgonul, Wang, and Zou]{yuksekgonul2022post}
Mert Yuksekgonul, Maggie Wang, and James Zou.
\newblock Post-hoc concept bottleneck models.
\newblock In \emph{ICLR 2022 Workshop on PAIR2Struct: Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data}, 2023.

\bibitem[Oikarinen et~al.(2023)Oikarinen, Das, Nguyen, and Weng]{oikarinenlabel}
Tuomas Oikarinen, Subhro Das, Lam~M Nguyen, and Tsui-Wei Weng.
\newblock Label-free concept bottleneck models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Kim et~al.(2023)Kim, Jung, Park, Kim, and Yoon]{probabilistic_cbms}
Eunji Kim, Dahuin Jung, Sangha Park, Siwon Kim, and Sungroh Yoon.
\newblock Probabilistic concept bottleneck models.
\newblock \emph{arXiv preprint arXiv:2306.01574}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{arXiv preprint arXiv:1912.01703}, 2019.

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin, Ghemawat, Irving, Isard, et~al.]{tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{Osdi}, volume~16, pages 265--283. Savannah, GA, USA, 2016.

\bibitem[Kingma and Welling(2013)]{vae}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Ioffe and Szegedy(2015)]{batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages 448--456. pmlr, 2015.

\bibitem[Maas et~al.(2013)Maas, Hannun, Ng, et~al.]{leaky_relu}
Andrew~L Maas, Awni~Y Hannun, Andrew~Y Ng, et~al.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{Proc. icml}, volume~30, page~3. Atlanta, Georgia, USA, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Pascanu et~al.(2012)Pascanu, Mikolov, and Bengio]{exploding_gradients}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock Understanding the exploding gradient problem.
\newblock \emph{CoRR, abs/1211.5063}, 2\penalty0 (417):\penalty0 1, 2012.

\end{thebibliography}
