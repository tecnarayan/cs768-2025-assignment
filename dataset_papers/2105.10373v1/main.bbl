% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Belkin15849}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal, ``{Reconciling modern machine-learning
  practice and the classical bias{\textendash}variance trade-off},''
  \emph{Proceedings of the National Academy of Sciences}, vol. 116, no.~32, pp.
  15\,849--15\,854, 2019.

\bibitem{pmlr-v89-belkin19a}
M.~Belkin, A.~Rakhlin, and A.~B. Tsybakov, ``Does data interpolation contradict
  statistical optimality?'' in \emph{Proceedings of Machine Learning Research},
  ser. Proceedings of Machine Learning Research, K.~Chaudhuri and M.~Sugiyama,
  Eds., vol.~89.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 16--18 Apr 2019,
  pp. 1611--1619.

\bibitem{Geiger2019ScalingDO}
M.~Geiger, A.~Jacot, S.~Spigler, F.~Gabriel, L.~Sagun, S.~d'Ascoli, G.~Biroli,
  C.~Hongler, and M.~Wyart, ``Scaling description of generalization with number
  of parameters in deep learning,'' \emph{ArXiv}, vol. abs/1901.01608, 2019.

\bibitem{Nakkiran2020Deep}
\BIBentryALTinterwordspacing
P.~Nakkiran, G.~Kaplun, Y.~Bansal, T.~Yang, B.~Barak, and I.~Sutskever, ``Deep
  double descent: Where bigger models and more data hurt,'' in
  \emph{International Conference on Learning Representations}, 2020. [Online].
  Available: \url{https://openreview.net/forum?id=B1g5sA4twr}
\BIBentrySTDinterwordspacing

\bibitem{abs-2003-01897}
\BIBentryALTinterwordspacing
P.~Nakkiran, P.~Venkat, S.~M. Kakade, and T.~Ma, ``Optimal regularization can
  mitigate double descent,'' \emph{CoRR}, vol. abs/2003.01897, 2020. [Online].
  Available: \url{https://arxiv.org/abs/2003.01897}
\BIBentrySTDinterwordspacing

\bibitem{kam-chris}
Z.~Deng, A.~Kammoun, and C.~Thrampoulidis, ``{A model of Double Descent for
  High-dimensional Binary Linear Classi¿cation},'' \emph{submitted to
  Information and Inference Journal of the IMA}, 2020.

\bibitem{hastie2019surprises}
T.~Hastie, A.~Montanari, S.~Rosset, and R.~J. Tibshirani, ``Surprises in
  high-dimensional ridgeless least squares interpolation,'' \emph{arXiv
  preprint arXiv:1903.08560}, 2019.

\bibitem{Opper}
S.~B\"os and M.~Opper, ``Dynamics of training,'' \emph{Advances in Neural
  Information Processing Systems}, pp. 141--147, 1997.

\bibitem{spigler}
S.~Spigler, M.~Geiger, S.~d'Ascoli, L.~Sagun, G.~Biroli, and M.~Wyart, ``A
  jamming transition from under- to over-parametrization affects generalization
  in deep learning,'' \emph{Journal of Physics A: Mathematical and
  Theoretical}, vol.~52, 10 2019.

\bibitem{Belkin2019TwoMO}
M.~Belkin, D.~Hsu, and J.~Xu, ``Two models of double descent for weak
  features,'' \emph{ArXiv}, vol. abs/1903.07571, 2019.

\bibitem{9051968}
V.~{Muthukumar}, K.~{Vodrahalli}, V.~{Subramanian}, and A.~{Sahai}, ``Harmless
  interpolation of noisy data in regression,'' \emph{IEEE Journal on Selected
  Areas in Information Theory}, vol.~1, no.~1, pp. 67--83, 2020.

\bibitem{mitra19}
P.~P. Mitra, ``Understanding overfitting peaks in generalization error:
  Analytical risk curves for $l_2$ and $l_1$ penalized interpolation,''
  \emph{arXiv:1906.03667}, 2019.

\bibitem{9174344}
G.~R. {Kini} and C.~{Thrampoulidis}, ``Analytic study of double descent in
  binary classification: The impact of loss,'' in \emph{2020 IEEE International
  Symposium on Information Theory (ISIT)}, 2020, pp. 2527--2532.

\bibitem{svm_kammoun}
A.~Kammoun and M.-S. Alouini, ``On the precise error analysis of support vector
  machines,'' \emph{Submitted to Open journal of signal processing}, 2020.

\bibitem{kammoun_chris}
Z.~Deng, A.~Kammoun, and C.~Thrampoulidis, ``A model of double descent for
  high-dimensional binary linear classification,'' \emph{Submitted to
  Information and inference: A journal of the IMA}, 2020.

\bibitem{cherkassky2004practical}
V.~Cherkassky and Y.~Ma, ``Practical selection of svm parameters and noise
  estimation for svm regression,'' \emph{Neural networks}, vol.~17, no.~1, pp.
  113--126, 2004.

\bibitem{Cherkassky2002}
------, ``Selection of meta-parameters for support vector regression,'' in
  \emph{Artificial Neural Networks --- ICANN 2002}.\hskip 1em plus 0.5em minus
  0.4em\relax Berlin, Heidelberg: Springer Berlin Heidelberg, 2002, pp.
  687--693.

\bibitem{pelckmans2005differogram}
K.~Pelckmans, J.~De~Brabanter, J.~A. Suykens, and B.~De~Moor, ``The
  differogram: Non-parametric noise variance estimation and its use for model
  selection,'' \emph{Neurocomputing}, vol.~69, no. 1-3, pp. 100--122, 2005.

\bibitem{xu2009robustness}
H.~Xu, C.~Caramanis, and S.~Mannor, ``Robustness and regularization of support
  vector machines.'' \emph{Journal of machine learning research}, vol.~10,
  no.~7, 2009.

\bibitem{hable2011qualitative}
R.~Hable and A.~Christmann, ``On qualitative robustness of support vector
  machines,'' \emph{Journal of Multivariate Analysis}, vol. 102, no.~6, pp.
  993--1007, 2011.

\bibitem{Lolas2020}
P.~Lolas, ``Regularization in high-dimensional regression and classification
  via random matrix theory,'' \emph{arXiv: Statistics Theory}, 2020.

\bibitem{Kobak2020TheOR}
D.~Kobak, J.~Lomond, and B.~Sanchez, ``The optimal ridge penalty for real-world
  high-dimensional data can be zero or negative due to the implicit ridge
  regularization,'' \emph{J. Mach. Learn. Res.}, vol.~21, pp. 169:1--169:16,
  2020.

\bibitem{Gor88}
Y.~Gordon, \emph{On Milman's inequality and random subspaces which escape
  through a mesh in $\mathbb{R}^n$}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 1988.

\bibitem{gordon85}
------, ``Some inequalities for gaussian processes and applications,''
  \emph{Israel Journal of Mathematics}, vol.~50, no.~4, pp. 265--289, dec 1985.

\bibitem{thrampoulidis-IT}
C.~Thrampoulidis, E.~Abbasi, and B.~Hassibi, ``{Precise Error Analysis of
  Regularized M-Estimators in High Dimensions},'' \emph{IEEE Transactions on
  Information Theory}, vol.~64, no.~8, Aug. 2018.

\bibitem{Sion58}
M.~Sion, ``On general minimax theorems,'' \emph{Pacific Journal of
  Mathematics}, vol.~8, pp. 171--176, 1958.

\bibitem{SIL06}
Z.~Bai and J.~W. Silverstein, ``{Spectral Analysis of Large Dimensional Random
  Matrices},'' \emph{Springer Series in Statistics}, 2009.

\bibitem{boyd}
S.~Boyd and L.~Vandenberghe, \emph{Convex Optimization}.\hskip 1em plus 0.5em
  minus 0.4em\relax Cambridge University Press, 2003.

\end{thebibliography}
