\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and Bellemare]{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron Courville, and Marc~G Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Akimov et~al.(2022)Akimov, Kurenkov, Nikulin, Tarasov, and Kolesnikov]{akimov2022let}
Dmitriy Akimov, Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov, and Sergey Kolesnikov.
\newblock Let offline rl flow: Training conservative agents in the latent space of normalizing flows.
\newblock \emph{arXiv preprint arXiv:2211.11096}, 2022.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021uncertainty}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified q-ensemble.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 7436--7447, 2021.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Ball et~al.(2023)Ball, Smith, Kostrikov, and Levine]{ball2023efficient}
Philip~J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine.
\newblock Efficient online reinforcement learning with offline data.
\newblock \emph{arXiv preprint arXiv:2302.02948}, 2023.

\bibitem[Beeson \& Montana(2022)Beeson and Montana]{beeson2022improving}
Alex Beeson and Giovanni Montana.
\newblock Improving td3-bc: Relaxed policy constraint for offline learning and stable online fine-tuning.
\newblock \emph{arXiv preprint arXiv:2211.11802}, 2022.

\bibitem[Bhatt et~al.(2019)Bhatt, Argus, Amiranashvili, and Brox]{bhatt2019crossnorm}
Aditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox.
\newblock Crossnorm: Normalization for off-policy td reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1902.05605}, 2019.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Ghadirzadeh, Yu, Gao, Wang, Li, Liang, Finn, and Zhang]{chen2022latent}
Xi~Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, and Chongjie Zhang.
\newblock Latent-variable advantage-weighted policy optimization for offline rl.
\newblock \emph{arXiv preprint arXiv:2203.08949}, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Ghadirzadeh, Yu, Wang, Gao, Li, Bin, Finn, and Zhang]{chen2022lapo}
Xi~Chen, Ali Ghadirzadeh, Tianhe Yu, Jianhao Wang, Alex~Yuan Gao, Wenzhe Li, Liang Bin, Chelsea Finn, and Chongjie Zhang.
\newblock Lapo: Latent-variable advantage-weighted policy optimization for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 36902--36913, 2022{\natexlab{b}}.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos, Rudolph, and Madry]{engstrom2020implementation}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry.
\newblock Implementation matters in deep policy gradients: A case study on ppo and trpo.
\newblock \emph{arXiv preprint arXiv:2005.12729}, 2020.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pp.\  1587--1596. PMLR, 2018.

\bibitem[Ghasemipour et~al.(2022)Ghasemipour, Gu, and Nachum]{ghasemipour2022so}
Seyed Kamyar~Seyed Ghasemipour, Shixiang~Shane Gu, and Ofir Nachum.
\newblock Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters.
\newblock \emph{arXiv preprint arXiv:2205.13703}, 2022.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\  1861--1870. PMLR, 2018.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and Meger]{henderson2018deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Hiraoka et~al.(2021)Hiraoka, Imagawa, Hashimoto, Onishi, and Tsuruoka]{hiraoka2021dropout}
Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka.
\newblock Dropout q-functions for doubly efficient reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2110.02034}, 2021.

\bibitem[Hu et~al.(2022)Hu, Yang, Zhao, and Zhang]{hu2022role}
Hao Hu, Yiqin Yang, Qianchuan Zhao, and Chongjie Zhang.
\newblock On the role of discount factor in offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9072--9098. PMLR, 2022.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, Singh, and Lewis]{jiang2015dependence}
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis.
\newblock The dependence of effective planning horizon on model accuracy.
\newblock In \emph{Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems}, pp.\  1181--1189, 2015.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and Levine]{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1179--1191, 2020.

\bibitem[Kumar et~al.(2022)Kumar, Agarwal, Geng, Tucker, and Levine]{kumar2022offline}
Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine.
\newblock Offline q-learning on diverse multi-task data both scales and generalizes.
\newblock \emph{arXiv preprint arXiv:2211.15144}, 2022.

\bibitem[Kurenkov \& Kolesnikov(2022)Kurenkov and Kolesnikov]{kurenkov2022showing}
Vladislav Kurenkov and Sergey Kolesnikov.
\newblock Showing your offline reinforcement learning work: Online evaluation budget matters.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11729--11752. PMLR, 2022.

\bibitem[Lee et~al.(2022)Lee, Nachum, Yang, Lee, Freeman, Xu, Guadarrama, Fischer, Jang, Michalewski, et~al.]{lee2022multi}
Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et~al.
\newblock Multi-game decision transformers.
\newblock \emph{arXiv preprint arXiv:2205.15241}, 2022.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lu et~al.(2022)Lu, Ball, Rudner, Parker-Holder, Osborne, and Teh]{lu2022challenges}
Cong Lu, Philip~J Ball, Tim~GJ Rudner, Jack Parker-Holder, Michael~A Osborne, and Yee~Whye Teh.
\newblock Challenges and opportunities in offline reinforcement learning from visual observations.
\newblock \emph{arXiv preprint arXiv:2206.04779}, 2022.

\bibitem[Nakamoto et~al.(2023)Nakamoto, Zhai, Singh, Mark, Ma, Finn, Kumar, and Levine]{nakamoto2023cal}
Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max~Sobol Mark, Yi~Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine.
\newblock Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning.
\newblock \emph{arXiv preprint arXiv:2303.05479}, 2023.

\bibitem[Neumann \& Gros(2022)Neumann and Gros]{neumann2022scaling}
Oren Neumann and Claudius Gros.
\newblock Scaling laws for a multi-agent reinforcement learning model.
\newblock \emph{arXiv preprint arXiv:2210.00849}, 2022.

\bibitem[Nikulin et~al.(2022)Nikulin, Kurenkov, Tarasov, Akimov, and Kolesnikov]{nikulin2022q}
Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, Dmitry Akimov, and Sergey Kolesnikov.
\newblock Q-ensemble for offline rl: Don't scale the ensemble, scale the batch size.
\newblock \emph{arXiv preprint arXiv:2211.11092}, 2022.

\bibitem[Nikulin et~al.(2023)Nikulin, Kurenkov, Tarasov, and Kolesnikov]{nikulin2023anti}
Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, and Sergey Kolesnikov.
\newblock Anti-exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:2301.13616}, 2023.

\bibitem[Prudencio et~al.(2022)Prudencio, Maximo, and Colombini]{prudencio2022survey}
Rafael~Figueiredo Prudencio, Marcos~ROA Maximo, and Esther~Luna Colombini.
\newblock A survey on offline reinforcement learning: Taxonomy, review, and open problems.
\newblock \emph{arXiv preprint arXiv:2203.01387}, 2022.

\bibitem[Rezaeifar et~al.(2022)Rezaeifar, Dadashi, Vieillard, Hussenot, Bachem, Pietquin, and Geist]{rezaeifar2022offline}
Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, L{\'e}onard Hussenot, Olivier Bachem, Olivier Pietquin, and Matthieu Geist.
\newblock Offline reinforcement learning as anti-exploration.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  8106--8114, 2022.

\bibitem[Sinha et~al.(2020)Sinha, Bharadhwaj, Srinivas, and Garg]{sinha2020d2rl}
Samarth Sinha, Homanga Bharadhwaj, Aravind Srinivas, and Animesh Garg.
\newblock D2rl: Deep dense architectures in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.09163}, 2020.

\bibitem[Smith et~al.(2022)Smith, Kostrikov, and Levine]{smith2022walk}
Laura Smith, Ilya Kostrikov, and Sergey Levine.
\newblock A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2208.07860}, 2022.

\bibitem[Tarasov et~al.(2022)Tarasov, Nikulin, Akimov, Kurenkov, and Kolesnikov]{tarasov2022corl}
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.
\newblock Corl: Research-oriented deep offline reinforcement learning library.
\newblock \emph{arXiv preprint arXiv:2210.07105}, 2022.

\bibitem[Wu et~al.(2022)Wu, Wu, Qiu, Wang, and Long]{wu2022supported}
Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long.
\newblock Supported policy optimization for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2202.06239}, 2022.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Yang et~al.(2022)Yang, Bai, Ma, Wang, Zhang, and Han]{yang2022rorl}
Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han.
\newblock Rorl: Robust offline reinforcement learning via conservative smoothing.
\newblock \emph{arXiv preprint arXiv:2206.02829}, 2022.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2019)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh]{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76 minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[Zhuang et~al.(2023)Zhuang, LEI, Liu, Wang, and Guo]{zhuang2023behavior}
Zifeng Zhuang, Kun LEI, Jinxin Liu, Donglin Wang, and Yilang Guo.
\newblock Behavior proximal policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=3c13LptpIph}.

\end{thebibliography}
