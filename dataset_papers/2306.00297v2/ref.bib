 @article{wu2023many,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2310.08391},
  year={2023}
}

@article{ahn2022learning,
  title={Learning threshold neurons via the ``edge of stability''},
  author={Ahn, Kwangjun and Bubeck, S{\'e}bastien and Chewi, Sinho and Lee, Yin Tat and Suarez, Felipe and Zhang, Yi},
  journal={NeurIPS 2023 (arXiv:2212.07469)},
  year={2023}
}

@inproceedings{wilson2017marginal,
	title        = {The marginal value of adaptive gradient methods in machine learning},
	author       = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {4148--4158}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  booktitle={2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020},
  pages={5747--5763},
  year={2020},
  organization={Association for Computational Linguistics (ACL)}
}
@article{kunstner2023noise,
  title={Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be},
  author={Kunstner, Frederik and Chen, Jacques and Lavington, Jonathan Wilder and Schmidt, Mark},
  journal={In International Conference on Learning
Representations (ICLR) (arXiv:2304.13960)},
  year={2023}
}
@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}


@inproceedings{zhang2019gradient,
  title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{jiang2022does,
  title={How Does Adaptive Optimization Impact Local Neural Network Geometry?},
  author={Jiang, Kaiqi and Malik, Dhruv and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2211.02254},
  year={2022}
}
 
@article{pan2023toward,
  title={Toward Understanding Why Adam Converges Faster Than SGD for Transformers},
  author={Pan, Yan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2306.00204},
  year={2023}
}

@article{ahn2023linear,
  title={Linear attention is (maybe) all you need (to understand transformer optimization)},
  author={Ahn, Kwangjun and Cheng, Xiang and Song, Minhak and Yun, Chulhee and Jadbabaie, Ali and Sra, Suvrit},
  journal={arXiv preprint arXiv:2310.01082},
  year={2023}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{schlag2021linear,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{erdogdu2016scaled,
  title={Scaled least squares estimator for glms in large-scale problems},
  author={Erdogdu, Murat A and Dicker, Lee H and Bayati, Mohsen},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}
@article{wortsman2023replacing,
  title={Replacing softmax with ReLU in Vision Transformers},
  author={Wortsman, Mitchell and Lee, Jaehoon and Gilmer, Justin and Kornblith, Simon},
  journal={arXiv preprint arXiv:2309.08586},
  year={2023}
}
@article{zhao2023transformers,
  title={Do Transformers Parse while Predicting the Masked Word?},
  author={Zhao, Haoyu and Panigrahi, Abhishek and Ge, Rong and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2303.08117},
  year={2023}
}
@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}

@article{zhang2023trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and 
Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}
@article{allen2023physics,
  title={Physics of Language Models: Part 1, Context-Free Grammar},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.13673},
  year={2023}
} 
@inproceedings{
jastrzebski2018residual,
title={Residual Connections Encourage Iterative Inference},
author={Stanis≈Çaw Jastrzebski and Devansh Arpit and Nicolas Ballas and Vikas Verma and Tong Che and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SJa9iHgAZ},
}
 
@inproceedings{wu2013approximate,
  title={Approximate matrix inversion for high-throughput data detection in the large-scale MIMO uplink},
  author={Wu, Michael and Yin, Bei and Vosoughi, Aida and Studer, Christoph and Cavallaro, Joseph R and Dick, Chris},
  booktitle={2013 IEEE international symposium on circuits and systems (ISCAS)},
  pages={2155--2158},
  year={2013},
  organization={IEEE}
}

@article{el2019implicit,
  title={Implicit deep learning},
  author={El Ghaoui, Laurent and Gu, Fangda and Travacca, Bertrand and Askari, Armin and Tsai, Alicia Y},
  journal={arXiv preprint arXiv:1908.06315},
  volume={2},
  year={2019}
}
@article{furstenberg1960products,
  title={Products of random matrices},
  author={Furstenberg, Harry and Kesten, Harry},
  journal={The Annals of Mathematical Statistics},
  volume={31},
  number={2},
  pages={457--469},
  year={1960},
  publisher={JSTOR}
}
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  year={1989},
}
@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1",
    year = "2019",
   
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  year={2017}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  year={1997},
}
@inproceedings{siegelmann1992computational,
  title={On the computational power of neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  booktitle={Proceedings of Workshop on Computational learning theory},
  year={1992}
}
@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}
@article{mansour2009domain,
  title={Domain adaptation: Learning bounds and algorithms},
  author={Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={arXiv preprint arXiv:0902.3430},
  year={2009}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}
@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@article{wei2022statistically,
  title={Statistically meaningful approximation: a case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022}
}
@article{perez2021attention,
  title={Attention is turing complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={The Journal of Machine Learning Research},
  year={2021}
}
@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@book{alon2016probabilistic,
  title={The probabilistic method},
  author={Alon, Noga and Spencer, Joel H},
  year={2016},
  publisher={John Wiley \& Sons}
}
@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}
@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2021}
}
@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}
@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{courty2017joint,
  title={Joint distribution optimal transportation for domain adaptation},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury and Rakotomamonjy, Alain},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}

@inproceedings{gregor2010learning,
  title={Learning fast approximations of sparse coding},
  author={Gregor, Karol and LeCun, Yann},
  booktitle={Proceedings of the 27th international conference on international conference on machine learning},
  pages={399--406},
  year={2010}
}


@inproceedings{von2022transformers,
  title={Transformers learn in-context by gradient descent},
  author={von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}



@misc{pesut2022who, 
title={Who models the models that model models? An exploration of GPT-3's in-context model fitting ability},
url={https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of}, 
journal={AI Alignment Forum}, 
author={Pesut, Lovre},
year={2022}, 
} 

@article{dinh2022lift,
  title={LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks},
  author={Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Rajput, Shashank and Gira, Michael and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  journal={arXiv preprint arXiv:2206.06565},
  year={2022}
}


@inproceedings{garnelo2018conditional,
  title={Conditional neural processes},
  author={Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, SM Ali},
  booktitle={International Conference on Machine Learning},
  year={2018}
}


@article{garnelo2018neural,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1807.01622},
  year={2018}
}

@article{kim2019attentive,
  title={Attentive neural processes},
  author={Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1901.05761},
  year={2019}
}

@article{nguyen2022transformer,
  title={Transformer neural processes: Uncertainty-aware meta learning via sequence modeling},
  author={Nguyen, Tung and Grover, Aditya},
  journal={arXiv preprint arXiv:2207.04179},
  year={2022}
}

@article{blanc2021decision,
  title={Decision tree heuristics can fail, even in the smoothed setting},
  author={Blanc, Guy and Lange, Jane and Qiao, Mingda and Tan, Li-Yang},
  journal={arXiv preprint arXiv:2107.00819},
  year={2021}
}

@inproceedings{brutzkus2020id3,
  title={Id3 learns juntas for smoothed product distributions},
  author={Brutzkus, Alon and Daniely, Amit and Malach, Eran},
  booktitle={Conference on Learning Theory},
  pages={902--915},
  year={2020},
  organization={PMLR}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics (ACL)",
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 year={2011}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={41--48},
  year={2009}
}

@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  year={1993},
  publisher={Elsevier}
}

@article{sanger1994neural,
  title={Neural network learning control of robot manipulators using gradually increasing task difficulty},
  author={Sanger, Terence D},
  journal={IEEE transactions on Robotics and Automation},
  year={1994},
}

@article{wu2020curricula,
  title={When do curricula work?},
  author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2012.03107},
  year={2020}
}

@inproceedings{bengio1995optimization,
  title={On the optimization of a synaptic learning rule},
  author={Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  booktitle={Preprints Conf. Optimality in Artificial and Biological Neural Networks},
  year={1995}
}


@inproceedings{
li2016learning,
title={Learning to Optimize},
author={Ke Li and Jitendra Malik},
booktitle={International Conference on Learning Representations},
year={2017}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning (ICML)},
  year={2017},
}

@article{ravi2016optimization,
  title={Optimization as a model for few-shot learning},
  author={Ravi, Sachin and Larochelle, Hugo},
  year={2017},
  journal={International Conference for Learning Representations (ICLR)}
}


@article{hospedales2020meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2004.05439},
  year={2020}
}

@book{thrun2012learning,
  title={Learning to learn},
  author={Thrun, Sebastian and Pratt, Lorien},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{naik1992meta,
  title={Meta-neural networks that learn by learning},
  author={Naik, Devang K and Mammone, Richard J},
  booktitle={International Joint Conference on Neural Networks (IJCNN)},
  year={1992},
}

@article{muller2021transformers,
  title={Transformers Can Do Bayesian Inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@article{lu2021pretrained,
  title={Pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018},
}

@article{yao2021self,
  title={Self-attention networks can process bounded hierarchical languages},
  author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2105.11115},
  year={2021}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
}

@article{bhattamishra2020ability,
  title={On the ability and limitations of transformers to recognize formal languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2009.11264},
  year={2020}
}

@article{bhattamishra2020computational,
  title={On the computational power of transformers and its implications in sequence modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  journal={arXiv preprint arXiv:2006.09286},
  year={2020}
}

@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}

@article{perez2019turing,
  title={On the turing completeness of modern neural network architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread}
}


@article{chan2022data,
  title={Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers},
  author={Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew K and Wang, Jane X and Singh, Aaditya and Richemond, Pierre H and McClelland, Jay and Hill, Felix},
  journal={arXiv preprint arXiv:2205.05055},
  year={2022}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2202.07206},
  year={2022}
}

@misc{rong_2021, 
title={Extrapolating to Unnatural Language Processing with GPT-3's In-context Learning: The Good, the Bad, and the Mysterious)},
url={http://ai.stanford.edu/blog/in-context-learning/}, 
journal={The Stanford AI Lab Blog}, 
author={Rong, Frieda },
year={2021}, 
} 

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},

}

@article{min2021noisy,
  title={Noisy channel language model prompting for few-shot text classification},
  author={Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2108.04106},
  year={2021}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{rubin2021learning,
  title={Learning To Retrieve Prompts for In-Context Learning},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  journal={arXiv preprint arXiv:2112.08633},
  year={2021}
}


@article{mishra2021reframing,
  title={Reframing Instructional Prompts to GPTk's Language},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2109.07830},
  year={2021}
}

@article{chen2021meta,
  title={Meta-learning via language model in-context tuning},
  author={Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  journal={arXiv preprint arXiv:2110.07814},
  year={2021}
}

@article{nakkiran2019more,
  title={More data can hurt for linear regression: Sample-wise double descent},
  author={Nakkiran, Preetum},
  journal={arXiv preprint arXiv:1912.07242},
  year={2019}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  year={2021},
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  year={2019},
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Neural Information Processing Systems},
  year={2020}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@inproceedings{xie2022an,
    title={An Explanation of In-context Learning as Implicit Bayesian Inference},
    author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
    booktitle={International Conference on Learning Representations},
    year={2022},
}
 
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  journal={OpenAI blog}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={Proceedings of BigScience -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  year={2021}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  year={1996},
}

@article{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@inproceedings{mishra2018simple,
  title={A Simple Neural Attentive Meta-Learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International conference on machine learning (ICML)},
  year={2016},
}



@article{zhang2022unveiling,
  title={Unveiling Transformers with LEGO: a synthetic reasoning task},
  author={Zhang, Yi and Backurs, Arturs and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
  journal={arXiv preprint arXiv:2206.04301},
  year={2022}
}

@article{suzgun2019memory,
  title={Memory-augmented recurrent neural networks can learn generalized dyck languages},
  author={Suzgun, Mirac and Gehrmann, Sebastian and Belinkov, Yonatan and Shieber, Stuart M},
  journal={arXiv preprint arXiv:1911.03329},
  year={2019}
}

@article{suzgun2019lstm,
  title={LSTM Networks Can Perform Dynamic Counting},
  author={Suzgun, Mirac and Gehrmann, Sebastian and Belinkov12, Yonatan and Shieber, Stuart M},
  journal={ACL},
  year={2019}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Association for Computational Linguistics (ACL) (Short Papers)},
  year={2018}
}

@inproceedings{miller2018stable,
  title={Stable Recurrent Models},
  author={Miller, John and Hardt, Moritz},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{siegelmann1995computational,
  title={On the computational power of neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  journal={Journal of computer and system sciences},
  year={1995},
}

@article{gers2001lstm,
  title={LSTM recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={Transactions on neural networks},
  year={2001},
}

@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}

@inproceedings{skachkova2018closing,
  title={Closing brackets with recurrent neural networks},
  author={Skachkova, Natalia and Trost, Thomas Alexander and Klakow, Dietrich},
  booktitle={EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  year={2018}
}

@inproceedings{edelman2022inductive,
  title={Inductive Biases and Variable Creation in Self-Attention Mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022},
}

@article{snell2021approximating,
  title={Approximating how single head attention learns},
  author={Snell, Charlie and Zhong, Ruiqi and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.07601},
  year={2021}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@inproceedings{balcan2021much,
  title={How much data is sufficient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design},
  author={Balcan, Maria-Florina and DeBlasio, Dan and Dick, Travis and Kingsford, Carl and Sandholm, Tuomas and Vitercik, Ellen},
  booktitle={Symposium on Theory of Computing (STOC)},
  year={2021}
}

@article{schwarzschild2021can,
  title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{selsam2018learning,
  title={Learning a SAT Solver from Single-Bit Supervision},
  author={Selsam, Daniel and Lamm, Matthew and Benedikt, B and Liang, Percy and de Moura, Leonardo and Dill, David L and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@incollection{balcan2020data,
  title={Data-driven algorithm design},
  author={Balcan, Maria-Florina},
  editor={Tim Roughgarden},
  booktitle={Beyond Worst Case Analysis of Algorithms},
  publisher={Cambridge University Press},
  year=2020,
}

@article{horvitz2001bayesian,
  title={A Bayesian Approach to Tackling Hard Computational Problems (Preliminary Report)},
  author={Horvitz, Eric and Ruan, Yongshao and Gomes, Carla and Kautz, Henry and Selman, Bart and Chickering, Max},
  journal={Electronic Notes in Discrete Mathematics},
  year={2001},
}
@article{xu2008satzilla,
  title={SATzilla: portfolio-based algorithm selection for SAT},
  author={Xu, Lin and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  journal={Journal of artificial intelligence research},
  year={2008}
}

@inproceedings{vinyals2015pointer,
 author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
 booktitle = {Neural Information Processing Systems (NeurIPS)},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 title = {Pointer Networks},
 year = {2015}
}

@article{bello2016neural,
  title={Neural combinatorial optimization with reinforcement learning},
  author={Bello, Irwan and Pham, Hieu and Le, Quoc V and Norouzi, Mohammad and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.09940},
  year={2016}
}

@article{khalil2017learning,
  title={Learning combinatorial optimization algorithms over graphs},
  author={Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={conference on knowledge discovery and data mining (KDD)},
  year={2016}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  year={2001},
}

@phdthesis{schmidhuber1987evolutionary,
  title={Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author={Schmidhuber, J{\"u}rgen},
  year={1987},
  school={Technische Universit{\"a}t M{\"u}nchen}
}

@inproceedings{hochreiter2001learning,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={International conference on artificial neural networks (ICANN)},
  year={2001},
}

@article{kirsch2021meta,
  title={Meta learning backpropagation and improving it},
  author={Kirsch, Louis and Schmidhuber, J{\"u}rgen},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}