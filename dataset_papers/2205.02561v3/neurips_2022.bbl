% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{tralc}
T.~Wu, P.~Zhou, K.~Liu, Y.~Yuan, X.~Wang, H.~Huang, and D.~O. Wu, ``Multi-agent
  deep reinforcement learning for urban traffic light control in vehicular
  networks,'' \emph{IEEE Transactions on Vehicular Technology}, vol.~69, no.~8,
  pp. 8243--8256, 2020.

\bibitem{cars}
Y.~Cao, W.~Yu, W.~Ren, and G.~Chen, ``An overview of recent progress in the
  study of distributed multi-agent coordination,'' \emph{IEEE Transactions on
  Industrial informatics}, vol.~9, no.~1, pp. 427--438, 2012.

\bibitem{robots}
M.~H{\"u}ttenrauch, A.~{\v{S}}o{\v{s}}i{\'c}, and G.~Neumann, ``Guided deep
  reinforcement learning for swarm systems,'' \emph{arXiv preprint
  arXiv:1709.06011}, 2017.

\bibitem{VDN}
P.~Sunehag, G.~Lever, A.~Gruslys, W.~M. Czarnecki, V.~Zambaldi, M.~Jaderberg,
  M.~Lanctot, N.~Sonnerat, J.~Z. Leibo, K.~Tuyls \emph{et~al.},
  ``Value-decomposition networks for cooperative multi-agent learning,''
  \emph{arXiv preprint arXiv:1706.05296}, 2017.

\bibitem{QMIX}
T.~Rashid, M.~Samvelyan, C.~Schroeder, G.~Farquhar, J.~Foerster, and
  S.~Whiteson, ``Qmix: Monotonic value function factorisation for deep
  multi-agent reinforcement learning,'' in \emph{Proceedings of the
  International Conference on Machine Learning}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2018, pp. 4295--4304.

\bibitem{QTRAN}
K.~Son, D.~Kim, W.~J. Kang, D.~E. Hostallero, and Y.~Yi, ``Qtran: Learning to
  factorize with transformation for cooperative multi-agent reinforcement
  learning,'' in \emph{Proceedings of the International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 5887--5896.

\bibitem{QATTEN}
Y.~Yang, J.~Hao, B.~Liao, K.~Shao, G.~Chen, W.~Liu, and H.~Tang, ``Qatten: A
  general framework for cooperative multiagent reinforcement learning,''
  \emph{arXiv preprint arXiv:2002.03939}, 2020.

\bibitem{QPLEX}
J.~Wang, Z.~Ren, T.~Liu, Y.~Yu, and C.~Zhang, ``Qplex: Duplex dueling
  multi-agent q-learning,'' in \emph{Proceedings of the International
  Conference on Learning Representations}, 2021.

\bibitem{REF}
S.~Iqbal, C.~A.~S. De~Witt, B.~Peng, W.~B{\"o}hmer, S.~Whiteson, and F.~Sha,
  ``Randomized entity-wise factorization for multi-agent reinforcement
  learning,'' in \emph{Proceedings of the International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 4596--4606.

\bibitem{MADDPG}
R.~Lowe, Y.~I. Wu, A.~Tamar, J.~Harb, O.~Pieter~Abbeel, and I.~Mordatch,
  ``Multi-agent actor-critic for mixed cooperative-competitive environments,''
  in \emph{Proceedings of the Neural Information Processing Systems}, vol.~30,
  2017.

\bibitem{COMA}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson,
  ``Counterfactual multi-agent policy gradients,'' in \emph{Proceedings of the
  AAAI Conference on Artificial Intelligence}, vol.~32, no.~1, 2018.

\bibitem{LICA}
M.~Zhou, Z.~Liu, P.~Sui, Y.~Li, and Y.~Y. Chung, ``Learning implicit credit
  assignment for cooperative multi-agent reinforcement learning,'' in
  \emph{Proceedings of the Neural Information Processing Systems}, vol.~33,
  2020, pp. 11\,853--11\,864.

\bibitem{DOP}
Y.~Wang, B.~Han, T.~Wang, H.~Dong, and C.~Zhang, ``Dop: Off-policy multi-agent
  decomposed policy gradients,'' in \emph{Proceedings of the International
  Conference on Learning Representations}, 2020.

\bibitem{VMIX}
J.~Su, S.~Adams, and P.~A. Beling, ``Value-decomposition multi-agent
  actor-critics,'' in \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~35, no.~13, 2021, pp. 11\,352--11\,360.

\bibitem{FOP}
T.~Zhang, Y.~Li, C.~Wang, G.~Xie, and Z.~Lu, ``Fop: Factorizing optimal joint
  policy of maximum-entropy multi-agent reinforcement learning,'' in
  \emph{Proceedings of the International Conference on Machine Learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 12\,491--12\,500.

\bibitem{CDS}
L.~Chenghao, T.~Wang, C.~Wu, Q.~Zhao, J.~Yang, and C.~Zhang, ``Celebrating
  diversity in shared multi-agent reinforcement learning,'' in
  \emph{Proceedings of the Neural Information Processing Systems}, vol.~34,
  2021.

\bibitem{PSS}
J.~K. Terry, N.~Grammel, A.~Hari, and L.~Santos, ``Parameter sharing is
  surprisingly useful for multi-agent deep reinforcement learning,''
  \emph{arXiv preprint arXiv:2005.13625}, 2020.

\bibitem{Archit}
H.~Tianfield, J.~Tian, and X.~Yao, ``On the architectures of complex
  multi-agent systems,'' in \emph{Proceedings of the Workshop on Knowledge Grid
  and Grid Intelligence}.\hskip 1em plus 0.5em minus 0.4em\relax Citeseer,
  2003, pp. 195--206.

\bibitem{MAPlan}
C.~Witteveen and M.~De~Weerdt, ``Multi-agent planning for non-cooperative
  agents.'' in \emph{Proceedings of the AAAI Spring Symposium: Distributed Plan
  and Schedule Management}, 2006, p. 169.

\bibitem{GRF}
K.~Kurach, A.~Raichuk, P.~Sta{\'n}czyk, M.~Zaj{\k{a}}c, O.~Bachem, L.~Espeholt,
  C.~Riquelme, D.~Vincent, M.~Michalski, O.~Bousquet \emph{et~al.}, ``Google
  research football: A novel reinforcement learning environment,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~34, no.~04, 2020, pp. 4501--4510.

\bibitem{SePS}
F.~Christianos, G.~Papoudakis, M.~A. Rahman, and S.~V. Albrecht, ``Scaling
  multi-agent reinforcement learning with selective parameter sharing,'' in
  \emph{Proceedings of the International Conference on Machine Learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 1989--1998.

\bibitem{pred1}
J.~Pav{\'o}n and J.~G{\'o}mez-Sanz, ``Agent oriented software engineering with
  ingenias,'' in \emph{Proceedings of the International Central and Eastern
  European Conference on Multi-Agent Systems}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2003, pp. 394--403.

\bibitem{pred2}
M.~Cossentino, S.~Gaglio, L.~Sabatucci, and V.~Seidita, ``The passi and agile
  passi mas meta-models compared with a unifying proposal,'' in
  \emph{Proceedings of the International Central and Eastern European
  Conference on Multi-Agent Systems}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2005, pp. 183--192.

\bibitem{pred3}
N.~Spanoudakis and P.~Moraitis, ``Using aseme methodology for model-driven
  agent systems development,'' in \emph{International Workshop on
  Agent-Oriented Software Engineering}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2010, pp. 106--127.

\bibitem{pred4}
N.~Bonjean, W.~Mefteh, M.-P. Gleizes, C.~Maurel, and F.~Migeon, ``Adelfe 2.0,
  handbook on agent-oriented design processes,'' 2014.

\bibitem{RODE}
T.~Wang, T.~Gupta, A.~Mahajan, B.~Peng, S.~Whiteson, and C.~Zhang, ``Rode:
  Learning roles to decompose multi-agent tasks,'' in \emph{Proceedings of the
  International Conference on Learning Representations}, 2021.

\bibitem{SMAC}
M.~Samvelyan, T.~Rashid, C.~S. De~Witt, G.~Farquhar, N.~Nardelli, T.~G. Rudner,
  C.-M. Hung, P.~H. Torr, J.~Foerster, and S.~Whiteson, ``The starcraft
  multi-agent challenge,'' \emph{arXiv preprint arXiv:1902.04043}, 2019.

\bibitem{Gumbel}
E.~Jang, S.~Gu, and B.~Poole, ``Categorical reparameterization with
  gumbel-softmax,'' \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem{ctde1}
F.~A. Oliehoek, M.~T. Spaan, and N.~Vlassis, ``Optimal and approximate q-value
  functions for decentralized pomdps,'' \emph{Journal of Artificial
  Intelligence Research}, vol.~32, pp. 289--353, 2008.

\bibitem{ctde2}
L.~Kraemer and B.~Banerjee, ``Multi-agent reinforcement learning as a rehearsal
  for decentralized planning,'' \emph{Neurocomputing}, vol. 190, pp. 82--94,
  2016.

\bibitem{pomdp}
F.~A. Oliehoek and C.~Amato, \emph{A concise introduction to decentralized
  POMDPs}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2016.

\bibitem{GRU}
K.~Cho, B.~Van~Merri{\"e}nboer, C.~Gulcehre, D.~Bahdanau, F.~Bougares,
  H.~Schwenk, and Y.~Bengio, ``Learning phrase representations using rnn
  encoder-decoder for statistical machine translation,'' \emph{arXiv preprint
  arXiv:1406.1078}, 2014.

\bibitem{ROMA}
T.~Wang, H.~Dong, V.~Lesser, and C.~Zhang, ``{ROMA}: Multi-agent reinforcement
  learning with emergent roles,'' in \emph{Proceedings of the International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2020, pp. 9876--9886.

\bibitem{IQL}
A.~Tampuu, T.~Matiisen, D.~Kodelja, I.~Kuzovkin, K.~Korjus, J.~Aru, J.~Aru, and
  R.~Vicente, ``Multiagent cooperation and competition with deep reinforcement
  learning,'' \emph{PloS one}, vol.~12, no.~4, p. e0172395, 2017.

\bibitem{commu1}
J.~Foerster, I.~A. Assael, N.~De~Freitas, and S.~Whiteson, ``Learning to
  communicate with deep multi-agent reinforcement learning,'' in
  \emph{Proceedings of the Neural Information Processing Systems}, vol.~29,
  2016.

\bibitem{commu2}
T.~Wang, J.~Wang, C.~Zheng, and C.~Zhang, ``Learning nearly decomposable value
  functions via communication minimization,'' in \emph{Proceedings of the
  International Conference on Learning Representations}, 2020.

\bibitem{explor1}
A.~Mahajan, T.~Rashid, M.~Samvelyan, and S.~Whiteson, ``Maven: Multi-agent
  variational exploration,'' in \emph{Proceedings of the Neural Information
  Processing Systems}, vol.~32, 2019.

\bibitem{explor2}
T.~Gupta, A.~Mahajan, B.~Peng, W.~B{\"o}hmer, and S.~Whiteson, ``Uneven:
  Universal value exploration for multi-agent reinforcement learning,'' in
  \emph{Proceedings of the International Conference on Machine Learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 3930--3941.

\bibitem{robust1}
K.~Zhang, T.~Sun, Y.~Tao, S.~Genc, S.~Mallya, and T.~Basar, ``Robust
  multi-agent reinforcement learning with model uncertainty,'' in
  \emph{Proceedings of the Neural Information Processing Systems}, vol.~33,
  2020, pp. 10\,571--10\,583.

\bibitem{robust2}
J.~Zhao, Y.~Zhao, W.~Wang, M.~Yang, X.~Hu, W.~Zhou, J.~Hao, and H.~Li,
  ``Coach-assisted multi-agent reinforcement learning framework for unexpected
  crashed agents,'' \emph{arXiv preprint arXiv:2203.08454}, 2022.

\bibitem{software}
D.~Smolko, ``Design and evaluation of the mobile agent architecture for
  distributed consistency management,'' in \emph{Proceedings of the
  International Conference on Software Engineering}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2001, pp. 799--800.

\bibitem{health}
T.~Alsinet, R.~B{\'e}jar, C.~Fernanadez, and F.~Many{\`a}, ``A multi-agent
  system architecture for monitoring medical protocols,'' in \emph{Proceedings
  of the International Conference on Autonomous Agents}, 2000, pp. 499--505.

\bibitem{traffic}
F.~Logi and S.~G. Ritchie, ``A multi-agent architecture for cooperative
  inter-jurisdictional traffic congestion management,'' \emph{Transportation
  Research Part C: Emerging Technologies}, vol.~10, no. 5-6, pp. 507--527,
  2002.

\bibitem{nguyen2022learning}
D.~Nguyen, P.~Nguyen, S.~Venkatesh, and T.~Tran, ``Learning to transfer role
  assignment across team sizes,'' \emph{arXiv preprint arXiv:2204.12937}, 2022.

\end{thebibliography}
