\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alexandrov et~al.(2020)Alexandrov, Benidis, Bohlke-Schneider, Flunkert, Gasthaus, Januschowski, Maddix, Rangapuram, Salinas, Schulz, Stella, TÃ¼rkmen, and Wang]{alexander2020gluonts}
Alexandrov, A., Benidis, K., Bohlke-Schneider, M., Flunkert, V., Gasthaus, J., Januschowski, T., Maddix, D.~C., Rangapuram, S., Salinas, D., Schulz, J., Stella, L., TÃ¼rkmen, A.~C., and Wang, Y.
\newblock Gluonts: Probabilistic and neural time series modeling in python.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (116):\penalty0 1--6, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/19-820.html}.

\bibitem[Awasthi et~al.(2022)Awasthi, Das, Sen, and Suresh]{awasthi2022mle}
Awasthi, P., Das, A., Sen, R., and Suresh, A.~T.
\newblock On the benefits of maximum likelihood estimation for regression and forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=zrW-LVXj2k1}.

\bibitem[Bergmeir et~al.(2023)Bergmeir, Bui, de~Nijs, and Stuckey]{bergmeir2023residential}
Bergmeir, C., Bui, Q., de~Nijs, F., and Stuckey, P.
\newblock Residential power and battery data, August 2023.
\newblock URL \url{https://doi.org/10.5281/zenodo.8219786}.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021foundation}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[CDC(2017)]{cdc}
CDC.
\newblock Flu portal dashboard, 2017.
\newblock URL \url{https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html}.

\bibitem[Chen et~al.(2001)Chen, Petty, Skabardonis, Varaiya, and Jia]{chen2001pems}
Chen, C., Petty, K., Skabardonis, A., Varaiya, P., and Jia, Z.
\newblock Freeway performance measurement system: mining loop detector data.
\newblock \emph{Transportation Research Record}, 1748\penalty0 (1):\penalty0 96--102, 2001.

\bibitem[Chen(2019)]{chen2019beijingair}
Chen, S.
\newblock {Beijing Multi-Site Air-Quality Data}.
\newblock UCI Machine Learning Repository, 2019.
\newblock {DOI}: https://doi.org/10.24432/C5RK5G.

\bibitem[Das et~al.(2023{\natexlab{a}})Das, Kong, Leach, Mathur, Sen, and Yu]{das2023tide}
Das, A., Kong, W., Leach, A., Mathur, S.~K., Sen, R., and Yu, R.
\newblock Long-term forecasting with ti{DE}: Time-series dense encoder.
\newblock \emph{Transactions on Machine Learning Research}, 2023{\natexlab{a}}.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=pCbC3aQB5W}.

\bibitem[Das et~al.(2023{\natexlab{b}})Das, Kong, Sen, and Zhou]{das2023predct}
Das, A., Kong, W., Sen, R., and Zhou, Y.
\newblock A decoder-only foundation model for time-series forecasting.
\newblock \emph{arXiv preprint arXiv:2310.10688}, 2023{\natexlab{b}}.

\bibitem[Dong et~al.(2023)Dong, Wu, Zhang, Zhang, Wang, and Long]{dong2023simmtm}
Dong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., and Long, M.
\newblock Sim{MTM}: A simple pre-training framework for masked time-series modeling.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ginTcBUnL8}.

\bibitem[Dooley et~al.(2023)Dooley, Khurana, Mohapatra, Naidu, and White]{dooley2023forecastpfn}
Dooley, S., Khurana, G.~S., Mohapatra, C., Naidu, S.~V., and White, C.
\newblock Forecast{PFN}: Synthetically-trained zero-shot forecasting.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=tScBQRNgjk}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Ekambaram et~al.(2023)Ekambaram, Jati, Nguyen, Sinthong, and Kalagnanam]{vijay2023tsmixer}
Ekambaram, V., Jati, A., Nguyen, N., Sinthong, P., and Kalagnanam, J.
\newblock Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, KDD '23, pp.\  459–469, New York, NY, USA, 2023. Association for Computing Machinery.
\newblock ISBN 9798400701030.
\newblock \doi{10.1145/3580305.3599533}.
\newblock URL \url{https://doi.org/10.1145/3580305.3599533}.

\bibitem[Ekambaram et~al.(2024)Ekambaram, Jati, Nguyen, Dayama, Reddy, Gifford, and Kalagnanam]{ekambaram2024ttms}
Ekambaram, V., Jati, A., Nguyen, N.~H., Dayama, P., Reddy, C., Gifford, W.~M., and Kalagnanam, J.
\newblock Ttms: Fast multi-level tiny time mixers for improved zero-shot and few-shot forecasting of multivariate time series.
\newblock \emph{arXiv preprint arXiv:2401.03955}, 2024.

\bibitem[Emami et~al.(2023)Emami, Sahu, and Graf]{emami2023buildingsbench}
Emami, P., Sahu, A., and Graf, P.
\newblock Buildingsbench: A large-scale dataset of 900k buildings and benchmark for short-term load forecasting.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.
\newblock URL \url{https://openreview.net/forum?id=c5rqd6PZn6}.

\bibitem[Feng et~al.(2024)Feng, Miao, Zhang, and Zhao]{feng2024latent}
Feng, S., Miao, C., Zhang, Z., and Zhao, P.
\newblock Latent diffusion transformer for probabilistic time series forecasting.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  11979--11987, 2024.

\bibitem[Garza \& Mergenthaler-Canseco(2023)Garza and Mergenthaler-Canseco]{garza2023timegpt}
Garza, A. and Mergenthaler-Canseco, M.
\newblock Timegpt-1.
\newblock \emph{arXiv preprint arXiv:2310.03589}, 2023.

\bibitem[Garza et~al.(2022)Garza, Canseco, Challú, and Olivares]{garza2022statsforecast}
Garza, F., Canseco, M.~M., Challú, C., and Olivares, K.~G.
\newblock {StatsForecast}: Lightning fast forecasting with statistical and econometric models.
\newblock {PyCon} Salt Lake City, Utah, US 2022, 2022.
\newblock URL \url{https://github.com/Nixtla/statsforecast}.

\bibitem[Gneiting \& Raftery(2007)Gneiting and Raftery]{gneiting2007strictly}
Gneiting, T. and Raftery, A.~E.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American statistical Association}, 102\penalty0 (477):\penalty0 359--378, 2007.

\bibitem[Godahewa et~al.(2021)Godahewa, Bergmeir, Webb, Hyndman, and Montero-Manso]{godahewa2021monash}
Godahewa, R.~W., Bergmeir, C., Webb, G.~I., Hyndman, R., and Montero-Manso, P.
\newblock Monash time series forecasting archive.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=wEc1mgAjU-}.

\bibitem[Gruver et~al.(2023)Gruver, Finzi, Qiu, and Wilson]{gruver2023llmtime}
Gruver, N., Finzi, M.~A., Qiu, S., and Wilson, A.~G.
\newblock Large language models are zero-shot time series forecasters.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=md68e8iZK1}.

\bibitem[Henry et~al.(2020)Henry, Dachapally, Pawar, and Chen]{henry2020qknorm}
Henry, A., Dachapally, P.~R., Pawar, S.~S., and Chen, Y.
\newblock Query-key normalization for transformers.
\newblock In Cohn, T., He, Y., and Liu, Y. (eds.), \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pp.\  4246--4253, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.379}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.379}.

\bibitem[Hyndman(2014)]{hyndman2014errors}
Hyndman, R.~J.
\newblock Errors on percentage errors, 4 2014.
\newblock URL \url{https://robjhyndman.com/hyndsight/smape/}.

\bibitem[Hyndman \& Athanasopoulos(2018)Hyndman and Athanasopoulos]{hyndman2018fpp}
Hyndman, R.~J. and Athanasopoulos, G.
\newblock \emph{Forecasting: principles and practice}.
\newblock OTexts, 2018.

\bibitem[Hyndman \& Koehler(2006)Hyndman and Koehler]{hyndman2006another}
Hyndman, R.~J. and Koehler, A.~B.
\newblock Another look at measures of forecast accuracy.
\newblock \emph{International journal of forecasting}, 22\penalty0 (4):\penalty0 679--688, 2006.

\bibitem[Jin et~al.(2023)Jin, Wang, Ma, Chu, Zhang, Shi, Chen, Liang, Li, Pan, et~al.]{jin2023timellm}
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J.~Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., et~al.
\newblock Time-llm: Time series forecasting by reprogramming large language models.
\newblock \emph{arXiv preprint arXiv:2310.01728}, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kim et~al.(2022)Kim, Kim, Tae, Park, Choi, and Choo]{kim2022reversible}
Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J.
\newblock Reversible instance normalization for accurate time-series forecasting against distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=cGDAkQo1C0p}.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and Liu]{lai2018modeling}
Lai, G., Chang, W.-C., Yang, Y., and Liu, H.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In \emph{The 41st international ACM SIGIR conference on research \& development in information retrieval}, pp.\  95--104, 2018.

\bibitem[Lim et~al.(2021)Lim, Ar{\i}k, Loeff, and Pfister]{lim2021tft}
Lim, B., Ar{\i}k, S.~{\"O}., Loeff, N., and Pfister, T.
\newblock Temporal fusion transformers for interpretable multi-horizon time series forecasting.
\newblock \emph{International Journal of Forecasting}, 37\penalty0 (4):\penalty0 1748--1764, 2021.

\bibitem[Liu et~al.(2022)Liu, Zeng, Chen, Xu, Lai, Ma, and Xu]{liu2022scinet}
Liu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and Xu, Q.
\newblock Scinet: Time series modeling and forecasting with sample convolution and interaction.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5816--5828, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Xia, Liang, Hu, Wang, Bai, Huang, Liu, Hooi, and Zimmermann]{liu2023largest}
Liu, X., Xia, Y., Liang, Y., Hu, J., Wang, Y., Bai, L., Huang, C., Liu, Z., Hooi, B., and Zimmermann, R.
\newblock Largest: A benchmark dataset for large-scale traffic forecasting.
\newblock \emph{arXiv preprint arXiv:2306.08259}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024)Liu, Hu, Li, Diao, Liang, Hooi, and Zimmermann]{liu2024unitime}
Liu, X., Hu, J., Li, Y., Diao, S., Liang, Y., Hooi, B., and Zimmermann, R.
\newblock Unitime: A language-empowered unified model for cross-domain time series forecasting.
\newblock In \emph{Proceedings of the ACM Web Conference 2024}, 2024.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2023itransformer}
Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock \emph{arXiv preprint arXiv:2310.06625}, 2023{\natexlab{b}}.

\bibitem[Ma et~al.(2023)Ma, Liu, Zheng, Huang, Zhu, Yu, and Kwok]{ma2023survey}
Ma, Q., Liu, Z., Zheng, Z., Huang, Z., Zhu, S., Yu, Z., and Kwok, J.~T.
\newblock A survey on time-series pre-trained models.
\newblock \emph{arXiv preprint arXiv:2305.10716}, 2023.

\bibitem[Makridakis et~al.(2020)Makridakis, Spiliotis, and Assimakopoulos]{makridakis2020m4}
Makridakis, S., Spiliotis, E., and Assimakopoulos, V.
\newblock The m4 competition: 100,000 time series and 61 forecasting methods.
\newblock \emph{International Journal of Forecasting}, 36\penalty0 (1):\penalty0 54--74, 2020.

\bibitem[Mancuso et~al.(2021)Mancuso, Piccialli, and Sudoso]{mancuso2021hierarchical}
Mancuso, P., Piccialli, V., and Sudoso, A.~M.
\newblock A machine learning approach for forecasting hierarchical time series.
\newblock \emph{Expert Systems with Applications}, 182:\penalty0 115102, 2021.

\bibitem[Mouatadid et~al.(2023)Mouatadid, Orenstein, Flaspohler, Oprescu, Cohen, Wang, Knight, Geogdzhayeva, Levang, Fraenkel, and Mackey]{mouatadid2023subseasonalclimateusa}
Mouatadid, S., Orenstein, P., Flaspohler, G.~E., Oprescu, M., Cohen, J., Wang, F., Knight, S.~E., Geogdzhayeva, M., Levang, S.~J., Fraenkel, E., and Mackey, L.
\newblock Subseasonalclimate{USA}: A dataset for subseasonal forecasting and benchmarking.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.
\newblock URL \url{https://openreview.net/forum?id=pWkrU6raMt}.

\bibitem[Nguyen et~al.(2023)Nguyen, Jewik, Bansal, Sharma, and Grover]{nguyen2023climatelearn}
Nguyen, T., Jewik, J.~K., Bansal, H., Sharma, P., and Grover, A.
\newblock Climatelearn: Benchmarking machine learning for weather and climate modeling.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.
\newblock URL \url{https://openreview.net/forum?id=RZJEkLFlPx}.

\bibitem[Nie et~al.(2023)Nie, Nguyen, Sinthong, and Kalagnanam]{nie2023patchtst}
Nie, Y., Nguyen, N.~H., Sinthong, P., and Kalagnanam, J.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Jbdc0vTOcol}.

\bibitem[Oreshkin et~al.(2020)Oreshkin, Carpov, Chapados, and Bengio]{oreshkin2020nbeats}
Oreshkin, B.~N., Carpov, D., Chapados, N., and Bengio, Y.
\newblock N-beats: Neural basis expansion analysis for interpretable time series forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=r1ecqn4YwB}.

\bibitem[Park et~al.(2022)Park, Maddix, Aubet, Kan, Gasthaus, and Wang]{park2022quantile}
Park, Y., Maddix, D., Aubet, F.-X., Kan, K., Gasthaus, J., and Wang, Y.
\newblock Learning quantile functions without quantile crossing for distribution-free time series forecasting.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  8127--8150. PMLR, 2022.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Rasul et~al.(2023)Rasul, Ashok, Williams, Khorasani, Adamopoulos, Bhagwatkar, Biloš, Ghonia, Hassen, Schneider, Garg, Drouin, Chapados, Nevmyvaka, and Rish]{rasul2023lagllama}
Rasul, K., Ashok, A., Williams, A.~R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Biloš, M., Ghonia, H., Hassen, N.~V., Schneider, A., Garg, S., Drouin, A., Chapados, N., Nevmyvaka, Y., and Rish, I.
\newblock Lag-llama: Towards foundation models for time series forecasting, 2023.

\bibitem[Richardson et~al.(2023)Richardson, Cook, Crane, Dunnington, François, Keane, Moldovan-Grünfeld, Ooms, Wujciak-Jens, and {Apache Arrow}]{richardson2023arrow}
Richardson, N., Cook, I., Crane, N., Dunnington, D., François, R., Keane, J., Moldovan-Grünfeld, D., Ooms, J., Wujciak-Jens, J., and {Apache Arrow}.
\newblock \emph{arrow: Integration to 'Apache' 'Arrow'}, 2023.
\newblock URL \url{https://github.com/apache/arrow/}.
\newblock R package version 14.0.2, https://arrow.apache.org/docs/r/.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and Januschowski]{salinas2020deepar}
Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent networks.
\newblock \emph{International Journal of Forecasting}, 36\penalty0 (3):\penalty0 1181--1191, 2020.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Shazeer, N.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Trindade(2015)]{trindade2015electricity}
Trindade, A.
\newblock {ElectricityLoadDiagrams20112014}.
\newblock UCI Machine Learning Repository, 2015.
\newblock {DOI}: https://doi.org/10.24432/C58C86.

\bibitem[Van~Ness et~al.(2023)Van~Ness, Shen, Wang, Jin, Maddix, and Gopalswamy]{van2023cross}
Van~Ness, M., Shen, H., Wang, H., Jin, X., Maddix, D.~C., and Gopalswamy, K.
\newblock Cross-frequency time series meta-forecasting.
\newblock \emph{arXiv preprint arXiv:2302.02077}, 2023.

\bibitem[van Panhuis et~al.(2018)van Panhuis, Cross, and Burke]{van2018tycho}
van Panhuis, W.~G., Cross, A., and Burke, D.~S.
\newblock Project tycho 2.0: a repository to improve the integration and reuse of data for global population health.
\newblock \emph{Journal of the American Medical Informatics Association}, 25:\penalty0 1608--1617, 2018.

\bibitem[Walmart Competition~Admin(2014)]{walmart2014sales}
Walmart Competition~Admin, W.~C.
\newblock Walmart recruiting - store sales forecasting, 2014.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Jiang, Jiang, Han, and Zhao]{wang2023libcity}
Wang, J., Jiang, J., Jiang, W., Han, C., and Zhao, W.~X.
\newblock Towards efficient and comprehensive urban spatial-temporal prediction: A unified library and performance benchmark.
\newblock \emph{arXiv preprint arXiv:2304.14343}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Wen, Zhang, Sun, Von~Krannichfeldt, and Wang]{wang2023proenfo}
Wang, Z., Wen, Q., Zhang, C., Sun, L., Von~Krannichfeldt, L., and Wang, Y.
\newblock Benchmarks and custom package for electrical load forecasting.
\newblock \emph{arXiv preprint arXiv:2307.07191}, 2023{\natexlab{b}}.

\bibitem[{Wikipedia contributors}(2024)]{wiki2023moirai}
{Wikipedia contributors}.
\newblock Moirai --- {W}ikipedia{,} the free encyclopedia, 2024.
\newblock URL \url{https://en.wikipedia.org/wiki/Moirai}.
\newblock [Online; accessed 21-January-2024].

\bibitem[Woo et~al.(2022)Woo, Liu, Sahoo, Kumar, and Hoi]{woo2022cost}
Woo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S.
\newblock Co{ST}: Contrastive learning of disentangled seasonal-trend representations for time series forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=PilZY3omXV2}.

\bibitem[Woo et~al.(2023)Woo, Liu, Kumar, and Sahoo]{woo2023pushing}
Woo, G., Liu, C., Kumar, A., and Sahoo, D.
\newblock Pushing the limits of pre-training for time series forecasting in the cloudops domain.
\newblock \emph{arXiv preprint arXiv:2310.05063}, 2023.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Wu, H., Xu, J., Wang, J., and Long, M.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22419--22430, 2021.

\bibitem[Wu et~al.(2023)Wu, Hu, Liu, Zhou, Wang, and Long]{wu2023timesnet}
Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ju_Uqw384Oq}.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layer}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10524--10533. PMLR, 2020.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Yang, G., Hu, E.~J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Gupta, Upadhyay, He, Goel, and Paul]{yang2022tableformer}
Yang, J., Gupta, A., Upadhyay, S., He, L., Goel, R., and Paul, S.
\newblock {T}able{F}ormer: Robust transformer modeling for table-text encoding.
\newblock In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  528--537, Dublin, Ireland, May 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.40}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.40}.

\bibitem[Yu et~al.(2016)Yu, Rao, and Dhillon]{yu2016temporal}
Yu, H.-F., Rao, N., and Dhillon, I.~S.
\newblock Temporal regularized matrix factorization for high-dimensional time series prediction.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Yue et~al.(2022)Yue, Wang, Duan, Yang, Huang, Tong, and Xu]{yue2022ts2vec}
Yue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., and Xu, B.
\newblock Ts2vec: Towards universal representation of time series.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  8980--8987, 2022.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{zeng2023dlinear}
Zeng, A., Chen, M., Zhang, L., and Xu, Q.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~37, pp.\  11121--11128, 2023.

\bibitem[Zerveas et~al.(2021)Zerveas, Jayaraman, Patel, Bhamidipaty, and Eickhoff]{zerveas2021transformer}
Zerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and Eickhoff, C.
\newblock A transformer-based framework for multivariate time series representation learning.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD conference on knowledge discovery \& data mining}, pp.\  2114--2124, 2021.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{zhang2019root}
Zhang, B. and Sennrich, R.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Wen, Zhang, Cai, Jin, Liu, Zhang, Liang, Pang, Song, et~al.]{zhang2023ssl}
Zhang, K., Wen, Q., Zhang, C., Cai, R., Jin, M., Liu, Y., Zhang, J., Liang, Y., Pang, G., Song, D., et~al.
\newblock Self-supervised learning for time series analysis: Taxonomy, progress, and prospects.
\newblock \emph{arXiv preprint arXiv:2306.10125}, 2023.

\bibitem[Zhang \& Yan(2023)Zhang and Yan]{zhang2023crossformer}
Zhang, Y. and Yan, J.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=vSVLM2j9eie}.

\bibitem[Zheng et~al.(2015)Zheng, Yi, Li, Li, Shan, Chang, and Li]{zheng2015chinaair}
Zheng, Y., Yi, X., Li, M., Li, R., Shan, Z., Chang, E., and Li, T.
\newblock Forecasting fine-grained air quality based on big data.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining}, pp.\  2267--2276, 2015.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Lu, Xiao, Su, Lyu, Ma, and Dou]{zhou2022kddcup2022}
Zhou, J., Lu, X., Xiao, Y., Su, J., Lyu, J., Ma, Y., and Dou, D.
\newblock Sdwpf: A dataset for spatial dynamic wind power forecasting challenge at kdd cup 2022.
\newblock \emph{arXiv preprint arXiv:2208.04360}, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Ma, Wen, Wang, Sun, and Jin]{zhou2022fedformer}
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R.
\newblock {FEDformer}: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In \emph{Proc. 39th International Conference on Machine Learning (ICML 2022)}, 2022{\natexlab{b}}.

\bibitem[Zhou et~al.(2023)Zhou, Niu, Wang, Sun, and Jin]{zhou2023gpt4ts}
Zhou, T., Niu, P., Wang, X., Sun, L., and Jin, R.
\newblock One fits all: Power general time series analysis by pretrained {LM}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=gMS6FVZvmF}.

\end{thebibliography}
