\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2018)]{allen2018natasha}
Allen-Zhu, Z.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2675--2686, 2018.

\bibitem[Anil et~al.(2019)Anil, Gupta, Koren, and Singer]{Anil2019MemoryEA}
Anil, R., Gupta, V., Koren, T., and Singer, Y.
\newblock Memory efficient adaptive optimization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Arnold et~al.(2019)Arnold, Manzagol, Babanezhad, Mitliagkas, and
  Roux]{arnold2019reducing}
Arnold, S.~M., Manzagol, P.-A., Babanezhad, R., Mitliagkas, I., and Roux, N.~L.
\newblock Reducing the variance in online optimization by transporting past
  gradients.
\newblock \emph{arXiv preprint arXiv:1906.03532}, 2019.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{carmon2017convex}
Carmon, Y., Duchi, J.~C., Hinder, O., and Sidford, A.
\newblock Convex until proven guilty: Dimension-free acceleration of gradient
  descent on non-convex functions.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  654--663. JMLR. org, 2017.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-1423}.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{duchi10adagrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2010.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  689--699, 2018.

\bibitem[Fang et~al.(2019)Fang, Lin, and Zhang]{fang2019sharp}
Fang, C., Lin, Z., and Zhang, T.
\newblock Sharp analysis for nonconvex sgd escaping from saddle points.
\newblock In \emph{Conference on Learning Theory}, pp.\  1192--1234, 2019.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017.

\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015beyond}
Hazan, E., Levy, K., and Shalev-Shwartz, S.
\newblock Beyond convexity: Stochastic quasi-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1594--1602, 2015.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2015DeepRL}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2015.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  315--323, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li \& Orabona(2019)Li and Orabona]{li2019convergence}
Li, X. and Orabona, F.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  983--992, 2019.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.~E.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pp.\  543--547, 1983.

\bibitem[Polyak(1964)]{polyak1964some}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{Raffel2019ExploringTL}
Raffel, C., Shazeer, N., Roberts, A.~K., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{ArXiv}, abs/1910.10683, 2019.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018on}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{Shazeer2018AdafactorAL}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{ICML}, 2018.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and
  Dean, J.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer, 2017.

\bibitem[Tran-Dinh et~al.(2019)Tran-Dinh, Pham, Phan, and
  Nguyen]{tran2019hybrid}
Tran-Dinh, Q., Pham, N.~H., Phan, D.~T., and Nguyen, L.~M.
\newblock A hybrid stochastic optimization framework for stochastic composite
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1907.03793}, 2019.

\bibitem[Tripuraneni et~al.(2018)Tripuraneni, Stern, Jin, Regier, and
  Jordan]{tripuraneni2018stochastic}
Tripuraneni, N., Stern, M., Jin, C., Regier, J., and Jordan, M.~I.
\newblock Stochastic cubic regularization for fast nonconvex optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2899--2908, 2018.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Ward, R., Wu, X., and Bottou, L.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6677--6686, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2019)You, Li, Hseu, Song, Demmel, and
  Hsieh]{you2019reducing}
You, Y., Li, J., Hseu, J., Song, X., Demmel, J., and Hsieh, C.-J.
\newblock Reducing bert pre-training time from 3 days to 76 minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  9793--9803, 2018.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{zhou2018stochastic}
Zhou, D., Xu, P., and Gu, Q.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  3925--3936. Curran Associates Inc.,
  2018.

\end{thebibliography}
