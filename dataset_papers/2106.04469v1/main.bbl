\begin{thebibliography}{}

\bibitem[Bazerque and Giannakis, 2009]{bazerque2009distributed}
Bazerque, J.~A. and Giannakis, G.~B. (2009).
\newblock Distributed spectrum sensing for cognitive radio networks by
  exploiting sparsity.
\newblock {\em IEEE Transactions on Signal Processing}, 58(3):1847--1862.

\bibitem[Beck et~al., 2014]{beck20141}
Beck, A., Nedi{\'c}, A., Ozdaglar, A., and Teboulle, M. (2014).
\newblock An $ o (1/k) $ gradient method for network resource allocation
  problems.
\newblock {\em IEEE Transactions on Control of Network Systems}, 1(1):64--73.

\bibitem[Chang and Lin, 2011]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J. (2011).
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):1--27.

\bibitem[Condat et~al., 2019]{condat2019proximal}
Condat, L., Kitahara, D., Contreras, A., and Hirabayashi, A. (2019).
\newblock Proximal splitting algorithms: A tour of recent advances, with new
  twists.
\newblock {\em arXiv preprint arXiv:1912.00137}.

\bibitem[Dvinskikh and Gasnikov, 2019]{dvinskikh2019decentralized}
Dvinskikh, D. and Gasnikov, A. (2019).
\newblock Decentralized and parallelized primal and dual accelerated methods
  for stochastic convex programming problems.
\newblock {\em arXiv preprint arXiv:1904.09015}.

\bibitem[Gan et~al., 2012]{gan2012optimal}
Gan, L., Topcu, U., and Low, S.~H. (2012).
\newblock Optimal decentralized protocol for electric vehicle charging.
\newblock {\em IEEE Transactions on Power Systems}, 28(2):940--951.

\bibitem[Giselsson et~al., 2013]{giselsson2013accelerated}
Giselsson, P., Doan, M.~D., Keviczky, T., De~Schutter, B., and Rantzer, A.
  (2013).
\newblock Accelerated gradient methods and dual decomposition in distributed
  model predictive control.
\newblock {\em Automatica}, 49(3):829--833.

\bibitem[Gorbunov et~al., 2020]{gorbunov2020linearly}
Gorbunov, E., Kovalev, D., Makarenko, D., and Richt{\'a}rik, P. (2020).
\newblock Linearly converging error compensated {SGD}.
\newblock In {\em Neural Information Processing Systems}.

\bibitem[Karimireddy et~al., 2019]{karimireddy2019error}
Karimireddy, S.~P., Rebjock, Q., Stich, S., and Jaggi, M. (2019).
\newblock Error feedback fixes {SignSGD} and other gradient compression
  schemes.
\newblock In {\em International Conference on Machine Learning}, pages
  3252--3261. PMLR.

\bibitem[Kolar et~al., 2010]{kolar2010estimating}
Kolar, M., Song, L., Ahmed, A., Xing, E.~P., et~al. (2010).
\newblock Estimating time-varying networks.
\newblock {\em Annals of Applied Statistics}, 4(1):94--123.

\bibitem[Kone{\v{c}}n{\'y} et~al., 2016]{konevcny2016federated}
Kone{\v{c}}n{\'y}, J., McMahan, H.~B., Yu, F.~X., Richt{\'a}rik, P., Suresh,
  A.~T., and Bacon, D. (2016).
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}.

\bibitem[Kovalev et~al., 2020]{kovalev2020optimal}
Kovalev, D., Salim, A., and Richt{\'a}rik, P. (2020).
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Kovalev et~al., 2021]{kovalev2021adom}
Kovalev, D., Shulgin, E., Richt{\'a}rik, P., Rogozin, A., and Gasnikov, A.
  (2021).
\newblock {ADOM}: {A}ccelerated decentralized optimization method for
  time-varying networks.
\newblock {\em arXiv preprint arXiv:2102.09234}.

\bibitem[Li et~al., 2018]{li2018sharp}
Li, H., Fang, C., Yin, W., and Lin, Z. (2018).
\newblock A sharp convergence rate analysis for distributed accelerated
  gradient methods.
\newblock {\em arXiv preprint arXiv:1810.01053}.

\bibitem[Li and Lin, 2021]{li2021accelerated}
Li, H. and Lin, Z. (2021).
\newblock Accelerated gradient tracking over time-varying graphs for
  decentralized optimization.
\newblock {\em arXiv preprint arXiv:2104.02596}.

\bibitem[Li et~al., 2020]{li2020federated}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V. (2020).
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em IEEE Signal Processing Magazine}, 37(3):50--60.

\bibitem[Maros and Jald{\'e}n, 2018]{maros2018panda}
Maros, M. and Jald{\'e}n, J. (2018).
\newblock Panda: A dual linearly converging method for distributed optimization
  over time-varying undirected graphs.
\newblock In {\em 2018 IEEE Conference on Decision and Control (CDC)}, pages
  6520--6525. IEEE.

\bibitem[McMahan et~al., 2017]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A. (2017).
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR.

\bibitem[Nedic et~al., 2017]{nedic2017achieving}
Nedic, A., Olshevsky, A., and Shi, W. (2017).
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock {\em SIAM Journal on Optimization}, 27(4):2597--2633.

\bibitem[Nesterov, 2003]{nesterov2003introductory}
Nesterov, Y. (2003).
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media.

\bibitem[Pu et~al., 2020]{pu2020push}
Pu, S., Shi, W., Xu, J., and Nedic, A. (2020).
\newblock Push-pull gradient methods for distributed optimization in networks.
\newblock {\em IEEE Transactions on Automatic Control}.

\bibitem[Qu and Li, 2019]{qu2019accelerated}
Qu, G. and Li, N. (2019).
\newblock Accelerated distributed nesterov gradient descent.
\newblock {\em IEEE Transactions on Automatic Control}, 65(6):2566--2581.

\bibitem[Rabbat and Nowak, 2004]{rabbat2004distributed}
Rabbat, M. and Nowak, R. (2004).
\newblock Distributed optimization in sensor networks.
\newblock In {\em Proceedings of the 3rd International Symposium on Information
  Processing in Sensor Networks}, pages 20--27.

\bibitem[Rockafellar, 2015]{rockafellar2015convex}
Rockafellar, R.~T. (2015).
\newblock {\em Convex analysis}.
\newblock Princeton university press.

\bibitem[Rogozin et~al., 2020]{rogozin2020towards}
Rogozin, A., Lukoshkin, V., Gasnikov, A., Kovalev, D., and Shulgin, E. (2020).
\newblock Towards accelerated rates for distributed optimization over
  time-varying networks.
\newblock {\em arXiv preprint arXiv:2009.11069}.

\bibitem[Salim et~al., 2021]{salim2021optimal}
Salim, A., Condat, L., Kovalev, D., and Richt{\'a}rik, P. (2021).
\newblock An optimal algorithm for strongly convex minimization under affine
  constraints.
\newblock {\em arXiv preprint arXiv:2102.11079}.

\bibitem[Salim et~al., 2020]{salim2020dualize}
Salim, A., Condat, L., Mishchenko, K., and Richt{\'a}rik, P. (2020).
\newblock Dualize, split, randomize: fast nonsmooth optimization algorithms.
\newblock {\em arXiv preprint arXiv:2004.02635}.

\bibitem[Scaman et~al., 2017]{scaman2017optimal}
Scaman, K., Bach, F., Bubeck, S., Lee, Y.~T., and Massouli{\'e}, L. (2017).
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In {\em international conference on machine learning}, pages
  3027--3036. PMLR.

\bibitem[Stich and Karimireddy, 2019]{stich2019error}
Stich, S.~U. and Karimireddy, S.~P. (2019).
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock {\em arXiv preprint arXiv:1909.05350}.

\bibitem[Ye et~al., 2020]{ye2020multi}
Ye, H., Luo, L., Zhou, Z., and Zhang, T. (2020).
\newblock Multi-consensus decentralized accelerated gradient descent.
\newblock {\em arXiv preprint arXiv:2005.00797}.

\bibitem[Zadeh, 1961]{zadeh1961time}
Zadeh, L.~A. (1961).
\newblock Time-varying networks, i.
\newblock {\em Proceedings of the IRE}, 49(10):1488--1503.

\end{thebibliography}
