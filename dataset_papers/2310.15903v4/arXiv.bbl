\begin{thebibliography}{10}

\bibitem{behnia2023implicit}
Tina Behnia, Ganesh~Ramachandra Kini, Vala Vakilian, and Christos Thrampoulidis.
\newblock On the implicit geometry of cross-entropy parameterizations for label-imbalanced data.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 10815--10838. PMLR, 2023.

\bibitem{bengio2013representation}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 35(8):1798--1828, 2013.

\bibitem{blondel2020learning}
Mathieu Blondel, Andr{\'e}~FT Martins, and Vlad Niculae.
\newblock Learning with fenchel-young losses.
\newblock {\em Journal of Machine Learning Research}, 21:1--69, 2020.

\bibitem{brinker2006unified}
Klaus Brinker, Johannes F{\"u}rnkranz, and Eyke H{\"u}llermeier.
\newblock A unified model for multilabel classification and ranking.
\newblock In {\em Proceedings of the 2006 conference on ECAI 2006: 17th European Conference on Artificial Intelligence August 29--September 1, 2006, Riva del Garda, Italy}, pages 489--493, 2006.

\bibitem{chan2022redunet}
Kwan Ho~Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi~Ma.
\newblock Redunet: A white-box deep network from the principle of maximizing rate reduction.
\newblock {\em The Journal of Machine Learning Research}, 23(1):4907--5009, 2022.

\bibitem{chen2022perfectly}
Mayee Chen, Daniel~Y Fu, Avanika Narayan, Michael Zhang, Zhao Song, Kayvon Fatahalian, and Christopher R{\'e}.
\newblock Perfectly balanced: Improving transfer and robustness of supervised contrastive learning.
\newblock In {\em International Conference on Machine Learning}, pages 3090--3122. PMLR, 2022.

\bibitem{cheng2010bayes}
Weiwei Cheng, Eyke H{\"u}llermeier, and Krzysztof~J Dembczynski.
\newblock Bayes optimal multilabel classification via probabilistic classifier chains.
\newblock In {\em International Conference on Machine Learning}, pages 279--286, 2010.

\bibitem{cybenko1989approximation}
G~Cybenko.
\newblock Approximation by superposition of sigmoidal functions.
\newblock {\em Mathematics of Control, Signals and Systems}, 2(4):303--314, 1989.

\bibitem{dang2023neural}
Hien Dang, Tho Tran, Stanley Osher, Hung Tran-The, Nhat Ho, and Tan Nguyen.
\newblock Neural collapse in deep linear networks: From balanced to imbalanced data.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{dembczynski2010regret}
Krzysztof Dembczy{\'n}ski, Willem Waegeman, Weiwei Cheng, and Eyke H{\"u}llermeier.
\newblock Regret analysis for performance metrics in multi-label classification: the case of hamming and subset zero-one loss.
\newblock In {\em Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part I 21}, pages 280--295. Springer, 2010.

\bibitem{fang2021exploring}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(43):e2103091118, 2021.

\bibitem{galanti2022note}
Tomer Galanti.
\newblock A note on the implicit bias towards minimal depth of deep neural networks.
\newblock {\em arXiv preprint arXiv:2202.09028}, 2022.

\bibitem{galanti2022generalization}
Tomer Galanti, Andr{\'a}s Gy{\"o}rgy, and Marcus Hutter.
\newblock Generalization bounds for transfer learning with pretrained classifiers.
\newblock {\em arXiv preprint arXiv:2212.12532}, 2022.

\bibitem{galanti2022role}
Tomer Galanti, Andr{\'a}s Gy{\"o}rgy, and Marcus Hutter.
\newblock On the role of neural collapse in transfer learning.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{gao2023study}
Peifeng Gao, Qianqian Xu, Peisong Wen, Huiyang Shao, Zhiyong Yang, and Qingming Huang.
\newblock A study of neural collapse phenomenon: Grassmannian frame, symmetry, generalization.
\newblock {\em arXiv preprint arXiv:2304.08914}, 2023.

\bibitem{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points---online stochastic gradient for tensor decomposition.
\newblock In {\em Proceedings of The 28th Conference on Learning Theory}, pages 797--842, 2015.

\bibitem{graf2021dissecting}
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt.
\newblock Dissecting supervised contrastive learning.
\newblock In {\em International Conference on Machine Learning}, pages 3821--3830. PMLR, 2021.

\bibitem{han2022neural}
XY~Han, Vardan Papyan, and David~L Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the central path.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{he2022law}
Hangfeng He and Weijie~J. Su.
\newblock A law of data separation in deep learning.
\newblock {\em Proceedings of the National Academy of Sciences}, 120(36):e2221704120, 2023.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and Pattern Recognition}, pages 770--778, 2016.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems}, 33:6840--6851, 2020.

\bibitem{hui2022limitations}
Like Hui, Mikhail Belkin, and Preetum Nakkiran.
\newblock Limitations of neural collapse for understanding generalization in deep learning.
\newblock {\em arXiv preprint arXiv:2202.08384}, 2022.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In {\em International Conference on Machine Learning}, pages 448--456. PMLR, 2015.

\bibitem{ji2022unconstrained}
Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie~J Su.
\newblock An unconstrained layer-peeled perspective on neural collapse.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{jiang2023generalized}
Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, and Zhihui Zhu.
\newblock Generalized neural collapse for a large number of classes.
\newblock {\em arXiv preprint arXiv:2310.05351}, 2023.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International conference on machine learning}, pages 1724--1732. PMLR, 2017.

\bibitem{kothapalli2023neural}
Vignesh Kothapalli.
\newblock Neural collapse: A review on modelling principles and generalization.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis, Department of Computer Science, University of Toronto}, 2009.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444, 2015.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database. at\&t labs, 2010.

\bibitem{lee2019first}
Jason~D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock First-order methods almost always avoid strict saddle points.
\newblock {\em Mathematical Programming}, 176:311--337, 2019.

\bibitem{li2022principled}
Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, and Qing Qu.
\newblock Principled and efficient transfer learning of deep models via neural collapse.
\newblock {\em arXiv preprint arXiv:2212.12206}, 2022.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2980--2988, 2017.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem{liu2021emerging}
Weiwei Liu, Haobo Wang, Xiaobo Shen, and Ivor~W Tsang.
\newblock The emerging trends of multi-label learning.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 44(11):7955--7974, 2021.

\bibitem{liu2023generalizing}
Weiyang Liu, Longhui Yu, Adrian Weller, and Bernhard Sch{\"o}lkopf.
\newblock Generalizing and decoupling neural collapse via hyperspherical uniformity gap.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{loshchilov2017sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{lu2020neural}
Jianfeng Lu and Stefan Steinerberger.
\newblock Neural collapse under cross-entropy loss.
\newblock {\em Applied and Computational Harmonic Analysis}, 59:224--241, 2022.
\newblock Special Issue on Harmonic Analysis and Machine Learning.

\bibitem{lu2022importance}
Yiping Lu, Wenlong Ji, Zachary Izzo, and Lexing Ying.
\newblock Importance tempering: Group robustness for overparameterized models.
\newblock {\em arXiv preprint arXiv:2209.08745}, 2022.

\bibitem{menon2019multilabel}
Aditya~K Menon, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
\newblock Multilabel reductions: what is my loss optimising?
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{mixon2020neural}
Dustin~G Mixon, Hans Parshall, and Jianzong Pi.
\newblock Neural collapse with unconstrained features.
\newblock {\em Sampling Theory, Signal Processing, and Data Analysis}, 2022.

\bibitem{moyano2018review}
Jose~M Moyano, Eva~L Gibaja, Krzysztof~J Cios, and Sebasti{\'a}n Ventura.
\newblock Review of ensembles of multi-label classifiers: models, experimental study and prospects.
\newblock {\em Information Fusion}, 44:33--45, 2018.

\bibitem{mukhoti2020calibrating}
Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania.
\newblock Calibrating deep neural networks using focal loss.
\newblock {\em Advances in Neural Information Processing Systems}, 33:15288--15299, 2020.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}, 2011.

\bibitem{papyan2020traces}
Vardan Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock {\em Journal of Machine Learning Research}, 21(252):1--64, 2020.

\bibitem{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock {\em Proceedings of the National Academy of Sciences}, 117(40):24652--24663, 2020.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{rangamani2022neural}
Akshay Rangamani and Andrzej Banburski-Fahey.
\newblock Neural collapse in deep homogeneous classifiers and the role of weight decay.
\newblock In {\em ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 4243--4247. IEEE, 2022.

\bibitem{rangamani2023feature}
Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso~A Poggio.
\newblock Feature learning in deep classifiers through intermediate neural collapse.
\newblock In {\em International Conference on Machine Learning}, pages 28729--28745. PMLR, 2023.

\bibitem{reddi2019stochastic}
Sashank~J Reddi, Satyen Kale, Felix Yu, Daniel Holtmann-Rice, Jiecao Chen, and Sanjiv Kumar.
\newblock Stochastic negative mining for learning with large output spaces.
\newblock In {\em The 22nd International Conference on Artificial Intelligence and Statistics}, pages 1940--1949. PMLR, 2019.

\bibitem{sharma2023learning}
Saurabh Sharma, Yongqin Xian, Ning Yu, and Ambuj Singh.
\newblock Learning prototype classifiers for long-tailed recognition.
\newblock {\em arXiv preprint arXiv:2302.00491}, 2023.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em CoRR}, abs/1409.1556, 2014.

\bibitem{smith2022cyclical}
Leslie~N Smith.
\newblock Cyclical focal loss.
\newblock {\em arXiv preprint arXiv:2202.08978}, 2022.

\bibitem{sun2015nonconvex}
Ju~Sun, Qing Qu, and John Wright.
\newblock When are nonconvex problems not scary?
\newblock In {\em NIPS Workshop on Nonconvex Optimization for Machine Learning}, 2015.

\bibitem{sun2016complete}
Ju~Sun, Qing Qu, and John Wright.
\newblock Complete dictionary recovery over the sphere ii: Recovery by riemannian trust-region method.
\newblock {\em IEEE Transactions on Information Theory}, 63(2):885--914, 2016.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and Pattern Recognition}, pages 2818--2826, 2016.

\bibitem{thrampoulidis2022imbalance}
Christos Thrampoulidis, Ganesh~Ramachandra Kini, Vala Vakilian, and Tina Behnia.
\newblock Imbalance trouble: Revisiting neural-collapse geometry.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~35, pages 27225--27238, 2022.

\bibitem{tirer2022extended}
Tom Tirer and Joan Bruna.
\newblock Extended unconstrained features model for exploring deep neural collapse.
\newblock In {\em International Conference on Machine Learning}, 2022.

\bibitem{trabucco2023effective}
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov.
\newblock Effective data augmentation with diffusion models.
\newblock {\em arXiv preprint arXiv:2302.07944}, 2023.

\bibitem{wang2022linear}
Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu.
\newblock Linear convergence analysis of neural collapse with unconstrained features.
\newblock In {\em OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}, 2022.

\bibitem{wang2023far}
Zijian Wang, Yadan Luo, Liang Zheng, Zi~Huang, and Mahsa Baktashmotlagh.
\newblock How far pre-trained models are from neural collapse on the target dataset informs their transferability.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5549--5558, 2023.

\bibitem{xie2023neural}
Liang Xie, Yibo Yang, Deng Cai, and Xiaofei He.
\newblock Neural collapse inspired attraction-repulsion-balanced loss for imbalanced learning.
\newblock {\em Neurocomputing}, 2023.

\bibitem{xie2022hidden}
Shuo Xie, Jiahao Qiu, Ankita Pasad, Li~Du, Qing Qu, and Hongyuan Mei.
\newblock Hidden state variability of pretrained language models can guide computation reduction for transfer learning.
\newblock In {\em Empirical Methods in Natural Language Processing}, 2022.

\bibitem{yang2022inducing}
Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao.
\newblock Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network?
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{yang2023neural}
Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng Tao.
\newblock Neural collapse inspired feature-classifier alignment for few-shot class-incremental learning.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{yaras2023law}
Can Yaras, Peng Wang, Wei Hu, Zhihui Zhu, Laura Balzano, and Qing Qu.
\newblock The law of parsimony in gradient descent for learning deep linear networks.
\newblock {\em arXiv preprint arXiv:2306.01154}, 2023.

\bibitem{yaras2022neural}
Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu.
\newblock Neural collapse with normalized features: A geometric analysis over the riemannian manifold.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{yu2023continual}
Longhui Yu, Tianyang Hu, Lanqing HONG, Zhen Liu, Adrian Weller, and Weiyang Liu.
\newblock Continual learning by modeling intra-class variation.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{yu2020learning}
Yaodong Yu, Kwan Ho~Ryan Chan, Chong You, Chaobing Song, and Yi~Ma.
\newblock Learning diverse and discriminative representations via the principle of maximal coding rate reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 33:9422--9434, 2020.

\bibitem{zhai2023investigating}
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu~Cai, Qing Qu, Yong~Jae Lee, and Yi~Ma.
\newblock Investigating the catastrophic forgetting in multimodal large language models.
\newblock {\em arXiv preprint arXiv:2309.10313}, 2023.

\bibitem{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking generalization.
\newblock {\em Communications of the ACM}, 64(3):107--115, 2021.

\bibitem{zhang2023emergence}
Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, and Qing Qu.
\newblock The emergence of reproducibility and consistency in diffusion models.
\newblock {\em arXiv preprint arXiv:2310.05264}, 2023.

\bibitem{zhang2020symmetry}
Yuqian Zhang, Qing Qu, and John Wright.
\newblock From symmetry to geometry: Tractable nonconvex problems.
\newblock {\em arXiv preprint arXiv:2007.06753}, 2020.

\bibitem{zhong2023understanding}
Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia.
\newblock Understanding imbalanced semantic segmentation through neural collapse.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19550--19560, 2023.

\bibitem{zhou2022optimization}
Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu.
\newblock On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features.
\newblock In {\em International Conference on Machine Learning}, pages 27179--27202. PMLR, 2022.

\bibitem{zhou2022all}
Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu.
\newblock Are all losses created equal: A neural collapse perspective.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{zhu2021geometric}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock {\em Advances in Neural Information Processing Systems}, 34:29820--29834, 2021.

\end{thebibliography}
