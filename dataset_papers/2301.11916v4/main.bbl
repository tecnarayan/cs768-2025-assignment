\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyurek2022learning}
E.~Aky{\"u}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Bansal et~al.(2022)Bansal, Gopalakrishnan, Dingliwal, Bodapati,
  Kirchhoff, and Roth]{bansal2022rethinking}
H.~Bansal, K.~Gopalakrishnan, S.~Dingliwal, S.~Bodapati, K.~Kirchhoff, and
  D.~Roth.
\newblock Rethinking the role of scale for in-context learning: An
  interpretability-based case study at 66 billion scale.
\newblock \emph{arXiv preprint arXiv:2212.09095}, 2022.

\bibitem[Blei et~al.(2003)Blei, Ng, and Jordan]{10.5555/944919.944937}
D.~M. Blei, A.~Y. Ng, and M.~I. Jordan.
\newblock Latent dirichlet allocation.
\newblock \emph{J. Mach. Learn. Res.}, 3\penalty0 (null):\penalty0 993â€“1022,
  mar 2003.
\newblock ISSN 1532-4435.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond,
  McClelland, and Hill]{chan2022data}
S.~C. Chan, A.~Santoro, A.~K. Lampinen, J.~X. Wang, A.~Singh, P.~H. Richemond,
  J.~McClelland, and F.~Hill.
\newblock Data distributional properties drive emergent few-shot learning in
  transformers.
\newblock \emph{arXiv preprint arXiv:2205.05055}, 2022.

\bibitem[Chatterjee et~al.(2019)Chatterjee, Narahari, Joshi, and
  Agrawal]{chatterjee-etal-2019-semeval}
A.~Chatterjee, K.~N. Narahari, M.~Joshi, and P.~Agrawal.
\newblock Semeval-2019 task 3: Emocontext contextual emotion detection in text.
\newblock In \emph{Proceedings of the 13th International Workshop on Semantic
  Evaluation}, pages 39--48, Minneapolis, Minnesota, USA, 2019. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/S19-2005}.
\newblock URL \url{https://www.aclweb.org/anthology/S19-2005}.

\bibitem[Chen et~al.(2023)Chen, Chen, and Zhou]{chen2023takes}
J.~Chen, L.~Chen, and T.~Zhou.
\newblock It takes one to tango but more make trouble? in-context training with
  different number of demonstrations.
\newblock \emph{arXiv preprint arXiv:2303.08119}, 2023.

\bibitem[Chen et~al.(2022)Chen, Ma, Wang, and Cohen]{chen2022program}
W.~Chen, X.~Ma, X.~Wang, and W.~W. Cohen.
\newblock Program of thoughts prompting: Disentangling computation from
  reasoning for numerical reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.12588}, 2022.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dai et~al.(2022)Dai, Sun, Dong, Hao, Sui, and Wei]{dai2022can}
D.~Dai, Y.~Sun, L.~Dong, Y.~Hao, Z.~Sui, and F.~Wei.
\newblock Why can gpt learn in-context? language models secretly perform
  gradient descent as meta optimizers.
\newblock \emph{arXiv preprint arXiv:2212.10559}, 2022.

\bibitem[Devroye et~al.(1996)Devroye, Gy{\"o}rfi, and Lugosi]{Devroye1996APT}
L.~Devroye, L.~Gy{\"o}rfi, and G.~Lugosi.
\newblock A probabilistic theory of pattern recognition.
\newblock In \emph{Stochastic Modelling and Applied Probability}, 1996.

\bibitem[Hahn and Goyal(2023)]{hahn2023theory}
M.~Hahn and N.~Goyal.
\newblock A theory of emergent in-context learning as implicit structure
  induction.
\newblock \emph{arXiv preprint arXiv:2303.07971}, 2023.

\bibitem[Han et~al.(2023)Han, Simig, Mihaylov, Tsvetkov, Celikyilmaz, and
  Wang]{han2023understanding}
X.~Han, D.~Simig, T.~Mihaylov, Y.~Tsvetkov, A.~Celikyilmaz, and T.~Wang.
\newblock Understanding in-context learning via supportive pretraining data.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 12660--12673,
  2023.

\bibitem[Jiang(2023)]{jiang2023latent}
H.~Jiang.
\newblock A latent space theory for emergent abilities in large language
  models.
\newblock \emph{arXiv preprint arXiv:2304.09960}, 2023.

\bibitem[LeBrun et~al.(2022)LeBrun, Sordoni, and
  O'Donnell]{lebrun2022evaluating}
B.~LeBrun, A.~Sordoni, and T.~J. O'Donnell.
\newblock Evaluating distributional distortion in neural language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Lehmann et~al.(2015)Lehmann, Isele, Jakob, Jentzsch, Kontokostas,
  Mendes, Hellmann, Morsey, Van~Kleef, Auer, et~al.]{lehmann2015dbpedia}
J.~Lehmann, R.~Isele, M.~Jakob, A.~Jentzsch, D.~Kontokostas, P.~N. Mendes,
  S.~Hellmann, M.~Morsey, P.~Van~Kleef, S.~Auer, et~al.
\newblock Dbpedia--a large-scale, multilingual knowledge base extracted from
  wikipedia.
\newblock \emph{Semantic web}, 6\penalty0 (2):\penalty0 167--195, 2015.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
B.~Lester, R.~Al-Rfou, and N.~Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, 2021.

\bibitem[Li et~al.(2023)Li, Ildiz, Papailiopoulos, and
  Oymak]{li2023transformers}
Y.~Li, M.~E. Ildiz, D.~Papailiopoulos, and S.~Oymak.
\newblock Transformers as algorithms: Generalization and implicit model
  selection in in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.07067}, 2023.

\bibitem[Liu et~al.(2022)Liu, Shen, Zhang, Dolan, Carin, and
  Chen]{liu-etal-2022-makes}
J.~Liu, D.~Shen, Y.~Zhang, B.~Dolan, L.~Carin, and W.~Chen.
\newblock What makes good in-context examples for {GPT}-3?
\newblock In \emph{Proceedings of Deep Learning Inside Out (DeeLIO 2022): The
  3rd Workshop on Knowledge Extraction and Integration for Deep Learning
  Architectures}, pages 100--114, Dublin, Ireland and Online, May 2022.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.deelio-1.10}.
\newblock URL \url{https://aclanthology.org/2022.deelio-1.10}.

\bibitem[Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and
  Stenetorp]{lu2022fantastically}
Y.~Lu, M.~Bartolo, A.~Moore, S.~Riedel, and P.~Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 8086--8098,
  2022.

\bibitem[Malo et~al.(2014)Malo, Sinha, Korhonen, Wallenius, and
  Takala]{Malo2014GoodDO}
P.~Malo, A.~Sinha, P.~Korhonen, J.~Wallenius, and P.~Takala.
\newblock Good debt or bad debt: Detecting semantic orientations in economic
  texts.
\newblock \emph{Journal of the Association for Information Science and
  Technology}, 65, 2014.

\bibitem[Miao et~al.(2016)Miao, Yu, and Blunsom]{pmlr-v48-miao16}
Y.~Miao, L.~Yu, and P.~Blunsom.
\newblock Neural variational inference for text processing.
\newblock In M.~F. Balcan and K.~Q. Weinberger, editors, \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pages 1727--1736, New York,
  New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v48/miao16.html}.

\bibitem[Miao et~al.(2017)Miao, Grefenstette, and Blunsom]{miao2017discovering}
Y.~Miao, E.~Grefenstette, and P.~Blunsom.
\newblock Discovering discrete latent topics with neural variational inference.
\newblock In \emph{International conference on machine learning}, pages
  2410--2419. PMLR, 2017.

\bibitem[Min et~al.(2022{\natexlab{a}})Min, Lewis, Hajishirzi, and
  Zettlemoyer]{min2022noisy}
S.~Min, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer.
\newblock Noisy channel language model prompting for few-shot text
  classification.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 5316--5330,
  2022{\natexlab{a}}.

\bibitem[Min et~al.(2022{\natexlab{b}})Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min-etal-2022-metaicl}
S.~Min, M.~Lewis, L.~Zettlemoyer, and H.~Hajishirzi.
\newblock {M}eta{ICL}: Learning to learn in context.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2791--2809, Seattle, United States, July
  2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.201}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.201}.

\bibitem[Min et~al.(2022{\natexlab{c}})Min, Lyu, Holtzman, Artetxe, Lewis,
  Hajishirzi, and Zettlemoyer]{min2022rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and
  L.~Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock In \emph{EMNLP}, 2022{\natexlab{c}}.

\bibitem[Mollas et~al.(2020)Mollas, Chrysopoulou, Karlos, and
  Tsoumakas]{mollas2020ethos}
I.~Mollas, Z.~Chrysopoulou, S.~Karlos, and G.~Tsoumakas.
\newblock Ethos: an online hate speech detection dataset, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Perez et~al.(2021)Perez, Kiela, and Cho]{perez2021true}
E.~Perez, D.~Kiela, and K.~Cho.
\newblock True few-shot learning with language models.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ShnM-rRh4T}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Reimers and Gurevych(2019)]{reimers-2019-sentence-bert}
N.~Reimers and I.~Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics, 11
  2019.
\newblock URL \url{https://arxiv.org/abs/1908.10084}.

\bibitem[Rubin et~al.(2021)Rubin, Herzig, and Berant]{rubin2021learning}
O.~Rubin, J.~Herzig, and J.~Berant.
\newblock Learning to retrieve prompts for in-context learning.
\newblock \emph{arXiv preprint arXiv:2112.08633}, 2021.

\bibitem[Saravia et~al.(2018)Saravia, Liu, Huang, Wu, and
  Chen]{saravia-etal-2018-carer}
E.~Saravia, H.-C.~T. Liu, Y.-H. Huang, J.~Wu, and Y.-S. Chen.
\newblock {CARER}: Contextualized affect representations for emotion
  recognition.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 3687--3697, Brussels, Belgium, Oct.-Nov.
  2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1404}.
\newblock URL \url{https://www.aclweb.org/anthology/D18-1404}.

\bibitem[Saunshi et~al.(2021)Saunshi, Malladi, and Arora]{saunshi2021a}
N.~Saunshi, S.~Malladi, and S.~Arora.
\newblock A mathematical exploration of why language models help solve
  downstream tasks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=vVjIW3sEc1s}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher-etal-2013-recursive}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Ng, and C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pages 1631--1642, Seattle, Washington, USA,
  Oct. 2013. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/D13-1170}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
N.~Stiennon, L.~Ouyang, J.~Wu, D.~Ziegler, R.~Lowe, C.~Voss, A.~Radford,
  D.~Amodei, and P.~F. Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3008--3021, 2020.

\bibitem[Su et~al.(2022)Su, Kasai, Wu, Shi, Wang, Xin, Zhang, Ostendorf,
  Zettlemoyer, Smith, et~al.]{su2022selective}
H.~Su, J.~Kasai, C.~H. Wu, W.~Shi, T.~Wang, J.~Xin, R.~Zhang, M.~Ostendorf,
  L.~Zettlemoyer, N.~A. Smith, et~al.
\newblock Selective annotation makes language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2209.01975}, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher,
  Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami,
  Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann,
  Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov,
  Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva,
  Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang,
  Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{Touvron2023}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei,
  N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C.
  Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu,
  B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou,
  H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S.
  Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao,
  X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton,
  J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith,
  R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan,
  P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang,
  A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock 7 2023{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/2307.09288}.

\bibitem[van~der Maaten and Hinton(2008)]{JMLR:v9:vandermaaten08a}
L.~van~der Maaten and G.~Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (86):\penalty0 2579--2605, 2008.
\newblock URL \url{http://jmlr.org/papers/v9/vandermaaten08a.html}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[von Oswald et~al.(2022)von Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
J.~von Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev,
  A.~Zhmoginov, and M.~Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock \emph{arXiv preprint arXiv:2212.07677}, 2022.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{EMNLP 2018}, page 353, 2018.

\bibitem[Wang and Komatsuzaki(2021)]{gpt-j}
B.~Wang and A.~Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and
  Zhou]{wang2022self}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~Le, E.~Chi, and D.~Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Warstadt et~al.(2018)Warstadt, Singh, and Bowman]{warstadt2018neural}
A.~Warstadt, A.~Singh, and S.~R. Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{arXiv preprint arXiv:1805.12471}, 2018.

\bibitem[Wei et~al.(2021)Wei, Xie, and Ma]{wei2021why}
C.~Wei, S.~M. Xie, and T.~Ma.
\newblock Why do pretrained language models help in downstream tasks? an
  analysis of head and prompt tuning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, E.~Chi, Q.~Le, and D.~Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2022an}
S.~M. Xie, A.~Raghunathan, P.~Liang, and T.~Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=RdJVFCHjUMI}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{zhang2015character}
X.~Zhang, J.~Zhao, and Y.~LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Zhou et~al.(2022)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang,
  Schuurmans, Bousquet, Le, and Chi]{zhou2022least}
D.~Zhou, N.~Sch{\"a}rli, L.~Hou, J.~Wei, N.~Scales, X.~Wang, D.~Schuurmans,
  O.~Bousquet, Q.~Le, and E.~Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2205.10625}, 2022.

\end{thebibliography}
