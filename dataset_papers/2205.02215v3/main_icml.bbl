\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Navarro, Mattina, Whatmough, and
  Saligrama]{acar2021federated}
Acar, D. A.~E., Zhao, Y., Navarro, R.~M., Mattina, M., Whatmough, P.~N., and
  Saligrama, V.
\newblock Federated learning based on dynamic regularization.
\newblock \emph{arXiv preprint arXiv:2111.04263}, 2021.

\bibitem[Aiyoshi \& Shimizu(1984)Aiyoshi and Shimizu]{aiyoshi1984solution}
Aiyoshi, E. and Shimizu, K.
\newblock A solution method for the static constrained stackelberg problem via
  penalty method.
\newblock \emph{IEEE Transactions on Automatic Control}, 29\penalty0
  (12):\penalty0 1111--1114, 1984.

\bibitem[Al-Khayyal et~al.(1992)Al-Khayyal, Horst, and Pardalos]{al1992global}
Al-Khayyal, F.~A., Horst, R., and Pardalos, P.~M.
\newblock Global optimization of concave functions subject to quadratic
  constraints: an application in nonlinear bilevel programming.
\newblock \emph{Annals of Operations Research}, 34\penalty0 (1):\penalty0
  125--147, 1992.

\bibitem[Arora et~al.(2020)Arora, Du, Kakade, Luo, and
  Saunshi]{arora2020provable}
Arora, S., Du, S., Kakade, S., Luo, Y., and Saunshi, N.
\newblock Provable representation learning for imitation learning via bi-level
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  367--376. PMLR, 2020.

\bibitem[Barazandeh et~al.(2021{\natexlab{a}})Barazandeh, Huang, and
  Michailidis]{barazandeh2021decentralized}
Barazandeh, B., Huang, T., and Michailidis, G.
\newblock A decentralized adaptive momentum method for solving a class of
  min-max optimization problems.
\newblock \emph{Signal Processing}, 189:\penalty0 108245, 2021{\natexlab{a}}.

\bibitem[Barazandeh et~al.(2021{\natexlab{b}})Barazandeh, Tarzanagh, and
  Michailidis]{barazandeh2021solving}
Barazandeh, B., Tarzanagh, D.~A., and Michailidis, G.
\newblock Solving a class of non-convex min-max games using adaptive momentum
  methods.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  3625--3629. IEEE,
  2021{\natexlab{b}}.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{basu2019qsparse}
Basu, D., Data, D., Karakus, C., and Diggavi, S.
\newblock Qsparse-local-{SGD}: Distributed {SGD} with quantization,
  sparsification and local computations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  14668--14679, 2019.

\bibitem[Bertinetto et~al.(2018)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018meta}
Bertinetto, L., Henriques, J.~F., Torr, P.~H., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock \emph{arXiv preprint arXiv:1805.08136}, 2018.

\bibitem[Bracken \& McGill(1973)Bracken and McGill]{bracken1973mathematical}
Bracken, J. and McGill, J.~T.
\newblock Mathematical programs with optimization problems in the constraints.
\newblock \emph{Operations Research}, 21\penalty0 (1):\penalty0 37--44, 1973.

\bibitem[Brown(1951)]{brown1951iterative}
Brown, G.~W.
\newblock Iterative solution of games by fictitious play.
\newblock \emph{Activity analysis of production and allocation}, 13\penalty0
  (1):\penalty0 374--376, 1951.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Sun, and Yin]{chen2021closing}
Chen, T., Sun, Y., and Yin, W.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient
  methods for bilevel problems.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Sun, and Yin]{chen2021single}
Chen, T., Sun, Y., and Yin, W.
\newblock A single-timescale stochastic bilevel optimization method.
\newblock \emph{arXiv preprint arXiv:2102.04671}, 2021{\natexlab{b}}.

\bibitem[Dagr{\'e}ou et~al.(2022)Dagr{\'e}ou, Ablin, Vaiter, and
  Moreau]{dagreou2022framework}
Dagr{\'e}ou, M., Ablin, P., Vaiter, S., and Moreau, T.
\newblock A framework for bilevel optimization that enables stochastic and
  global variance reduction algorithms.
\newblock \emph{arXiv preprint arXiv:2201.13409}, 2022.

\bibitem[Dai et~al.(2017)Dai, He, Pan, Boots, and Song]{dai2017learning}
Dai, B., He, N., Pan, Y., Boots, B., and Song, L.
\newblock Learning from conditional distributions via dual embeddings.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1458--1467.
  PMLR, 2017.

\bibitem[Daskalakis \& Panageas(2018)Daskalakis and
  Panageas]{daskalakis2018limit}
Daskalakis, C. and Panageas, I.
\newblock The limit points of (optimistic) gradient descent in min-max
  optimization.
\newblock \emph{arXiv preprint arXiv:1807.03907}, 2018.

\bibitem[Deng \& Mahdavi(2021)Deng and Mahdavi]{deng2021local}
Deng, Y. and Mahdavi, M.
\newblock Local stochastic gradient descent ascent: Convergence analysis and
  communication efficiency.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1387--1395. PMLR, 2021.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020distributionally}
Deng, Y., Kamani, M.~M., and Mahdavi, M.
\newblock Distributionally robust federated averaging.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15111--15122, 2020.

\bibitem[Diakonikolas et~al.(2021)Diakonikolas, Daskalakis, and
  Jordan]{diakonikolas2021efficient}
Diakonikolas, J., Daskalakis, C., and Jordan, M.
\newblock Efficient methods for structured nonconvex-nonconcave min-max
  optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2746--2754. PMLR, 2021.

\bibitem[Domke(2012)]{domke2012generic}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  318--326.
  PMLR, 2012.

\bibitem[Edmunds \& Bard(1991)Edmunds and Bard]{edmunds1991algorithms}
Edmunds, T.~A. and Bard, J.~F.
\newblock Algorithms for nonlinear bilevel mathematical programs.
\newblock \emph{IEEE transactions on Systems, Man, and Cybernetics},
  21\penalty0 (1):\penalty0 83--89, 1991.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1126--1135. PMLR, 2017.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1165--1173. PMLR, 2017.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1568--1577. PMLR, 2018.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi \& Wang(2018)Ghadimi and Wang]{ghadimi2018approximation}
Ghadimi, S. and Wang, M.
\newblock Approximation methods for bilevel programming.
\newblock \emph{arXiv preprint arXiv:1802.02246}, 2018.

\bibitem[Ghadimi et~al.(2020)Ghadimi, Ruszczynski, and Wang]{ghadimi2020single}
Ghadimi, S., Ruszczynski, A., and Wang, M.
\newblock A single timescale stochastic approximation method for nested
  stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0
  960--979, 2020.

\bibitem[Gidel et~al.(2018)Gidel, Berard, Vignoud, Vincent, and
  Lacoste-Julien]{gidel2018variational}
Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:1802.10551}, 2018.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020iteration}
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S.
\newblock On the iteration complexity of hypergradient computation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3748--3758. PMLR, 2020.

\bibitem[Guo et~al.(2021)Guo, Xu, Yin, Jin, and Yang]{guo2021stochastic}
Guo, Z., Xu, Y., Yin, W., Jin, R., and Yang, T.
\newblock On stochastic moving-average estimators for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2104.14840}, 2021.

\bibitem[Hansen et~al.(1992)Hansen, Jaumard, and Savard]{hansen1992new}
Hansen, P., Jaumard, B., and Savard, G.
\newblock New branch-and-bound rules for linear bilevel programming.
\newblock \emph{SIAM Journal on scientific and Statistical Computing},
  13\penalty0 (5):\penalty0 1194--1217, 1992.

\bibitem[Hong et~al.(2020)Hong, Wai, Wang, and Yang]{hong2020two}
Hong, M., Wai, H.-T., Wang, Z., and Yang, Z.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock \emph{arXiv preprint arXiv:2007.05170}, 2020.

\bibitem[Hou et~al.(2021)Hou, Thekumparampil, Fanti, and Oh]{hou2021efficient}
Hou, C., Thekumparampil, K.~K., Fanti, G., and Oh, S.
\newblock Efficient algorithms for federated saddle point optimization.
\newblock \emph{arXiv preprint arXiv:2102.06333}, 2021.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Hsu, T.-M.~H., Qi, H., and Brown, M.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Huang \& Huang(2021)Huang and Huang]{huang2021biadam}
Huang, F. and Huang, H.
\newblock Biadam: Fast adaptive bilevel optimization methods.
\newblock \emph{arXiv preprint arXiv:2106.11396}, 2021.

\bibitem[Huang et~al.(2021)Huang, Li, and Huang]{huang2021compositional}
Huang, F., Li, J., and Huang, H.
\newblock Compositional federated learning: Applications in distributionally
  robust averaging and meta learning.
\newblock \emph{arXiv preprint arXiv:2106.11264}, 2021.

\bibitem[Ji \& Liang(2021)Ji and Liang]{Ji2021LowerBA}
Ji, K. and Liang, Y.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock \emph{ArXiv}, abs/2102.03926, 2021.

\bibitem[Ji et~al.(2020{\natexlab{a}})Ji, Yang, and Liang]{Ji2020ProvablyFA}
Ji, K., Yang, J., and Liang, Y.
\newblock Provably faster algorithms for bilevel optimization and applications
  to meta-learning.
\newblock \emph{ArXiv}, abs/2010.07962, 2020{\natexlab{a}}.

\bibitem[Ji et~al.(2020{\natexlab{b}})Ji, Yang, and Liang]{ji2020theoretical}
Ji, K., Yang, J., and Liang, Y.
\newblock Theoretical convergence of multi-step model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:2002.07836}, 2020{\natexlab{b}}.

\bibitem[Ji et~al.(2021)Ji, Yang, and Liang]{ji2021bilevel}
Ji, K., Yang, J., and Liang, Y.
\newblock Bilevel optimization: Convergence analysis and enhanced design.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4882--4892. PMLR, 2021.

\bibitem[Ji(2018)]{shaoxiong}
Ji, S.
\newblock A pytorch implementation of federated learning.
\newblock Mar 2018.
\newblock \doi{10.5281/zenodo.4321561}.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2019first}
Khaled, A., Mishchenko, K., and Richt{\'a}rik, P.
\newblock First analysis of local {GD} on heterogeneous data.
\newblock \emph{arXiv preprint arXiv:1909.04715}, 2019.

\bibitem[Khanduri et~al.(2021)Khanduri, Zeng, Hong, Wai, Wang, and
  Yang]{Khanduri2021ANA}
Khanduri, P., Zeng, S., Hong, M., Wai, H.-T., Wang, Z., and Yang, Z.
\newblock A near-optimal algorithm for stochastic bilevel optimization via
  double-momentum.
\newblock \emph{arXiv preprint arXiv:2102.07367}, 2021.

\bibitem[Khodak et~al.(2021)Khodak, Tu, Li, Li, Balcan, Smith, and
  Talwalkar]{khodak2021federated}
Khodak, M., Tu, R., Li, T., Li, L., Balcan, M.-F.~F., Smith, V., and Talwalkar,
  A.
\newblock Federated hyperparameter tuning: Challenges, baselines, and
  connections to weight-sharing.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Kini et~al.(2021)Kini, Paraskevas, Oymak, and
  Thrampoulidis]{kini2021label}
Kini, G.~R., Paraskevas, O., Oymak, S., and Thrampoulidis, C.
\newblock Label-imbalanced and group-sensitive classification under
  overparameterization.
\newblock \emph{arXiv preprint arXiv:2103.01550}, 2021.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2018)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny2016federated}
Kone{\v{c}}n{\`y}, J., McMahan, H.~B., Ramage, D., and Richt{\'a}rik, P.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Gu, and Huang]{li2020improved}
Li, J., Gu, B., and Huang, H.
\newblock Improved bilevel model: Fast and optimal algorithm with theoretical
  guarantee.
\newblock \emph{arXiv preprint arXiv:2009.00690}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2021)Li, Zhang, Thrampoulidis, Chen, and
  Oymak]{li2021autobalance}
Li, M., Zhang, X., Thrampoulidis, C., Chen, J., and Oymak, S.
\newblock Autobalance: Optimized loss functions for imbalanced data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  429--450, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of {FedAvg} on non-{IID} data.
\newblock \emph{arXiv preprint arXiv:1907.02189}, 2019.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020gradient}
Lin, T., Jin, C., and Jordan, M.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6083--6093. PMLR, 2020.

\bibitem[Liu et~al.(2018)Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}, 2018.

\bibitem[Liu et~al.(2019)Liu, Mroueh, Ross, Zhang, Cui, Das, and
  Yang]{liu2019towards}
Liu, M., Mroueh, Y., Ross, J., Zhang, W., Cui, X., Das, P., and Yang, T.
\newblock Towards better understanding of adaptive gradient algorithms in
  generative adversarial nets.
\newblock \emph{arXiv preprint arXiv:1912.11940}, 2019.

\bibitem[Liu et~al.(2020)Liu, Mu, Yuan, Zeng, and Zhang]{liu2020generic}
Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J.
\newblock A generic first-order algorithmic framework for bi-level programming
  beyond lower-level singleton.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6305--6315. PMLR, 2020.

\bibitem[Liu et~al.(2021)Liu, Gao, Zhang, Meng, and Lin]{liu2021investigating}
Liu, R., Gao, J., Zhang, J., Meng, D., and Lin, Z.
\newblock Investigating bi-level optimization for learning and vision from a
  unified perspective: A survey and beyond.
\newblock \emph{arXiv preprint arXiv:2101.11517}, 2021.

\bibitem[Luo et~al.(2020)Luo, Ye, Huang, and Zhang]{luo2020stochastic}
Luo, L., Ye, H., Huang, Z., and Zhang, T.
\newblock Stochastic recursive gradient descent ascent for stochastic
  nonconvex-strongly-concave minimax problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20566--20577, 2020.

\bibitem[Lv et~al.(2007)Lv, Hu, Wang, and Wan]{lv2007penalty}
Lv, Y., Hu, T., Wang, G., and Wan, Z.
\newblock A penalty function method based on kuhn--tucker condition for solving
  linear bilevel programming.
\newblock \emph{Applied Mathematics and Computation}, 188\penalty0
  (1):\penalty0 808--813, 2007.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2113--2122. PMLR, 2015.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan17fedavg}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics, {AISTATS} 2017, 20-22 April 2017,
  Fort Lauderdale, FL, {USA}}, pp.\  1273--1282, 2017.
\newblock URL \url{http://proceedings.mlr.press/v54/mcmahan17a.html}.

\bibitem[Mitra et~al.(2021)Mitra, Jaafar, Pappas, and Hassani]{mitra2021linear}
Mitra, A., Jaafar, R., Pappas, G., and Hassani, H.
\newblock Linear convergence in federated learning: Tackling client
  heterogeneity and sparse gradients.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mohri, M., Sivek, G., and Suresh, A.~T.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4615--4625. PMLR, 2019.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Ozdaglar, and
  Pattathil]{mokhtari2020unified}
Mokhtari, A., Ozdaglar, A., and Pattathil, S.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: Proximal point approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1497--1507. PMLR, 2020.

\bibitem[Moore(2010)]{moore2010bilevel}
Moore, G.~M.
\newblock \emph{Bilevel programming algorithms for machine learning model
  selection}.
\newblock Rensselaer Polytechnic Institute, 2010.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Nazari, P., Tarzanagh, D.~A., and Michailidis, G.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Nedi{\'c} \& Ozdaglar(2009)Nedi{\'c} and
  Ozdaglar]{nedic2009subgradient}
Nedi{\'c}, A. and Ozdaglar, A.
\newblock Subgradient methods for saddle-point problems.
\newblock \emph{Journal of optimization theory and applications}, 142\penalty0
  (1):\penalty0 205--228, 2009.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Nemirovski, A.
\newblock Prox-method with rate of convergence $o(1/t)$ for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nichol \& Schulman(2018)Nichol and Schulman]{nichol2018reptile}
Nichol, A. and Schulman, J.
\newblock Reptile: a scalable metalearning algorithm.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2\penalty0 (3):\penalty0 4,
  2018.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed2019solving}
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J.~D., and Razaviyayn, M.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Pathak \& Wainwright(2020)Pathak and Wainwright]{pathak2020fedsplit}
Pathak, R. and Wainwright, M.~J.
\newblock Fedsplit: an algorithmic framework for fast federated optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7057--7066, 2020.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{International conference on machine learning}, pp.\
  737--746. PMLR, 2016.

\bibitem[Rafique et~al.(2021)Rafique, Liu, Lin, and Yang]{rafique2021weakly}
Rafique, H., Liu, M., Lin, Q., and Yang, T.
\newblock Weakly-convex--concave min--max optimization: provable algorithms and
  applications in machine learning.
\newblock \emph{Optimization Methods and Software}, pp.\  1--35, 2021.

\bibitem[Rasouli et~al.(2020)Rasouli, Sun, and Rajagopal]{rasouli2020fedgan}
Rasouli, M., Sun, T., and Rajagopal, R.
\newblock Fedgan: Federated generative adversarial networks for distributed
  data.
\newblock \emph{arXiv preprint arXiv:2006.07228}, 2020.

\bibitem[Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\`y}, Kumar, and McMahan]{reddi2020adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\`y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Farnia, Pedarsani, and
  Jadbabaie]{reisizadeh2020robust}
Reisizadeh, A., Farnia, F., Pedarsani, R., and Jadbabaie, A.
\newblock Robust federated learning: The case of affine distribution shifts.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21554--21565, 2020.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and
  Boots]{shaban2019truncated}
Shaban, A., Cheng, C.-A., Hatch, N., and Boots, B.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1723--1732. PMLR, 2019.

\bibitem[Shen et~al.(2021)Shen, Du, Zhao, Zhang, Ji, and Gao]{shen2021fedmm}
Shen, Y., Du, J., Zhao, H., Zhang, B., Ji, Z., and Gao, M.
\newblock Fedmm: Saddle point optimization for federated adversarial domain
  adaptation.
\newblock \emph{arXiv preprint arXiv:2110.08477}, 2021.

\bibitem[Shi et~al.(2005)Shi, Lu, and Zhang]{shi2005extended}
Shi, C., Lu, J., and Zhang, G.
\newblock An extended kuhn--tucker approach for linear bilevel programming.
\newblock \emph{Applied Mathematics and Computation}, 162\penalty0
  (1):\penalty0 51--63, 2005.

\bibitem[Sinha et~al.(2017)Sinha, Malo, and Deb]{sinha2017review}
Sinha, A., Malo, P., and Deb, K.
\newblock A review on bilevel optimization: from classical to evolutionary
  approaches and applications.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 22\penalty0
  (2):\penalty0 276--295, 2017.

\bibitem[Stich(2019)]{stich2018local}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1g2JnRcFX}.

\bibitem[Stich \& Karimireddy(2019)Stich and Karimireddy]{stich2019error}
Stich, S.~U. and Karimireddy, S.~P.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2019.

\bibitem[Thekumparampil et~al.(2019)Thekumparampil, Jain, Netrapalli, and
  Oh]{thekumparampil2019efficient}
Thekumparampil, K.~K., Jain, P., Netrapalli, P., and Oh, S.
\newblock Efficient algorithms for smooth minimax optimization.
\newblock \emph{arXiv preprint arXiv:1907.01543}, 2019.

\bibitem[Tran~Dinh et~al.(2020)Tran~Dinh, Liu, and Nguyen]{tran2020hybrid}
Tran~Dinh, Q., Liu, D., and Nguyen, L.
\newblock Hybrid variance-reduced sgd algorithms for minimax problems with
  nonconvex-linear function.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11096--11107, 2020.

\bibitem[Wang \& Joshi(2018)Wang and Joshi]{wang2018cooperative}
Wang, J. and Joshi, G.
\newblock Cooperative {SGD}: A unified framework for the design and analysis of
  communication-efficient {SGD} algorithms.
\newblock \emph{arXiv preprint arXiv:1808.07576}, 2018.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Wang et~al.(2016)Wang, Liu, and Fang]{wang2016accelerating}
Wang, M., Liu, J., and Fang, E.
\newblock Accelerating stochastic composition optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Wang et~al.(2017)Wang, Fang, and Liu]{wang2017stochastic}
Wang, M., Fang, E.~X., and Liu, H.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1-2):\penalty0
  419--449, 2017.

\bibitem[Wang et~al.(2019)Wang, Tuor, Salonidis, Leung, Makaya, He, and
  Chan]{wang2019adaptive}
Wang, S., Tuor, T., Salonidis, T., Leung, K.~K., Makaya, C., He, T., and Chan,
  K.
\newblock Adaptive federated learning in resource constrained edge computing
  systems.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 37\penalty0
  (6):\penalty0 1205--1221, 2019.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Wu, Y.~F., Zhang, W., Xu, P., and Gu, Q.
\newblock A finite-time analysis of two time-scale actor-critic methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17617--17628, 2020.

\bibitem[Xie et~al.(2021)Xie, Zhang, Zhang, Shen, and Qian]{xie2021federated}
Xie, J., Zhang, C., Zhang, Y., Shen, Z., and Qian, H.
\newblock A federated learning framework for nonconvex-pl minimax problems.
\newblock \emph{arXiv preprint arXiv:2105.14216}, 2021.

\bibitem[Yan et~al.(2020)Yan, Xu, Lin, Liu, and Yang]{yan2020optimal}
Yan, Y., Xu, Y., Lin, Q., Liu, W., and Yang, T.
\newblock Optimal epoch stochastic gradient descent ascent methods for min-max
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5789--5800, 2020.

\bibitem[Yang et~al.(2020)Yang, Kiyavash, and He]{yang2020global}
Yang, J., Kiyavash, N., and He, N.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1153--1165, 2020.

\bibitem[Yoon \& Ryu(2021)Yoon and Ryu]{yoon2021accelerated}
Yoon, T. and Ryu, E.~K.
\newblock Accelerated algorithms for smooth convex-concave minimax problems
  with o (1/k\^{} 2) rate on squared gradient norm.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12098--12109. PMLR, 2021.

\bibitem[Yu et~al.(2019)Yu, Yang, and Zhu]{yu2019parallel}
Yu, H., Yang, S., and Zhu, S.
\newblock Parallel restarted {SGD} with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  5693--5700, 2019.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Zinkevich, M., Weimer, M., Li, L., and Smola, A.~J.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2595--2603, 2010.

\end{thebibliography}
