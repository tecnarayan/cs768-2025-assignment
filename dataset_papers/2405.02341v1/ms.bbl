\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pages 308--318, 2016.

\bibitem[Agarwal et~al.(2018)Agarwal, Suresh, Yu, Kumar, and
  McMahan]{agarwal2018cpsgd}
Naman Agarwal, Ananda~Theertha Suresh, Felix Xinnan~X Yu, Sanjiv Kumar, and
  Brendan McMahan.
\newblock {cpSGD}: Communication-efficient and differentially-private
  distributed sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7564--7575, 2018.

\bibitem[Agarwal et~al.(2021)Agarwal, Kairouz, and Liu]{agarwal2021skellam}
Naman Agarwal, Peter Kairouz, and Ziyu Liu.
\newblock The {Skellam} mechanism for differentially private federated
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5052--5064, 2021.

\bibitem[Ailon and Chazelle(2006)]{ailon2006approximate}
Nir Ailon and Bernard Chazelle.
\newblock Approximate nearest neighbors and the fast johnson-lindenstrauss
  transform.
\newblock In \emph{Proceedings of the thirty-eighth annual ACM symposium on
  Theory of computing}, pages 557--563, 2006.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh17qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  1709--1720, 2017.

\bibitem[Andrew et~al.(2021)Andrew, Thakkar, McMahan, and
  Ramaswamy]{andrew2021differentially}
Galen Andrew, Om~Thakkar, Brendan McMahan, and Swaroop Ramaswamy.
\newblock Differentially private learning with adaptive clipping.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17455--17466, 2021.

\bibitem[Asi et~al.(2022)Asi, Feldman, and Talwar]{asi2022optimal}
Hilal Asi, Vitaly Feldman, and Kunal Talwar.
\newblock Optimal algorithms for mean estimation under local differential
  privacy.
\newblock In \emph{International Conference on Machine Learning}, pages
  1046--1056. PMLR, 2022.

\bibitem[Asi et~al.(2023)Asi, Feldman, Nelson, Nguyen, and Talwar]{asi2023fast}
Hilal Asi, Vitaly Feldman, Jelani Nelson, Huy Nguyen, and Kunal Talwar.
\newblock Fast optimal locally private mean estimation via random projections.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=K3JgUvDSYX}.

\bibitem[Authors.(2019)]{stackoverflow2019}
The TensorFlow~Federated Authors.
\newblock Tensorflow federated stack overflow dataset, 2019.

\bibitem[Balle and Wang(2018)]{balle2018improving}
Borja Balle and Yu-Xiang Wang.
\newblock Improving the gaussian mechanism for differential privacy: Analytical
  calibration and optimal denoising.
\newblock In \emph{International Conference on Machine Learning}, pages
  394--403. PMLR, 2018.

\bibitem[Balle et~al.(2018)Balle, Barthe, and Gaboardi]{balle2018privacy}
Borja Balle, Gilles Barthe, and Marco Gaboardi.
\newblock Privacy amplification by subsampling: Tight analyses via couplings
  and divergences.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Barnes et~al.(2020)Barnes, Inan, Isik, and
  {\"O}zg{\"u}r]{barnes2020rtopk}
Leighton~Pate Barnes, Huseyin~A Inan, Berivan Isik, and Ayfer {\"O}zg{\"u}r.
\newblock rtop-k: A statistical estimation approach to distributed sgd.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (3):\penalty0 897--907, 2020.

\bibitem[Bell et~al.(2020)Bell, Bonawitz, Gasc{\'o}n, Lepoint, and
  Raykova]{bell2020secure}
James~Henry Bell, Kallista~A Bonawitz, Adri{\`a} Gasc{\'o}n, Tancr{\`e}de
  Lepoint, and Mariana Raykova.
\newblock Secure single-server aggregation with (poly) logarithmic overhead.
\newblock In \emph{Proceedings of the 2020 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 1253--1269, 2020.

\bibitem[Bhowmick et~al.(2018)Bhowmick, Duchi, Freudiger, Kapoor, and
  Rogers]{bhowmick2018protection}
Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan
  Rogers.
\newblock Protection against reconstruction and its applications in private
  federated learning.
\newblock \emph{arXiv preprint arXiv:1812.00984}, 2018.

\bibitem[Bonawitz et~al.(2016)Bonawitz, Ivanov, Kreuter, Marcedone, McMahan,
  Patel, Ramage, Segal, and Seth]{bonawitz2016practical}
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H~Brendan
  McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for federated learning on user-held
  data.
\newblock \emph{arXiv preprint arXiv:1611.04482}, 2016.

\bibitem[Braverman et~al.(2016)Braverman, Garg, Ma, Nguyen, and
  Woodruff]{braverman2016communication}
Mark Braverman, Ankit Garg, Tengyu Ma, Huy~L Nguyen, and David~P Woodruff.
\newblock Communication lower bounds for statistical estimation problems via a
  distributed data processing inequality.
\newblock In \emph{Proceedings of the forty-eighth annual ACM Symposium on
  Theory of Computing}, pages 1011--1020, 2016.

\bibitem[Canonne et~al.(2020)Canonne, Kamath, and Steinke]{canonne2020discrete}
Cl{\'e}ment~L Canonne, Gautam Kamath, and Thomas Steinke.
\newblock The discrete gaussian for differential privacy.
\newblock \emph{arXiv preprint arXiv:2004.00010}, 2020.

\bibitem[Chan et~al.(2012)Chan, Li, Shi, and Xu]{chan2012differentially}
T~H~Hubert Chan, Mingfei Li, Elaine Shi, and Wenchang Xu.
\newblock Differentially private continual monitoring of heavy hitters from
  distributed streams.
\newblock In \emph{Privacy Enhancing Technologies: 12th International
  Symposium, PETS 2012, Vigo, Spain, July 11-13, 2012. Proceedings 12}, pages
  140--159. Springer, 2012.

\bibitem[Chen et~al.(2020)Chen, Kairouz, and Ozgur]{chen2020breaking}
Wei-Ning Chen, Peter Kairouz, and Ayfer Ozgur.
\newblock Breaking the communication-privacy-accuracy trilemma.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Choo, Kairouz, and
  Suresh]{chen2022fundamental}
Wei-Ning Chen, Christopher A~Choquette Choo, Peter Kairouz, and Ananda~Theertha
  Suresh.
\newblock The fundamental price of secure aggregation in differentially private
  federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3056--3089. PMLR, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Ozgur, and
  Kairouz]{chen2022poisson}
Wei-Ning Chen, Ayfer Ozgur, and Peter Kairouz.
\newblock The {Poisson} binomial mechanism for unbiased federated learning with
  secure aggregation.
\newblock In \emph{International Conference on Machine Learning}, pages
  3490--3506. PMLR, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2023)Chen, Song, Ozgur, and Kairouz]{chen2023privacy}
Wei-Ning Chen, Dan Song, Ayfer Ozgur, and Peter Kairouz.
\newblock Privacy amplification via compression: Achieving the optimal
  privacy-accuracy-communication trade-off in distributed mean estimation.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=izNfcaHJk0}.

\bibitem[Choquette-Choo et~al.(2022)Choquette-Choo, McMahan, Rush, and
  Thakurta]{choquette2022multi}
Christopher~A Choquette-Choo, H~Brendan McMahan, Keith Rush, and Abhradeep
  Thakurta.
\newblock Multi-epoch matrix factorization mechanisms for private machine
  learning.
\newblock \emph{arXiv preprint arXiv:2211.06530}, 2022.

\bibitem[Choquette-Choo et~al.(2023{\natexlab{a}})Choquette-Choo, Dvijotham,
  Pillutla, Ganesh, Steinke, and Thakurta]{choquette2023correlated}
Christopher~A Choquette-Choo, Krishnamurthy Dvijotham, Krishna Pillutla, Arun
  Ganesh, Thomas Steinke, and Abhradeep Thakurta.
\newblock Correlated noise provably beats independent noise for differentially
  private learning.
\newblock \emph{arXiv preprint arXiv:2310.06771}, 2023{\natexlab{a}}.

\bibitem[Choquette-Choo et~al.(2023{\natexlab{b}})Choquette-Choo, Ganesh,
  Steinke, and Thakurta]{choquette2023privacy}
Christopher~A Choquette-Choo, Arun Ganesh, Thomas Steinke, and Abhradeep
  Thakurta.
\newblock Privacy amplification for matrix mechanisms.
\newblock \emph{arXiv preprint arXiv:2310.15526}, 2023{\natexlab{b}}.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Van~Schaik]{cohen2017emnist}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van~Schaik.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock In \emph{2017 international joint conference on neural networks
  (IJCNN)}, pages 2921--2926. IEEE, 2017.

\bibitem[Denisov et~al.(2022)Denisov, McMahan, Rush, Smith, and
  Guha~Thakurta]{denisov2022improved}
Sergey Denisov, H~Brendan McMahan, John Rush, Adam Smith, and Abhradeep
  Guha~Thakurta.
\newblock Improved differential privacy for sgd via optimal private linear
  operators on adaptive streams.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 5910--5924, 2022.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and
  Smith]{dwork2006calibrating}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Theory of cryptography conference}, pages 265--284.
  Springer, 2006.

\bibitem[Dwork et~al.(2010)Dwork, Naor, Pitassi, and
  Rothblum]{dwork2010differential}
Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy~N Rothblum.
\newblock Differential privacy under continual observation.
\newblock In \emph{Proceedings of the 42nd ACM Symposium on Theory of
  Computing}, pages 715--724, 2010.

\bibitem[Edmonds et~al.(2020)Edmonds, Nikolov, and Ullman]{edmonds2020power}
Alexander Edmonds, Aleksandar Nikolov, and Jonathan Ullman.
\newblock The power of factorization mechanisms in local and central
  differential privacy.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 425--438, 2020.

\bibitem[Erlingsson et~al.(2019)Erlingsson, Feldman, Mironov, Raghunathan,
  Talwar, and Thakurta]{erlingsson2019amplification}
{\'U}lfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal
  Talwar, and Abhradeep Thakurta.
\newblock Amplification by shuffling: From local to central differential
  privacy via anonymity.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 2468--2479. SIAM, 2019.

\bibitem[Farokhi(2021)]{farokhi2021gradient}
Farhad Farokhi.
\newblock Gradient sparsification can improve performance of
  differentially-private convex machine learning.
\newblock In \emph{2021 60th IEEE Conference on Decision and Control (CDC)},
  pages 1695--1700. IEEE, 2021.

\bibitem[Feldman and Talwar(2021)]{feldman2021lossless}
Vitaly Feldman and Kunal Talwar.
\newblock Lossless compression of efficient private local randomizers.
\newblock In \emph{International Conference on Machine Learning}, pages
  3208--3219. PMLR, 2021.

\bibitem[Feldman et~al.(2022)Feldman, McMillan, and Talwar]{feldman2022hiding}
Vitaly Feldman, Audra McMillan, and Kunal Talwar.
\newblock Hiding among the clones: A simple and nearly optimal analysis of
  privacy amplification by shuffling.
\newblock In \emph{2021 IEEE 62nd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 954--964. IEEE, 2022.

\bibitem[Feldman et~al.(2023)Feldman, McMillan, and
  Talwar]{feldman2023stronger}
Vitaly Feldman, Audra McMillan, and Kunal Talwar.
\newblock Stronger privacy amplification by shuffling for {R\'e}nyi and
  approximate differential privacy.
\newblock In \emph{Proceedings of the 2023 Annual ACM-SIAM Symposium on
  Discrete Algorithms (SODA)}, pages 4966--4981. SIAM, 2023.

\bibitem[Gandikota et~al.(2019)Gandikota, Kane, Maity, and
  Mazumdar]{g2019vqsgd}
Venkata Gandikota, Daniel Kane, Raj~Kumar Maity, and Arya Mazumdar.
\newblock {vqSGD}: Vector quantized stochastic gradient descent, 2019.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Girgis et~al.(2021)Girgis, Data, Diggavi, Kairouz, and
  Suresh]{girgis2021shuffled}
Antonious Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and
  Ananda~Theertha Suresh.
\newblock Shuffled model of differential privacy in federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2521--2529. PMLR, 2021.

\bibitem[Girgis and Diggavi(2023)]{girgis2023multi}
Antonious~M Girgis and Suhas Diggavi.
\newblock Multi-message shuffled privacy in federated learning.
\newblock \emph{arXiv preprint arXiv:2302.11152}, 2023.

\bibitem[Guha~Thakurta and Smith(2013)]{guha2013nearly}
Abhradeep Guha~Thakurta and Adam Smith.
\newblock (nearly) optimal algorithms for private online learning in
  full-information and bandit settings.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Hardt and Talwar(2010)]{hardt2010geometry}
Moritz Hardt and Kunal Talwar.
\newblock On the geometry of differential privacy.
\newblock In \emph{Proceedings of the forty-second ACM symposium on Theory of
  computing}, pages 705--714, 2010.

\bibitem[Henzinger et~al.(2023)Henzinger, Upadhyay, and
  Upadhyay]{henzinger2023almost}
Monika Henzinger, Jalaj Upadhyay, and Sarvagya Upadhyay.
\newblock Almost tight error bounds on differentially private continual
  counting.
\newblock In \emph{Proceedings of the 2023 Annual ACM-SIAM Symposium on
  Discrete Algorithms (SODA)}, pages 5003--5039. SIAM, 2023.

\bibitem[Henzinger et~al.(2024)Henzinger, Upadhyay, and
  Upadhyay]{henzinger2024unifying}
Monika Henzinger, Jalaj Upadhyay, and Sarvagya Upadhyay.
\newblock A unifying framework for differentially private sums under continual
  observation.
\newblock In \emph{Proceedings of the 2024 Annual ACM-SIAM Symposium on
  Discrete Algorithms (SODA)}, pages 995--1018. SIAM, 2024.

\bibitem[Honaker(2015)]{honaker2015efficient}
James Honaker.
\newblock Efficient use of differentially private binary trees.
\newblock \emph{Theory and Practice of Differential Privacy (TPDP 2015),
  London, UK}, 2:\penalty0 26--27, 2015.

\bibitem[Hu et~al.(2021)Hu, Gong, and Guo]{hu2021federated}
Rui Hu, Yanmin Gong, and Yuanxiong Guo.
\newblock Federated learning with sparsification-amplified privacy and adaptive
  optimization.
\newblock In Zhi-Hua Zhou, editor, \emph{Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence, {IJCAI-21}}, pages
  1463--1469. International Joint Conferences on Artificial Intelligence
  Organization, 8 2021.
\newblock \doi{10.24963/ijcai.2021/202}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2021/202}.
\newblock Main Track.

\bibitem[Isik et~al.(2022)Isik, Weissman, and No]{isik2022information}
Berivan Isik, Tsachy Weissman, and Albert No.
\newblock An information-theoretic justification for model pruning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3821--3846. PMLR, 2022.

\bibitem[Isik et~al.(2023{\natexlab{a}})Isik, Chen, Ozgur, Weissman, and
  No]{isik2023exact}
Berivan Isik, Wei-Ning Chen, Ayfer Ozgur, Tsachy Weissman, and Albert No.
\newblock Exact optimality of communication-privacy-utility tradeoffs in
  distributed mean estimation.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=7ETbK9lQd7}.

\bibitem[Isik et~al.(2023{\natexlab{b}})Isik, Pase, Gunduz, Weissman, and
  Michele]{isik2023sparse}
Berivan Isik, Francesco Pase, Deniz Gunduz, Tsachy Weissman, and Zorzi Michele.
\newblock Sparse random networks for communication-efficient federated
  learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=k1FHgri5y3-}.

\bibitem[Jain et~al.(2023)Jain, Raskhodnikova, Sivakumar, and
  Smith]{jain2023price}
Palak Jain, Sofya Raskhodnikova, Satchit Sivakumar, and Adam Smith.
\newblock The price of differential privacy under continual observation.
\newblock In \emph{International Conference on Machine Learning}, pages
  14654--14678. PMLR, 2023.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Kairouz et~al.(2021{\natexlab{a}})Kairouz, Liu, and
  Steinke]{kairouz2021distributed}
Peter Kairouz, Ziyu Liu, and Thomas Steinke.
\newblock The distributed discrete gaussian mechanism for federated learning
  with secure aggregation.
\newblock In \emph{International Conference on Machine Learning}, pages
  5201--5212. PMLR, 2021{\natexlab{a}}.

\bibitem[Kairouz et~al.(2021{\natexlab{b}})Kairouz, McMahan, Song, Thakkar,
  Thakurta, and Xu]{kairouz2021practical}
Peter Kairouz, Brendan McMahan, Shuang Song, Om~Thakkar, Abhradeep Thakurta,
  and Zheng Xu.
\newblock Practical and private (deep) learning without sampling or shuffling.
\newblock In \emph{International Conference on Machine Learning}, pages
  5213--5225. PMLR, 2021{\natexlab{b}}.

\bibitem[Kairouz et~al.(2021{\natexlab{c}})Kairouz, McMahan, Avent, Bellet,
  Bennis, Bhagoji, Bonawitz, Charles, Cormode, Cummings,
  et~al.]{kairouz2021advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021{\natexlab{c}}.

\bibitem[Kasiviswanathan et~al.(2011)Kasiviswanathan, Lee, Nissim,
  Raskhodnikova, and Smith]{kasiviswanathan2011can}
Shiva~Prasad Kasiviswanathan, Homin~K Lee, Kobbi Nissim, Sofya Raskhodnikova,
  and Adam Smith.
\newblock What can we learn privately?
\newblock \emph{SIAM Journal on Computing}, 40\penalty0 (3):\penalty0 793--826,
  2011.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Li et~al.(2015)Li, Miklau, Hay, McGregor, and Rastogi]{li2015matrix}
Chao Li, Gerome Miklau, Michael Hay, Andrew McGregor, and Vibhor Rastogi.
\newblock The matrix mechanism: optimizing linear counting queries under
  differential privacy.
\newblock \emph{The VLDB journal}, 24:\penalty0 757--781, 2015.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2018deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SkhQHMW0W}.

\bibitem[Lyubarskii and Vershynin(2010)]{lyubarskii2010uncertainty}
Yurii Lyubarskii and Roman Vershynin.
\newblock Uncertainty principles and vector quantization.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (7):\penalty0 3491--3501, 2010.

\bibitem[McKenna et~al.(2018)McKenna, Miklau, Hay, and
  Machanavajjhala]{mckenna2018optimizing}
Ryan McKenna, Gerome Miklau, Michael Hay, and Ashwin Machanavajjhala.
\newblock Optimizing error of high-dimensional statistical queries under
  differential privacy.
\newblock \emph{arXiv preprint arXiv:1808.03537}, 2018.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson, and
  Arcas]{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, S~Hampson, and BA~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data (2016).
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Mironov et~al.(2019)Mironov, Talwar, and Zhang]{mironov2019r}
Ilya Mironov, Kunal Talwar, and Li~Zhang.
\newblock R$\backslash$'enyi differential privacy of the sampled gaussian
  mechanism.
\newblock \emph{arXiv preprint arXiv:1908.10530}, 2019.

\bibitem[Mitchell et~al.(2022)Mitchell, Ball{\'e}, Charles, and
  Kone{\v{c}}n{\`y}]{mitchell2022optimizing}
Nicole Mitchell, Johannes Ball{\'e}, Zachary Charles, and Jakub
  Kone{\v{c}}n{\`y}.
\newblock Optimizing the communication-accuracy trade-off in federated learning
  with rate-distortion theory.
\newblock \emph{arXiv preprint arXiv:2201.02664}, 2022.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica,
  Braverman, Gonzalez, and Arora]{rothchild2020fetchsgd}
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
  Vladimir Braverman, Joseph Gonzalez, and Raman Arora.
\newblock Fetchsgd: Communication-efficient federated learning with sketching.
\newblock In \emph{International Conference on Machine Learning}, pages
  8253--8265. PMLR, 2020.

\bibitem[Sarlos(2006)]{sarlos2006improved}
Tamas Sarlos.
\newblock Improved approximation algorithms for large matrices via random
  projections.
\newblock In \emph{2006 47th annual IEEE symposium on foundations of computer
  science (FOCS'06)}, pages 143--152. IEEE, 2006.

\bibitem[Shah et~al.(2022)Shah, Chen, Balle, Kairouz, and
  Theis]{shah2022optimal}
Abhin Shah, Wei-Ning Chen, Johannes Balle, Peter Kairouz, and Lucas Theis.
\newblock Optimal compression of locally differentially private mechanisms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 7680--7723. PMLR, 2022.

\bibitem[Suresh et~al.(2017)Suresh, Yu, Kumar, and McMahan]{an2016distributed}
Ananda~Theertha Suresh, Felix~X. Yu, Sanjiv Kumar, and H.~Brendan McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML’17, page 3329–3337. JMLR.org, 2017.

\bibitem[Upadhyay and Upadhyay(2021)]{upadhyay2021framework}
Jalaj Upadhyay and Sarvagya Upadhyay.
\newblock A framework for private matrix analysis in sliding window model.
\newblock In \emph{International Conference on Machine Learning}, pages
  10465--10475. PMLR, 2021.

\bibitem[Vargaftik et~al.(2021)Vargaftik, Ben-Basat, Portnoy, Mendelson,
  Ben-Itzhak, and Mitzenmacher]{vargaftik2021drive}
Shay Vargaftik, Ran Ben-Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak,
  and Michael Mitzenmacher.
\newblock Drive: One-bit distributed mean estimation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 362--377, 2021.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2019)Wang, Balle, and Kasiviswanathan]{wang2019subsampled}
Yu-Xiang Wang, Borja Balle, and Shiva~Prasad Kasiviswanathan.
\newblock Subsampled {R\'e}nyi differential privacy and analytical moments
  accountant.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1226--1235. PMLR, 2019.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem[Warner(1965)]{warner1965randomized}
Stanley~L Warner.
\newblock Randomized response: A survey technique for eliminating evasive
  answer bias.
\newblock \emph{Journal of the American Statistical Association}, 60\penalty0
  (309):\penalty0 63--69, 1965.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem[Yuan et~al.(2016)Yuan, Yang, Zhang, and Hao]{yuan2016optimal}
Ganzhao Yuan, Yin Yang, Zhenjie Zhang, and Zhifeng Hao.
\newblock Optimal linear aggregate query processing under approximate
  differential privacy.
\newblock \emph{CoRR, abs/1602.04302}, 2016.

\bibitem[Zhu and Wang(2019)]{zhu2019poission}
Yuqing Zhu and Yu-Xiang Wang.
\newblock Poission subsampled {R\'e}nyi differential privacy.
\newblock In \emph{International Conference on Machine Learning}, pages
  7634--7642. PMLR, 2019.

\end{thebibliography}
