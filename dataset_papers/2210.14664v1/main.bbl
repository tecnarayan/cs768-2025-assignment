\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bachem et~al.(2018)Bachem, Lucic, and Krause]{bachem2018scalable}
O.~Bachem, M.~Lucic, and A.~Krause.
\newblock Scalable k-means clustering via lightweight coresets.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1119--1127, 2018.

\bibitem[Balcan et~al.(2013)Balcan, Ehrlich, and Liang]{balcan2013distributed}
M.-F.~F. Balcan, S.~Ehrlich, and Y.~Liang.
\newblock Distributed $ k $-means and $ k $-median clustering on general
  topologies.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Bar-Yossef et~al.(2004)Bar-Yossef, Jayram, Kumar, and
  Sivakumar]{bar2004information}
Z.~Bar-Yossef, T.~S. Jayram, R.~Kumar, and D.~Sivakumar.
\newblock An information statistics approach to data stream and communication
  complexity.
\newblock \emph{Journal of Computer and System Sciences}, 68\penalty0
  (4):\penalty0 702--732, 2004.

\bibitem[Bertin-Mahieux et~al.(2011)Bertin-Mahieux, Ellis, Whitman, and
  Lamere]{bertin2011million}
T.~Bertin-Mahieux, D.~P. Ellis, B.~Whitman, and P.~Lamere.
\newblock The million song dataset.
\newblock 2011.

\bibitem[Bonawitz et~al.(2017)Bonawitz, Ivanov, Kreuter, Marcedone, McMahan,
  Patel, Ramage, Segal, and Seth]{bonawitz2017practical}
K.~Bonawitz, V.~Ivanov, B.~Kreuter, A.~Marcedone, H.~B. McMahan, S.~Patel,
  D.~Ramage, A.~Segal, and K.~Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In \emph{proceedings of the 2017 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 1175--1191, 2017.

\bibitem[Boutsidis et~al.(2013)Boutsidis, Drineas, and
  Magdon-Ismail]{boutsidis2013near}
C.~Boutsidis, P.~Drineas, and M.~Magdon-Ismail.
\newblock Near-optimal coresets for least-squares regression.
\newblock \emph{IEEE transactions on information theory}, 59\penalty0
  (10):\penalty0 6880--6892, 2013.

\bibitem[Braverman et~al.(2016)Braverman, Feldman, and Lang]{braverman2016new}
V.~Braverman, D.~Feldman, and H.~Lang.
\newblock New frameworks for offline and streaming coreset constructions.
\newblock \emph{CoRR}, abs/1612.00889, 2016.

\bibitem[B{\"u}rgisser and Cucker(2010)]{burgisser2010smoothed}
P.~B{\"u}rgisser and F.~Cucker.
\newblock Smoothed analysis of moore--penrose inversion.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 31\penalty0
  (5):\penalty0 2769--2783, 2010.

\bibitem[Chen et~al.(2020)Chen, Jin, Sun, and Yin]{chen2020vafl}
T.~Chen, X.~Jin, Y.~Sun, and W.~Yin.
\newblock {VAFL}: a method of vertical asynchronous federated learning.
\newblock \emph{arXiv preprint arXiv:2007.06081}, 2020.

\bibitem[Chen et~al.(2021)Chen, Ma, Fan, Kang, Xu, and
  Yang]{chen2021secureboost+}
W.~Chen, G.~Ma, T.~Fan, Y.~Kang, Q.~Xu, and Q.~Yang.
\newblock Secureboost+: A high performance gradient boosting tree framework for
  large scale vertical federated learning.
\newblock \emph{arXiv preprint arXiv:2110.10927}, 2021.

\bibitem[Cheng et~al.(2021)Cheng, Fan, Jin, Liu, Chen, Papadopoulos, and
  Yang]{cheng2021secureboost}
K.~Cheng, T.~Fan, Y.~Jin, Y.~Liu, T.~Chen, D.~Papadopoulos, and Q.~Yang.
\newblock Secureboost: A lossless federated learning framework.
\newblock \emph{IEEE Intelligent Systems}, 36\penalty0 (6):\penalty0 87--98,
  2021.

\bibitem[Chhaya et~al.(2020)Chhaya, Dasgupta, and Shit]{chhaya2020coresets}
R.~Chhaya, A.~Dasgupta, and S.~Shit.
\newblock On coresets for regularized regression.
\newblock In \emph{International conference on machine learning}, pages
  1866--1876. PMLR, 2020.

\bibitem[Cohen et~al.(2015)Cohen, Lee, Musco, Musco, Peng, and
  Sidford]{cohen2015uniform}
M.~B. Cohen, Y.~T. Lee, C.~Musco, C.~Musco, R.~Peng, and A.~Sidford.
\newblock Uniform sampling for matrix approximation.
\newblock In \emph{Proceedings of the 2015 Conference on Innovations in
  Theoretical Computer Science}, pages 181--190. ACM, 2015.

\bibitem[Cohen et~al.(2017)Cohen, Musco, and Musco]{cohen2017input}
M.~B. Cohen, C.~Musco, and C.~Musco.
\newblock Input sparsity time low-rank approximation via ridge leverage score
  sampling.
\newblock In \emph{Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
  on Discrete Algorithms}, pages 1758--1777. SIAM, 2017.

\bibitem[Cohen{-}Addad et~al.(2021)Cohen{-}Addad, Saulpic, and
  Schwiegelshohn]{cohenaddad2021new}
V.~Cohen{-}Addad, D.~Saulpic, and C.~Schwiegelshohn.
\newblock A new coreset framework for clustering.
\newblock In S.~Khuller and V.~V. Williams, editors, \emph{{STOC} '21: 53rd
  Annual {ACM} {SIGACT} Symposium on Theory of Computing, Virtual Event, Italy,
  June 21-25, 2021}, pages 169--182. {ACM}, 2021.

\bibitem[Cohen{-}Addad et~al.(2022)Cohen{-}Addad, Larsen, Saulpic, and
  Schwiegelshohn]{cohenaddad2022towards}
V.~Cohen{-}Addad, K.~G. Larsen, D.~Saulpic, and C.~Schwiegelshohn.
\newblock Towards optimal lower bounds for k-median and k-means coresets.
\newblock In \emph{Proceedings of the firty-fourth annual ACM symposium on
  Theory of computing}, 2022.

\bibitem[Das et~al.(2020)Das, Hashemi, Sanghavi, and Dhillon]{das2020improved}
R.~Das, A.~Hashemi, S.~Sanghavi, and I.~S. Dhillon.
\newblock Improved convergence rates for non-convex federated learning with
  compression.
\newblock \emph{arXiv e-prints}, pages arXiv--2012, 2020.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Ding et~al.(2016)Ding, Liu, Huang, and Li]{ding2016kmeans}
H.~Ding, Y.~Liu, L.~Huang, and J.~Li.
\newblock K-means clustering with distributed dimensions.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Drineas et~al.(2006)Drineas, Mahoney, and
  Muthukrishnan]{drineas2006sampling}
P.~Drineas, M.~W. Mahoney, and S.~Muthukrishnan.
\newblock Sampling algorithms for $l_2$ regression and applications.
\newblock In \emph{Proceedings of the seventeenth annual ACM-SIAM symposium on
  Discrete algorithm}, pages 1127--1136. Society for Industrial and Applied
  Mathematics, 2006.

\bibitem[Fatkhullin et~al.(2021)Fatkhullin, Sokolov, Gorbunov, Li, and
  Richt{\'a}rik]{fatkhullin2021ef21}
I.~Fatkhullin, I.~Sokolov, E.~Gorbunov, Z.~Li, and P.~Richt{\'a}rik.
\newblock {EF21} with bells \& whistles: Practical algorithmic extensions of
  modern error feedback.
\newblock \emph{arXiv preprint arXiv:2110.03294}, 2021.

\bibitem[Feldman and Langberg(2011)]{feldman2011unified}
D.~Feldman and M.~Langberg.
\newblock A unified framework for approximating and clustering data.
\newblock In \emph{Proceedings of the forty-third annual ACM symposium on
  Theory of computing}, pages 569--578. ACM, 2011.

\bibitem[Feldman et~al.(2013)Feldman, Schmidt, and Sohler]{feldman2013turning}
D.~Feldman, M.~Schmidt, and C.~Sohler.
\newblock Turning big data into tiny data: Constant-size coresets for
  $k$-means, pca and projective clustering.
\newblock In \emph{Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium
  on Discrete Algorithms}, pages 1434--1453. SIAM, 2013.

\bibitem[Gorbunov et~al.(2021{\natexlab{a}})Gorbunov, Burlachenko, Li, and
  Richt{\'a}rik]{gorbunov2021marina}
E.~Gorbunov, K.~P. Burlachenko, Z.~Li, and P.~Richt{\'a}rik.
\newblock {MARINA}: Faster non-convex distributed learning with compression.
\newblock In \emph{International Conference on Machine Learning}, pages
  3788--3798. PMLR, 2021{\natexlab{a}}.

\bibitem[Gorbunov et~al.(2021{\natexlab{b}})Gorbunov, Hanzely, and
  Richt{\'a}rik]{gorbunov2021local}
E.~Gorbunov, F.~Hanzely, and P.~Richt{\'a}rik.
\newblock Local sgd: Unified theory and new efficient methods.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3556--3564. PMLR, 2021{\natexlab{b}}.

\bibitem[Gu et~al.(2020)Gu, Xu, Huo, Deng, and Huang]{gu2020privacy}
B.~Gu, A.~Xu, Z.~Huo, C.~Deng, and H.~Huang.
\newblock Privacy-preserving asynchronous federated learning algorithms for
  multi-party vertically collaborative learning.
\newblock \emph{arXiv preprint arXiv:2008.06233}, 2020.

\bibitem[{Har-Peled} and Mazumdar(2004)]{harpeled2004on}
S.~{Har-Peled} and S.~Mazumdar.
\newblock On coresets for $k$-means and $k$-median clustering.
\newblock In \emph{36th Annual ACM Symposium on Theory of Computing,}, pages
  291--300, 2004.

\bibitem[Hardy et~al.(2017)Hardy, Henecka, Ivey-Law, Nock, Patrini, Smith, and
  Thorne]{hardy2017private}
S.~Hardy, W.~Henecka, H.~Ivey-Law, R.~Nock, G.~Patrini, G.~Smith, and
  B.~Thorne.
\newblock Private federated learning on vertically partitioned data via entity
  resolution and additively homomorphic encryption.
\newblock \emph{arXiv preprint arXiv:1711.10677}, 2017.

\bibitem[He et~al.(2021)He, Du, Zhu, Zhang, Liang, and Chan]{he2021secure}
D.~He, R.~Du, S.~Zhu, M.~Zhang, K.~Liang, and S.~Chan.
\newblock Secure logistic regression for vertical federated learning.
\newblock \emph{IEEE Internet Computing}, 2021.

\bibitem[Hu et~al.(2020)Hu, Guo, Li, Pei, and Gong]{hu2020personalized}
R.~Hu, Y.~Guo, H.~Li, Q.~Pei, and Y.~Gong.
\newblock Personalized federated learning with differential privacy.
\newblock \emph{IEEE Internet of Things Journal}, 7\penalty0 (10):\penalty0
  9530--9539, 2020.

\bibitem[Huang and Vishnoi(2020)]{huang2020coresets}
L.~Huang and N.~K. Vishnoi.
\newblock Coresets for clustering in euclidean spaces: Importance sampling is
  nearly optimal.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1416--1429, 2020.

\bibitem[Huang et~al.(2018)Huang, Jiang, Li, and Wu]{huang2018epsilon}
L.~Huang, S.~H.-C. Jiang, J.~Li, and X.~Wu.
\newblock Epsilon-coresets for clustering (with outliers) in doubling metrics.
\newblock In \emph{2018 IEEE 59th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 814--825. IEEE, 2018.

\bibitem[Huang et~al.(2020)Huang, Sudhir, and Vishnoi]{huang2020coresetsFR}
L.~Huang, K.~Sudhir, and N.~Vishnoi.
\newblock Coresets for regressions with panel data.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 325--337, 2020.

\bibitem[Jubran et~al.(2019)Jubran, Maalouf, and Feldman]{jubran2019fast}
I.~Jubran, A.~Maalouf, and D.~Feldman.
\newblock Fast and accurate least-mean-squares solvers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8305--8316, 2019.

\bibitem[Kaggle()]{kagglekc}
Kaggle.
\newblock Kc house data.
\newblock \url{https://www.kaggle.com/datasets/shivachandel/kc-house-data}.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2021advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021.

\bibitem[Kalyanasundaram and Schintger(1992)]{kalyanasundaram1992probabilistic}
B.~Kalyanasundaram and G.~Schintger.
\newblock The probabilistic communication complexity of set intersection.
\newblock \emph{SIAM Journal on Discrete Mathematics}, 5\penalty0 (4):\penalty0
  545--557, 1992.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
S.~P. Karimireddy, Q.~Rebjock, S.~Stich, and M.~Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock In \emph{International Conference on Machine Learning}, pages
  3252--3261. PMLR, 2019.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~Reddi, S.~Stich, and A.~T. Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, F.~X. Yu, P.~Richt{\'a}rik, A.~T. Suresh,
  and D.~Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Kumar et~al.(2004)Kumar, Sabharwal, and Sen]{kumar2004simple}
A.~Kumar, Y.~Sabharwal, and S.~Sen.
\newblock A simple linear time (1+/spl epsiv/)-approximation algorithm for
  k-means clustering in any dimensions.
\newblock In \emph{45th Annual IEEE Symposium on Foundations of Computer
  Science}, pages 454--462. IEEE, 2004.

\bibitem[Kushilevitz(1997)]{kushilevitz1997communication}
E.~Kushilevitz.
\newblock Communication complexity.
\newblock In \emph{Advances in Computers}, volume~44, pages 331--360. Elsevier,
  1997.

\bibitem[Li et~al.(2013)Li, Miller, and Peng]{li2013iterative}
M.~Li, G.~L. Miller, and R.~Peng.
\newblock Iterative row sampling.
\newblock In \emph{2013 IEEE 54th Annual Symposium on Foundations of Computer
  Science}, pages 127--136. IEEE, 2013.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{li2020federated}
T.~Li, A.~K. Sahu, A.~Talwalkar, and V.~Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020{\natexlab{a}}.

\bibitem[Li and Richt{\'a}rik(2020)]{li2020unified}
Z.~Li and P.~Richt{\'a}rik.
\newblock A unified analysis of stochastic gradient methods for nonconvex
  federated optimization.
\newblock \emph{arXiv preprint arXiv:2006.07013}, 2020.

\bibitem[Li and Richt{\'a}rik(2021)]{li2021canita}
Z.~Li and P.~Richt{\'a}rik.
\newblock {CANITA}: Faster rates for distributed convex optimization with
  communication compression.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13770--13781, 2021.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Kovalev, Qian, and
  Richt{\'a}rik]{li2020acceleration}
Z.~Li, D.~Kovalev, X.~Qian, and P.~Richt{\'a}rik.
\newblock Acceleration for compressed gradient descent in distributed and
  federated optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  5895--5904. PMLR, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Zhao, Li, and Chi]{li2022soteriafl}
Z.~Li, H.~Zhao, B.~Li, and Y.~Chi.
\newblock {SoteriaFL}: A unified framework for private federated learning with
  communication compression.
\newblock \emph{arXiv preprint arXiv:2206.09888}, 2022.

\bibitem[Liu et~al.(2021)Liu, Xie, Kenthapadi, Koyejo, and Li]{liu2021rvfr}
J.~Liu, C.~Xie, K.~Kenthapadi, O.~O. Koyejo, and B.~Li.
\newblock Rvfr: Robust vertical federated learning via feature subspace
  recovery.
\newblock \emph{1st NeurIPS Workshop on New Frontiers in Federated Learning
  (NFFL 2021)}, 2021.

\bibitem[Liu et~al.(2019)Liu, Kang, Zhang, Li, Cheng, Chen, Hong, and
  Yang]{liu2019communication}
Y.~Liu, Y.~Kang, X.~Zhang, L.~Li, Y.~Cheng, T.~Chen, M.~Hong, and Q.~Yang.
\newblock A communication efficient collaborative learning framework for
  distributed features.
\newblock \emph{arXiv preprint arXiv:1912.11187}, 2019.

\bibitem[Lu et~al.(2020)Lu, Li, He, Wang, Narayanan, and Chan]{lu2020robust}
H.~Lu, M.~Li, T.~He, S.~Wang, V.~Narayanan, and K.~S. Chan.
\newblock Robust coreset construction for distributed machine learning.
\newblock \emph{{IEEE} J. Sel. Areas Commun.}, 38\penalty0 (10):\penalty0
  2400--2417, 2020.

\bibitem[Lucic et~al.(2017)Lucic, Faulkner, Krause, and
  Feldman]{lucic2017training}
M.~Lucic, M.~Faulkner, A.~Krause, and D.~Feldman.
\newblock Training {G}aussian mixture models at scale via coresets.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 5885--5909, 2017.

\bibitem[Luo et~al.(2021)Luo, Wu, Xiao, and Ooi]{luo2021feature}
X.~Luo, Y.~Wu, X.~Xiao, and B.~C. Ooi.
\newblock Feature inference attack on model predictions in vertical federated
  learning.
\newblock In \emph{2021 IEEE 37th International Conference on Data Engineering
  (ICDE)}, pages 181--192. IEEE, 2021.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mishchenko et~al.(2019)Mishchenko, Gorbunov, Tak{\'a}{\v{c}}, and
  Richt{\'a}rik]{mishchenko2019distributed}
K.~Mishchenko, E.~Gorbunov, M.~Tak{\'a}{\v{c}}, and P.~Richt{\'a}rik.
\newblock Distributed learning with compressed gradient differences.
\newblock \emph{arXiv preprint arXiv:1901.09269}, 2019.

\bibitem[Mitra et~al.(2021)Mitra, Jaafar, Pappas, and Hassani]{mitra2021linear}
A.~Mitra, R.~Jaafar, G.~J. Pappas, and H.~Hassani.
\newblock Linear convergence in federated learning: Tackling client
  heterogeneity and sparse gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14606--14619, 2021.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Phillips(2016)]{phillips2016coresets}
J.~M. Phillips.
\newblock Coresets and sketches.
\newblock \emph{CoRR}, abs/1601.00617, 2016.

\bibitem[Razborov(1992)]{razborov1992distributional}
A.~Razborov.
\newblock On the distributional complexity of disjointness.
\newblock \emph{Theoretical Computer Science}, 106\penalty0 (2):\penalty0
  385--390, 1992.

\bibitem[Richt{\'a}rik et~al.(2021)Richt{\'a}rik, Sokolov, and
  Fatkhullin]{richtarik2021ef21}
P.~Richt{\'a}rik, I.~Sokolov, and I.~Fatkhullin.
\newblock {EF21}: A new, simpler, theoretically better, and practically faster
  error feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Richt{\'a}rik et~al.(2022)Richt{\'a}rik, Sokolov, Gasanov, Fatkhullin,
  Li, and Gorbunov]{richtarik20223pc}
P.~Richt{\'a}rik, I.~Sokolov, E.~Gasanov, I.~Fatkhullin, Z.~Li, and
  E.~Gorbunov.
\newblock {3PC}: Three point compressors for communication-efficient
  distributed training and a better theory for lazy aggregation.
\newblock In \emph{International Conference on Machine Learning}, pages
  18596--18648. PMLR, 2022.

\bibitem[Sun et~al.(2021)Sun, Yang, Yao, Zhang, Gao, Xie, and
  Wang]{sun2021vertical}
J.~Sun, X.~Yang, Y.~Yao, A.~Zhang, W.~Gao, J.~Xie, and C.~Wang.
\newblock Vertical federated learning without revealing intersection
  membership.
\newblock \emph{arXiv preprint arXiv:2106.05508}, 2021.

\bibitem[Tian et~al.(2020)Tian, Zhang, Hou, Liu, and Ren]{tian2020federboost}
Z.~Tian, R.~Zhang, X.~Hou, J.~Liu, and K.~Ren.
\newblock Federboost: Private federated learning for gbdt.
\newblock \emph{arXiv preprint arXiv:2011.02796}, 2020.

\bibitem[Truex et~al.(2020)Truex, Liu, Chow, Gursoy, and Wei]{truex2020ldp}
S.~Truex, L.~Liu, K.-H. Chow, M.~E. Gursoy, and W.~Wei.
\newblock Ldp-fed: Federated learning with local differential privacy.
\newblock In \emph{Proceedings of the Third ACM International Workshop on Edge
  Systems, Analytics and Networking}, pages 61--66, 2020.

\bibitem[Varadarajan and Xiao(2012)]{varadarajan2012sensitivity}
K.~Varadarajan and X.~Xiao.
\newblock On the sensitivity of shape fitting problems.
\newblock In \emph{IARCS Annual Conference on Foundations of Software
  Technology and Theoretical Computer Science (FSTTCS 2012)}. Schloss
  Dagstuhl-Leibniz-Zentrum fuer Informatik, 2012.

\bibitem[Vassilvitskii and Arthur(2006)]{vassilvitskii2006k}
S.~Vassilvitskii and D.~Arthur.
\newblock k-means++: The advantages of careful seeding.
\newblock In \emph{Proceedings of the eighteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1027--1035, 2006.

\bibitem[Vempala et~al.(2020)Vempala, Wang, and
  Woodruff]{vempala2020communication}
S.~S. Vempala, R.~Wang, and D.~P. Woodruff.
\newblock The communication complexity of optimization.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1733--1752. SIAM, 2020.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Charles, Xu, Joshi, McMahan,
  Al-Shedivat, Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
J.~Wang, Z.~Charles, Z.~Xu, G.~Joshi, H.~B. McMahan, M.~Al-Shedivat, G.~Andrew,
  S.~Avestimehr, K.~Daly, D.~Data, et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Guo, and Ding]{wang2021robust}
Z.~Wang, Y.~Guo, and H.~Ding.
\newblock Robust and fully-dynamic coreset for continuous-and-bounded learning
  (with outliers) problems.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Wei et~al.(2020)Wei, Li, Ding, Ma, Yang, Farokhi, Jin, Quek, and
  Poor]{wei2020federated}
K.~Wei, J.~Li, M.~Ding, C.~Ma, H.~H. Yang, F.~Farokhi, S.~Jin, T.~Q. Quek, and
  H.~V. Poor.
\newblock Federated learning with differential privacy: Algorithms and
  performance analysis.
\newblock \emph{IEEE Transactions on Information Forensics and Security},
  15:\penalty0 3454--3469, 2020.

\bibitem[Wei et~al.(2022)Wei, Li, Ma, Ding, Wei, Wu, Chen, and
  Ranbaduge]{wei2022vertical}
K.~Wei, J.~Li, C.~Ma, M.~Ding, S.~Wei, F.~Wu, G.~Chen, and T.~Ranbaduge.
\newblock Vertical federated learning: Challenges, methodologies and
  experiments.
\newblock \emph{arXiv preprint arXiv:2202.04309}, 2022.

\bibitem[Weng et~al.(2020)Weng, Zhang, Xue, Wei, Ji, and Zong]{weng2020privacy}
H.~Weng, J.~Zhang, F.~Xue, T.~Wei, S.~Ji, and Z.~Zong.
\newblock Privacy leakage of real-world vertical federated learning.
\newblock \emph{arXiv preprint arXiv:2011.09290}, 2020.

\bibitem[Yang et~al.(2019{\natexlab{a}})Yang, Fan, Chen, Shi, and
  Yang]{yang2019quasi}
K.~Yang, T.~Fan, T.~Chen, Y.~Shi, and Q.~Yang.
\newblock A quasi-newton method based vertical federated learning framework for
  logistic regression.
\newblock \emph{arXiv preprint arXiv:1912.00513}, 2019{\natexlab{a}}.

\bibitem[Yang et~al.(2019{\natexlab{b}})Yang, Liu, Chen, and
  Tong]{yang2019federated}
Q.~Yang, Y.~Liu, T.~Chen, and Y.~Tong.
\newblock Federated machine learning: Concept and applications.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  10\penalty0 (2):\penalty0 1--19, 2019{\natexlab{b}}.

\bibitem[Yang et~al.(2019{\natexlab{c}})Yang, Ren, Zhou, and
  Liu]{yang2019parallel}
S.~Yang, B.~Ren, X.~Zhou, and L.~Liu.
\newblock Parallel distributed logistic regression for vertical federated
  learning without third-party coordinator.
\newblock \emph{arXiv preprint arXiv:1911.09824}, 2019{\natexlab{c}}.

\bibitem[Zhao et~al.(2021{\natexlab{a}})Zhao, Burlachenko, Li, and
  Richt{\'a}rik]{zhao2021faster}
H.~Zhao, K.~Burlachenko, Z.~Li, and P.~Richt{\'a}rik.
\newblock Faster rates for compressed federated learning with client-variance
  reduction.
\newblock \emph{arXiv preprint arXiv:2112.13097}, 2021{\natexlab{a}}.

\bibitem[Zhao et~al.(2021{\natexlab{b}})Zhao, Li, and
  Richt{\'a}rik]{zhao2021fedpage}
H.~Zhao, Z.~Li, and P.~Richt{\'a}rik.
\newblock {FedPAGE}: A fast local stochastic gradient method for
  communication-efficient federated learning.
\newblock \emph{arXiv preprint arXiv:2108.04755}, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2022)Zhao, Li, Li, Richt{\'a}rik, and Chi]{zhao2022beer}
H.~Zhao, B.~Li, Z.~Li, P.~Richt{\'a}rik, and Y.~Chi.
\newblock {BEER}: Fast {$O(1/T)$} rate for decentralized nonconvex optimization
  with communication compression.
\newblock \emph{arXiv preprint arXiv:2201.13320}, 2022.

\bibitem[Zhao et~al.(2020)Zhao, Zhao, Yang, Wang, Wang, Lyu, Niyato, and
  Lam]{zhao2020local}
Y.~Zhao, J.~Zhao, M.~Yang, T.~Wang, N.~Wang, L.~Lyu, D.~Niyato, and K.-Y. Lam.
\newblock Local differential privacy-based federated learning for internet of
  things.
\newblock \emph{IEEE Internet of Things Journal}, 8\penalty0 (11):\penalty0
  8836--8853, 2020.

\end{thebibliography}
