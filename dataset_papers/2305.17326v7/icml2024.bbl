\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Li(2020)Allen-Zhu and Li]{allen2020towards}
Allen-Zhu, Z. and Li, Y.
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020.

\bibitem[Amari(2014)]{amari2014information}
Amari, S.-i.
\newblock Information geometry of positive measures and positive-definite
  matrices: Decomposable dually flat structure.
\newblock \emph{Entropy}, 16\penalty0 (4):\penalty0 2131--2145, 2014.

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer,
  Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma}
Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M.~D., McAleer, S., Jiang,
  A.~Q., Deng, J., Biderman, S., and Welleck, S.
\newblock Llemma: An open language model for mathematics.
\newblock \emph{arXiv preprint arXiv:2310.10631}, 2023.

\bibitem[Bach(2022)]{bach2022information}
Bach, F.
\newblock Information theory with kernel methods.
\newblock \emph{IEEE Transactions on Information Theory}, 2022.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman2019learning}
Bachman, P., Hjelm, R.~D., and Buchwalter, W.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock \emph{arXiv preprint arXiv:1906.00910}, 2019.

\bibitem[Balestriero \& LeCun(2022)Balestriero and
  LeCun]{balestriero2022contrastive}
Balestriero, R. and LeCun, Y.
\newblock Contrastive and non-contrastive self-supervised learning recover
  global and local spectral embedding methods.
\newblock \emph{arXiv preprint arXiv:2205.11508}, 2022.

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg}
Bardes, A., Ponce, J., and LeCun, Y.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.04906}, 2021.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9912--9924, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  9650--9660, 2021.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1597--1607. PMLR, 2020{\natexlab{a}}.

\bibitem[Chen \& He(2021)Chen and He]{chen2021exploring}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on Computer Vision
  and Pattern Recognition}, pp.\  15750--15758, 2021.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and
  He]{chen2020improved}
Chen, X., Fan, H., Girshick, R., and He, K.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{b}}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,
  M., Tworek, J., Hilton, J., Nakano, R., et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cover(1999)]{cover1999elements}
Cover, T.~M.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Galanti et~al.(2021)Galanti, Gy{\"o}rgy, and Hutter]{galanti2021role}
Galanti, T., Gy{\"o}rgy, A., and Hutter, M.
\newblock On the role of neural collapse in transfer learning.
\newblock \emph{arXiv preprint arXiv:2112.15121}, 2021.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao2021simcse}
Gao, T., Yao, X., and Chen, D.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}, 2021.

\bibitem[Garrido et~al.(2022)Garrido, Chen, Bardes, Najman, and
  Lecun]{garrido2022duality}
Garrido, Q., Chen, Y., Bardes, A., Najman, L., and Lecun, Y.
\newblock On the duality between contrastive and non-contrastive
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2206.02574}, 2022.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.,
  Buchatskaya, E., Doersch, C., Avila~Pires, B., Guo, Z., Gheshlaghi~Azar, M.,
  et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in Neural Iformation Processing Systems}, 33:\penalty0
  21271--21284, 2020.

\bibitem[Hall(2013)]{hall2013lie}
Hall, B.~C.
\newblock \emph{Lie groups, Lie algebras, and representations}.
\newblock Springer, 2013.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5000--5011, 2021.

\bibitem[HaoChen et~al.(2022)HaoChen, Wei, Kumar, and Ma]{haochen2022beyond}
HaoChen, J.~Z., Wei, C., Kumar, A., and Ma, T.
\newblock Beyond separability: Analyzing the linear transferability of
  contrastive representations to related subpopulations.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{DBLP:journals/corr/HeZRS15}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{CoRR}, abs/1512.03385, 2015.
\newblock URL \url{http://arxiv.org/abs/1512.03385}.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  9729--9738, 2020.

\bibitem[Henaff(2020)]{henaff2020data}
Henaff, O.
\newblock Data-efficient image recognition with contrastive predictive coding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4182--4192. PMLR, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang,
  Song, and Steinhardt]{hendrycks2021measuring}
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song,
  D., and Steinhardt, J.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}, 2021.

\bibitem[Higham(2008)]{higham2008functions}
Higham, N.~J.
\newblock \emph{Functions of matrices: theory and computation}.
\newblock SIAM, 2008.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
Hjelm, R.~D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,
  Trischler, A., and Bengio, Y.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hu et~al.(2022)Hu, Liu, Zhou, Wang, and Huang]{hu2022your}
Hu, T., Liu, Z., Zhou, F., Wang, W., and Huang, W.
\newblock Your contrastive learning is secretly doing stochastic neighbor
  embedding.
\newblock \emph{arXiv preprint arXiv:2205.14814}, 2022.

\bibitem[Hua(2021)]{Hua2021SimSiam}
Hua, T.
\newblock Simsiam.
\newblock \url{https://github.com/PatrickHua/SimSiam}, 2021.

\bibitem[Huang et~al.(2021)Huang, Yi, and Zhao]{huang2021towards}
Huang, W., Yi, M., and Zhao, X.
\newblock Towards the generalization of contrastive self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2111.00743}, 2021.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{DBLP:journals/corr/IoffeS15}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{CoRR}, abs/1502.03167, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03167}.

\bibitem[Kim et~al.(2023)Kim, Kang, Hwang, Shin, and Rhee]{kim2023vne}
Kim, J., Kang, S., Hwang, D., Shin, J., and Rhee, W.
\newblock Vne: An effective method for improving deep representation by
  manipulating eigenvalue distribution.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  3799--3810, 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Citeseer}, 2009.

\bibitem[Lee et~al.(2020)Lee, Lei, Saunshi, and Zhuo]{lee2020predicting}
Lee, J.~D., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2008.01064}, 2020.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee2021predicting}
Lee, J.~D., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 309--323, 2021.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski,
  Ramasesh, Slone, Anil, Schlag, Gutman-Solo, Wu, Neyshabur, Gur-Ari, and
  Misra]{lewkowycz2022solving}
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh,
  V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B.,
  Gur-Ari, G., and Misra, V.
\newblock Solving quantitative reasoning problems with language models, 2022.

\bibitem[Li et~al.(2021)Li, Pogodin, Sutherland, and Gretton]{li2021self}
Li, Y., Pogodin, R., Sutherland, D.~J., and Gretton, A.
\newblock Self-supervised learning with kernel dependence maximization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 15543--15556, 2021.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona,
  Ramanan, Doll{\'{a}}r, and Zitnick]{MSCOCO2014Lin}
Lin, T., Maire, M., Belongie, S.~J., Bourdev, L.~D., Girshick, R.~B., Hays, J.,
  Perona, P., Ramanan, D., Doll{\'{a}}r, P., and Zitnick, C.~L.
\newblock Microsoft {COCO:} common objects in context.
\newblock \emph{CoRR}, abs/1405.0312, 2014.
\newblock URL \url{http://arxiv.org/abs/1405.0312}.

\bibitem[Liu et~al.(2022)Liu, Wang, Li, and Wang]{liu2022self}
Liu, X., Wang, Z., Li, Y.-L., and Wang, S.
\newblock Self-supervised learning via maximum entropy coding.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 34091--34105, 2022.

\bibitem[Logeswaran \& Lee(2018)Logeswaran and Lee]{logeswaran2018efficient}
Logeswaran, L. and Lee, H.
\newblock An efficient framework for learning sentence representations.
\newblock \emph{arXiv preprint arXiv:1803.02893}, 2018.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and
  Hutter]{DBLP:journals/corr/LoshchilovH16a}
Loshchilov, I. and Hutter, F.
\newblock {SGDR:} stochastic gradient descent with restarts.
\newblock \emph{CoRR}, abs/1608.03983, 2016.
\newblock URL \url{http://arxiv.org/abs/1608.03983}.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and
  Zhang]{luo2023wizardmath}
Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen,
  S., and Zhang, D.
\newblock Wizardmath: Empowering mathematical reasoning for large language
  models via reinforced evol-instruct, 2023.

\bibitem[Ma et~al.(2023)Ma, You, Reddi, Jayasumana, Jain, Yu, Chang, and
  Kumar]{ma2023do}
Ma, J., You, C., Reddi, S.~J., Jayasumana, S., Jain, H., Yu, F., Chang, S.-F.,
  and Kumar, S.
\newblock Do we need neural collapse? learning diverse features for
  fine-grained and long-tail classification.
\newblock \emph{OpenReviewNet}, 2023.

\bibitem[Ma et~al.(2007)Ma, Derksen, Hong, and Wright]{ma2007segmentation}
Ma, Y., Derksen, H., Hong, W., and Wright, J.
\newblock Segmentation of multivariate mixed data via lossy data coding and
  compression.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 29\penalty0 (9):\penalty0 1546--1562, 2007.

\bibitem[Misra \& Maaten(2020)Misra and Maaten]{misra2020self}
Misra, I. and Maaten, L. v.~d.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  6707--6717, 2020.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{Nair2010RectifiedLU}
Nair, V. and Hinton, G.~E.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{International Conference on Machine Learning}, 2010.

\bibitem[Nozawa \& Sato(2021)Nozawa and Sato]{nozawa2021understanding}
Nozawa, K. and Sato, I.
\newblock Understanding negative samples in instance discriminative
  self-supervised representation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5784--5797, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Papyan, V., Han, X., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Paster et~al.(2023)Paster, Santos, Azerbayev, and
  Ba]{paster2023openwebmath}
Paster, K., Santos, M.~D., Azerbayev, Z., and Ba, J.
\newblock Openwebmath: An open dataset of high-quality mathematical web text.
\newblock \emph{arXiv preprint arXiv:2310.06786}, 2023.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.]{pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Oct):\penalty0 2825--2830, 2011.

\bibitem[Pokle et~al.(2022)Pokle, Tian, Li, and Risteski]{pokle2022contrasting}
Pokle, A., Tian, J., Li, Y., and Risteski, A.
\newblock Contrasting the landscape of contrastive and non-contrastive
  learning.
\newblock \emph{arXiv preprint arXiv:2203.15702}, 2022.

\bibitem[Robinson et~al.(2021)Robinson, Chuang, Sra, and
  Jegelka]{robinson2021contrastive}
Robinson, J.~D., Chuang, C.-Y., Sra, S., and Jegelka, S.
\newblock Contrastive learning with hard negative samples.
\newblock In \emph{ICLR}, 2021.

\bibitem[Roy \& Vetterli(2007)Roy and Vetterli]{roy2007effective}
Roy, O. and Vetterli, M.
\newblock The effective rank: A measure of effective dimensionality.
\newblock In \emph{2007 15th European signal processing conference}, pp.\
  606--610. IEEE, 2007.

\bibitem[Shen et~al.(2022)Shen, Jones, Kumar, Xie, HaoChen, Ma, and
  Liang]{shen2022connect}
Shen, K., Jones, R.~M., Kumar, A., Xie, S.~M., HaoChen, J.~Z., Ma, T., and
  Liang, P.
\newblock Connect, not collapse: Explaining contrastive learning for
  unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  19847--19878. PMLR, 2022.

\bibitem[Tan et~al.(2023{\natexlab{a}})Tan, Yang, Huang, Yuan, and
  Zhang]{tan2023information}
Tan, Z., Yang, J., Huang, W., Yuan, Y., and Zhang, Y.
\newblock Information flow in self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2309.17281}, 2023{\natexlab{a}}.

\bibitem[Tan et~al.(2023{\natexlab{b}})Tan, Zhang, Yang, and
  Yuan]{tan2023contrastive}
Tan, Z., Zhang, Y., Yang, J., and Yuan, Y.
\newblock Contrastive learning is spectral clustering on similarity graph.
\newblock \emph{arXiv preprint arXiv:2303.15103}, 2023{\natexlab{b}}.

\bibitem[Tao et~al.(2022)Tao, Wang, Zhu, Dong, Song, Huang, and
  Dai]{tao2022exploring}
Tao, C., Wang, H., Zhu, X., Dong, J., Song, S., Huang, G., and Dai, J.
\newblock Exploring the equivalence of siamese self-supervised learning via a
  unified gradient framework.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  14431--14440, 2022.

\bibitem[Tian(2022)]{tian2022deep}
Tian, Y.
\newblock Deep contrastive learning is provably (almost) principal component
  analysis.
\newblock \emph{arXiv preprint arXiv:2201.12680}, 2022.

\bibitem[Tian et~al.(2019)Tian, Krishnan, and Isola]{tian2019contrastive}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock \emph{arXiv preprint arXiv:1906.05849}, 2019.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Krishnan, and
  Isola]{tian2020contrastive}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16}, pp.\  776--794.
  Springer, 2020{\natexlab{a}}.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P.
\newblock What makes for good views for contrastive learning.
\newblock \emph{arXiv preprint arXiv:2005.10243}, 2020{\natexlab{b}}.

\bibitem[Tian et~al.(2021)Tian, Chen, and Ganguli]{tian2021understanding}
Tian, Y., Chen, X., and Ganguli, S.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10268--10278. PMLR, 2021.

\bibitem[Tong et~al.(2023)Tong, Chen, Ma, and Lecun]{tong2023emp}
Tong, S., Chen, Y., Ma, Y., and Lecun, Y.
\newblock Emp-ssl: Towards self-supervised learning in one training epoch.
\newblock \emph{arXiv preprint arXiv:2304.03977}, 2023.

\bibitem[Tosh et~al.(2020)Tosh, Krishnamurthy, and Hsu]{tosh2020contrastive}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{arXiv:2003.02234}, 2020.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1179--1206. PMLR, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tsai et~al.(2021{\natexlab{a}})Tsai, Bai, Morency, and
  Salakhutdinov]{DBLP:journals/corr/abs-2104-13712}
Tsai, Y.~H., Bai, S., Morency, L., and Salakhutdinov, R.
\newblock A note on connecting barlow twins with negative-sample-free
  contrastive learning.
\newblock \emph{CoRR}, abs/2104.13712, 2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2104.13712}.

\bibitem[Tsai et~al.(2021{\natexlab{b}})Tsai, Bai, Morency, and
  Salakhutdinov]{tsai2021note}
Tsai, Y.-H.~H., Bai, S., Morency, L.-P., and Salakhutdinov, R.
\newblock A note on connecting barlow twins with negative-sample-free
  contrastive learning.
\newblock \emph{arXiv preprint arXiv:2104.13712}, 2021{\natexlab{b}}.

\bibitem[van~der Maaten \& Hinton(2008)van~der Maaten and
  Hinton]{Maaten2008VisualizingDU}
van~der Maaten, L. and Hinton, G.~E.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 2579--2605,
  2008.

\bibitem[von Neumann(1932)]{john1932mathematische}
von Neumann, J.
\newblock Mathematische grundlagen der quantenmechanik, 1932.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang2020understanding}
Wang, T. and Isola, P.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9929--9939. PMLR, 2020.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang2022chaos}
Wang, Y., Zhang, Q., Wang, Y., Yang, J., and Lin, Z.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock \emph{arXiv preprint arXiv:2203.13457}, 2022.

\bibitem[Wen \& Li(2022)Wen and Li]{wen2022mechanism}
Wen, Z. and Li, Y.
\newblock The mechanism of prediction head in non-contrastive self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2205.06226}, 2022.

\bibitem[Witten(2020)]{witten2020mini}
Witten, E.
\newblock A mini-introduction to information theory.
\newblock \emph{La Rivista del Nuovo Cimento}, 43\penalty0 (4):\penalty0
  187--227, 2020.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu2018unsupervised}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  3733--3742, 2018.

\bibitem[Ye et~al.(2019)Ye, Zhang, Yuen, and Chang]{ye2019unsupervised}
Ye, M., Zhang, X., Yuen, P.~C., and Chang, S.-F.
\newblock Unsupervised embedding learning via invariant and spreading instance
  feature.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  6210--6219, 2019.

\bibitem[You et~al.(2017)You, Gitman, and
  Ginsburg]{DBLP:journals/corr/abs-1708-03888}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Scaling {SGD} batch size to 32k for imagenet training.
\newblock \emph{CoRR}, abs/1708.03888, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.03888}.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and
  Liu]{yu2023metamath}
Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J.~T., Li, Z.,
  Weller, A., and Liu, W.
\newblock Metamath: Bootstrap your own mathematical questions for large
  language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12310--12320. PMLR, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhou et~al.(2022)Zhou, You, Li, Liu, Liu, Qu, and Zhu]{zhou2022all}
Zhou, J., You, C., Li, X., Liu, K., Liu, S., Qu, Q., and Zhu, Z.
\newblock Are all losses created equal: A neural collapse perspective.
\newblock \emph{arXiv preprint arXiv:2210.02192}, 2022.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and
  Qu]{zhu2021geometric}
Zhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., and Qu, Q.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29820--29834, 2021.

\bibitem[Zhuo et~al.(2023)Zhuo, Wang, Ma, and Wang]{zhuo2023towards}
Zhuo, Z., Wang, Y., Ma, J., and Wang, Y.
\newblock Towards a unified theoretical understanding of non-contrastive
  learning via rank differential mechanism.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge, and
  Brendel]{zimmermann2021contrastive}
Zimmermann, R.~S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W.
\newblock Contrastive learning inverts the data generating process.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12979--12990. PMLR, 2021.

\end{thebibliography}
