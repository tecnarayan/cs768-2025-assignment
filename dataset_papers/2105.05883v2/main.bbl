\begin{thebibliography}{20}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock {Optimization methods for large-scale machine learning}.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.
\newblock ISSN 00361445.
\newblock \doi{10.1137/16M1080173}.

\bibitem[Chen et~al.(2020)Chen, Horvath, and
  Richtarik]{Richtarik_optimal_sampling}
Chen, W., Horvath, S., and Richtarik, P.
\newblock {Optimal Client Sampling for Federated Learning}.
\newblock \emph{Workshop in NeurIPS: Privacy Preserving Machine Learning},
  2020.

\bibitem[Haddadpour et~al.(2019)Haddadpour, Kamani, Mahdavi, and
  Cadambe]{Haddadpour2019}
Haddadpour, F., Kamani, M.~M., Mahdavi, M., and Cadambe, V.~R.
\newblock {Local SGD with periodic averaging: Tighter analysis and adaptive
  synchronization}.
\newblock \emph{Advances in Neural Information Processing Systems}, 32\penalty0
  (2), 2019.
\newblock ISSN 10495258.

\bibitem[{Harry Hsu} et~al.(2019){Harry Hsu}, Qi, and Brown]{FL_and_CIFAR_dir}
{Harry Hsu}, T.~M., Qi, H., and Brown, M.
\newblock {Measuring the effects of non-identical data distribution for
  federated visual classification}.
\newblock \emph{arXiv}, 2019.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{SCAFFOLD}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock {SCAFFOLD}: Stochastic controlled averaging for federated learning.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5132--5143. PMLR,
  13--18 Jul 2020.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richtarik]{pmlr-v108-bayoumi20a}
Khaled, A., Mishchenko, K., and Richtarik, P.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  4519--4529. PMLR, 26--28 Aug 2020.

\bibitem[Krizhevsky(2009)]{CIFAR-10}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Ha]{Lecun1998}
LeCun, Y., Bottou, L., Bengio, Y., and Ha, P.
\newblock {LeNet}.
\newblock \emph{Proceedings of the IEEE}, \penalty0 (November):\penalty0 1--46,
  1998.
\newblock ISSN 00189219.
\newblock \doi{10.1109/5.726791}.

\bibitem[Li et~al.(2018)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{FedProx}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock {Federated Optimization in Heterogeneous Networks}.
\newblock \emph{Proceedings of the 1 st Adaptive \& Multitask Learning
  Workshop, Long Beach, California, 2019}, pp.\  1--28, 2018.

\bibitem[Li et~al.(2020)Li, Huang, Yang, Wang, and Zhang]{ontheconvergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Lin et~al.(2020)Lin, Stich, Patel, and Jaggi]{Lin2020Don't}
Lin, T., Stich, S.~U., Patel, K.~K., and Jaggi, M.
\newblock Don't use large mini-batches, use local sgd.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{FedAvg}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock {Communication-Efficient Learning of Deep Networks from Decentralized
  Data}.
\newblock In Singh, A. and Zhu, J. (eds.), \emph{Proceedings of the 20th
  International Conference on Artificial Intelligence and Statistics},
  volume~54 of \emph{Proceedings of Machine Learning Research}, pp.\
  1273--1282, Fort Lauderdale, FL, USA, 20--22 Apr 2017. PMLR.

\bibitem[{Nishio} \& {Yonetani}(2019){Nishio} and
  {Yonetani}]{sampling_mobile_edge}
{Nishio}, T. and {Yonetani}, R.
\newblock Client selection for federated learning with heterogeneous resources
  in mobile edge.
\newblock In \emph{ICC 2019 - 2019 IEEE International Conference on
  Communications (ICC)}, pp.\  1--7, 2019.
\newblock \doi{10.1109/ICC.2019.8761315}.

\bibitem[Sattler et~al.(2019)Sattler, M{\"{u}}ller, and Samek]{CFL}
Sattler, F., M{\"{u}}ller, K.-R., and Samek, W.
\newblock {Clustered Federated Learning: Model-Agnostic Distributed Multi-Task
  Optimization under Privacy Constraints}.
\newblock pp.\  1--16, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.01991}.

\bibitem[Stich(2019)]{stich2018local}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1g2JnRcFX}.

\bibitem[{Wang} et~al.(2019){Wang}, {Sahu}, {Yang}, {Joshi}, and {Kar}]{MATCHA}
{Wang}, J., {Sahu}, A.~K., {Yang}, Z., {Joshi}, G., and {Kar}, S.
\newblock Matcha: Speeding up decentralized sgd via matching decomposition
  sampling.
\newblock In \emph{2019 Sixth Indian Control Conference (ICC)}, pp.\  299--300,
  2019.
\newblock \doi{10.1109/ICC47138.2019.9123209}.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{FedNova}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Ward(1963)]{WARD_theorem}
Ward, J.~H.
\newblock Hierarchical grouping to optimize an objective function.
\newblock \emph{Journal of the American Statistical Association}, 58\penalty0
  (301):\penalty0 236--244, 1963.
\newblock ISSN 01621459.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, Mcmahan,
  Shamir, and Srebro]{pmlr-v119-woodworth20a}
Woodworth, B., Patel, K.~K., Stich, S., Dai, Z., Bullins, B., Mcmahan, B.,
  Shamir, O., and Srebro, N.
\newblock Is local {SGD} better than minibatch {SGD}?
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10334--10343. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/woodworth20a.html}.

\bibitem[Yu et~al.(2019)Yu, Yang, and Zhu]{Yu_Yang_Zhu_2019}
Yu, H., Yang, S., and Zhu, S.
\newblock {Parallel Restarted SGD with Faster Convergence and Less
  Communication: Demystifying Why Model Averaging Works for Deep Learning}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33\penalty0 (01):\penalty0 5693--5700, 2019.
\newblock \doi{10.1609/aaai.v33i01.33015693}.

\end{thebibliography}
