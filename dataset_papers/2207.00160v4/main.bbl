\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{LWAFF21}

\bibitem[ACG{\etalchar{+}}16]{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In {\em Proceedings of the 2016 ACM SIGSAC conference on computer and
  communications security}, pages 308--318, 2016.

\bibitem[ADF{\etalchar{+}}21]{asi2021private}
Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar.
\newblock Private adaptive gradient methods for convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  383--392. PMLR, 2021.

\bibitem[AFKT21]{afkt21}
Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar.
\newblock Private stochastic convex optimization: Optimal rates in $\ell_1$
  geometry.
\newblock {\em arXiv preprint arXiv:2103.01516}, 2021.

\bibitem[AGM{\etalchar{+}}21]{PDA-DPMD}
Ehsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas
  Steinke, Vinith~M. Suriyakumar, Om~Thakkar, and Abhradeep Thakurta.
\newblock Public data-assisted mirror descent for private model training.
\newblock {\em CoRR}, abs/2112.00193, 2021.

\bibitem[AWBR09]{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter Bartlett, and Pradeep Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 22, 2009.

\bibitem[AZG20]{aghajanyan2020intrinsic}
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.
\newblock Intrinsic dimensionality explains the effectiveness of language model
  fine-tuning.
\newblock {\em arXiv preprint arXiv:2012.13255}, 2020.

\bibitem[BBG18]{balle2018privacy}
Borja Balle, Gilles Barthe, and Marco Gaboardi.
\newblock Privacy amplification by subsampling: Tight analyses via couplings
  and divergences.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[BE02]{BE02}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em The Journal of Machine Learning Research}, 2:499--526, 2002.

\bibitem[BFGT20]{BFGT20}
Raef Bassily, Vitaly Feldman, Crist{\'o}bal Guzm{\'a}n, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock {\em Advances in Neural Information Processing Systems},
  33:4381--4391, 2020.

\bibitem[BFTT19]{bassily2019private}
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta.
\newblock Private stochastic convex optimization with optimal rates.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11279--11288, 2019.

\bibitem[BGN21]{bgn21}
Raef Bassily, Crist{\'o}bal Guzm{\'a}n, and Anupama Nandi.
\newblock Non-euclidean differentially private stochastic convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 474--499. PMLR, 2021.

\bibitem[BST14]{bassily2014private}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em 2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, pages 464--473. IEEE, 2014.

\bibitem[CMS11]{chaudhuri2011differentially}
Kamalika Chaudhuri, Claire Monteleoni, and Anand~D Sarwate.
\newblock Differentially private empirical risk minimization.
\newblock {\em Journal of Machine Learning Research}, 12(3), 2011.

\bibitem[DBH{\etalchar{+}}22]{de2022unlocking}
Soham De, Leonard Berrada, Jamie Hayes, Samuel~L Smith, and Borja Balle.
\newblock Unlocking high-accuracy differentially private image classification
  through scale.
\newblock {\em arXiv preprint arXiv:2204.13650}, 2022.

\bibitem[DBK{\etalchar{+}}20]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[DCLT18]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dem97]{demmel1997applied}
James~W Demmel.
\newblock {\em Applied numerical linear algebra}.
\newblock SIAM, 1997.

\bibitem[DR{\etalchar{+}}14]{dwork2014algorithmic}
Cynthia Dwork, Aaron Roth, et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Found. Trends Theor. Comput. Sci.}, 9(3-4):211--407, 2014.

\bibitem[FKT20]{FKT20}
Vitaly Feldman, Tomer Koren, and Kunal Talwar.
\newblock Private stochastic convex optimization: optimal rates in linear time.
\newblock In {\em Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 439--449, 2020.

\bibitem[FTS17]{fts17}
Kazuto Fukuchi, Quang~Khai Tran, and Jun Sakuma.
\newblock Differentially private empirical risk minimization with input
  perturbation.
\newblock In {\em International Conference on Discovery Science}, pages 82--90.
  Springer, 2017.

\bibitem[GARD18]{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock {\em arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[GKX19]{ghorbani2019investigation}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In {\em International Conference on Machine Learning}, pages
  2232--2241. PMLR, 2019.

\bibitem[GLL22]{gll22}
Sivakanth Gopi, Yin~Tat Lee, and Daogao Liu.
\newblock Private convex optimization via exponential mechanism.
\newblock {\em arXiv preprint arXiv:2203.00263}, 2022.

\bibitem[GTU22]{gtu22}
Arun Ganesh, Abhradeep Thakurta, and Jalaj Upadhyay.
\newblock Langevin diffusion: An almost universal algorithm for private
  euclidean (convex) optimization.
\newblock {\em arXiv preprint arXiv:2204.01585}, 2022.

\bibitem[GWG19]{granziol2019deep}
Diego Granziol, Xingchen Wan, and Timur Garipov.
\newblock Deep curvature suite.
\newblock {\em arXiv preprint arXiv:1912.09656}, 2019.

\bibitem[HSW{\etalchar{+}}21]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[HZRS16]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[INS{\etalchar{+}}19]{iyengar2019towards}
Roger Iyengar, Joseph~P Near, Dawn Song, Om~Thakkar, Abhradeep Thakurta, and
  Lun Wang.
\newblock Towards practical differentially private convex optimization.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, 2019.

\bibitem[JT14]{jain2014near}
Prateek Jain and Abhradeep~Guha Thakurta.
\newblock (near) dimension independent risk bounds for differentially private
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  476--484, 2014.

\bibitem[Kam20]{gautum14}
Gautam Kamath.
\newblock {Lecture 14 --- Private ML and Stats: Modern ML}.
\newblock \url{http://www.gautamkamath.com/CS860notes/lec14.pdf}, 2020.

\bibitem[KDRT21]{KRRT21}
Peter Kairouz, Monica~Ribero Diaz, Keith Rush, and Abhradeep Thakurta.
\newblock (nearly) dimension independent private erm with adagrad rates\\{via}
  publicly estimated subspaces.
\newblock In {\em COLT}, 2021.

\bibitem[KLL21]{KLL21}
Janardhan Kulkarni, Yin~Tat Lee, and Daogao Liu.
\newblock Private non-smooth empirical risk minimization and stochastic convex
  optimization in subquadratic steps.
\newblock {\em arXiv preprint arXiv:2103.15352}, 2021.

\bibitem[KMH{\etalchar{+}}20]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[KST12]{kifer2012private}
Daniel Kifer, Adam Smith, and Abhradeep Thakurta.
\newblock Private convex empirical risk minimization and high-dimensional
  regression.
\newblock In {\em Conference on Learning Theory}, pages 25--1, 2012.

\bibitem[LFLY18]{li2018measuring}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock {\em arXiv preprint arXiv:1804.08838}, 2018.

\bibitem[LL21]{ll21}
Daogao Liu and Zhou Lu.
\newblock Curse of dimensionality in unconstrained private convex erm.
\newblock {\em arXiv preprint arXiv:2105.13637}, 2021.

\bibitem[LOG{\etalchar{+}}19]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[LTLH21]{li2021large}
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto.
\newblock Large language models can be strong differentially private learners.
\newblock {\em arXiv preprint arXiv:2110.05679}, 2021.

\bibitem[LVS{\etalchar{+}}21]{liu2021leveraging}
Terrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, and
  Zhiwei~Steven Wu.
\newblock Leveraging public data for practical private query release.
\newblock {\em arXiv preprint arXiv:2102.08598}, 2021.

\bibitem[LWAFF21]{luo2021scalable}
Zelun Luo, Daniel~J Wu, Ehsan Adeli, and Li~Fei-Fei.
\newblock Scalable differential privacy with sparse network finetuning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5059--5068, 2021.

\bibitem[MMZ22]{ma2022dimension}
Yi-An Ma, Teodor~Vanislavov Marinov, and Tong Zhang.
\newblock Dimension independent generalization of dp-sgd for overparameterized
  smooth convex optimization.
\newblock {\em arXiv preprint arXiv:2206.01836}, 2022.

\bibitem[MRTZ17]{mcmahan2017learning}
H~Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock {\em arXiv preprint arXiv:1710.06963}, 2017.

\bibitem[MTKC22]{mehta2022large}
Harsh Mehta, Abhradeep Thakurta, Alexey Kurakin, and Ashok Cutkosky.
\newblock Large scale transfer learning for differentially private image
  classification.
\newblock {\em arXiv preprint arXiv:2205.02973}, 2022.

\bibitem[NDR17]{novikova2017e2e}
Jekaterina Novikova, Ond{\v{r}}ej Du{\v{s}}ek, and Verena Rieser.
\newblock The e2e dataset: New challenges for end-to-end generation.
\newblock {\em arXiv preprint arXiv:1706.09254}, 2017.

\bibitem[RNSS18]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[RWC{\etalchar{+}}19]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem[SCS13]{song2013stochastic}
Shuang Song, Kamalika Chaudhuri, and Anand~D Sarwate.
\newblock Stochastic gradient descent with differentially private updates.
\newblock In {\em 2013 IEEE Global Conference on Signal and Information
  Processing}, pages 245--248. IEEE, 2013.

\bibitem[SDCW19]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[SPW{\etalchar{+}}13]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem[SSTT21]{song2021evading}
Shuang Song, Thomas Steinke, Om~Thakkar, and Abhradeep Thakurta.
\newblock Evading the curse of dimensionality in unconstrained private glms.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2638--2646. PMLR, 2021.

\bibitem[STT20]{song2020characterizing}
Shuang Song, Om~Thakkar, and Abhradeep Thakurta.
\newblock Characterizing private clipped gradient descent on convex generalized
  linear problems.
\newblock {\em arXiv preprint arXiv:2006.06783}, 2020.

\bibitem[WLK{\etalchar{+}}17]{WLKCJN17}
Xi~Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey~F.
  Naughton.
\newblock Bolt-on differential privacy for scalable stochastic gradient
  descent-based analytics.
\newblock In Semih Salihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, and Dan
  Suciu, editors, {\em Proceedings of the 2017 {ACM} International Conference
  on Management of Data, {SIGMOD}}, 2017.

\bibitem[WYX17]{wang17amd}
Di~Wang, Minwei Ye, and Jinhui Xu.
\newblock Differentially private empirical risk minimization revisited: Faster
  and more general.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[YNB{\etalchar{+}}21]{yu2021differentially}
Da~Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin~A Inan, Gautam
  Kamath, Janardhan Kulkarni, Yin~Tat Lee, Andre Manoel, Lukas Wutschitz,
  et~al.
\newblock Differentially private fine-tuning of language models.
\newblock {\em arXiv preprint arXiv:2110.06500}, 2021.

\bibitem[YZCL21a]{yu2021not}
Da~Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu.
\newblock Do not let privacy overbill utility: Gradient embedding perturbation
  for private learning.
\newblock {\em arXiv preprint arXiv:2102.12677}, 2021.

\bibitem[YZCL21b]{YZCL21}
Da~Yu, Huishuai Zhang, Wei Chen, and Tie{-}Yan Liu.
\newblock Do not let privacy overbill utility: Gradient embedding perturbation
  for private learning.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[ZZMW17]{zzmw17}
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang.
\newblock Efficient private erm for smooth objectives.
\newblock In {\em IJCAI}, 2017.

\end{thebibliography}
