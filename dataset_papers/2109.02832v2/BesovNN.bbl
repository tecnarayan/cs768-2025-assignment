\begin{thebibliography}{75}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Alipanahi et~al.(2015)Alipanahi, Delong, Weirauch and
  Frey}]{Alipanahi2015PredictingTS}
\textsc{Alipanahi, B.}, \textsc{Delong, A.}, \textsc{Weirauch, M.~T.} and
  \textsc{Frey, B.~J.} (2015).
\newblock Predicting the sequence specificities of {DNA}- and {RNA}-binding
  proteins by deep learning.
\newblock \textit{Nature Biotechnology}, \textbf{33} 831--838.

\bibitem[{Barron(1993)}]{barron1993universal}
\textsc{Barron, A.~R.} (1993).
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock \textit{IEEE Transactions on Information theory}, \textbf{39}
  930--945.

\bibitem[{Blalock et~al.(2020)Blalock, Ortiz, Frankle and
  Guttag}]{blalock2020state}
\textsc{Blalock, D.}, \textsc{Ortiz, J. J.~G.}, \textsc{Frankle, J.} and
  \textsc{Guttag, J.} (2020).
\newblock What is the state of neural network pruning?
\newblock \textit{arXiv preprint arXiv:2003.03033}.

\bibitem[{Chen et~al.(2017)Chen, Papandreou, Kokkinos, Murphy and
  Yuille}]{chen2017deeplab}
\textsc{Chen, L.-C.}, \textsc{Papandreou, G.}, \textsc{Kokkinos, I.},
  \textsc{Murphy, K.} and \textsc{Yuille, A.~L.} (2017).
\newblock Deep{LAB}: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected {CRF}s.
\newblock \textit{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, \textbf{40} 834--848.

\bibitem[{Chen et~al.(2019{\natexlab{a}})Chen, Jiang, Liao and
  Zhao}]{chen2019efficient}
\textsc{Chen, M.}, \textsc{Jiang, H.}, \textsc{Liao, W.} and \textsc{Zhao, T.}
  (2019{\natexlab{a}}).
\newblock Efficient approximation of deep {ReLU} networks for functions on low
  dimensional manifolds.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Chen et~al.(2019{\natexlab{b}})Chen, Jiang, Liao and
  Zhao}]{chen1908nonparametric}
\textsc{Chen, M.}, \textsc{Jiang, H.}, \textsc{Liao, W.} and \textsc{Zhao, T.}
  (2019{\natexlab{b}}).
\newblock Nonparametric regression on low-dimensional manifolds using deep
  {ReLU} networks.
\newblock \textit{arXiv preprint arXiv:1908.01842}.

\bibitem[{Chen et~al.(2020)Chen, Liu, Liao and Zhao}]{chen2020doubly}
\textsc{Chen, M.}, \textsc{Liu, H.}, \textsc{Liao, W.} and \textsc{Zhao, T.}
  (2020).
\newblock Doubly robust off-policy learning on low-dimensional manifolds by
  deep neural networks.
\newblock \textit{arXiv preprint arXiv:2011.01797}.

\bibitem[{Chui and Mhaskar(2018)}]{chui2018deep}
\textsc{Chui, C.~K.} and \textsc{Mhaskar, H.~N.} (2018).
\newblock Deep nets for local manifold learning.
\newblock \textit{Frontiers in Applied Mathematics and Statistics}, \textbf{4}
  12.

\bibitem[{Cloninger and Klock(2020)}]{cloninger2020relu}
\textsc{Cloninger, A.} and \textsc{Klock, T.} (2020).
\newblock {ReLU} nets adapt to intrinsic dimensionality beyond the target
  domain.
\newblock \textit{arXiv preprint arXiv:2008.02545}.

\bibitem[{Conway et~al.(1987)Conway, Sloane and Bannai}]{Conway:1987:SLG:39091}
\textsc{Conway, J.~H.}, \textsc{Sloane, N. J.~A.} and \textsc{Bannai, E.}
  (1987).
\newblock \textit{Sphere-packings, Lattices, and Groups}.
\newblock Springer-Verlag, Berlin, Heidelberg.

\bibitem[{Cybenko(1989)}]{cybenko1989approximation}
\textsc{Cybenko, G.} (1989).
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \textit{Mathematics of control, signals and systems}, \textbf{2}
  303--314.

\bibitem[{DeVore and Lorentz(1993)}]{devore1993constructive}
\textsc{DeVore, R.~A.} and \textsc{Lorentz, G.~G.} (1993).
\newblock \textit{Constructive Approximation}, vol. 303.
\newblock Springer Science \& Business Media.

\bibitem[{DeVore and Popov(1988)}]{devore1988interpolation}
\textsc{DeVore, R.~A.} and \textsc{Popov, V.~A.} (1988).
\newblock Interpolation of {Besov} spaces.
\newblock \textit{Transactions of the American Mathematical Society},
  \textbf{305} 397--414.

\bibitem[{Dispa(2003)}]{dispa2003intrinsic}
\textsc{Dispa, S.} (2003).
\newblock Intrinsic characterizations of {Besov} spaces on lipschitz domains.
\newblock \textit{Mathematische Nachrichten}, \textbf{260} 21--33.

\bibitem[{D{\~u}ng(2011)}]{dung2011optimal}
\textsc{D{\~u}ng, D.} (2011).
\newblock Optimal adaptive sampling recovery.
\newblock \textit{Advances in Computational Mathematics}, \textbf{34} 1--41.

\bibitem[{Fang et~al.(2020)Fang, Feng, Huang and Zhou}]{fang2020theory}
\textsc{Fang, Z.}, \textsc{Feng, H.}, \textsc{Huang, S.} and \textsc{Zhou,
  D.-X.} (2020).
\newblock Theory of deep convolutional neural networks {II}: Spherical
  analysis.
\newblock \textit{Neural Networks}, \textbf{131} 154--162.

\bibitem[{Federer(1959)}]{federer1959curvature}
\textsc{Federer, H.} (1959).
\newblock Curvature measures.
\newblock \textit{Transactions of the American Mathematical Society},
  \textbf{93} 418--491.

\bibitem[{Geer and van~de Geer(2000)}]{geer2000empirical}
\textsc{Geer, S.~A.} and \textsc{van~de Geer, S.} (2000).
\newblock \textit{Empirical Processes in M-estimation}, vol.~6.
\newblock Cambridge University press.

\bibitem[{Geller and Pesenson(2011)}]{geller2011band}
\textsc{Geller, D.} and \textsc{Pesenson, I.~Z.} (2011).
\newblock Band-limited localized parseval frames and {Besov} spaces on compact
  homogeneous manifolds.
\newblock \textit{Journal of Geometric Analysis}, \textbf{21} 334--371.

\bibitem[{Girshick(2015)}]{girshick2015fast}
\textsc{Girshick, R.} (2015).
\newblock Fast {R-CNN}.
\newblock In \textit{Proceedings of the IEEE International Conference on
  Computer Vision}.

\bibitem[{Gong et~al.(2019)Gong, Boddeti and Jain}]{gong2019intrinsic}
\textsc{Gong, S.}, \textsc{Boddeti, V.~N.} and \textsc{Jain, A.~K.} (2019).
\newblock On the intrinsic dimensionality of image representations.
\newblock In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}.

\bibitem[{Graves et~al.(2013)Graves, Mohamed and Hinton}]{graves2013speech}
\textsc{Graves, A.}, \textsc{Mohamed, A.-r.} and \textsc{Hinton, G.} (2013).
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \textit{2013 IEEE International Conference on Acoustics, Speech
  and Signal Processing}. IEEE.

\bibitem[{Hamers and Kohler(2006)}]{hamers2006nonasymptotic}
\textsc{Hamers, M.} and \textsc{Kohler, M.} (2006).
\newblock Nonasymptotic bounds on the ${L}_2$ error of neural network
  regression estimates.
\newblock \textit{Annals of the Institute of Statistical Mathematics},
  \textbf{58} 131--151.

\bibitem[{Han et~al.(2016)Han, Pool, Narang, Mao, Gong, Tang, Elsen, Vajda,
  Paluri, Tran et~al.}]{han2016dsd}
\textsc{Han, S.}, \textsc{Pool, J.}, \textsc{Narang, S.}, \textsc{Mao, H.},
  \textsc{Gong, E.}, \textsc{Tang, S.}, \textsc{Elsen, E.}, \textsc{Vajda, P.},
  \textsc{Paluri, M.}, \textsc{Tran, J.} \textsc{et~al.} (2016).
\newblock Dsd: Dense-sparse-dense training for deep neural networks.
\newblock \textit{arXiv preprint arXiv:1607.04381}.

\bibitem[{Han et~al.(2015)Han, Pool, Tran and Dally}]{han2015learning}
\textsc{Han, S.}, \textsc{Pool, J.}, \textsc{Tran, J.} and \textsc{Dally,
  W.~J.} (2015).
\newblock Learning both weights and connections for efficient neural networks.
\newblock \textit{arXiv preprint arXiv:1506.02626}.

\bibitem[{He et~al.(2016)He, Zhang, Ren and Sun}]{he2016deep}
\textsc{He, K.}, \textsc{Zhang, X.}, \textsc{Ren, S.} and \textsc{Sun, J.}
  (2016).
\newblock Deep residual learning for image recognition.
\newblock In \textit{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}.

\bibitem[{Hinton and Salakhutdinov(2006)}]{hinton2006reducing}
\textsc{Hinton, G.~E.} and \textsc{Salakhutdinov, R.~R.} (2006).
\newblock Reducing the dimensionality of data with neural networks.
\newblock \textit{Science}, \textbf{313} 504--507.

\bibitem[{Hu et~al.(2020)Hu, Shang and Cheng}]{hu2020sharp}
\textsc{Hu, T.}, \textsc{Shang, Z.} and \textsc{Cheng, G.} (2020).
\newblock Sharp rate of convergence for deep neural network classifiers under
  the teacher-student setting.
\newblock \textit{arXiv preprint arXiv:2001.06892}.

\bibitem[{Huang et~al.(2018)Huang, Ash, Langford and
  Schapire}]{huang2018learning}
\textsc{Huang, F.}, \textsc{Ash, J.}, \textsc{Langford, J.} and
  \textsc{Schapire, R.} (2018).
\newblock Learning deep resnet blocks sequentially using boosting theory.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Jaffard et~al.(2001)Jaffard, Meyer and Ryan}]{jaffard2001wavelets}
\textsc{Jaffard, S.}, \textsc{Meyer, Y.} and \textsc{Ryan, R.~D.} (2001).
\newblock \textit{Wavelets: tools for science and technology}.
\newblock SIAM.

\bibitem[{Jiang et~al.(2017)Jiang, Jiang, Zhi, Dong, Li, Ma, Wang, Dong, Shen
  and Wang}]{jiang2017artificial}
\textsc{Jiang, F.}, \textsc{Jiang, Y.}, \textsc{Zhi, H.}, \textsc{Dong, Y.},
  \textsc{Li, H.}, \textsc{Ma, S.}, \textsc{Wang, Y.}, \textsc{Dong, Q.},
  \textsc{Shen, H.} and \textsc{Wang, Y.} (2017).
\newblock Artificial intelligence in healthcare: past, present and future.
\newblock \textit{Stroke and vascular neurology}, \textbf{2} 230--243.

\bibitem[{Kim et~al.(2018)Kim, Ohn and Kim}]{kim2018fast}
\textsc{Kim, Y.}, \textsc{Ohn, I.} and \textsc{Kim, D.} (2018).
\newblock Fast convergence rates of deep neural networks for classification.
\newblock \textit{arXiv preprint arXiv:1812.03599}.

\bibitem[{Kohler and Krzy{\.z}ak(2005)}]{kohler2005adaptive}
\textsc{Kohler, M.} and \textsc{Krzy{\.z}ak, A.} (2005).
\newblock Adaptive regression estimation with multilayer feedforward neural
  networks.
\newblock \textit{Nonparametric Statistics}, \textbf{17} 891--913.

\bibitem[{Kohler et~al.(2020)Kohler, Krzyzak and Walter}]{kohler2020rate}
\textsc{Kohler, M.}, \textsc{Krzyzak, A.} and \textsc{Walter, B.} (2020).
\newblock On the rate of convergence of image classifiers based on
  convolutional neural networks.
\newblock \textit{arXiv preprint arXiv:2003.01526}.

\bibitem[{Kohler and Langer(2020)}]{kohler2020statistical}
\textsc{Kohler, M.} and \textsc{Langer, S.} (2020).
\newblock Statistical theory for image classification using deep convolutional
  neural networks with cross-entropy loss.
\newblock \textit{arXiv preprint arXiv:2011.13602}.

\bibitem[{Kohler and Mehnert(2011)}]{kohler2011analysis}
\textsc{Kohler, M.} and \textsc{Mehnert, J.} (2011).
\newblock Analysis of the rate of convergence of least squares neural network
  regression estimates in case of measurement errors.
\newblock \textit{Neural Networks}, \textbf{24} 273--279.

\bibitem[{Krizhevsky et~al.(2012)Krizhevsky, Sutskever and
  Hinton}]{krizhevsky2012imagenet}
\textsc{Krizhevsky, A.}, \textsc{Sutskever, I.} and \textsc{Hinton, G.~E.}
  (2012).
\newblock {ImageNet} classification with deep convolutional neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard
  and Jackel}]{lecun1989backpropagation}
\textsc{LeCun, Y.}, \textsc{Boser, B.}, \textsc{Denker, J.~S.},
  \textsc{Henderson, D.}, \textsc{Howard, R.~E.}, \textsc{Hubbard, W.} and
  \textsc{Jackel, L.~D.} (1989).
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \textit{Neural Computation}, \textbf{1} 541--551.

\bibitem[{Lee et~al.(2017)Lee, Ge, Ma, Risteski and Arora}]{lee2017ability}
\textsc{Lee, H.}, \textsc{Ge, R.}, \textsc{Ma, T.}, \textsc{Risteski, A.} and
  \textsc{Arora, S.} (2017).
\newblock On the ability of neural nets to express distributions.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Lee(2006)}]{lee2006riemannian}
\textsc{Lee, J.~M.} (2006).
\newblock \textit{Riemannian manifolds: an introduction to curvature}, vol.
  176.
\newblock Springer Science \& Business Media.

\bibitem[{Long et~al.(2015)Long, Shelhamer and Darrell}]{long2015fully}
\textsc{Long, J.}, \textsc{Shelhamer, E.} and \textsc{Darrell, T.} (2015).
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \textit{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}.

\bibitem[{Lu et~al.(2017)Lu, Pu, Wang, Hu and Wang}]{lu2017expressive}
\textsc{Lu, Z.}, \textsc{Pu, H.}, \textsc{Wang, F.}, \textsc{Hu, Z.} and
  \textsc{Wang, L.} (2017).
\newblock The expressive power of neural networks: A view from the width.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{McCaffrey and Gallant(1994)}]{mccaffrey1994convergence}
\textsc{McCaffrey, D.~F.} and \textsc{Gallant, A.~R.} (1994).
\newblock Convergence rates for single hidden layer feedforward networks.
\newblock \textit{Neural Networks}, \textbf{7} 147--158.

\bibitem[{Mhaskar and Micchelli(1992)}]{mhaskar1992approximation}
\textsc{Mhaskar, H.~N.} and \textsc{Micchelli, C.~A.} (1992).
\newblock Approximation by superposition of sigmoidal and radial basis
  functions.
\newblock \textit{Advances in Applied mathematics}, \textbf{13} 350--373.

\bibitem[{Miotto et~al.(2018)Miotto, Wang, Wang, Jiang and
  Dudley}]{miotto2018deep}
\textsc{Miotto, R.}, \textsc{Wang, F.}, \textsc{Wang, S.}, \textsc{Jiang, X.}
  and \textsc{Dudley, J.~T.} (2018).
\newblock Deep learning for healthcare: review, opportunities and challenges.
\newblock \textit{Briefings in Bioinformatics}, \textbf{19} 1236--1246.

\bibitem[{Montanelli and Yang(2020)}]{montanelli2020error}
\textsc{Montanelli, H.} and \textsc{Yang, H.} (2020).
\newblock Error bounds for deep {ReLU} networks using the {Kolmogorov--Arnold}
  superposition theorem.
\newblock \textit{Neural Networks}, \textbf{129} 1--6.

\bibitem[{Nakada and Imaizumi(2019)}]{nakada2019adaptive}
\textsc{Nakada, R.} and \textsc{Imaizumi, M.} (2019).
\newblock Adaptive approximation and estimation of deep neural network with
  intrinsic dimensionality.
\newblock \textit{arXiv preprint arXiv:1907.02177}.

\bibitem[{Nitanda and Suzuki(2018)}]{nitanda2018functional}
\textsc{Nitanda, A.} and \textsc{Suzuki, T.} (2018).
\newblock Functional gradient boosting based on residual network perception.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Niyogi et~al.(2008)Niyogi, Smale and Weinberger}]{niyogi2008finding}
\textsc{Niyogi, P.}, \textsc{Smale, S.} and \textsc{Weinberger, S.} (2008).
\newblock Finding the homology of submanifolds with high confidence from random
  samples.
\newblock \textit{Discrete \& Computational Geometry}, \textbf{39} 419--441.

\bibitem[{Ohn and Kim(2019)}]{ohn2019smooth}
\textsc{Ohn, I.} and \textsc{Kim, Y.} (2019).
\newblock Smooth function approximation by deep neural networks with general
  activation functions.
\newblock \textit{Entropy}, \textbf{21} 627.

\bibitem[{Oono and Suzuki(2019)}]{oono2019approximation}
\textsc{Oono, K.} and \textsc{Suzuki, T.} (2019).
\newblock Approximation and non-parametric estimation of {ResNet}-type
  convolutional neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Osher et~al.(2017)Osher, Shi and Zhu}]{osher2017low}
\textsc{Osher, S.}, \textsc{Shi, Z.} and \textsc{Zhu, W.} (2017).
\newblock Low dimensional manifold model for image processing.
\newblock \textit{SIAM Journal on Imaging Sciences}, \textbf{10} 1669--1690.

\bibitem[{Park(2009)}]{park2009convergence}
\textsc{Park, C.} (2009).
\newblock Convergence rates of generalization errors for margin-based
  classification.
\newblock \textit{Journal of Statistical Planning and Inference}, \textbf{139}
  2543--2551.

\bibitem[{Petersen and Voigtlaender(2020)}]{petersen2020equivalence}
\textsc{Petersen, P.} and \textsc{Voigtlaender, F.} (2020).
\newblock Equivalence of approximation by convolutional neural networks and
  fully-connected networks.
\newblock \textit{Proceedings of the American Mathematical Society},
  \textbf{148} 1567--1581.

\bibitem[{Schmidt-Hieber(2017)}]{schmidt2017nonparametric}
\textsc{Schmidt-Hieber, J.} (2017).
\newblock Nonparametric regression using deep neural networks with {ReLU}
  activation function.
\newblock \textit{arXiv preprint arXiv:1708.06633}.

\bibitem[{Schmidt-Hieber(2019)}]{schmidt2019deep}
\textsc{Schmidt-Hieber, J.} (2019).
\newblock Deep {ReLU} network approximation of functions on a manifold.
\newblock \textit{arXiv preprint arXiv:1908.00695}.

\bibitem[{Sermanet et~al.(2013)Sermanet, Eigen, Zhang, Mathieu, Fergus and
  LeCun}]{sermanet2013overfeat}
\textsc{Sermanet, P.}, \textsc{Eigen, D.}, \textsc{Zhang, X.}, \textsc{Mathieu,
  M.}, \textsc{Fergus, R.} and \textsc{LeCun, Y.} (2013).
\newblock Overfeat: Integrated recognition, localization and detection using
  convolutional networks.
\newblock \textit{arXiv preprint arXiv:1312.6229}.

\bibitem[{Shaham et~al.(2018)Shaham, Cloninger and
  Coifman}]{shaham2018provable}
\textsc{Shaham, U.}, \textsc{Cloninger, A.} and \textsc{Coifman, R.~R.} (2018).
\newblock Provable approximation properties for deep neural networks.
\newblock \textit{Applied and Computational Harmonic Analysis}, \textbf{44}
  537--557.

\bibitem[{Shen and Wong(1994)}]{shen1994convergence}
\textsc{Shen, X.} and \textsc{Wong, W.~H.} (1994).
\newblock Convergence rate of sieve estimates.
\newblock \textit{The Annals of Statistics} 580--615.

\bibitem[{Simonyan and Zisserman(2014)}]{simonyan2014very}
\textsc{Simonyan, K.} and \textsc{Zisserman, A.} (2014).
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \textit{arXiv preprint arXiv:1409.1556}.

\bibitem[{Spivak(1970)}]{spivak1970comprehensive}
\textsc{Spivak, M.~D.} (1970).
\newblock \textit{A comprehensive introduction to differential geometry}.
\newblock Publish or Perish.

\bibitem[{Suzuki(2019)}]{suzuki2018adaptivity}
\textsc{Suzuki, T.} (2019).
\newblock Adaptivity of deep {ReLU} network for learning in besov and mixed
  smooth besov spaces: optimal rate and curse of dimensionality.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Suzuki and Nitanda(2019)}]{suzuki2019deep}
\textsc{Suzuki, T.} and \textsc{Nitanda, A.} (2019).
\newblock Deep learning is adaptive to intrinsic dimensionality of model
  smoothness in anisotropic besov space.
\newblock \textit{arXiv preprint arXiv:1910.12799}.

\bibitem[{Tenenbaum et~al.(2000)Tenenbaum, De~Silva and
  Langford}]{tenenbaum2000global}
\textsc{Tenenbaum, J.~B.}, \textsc{De~Silva, V.} and \textsc{Langford, J.~C.}
  (2000).
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock \textit{Science}, \textbf{290} 2319--2323.

\bibitem[{Triebel(1983)}]{triebel1983theory}
\textsc{Triebel, H.} (1983).
\newblock \textit{Theory of Function Spaces}.
\newblock Modern Birkh\"{a}user Classics, Birkh\"{a}user Basel.

\bibitem[{Triebel(1992)}]{triebel1994theory}
\textsc{Triebel, H.} (1992).
\newblock \textit{Theory of function spaces II}.
\newblock Monographs in Mathematics, Birkh\"{a}user Basel.

\bibitem[{Tu(2010)}]{tu2010introduction}
\textsc{Tu, L.} (2010).
\newblock \textit{An Introduction to Manifolds}.
\newblock Universitext, Springer New York.

\bibitem[{Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun,
  Cao, Gao, Macherey et~al.}]{wu2016google}
\textsc{Wu, Y.}, \textsc{Schuster, M.}, \textsc{Chen, Z.}, \textsc{Le, Q.~V.},
  \textsc{Norouzi, M.}, \textsc{Macherey, W.}, \textsc{Krikun, M.},
  \textsc{Cao, Y.}, \textsc{Gao, Q.}, \textsc{Macherey, K.} \textsc{et~al.}
  (2016).
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \textit{arXiv preprint arXiv:1609.08144}.

\bibitem[{Yarotsky(2017)}]{yarotsky2017error}
\textsc{Yarotsky, D.} (2017).
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock \textit{Neural Networks}, \textbf{94} 103--114.

\bibitem[{Young et~al.(2018)Young, Hazarika, Poria and
  Cambria}]{young2018recent}
\textsc{Young, T.}, \textsc{Hazarika, D.}, \textsc{Poria, S.} and
  \textsc{Cambria, E.} (2018).
\newblock Recent trends in deep learning based natural language processing.
\newblock \textit{IEEE Computational Intelligence Magazine}, \textbf{13}
  55--75.

\bibitem[{Zagoruyko and Komodakis(2016)}]{zagoruyko2016wide}
\textsc{Zagoruyko, S.} and \textsc{Komodakis, N.} (2016).
\newblock Wide residual networks.
\newblock \textit{arXiv preprint arXiv:1605.07146}.

\bibitem[{Zhang et~al.(2020)Zhang, Wu, Zhang, Zhu, Zhang, Lin, Sun, He,
  Mueller, Manmatha et~al.}]{zhang2020resnest}
\textsc{Zhang, H.}, \textsc{Wu, C.}, \textsc{Zhang, Z.}, \textsc{Zhu, Y.},
  \textsc{Zhang, Z.}, \textsc{Lin, H.}, \textsc{Sun, Y.}, \textsc{He, T.},
  \textsc{Mueller, J.}, \textsc{Manmatha, R.} \textsc{et~al.} (2020).
\newblock {ResNeSt}: Split-attention networks.
\newblock \textit{arXiv preprint arXiv:2004.08955}.

\bibitem[{Zhou(2020{\natexlab{a}})}]{zhou2020theory}
\textsc{Zhou, D.-X.} (2020{\natexlab{a}}).
\newblock Theory of deep convolutional neural networks: Downsampling.
\newblock \textit{Neural Networks}, \textbf{124} 319--327.

\bibitem[{Zhou(2020{\natexlab{b}})}]{zhou2020universality}
\textsc{Zhou, D.-X.} (2020{\natexlab{b}}).
\newblock Universality of deep convolutional neural networks.
\newblock \textit{Applied and Computational Harmonic Analysis}, \textbf{48}
  787--794.

\bibitem[{Zhou and Troyanskaya(2015)}]{zhou2015predicting}
\textsc{Zhou, J.} and \textsc{Troyanskaya, O.~G.} (2015).
\newblock Predicting effects of noncoding variants with deep learning--based
  sequence model.
\newblock \textit{Nature Methods}, \textbf{12} 931--934.

\end{thebibliography}
