\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bachman and Precup(2015)]{bachman2015training}
Philip Bachman and Doina Precup.
\newblock Training deep generative models: Variations on a theme.
\newblock In \emph{NIPS Approximate Inference Workshop}, 2015.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Burda et~al.(2016)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.
\newblock Importance weighted autoencoders.
\newblock In \emph{ICLR}, 2016.

\bibitem[Carpenter et~al.(2017)Carpenter, Gelman, Hoffman, Lee, Goodrich,
  Betancourt, Brubaker, Guo, Li, and Riddell]{carpenter2017stan}
Bob Carpenter, Andrew Gelman, Matthew~D Hoffman, Daniel Lee, Ben Goodrich,
  Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen
  Riddell.
\newblock Stan: A probabilistic programming language.
\newblock \emph{Journal of statistical software}, 76\penalty0 (1), 2017.

\bibitem[Chen et~al.(2018)Chen, Tao, Zhang, Henao, and
  Duke]{Chen_Tao_Zhang_Henao_Duke_2018}
Liqun Chen, Chenyang Tao, Ruiyi Zhang, Ricardo Henao, and Lawrence~Carin Duke.
\newblock Variational inference and model selection with generalized evidence
  bounds.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and
  Jacobsen]{chen2019residual}
Tian~Qi Chen, Jens Behrmann, David~K Duvenaud, and J{\"o}rn-Henrik Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Cremer et~al.(2017)Cremer, Morris, and
  Duvenaud]{cremer2017reinterpreting}
Chris Cremer, Quaid Morris, and David Duvenaud.
\newblock Reinterpreting importance-weighted autoencoders.
\newblock In \emph{ICLR (Workshop)}, 2017.

\bibitem[Dinh et~al.(2015)Dinh, Krueger, and Bengio]{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock In \emph{ICLR (Workshop)}, 2015.

\bibitem[Dinh et~al.(2017)Dinh, Sohl{-}Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl{-}Dickstein, and Samy Bengio.
\newblock Density estimation using real {NVP}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Domke and Sheldon(2018)]{domke2018importance}
Justin Domke and Daniel Sheldon.
\newblock Importance weighting and variational inference.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Domke and Sheldon(2019)]{Domke2019DivideAC}
Justin Domke and Daniel Sheldon.
\newblock Divide and couple: Using monte carlo variational objectives for
  posterior approximation.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and
  Papamakarios]{durkan2019neural}
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
\newblock Neural spline flows.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Fjelde et~al.(2020)Fjelde, Xu, Tarek, Yalburgi, and
  Ge]{pmlr-v118-fjelde20a}
Tor~Erlend Fjelde, Kai Xu, Mohamed Tarek, Sharan Yalburgi, and Hong Ge.
\newblock Bijectors.jl: Flexible transformations for probability distributions.
\newblock In \emph{Advances in Approximate Bayesian Inference}, 2020.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Matthew~D. Hoffman, David~M. Blei, Chong Wang, and John Paisley.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Huang et~al.(2018)Huang, Krueger, Lacoste, and
  Courville]{huang2018neural}
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville.
\newblock Neural autoregressive flows.
\newblock In \emph{ICML}, 2018.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Durk~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and Max
  Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman, and
  Blei]{kucukelbir2017automatic}
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David~M Blei.
\newblock Automatic differentiation variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 430--474, 2017.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015autograd}
Dougal Maclaurin, David Duvenaud, and Ryan~P Adams.
\newblock Autograd: Effortless gradients in numpy.
\newblock In \emph{ICML AutoML (Workshop)}, 2015.

\bibitem[Maddison et~al.(2017)Maddison, Lawson, Tucker, Heess, Norouzi, Mnih,
  Doucet, and Teh]{maddison2017filtering}
Chris~J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi,
  Andriy Mnih, Arnaud Doucet, and Yee Teh.
\newblock Filtering variational objectives.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Mishkin et~al.(2018)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{Mishkin_Kunstner_Nielsen_Schmidt_Khan_2018}
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and
  Mohammad~Emtiyaz Khan.
\newblock \emph{SLANG: Fast Structured Covariance Approximations for Bayesian
  Deep Learning with Natural Gradient}.
\newblock 2018.

\bibitem[Naesseth et~al.(2018)Naesseth, Linderman, Ranganath, and
  Blei]{naesseth2017variational}
Christian~A Naesseth, Scott~W Linderman, Rajesh Ranganath, and David~M Blei.
\newblock Variational sequential monte carlo.
\newblock In \emph{AISTATS}, 2018.

\bibitem[Papamakarios et~al.()Papamakarios, Nalisnick, Rezende, Mohamed, and
  Lakshminarayanan]{papamakarios2019normalizing}
George Papamakarios, Eric~T. Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{CoRR}.
\newblock URL \url{http://arxiv.org/abs/1912.02762}.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Rainforth et~al.(2018)Rainforth, Kosiorek, Le, Maddison, Igl, Wood,
  and Teh]{rainforth2018tighter}
Tom Rainforth, Adam~R Kosiorek, Tuan~Anh Le, Chris~J Maddison, Maximilian Igl,
  Frank Wood, and Yee~Whye Teh.
\newblock Tighter variational bounds are not necessarily better.
\newblock In \emph{ICML}, 2018.

\bibitem[Ranganath et~al.(2013)Ranganath, Wang, David, and
  Xing]{ranganath2013adaptive}
Rajesh Ranganath, Chong Wang, Blei David, and Eric Xing.
\newblock An adaptive learning rate for stochastic variational inference.
\newblock In \emph{ICML}, 2013.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath2014black}
Rajesh Ranganath, Sean Gerrish, and David Blei.
\newblock Black box variational inference.
\newblock In \emph{Artificial Intelligence and Statistics}, 2014.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{ICML}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{ICML}, 2014.

\bibitem[Roeder et~al.(2017)Roeder, Wu, and Duvenaud]{roeder2017sticking}
Geoffrey Roeder, Yuhuai Wu, and David~K Duvenaud.
\newblock Sticking the landing: Simple, lower-variance gradient estimators for
  variational inference.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Saul et~al.(1996)Saul, Jaakkola, and Jordan]{saul1996mean}
Lawrence~K Saul, Tommi Jaakkola, and Michael~I Jordan.
\newblock Mean field theory for sigmoid belief networks.
\newblock \emph{Journal of artificial intelligence research}, 4:\penalty0
  61--76, 1996.

\bibitem[{Stan Developers}(2018)]{stanmodels}
{Stan Developers}.
\newblock \emph{Example Models}, 2018.
\newblock URL \url{https://github.com/stan-dev/example-models}.

\bibitem[{Stan Development Team}(2018)]{StanDevelopmentTeam2018}
{Stan Development Team}.
\newblock \emph{The Stan Core Library, Version 2.18.0.}, 2018.
\newblock URL \url{http://mc-stan.org}.

\bibitem[Tucker et~al.(2019)Tucker, Lawson, Gu, and Maddison]{tucker2018doubly}
George Tucker, Dieterich Lawson, Shixiang Gu, and Chris~J Maddison.
\newblock Doubly reparameterized gradient estimators for monte carlo
  objectives.
\newblock In \emph{ICLR}, 2019.

\bibitem[{Virtanen} et~al.(2020){Virtanen}, {Gommers}, {Oliphant}, {Haberland},
  {Reddy}, {Cournapeau}, {Burovski}, {Peterson}, {Weckesser}, {Bright}, {van
  der Walt}, {Brett}, {Wilson}, {Jarrod Millman}, {Mayorov}, {Nelson}, {Jones},
  {Kern}, {Larson}, {Carey}, {Polat}, {Feng}, {Moore}, {Vand erPlas},
  {Laxalde}, {Perktold}, {Cimrman}, {Henriksen}, {Quintero}, {Harris},
  {Archibald}, {Ribeiro}, {Pedregosa}, {van Mulbregt}, and
  {Contributors}]{scipy}
Pauli {Virtanen}, Ralf {Gommers}, Travis~E. {Oliphant}, Matt {Haberland}, Tyler
  {Reddy}, David {Cournapeau}, Evgeni {Burovski}, Pearu {Peterson}, Warren
  {Weckesser}, Jonathan {Bright}, St{\'e}fan~J. {van der Walt}, Matthew
  {Brett}, Joshua {Wilson}, K.~{Jarrod Millman}, Nikolay {Mayorov}, Andrew
  R.~J. {Nelson}, Eric {Jones}, Robert {Kern}, Eric {Larson}, CJ~{Carey},
  {\.I}lhan {Polat}, Yu~{Feng}, Eric~W. {Moore}, Jake {Vand erPlas}, Denis
  {Laxalde}, Josef {Perktold}, Robert {Cimrman}, Ian {Henriksen}, E.~A.
  {Quintero}, Charles~R {Harris}, Anne~M. {Archibald}, Ant{\^o}nio~H.
  {Ribeiro}, Fabian {Pedregosa}, Paul {van Mulbregt}, and SciPy 1.~0
  {Contributors}.
\newblock {SciPy 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{https://doi.org/10.1038/s41592-019-0686-2}.

\end{thebibliography}
