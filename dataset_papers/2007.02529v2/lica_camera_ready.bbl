\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{entropy-reg-icml19}
Zafarali Ahmed, Nicolas Le~Roux, Mohammad Norouzi, and Dale Schuurmans.
\newblock Understanding the impact of entropy on policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  151--160, 2019.

\bibitem{balduzzi2015compatible}
David Balduzzi and Muhammad Ghifary.
\newblock Compatible value gradients for reinforcement learning of continuous
  deep policies.
\newblock {\em arXiv preprint arXiv:1509.03005}, 2015.

\bibitem{cao2012overview}
Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen.
\newblock An overview of recent progress in the study of distributed
  multi-agent coordination.
\newblock {\em IEEE Transactions on Industrial informatics}, 9(1):427--438,
  2012.

\bibitem{liir}
Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao.
\newblock Liir: Learning individual intrinsic reward in multi-agent
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4405--4416, 2019.

\bibitem{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock {\em arXiv preprint arXiv:1802.01561}, 2018.

\bibitem{eysenbach2019if}
Benjamin Eysenbach and Sergey Levine.
\newblock If maxent rl is the answer, what is the question?
\newblock {\em arXiv preprint arXiv:1910.01913}, 2019.

\bibitem{fairbank-rl-value-gradient}
Michael Fairbank.
\newblock Reinforcement learning by value gradients.
\newblock {\em arXiv preprint arXiv:0803.3539}, 2008.

\bibitem{fairbank-value-gradient-learning}
Michael Fairbank and Eduardo Alonso.
\newblock Value-gradient learning.
\newblock In {\em The 2012 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2012.

\bibitem{feichtenhofer2017spatiotemporal}
Christoph Feichtenhofer, Axel Pinz, and Richard~P Wildes.
\newblock Spatiotemporal multiplier networks for video action recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4768--4777, 2017.

\bibitem{coma}
Jakob~N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
  Shimon Whiteson.
\newblock Counterfactual multi-agent policy gradients.
\newblock In {\em Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem{hypernetwork}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock {\em arXiv preprint arXiv:1609.09106}, 2016.

\bibitem{entropy-eg1}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1352--1361. JMLR. org, 2017.

\bibitem{soft-actor-critic}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{heess2015memory-rnn-svg}
Nicolas Heess, Jonathan~J Hunt, Timothy~P Lillicrap, and David Silver.
\newblock Memory-based control with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1512.04455}, 2015.

\bibitem{svg}
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and
  Yuval Tassa.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2944--2952, 2015.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{swarm-example}
Maximilian H{\"u}ttenrauch, Adrian {\v{S}}o{\v{s}}i{\'c}, and Gerhard Neumann.
\newblock Guided deep reinforcement learning for swarm systems.
\newblock {\em arXiv preprint arXiv:1709.06011}, 2017.

\bibitem{actor-attention-critic}
Shariq Iqbal and Fei Sha.
\newblock Actor-attention-critic for multi-agent reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2961--2970, 2019.

\bibitem{jaakkola1995reinforcement}
Tommi Jaakkola, Satinder~P Singh, and Michael~I Jordan.
\newblock Reinforcement learning algorithm for partially observable markov
  decision problems.
\newblock In {\em Advances in neural information processing systems}, pages
  345--352, 1995.

\bibitem{gumbel-softmax}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock {\em arXiv preprint arXiv:1611.01144}, 2016.

\bibitem{kaelbling1998planning}
Leslie~Pack Kaelbling, Michael~L Littman, and Anthony~R Cassandra.
\newblock Planning and acting in partially observable stochastic domains.
\newblock {\em Artificial intelligence}, 101(1-2):99--134, 1998.

\bibitem{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{kinnear1994advances}
Kenneth~E Kinnear, William~B Langdon, Lee Spector, Peter~J Angeline, and
  Una-May O'Reilly.
\newblock {\em Advances in genetic programming}, volume~3.
\newblock MIT press, 1994.

\bibitem{kraemer2016multi}
Landon Kraemer and Bikramjit Banerjee.
\newblock Multi-agent reinforcement learning as a rehearsal for decentralized
  planning.
\newblock {\em Neurocomputing}, 190:82--94, 2016.

\bibitem{ddpg}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{maddpg}
Ryan Lowe, Yi~I Wu, Aviv Tamar, Jean Harb, OpenAI~Pieter Abbeel, and Igor
  Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In {\em Advances in neural information processing systems}, pages
  6379--6390, 2017.

\bibitem{maven}
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.
\newblock Maven: Multi-agent variational exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7611--7622, 2019.

\bibitem{mahmood2014weighted}
A~Rupam Mahmood, Hado~P van Hasselt, and Richard~S Sutton.
\newblock Weighted importance sampling for off-policy learning with linear
  function approximation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3014--3022, 2014.

\bibitem{policy-early-convergence-icml16}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{nachum2017bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2775--2785, 2017.

\bibitem{dec-pomdp}
Frans~A Oliehoek, Christopher Amato, et~al.
\newblock {\em A concise introduction to decentralized POMDPs}, volume~1.
\newblock Springer, 2016.

\bibitem{oliehoek2008optimal}
Frans~A Oliehoek, Matthijs~TJ Spaan, and Nikos Vlassis.
\newblock Optimal and approximate q-value functions for decentralized pomdps.
\newblock {\em Journal of Artificial Intelligence Research}, 32:289--353, 2008.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in neural information processing systems}, pages
  8026--8037, 2019.

\bibitem{diffreward-2012}
Scott Proper and Kagan Tumer.
\newblock Modeling difference rewards for multiagent learning.
\newblock In {\em AAMAS}, pages 1397--1398, 2012.

\bibitem{qmix}
Tabish Rashid, Mikayel Samvelyan, Christian~Schroeder De~Witt, Gregory
  Farquhar, Jakob Foerster, and Shimon Whiteson.
\newblock Qmix: monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock {\em arXiv preprint arXiv:1401.4082}, 2014.

\bibitem{smac}
Mikayel Samvelyan, Tabish Rashid, Christian~Schroeder de Witt, Gregory
  Farquhar, Nantas Nardelli, Tim G.~J. Rudner, Chia-Man Hung, Philiph H.~S.
  Torr, Jakob Foerster, and Shimon Whiteson.
\newblock {The} {StarCraft} {Multi}-{Agent} {Challenge}.
\newblock {\em CoRR}, abs/1902.04043, 2019.

\bibitem{schulman2017equivalence}
John Schulman, Xi Chen, and Pieter Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock {\em arXiv preprint arXiv:1704.06440}, 2017.

\bibitem{gae}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{dpg}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em International Conference on Machine Learning}, 2014.

\bibitem{singh1994learning}
Satinder~P Singh, Tommi Jaakkola, and Michael~I Jordan.
\newblock Learning without state-estimation in partially observable markovian
  decision processes.
\newblock In {\em Machine Learning Proceedings 1994}, pages 284--292. Elsevier,
  1994.

\bibitem{qtran}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock Qtran: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{vdn}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z Leibo, Karl
  Tuyls, et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In {\em Proceedings of the 17th international conference on
  autonomous agents and multiagent systems}, pages 2085--2087. International
  Foundation for Autonomous Agents and Multiagent Systems, 2018.

\bibitem{td-lambda}
Richard~S Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3(1):9--44, 1988.

\bibitem{diffreward-2007}
Kagan Tumer and Adrian Agogino.
\newblock Distributed agent-based air traffic flow management.
\newblock In {\em Proceedings of the 6th international joint conference on
  Autonomous agents and multiagent systems}, pages 1--8, 2007.

\bibitem{sqddpg}
Jianhong {Wang}, Yuan {Zhang}, Tae-Kyun {Kim}, and Yunjie {Gu}.
\newblock {Shapley Q-value: A Local Reward Approach to Solve Global Reward
  Games}.
\newblock In {\em Thirty-fourth AAAI conference on artificial intelligence},
  2020.

\bibitem{wang2016sample}
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
  Kavukcuoglu, and Nando de Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock {\em arXiv preprint arXiv:1611.01224}, 2016.

\bibitem{weber2019credit}
Th{\'e}ophane Weber, Nicolas Heess, Lars Buesing, and David Silver.
\newblock Credit assignment techniques in stochastic computation graphs.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2650--2660, 2019.

\bibitem{policy-gradient-critics}
Daan Wierstra and J{\"u}rgen Schmidhuber.
\newblock Policy gradient critics.
\newblock In {\em European Conference on Machine Learning}, pages 466--477.
  Springer, 2007.

\bibitem{original-entr-reg}
Ronald~J Williams and Jing Peng.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock {\em Connection Science}, 3(3):241--268, 1991.

\bibitem{mi-rnn}
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Russ~R Salakhutdinov.
\newblock On multiplicative integration with recurrent neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2856--2864, 2016.

\bibitem{routing-example}
Dayong Ye, Minjie Zhang, and Yun Yang.
\newblock A multi-agent framework for packet routing in wireless sensor
  networks.
\newblock {\em sensors}, 15(5):10026--10047, 2015.

\end{thebibliography}
