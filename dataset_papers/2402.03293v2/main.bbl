\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bingham \& Mannila(2001)Bingham and Mannila]{bingham2001random}
Bingham, E. and Mannila, H.
\newblock Random projection in dimensionality reduction: applications to image and text data.
\newblock In \emph{KDD}, pp.\  245--250, 2001.
\newblock URL \url{https://dl.acm.org/doi/10.1145/502512.502546}.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary, Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, pp.\  1877--1901, 2020.
\newblock URL \url{https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Cettolo et~al.(2017)Cettolo, Federico, Bentivogli, Niehues, St{\"u}ker, Sudoh, Yoshino, and Federmann]{cettolo2017overview}
Cettolo, M., Federico, M., Bentivogli, L., Niehues, J., St{\"u}ker, S., Sudoh, K., Yoshino, K., and Federmann, C.
\newblock Overview of the {IWSLT} 2017 evaluation campaign.
\newblock In \emph{IWSLT}, pp.\  2--14, 2017.
\newblock URL \url{https://aclanthology.org/2017.iwslt-1.1}.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.
\newblock URL \url{https://arxiv.org/abs/1604.06174}.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{NeurIPS}, 2019.
\newblock URL \url{https://papers.nips.cc/paper_files/paper/2019/hash/b8002139cdde66b87638f7f91d169d96-Abstract.html}.

\bibitem[Dasgupta(2000)]{dasgupta2000experiments}
Dasgupta, S.
\newblock Experiments with random projection.
\newblock In \emph{UAI}, pp.\  143--151, 2000.
\newblock URL \url{https://dl.acm.org/doi/10.5555/647234.719759}.

\bibitem[Dasgupta \& Gupta(2003)Dasgupta and Gupta]{dasgupta2003elementary}
Dasgupta, S. and Gupta, A.
\newblock An elementary proof of a theorem of {J}ohnson and {L}indenstrauss.
\newblock \emph{Random Structures \& Algorithms}, 22\penalty0 (1):\penalty0 60--65, 2003.
\newblock URL \url{https://doi.org/10.1002/rsa.10073}.

\bibitem[DeepMind et~al.(2020)DeepMind, Babuschkin, Baumli, Bell, Bhupatiraju, Bruce, Buchlovsky, Budden, Cai, Clark, Danihelka, Dedieu, Fantacci, Godwin, Jones, Hemsley, Hennigan, Hessel, Hou, Kapturowski, Keck, Kemaev, King, Kunesch, Martens, Merzic, Mikulik, Norman, Papamakarios, Quan, Ring, Ruiz, Sanchez, Sartran, Schneider, Sezener, Spencer, Srinivasan, Stanojevi\'{c}, Stokowiec, Wang, Zhou, and Viola]{deepmind2020jax}
DeepMind, Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A., Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S., Kapturowski, S., Keck, T., Kemaev, I., King, M., Kunesch, M., Martens, L., Merzic, H., Mikulik, V., Norman, T., Papamakarios, G., Quan, J., Ring, R., Ruiz, F., Sanchez, A., Sartran, L., Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., Stanojevi\'{c}, M., Stokowiec, W., Wang, L., Zhou, G., and Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/google-deepmind}.

\bibitem[Dettmers et~al.(2021)Dettmers, Lewis, Shleifer, and Zettlemoyer]{dettmers20218bit}
Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L.
\newblock 8-bit optimizers via block-wise quantization.
\newblock In \emph{ICLR}, 2021.
\newblock URL \url{https://openreview.net/forum?id=shpkpVXzo3h}.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock {QL}o{RA}: Efficient finetuning of quantized {LLM}s.
\newblock In \emph{NeurIPS}, 2023.
\newblock URL \url{https://openreview.net/forum?id=OUIFPHEgJU}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{ICML}, 2020.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{JMLR}, 12\penalty0 (7), 2011.
\newblock URL \url{https://jmlr.org/papers/v12/duchi11a.html}.

\bibitem[Feinberg et~al.(2023)Feinberg, Chen, Sun, Anil, and Hazan]{feinberg2023sketchy}
Feinberg, V., Chen, X., Sun, Y.~J., Anil, R., and Hazan, E.
\newblock Sketchy: Memory-efficient adaptive regularization with frequent directions.
\newblock In \emph{NeurIPS}, 2023.
\newblock URL \url{https://openreview.net/forum?id=DeZst6dKyi}.

\bibitem[Finesso \& Spreij(2006)Finesso and Spreij]{finesso2006nonnegative}
Finesso, L. and Spreij, P.
\newblock Nonnegative matrix factorization and {I}-divergence alternating minimization.
\newblock \emph{Linear Algebra and its Applications}, 416\penalty0 (2-3):\penalty0 270--287, 2006.
\newblock URL \url{https://doi.org/10.1016/j.laa.2005.11.012}.

\bibitem[Goh(2017)]{goh2017why}
Goh, G.
\newblock Why momentum really works.
\newblock \emph{Distill}, 2017.
\newblock URL \url{http://distill.pub/2017/momentum}.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Hinton, G., Srivastava, N., and Swersky, K.
\newblock Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.
\newblock \emph{Coursera}, 2012.
\newblock URL \url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, de~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameterefficient}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de~Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In \emph{ICML}, pp.\  2790--2799, 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/houlsby19a.html}.

\bibitem[Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Hu, E.~J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{ICLR}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Indyk \& Motwani(1998)Indyk and Motwani]{indyk1998approximate}
Indyk, P. and Motwani, R.
\newblock Approximate nearest neighbors: towards removing the curse of dimensionality.
\newblock In \emph{STOC}, pp.\  604--613, 1998.
\newblock URL \url{https://dl.acm.org/doi/10.1145/276698.276876}.

\bibitem[Jelassi \& Li(2022)Jelassi and Li]{jelassi2022towards}
Jelassi, S. and Li, Y.
\newblock Towards understanding how momentum improves generalization in deep learning.
\newblock In \emph{ICML}, pp.\  9965--10040, 2022.
\newblock URL \url{https://openreview.net/forum?id=lf0W6tcWmh-}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{ICLR}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report}, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Laurent \& Massart(2000)Laurent and Massart]{laurent2000adaptive}
Laurent, B. and Massart, P.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock \emph{Annals of statistics}, 28\penalty0 (5):\penalty0 1302--1338, 2000.
\newblock URL \url{https://www.jstor.org/stable/2674095}.

\bibitem[Li et~al.(2020)Li, Chaudhari, Yang, Lam, Ravichandran, Bhotika, and Soatto]{li2020rethinking}
Li, H., Chaudhari, P., Yang, H., Lam, M., Ravichandran, A., Bhotika, R., and Soatto, S.
\newblock Rethinking the hyperparameters for fine-tuning.
\newblock In \emph{ICLR}, 2020.
\newblock URL \url{https://openreview.net/forum?id=B1g8VkHFPH}.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Li, X.~L. and Liang, P.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{ACL-IJCNLP}, volume~1, pp.\  4582--4597, 2021.
\newblock URL \url{https://aclanthology.org/2021.acl-long.353}.

\bibitem[Lialin et~al.(2023)Lialin, Shivagunde, Muckatira, and Rumshisky]{lialin2023stack}
Lialin, V., Shivagunde, N., Muckatira, S., and Rumshisky, A.
\newblock Stack more layers differently: High-rank training through low-rank updates.
\newblock \emph{arXiv preprint arXiv: 2307.05695}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.05695}.

\bibitem[Liberty(2013)]{liberty2013simple}
Liberty, E.
\newblock Simple and deterministic matrix sketching.
\newblock In \emph{KDD}, pp.\  581--588, 2013.
\newblock URL \url{https://doi.org/10.1145/2487575.2487623}.

\bibitem[Lin(2004)]{lin2004rouge}
Lin, C.-Y.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pp.\  74--81, 2004.
\newblock URL \url{https://aclanthology.org/W04-1013}.

\bibitem[Lin et~al.(2020)Lin, Madotto, and Fung]{lin2020exploring}
Lin, Z., Madotto, A., and Fung, P.
\newblock Exploring versatile generative language model via parameter-efficient transfer learning.
\newblock In \emph{EMNLP Findings}, pp.\  441--459, 2020.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.41}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017sgdr}
Loshchilov, I. and Hutter, F.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Lv et~al.(2023)Lv, Yang, Liu, Gao, Guo, and Qiu]{lv2023full}
Lv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., and Qiu, X.
\newblock Full parameter fine-tuning for large language models with limited resources.
\newblock \emph{arXiv preprint arXiv: 2306.09782}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.09782}.

\bibitem[Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and Arora]{malladi2023finetuning}
Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.~D., Chen, D., and Arora, S.
\newblock Fine-tuning language models with just forward passes.
\newblock In \emph{NeurIPS}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Vota6rFhBQ}.

\bibitem[Masters \& Luschi(2018)Masters and Luschi]{masters2018revisiting}
Masters, D. and Luschi, C.
\newblock Revisiting small batch training for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.07612}, 2018.
\newblock URL \url{https://arxiv.org/abs/1804.07612}.

\bibitem[Matou{\v{s}}ek(2008)]{matouvsek2008variants}
Matou{\v{s}}ek, J.
\newblock On variants of the {J}ohnson--{L}indenstrauss lemma.
\newblock \emph{Random Structures \& Algorithms}, 33\penalty0 (2):\penalty0 142--156, 2008.
\newblock URL \url{https://doi.org/10.1002/rsa.20218}.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and Wu]{micikevicius2018mixed}
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., and Wu, H.
\newblock Mixed precision training.
\newblock In \emph{ICLR}, 2018.
\newblock URL \url{https://openreview.net/forum?id=r1gs9JgRZ}.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{narayan2018don}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Donâ€™t give me the details, just the summary! {T}opic-aware convolutional neural networks for extreme summarization.
\newblock In \emph{EMNLP}, pp.\  1797--1807, 2018.
\newblock URL \url{https://aclanthology.org/D18-1206}.

\bibitem[Nesterov(1998)]{nesterov1998introductory}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer, 1998.
\newblock URL \url{https://doi.org/10.1007/978-1-4419-8853-9}.

\bibitem[Oktay et~al.(2020)Oktay, McGreivy, Aduol, Beatson, and Adams]{oktay2020randomized}
Oktay, D., McGreivy, N., Aduol, J., Beatson, A., and Adams, R.~P.
\newblock Randomized automatic differentiation.
\newblock In \emph{ICLR}, 2020.
\newblock URL \url{https://openreview.net/forum?id=xpx9zj7CUlY}.

\bibitem[Post(2018)]{post2018call}
Post, M.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{WMT}, pp.\  186--191, 2018.
\newblock URL \url{https://aclanthology.org/W18-6319}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 2019.
\newblock URL \url{https://openai.com/research/better-language-models}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training {G}opher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{JMLR}, 21\penalty0 (1):\penalty0 5485--5551, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{CVPR}, pp.\  10684--10695, 2022.
\newblock URL \url{https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html}.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica, Braverman, Gonzalez, and Arora]{rothchild2020fetchsgd}
Rothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I., Braverman, V., Gonzalez, J., and Arora, R.
\newblock {F}etch{SGD}: Communication-efficient federated learning with sketching.
\newblock In \emph{ICML}, pp.\  8253--8265, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/rothchild20a.html}.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{shazeer2018adafactor}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{ICML}, pp.\  4596--4604, 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/shazeer18a.html}.

\bibitem[Shlens(2014)]{shlens2014tutorial}
Shlens, J.
\newblock A tutorial on principal component analysis.
\newblock \emph{arXiv preprint arXiv: 1404.1100}, 2014.
\newblock URL \url{https://arxiv.org/abs/1404.1100}.

\bibitem[Smith et~al.(2018)Smith, Kindermans, Ying, and Le]{smith2018don}
Smith, S.~L., Kindermans, P.-J., Ying, C., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{ICLR}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1Yy1BxCZ}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R., Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv: 2307.09288}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.09288}.

\bibitem[Wang et~al.(2013)Wang, Chen, Smola, and Xing]{wang2013variance}
Wang, C., Chen, X., Smola, A.~J., and Xing, E.~P.
\newblock Variance reduction for stochastic gradient optimization.
\newblock In \emph{NIPS}, 2013.
\newblock URL \url{https://papers.nips.cc/paper_files/paper/2013/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html}.

\bibitem[Wu et~al.(2021)Wu, Ouyang, Ziegler, Stiennon, Lowe, Leike, and Christiano]{wu2021recursively}
Wu, J., Ouyang, L., Ziegler, D.~M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P.
\newblock Recursively summarizing books with human feedback.
\newblock \emph{arXiv preprint arXiv:2109.10862}, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.10862}.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.
\newblock URL \url{https://arxiv.org/abs/1708.07747}.

\bibitem[Zaken et~al.(2022)Zaken, Goldberg, and Ravfogel]{zaken2022bitfit}
Zaken, E.~B., Goldberg, Y., and Ravfogel, S.
\newblock Bit{F}it: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock In \emph{ACL}, volume~2, pp.\  1--9, 2022.
\newblock URL \url{https://aclanthology.org/2022.acl-short.1}.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore}
Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y.
\newblock Ga{L}ore: Memory-efficient {LLM} training by gradient low-rank projection.
\newblock \emph{arXiv preprint arXiv: 2403.03507}, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.03507}.

\end{thebibliography}
