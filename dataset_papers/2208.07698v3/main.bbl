\begin{thebibliography}{}

\bibitem[Agakov and Barber, 2004]{Agakov2004}
Agakov, F.~V. and Barber, D. (2004).
\newblock An auxiliary variational method.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Arbel et~al., 2021]{Arbel2021}
Arbel, M., Matthews, A.~G., and Doucet, A. (2021).
\newblock Annealed flow transport {M}onte {C}arlo.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Ba et~al., 2016]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E. (2016).
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}.

\bibitem[Babuschkin et~al., 2020]{deepmind2020jax}
Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky,
  P., Budden, D., Cai, T., Clark, A., Danihelka, I., Fantacci, C., Godwin, J.,
  Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I.,
  King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G.,
  Ring, R., Ruiz, F., Sanchez, A., Schneider, R., Sezener, E., Spencer, S.,
  Srinivasan, S., Stokowiec, W., and Viola, F. (2020).
\newblock The {D}eep{M}ind {JAX} {E}cosystem.

\bibitem[Bernton et~al., 2019]{bernton2019SBsamplers}
Bernton, E., Heng, J., Doucet, A., and Jacob, P.~E. (2019).
\newblock Schr\"odinger bridge samplers.
\newblock {\em arXiv preprint arXiv:1912.13170}.

\bibitem[Bradbury et~al., 2018]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q. (2018).
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs.

\bibitem[Brekelmans et~al., 2022]{brekelmans2021improving}
Brekelmans, R., Huang, S., Ghassemi, M., Ver~Steeg, G., Grosse, R.~B., and
  Makhzani, A. (2022).
\newblock Improving mutual information estimation with annealed and
  energy-based bounds.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Burda et~al., 2016]{Burda2015}
Burda, Y., Grosse, R.~B., and Salakhutdinov, R. (2016).
\newblock Importance weighted autoencoders.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Crooks, 1998]{crooks1998nonequilibrium}
Crooks, G.~E. (1998).
\newblock Nonequilibrium measurements of free energy differences for
  microscopically reversible {M}arkovian systems.
\newblock {\em Journal of Statistical Physics}, 90(5-6):1481--1487.

\bibitem[Dai et~al., 2020]{dai2020invitation}
Dai, C., Heng, J., Jacob, P.~E., and Whiteley, N. (2020).
\newblock An invitation to sequential {M}onte {C}arlo samplers.
\newblock {\em arXiv preprint arXiv:2007.11936}.

\bibitem[De~Bortoli et~al., 2021a]{Heng2021diffusionbridges}
De~Bortoli, V., Doucet, A., Heng, J., and Thornton, J. (2021a).
\newblock Simulating diffusion bridges with score matching.
\newblock {\em arXiv preprint arXiv:2111.07243}.

\bibitem[De~Bortoli et~al., 2021b]{debortoli2021diffusion}
De~Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021b).
\newblock Diffusion {S}chr{\"o}dinger bridge with applications to score-based
  generative modeling.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Del~Moral et~al., 2006]{Del-Moral:2006}
Del~Moral, P., Doucet, A., and Jasra, A. (2006).
\newblock Sequential {M}onte {C}arlo samplers.
\newblock {\em Journal of the Royal Statistical Society: Series \textup{B}},
  68(3):411--436.

\bibitem[Dinh et~al., 2014]{dinh2014nice}
Dinh, L., Krueger, D., and Bengio, Y. (2014).
\newblock Nice: Non-linear independent components estimation.
\newblock {\em arXiv preprint arXiv:1410.8516}.

\bibitem[Dockhorn et~al., 2022]{Dockhorn2022}
Dockhorn, T., Vahdat, A., and Kreis, K. (2022).
\newblock Score-based generative modeling with critically-damped {L}angevin
  diffusion.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Doucet et~al., 2022]{doucet2022annealed}
Doucet, A., Grathwohl, W.~S., Matthews, A. G. D.~G., and Strathmann, H. (2022).
\newblock Annealed importance sampling meets score matching.
\newblock In {\em ICLR Workshop on Deep Generative Models for Highly Structured
  Data}.

\bibitem[Fournier and Tardif, 2021]{fournier2021simulated}
Fournier, N. and Tardif, C. (2021).
\newblock On the simulated annealing in $\mathbb{R}^d$.
\newblock {\em Journal of Functional Analysis}, 281(5):109086.

\bibitem[{Geffner} and {Domke}, 2021]{Geffner:2021}
{Geffner}, T. and {Domke}, J. (2021).
\newblock {MCMC} variational inference via uncorrected {H}amiltonian annealing.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Gelman and Meng, 1998]{gelman1998simulating}
Gelman, A. and Meng, X.-L. (1998).
\newblock Simulating normalizing constants: From importance sampling to bridge
  sampling to path sampling.
\newblock {\em Statistical Science}, 13(2):163--185.

\bibitem[Grillo, 1994]{grillo1994logarithmic}
Grillo, G. (1994).
\newblock Logarithmic {S}obolev inequalities and {L}angevin algorithms in
  $\mathbb{R}^n$.
\newblock {\em Stochastic Analysis and Applications}, 12(3):309--328.

\bibitem[Grosse et~al., 2013]{Grosse2013}
Grosse, R.~B., Maddison, C.~J., and Salakhutdinov, R.~R. (2013).
\newblock Annealing between distributions by averaging moments.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Haussmann and Pardoux, 1986]{haussmann1986time}
Haussmann, U.~G. and Pardoux, E. (1986).
\newblock Time reversal of diffusions.
\newblock {\em The Annals of Probability}, 14(3):1188--1205.

\bibitem[Heng et~al., 2020]{heng2017controlled}
Heng, J., Bishop, A.~N., Deligiannidis, G., and Doucet, A. (2020).
\newblock Controlled sequential {M}onte {C}arlo.
\newblock {\em The Annals of Statistics}, 48(5):2904--2929.

\bibitem[Ho et~al., 2020]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P. (2020).
\newblock Denoising diffusion probabilistic models.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Horowitz, 1991]{horowitz1991generalized}
Horowitz, A.~M. (1991).
\newblock A generalized guided {M}onte {C}arlo algorithm.
\newblock {\em Physics Letters B}, 268(2):247--252.

\bibitem[Huang et~al., 2018]{huang2018improving}
Huang, C.-W., Tan, S., Lacoste, A., and Courville, A.~C. (2018).
\newblock Improving explorability in variational inference with annealed
  variational objectives.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Hyv{\"a}rinen, 2005]{Hyvarinen:2005a}
Hyv{\"a}rinen, A. (2005).
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em The Journal of Machine Learning Research}, 6:695--709.

\bibitem[Jankowiak and Phan, 2022]{jankowiak2022surrogate}
Jankowiak, M. and Phan, D. (2022).
\newblock Surrogate likelihoods for variational annealed importance sampling.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Jarzynski, 1997]{jarzynski1997nonequilibrium}
Jarzynski, C. (1997).
\newblock Nonequilibrium equality for free energy differences.
\newblock {\em Physical Review Letters}, 78(14):2690--2963.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational {B}ayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[Klebaner, 2012]{klebaner2012introduction}
Klebaner, F.~C. (2012).
\newblock {\em Introduction to Stochastic Calculus with Applications}.
\newblock Imperial College Press.

\bibitem[Leimkuhler and Matthews, 2016]{leimkuhler2016molecular}
Leimkuhler, B. and Matthews, C. (2016).
\newblock {\em Molecular Dynamics with Deterministic and Stochastic Numerical
  Methods}.
\newblock Springer.

\bibitem[Masrani et~al., 2021]{masrani2021q}
Masrani, V., Brekelmans, R., Bui, T., Nielsen, F., Galstyan, A., Ver~Steeg, G.,
  and Wood, F. (2021).
\newblock q-paths: Generalizing the geometric annealing path using power means.
\newblock In {\em Uncertainty in Artificial Intelligence}.

\bibitem[Masrani et~al., 2019]{Masrani2019}
Masrani, V., Le, T.~A., and Wood, F. (2019).
\newblock The thermodynamic variational objective.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Matthews et~al., 2022]{Matthews2022}
Matthews, A. G. D.~G., Arbel, M., Rezende, D.~J., and Doucet, A. (2022).
\newblock Continual repeated annealed flow transport {M}onte {C}arlo.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Neal, 2001]{neal2001annealed}
Neal, R.~M. (2001).
\newblock Annealed importance sampling.
\newblock {\em Statistics and Computing}, 11(2):125--139.

\bibitem[Neal, 2011]{neal2011mcmc}
Neal, R.~M. (2011).
\newblock {MCMC} using {H}amiltonian dynamics.
\newblock {\em Handbook of Markov Chain Monte Carlo}, 2(11):2.

\bibitem[Opper, 2019]{Opper2019}
Opper, M. (2019).
\newblock Variational inference for stochastic differential equations.
\newblock {\em Annalen der Physik}, 531:1800233.

\bibitem[Ramachandran et~al., 2017]{ramachandran2017searching}
Ramachandran, P., Zoph, B., and Le, Q.~V. (2017).
\newblock Searching for activation functions.
\newblock {\em arXiv preprint arXiv:1710.05941}.

\bibitem[Rezende et~al., 2014]{rezende+al:2014:icml}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Roberts and Tweedie, 1996]{roberts1996exponential}
Roberts, G.~O. and Tweedie, R.~L. (1996).
\newblock Exponential convergence of {L}angevin distributions and their
  discrete approximations.
\newblock {\em Bernoulli}, pages 341--363.

\bibitem[Salakhutdinov and Murray, 2008]{salakhutdinov2008quantitative}
Salakhutdinov, R. and Murray, I. (2008).
\newblock On the quantitative analysis of deep belief networks.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Salimans et~al., 2015]{salimans2015markov}
Salimans, T., Kingma, D., and Welling, M. (2015).
\newblock Markov chain {M}onte {C}arlo and variational inference: Bridging the
  gap.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Schmiedl and Seifert, 2007]{schmiedl2007optimal}
Schmiedl, T. and Seifert, U. (2007).
\newblock Optimal finite-time processes in stochastic thermodynamics.
\newblock {\em Physical Review Letters}, 98(10):108301.

\bibitem[Sohl-Dickstein et~al., 2015]{sohl2015deep}
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015).
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Song et~al., 2021a]{song2021maximum}
Song, Y., Durkan, C., Murray, I., and Ermon, S. (2021a).
\newblock Maximum likelihood training of score-based diffusion models.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Song et~al., 2021b]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole,
  B. (2021b).
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Sottinen and S{\"a}rkk{\"a}, 2008]{sottinen2008application}
Sottinen, T. and S{\"a}rkk{\"a}, S. (2008).
\newblock Application of {G}irsanov theorem to particle filtering of discretely
  observed continuous-time non-linear systems.
\newblock {\em Bayesian Analysis}, 3(3):555--584.

\bibitem[Tang and Zhou, 2021]{tang2021simulated}
Tang, W. and Zhou, X.~Y. (2021).
\newblock Simulated annealing from continuum to discretization: a convergence
  analysis via the {E}yring--{K}ramers law.
\newblock {\em arXiv preprint arXiv:2102.02339}.

\bibitem[Thin et~al., 2021]{thin2021MCVAE}
Thin, A., Kotelevskii, N., Durmus, A., Panov, M., Moulines, E., and Doucet, A.
  (2021).
\newblock Monte {C}arlo variational auto-encoders.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Vincent, 2011]{vincent2011connection}
Vincent, P. (2011).
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural Computation}, 23(7):1661--1674.

\bibitem[Wu et~al., 2020]{wunoe2020stochastic}
Wu, H., K{\"o}hler, J., and No{\'e}, F. (2020).
\newblock Stochastic normalizing flows.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[{Zhang} et~al., 2021]{Zhang2021}
{Zhang}, G., {Hsu}, K., {Li}, J., {Finn}, C., and {Grosse}, R. (2021).
\newblock Differentiable annealed importance sampling and the perils of
  gradient noise.
\newblock In {\em Advances in Neural Information Processing Systems}.

\end{thebibliography}
