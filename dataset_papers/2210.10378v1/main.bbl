\begin{thebibliography}{10}

\bibitem{long2018conditional}
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael~I Jordan.
\newblock Conditional adversarial domain adaptation.
\newblock In {\em Advances in neural information processing systems}, pages
  1640--1650, 2018.

\bibitem{na2021fixbi}
Jaemin Na, Heechul Jung, Hyung~Jin Chang, and Wonjun Hwang.
\newblock Fixbi: Bridging domain spaces for unsupervised domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1094--1103, 2021.

\bibitem{li2020maximum}
Jingjing Li, Erpeng Chen, Zhengming Ding, Lei Zhu, Ke~Lu, and Heng~Tao Shen.
\newblock Maximum density divergence for domain adaptation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2020.

\bibitem{jing2021balanced}
Mengmeng Jing, Jingjing Li, Lei Zhu, Zhengming Ding, Ke~Lu, and Yang Yang.
\newblock Balanced open set domain adaptation via centroid alignment.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 8013--8020, 2021.

\bibitem{li2020model}
Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si~Wu.
\newblock Model adaptation: Unsupervised domain adaptation without source data.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9641--9650, 2020.

\bibitem{kundu2020universal}
Jogendra~Nath Kundu, Naveen Venkat, R~Venkatesh Babu, et~al.
\newblock Universal source-free domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4544--4553, 2020.

\bibitem{liang2020we}
Jian Liang, Dapeng Hu, and Jiashi Feng.
\newblock Do we really need to access the source data? source hypothesis
  transfer for unsupervised domain adaptation.
\newblock In {\em International Conference on Machine Learning}, pages
  6028--6039. PMLR, 2020.

\bibitem{yang2021exploiting}
Shiqi Yang, Yaxing Wang, Joost van~de Weijer, Luis Herranz, and Shangling Jui.
\newblock Exploiting the intrinsic neighborhood structure for source-free
  domain adaptation.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in neural information processing systems},
  volume~30, 2017.

\bibitem{xu2018reinforced}
Ju~Xu and Zhanxing Zhu.
\newblock Reinforced continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em International Conference on Machine Learning}, pages
  3987--3995. PMLR, 2017.

\bibitem{nguyen2018variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{yang2021generalized}
Shiqi Yang, Yaxing Wang, Joost van~de Weijer, Luis Herranz, and Shangling Jui.
\newblock Generalized source-free domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8978--8987, 2021.

\bibitem{kumar2021fine}
Ananya Kumar, Aditi Raghunathan, Robbie~Matthew Jones, Tengyu Ma, and Percy
  Liang.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2000probability}
T~Zhang and FJ~Oles.
\newblock A probability analysis on the value of unlabeled data for
  classification problems.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2000.

\bibitem{seeger2000input}
Matthias Seeger.
\newblock Input-dependent regularization of conditional density models.
\newblock Technical report, 2000.

\bibitem{wang2022continual}
Qin Wang, Olga Fink, Luc Van~Gool, and Dengxin Dai.
\newblock Continual test-time domain adaptation.
\newblock In {\em Proceedings of Conference on Computer Vision and Pattern
  Recognition}, 2022.

\bibitem{wang2020tent}
Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell.
\newblock Tent: Fully test-time adaptation by entropy minimization.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2017revisiting}
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou.
\newblock Revisiting batch normalization for practical domain adaptation.
\newblock In {\em International Conference on Learning Representations
  Workshop}, 2017.

\bibitem{nado2020evaluating}
Zachary Nado, Shreyas Padhy, D~Sculley, Alexander D'Amour, Balaji
  Lakshminarayanan, and Jasper Snoek.
\newblock Evaluating prediction-time batch normalization for robustness under
  covariate shift.
\newblock {\em arXiv preprint arXiv:2006.10963}, 2020.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International Conference on Machine Learning}, pages
  448--456. PMLR, 2015.

\bibitem{wilson2020bayesian}
Andrew~G Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 4697--4708, 2020.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International conference on machine learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{xiao2021bit}
Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao, and Cees G~M Snoek.
\newblock A bit more bayesian: Domain-invariant learning with uncertainty.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2021.

\bibitem{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~24, 2011.

\bibitem{mackay1992practical}
David~JC MacKay.
\newblock A practical bayesian framework for backpropagation networks.
\newblock {\em Neural computation}, 4(3):448--472, 1992.

\bibitem{fortuin2022bayesian}
Vincent Fortuin, Adri{\`a} Garriga-Alonso, Sebastian~W. Ober, Florian Wenzel,
  Gunnar Ratsch, Richard~E Turner, Mark van~der Wilk, and Laurence Aitchison.
\newblock Bayesian neural network priors revisited.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{silvestro2020prior}
Daniele Silvestro and Tobias Andermann.
\newblock Prior choice affects ability of bayesian neural networks to identify
  unknowns.
\newblock {\em arXiv preprint arXiv:2005.04987}, 2020.

\bibitem{pan2010survey}
Sinno~Jialin Pan and Qiang Yang.
\newblock A survey on transfer learning.
\newblock {\em IEEE Transactions on knowledge and data engineering},
  22(10):1345--1359, 2010.

\bibitem{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1-2):151--175, 2010.

\bibitem{jing2020adaptive}
Mengmeng Jing, Jidong Zhao, Jingjing Li, Lei Zhu, Yang Yang, and Heng~Tao Shen.
\newblock Adaptive component embedding for domain adaptation.
\newblock {\em IEEE transactions on cybernetics}, 2020.

\bibitem{li2019locality}
Jingjing Li, Mengmeng Jing, Ke~Lu, Lei Zhu, and Heng~Tao Shen.
\newblock Locality preserving joint transfer for domain adaptation.
\newblock {\em IEEE Transactions on Image Processing}, 28(12):6103--6115, 2019.

\bibitem{saito2018maximum}
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada.
\newblock Maximum classifier discrepancy for unsupervised domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3723--3732, 2018.

\bibitem{jing2022adversarial}
Mengmeng Jing, Lichao Meng, Jingjing Li, Lei Zhu, and Heng~Tao Shen.
\newblock Adversarial mixup ratio confusion for unsupervised domain adaptation.
\newblock {\em IEEE Transactions on Multimedia}, 2022.

\bibitem{long2013transfer}
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip~S Yu.
\newblock Transfer feature learning with joint distribution adaptation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 2200--2207, 2013.

\bibitem{li2018transfer}
Jingjing Li, Ke~Lu, Zi~Huang, Lei Zhu, and Heng~Tao Shen.
\newblock Transfer independently together: A generalized framework for domain
  adaptation.
\newblock {\em IEEE transactions on cybernetics}, 49(6):2144--2155, 2018.

\bibitem{long2018transferable}
Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael~I Jordan.
\newblock Transferable representation learning with deep adaptation networks.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  41(12):3071--3085, 2018.

\bibitem{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 1406--1415, 2019.

\bibitem{huang2021model}
Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.
\newblock Model adaptation: Historical contrastive learning for unsupervised
  domain adaptation without source data.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem{ishii2021source}
Masato Ishii and Masashi Sugiyama.
\newblock Source-free domain adaptation via distributional alignment by
  matching batch normalization statistics.
\newblock {\em arXiv preprint arXiv:2101.10842}, 2021.

\bibitem{sun2020test}
Yu~Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt.
\newblock Test-time training with self-supervision for generalization under
  distribution shifts.
\newblock In {\em International Conference on Machine Learning}, pages
  9229--9248. PMLR, 2020.

\bibitem{zhou2021bayesian}
Aurick Zhou and Sergey Levine.
\newblock Bayesian adaptation for covariate shift.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{hinton1993keeping}
Geoffrey~E Hinton and Drew Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In {\em Proceedings of the sixth annual conference on Computational
  learning theory}, pages 5--13, 1993.

\bibitem{hernandez2015probabilistic}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato and Ryan Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In {\em International conference on machine learning}, pages
  1861--1869. PMLR, 2015.

\bibitem{neal2012bayesian}
Radford~M Neal.
\newblock {\em Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem{saenko2010adapting}
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.
\newblock Adapting visual category models to new domains.
\newblock In {\em ECCV}, pages 213--226. Springer, 2010.

\bibitem{venkateswara2017deep}
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman
  Panchanathan.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5018--5027, 2017.

\bibitem{hendrycks2018benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{BMVC2016_87}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In Edwin R.~Hancock Richard C.~Wilson and William A.~P. Smith,
  editors, {\em Proceedings of the British Machine Vision Conference (BMVC)},
  pages 87.1--87.12. BMVA Press, September 2016.

\bibitem{croce2021robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{adam2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{hendrycks2020augmix}
Dan Hendrycks*, Norman Mu*, Ekin~Dogus Cubuk, Barret Zoph, Justin Gilmer, and
  Balaji Lakshminarayanan.
\newblock Augmix: A simple method to improve robustness and uncertainty under
  data shift.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{kingma2015variational}
Durk~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{yosinski2014transferable}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock How transferable are features in deep neural networks?
\newblock In {\em Advances in neural information processing systems}, pages
  3320--3328, 2014.

\bibitem{ganin2015unsupervised}
Yaroslav Ganin and Victor Lempitsky.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In {\em International conference on machine learning}, pages
  1180--1189. PMLR, 2015.

\bibitem{cui2020towards}
Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi~Tian.
\newblock Towards discriminability and diversity: Batch nuclear-norm
  maximization under label insufficient situations.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3941--3950, 2020.

\bibitem{xu2019larger}
Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin.
\newblock Larger norm more transferable: An adaptive feature norm approach for
  unsupervised domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1426--1435, 2019.

\bibitem{zhang2019bridging}
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock In {\em International Conference on Machine Learning}, pages
  7404--7413. PMLR, 2019.

\bibitem{yang2020bi}
Guanglei Yang, Haifeng Xia, Mingli Ding, and Zhengming Ding.
\newblock Bi-directional generation for unsupervised domain adaptation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 6615--6622, 2020.

\end{thebibliography}
