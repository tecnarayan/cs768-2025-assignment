 @article{Bondeson_Ward_1994, title={Stabilization of External Modes in Tokamaks by Resistive Walls and Plasma Rotation}, volume={72}, number={17}, journal={Physical Review Letters}, author={Bondeson, A. and Ward, D.J.}, year={1994}, pages={2709–2712} }
@article{tschantz2020reinforcement,
  title={Reinforcement learning through active inference},
  author={Tschantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},
  journal={arXiv preprint arXiv:2002.12636},
  year={2020}
}
@article{fenstermacher2022diii,
  title={DIII-D research advancing the physics basis for optimizing the tokamak approach to fusion energy},
  author={Fenstermacher, Max E and Abbate, J and Abe, S and Abrams, T and Adams, M and Adamson, B and Aiba, N and Akiyama, T and Aleynikov, P and Allen, E and others},
  journal={Nuclear Fusion},
  volume={62},
  number={4},
  pages={042024},
  year={2022},
  publisher={IOP Publishing}
}
@inproceedings{schultheis2020receding,
  title={Receding horizon curiosity},
  author={Schultheis, Matthias and Belousov, Boris and Abdulsamad, Hany and Peters, Jan},
  booktitle={Conference on robot learning},
  pages={1278--1288},
  year={2020},
  organization={PMLR}
}
 @article{Groebner_Burrell_Seraydarian_1990, title={Role of edge electric field and poloidal rotation in the L-H transition}, volume={64}, ISSN={00319007}, DOI={10.1103/PhysRevLett.64.3015}, abstractNote={Marked changes in the edge radial electric field Er and in the edge poloidal rotation velocity v are important signatures of the L-H transition in the DIII-D tokamak. Shear exists in Er and v and increases from the L mode to the H mode. A comparison of experiment with theory shows that shear in Er and v is sufficient to suppress edge fluctuations, that ion-orbit loss is large enough to be the source of v, and that Er and v may play a causal role in the L-H transition. © 1990 The American Physical Society.}, number={25}, journal={Physical Review Letters}, author={Groebner, R. J. and Burrell, K. H. and Seraydarian, R. P.}, year={1990}, pages={3015–3018} }


@article{stillerman1997mdsplus,
  title={MDSplus data acquisition system},
  author={Stillerman, JA and Fredian, TW and Klare, KA and Manduchi, G},
  journal={Review of Scientific Instruments},
  volume={68},
  number={1},
  pages={939--942},
  year={1997},
  publisher={American Institute of Physics}
}

@article{abbate2021data,
  title={Data-driven profile prediction for DIII-D},
  author={Abbate, Joseph and Conlin, R and Kolemen, E},
  journal={Nuclear Fusion},
  volume={61},
  number={4},
  pages={046027},
  year={2021},
  publisher={IOP Publishing}
}

@article{williams1996gaussian,
  title={Gaussian processes for regression},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  year={1996},
  publisher={MIT}
}

@inproceedings{rasmussen2003gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Summer school on machine learning},
  pages={63--71},
  year={2003},
  organization={Springer}
}

@article{osband_deep_exploration,
  author  = {Ian Osband and Benjamin Van Roy and Daniel J. Russo and Zheng Wen},
  title   = {Deep Exploration via Randomized Value Functions},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {124},
  pages   = {1-62},
  url     = {http://jmlr.org/papers/v20/18-339.html}
}

@article{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{humphreys2015novel,
  title={Novel aspects of plasma control in ITER},
  author={Humphreys, David and Ambrosino, G and de Vries, Peter and Felici, Federico and Kim, Sun H and Jackson, Gary and Kallenbach, A and Kolemen, Egemen and Lister, J and Moreau, D and others},
  journal={Physics of Plasmas},
  volume={22},
  number={2},
  pages={021806},
  year={2015},
  publisher={AIP Publishing LLC}
}

@article{de2005tutorial,
  title={A tutorial on the cross-entropy method},
  author={De Boer, Pieter-Tjerk and Kroese, Dirk P and Mannor, Shie and Rubinstein, Reuven Y},
  journal={Annals of operations research},
  volume={134},
  number={1},
  pages={19--67},
  year={2005},
  publisher={Springer}
}

@inproceedings{curi_hucrl,
  author={Sebastian Curi and Felix Berkenkamp and Andreas Krause},
  title={Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning},
  year={2020},
  cdate={1577836800000},
  url={https://proceedings.neurips.cc/paper/2020/hash/a36b598abb934e4528412e5a2127b931-Abstract.html},
  booktitle={NeurIPS},
}

@InProceedings{pmlr-v28-tesch13,
  title = 	 {Expensive Function Optimization with Stochastic Binary Outcomes},
  author = 	 {Tesch, Matthew and Schneider, Jeff and Choset, Howie},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1283--1291},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/tesch13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/tesch13.html},
  abstract = 	 {Real world systems often have parameterized controllers which can be tuned to improve performance. Bayesian optimization methods provide for efficient optimization of these controllers, so as to reduce the number of required experiments on the expensive physical system. In this paper we address Bayesian optimization in the setting where performance is only observed through a stochastic binary outcome – success or failure of the experiment. Unlike bandit problems, the goal is to maximize the system performance after this offline training phase rather than minimize regret during training.  In this work we define the stochastic binary optimization problem and propose an approach using an adaptation of Gaussian Processes for classification that presents a Bayesian optimization framework for this problem.  We propose an experiment selection metric for this setting based on expected improvement.  We demonstrate the algorithm’s performance on synthetic problems and on a real snake robot learning to move over an obstacle.}
}


@article{hernandez2014predictive,
  title={Predictive entropy search for efficient global optimization of black-box functions},
  author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Hoffman, Matthew W and Ghahramani, Zoubin},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{strens2000bayesian,
  title={A Bayesian framework for reinforcement learning},
  author={Strens, Malcolm},
  booktitle={ICML},
  volume={2000},
  pages={943--950},
  year={2000}
}

@article{russo2018tutorial,
  title={A tutorial on thompson sampling},
  author={Russo, Daniel J and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={1},
  pages={1--96},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@inproceedings{mehta2022barl,
  title={An Experimental Design Perspective on Model-Based Reinforcement Learning},
  author={Mehta, Viraj and Paria, Biswajit and Schneider, Jeff and Ermon, Stefano and Neiswanger, Willie},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@InProceedings{pmlr-v120-buisson-fenet20a,
  title = 	 {Actively Learning Gaussian Process Dynamics},
  author =       {Buisson-Fenet, Mona and Solowjow, Friedrich and Trimpe, Sebastian},
  booktitle = 	 {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
  pages = 	 {5--15},
  year = 	 {2020},
  editor = 	 {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
  volume = 	 {120},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v120/buisson-fenet20a/buisson-fenet20a.pdf},
  url = 	 {https://proceedings.mlr.press/v120/buisson-fenet20a.html},
  abstract = 	 {Despite the availability of ever more data enabled through modern sensor and computer technology, it still remains an open problem to learn dynamical systems in a sample-efficient way.We propose active learning strategies that leverage information-theoretical properties arising naturally during Gaussian process regression, while respecting constraints on the sampling process imposed by the system dynamics. Sample points are selected in regions with high uncertainty, leading to exploratory behavior and data-efficient training of the model.All results are verified in an extensive numerical benchmark.}
}

@InProceedings{context-achterhold21a,
  title = 	 { Explore the Context: Optimal Data Collection for Context-Conditional Dynamics Models },
  author =       {Achterhold, Jan and Stueckler, Joerg},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3529--3537},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/achterhold21a/achterhold21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/achterhold21a.html},
  abstract = 	 { In this paper, we learn dynamics models for parametrized families of dynamical systems with varying properties. The dynamics models are formulated as stochastic processes conditioned on a latent context variable which is inferred from observed transitions of the respective system. The probabilistic formulation allows us to compute an action sequence which, for a limited number of environment interactions, optimally explores the given system within the parametrized family. This is achieved by steering the system through transitions being most informative for the context variable. We demonstrate the effectiveness of our method for exploration on a non-linear toy-problem and two well-known reinforcement learning environments. }
}


@phdthesis{neal1995bayesian,
  title={Bayesian Learning for Neural Networks},
  author={Neal, Radford M},
  year={1995},
  school={University of Toronto}
}

@article{mackay1994bayesian,
  title={Bayesian nonlinear modeling for the prediction competition},
  author={MacKay, David JC and others},
  journal={ASHRAE transactions},
  volume={100},
  number={2},
  pages={1053--1062},
  year={1994},
  publisher={Citeseer}
}

@inproceedings{rahimi2007random,
  title={Random Features for Large-Scale Kernel Machines.},
  author={Rahimi, Ali and Recht, Benjamin and others},
  booktitle={NIPS},
  volume={3},
  number={4},
  pages={5},
  year={2007},
  organization={Citeseer}
}

@article{weng2020exploration,
  title   = "Exploration Strategies in Deep Reinforcement Learning",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2020",
  url     = "https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html"
}

@inproceedings{neiswanger2021bayesian,
  title         = {Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information},
  author        = {Neiswanger, Willie and Wang, Ke Alexander and Ermon, Stefano},
  booktitle     = {International Conference on Machine Learning},
  year          = {2021},
  organization  = {PMLR}
}

@inproceedings{wilson2020efficiently,
  title={Efficiently sampling functions from Gaussian process posteriors},
  author={Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  booktitle={International Conference on Machine Learning},
  pages={10292--10302},
  year={2020},
  organization={PMLR}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{ChenUCB,
  author    = {Richard Y. Chen and
               Szymon Sidor and
               Pieter Abbeel and
               John Schulman},
  title     = {{UCB} and InfoGain Exploration via $Q$-Ensembles},
  journal   = {CoRR},
  volume    = {abs/1706.01502},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.01502},
  eprinttype = {arXiv},
  eprint    = {1706.01502},
  timestamp = {Mon, 13 Aug 2018 16:48:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChenSAS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{OsbandBootstrapped,
 author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Exploration via Bootstrapped DQN},
 url = {https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf},
 volume = {29},
 year = {2016}
}

@INPROCEEDINGS{heiden2021disect, 
    AUTHOR    = {Eric Heiden AND Miles Macklin AND Yashraj S Narang AND Dieter Fox AND Animesh Garg AND Fabio Ramos}, 
    TITLE     = {{DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2021}, 
    ADDRESS   = {Virtual}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2021.XVII.067} 
}

@INPROCEEDINGS{mujoco,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}}

@misc{jameson2006using,
  title={Using computational fluid dynamics for aerodynamics},
  author={Jameson, Antony and Fatica, Massimiliano},
  year={2006},
  publisher={Citeseer}
}
@inproceedings{sekar2020planning,
  title={Planning to explore via self-supervised world models},
  author={Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  booktitle={International Conference on Machine Learning},
  pages={8583--8592},
  year={2020},
  organization={PMLR}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{toromanoff2020end,
  title={End-to-end model-free reinforcement learning for urban driving using implicit affordances},
  author={Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7153--7162},
  year={2020}
}
@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{deepmind_plasma,
  title={Magnetic control of tokamak plasmas through deep reinforcement learning},
  author={Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and de Las Casas, Diego and others},
  journal={Nature},
  volume={602},
  number={7897},
  pages={414--419},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}

@inproceedings{
ash2022anticoncentrated,
title={Anti-Concentrated Confidence Bonuses For Scalable Exploration},
author={Jordan T. Ash and Cyril Zhang and Surbhi Goel and Akshay Krishnamurthy and Sham M. Kakade},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RXQ-FPbQYVn}
}

@article{zhou2019optimization,
  title={Optimization of molecules via deep reinforcement learning},
  author={Zhou, Zhenpeng and Kearnes, Steven and Li, Li and Zare, Richard N and Riley, Patrick},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={1--10},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{pinneri2020iCEM,
  title={Sample-efficient Cross-Entropy Method for Real-time Planning},
  author={Pinneri, Cristina and Sawant, Shambhuraj and Blaes, Sebastian and Achterhold, Jan and Stueckler, Joerg and Rolinek, Michal and Martius, Georg},
  journal={arXiv preprint arXiv:2008.06389},
  year={2020}
}


@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}


@InProceedings{pmlr-v97-pathak19a,
  title = 	 {Self-Supervised Exploration via Disagreement},
  author =       {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5062--5071},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/pathak19a/pathak19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/pathak19a.html},
  abstract = 	 {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent’s policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/}
}

@article{BellemareCounts,
  author    = {Marc G. Bellemare and
               Sriram Srinivasan and
               Georg Ostrovski and
               Tom Schaul and
               David Saxton and
               R{\'{e}}mi Munos},
  title     = {Unifying Count-Based Exploration and Intrinsic Motivation},
  journal   = {CoRR},
  volume    = {abs/1606.01868},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01868},
  eprinttype = {arXiv},
  eprint    = {1606.01868},
  timestamp = {Fri, 04 Sep 2020 14:32:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BellemareSOSSM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TangNipsHashCount,
  author    = {Haoran Tang and
               Rein Houthooft and
               Davis Foote and
               Adam Stooke and
               Xi Chen and
               Yan Duan and
               John Schulman and
               Filip De Turck and
               Pieter Abbeel},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {{\#}Exploration: {A} Study of Count-Based Exploration for Deep Reinforcement
               Learning},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {2753--2762},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/TangHFSCDSTA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{VIME_NIPS2016,
 author = {Houthooft, Rein and Chen, Xi and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {VIME: Variational Information Maximizing Exploration},
 url = {https://proceedings.neurips.cc/paper/2016/file/abd815286ba1007abfbb8415b83ae2cf-Paper.pdf},
 volume = {29},
 year = {2016}
}


@InProceedings{pmlr-v125-agarwal20b,
  title = 	 {Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal},
  author =       {Agarwal, Alekh and Kakade, Sham and Yang, Lin F.},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {67--83},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/agarwal20b/agarwal20b.pdf},
  url = 	 {https://proceedings.mlr.press/v125/agarwal20b.html},
  abstract = 	 { This work considers the sample and computational complexity of obtaining an $\epsilon$-optimal policy in a discounted Markov Decision Process (MDP), given only access to a generative model. In this model, the learner accesses the underlying transition model via a sampling oracle that provides a sample of the next state, when given any state-action pair as input. We are interested in a basic and unresolved question in model based planning: is this naïve “plug-in” approach — where we build the maximum likelihood estimate of the transition model in the MDP from observations and then find an optimal policy in this empirical MDP — non-asymptotically, minimax optimal? Our main result answers this question positively. With regards to computation, our result provides a simpler approach towards minimax optimal planning: in comparison to prior model-free results,  we show that using \emph{any} high accuracy, black-box planning oracle in the empirical model suffices to obtain the minimax error rate. The key proof technique uses a leave-one-out analysis, in a novel “absorbing MDP” construction, to decouple the statistical dependency issues that arise in the analysis of model-based planning; this construction may be helpful more generally.}
}

@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@inproceedings{LiSampleSizeGenerativeRL,
 author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12861--12872},
 publisher = {Curran Associates, Inc.},
 title = {Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model},
 url = {https://proceedings.neurips.cc/paper/2020/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{chua_pets,
 author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
 url = {https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{ross2007bayes,
  title={Bayes-Adaptive POMDPs.},
  author={Ross, Stephane and Chaib-draa, Brahim and Pineau, Joelle},
  booktitle={NIPS},
  pages={1225--1232},
  year={2007}
}

@InProceedings{TD3Fujimoto,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author =       {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@inproceedings{
Wang2020POPLIN,
title={Exploring Model-based Planning with Policy Networks},
author={Tingwu Wang and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1exf64KwH}
}

@article{kearns2002sparse,
  title={A sparse sampling algorithm for near-optimal planning in large Markov decision processes},
  author={Kearns, Michael and Mansour, Yishay and Ng, Andrew Y},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={193--208},
  year={2002},
  publisher={Springer}
}

@book{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  publisher={University of London, University College London (United Kingdom)}
}

@misc{transp,
title = {TRANSP},
author = {Breslau, Joshua and Gorelenkova, Marina and Poli, Francesca and Sachdev, Jai and Pankin, Alexei and Perumpilly, Gopan and USDOE Office of Science},
abstractNote = {TRANSP is a 1.5D equilibrium and transport solver for interpretation and prediction of tokamak discharges.},
doi = {10.11578/dc.20180627.4},
url = {https://www.osti.gov/biblio/1489900}, year = {2018},
month = {6}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@article{shahriari2015taking,
  title={Taking the human out of the loop: A review of Bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2015},
  publisher={IEEE}
}

@article{chaloner1995bayesian,
  title={Bayesian experimental design: A review},
  author={Chaloner, Kathryn and Verdinelli, Isabella},
  journal={Statistical Science},
  pages={273--304},
  year={1995},
  publisher={JSTOR}
}

@Book{Sutton+Barto:1998,
  author =       "Sutton, Richard S. and Barto, Andrew G.",
  title =        "Reinforcement Learning: An Introduction",
  publisher =    "MIT Press",
  year =         "1998",
  ISBN =         "0-262-19398-1",
  address =   "Cambridge, MA, USA",
  url = "http://www.cs.ualberta.ca/%7Esutton/book/ebook/the-book.html",
  bib2html_rescat = "Function Approximation, Partial Observability, Learning Methods, General RL, Applications",
}

@InProceedings{pmlr-v84-kamthe18a,
  title = 	 {Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control},
  author = 	 {Kamthe, Sanket and Deisenroth, Marc},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1701--1710},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/kamthe18a/kamthe18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/kamthe18a.html},
  abstract = 	 { Trial-and-error based reinforcement learning (RL) has seen rapid advancements in recent times, especially with the advent of deep neural networks. However, the majority of autonomous RL algorithms require a large number of interactions with the environment. A large number of interactions may be impractical in many real-world applications, such as robotics, and many practical systems have to obey limitations in the form of state space or control constraints. To reduce the number of system interactions while simultaneously handling constraints, we propose a model-based RL framework based on probabilistic Model Predictive Control (MPC). In particular, we propose to learn a probabilistic transition model using Gaussian Processes (GPs) to incorporate model uncertainty into long-term predictions, thereby, reducing the impact of model errors. We then use MPC to find a control sequence that minimises the expected long-term cost. We provide theoretical guarantees for first-order optimality in the GP-based transition models with deterministic approximate inference for long-term planning. We demonstrate that our approach does not only achieve state-of-the-art data efficiency, but also is a principled way for RL in constrained environments.}
}

@inproceedings{pilco,
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy
search method. PILCO reduces model bias, one of the key problems of model-based reinforcement
learning, in a principled way. By learning a probabilistic dynamics model and explicitly
incorporating model uncertainty into long-term planning, PILCO can cope with very
little data and facilitates learning from scratch in only a few trials. Policy evaluation
is performed in closed form using state-of-the-art approximate inference. Furthermore,
policy gradients are computed analytically for policy improvement. We report unprecedented
learning efficiency on challenging and high-dimensional control tasks.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {465–472},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{mbpo,
 author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {When to Trust Your Model: Model-Based Policy Optimization},
 url = {https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@article{ryzhov2019bayesian,
  title={Bayesian exploration for approximate dynamic programming},
  author={Ryzhov, Ilya O and Mes, Martijn RK and Powell, Warren B and van den Berg, Gerald},
  journal={Operations research},
  volume={67},
  number={1},
  pages={198--214},
  year={2019},
  publisher={INFORMS}
}
@article{ryzhov2011information,
  title={Information collection on a graph},
  author={Ryzhov, Ilya O and Powell, Warren B},
  journal={Operations Research},
  volume={59},
  number={1},
  pages={188--201},
  year={2011},
  publisher={INFORMS}
}

@article{joe_rory_profiles,
	doi = {10.1088/1741-4326/abe08d},
	url = {https://doi.org/10.1088/1741-4326/abe08d},
	year = 2021,
	month = {mar},
	publisher = {{IOP} Publishing},
	volume = {61},
	number = {4},
	pages = {046027},
	author = {J. Abbate and R. Conlin and E. Kolemen},
	title = {Data-driven profile prediction for {DIII}-D},
	journal = {Nuclear Fusion},
	abstract = {A new, fully data-driven algorithm has been developed that uses a neural network to predict plasma profiles on a scale of τ E into the future given an actuator trajectory and the plasma state history. The model was trained and tested on DIII-D data from the 2013–2018 experimental campaigns. The model runs in tens of milliseconds and is very simple to use. This makes it a potentially useful tool for operators and physicists when planning plasma scenarios. It is also fast enough to be used for real-time model-predictive control.}
}


@InProceedings{shyam_max,
  title = 	 {Model-Based Active Exploration},
  author =       {Shyam, Pranav and Ja{\'{s}}kowski, Wojciech and Gomez, Faustino},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5779--5788},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/shyam19a/shyam19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/shyam19a.html},
  abstract = 	 {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient <em>active</em> exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.}
}

@Article{Pineda2021MBRL,
  author  = {Luis Pineda and Brandon Amos and Amy Zhang and Nathan O. Lambert and Roberto Calandra},
  journal = {Arxiv},
  title   = {MBRL-Lib: A Modular Library for Model-based Reinforcement Learning},
  year    = {2021},
  url     = {https://arxiv.org/abs/2104.10159},
}

@inproceedings{dearden1998bayesian,
  title={Bayesian Q-learning},
  author={Dearden, Richard and Friedman, Nir and Russell, Stuart},
  booktitle={Aaai/iaai},
  pages={761--768},
  year={1998}
}

@article{DeardenMBBE,
  author    = {Richard Dearden and
               Nir Friedman and
               David Andre},
  title     = {Model-Based Bayesian Exploration},
  journal   = {CoRR},
  volume    = {abs/1301.6690},
  year      = {1999},
  url       = {http://arxiv.org/abs/1301.6690},
  eprinttype = {arXiv},
  eprint    = {1301.6690},
  timestamp = {Mon, 13 Aug 2018 16:47:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-6690.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{SledgeGPEVOI,
  author={Sledge, Isaac J. and Emigh, Matthew S. and Príncipe, José C.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Guided Policy Exploration for Markov Decision Processes Using an Uncertainty-Based Value-of-Information Criterion}, 
  year={2018},
  volume={29},
  number={6},
  pages={2080-2098},
  doi={10.1109/TNNLS.2018.2812709}
}

@inproceedings{kolter2009nearbayesian,
  title={Near-Bayesian exploration in polynomial time},
  author={Kolter, J Zico and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={513--520},
  year={2009}
}

@article{GhavamzadehBayesRL,
  author    = {Mohammad Ghavamzadeh and
               Shie Mannor and
               Joelle Pineau and
               Aviv Tamar},
  title     = {Bayesian Reinforcement Learning: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/1609.04436},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04436},
  eprinttype = {arXiv},
  eprint    = {1609.04436},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GhavamzadehMPT16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wilson2014using,
  title={Using trajectory data to improve bayesian optimization for reinforcement learning},
  author={Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={253--282},
  year={2014},
  publisher={JMLR. org}
}

@book{smith2007probabilistic,
  title={Probabilistic planning for robotic exploration},
  author={Smith, Trey},
  year={2007},
  publisher={Carnegie Mellon University}
}

@inproceedings{lopes_exploration,
 author = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-yves},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress},
 url = {https://proceedings.neurips.cc/paper/2012/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf},
 volume = {25},
 year = {2012}
}

@inproceedings{jin_sample-efficient_2020,
	title = {Sample-{Efficient} {Reinforcement} {Learning} of {Undercomplete} {POMDPs}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/d783823cc6284b929c2cd8df2167d212-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jin, Chi and Kakade, Sham and Krishnamurthy, Akshay and Liu, Qinghua},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {18530--18539},
}

@article{ross2008online,
  title={Online planning algorithms for POMDPs},
  author={Ross, St{\'e}phane and Pineau, Joelle and Paquet, S{\'e}bastien and Chaib-Draa, Brahim},
  journal={Journal of Artificial Intelligence Research},
  volume={32},
  pages={663--704},
  year={2008}
}
@inproceedings{guez_bayes_mdp,
author = {Guez, Arthur and Silver, David and Dayan, Peter},
title = {Efficient Bayes-Adaptive Reinforcement Learning Using Sample-Based Search},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian model-based reinforcement learning is a formally elegant approach to learning
optimal behaviour under model uncertainty, trading off exploration and exploitation
in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously
taxing, since the search space becomes enormous. In this paper we introduce a tractable,
sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo
tree search. Our approach outperformed prior Bayesian model-based RL algorithms by
a significant margin on several well-known benchmark problems - because it avoids
expensive applications of Bayes rule within the search tree by lazily sampling models
from the current beliefs. We illustrate the advantages of our approach by showing
it working in an infinite state space domain which is qualitatively out of reach of
almost all previous work in Bayesian exploration.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1025–1033},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{RussoIDS,
 author = {Russo, Daniel and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Optimize via Information-Directed Sampling},
 url = {https://proceedings.neurips.cc/paper/2014/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf},
 volume = {27},
 year = {2014}
}

@InProceedings{ball_rp1,
  title = 	 {Ready Policy One: World Building Through Active Learning},
  author =       {Ball, Philip and Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {591--601},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/ball20a/ball20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/ball20a.html},
  abstract = 	 {Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.}
}

@inproceedings{
nikolov2018informationdirected,
title={Information-Directed Exploration for Deep Reinforcement Learning},
author={Nikolay Nikolov and Johannes Kirschner and Felix Berkenkamp and Andreas Krause},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Byx83s09Km},
}

@misc{arumugam2021value,
      title={The Value of Information When Deciding What to Learn}, 
      author={Arumugam, Dilip and Van Roy, Benjamin},
      booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
      year={2021},
      eprint={2110.13973},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{idrl,
  author    = {David Lindner and
               Matteo Turchetta and
               Sebastian Tschiatschek and
               Kamil Ciosek and
               Andreas Krause},
  title     = {Information Directed Reward Learning for Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2102.12466},
  timestamp = {Tue, 02 Mar 2021 12:11:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-12466.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
burda2018exploration,
title={Exploration by random network distillation},
author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1lJJnR5Ym},
}

@inproceedings{schmidhuber1991possibility,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
  pages={222--227},
  year={1991}
}

@article{char2019offline,
  title={Offline contextual bayesian optimization},
  author={Char, Ian and Chung, Youngseog and Neiswanger, Willie and Kandasamy, Kirthevasan and Nelson, Andrew O and Boyer, Mark and Kolemen, Egemen and Schneider, Jeff},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={4627--4638},
  year={2019}
}

@inproceedings{mehta2021neural,
  title={Neural dynamical systems: Balancing structure and flexibility in physical prediction},
  author={Mehta, Viraj and Char, Ian and Neiswanger, Willie and Chung, Youngseog and Nelson, Andrew and Boyer, Mark and Kolemen, Egemen and Schneider, Jeff},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
  pages={3735--3742},
  year={2021},
  organization={IEEE}
}

@inproceedings{lee2021sunrise,
  title={Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning},
  author={Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={6131--6141},
  year={2021},
  organization={PMLR}
}
@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{hennig2012entropy,
  title={Entropy Search for Information-Efficient Global Optimization.},
  author={Hennig, Philipp and Schuler, Christian J},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={6},
  year={2012}
}