\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019.

\bibitem[Arora et~al.(2018{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{arora2018convergence}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Arora et~al.(2018{\natexlab{b}})Arora, Li, and
  Lyu]{arora2018theoretical}
Arora, S., Li, Z., and Lyu, K.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock \emph{arXiv preprint arXiv:1812.03981}, 2018{\natexlab{b}}.

\bibitem[Atwood \& Towsley(2016)Atwood and Towsley]{atwood2016diffusion}
Atwood, J. and Towsley, D.
\newblock Diffusion-convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1993--2001, 2016.

\bibitem[Axelsson(1985)]{axelsson1985survey}
Axelsson, O.
\newblock A survey of preconditioned iterative methods for linear systems of
  algebraic equations.
\newblock \emph{BIT Numerical Mathematics}, 25\penalty0 (1):\penalty0 165--187,
  1985.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bruna et~al.(2013)Bruna, Zaremba, Szlam, and LeCun]{bruna2013spectral}
Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y.
\newblock Spectral networks and locally connected networks on graphs.
\newblock \emph{arXiv preprint arXiv:1312.6203}, 2013.

\bibitem[Cai et~al.(2019)Cai, Gao, Hou, Chen, Wang, He, Zhang, and
  Wang]{cai2019gram}
Cai, T., Gao, R., Hou, J., Chen, S., Wang, D., He, D., Zhang, Z., and Wang, L.
\newblock A gram-gauss-newton method learning overparameterized deep neural
  networks for regression problems.
\newblock \emph{arXiv preprint arXiv:1905.11675}, 2019.

\bibitem[Chen et~al.(2020)Chen, Tang, Qi, Li, and Xiao]{chen2020learning}
Chen, Y., Tang, X., Qi, X., Li, C.-G., and Xiao, R.
\newblock Learning graph normalization for graph neural networks, 2020.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3844--3852, 2016.

\bibitem[Demmel(1997)]{demmel1997applied}
Demmel, J.~W.
\newblock \emph{Applied numerical linear algebra}, volume~56.
\newblock Siam, 1997.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685. PMLR, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Hou, Salakhutdinov, Poczos, Wang, and
  Xu]{du2019graph}
Du, S.~S., Hou, K., Salakhutdinov, R.~R., Poczos, B., Wang, R., and Xu, K.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5724--5734, 2019{\natexlab{b}}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
  Bresson]{dwivedi2020benchmarking}
Dwivedi, V.~P., Joshi, C.~K., Laurent, T., Bengio, Y., and Bresson, X.
\newblock Benchmarking graph neural networks.
\newblock \emph{arXiv preprint arXiv:2003.00982}, 2020.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1273--1272, 2017.

\bibitem[Gori et~al.(2005)Gori, Monfardini, and Scarselli]{gori2005new}
Gori, M., Monfardini, G., and Scarselli, F.
\newblock A new model for learning in graph domains.
\newblock In \emph{Proceedings. 2005 IEEE International Joint Conference on
  Neural Networks, 2005.}, volume~2, pp.\  729--734. IEEE, 2005.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1024--1034, 2017.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1225--1234. PMLR, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hoffer et~al.(2018)Hoffer, Banner, Golan, and Soudry]{hoffer2018norm}
Hoffer, E., Banner, R., Golan, I., and Soudry, D.
\newblock Norm matters: efficient and accurate normalization schemes in deep
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2160--2170, 2018.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{horn2012matrix}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{arXiv preprint arXiv:2005.00687}, 2020.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Ivanov \& Burnaev(2018)Ivanov and Burnaev]{ivanov2018anonymous}
Ivanov, S. and Burnaev, E.
\newblock Anonymous walk embeddings.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2191--2200, 2018.

\bibitem[Keriven \& Peyr{\'e}(2019)Keriven and Peyr{\'e}]{keriven2019universal}
Keriven, N. and Peyr{\'e}, G.
\newblock Universal invariant and equivariant graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7092--7101, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kohler et~al.(2019)Kohler, Daneshmand, Lucchi, Hofmann, Zhou, and
  Neymeyr]{kohler2019exponential}
Kohler, J., Daneshmand, H., Lucchi, A., Hofmann, T., Zhou, M., and Neymeyr, K.
\newblock Exponential convergence rates for batch normalization: The power of
  length-direction decoupling in non-convex optimization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  806--815, 2019.

\bibitem[Li et~al.(2020)Li, Xiong, Thabet, and Ghanem]{li2020deepergcn}
Li, G., Xiong, C., Thabet, A., and Ghanem, B.
\newblock Deepergcn: All you need to train deeper gcns.
\newblock \emph{arXiv preprint arXiv:2006.07739}, 2020.

\bibitem[Li \& Arora(2019)Li and Arora]{li2019exponential}
Li, Z. and Arora, S.
\newblock An exponential learning rate schedule for deep learning.
\newblock \emph{arXiv preprint arXiv:1910.07454}, 2019.

\bibitem[Loukas(2020)]{Loukas2020}
Loukas, A.
\newblock How hard is to distinguish graphs with graph neural networks?
\newblock In \emph{Advances in neural information processing systems}, 2020.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1QRgziT-}.

\bibitem[Monti et~al.(2017)Monti, Boscaini, Masci, Rodola, Svoboda, and
  Bronstein]{monti2017geometric}
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein,
  M.~M.
\newblock Geometric deep learning on graphs and manifolds using mixture model
  cnns.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5115--5124, 2017.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  901--909, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen,
  X.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2234--2242, 2016.

\bibitem[Santoro et~al.(2017)Santoro, Raposo, Barrett, Malinowski, Pascanu,
  Battaglia, and Lillicrap]{santoro2017simple}
Santoro, A., Raposo, D., Barrett, D.~G., Malinowski, M., Pascanu, R.,
  Battaglia, P., and Lillicrap, T.
\newblock A simple neural network module for relational reasoning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4967--4976, 2017.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A.
\newblock How does batch normalization help optimization?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2483--2493, 2018.

\bibitem[Sato et~al.(2019)Sato, Yamada, and Kashima]{sato2019approximation}
Sato, R., Yamada, M., and Kashima, H.
\newblock Approximation ratios of graph neural networks for combinatorial
  problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4083--4092, 2019.

\bibitem[Scarselli et~al.(2008)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{scarselli2008graph}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
\newblock The graph neural network model.
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (1):\penalty0 61--80, 2008.

\bibitem[Scarselli et~al.(2018)Scarselli, Tsoi, and
  Hagenbuchner]{scarselli2018vapnik}
Scarselli, F., Tsoi, A.~C., and Hagenbuchner, M.
\newblock The vapnik--chervonenkis dimension of graph and recursive neural
  networks.
\newblock \emph{Neural Networks}, 108:\penalty0 248--259, 2018.

\bibitem[Shen et~al.(2020)Shen, Yao, Gholami, Mahoney, and
  Keutzer]{shen2020powernorm}
Shen, S., Yao, Z., Gholami, A., Mahoney, M.~W., and Keutzer, K.
\newblock Powernorm: Rethinking batch normalization in transformers, 2020.

\bibitem[Shervashidze et~al.(2011)Shervashidze, Schweitzer, Leeuwen, Mehlhorn,
  and Borgwardt]{shervashidze2011weisfeiler}
Shervashidze, N., Schweitzer, P., Leeuwen, E. J.~v., Mehlhorn, K., and
  Borgwardt, K.~M.
\newblock Weisfeiler-lehman graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Sep):\penalty0 2539--2561, 2011.

\bibitem[Stokes et~al.(2020)Stokes, Yang, Swanson, Jin, Cubillos-Ruiz, Donghia,
  MacNair, French, Carfrae, Bloom-Ackerman, et~al.]{stokes2020deep}
Stokes, J.~M., Yang, K., Swanson, K., Jin, W., Cubillos-Ruiz, A., Donghia,
  N.~M., MacNair, C.~R., French, S., Carfrae, L.~A., Bloom-Ackerman, Z., et~al.
\newblock A deep learning approach to antibiotic discovery.
\newblock \emph{Cell}, 180\penalty0 (4):\penalty0 688--702, 2020.

\bibitem[Sukhbaatar et~al.(2016)Sukhbaatar, Fergus,
  et~al.]{sukhbaatar2016learning}
Sukhbaatar, S., Fergus, R., et~al.
\newblock Learning multiagent communication with backpropagation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2244--2252, 2016.

\bibitem[Sun et~al.(2020)Sun, Jiang, Trulls, Tagliasacchi, and Yi]{sun2020acne}
Sun, W., Jiang, W., Trulls, E., Tagliasacchi, A., and Yi, K.~M.
\newblock Acne: Attentive context normalization for robust
  permutation-equivariant learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11286--11295, 2020.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and
  Lempitsky]{ulyanov2016instance}
Ulyanov, D., Vedaldi, A., and Lempitsky, V.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint arXiv:1607.08022}, 2016.

\bibitem[Velickovic et~al.(2018)Velickovic, Cucurull, Casanova, Romero, Lio,
  and Bengio]{velivckovic2017graph}
Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y.
\newblock Graph attention networks.
\newblock 2018.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  3--19, 2018.

\bibitem[Wu et~al.(2017)Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
  Leswing, and Pande]{DBLP:journals/corr/WuRFGGPLP17}
Wu, Z., Ramsundar, B., Feinberg, E.~N., Gomes, J., Geniesse, C., Pappu, A.~S.,
  Leswing, K., and Pande, V.~S.
\newblock Moleculenet: {A} benchmark for molecular machine learning.
\newblock \emph{CoRR}, abs/1703.00564, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.00564}.

\bibitem[Wu et~al.(2020)Wu, Pan, Chen, Long, Zhang, and
  Philip]{wu2020comprehensive}
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S.~Y.
\newblock A comprehensive survey on graph neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan,
  Y., Wang, L., and Liu, T.-Y.
\newblock On layer normalization in the transformer architecture.
\newblock \emph{arXiv preprint arXiv:2002.04745}, 2020.

\bibitem[Xu et~al.(2018)Xu, Li, Tian, Sonobe, Kawarabayashi, and
  Jegelka]{xu2018representation}
Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5453--5462, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018how}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryGs6iA5Km}.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Du, ichi Kawarabayashi, and
  Jegelka]{Xu2020What}
Xu, K., Li, J., Zhang, M., Du, S.~S., ichi Kawarabayashi, K., and Jegelka, S.
\newblock What can neural networks reason about?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rJxbJeHFPS}.

\bibitem[Xu et~al.(2021)Xu, Zhang, Li, Du, Kawarabayashi, and
  Jegelka]{xu2020neural}
Xu, K., Zhang, M., Li, J., Du, S.~S., Kawarabayashi, K.-I., and Jegelka, S.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yan et~al.(2019)Yan, Wan, Zhang, Zhang, Wei, and Sun]{yan2019towards}
Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y., and Sun, J.
\newblock Towards stabilizing batch statistics in backward propagation of batch
  normalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yanardag \& Vishwanathan(2015)Yanardag and
  Vishwanathan]{yanardag2015deep}
Yanardag, P. and Vishwanathan, S.
\newblock Deep graph kernels.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  1365--1374, 2015.

\bibitem[Yang et~al.(2020)Yang, Wang, Yao, Liu, and
  Abdelzaher]{yang2020revisiting}
Yang, C., Wang, R., Yao, S., Liu, S., and Abdelzaher, T.
\newblock Revisiting" over-smoothing" in deep gcns.
\newblock \emph{arXiv preprint arXiv:2003.13663}, 2020.

\bibitem[Yi et~al.(2018)Yi, Trulls, Ono, Lepetit, Salzmann, and
  Fua]{yi2018learning}
Yi, K.~M., Trulls, E., Ono, Y., Lepetit, V., Salzmann, M., and Fua, P.
\newblock Learning to find good correspondences.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2666--2674, 2018.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021transformers}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y.
\newblock Do transformers really perform bad for graph representation?, 2021.

\bibitem[Ying et~al.(2018)Ying, He, Chen, Eksombatchai, Hamilton, and
  Leskovec]{ying2018graph}
Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W.~L., and Leskovec, J.
\newblock Graph convolutional neural networks for web-scale recommender
  systems.
\newblock In \emph{kdd}, pp.\  974--983, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Cui, Neumann, and Chen]{zhang2018end}
Zhang, M., Cui, Z., Neumann, M., and Chen, Y.
\newblock An end-to-end deep learning architecture for graph classification.
\newblock pp.\  4438--4445, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Cui, and Zhu]{zhang2020deep}
Zhang, Z., Cui, P., and Zhu, W.
\newblock Deep learning on graphs: A survey.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2020.

\bibitem[Zhao \& Akoglu(2020)Zhao and Akoglu]{Zhao2020PairNorm:}
Zhao, L. and Akoglu, L.
\newblock Pairnorm: Tackling oversmoothing in gnns.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkecl1rtwB}.

\bibitem[Zhou et~al.(2018)Zhou, Cui, Zhang, Yang, Liu, Wang, Li, and
  Sun]{zhou2018graph}
Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M.
\newblock Graph neural networks: A review of methods and applications.
\newblock \emph{arXiv preprint arXiv:1812.08434}, 2018.

\bibitem[Zhou et~al.(2020{\natexlab{a}})Zhou, Dong, Lee, Hooi, Xu, and
  Feng]{zhou2020effective}
Zhou, K., Dong, Y., Lee, W.~S., Hooi, B., Xu, H., and Feng, J.
\newblock Effective training strategies for deep graph neural networks,
  2020{\natexlab{a}}.

\bibitem[Zhou et~al.(2020{\natexlab{b}})Zhou, Huang, Li, Zha, Chen, and
  Hu]{zhou2020towards}
Zhou, K., Huang, X., Li, Y., Zha, D., Chen, R., and Hu, X.
\newblock Towards deeper graph neural networks with differentiable group
  normalization.
\newblock \emph{arXiv preprint arXiv:2006.06972}, 2020{\natexlab{b}}.

\bibitem[Zou et~al.(2019)Zou, Hu, Wang, Jiang, Sun, and Gu]{zou2019layer}
Zou, D., Hu, Z., Wang, Y., Jiang, S., Sun, Y., and Gu, Q.
\newblock Layer-dependent importance sampling for training deep and large graph
  convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11249--11259, 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
