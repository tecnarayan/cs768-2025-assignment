\begin{thebibliography}{143}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{NeurIPS}, 35:\penalty0 23716--23736, 2022.

\bibitem[Alizadeh et~al.(2023)Alizadeh, Mirzadeh, Belenko, Khatamifard, Cho, Mundo, Rastegari, and Farajtabar]{DBLP:journals/corr/abs-2312-11514}
Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C.~Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar.
\newblock {LLM} in a flash: Efficient large language model inference with limited memory.
\newblock \emph{CoRR}, abs/2312.11514, 2023.

\bibitem[Aminabadi et~al.(2022)Aminabadi, Rajbhandari, Awan, Li, Li, Zheng, Ruwase, Smith, Zhang, Rasley, and He]{DBLP:conf/sc/AminabadiRALLZRSZRH22}
Reza~Yazdani Aminabadi, Samyam Rajbhandari, Ammar~Ahmad Awan, Cheng Li, Du~Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He.
\newblock Deepspeed- inference: Enabling efficient inference of transformer models at unprecedented scale.
\newblock In \emph{{SC22}}, pages 46:1--46:15, 2022.

\bibitem[Anthropic(2023)]{claude}
Anthropic.
\newblock Introducing {Claude}, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{mbpp}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, Jitsev, Kornblith, Koh, Ilharco, Wortsman, and Schmidt]{DBLP:journals/corr/abs-2308-01390}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir~Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang~Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock \emph{CoRR}, abs/2308.01390, 2023.

\bibitem[Bai et~al.(2023{\natexlab{a}})Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023{\natexlab{a}}.

\bibitem[Bai et~al.(2023{\natexlab{b}})Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{bai2023qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock \emph{arXiv preprint arXiv:2308.12966}, 2023{\natexlab{b}}.

\bibitem[Banerjee et~al.(2019)Banerjee, Pal, Mitra, and Baral]{DBLP:conf/acl/BanerjeePMB19}
Pratyay Banerjee, Kuntal~Kumar Pal, Arindam Mitra, and Chitta Baral.
\newblock Careful selection of knowledge to solve open book question answering.
\newblock In \emph{{ACL}}, pages 6120--6129. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/P19-1615}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1615}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In \emph{{AAAI}}, pages 7432--7439, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{NeurIPS}, 33:\penalty0 1877--1901, 2020.

\bibitem[Cha et~al.(2023)Cha, Kang, Mun, and Roh]{cha2023honeybee}
Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.
\newblock Honeybee: Locality-enhanced projector for multimodal llm.
\newblock \emph{arXiv preprint arXiv:2312.06742}, 2023.

\bibitem[Chang et~al.(2022)Chang, Cao, Narang, Gao, Suzuki, and Bisk]{DBLP:conf/cvpr/ChangCNGSB22}
Yingshan Chang, Guihong Cao, Mridu Narang, Jianfeng Gao, Hisami Suzuki, and Yonatan Bisk.
\newblock Webqa: Multihop and multimodal {QA}.
\newblock In \emph{{CVPR}}, pages 16474--16483. {IEEE}, 2022.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Chen, Zhang, Chen, Wu, Zhang, Chen, Li, Wan, and Wang]{chen2024allava}
Guiming~Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang.
\newblock Allava: Harnessing gpt4v-synthesized data for a lite vision-language model.
\newblock \emph{arXiv preprint arXiv:2402.11684}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin]{chen2023sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock \emph{arXiv preprint arXiv:2311.12793}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Li, Dong, Zhang, Zang, Chen, Duan, Wang, Qiao, Lin, and Zhao]{mmstar}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, and Feng Zhao.
\newblock Are we on the right way for evaluating large vision-language models?
\newblock \emph{arXiv preprint arXiv:2403.20330}, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2015)Chen, Fang, Lin, Vedantam, Gupta, Dollar, and Zitnick]{chen2015microsoft}
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C.~Lawrence Zitnick.
\newblock Microsoft coco captions: Data collection and evaluation server, 2015.

\bibitem[Chen et~al.(2024{\natexlab{c}})Chen, Wang, Wu, Gao, Xu, and Hu]{tomgpt}
Yunkai Chen, Qimeng Wang, Shiwei Wu, Yan Gao, Tong Xu, and Yao Hu.
\newblock Tomgpt: Reliable text-only training approach for cost-effective multi-modal large language model.
\newblock \emph{ACM Trans. Knowl. Discov. Data}, 2024{\natexlab{c}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wu, Wang, Su, Chen, Xing, Muyan, Zhang, Zhu, Lu, et~al.]{chen2023internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock \emph{arXiv preprint arXiv:2312.14238}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2024{\natexlab{d}})Chen, Wang, Tian, Ye, Gao, Cui, Tong, Hu, Luo, Ma, et~al.]{chen2024far}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et~al.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock \emph{arXiv preprint arXiv:2404.16821}, 2024{\natexlab{d}}.

\bibitem[Chi et~al.(2021)Chi, Chung, Wu, Hsieh, Chen, Li, and Lee]{DBLP:conf/slt/ChiCWHC0L21}
Po{-}Han Chi, Pei{-}Hung Chung, Tsung{-}Han Wu, Chun{-}Cheng Hsieh, Yen{-}Hao Chen, Shang{-}Wen Li, and Hung{-}yi Lee.
\newblock Audio albert: {A} lite bert for self-supervised learning of audio representation.
\newblock In \emph{{IEEE} {SLT}}, pages 344--350, 2021.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{J. Mach. Learn. Res.}, 24:\penalty0 240:1--240:113, 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{DBLP:conf/nips/ChristianoLBMLA17}
Paul~F. Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{NeurIPS}, pages 4299--4307, 2017.

\bibitem[Chu et~al.(2023)Chu, Qiao, Lin, Xu, Yang, Hu, Wei, Zhang, Zhang, Wei, et~al.]{chu2023mobilevlm}
Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo~Zhang, Xiaolin Wei, et~al.
\newblock Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.
\newblock \emph{arXiv preprint arXiv:2312.16886}, 2023.

\bibitem[Chu et~al.(2024)Chu, Qiao, Zhang, Xu, Wei, Yang, Sun, Hu, Lin, Zhang, et~al.]{chu2024mobilevlm}
Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo~Zhang, et~al.
\newblock Mobilevlm v2: Faster and stronger baseline for vision language model.
\newblock \emph{arXiv preprint arXiv:2402.03766}, 2024.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{DBLP:journals/corr/abs-2210-11416}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{CoRR}, abs/2210.11416, 2022.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the {AI2} reasoning challenge.
\newblock \emph{CoRR}, abs/1803.05457, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dai et~al.(2024)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale~N Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock \emph{NeurIPS}, 36, 2024.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.

\bibitem[Dong et~al.(2023)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and Sui]{DBLP:journals/corr/abs-2301-00234}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, Lei Li, and Zhifang Sui.
\newblock A survey for in-context learning.
\newblock \emph{CoRR}, abs/2301.00234, 2023.

\bibitem[Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang, Duan, Cao, et~al.]{dong2024ixc2}
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et~al.
\newblock Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.
\newblock \emph{arXiv preprint arXiv:2401.16420}, 2024.

\bibitem[Du et~al.(2022)Du, Qian, Liu, Ding, Qiu, Yang, and Tang]{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank infilling.
\newblock In \emph{ACL}, pages 320--335, 2022.

\bibitem[Duan et~al.(2023)Duan, Xia, Zhou, Tang, Zhu, and Zhao]{DBLP:conf/nips/DuanXZTZZ23}
Haoyi Duan, Yan Xia, Mingze Zhou, Li~Tang, Jieming Zhu, and Zhou Zhao.
\newblock Cross-modal prompts: Adapting large pre-trained models for audio-visual downstream tasks.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Feng et~al.(2024)Feng, Hao, Zhang, Han, and Wang]{DBLP:journals/corr/abs-2403-03432}
Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu~Han, and Hao Wang.
\newblock Mixture-of-loras: An efficient multitask tuning for large language models.
\newblock \emph{CoRR}, abs/2403.03432, 2024.

\bibitem[Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng, et~al.]{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2306.13394}, 2023.

\bibitem[Goel et~al.(2022)Goel, Bansal, Bhatia, Rossi, Vinay, and Grover]{DBLP:conf/nips/GoelBBRVG22}
Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan~A. Rossi, Vishwa Vinay, and Aditya Grover.
\newblock Cyclip: Cyclic contrastive language-image pretraining.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Group(2017)]{chroma}
Chroma Group.
\newblock Chroma - the open-source embedding database.
\newblock https://github.com/chroma-core/chroma, 2017.

\bibitem[Guan et~al.(2023)Guan, Liu, Wu, Xian, Li, Liu, Wang, Chen, Huang, Yacoob, et~al.]{guan2023hallusionbench}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et~al.
\newblock Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models.
\newblock \emph{arXiv preprint arXiv:2310.14566}, 2023.

\bibitem[He et~al.(2024)He, Li, Jang, Jia, Cao, Shah, Shrivastava, and Lim]{he2024malmm}
Bo~He, Hengduo Li, Young~Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim.
\newblock Ma-lmm: Memory-augmented large multimodal model for long-term video understanding.
\newblock In \emph{{CVPR}}, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hong et~al.(2023)Hong, Wang, Lv, Xu, Yu, Ji, Wang, Wang, Dong, Ding, et~al.]{hong2023cogagent}
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et~al.
\newblock Cogagent: A visual language model for gui agents.
\newblock \emph{arXiv preprint arXiv:2312.08914}, 2023.

\bibitem[Hu et~al.(2023)Hu, Shi, Xu, Ye, Ye, Yan, Li, Qian, Zhang, and Huang]{hu2023paperowl}
Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi~Qian, Ji~Zhang, and Fei Huang.
\newblock mplug-paperowl: Scientific diagram analysis with the multimodal large language model.
\newblock \emph{arXiv preprint arXiv:2311.18248}, 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{DBLP:conf/iclr/HuSWALWWC22}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{{ICLR}}, 2022.

\bibitem[Hu et~al.(2020)Hu, Richardson, Xu, Li, K{\"{u}}bler, and Moss]{ocnli}
Hai Hu, Kyle Richardson, Liang Xu, Lu~Li, Sandra K{\"{u}}bler, and Lawrence~S. Moss.
\newblock {OCNLI:} original chinese natural language inference.
\newblock In \emph{{EMNLP}}, volume {EMNLP} 2020 of \emph{Findings of {ACL}}, pages 3512--3526, 2020.

\bibitem[Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, et~al.]{minicpm}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al.
\newblock Minicpm: Unveiling the potential of small language models with scalable training strategies.
\newblock \emph{arXiv preprint arXiv:2404.06395}, 2024.

\bibitem[Huang and Chang(2023)]{DBLP:conf/acl/0009C23}
Jie Huang and Kevin~Chen{-}Chuan Chang.
\newblock Towards reasoning in large language models: {A} survey.
\newblock In \emph{{ACL}}, 2023.

\bibitem[{IDEFICS}(2023)]{idefics2023}
{IDEFICS}.
\newblock Introducing idefics: An open reproduction of state-of-the-art visual language model.
\newblock \url{https://huggingface.co/blog/idefics}, 2023.

\bibitem[Jain et~al.(2023)Jain, Yang, and Shi]{DBLP:journals/corr/abs-2312-14233}
Jitesh Jain, Jianwei Yang, and Humphrey Shi.
\newblock Vcoder: Versatile vision encoders for multimodal large language models.
\newblock \emph{CoRR}, abs/2312.14233, 2023.

\bibitem[Ji et~al.(2023)Ji, Deng, Gong, Peng, Niu, Zhang, Ma, and Li]{belle2023exploring}
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li.
\newblock Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases.
\newblock \emph{arXiv preprint arXiv:2303.14742}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{DBLP:journals/corr/abs-2001-08361}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{CoRR}, abs/2001.08361, 2020.

\bibitem[Kembhavi et~al.(2016)Kembhavi, Salvato, Kolve, Seo, Hajishirzi, and Farhadi]{kembhavi2016ai2d}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In \emph{ECCV}, pages 235--251, 2016.

\bibitem[Kenton and Toutanova(2019)]{devlin2018bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL}, pages 4171--4186, 2019.

\bibitem[Kim et~al.(2022)Kim, Hong, Yim, Nam, Park, Yim, Hwang, Yun, Han, and Park]{kim2022synthdog}
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
\newblock Ocr-free document understanding transformer.
\newblock In \emph{ECCV}, 2022.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard~H. Hovy.
\newblock {RACE:} large-scale reading comprehension dataset from examinations.
\newblock In \emph{{EMNLP}}, pages 785--794, 2017.

\bibitem[Laurençon et~al.(2023)Laurençon, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, Kiela, Cord, and Sanh]{laurencon2023idefics}
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Yang, Zhang, Pu, and Liu]{li2023otterhd}
Bo~Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu.
\newblock Otterhd: A high-resolution multi-modality model.
\newblock \emph{arXiv preprint arXiv:2311.04219}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, Chen, Wang, Yang, and Liu]{li2023otter}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock \emph{arXiv preprint arXiv:2305.03726}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Wang, Wang, Ge, Ge, and Shan]{li2023seed}
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
\newblock Seed-bench: Benchmarking multimodal llms with generative comprehension.
\newblock \emph{arXiv preprint arXiv:2307.16125}, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin]{cmmlu}
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.
\newblock {CMMLU}: Measuring massive multitask language understanding in {Chinese}.
\newblock \emph{arXiv preprint arXiv:2306.09212}, 2023{\natexlab{d}}.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Wang, Zhu, Kuo, Xu, Chen, Jain, Shi, and Wen]{li2024cumo}
Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-wen Kuo, Lu~Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin Wen.
\newblock Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts.
\newblock \emph{arXiv:}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi]{DBLP:conf/nips/LiSGJXH21}
Junnan Li, Ramprasaath~R. Selvaraju, Akhilesh Gotmare, Shafiq~R. Joty, Caiming Xiong, and Steven~Chu{-}Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock In \emph{NeurIPS}, pages 9694--9705, 2021.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{ICML}, pages 12888--12900, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Savarese, and Hoi]{DBLP:journals/corr/abs-2206-02967}
Junnan Li, Silvio Savarese, and Steven C.~H. Hoi.
\newblock Masked unsupervised self-training for zero-shot image classification.
\newblock \emph{CoRR}, abs/2206.02967, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{e}})Li, Li, Savarese, and Hoi]{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{ICML}, pages 19730--19742. PMLR, 2023{\natexlab{e}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Wang, Xu, Wang, Feng, Kong, and Liu]{DBLP:journals/corr/abs-2403-00231}
Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi~Liu.
\newblock Multimodal arxiv: {A} dataset for improving scientific comprehension of large vision-language models.
\newblock \emph{CoRR}, abs/2403.00231, 2024{\natexlab{b}}.

\bibitem[Li and Liang(2021)]{DBLP:conf/acl/LiL20}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{{ACL/IJCNLP}}, pages 4582--4597, 2021.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Zhang, Wang, Zhong, Chen, Chu, Liu, and Jia]{li2024miniGemini}
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.
\newblock Mini-gemini: Mining the potential of multi-modality vision language models.
\newblock \emph{arXiv preprint arXiv:2403.18814}, 2024{\natexlab{c}}.

\bibitem[Li et~al.(2023{\natexlab{f}})Li, Yang, Liu, Ma, Zhang, Yang, Sun, Liu, and Bai]{li2023monkey}
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
\newblock Monkey: Image resolution and text label are important things for large multi-modal models.
\newblock \emph{arXiv preprint arXiv:2311.06607}, 2023{\natexlab{f}}.

\bibitem[Liang et~al.(2022)Liang, Zhang, Kwon, Yeung, and Zou]{DBLP:conf/nips/LiangZKYZ22}
Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James~Y. Zou.
\newblock Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Lin et~al.(2023{\natexlab{a}})Lin, Zhu, Ye, Ning, Jin, and Yuan]{lin2023video}
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li~Yuan.
\newblock Video-llava: Learning united visual representation by alignment before projection.
\newblock \emph{arXiv preprint arXiv:2311.10122}, 2023{\natexlab{a}}.

\bibitem[Lin et~al.(2024{\natexlab{a}})Lin, Peng, Zhang, Sun, Li, Zhao, Xiao, Xu, Qiu, Li, Ji, Li, and Lin]{DBLP:journals/corr/abs-2401-02669}
Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong Xiao, Qi~Xu, Xiafei Qiu, Shen Li, Zhigang Ji, Yong Li, and Wei Lin.
\newblock Infinite-llm: Efficient {LLM} service for long context with distattention and distributed kvcache.
\newblock \emph{CoRR}, abs/2401.02669, 2024{\natexlab{a}}.

\bibitem[Lin et~al.(2024{\natexlab{b}})Lin, Tang, Ye, Cui, Zhu, Jin, Zhang, Ning, and Yuan]{lin2024moellava}
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li~Yuan.
\newblock Moe-llava: Mixture of experts for large vision-language models.
\newblock \emph{arXiv preprint arXiv:2401.15947}, 2024{\natexlab{b}}.

\bibitem[Lin et~al.(2023{\natexlab{b}})Lin, Yin, Ping, Lu, Molchanov, Tao, Mao, Kautz, Shoeybi, and Han]{DBLP:journals/corr/abs-2312-07533}
Ji~Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.
\newblock {VILA:} on pre-training for visual language models.
\newblock \emph{CoRR}, abs/2312.07533, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2023{\natexlab{c}})Lin, Liu, Zhang, Gao, Qiu, Xiao, Qiu, Lin, Shao, Chen, et~al.]{lin2023sphinx}
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et~al.
\newblock Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.
\newblock \emph{arXiv preprint arXiv:2311.07575}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{NeurIPS}, 36, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Li, Li, Li, Zhang, Shen, and Lee]{liu2024llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, January 2024{\natexlab{a}}.
\newblock URL \url{https://llava-vl.github.io/blog/2024-01-30-llava-next/}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Cheng, Liu, Zhang, Li, Ren, Zou, Yang, Su, Zhu, Zhang, Gao, and Li]{liu2023llavaplus}
Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li.
\newblock Llava-plus: Learning to use tools for creating multimodal agents.
\newblock \emph{arXiv:2311.05437}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu, et~al.]{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock \emph{arXiv preprint arXiv:2307.06281}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yang, Liu, Li, Ma, Zhang, and Bai]{liu2024textmonkey}
Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.
\newblock Textmonkey: An ocr-free large multimodal model for understanding document.
\newblock \emph{arXiv preprint arXiv:2403.04473}, 2024{\natexlab{b}}.

\bibitem[Lu et~al.(2024)Lu, Liu, Zhang, Wang, Dong, Liu, Sun, Ren, Li, Sun, et~al.]{lu2024deepseekvl}
Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al.
\newblock Deepseek-vl: Towards real-world vision-language understanding.
\newblock \emph{arXiv preprint arXiv:2403.05525}, 2024.

\bibitem[Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and Kalyan]{lu2022scienceqa}
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock \emph{NeurIPS}, 35:\penalty0 2507--2521, 2022.

\bibitem[McKinzie et~al.(2024)McKinzie, Gan, Fauconnier, Dodge, Zhang, Dufter, Shah, Du, Peng, Weers, et~al.]{mckinzie2024mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et~al.
\newblock Mm1: Methods, analysis \& insights from multimodal llm pre-training.
\newblock \emph{arXiv preprint arXiv:2403.09611}, 2024.

\bibitem[Minaee et~al.(2024)Minaee, Mikolov, Nikzad, Chenaghlu, Socher, Amatriain, and Gao]{DBLP:journals/corr/abs-2402-06196}
Shervin Minaee, Tom{\'{a}}s Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao.
\newblock Large language models: {A} survey.
\newblock \emph{CoRR}, abs/2402.06196, 2024.

\bibitem[Nguyen et~al.(2020)Nguyen, Hassner, Archambeau, and Seeger]{nguyen2020leep}
Cuong~V Nguyen, Tal Hassner, Cedric Archambeau, and Matthias Seeger.
\newblock Leep: A new measure to evaluate transferability of learned representations.
\newblock In \emph{ICML}, 2020.

\bibitem[OpenAI(2022)]{openai2020chatgpt}
OpenAI.
\newblock Chatgpt.
\newblock \url{https://openai.com/blog/chatgpt}, 2022.

\bibitem[OpenAI(2023{\natexlab{a}})]{gpt4v}
OpenAI.
\newblock Gpt-4v(ision) system card.
\newblock \url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2023gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023{\natexlab{b}}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{DBLP:conf/nips/Ouyang0JAWMZASR22}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}, 2023.

\bibitem[Perez et~al.(2018)Perez, Strub, de~Vries, Dumoulin, and Courville]{film}
Ethan Perez, Florian Strub, Harm de~Vries, Vincent Dumoulin, and Aaron~C. Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{{AAAI}}, pages 3942--3951, 2018.

\bibitem[Qian et~al.(2023)Qian, Xu, and Hu]{DBLP:conf/nips/0001XH23}
Qi~Qian, Yuanhong Xu, and Juhua Hu.
\newblock Intra-modal proxy learning for zero-shot visual categorization with {CLIP}.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{ICML}, pages 8748--8763, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The J. Mach. Learn. Res.}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Rajbhandari et~al.(2021)Rajbhandari, Ruwase, Rasley, Smith, and He]{DBLP:conf/sc/RajbhandariRRSH21}
Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He.
\newblock Zero-infinity: breaking the {GPU} memory wall for extreme scale deep learning.
\newblock In \emph{International Conference for High Performance Computing, Networking, Storage and Analysis, {SC} 2021, St. Louis, Missouri, USA, November 14-19, 2021}, page~59. {ACM}, 2021.

\bibitem[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{reid2024gemini1_5}
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock \emph{arXiv preprint arXiv:2403.05530}, 2024.

\bibitem[Ren et~al.(2023)Ren, Yao, Li, Sun, and Hou]{Ren2023TimeChat}
Shuhuai Ren, Linli Yao, Shicheng Li, Xu~Sun, and Lu~Hou.
\newblock Timechat: A time-sensitive multimodal large language model for long video understanding.
\newblock \emph{ArXiv}, abs/2312.02051, 2023.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale, 2019.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, et~al.]{DBLP:conf/iclr/SanhWRBSACSRDBX22}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{{ICLR}}, 2022.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Bras, and Choi]{siqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan~Le Bras, and Yejin Choi.
\newblock {SocialIQA}: Commonsense reasoning about social interactions.
\newblock \emph{CoRR}, abs/1904.09728, 2019.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.

\bibitem[{StepFun Research Team}(2024)]{step1v2023}
{StepFun Research Team}.
\newblock Step-1v: A hundred billion parameter multimodal large model.
\newblock \url{https://platform.stepfun.com}, 2024.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{DBLP:conf/nips/StiennonO0ZLVRA20}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F. Christiano.
\newblock Learning to summarize with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.

\bibitem[Taori et~al.(2023{\natexlab{a}})Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023{\natexlab{a}}.

\bibitem[Taori et~al.(2023{\natexlab{b}})Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{taori2023alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \emph{Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html}, 3\penalty0 (6):\penalty0 7, 2023{\natexlab{b}}.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Team(2024)]{DBRX}
The Mosaic~Research Team.
\newblock Introducing dbrx: A new state-of-the-art open llm, March 2024.
\newblock URL \url{https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals, and Hill]{tsimpoukelli2021multimodal}
Maria Tsimpoukelli, Jacob~L Menick, Serkan Cabi, SM~Eslami, Oriol Vinyals, and Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 200--212, 2021.

\bibitem[Valipour et~al.(2022)Valipour, Rezagholizadeh, Kobyzev, and Ghodsi]{valipour2022dylora}
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi.
\newblock Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation.
\newblock \emph{arXiv preprint arXiv:2210.07558}, 2022.

\bibitem[Vaswani et~al.(2017{\natexlab{a}})Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{DBLP:conf/nips/VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017{\natexlab{a}}.

\bibitem[Vaswani et~al.(2017{\natexlab{b}})Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{NeurIPS}, 30, 2017{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song, et~al.]{wang2023cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock \emph{arXiv preprint arXiv:2311.03079}, 2023.

\bibitem[Wei et~al.(2023)Wei, Kong, Chen, Zhao, Ge, Yang, Sun, Han, and Zhang]{wei2023vary}
Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.
\newblock Vary: Scaling up the vision vocabulary for large vision-language models.
\newblock \emph{arXiv preprint arXiv:2312.06109}, 2023.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{ICLR}, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{Trans. Mach. Learn. Res.}, 2022, 2022{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Yin, Qi, Wang, Tang, and Duan]{wu2023visual}
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation models.
\newblock \emph{arXiv preprint arXiv:2303.04671}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Fei, Qu, Ji, and Chua]{wu2023nextgpt}
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua.
\newblock Next-gpt: Any-to-any multimodal llm.
\newblock \emph{arXiv preprint arXiv:2309.05519}, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2020)Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian, Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou, Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and Lan]{clue}
Liang Xu, Hai Hu, Xuanwei Zhang, Lu~Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo~Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He~Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan.
\newblock {CLUE:} {A} chinese language understanding evaluation benchmark.
\newblock In \emph{{COLING}}, pages 4762--4772, 2020.

\bibitem[Xu et~al.(2024)Xu, Yao, Guo, Cui, Ni, Ge, Chua, Liu, Sun, and Huang]{xu2024llava_uhd}
Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang.
\newblock Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images.
\newblock \emph{arXiv preprint arXiv:2403.11703}, 2024.

\bibitem[Yang et~al.(2023)Yang, Li, Wang, Lin, Azarnasab, Ahmed, Liu, Liu, Zeng, and Wang]{yang2023mmreact}
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce~Liu, Michael Zeng, and Lijuan Wang.
\newblock Mm-react: Prompting chatgpt for multimodal reasoning and action.
\newblock \emph{arXiv preprint arXiv:2303.11381}, 2023.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov, and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language understanding.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Yi et~al.(2024)Yi, Zhan, and Ye]{DBLP:journals/corr/abs-2403-13797}
Chao Yi, De{-}Chuan Zhan, and Han{-}Jia Ye.
\newblock Bridge the modality and capacity gaps in vision-language model selection.
\newblock \emph{CoRR}, abs/2403.13797, 2024.

\bibitem[Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang, et~al.]{ai2024yi}
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al.
\newblock Yi: Open foundation models by 01. ai.
\newblock \emph{arXiv preprint arXiv:2403.04652}, 2024.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Chen, Codella, Dai, Gao, Hu, Huang, Li, Li, et~al.]{yuan2021florence}
Lu~Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock \emph{arXiv preprint arXiv:2111.11432}, 2021.

\bibitem[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, et~al.]{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock \emph{arXiv preprint arXiv:2311.16502}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In \emph{{ACL}}, pages 4791--4800, 2019.

\bibitem[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker, Tombari, Purohit, Ryoo, Sindhwani, et~al.]{zeng2022socratic}
Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et~al.
\newblock Socratic models: Composing zero-shot multimodal reasoning with language.
\newblock \emph{arXiv preprint arXiv:2204.00598}, 2022.

\bibitem[Zhai et~al.(2023)Zhai, Mustafa, Kolesnikov, and Beyer]{zhai2023siglip}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In \emph{ICCV}, pages 11975--11986, 2023.

\bibitem[Zhan et~al.(2024)Zhan, Dai, Ye, Zhou, Zhang, Liu, Zhang, Yuan, Zhang, Li, et~al.]{zhan2024anygpt}
Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge~Zhang, Linyang Li, et~al.
\newblock Anygpt: Unified multimodal llm with discrete sequence modeling.
\newblock \emph{arXiv preprint arXiv:2402.12226}, 2024.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Huang, Ding, Zhan, and Ye]{DBLP:conf/nips/ZhangHDZY23}
Yi{-}Kai Zhang, Ting{-}Ji Huang, Yao{-}Xiang Ding, De{-}Chuan Zhan, and Han{-}Jia Ye.
\newblock Model spider: Learning to rank pre-trained models efficiently.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Zheng et~al.(2019)Zheng, Huang, and Sun]{chid}
Chujie Zheng, Minlie Huang, and Aixin Sun.
\newblock Chid: {A} large-scale chinese idiom dataset for cloze test.
\newblock In \emph{{ACL}}, pages 778--787, 2019.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy]{DBLP:conf/nips/ZhouLX0SMMEYYZG23}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.
\newblock {LIMA:} less is more for alignment.
\newblock In \emph{{NeurIPS}}, 2023.

\bibitem[Zhu et~al.(2023{\natexlab{a}})Zhu, Lin, Ning, Yan, Cui, Wang, Pang, Jiang, Zhang, Li, Zhang, Li, Liu, and Yuan]{DBLP:journals/corr/abs-2310-01852}
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Hongfa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Li~Yuan.
\newblock Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment.
\newblock \emph{CoRR}, abs/2310.01852, 2023{\natexlab{a}}.

\bibitem[Zhu et~al.(2024)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock In \emph{ICLR}, 2024.

\bibitem[Zhu et~al.(2023{\natexlab{b}})Zhu, Hessel, Awadalla, Gadre, Dodge, Fang, Yu, Schmidt, Wang, and Choi]{DBLP:conf/nips/ZhuHAGDFYSW023}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal {C4:} an open, billion-scale corpus of images interleaved with text.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{DBLP:journals/corr/abs-1909-08593}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul~F. Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{CoRR}, abs/1909.08593, 2019.

\end{thebibliography}
