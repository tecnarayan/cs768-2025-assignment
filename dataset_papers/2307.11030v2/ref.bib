@article{izmailov2022feature,
  title={On Feature Learning in the Presence of Spurious Correlations},
  author={Izmailov, Pavel and Kirichenko, Polina and Gruver, Nate and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2210.11369},
  year={2022}
}

@article{vardi2022gradient,
  title={Gradient Methods Provably Converge to Non-Robust Networks},
  author={Vardi, Gal and Yehudai, Gilad and Shamir, Ohad},
  journal={arXiv preprint arXiv:2202.04347},
  year={2022}
}

@article{lee2021predicting,
  title={Predicting what you already know helps: Provable self-supervised learning},
  author={Lee, Jason D and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={309--323},
  year={2021}
}

@inproceedings{damian2022neural,
  title={Neural networks can learn representations with gradient descent},
  author={Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle={Conference on Learning Theory},
  pages={5413--5452},
  year={2022},
  organization={PMLR}
}

@article{ba2014deep,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{kim2018paraphrasing,
  title={Paraphrasing complex network: Network compression via factor transfer},
  author={Kim, Jangho and Park, SeongUk and Kwak, Nojun},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{zagoruyko2016paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1612.03928},
  year={2016}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@article{chen2017learning,
  title={Learning efficient object detection models with knowledge distillation},
  author={Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{phuong2019towards,
  title={Towards understanding knowledge distillation},
  author={Phuong, Mary and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={5142--5151},
  year={2019},
  organization={PMLR}
}

@article{kirichenko2022last,
  title={Last layer re-training is sufficient for robustness to spurious correlations},
  author={Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2204.02937},
  year={2022}
}

@inproceedings{nacson2022implicit,
  title={Implicit bias of the step size in linear diagonal neural networks},
  author={Nacson, Mor Shpigel and Ravichandran, Kavya and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={16270--16295},
  year={2022},
  organization={PMLR}
}


@article{udell2019big,
  title={Why are big data matrices approximately low rank?},
  author={Udell, Madeleine and Townsend, Alex},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={1},
  number={1},
  pages={144--160},
  year={2019},
  publisher={SIAM}
}

@book{wainwright2019, 
    place={Cambridge}, 
    series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
    title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint}, 
    publisher={Cambridge University Press}, 
    author={Wainwright, Martin J.}, 
    year={2019}, 
    collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@article{bartlett2003,
author = {Bartlett, Peter L. and Mendelson, Shahar},
title = {Rademacher and Gaussian Complexities: Risk Bounds and Structural Results},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {463–482},
numpages = {20},
keywords = {maximum discrepancy, error bounds, Rademacher averages, data-dependent complexity}
}

@article{haochen2021provable,
  title={Provable guarantees for self-supervised deep learning with spectral contrastive loss},
  author={HaoChen, Jeff Z and Wei, Colin and Gaidon, Adrien and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5000--5011},
  year={2021}
}

@inproceedings{louis2014approximation,
  title={Approximation algorithm for sparsest k-partitioning},
  author={Louis, Anand and Makarychev, Konstantin},
  booktitle={Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1244--1255},
  year={2014},
  organization={SIAM}
}

@book{golub2013,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press},
  address={Baltimore, MD, USA}
}

@Article{eckart1936,
    author={Eckart, Carl and Young, Gale},
    title={The approximation of one matrix by another of lower rank},
    journal={Psychometrika},
    year={1936},
    month={Sep},
    day={01},
    volume={1},
    number={3},
    pages={211-218},
    issn={1860-0980}
}


@inproceedings{balcan2005pac,
  title={A PAC-style model for learning from labeled and unlabeled data},
  author={Balcan, Maria-Florina and Blum, Avrim},
  booktitle={International Conference on Computational Learning Theory},
  pages={111--126},
  year={2005},
  organization={Springer}
}

@misc{balcan200621,
  title={21 an augmented pac model for semi-supervised learning},
  author={Balcan, Maria-Florina and Blum, Avrim},
  year={2006}
}

@book{ledoux2013,
    author={Ledoux, Michel and Talagrand, Michel},
    title={Probability in Banach Spaces: isoperimetry and processes},
    publisher={Springer Science \& Business Media}, 
    year={2013}
}

@inproceedings{maurer2016vector,
  title={A vector-contraction inequality for rademacher complexities},
  author={Maurer, Andreas},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={3--17},
  year={2016},
  organization={Springer}
}

@inproceedings{sener_active_2018,
title={Active Learning for Convolutional Neural Networks: A Core-Set Approach},
author={Ozan Sener and Silvio Savarese},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1aIuk-RW},
}

@inproceedings{liu2019knowledge,
  title={Knowledge distillation via instance relationship graph},
  author={Liu, Yufan and Cao, Jiajiong and Li, Bing and Yuan, Chunfeng and Hu, Weiming and Li, Yangxi and Duan, Yunqiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7096--7104},
  year={2019}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@article{liu2021data,
  title={Data-free knowledge transfer: A survey},
  author={Liu, Yuang and Zhang, Wei and Wang, Jun and Wang, Jianyong},
  journal={arXiv preprint arXiv:2112.15278},
  year={2021}
}

@inproceedings{chen2021deep,
  title={Deep metric learning with graph consistency},
  author={Chen, Binghui and Li, Pengyu and Yan, Zhaoyi and Wang, Biao and Zhang, Lei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={2},
  pages={982--990},
  year={2021}
}

@inproceedings{wangmakes,
  title={What Makes a" Good" Data Augmentation in Knowledge Distillation-A Statistical Perspective},
  author={Wang, Huan and Lohit, Suhas and Jones, Michael Jeffrey and Fu, Yun},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{qian2022improved,
  title={Improved Knowledge Distillation via Full Kernel Matrix Transfer},
  author={Qian, Qi and Li, Hao and Hu, Juhua},
  booktitle={Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)},
  pages={612--620},
  year={2022},
  organization={SIAM}
}

@software{huy_phan_2021_4431043,
  author       = {Huy Phan},
  title        = {huyvnphan/PyTorch\_CIFAR10},
  month        = {jan},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {v3.0.1},
  doi          = {10.5281/zenodo.4431043}
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3967--3976},
  year={2019}
}

@article{wang2021knowledge,
  title={Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks},
  author={Wang, Lin and Yoon, Kuk-Jin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}

@article{hsu2021generalization,
  title={Generalization bounds via distillation},
  author={Hsu, Daniel and Ji, Ziwei and Telgarsky, Matus and Wang, Lan},
  journal={arXiv preprint arXiv:2104.05641},
  year={2021}
}

@article{von_luxburg_tutorial_2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {1573-1375},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
	language = {en},
	number = {4},
	urldate = {2020-10-28},
	journal = {Statistics and Computing},
	author = {von Luxburg, Ulrike},
	month = dec,
	year = {2007},
	pages = {395--416},
}
% url = {https://doi.org/10.1007/},doi = {10.1007/s11222-007-9033-z},



@inproceedings{calder_poisson_2020,
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {Proceedings of Machine Learning Research},
	month = nov,
	year = {2020},
	pages = {1306--1316},
	title={Poisson Learning: Graph-Based semi-supervised learning at very low label rates},
      author={Calder, Jeff and Cook, Brendan and Thorpe, Matthew and Slep{\v{c}ev}, Dejan}
}



@article{calder_rates_2020,
	title = {Rates of Convergence for {Laplacian} Semi-Supervised Learning with Low Labeling Rates},
	url = {http://arxiv.org/abs/2006.02765},
	author = {Calder, Jeff and Slep{\v{c}ev}, Dejan and Thorpe, Matthew},
	month = jun,
	year = {2020},
    journal = {preprint arXiv}
}



@article{calder2022improved,
  title={Improved spectral convergence rates for graph {Laplacians} on $\varepsilon$-graphs and k-{NN} graphs},
  author = {Jeff Calder and Garc\'ia Trillos, N.},
  journal = "Applied and Computational Harmonic Analysis",
  volume = {60},
  pages = {123--175},
  arxiv = {https://arxiv.org/abs/1910.13476},
  code = {https://github.com/jwcalder/kNNSpectralRates},
  year={2022}
}
% url = {https://doi.org/10.1016/j.acha.2022.02.004},

@article{calder2020properly,
	author = {Jeff Calder and Dejan Slep\v{c}ev},
	journal = {Applied Mathematics and Optimization: Special Issue on Optimization in Data Science},
	title = "{Properly-weighted graph {L}aplacian for semi-supervised learning}",
   volume = {82},
   pages = {1111--1159},
   arxiv = {https://arxiv.org/abs/1810.04351},
	year = "2020"
}
% url = {https://doi.org/10.1007/s00245-019-09637-3},

@inproceedings{zhu_semi-supervised_2003,
	address = {Washington, DC, USA},
	title = {Semi-supervised learning using {Gaussian} fields and harmonic functions},
	isbn = {978-1-57735-189-4},
	abstract = {An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.},
	urldate = {2020-06-11},
	booktitle = {Proceedings of the 20th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {AAAI Press},
	author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
	month = aug,
	year = {2003},
	pages = {912--919},
}

@incollection{bertozzi2019graph,
  title={Graph-based optimization approaches for machine learning, uncertainty quantification and networks},
  author={Bertozzi, Andrea L and Merkurjev, Ekaterina},
  booktitle={Handbook of Numerical Analysis},
  volume={20},
  pages={503--531},
  year={2019},
  publisher={Elsevier}
}


@article{zhou2018graph,
title = {Graph neural networks: {A} review of methods and applications},
journal = {AI Open},
volume = {1},
pages = {57-81},
year = {2020},
issn = {2666-6510},
author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
keywords = {Deep learning, Graph neural network},
}
%doi = {https://doi.org/10.1016/j.aiopen.2021.01.001},



@inproceedings{zhou2003llgc,
 author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas and Weston, Jason and Sch\"{o}lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Learning with Local and Global Consistency},
 volume = {16},
 year = {2003}
}
%url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf},


@inproceedings{nadler2009infiniteunlabelled,
author = {Nadler, Boaz and Srebro, Nathan and Zhou, Xueyuan},
title = {Semi-Supervised Learning with the Graph {Laplacian}: {The} Limit of Infinite Unlabelled Data},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points. We show that in ℝd, d ≥ 2, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function. We also contrast the method with the Laplacian Eigenvector method, and discuss the "smoothness" assumptions associated with this alternate method.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {1330–1338},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}


@article{shi2017weighted,
  title={Weighted nonlocal {Laplacian} on interpolation from sparse data},
  author={Shi, Zuoqiang and Osher, Stanley and Zhu, Wei},
  journal={Journal of Scientific Computing},
  volume={73},
  number={2},
  pages={1164--1177},
  year={2017},
  publisher={Springer}
}


@article{garcia2020error,
  title={Error estimates for spectral convergence of the graph {L}aplacian on random geometric graphs toward the {Laplace--Beltrami} operator},
  author={Garc{\'\i}a Trillos, Nicol{\'a}s and Gerlach, Moritz and Hein, Matthias and Slep{\v{c}}ev, Dejan},
  journal={Foundations of Computational Mathematics},
  volume={20},
  number={4},
  pages={827--887},
  year={2020},
  publisher={Springer}
}
@inproceedings{hein2005graphs,
  title={From Graphs to Manifolds-Weak and Strong Pointwise Consistency of Graph Laplacians.},
  author={Hein, Matthias and Audibert, Jean-Yves and Von Luxburg, Ulrike},
  booktitle={COLT},
  volume={3559},
  pages={470--485},
  year={2005},
  organization={Springer}
}
@article{hein2007graph,
  title={Graph {L}aplacians and their convergence on random neighborhood graphs.},
  author={Hein, Matthias and Audibert, Jean-Yves and Luxburg, Ulrike von},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={6},
  year={2007}
}


@inproceedings{belkin2004regularization,
  title={Regularization and semi-supervised learning on large graphs},
  author={Belkin, Mikhail and Matveeva, Irina and Niyogi, Partha},
  booktitle={Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada, July 1-4, 2004. Proceedings 17},
  pages={624--638},
  year={2004},
  organization={Springer}
}
@article{belkin2004semi,
  title={Semi-supervised learning on {Riemannian} manifolds},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Machine learning},
  volume={56},
  pages={209--239},
  year={2004},
  publisher={Springer}
}



@article{schreiber2020apricot,
  author  = {Jacob Schreiber and Jeffrey Bilmes and William Stafford Noble},
  title   = {apricot: Submodular selection for data summarization in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {161},
  pages   = {1--6},
}

@phdthesis{mirzasoleiman2015dissertation,
	author = {Baharan Mirzasoleiman},
	school = {ETH Zurich},
	title = {Big Data Summarization Using Submodular Functions},
	year = {2017}}


@article{calder_consistency_2019,
	journal = {SIAM J. on Mathematics of Data Science},
	
  year = {2019},
  month = jan,
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {1},
  number = {4},
  pages = {780--812},
  author = {Jeff Calder},
  title = {Consistency of {Lipschitz} Learning with Infinite Unlabeled Data and Finite Labeled Data}
}
%doi = {10.1137/18m1199241},


@inproceedings{welling2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Welling, Max and Kipf, Thomas N},
  booktitle={J. International Conference on Learning Representations (ICLR 2017)},
  year={2016}
}

@article{parulekar2023infonce,
  title={InfoNCE Loss Provably Learns Cluster-Preserving Representations},
  author={Parulekar, Advait and Collins, Liam and Shanmugam, Karthikeyan and Mokhtari, Aryan and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2302.07920},
  year={2023}
}

@article{lv2021generalization,
  title={Generalization bounds for graph convolutional neural networks via rademacher complexity},
  author={Lv, Shaogao},
  journal={arXiv preprint arXiv:2102.10234},
  year={2021}
}

@article{wei2019data,
  title={Data-dependent sample complexity of deep neural networks via lipschitz augmentation},
  author={Wei, Colin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018},
  organization={PMLR}
}

@inproceedings{yang2023sample,
  title={Sample efficiency of data augmentation consistency regularization},
  author={Yang, Shuo and Dong, Yijun and Ward, Rachel and Dhillon, Inderjit S and Sanghavi, Sujay and Lei, Qi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3825--3853},
  year={2023},
  organization={PMLR}
}

@inproceedings{wei2021theoretical,
  title={Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
  author={Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{cai2021theory,
  title={A theory of label propagation for subpopulation shift},
  author={Cai, Tianle and Gao, Ruiqi and Lee, Jason and Lei, Qi},
  booktitle={International Conference on Machine Learning},
  pages={1170--1182},
  year={2021},
  organization={PMLR}
}

@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{bilmes-submodularity-in-ai-ml-2022,
  author    = {Jeffrey Bilmes},
  title     = {{Submodularity In Machine Learning and Artificial Intelligence}},
  month = {Jan},
  archivePrefix = {arXiv},
  arxivId = {2202.00132},
  journal   = {Arxiv},
  volume    = {abs/2202.00132},
  year      = {2022},
}


@incollection{krause14survey,
	author = {Andreas Krause and Daniel Golovin},
	booktitle = {Tractability: Practical Approaches to Hard Problems},
	month = {February},
	publisher = {Cambridge University Press},
	title = {Submodular Function Maximization},
	year = {2014}}


@inproceedings{mirzasoleiman2015lazier,
  title={Lazier than lazy greedy},
  author={Mirzasoleiman, Baharan and Badanidiyuru, Ashwinkumar and Karbasi, Amin and Vondr{\'a}k, Jan and Krause, Andreas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  number={1},
  year={2015}
}

@article{apricot2020,
author = {Schreiber, Jacob and Bilmes, Jeffrey and Noble, William Stafford},
title = {Apricot: Submodular Selection for Data Summarization in Python},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {We present apricot, an open source Python package for selecting representative subsets from large data sets using submodular optimization. The package implements several efficient greedy selection algorithms that offer strong theoretical guarantees on the quality of the selected set. Additionally, several submodular set functions are implemented, including facility location, which is broadly applicable but requires memory quadratic in the number of examples in the data set, and a feature-based function that is less broadly applicable but can scale to millions of examples. Apricot is extremely efficient, using both algorithmic speedups such as the lazy greedy algorithm and memoization as well as code optimization using numba. We demonstrate the use of subset selection by training machine learning models to comparable accuracy using either the full data set or a representative subset thereof. This paper presents an explanation of submodular selection, an overview of the features in apricot, and applications to two data sets. The code and tutorial Jupyter notebooks are available at https://github.com/jmschrei/apricot},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {161},
numpages = {6},
keywords = {big data, machine learning, submodularity, submodular selection, subset selection}
}

@article{ji2020knowledge,
  title={Knowledge distillation in wide neural networks: Risk bound, data efficiency and imperfect teacher},
  author={Ji, Guangda and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20823--20833},
  year={2020}
}

@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}

@article{harutyunyan2023supervision,
  title={Supervision Complexity and its Role in Knowledge Distillation},
  author={Harutyunyan, Hrayr and Rawat, Ankit Singh and Menon, Aditya Krishna and Kim, Seungyeon and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2301.12245},
  year={2023}
}

@article{caron2020unsupervised,
  title={Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15750--15758},
  year={2021}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@article{ma2022distilling,
  title={Distilling Knowledge from Self-Supervised Teacher by Embedding Graph Alignment},
  author={Ma, Yuchen and Chen, Yanbei and Akata, Zeynep},
  journal={arXiv preprint arXiv:2211.13264},
  year={2022}
}

@article{bachman2014learning,
  title={Learning with pseudo-ensembles},
  author={Bachman, Philip and Alsharif, Ouais and Precup, Doina},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{laine2016temporal,
  title={Temporal ensembling for semi-supervised learning},
  author={Laine, Samuli and Aila, Timo},
  journal={arXiv preprint arXiv:1610.02242},
  year={2016}
}

@article{sohn2020fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={596--608},
  year={2020}
}

@article{krizhevsky2017imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@incollection{simard2002transformation,
  title={Transformation invariance in pattern recognition—tangent distance and tangent propagation},
  author={Simard, Patrice Y and LeCun, Yann A and Denker, John S and Victorri, Bernard},
  booktitle={Neural networks: tricks of the trade},
  pages={239--274},
  year={2002},
  publisher={Springer}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}


@article{chen2020group,
  title={A group-theoretic framework for data augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={9885--9955},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{mei2021learning,
  title={Learning with invariances in random features and kernel models},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={3351--3418},
  year={2021},
  organization={PMLR}
}

@inproceedings{shen2022data,
  title={Data augmentation as feature manipulation},
  author={Shen, Ruoqi and Bubeck, Sebastien and Gunasekar, Suriya},
  booktitle={International Conference on Machine Learning},
  pages={19773--19808},
  year={2022},
  organization={PMLR}
}





@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@Techreport{cifar,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
 address = {Toronto, Ontario},
 institution = {University of Toronto},
 number = {0},
 publisher = {Technical report, University of Toronto},
 title = {Learning multiple layers of features from tiny images},
 year = {2009},
 title_with_no_special_chars = {Learning multiple layers of features from tiny images}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}