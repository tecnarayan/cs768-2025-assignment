\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, {Allen Zhu}, Bullins, Hazan, and
  Ma]{DBLP:conf/stoc/AgarwalZBHM17}
Naman Agarwal, Zeyuan {Allen Zhu}, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima faster than gradient descent.
\newblock In \emph{STOC}, pages 1195--1199, 2017.

\bibitem[Allen-Zhu(2017)]{natasha2}
Zeyuan Allen-Zhu.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock \emph{CoRR}, /abs/1708.08694, 2017.

\bibitem[Balcan et~al.(2016)Balcan, Du, Wang, and
  Yu]{DBLP:conf/colt/BalcanDWY16}
Maria{-}Florina Balcan, Simon~Shaolei Du, Yining Wang, and Adams~Wei Yu.
\newblock An improved gap-dependency analysis of the noisy power method.
\newblock In \emph{{COLT}}, pages 284--309, 2016.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and
  Sidford]{DBLP:journals/corr/CarmonDHS16}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for non-convex optimization.
\newblock \emph{CoRR}, abs/1611.00756, 2016.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{DBLP:conf/icml/CarmonDHS17}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock "convex until proven guilty": Dimension-free acceleration of gradient
  descent on non-convex functions.
\newblock In \emph{ICML}, pages 654--663, 2017.

\bibitem[Cartis et~al.(2011{\natexlab{a}})Cartis, Gould, and Toint]{Cartis2011}
Coralia Cartis, Nicholas I.~M. Gould, and Philippe~L. Toint.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part i: motivation, convergence and numerical results.
\newblock \emph{Math. Program.}, 127\penalty0 (2):\penalty0 245--295, Apr
  2011{\natexlab{a}}.

\bibitem[Cartis et~al.(2011{\natexlab{b}})Cartis, Gould, and
  Toint]{Cartis2011b}
Coralia Cartis, Nicholas I.~M. Gould, and Philippe~L. Toint.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part ii: worst-case function- and derivative-evaluation complexity.
\newblock \emph{Math. Program.}, 130\penalty0 (2):\penalty0 295--319, Dec
  2011{\natexlab{b}}.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{pmlr-v40-Ge15}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points --- online stochastic gradient for tensor
  decomposition.
\newblock In \emph{COLT}, pages 797--842, 2015.

\bibitem[Ghadimi and Lan(2013)]{DBLP:journals/siamjo/GhadimiL13a}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi and Lan(2016)]{DBLP:journals/mp/GhadimiL16}
Saeed Ghadimi and Guanghui Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock \emph{Math. Program.}, 156\penalty0 (1-2):\penalty0 59--99, 2016.

\bibitem[Ghadimi et~al.(2016)Ghadimi, Lan, and
  Zhang]{DBLP:journals/mp/GhadimiLZ16}
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock \emph{Math. Program.}, 155\penalty0 (1-2):\penalty0 267--305, 2016.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Hardt and Price(2014)]{DBLP:conf/nips/HardtP14}
Moritz Hardt and Eric Price.
\newblock The noisy power method: {A} meta algorithm with applications.
\newblock In \emph{{NIPS}}, pages 2861--2869, 2014.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and
  Shalev{-}Shwartz]{DBLP:conf/icml/HazanLS16}
Elad Hazan, Kfir~Yehuda Levy, and Shai Shalev{-}Shwartz.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{ICML}, pages 1833--1841, 2016.

\bibitem[Jin et~al.(2017{\natexlab{a}})Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{ICML}, pages 1724--1732, 2017{\natexlab{a}}.

\bibitem[Jin et~al.(2017{\natexlab{b}})Jin, Netrapalli, and Jordan]{AGNON}
Chi Jin, Praneeth Netrapalli, and Michael~I. Jordan.
\newblock Accelerated gradient descent escapes saddle points faster than
  gradient descent.
\newblock \emph{CoRR}, abs/1711.10456, 2017{\natexlab{b}}.

\bibitem[Kohler and Lucchi(2017)]{DBLP:conf/icml/KohlerL17}
Jonas~Moritz Kohler and Aur{\'{e}}lien Lucchi.
\newblock Sub-sampled cubic regularization for non-convex optimization.
\newblock In \emph{ICML}, pages 1895--1904, 2017.

\bibitem[Kuczynski and Wozniakowski(1992)]{doi:10.1137/0613066}
J.~Kuczynski and H.~Wozniakowski.
\newblock Estimating the largest eigenvalue by the power and lanczos algorithms
  with a random start.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 13\penalty0
  (4):\penalty0 1094--1122, 1992.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and
  Jordan]{DBLP:journals/corr/LeiJCJ17}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{NIPS}, pages 2345--2355, 2017.

\bibitem[Levy(2016)]{DBLP:journals/corr/Levy16a}
Kfir~Y. Levy.
\newblock The power of normalization: Faster evasion of saddle points.
\newblock \emph{CoRR}, abs/1611.04831, 2016.

\bibitem[Li and Lin(2015)]{Li:2015:APG:2969239.2969282}
Huan Li and Zhouchen Lin.
\newblock Accelerated proximal gradient methods for nonconvex programming.
\newblock In \emph{NIPS}, pages 379--387, 2015.

\bibitem[Liu and Yang(2017{\natexlab{a}})]{DBLP:journals/corr/SNCG}
Mingrui Liu and Tianbao Yang.
\newblock Stochastic non-convex optimization with strong high probability
  second-order convergence.
\newblock \emph{CoRR}, abs/1710.09447, 2017{\natexlab{a}}.

\bibitem[Liu and Yang(2017{\natexlab{b}})]{DBLP:journals/corr/noisynegative}
Mingrui Liu and Tianbao Yang.
\newblock On noisy negative curvature descent: Competing with gradient descent
  for faster non-convex optimization.
\newblock \emph{CoRR}, abs/1709.08571, 2017{\natexlab{b}}.

\bibitem[Martens(2010)]{conf/icml/Martens10}
James Martens.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{ICML}, pages 735--742, 2010.

\bibitem[Nesterov(2004)]{opac-b1104789}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization : a basic course}.
\newblock Applied optimization. Kluwer Academic Publ., 2004.
\newblock ISBN 1-4020-7553-7.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Math. Program.}, 108\penalty0 (1):\penalty0 177--205, 2006.

\bibitem[O'Neill and Wright(2017)]{corrACGWright}
Michael O'Neill and Stephen~J. Wright.
\newblock Behavior of accelerated gradient methods near critical points of
  nonconvex problems.
\newblock \emph{CoRR}, abs/1706.07993, 2017.

\bibitem[Reddi et~al.(2017)Reddi, Zaheer, Sra, Poczos, Bach, Salakhutdinov, and
  Smola]{reddi2017generic}
Sashank~J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach,
  Ruslan Salakhutdinov, and Alexander~J Smola.
\newblock A generic approach for escaping saddle points.
\newblock \emph{arXiv preprint arXiv:1709.01434}, 2017.

\bibitem[Royer and Wright(2017)]{clement17}
Clement~W. Royer and Stephen~J. Wright.
\newblock Complexity analysis of second-order line-search algorithms for smooth
  nonconvex optimization.
\newblock \emph{CoRR}, abs/1706.03131, 2017.

\bibitem[Xu et~al.(2017)Xu, Roosta-Khorasani, and
  Mahoney]{peng16inexacthessian}
Peng Xu, Farbod Roosta-Khorasani, and Michael~W. Mahoney.
\newblock Newton-type methods for non-convex optimization under inexact hessian
  information.
\newblock \emph{CoRR}, abs/1708.07164, 2017.

\bibitem[Yang et~al.(2016)Yang, Lin, and Li]{yangnonconvexmo}
Tianbao Yang, Qihang Lin, and Zhe Li.
\newblock Unified convergence analysis of stochastic momentum methods for
  convex and non-convex optimization.
\newblock volume abs/1604.03257, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Liang, and
  Charikar]{DBLP:conf/colt/ZhangLC17}
Yuchen Zhang, Percy Liang, and Moses Charikar.
\newblock A hitting time analysis of stochastic gradient langevin dynamics.
\newblock In \emph{COLT}, pages 1980--2022, 2017.

\end{thebibliography}
