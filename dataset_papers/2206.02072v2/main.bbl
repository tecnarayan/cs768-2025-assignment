\begin{thebibliography}{170}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abachi et~al.(2020)Abachi, Ghavamzadeh, and
  Farahmand]{abachi2020policy}
Romina Abachi, Mohammad Ghavamzadeh, and Amir-massoud Farahmand.
\newblock Policy-aware model learning for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2003.00030}, 2020.

\bibitem[Abbasi-Yadkori and Szepesvari(2014)]{abbasi2014bayesian}
Yasin Abbasi-Yadkori and Csaba Szepesvari.
\newblock Bayesian optimal control of smoothly parameterized systems: The lazy
  posterior sampling algorithm.
\newblock \emph{arXiv preprint arXiv:1406.3926}, 2014.

\bibitem[Abel(2020)]{abel2020thesis}
David Abel.
\newblock \emph{A Theory of Abstraction in Reinforcement Learning}.
\newblock PhD thesis, Brown University, 2020.

\bibitem[Abel et~al.(2016)Abel, Hershkowitz, and Littman]{abel2016near}
David Abel, David Hershkowitz, and Michael Littman.
\newblock Near optimal behavior via approximate state abstraction.
\newblock In \emph{International Conference on Machine Learning}, pages
  2915--2923. PMLR, 2016.

\bibitem[Abel et~al.(2018)Abel, Arumugam, Lehnert, and Littman]{abel2018state}
David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman.
\newblock State abstractions for lifelong reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 10--19,
  2018.

\bibitem[Abel et~al.(2019)Abel, Arumugam, Asadi, Jinnai, Littman, and
  Wong]{abel2019state}
David Abel, Dilip Arumugam, Kavosh Asadi, Yuu Jinnai, Michael~L Littman, and
  Lawson~LS Wong.
\newblock State abstraction as compression in apprenticeship learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3134--3142, 2019.

\bibitem[Abel et~al.(2020)Abel, Umbanhowar, Khetarpal, Arumugam, Precup, and
  Littman]{abel2020value}
David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam, Doina Precup,
  and Michael Littman.
\newblock Value preserving state-action abstractions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1639--1650. PMLR, 2020.

\bibitem[Aeron et~al.(2010)Aeron, Saligrama, and Zhao]{aeron2010information}
Shuchin Aeron, Venkatesh Saligrama, and Manqi Zhao.
\newblock Information theoretic bounds for compressed sensing.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (10):\penalty0 5111--5130, 2010.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock {FLAMBE}: Structural complexity and representation learning of low
  rank {MDP}s.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 20095--20107. Curran Associates, Inc., 2020.

\bibitem[Agrawal and Jia(2017)]{agrawal2017optimistic}
Shipra Agrawal and Randy Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1184--1194, 2017.

\bibitem[Alemi et~al.(2018)Alemi, Poole, Fischer, Dillon, Saurous, and
  Murphy]{alemi2018fixing}
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif~A Saurous, and
  Kevin Murphy.
\newblock Fixing a broken {ELBO}.
\newblock In \emph{International Conference on Machine Learning}, pages
  159--168. PMLR, 2018.

\bibitem[Araya-L{\'o}pez et~al.(2012)Araya-L{\'o}pez, Thomas, and
  Buffet]{araya2012near}
Mauricio Araya-L{\'o}pez, Vincent Thomas, and Olivier Buffet.
\newblock Near-optimal {BRL} using optimistic local transitions.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning}, pages 515--522, 2012.

\bibitem[Arimoto(1972)]{arimoto1972algorithm}
Suguru Arimoto.
\newblock An algorithm for computing the capacity of arbitrary discrete
  memoryless channels.
\newblock \emph{IEEE Transactions on Information Theory}, 18\penalty0
  (1):\penalty0 14--20, 1972.

\bibitem[Arumugam and Van~Roy(2020)]{arumugam2020randomized}
Dilip Arumugam and Benjamin Van~Roy.
\newblock Randomized value functions via posterior state-abstraction sampling.
\newblock \emph{arXiv preprint arXiv:2010.02383}, 2020.

\bibitem[Arumugam and Van~Roy(2021{\natexlab{a}})]{arumugam2021deciding}
Dilip Arumugam and Benjamin Van~Roy.
\newblock Deciding what to learn: {A} rate-distortion approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  373--382. PMLR, 2021{\natexlab{a}}.

\bibitem[Arumugam and Van~Roy(2021{\natexlab{b}})]{arumugam2021the}
Dilip Arumugam and Benjamin Van~Roy.
\newblock The value of information when deciding what to learn.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Asadi et~al.(2018)Asadi, Misra, and Littman]{asadi2018lipschitz}
Kavosh Asadi, Dipendra Misra, and Michael Littman.
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  264--273. PMLR, 2018.

\bibitem[Asmuth et~al.(2009)Asmuth, Li, Littman, Nouri, and
  Wingate]{asmuth2009bayesian}
John Asmuth, Lihong Li, Michael~L Littman, Ali Nouri, and David Wingate.
\newblock A {B}ayesian sampling approach to exploration in reinforcement
  learning.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pages 19--26, 2009.

\bibitem[{\AA}str{\"o}m(1965)]{aastrom1965optimal}
Karl~Johan {\AA}str{\"o}m.
\newblock Optimal control of {M}arkov processes with incomplete state
  information.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 10\penalty0
  (1):\penalty0 174--205, 1965.

\bibitem[Auer et~al.(2009)Auer, Jaksch, and Ortner]{auer2009near}
Peter Auer, Thomas Jaksch, and Ronald Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  89--96, 2009.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Bartlett and Tewari(2009)]{bartlett2009regal}
Peter~L Bartlett and Ambuj Tewari.
\newblock {REGAL}: a regularization based algorithm for reinforcement learning
  in weakly communicating {MDP}s.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pages 35--42, 2009.

\bibitem[Bellman(1957)]{bellman1957markovian}
Richard Bellman.
\newblock A {M}arkovian decision process.
\newblock \emph{Journal of Mathematics and Mechanics}, pages 679--684, 1957.

\bibitem[Berger(1971)]{berger1971rate}
Toby Berger.
\newblock \emph{{Rate Distortion Theory: A Mathematical Basis for Data
  Compression}}.
\newblock Prentice-Hall, 1971.

\bibitem[Berger and Gibson(1998)]{berger1998lossy}
Toby Berger and Jerry~D Gibson.
\newblock Lossy source coding.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0
  (6):\penalty0 2693--2723, 1998.

\bibitem[Berry et~al.(1997)Berry, Chen, Zame, Heath, and Shepp]{berry1997}
Donald~A. Berry, Robert~W. Chen, Alan Zame, David~C. Heath, and Larry~A. Shepp.
\newblock Bandit problems with infinitely many arms.
\newblock \emph{Ann. Statist.}, 25\penalty0 (5):\penalty0 2103--2116, 10 1997.

\bibitem[Bertsekas and Shreve(1996)]{bertsekas1996stochastic}
Dimitri Bertsekas and Steven~E Shreve.
\newblock \emph{Stochastic optimal control: the discrete-time case}, volume~5.
\newblock Athena Scientific, 1996.

\bibitem[Bertsekas(1995)]{bertsekas1995dynamic}
Dimitri~P. Bertsekas.
\newblock \emph{Dynamic Programming and Optimal Control}.
\newblock Athena Scientific, 1995.

\bibitem[Bertsekas and Casta{\~n}on(1989)]{bertsekas1989adaptive}
Dimitri~P. Bertsekas and David~A. Casta{\~n}on.
\newblock Adaptive aggregation methods for infinite horizon dynamic
  programming.
\newblock \emph{IEEE Transactions on Automatic Control}, 34\penalty0
  (6):\penalty0 589--598, 1989.

\bibitem[Blahut(1972)]{blahut1972computation}
Richard Blahut.
\newblock Computation of channel capacity and rate-distortion functions.
\newblock \emph{IEEE Transactions on Information Theory}, 18\penalty0
  (4):\penalty0 460--473, 1972.

\bibitem[Bonald and Proutiere(2013)]{bonald2013two}
Thomas Bonald and Alexandre Proutiere.
\newblock Two-target algorithms for infinite-armed bandits with {B}ernoulli
  rewards.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2184--2192, 2013.

\bibitem[Borkar et~al.(2001)Borkar, Mitter, and Tatikonda]{borkar2001markov}
Vivek~S Borkar, Sanjoy Mitter, and Sekhar Tatikonda.
\newblock Markov control problems under communication contraints.
\newblock \emph{Communications in Information and Systems}, 1\penalty0
  (1):\penalty0 15--32, 2001.

\bibitem[Boukris(1973)]{boukris1973upper}
Pinhas Boukris.
\newblock An upper bound on the speed of convergence of the {B}lahut algorithm
  for computing rate-distortion functions (corresp.).
\newblock \emph{IEEE Transactions on Information Theory}, 19\penalty0
  (5):\penalty0 708--709, 1973.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock {R-MAX}-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Bubeck and Sellke(2020)]{bubeck2020first}
S{\'e}bastien Bubeck and Mark Sellke.
\newblock First-order {B}ayesian regret analysis of {T}hompson sampling.
\newblock In \emph{Algorithmic Learning Theory}, pages 196--233. PMLR, 2020.

\bibitem[Bubeck et~al.(2011)Bubeck, Munos, Stoltz, and
  Szepesv{\'a}ri]{bubeck2011x}
S{\'e}bastien Bubeck, R{\'e}mi Munos, Gilles Stoltz, and Csaba Szepesv{\'a}ri.
\newblock X-armed bandits.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (5), 2011.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, et~al.]{bubeck2012regret}
S{\'e}bastien Bubeck, Nicol{\`o} Cesa-Bianchi, et~al.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  5\penalty0 (1):\penalty0 1--122, 2012.

\bibitem[Castro and Precup(2007)]{castro2007using}
Pablo~Samuel Castro and Doina Precup.
\newblock Using linear programming for {B}ayesian exploration in {M}arkov
  decision processes.
\newblock In \emph{IJCAI}, volume 24372442, 2007.

\bibitem[Chiang and Boyd(2004)]{chiang2004geometric}
Mung Chiang and Stephen Boyd.
\newblock Geometric programming duals of channel capacity and rate distortion.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (2):\penalty0 245--258, 2004.

\bibitem[Cover and Thomas(2012)]{cover2012elements}
Thomas~M Cover and Joy~A Thomas.
\newblock \emph{Elements of {I}nformation {T}heory}.
\newblock John Wiley \& Sons, 2012.

\bibitem[Csisz{\'a}r(1974{\natexlab{a}})]{csiszar1974computation}
Imre Csisz{\'a}r.
\newblock On the computation of rate-distortion functions (corresp.).
\newblock \emph{IEEE Transactions on Information Theory}, 20\penalty0
  (1):\penalty0 122--124, 1974{\natexlab{a}}.

\bibitem[Csisz{\'a}r(1974{\natexlab{b}})]{csiszar1974extremum}
Imre Csisz{\'a}r.
\newblock On an extremum problem of information theory.
\newblock \emph{Studia Scientiarum Mathematicarum Hungarica}, 9,
  1974{\natexlab{b}}.

\bibitem[Cui et~al.(2020)Cui, Chow, and Ghavamzadeh]{cui2020control}
Brandon Cui, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock Control-aware representations for model-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying {PAC} and regret: uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 5717--5727, 2017.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{dann2018oracle}
Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford,
  and Robert~E Schapire.
\newblock On oracle-efficient {PAC} {RL} with rich observations.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 1429--1439, 2018.

\bibitem[Dauwels(2005)]{dauwels2005numerical}
Justin Dauwels.
\newblock Numerical computation of the capacity of continuous memoryless
  channels.
\newblock In \emph{Proceedings of the 26th Symposium on Information Theory in
  the BENELUX}, pages 221--228. Citeseer, 2005.

\bibitem[Dayan(1993)]{dayan1993improving}
Peter Dayan.
\newblock Improving generalization for temporal difference learning: {T}he
  successor representation.
\newblock \emph{Neural computation}, 5\penalty0 (4):\penalty0 613--624, 1993.

\bibitem[Dean and Givan(1997)]{dean1997model}
Thomas Dean and Robert Givan.
\newblock Model minimization in {M}arkov decision processes.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 106--111. AAAI Press, 1997.

\bibitem[Dearden et~al.(1998)Dearden, Friedman, and
  Russell]{dearden1998bayesian}
Richard Dearden, Nir Friedman, and Stuart Russell.
\newblock {B}ayesian {Q}-learning.
\newblock In \emph{Proceedings of the Fifteenth National/Tenth Conference on
  Artificial Intelligence/Innovative Applications of Artificial Intelligence},
  pages 761--768, 1998.

\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{dearden1999model}
Richard Dearden, Nir Friedman, and David Andre.
\newblock Model based {B}ayesian exploration.
\newblock In \emph{Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence}, pages 150--159, 1999.

\bibitem[Deshpande and Montanari(2012)]{deshpande2012linear}
Yash Deshpande and Andrea Montanari.
\newblock Linear bandits in high dimension and recommendation systems.
\newblock In \emph{2012 50th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 1750--1754. IEEE, 2012.

\bibitem[Dong and Van~Roy(2018)]{dong2018information}
Shi Dong and Benjamin Van~Roy.
\newblock An information-theoretic analysis for {T}hompson sampling with many
  actions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4157--4165, 2018.

\bibitem[Dong et~al.(2019)Dong, Van~Roy, and Zhou]{dong2019provably}
Shi Dong, Benjamin Van~Roy, and Zhengyuan Zhou.
\newblock Provably efficient reinforcement learning with aggregated states.
\newblock \emph{arXiv preprint arXiv:1912.06366}, 2019.

\bibitem[Dong et~al.(2021)Dong, Van~Roy, and Zhou]{dong2021simple}
Shi Dong, Benjamin Van~Roy, and Zhengyuan Zhou.
\newblock Simple agent, complex environment: Efficient reinforcement learning
  with agent state.
\newblock \emph{arXiv preprint arXiv:2102.05261}, 2021.

\bibitem[D'Oro et~al.(2020)D'Oro, Metelli, Tirinzoni, Papini, and
  Restelli]{d2020gradient}
Pierluca D'Oro, Alberto~Maria Metelli, Andrea Tirinzoni, Matteo Papini, and
  Marcello Restelli.
\newblock Gradient-aware model-based policy search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 3801--3808, 2020.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019provably}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and
  John Langford.
\newblock Provably efficient {RL} with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pages
  1665--1674. PMLR, 2019.

\bibitem[Duchi(2021)]{duchi21ItLectNotes}
John~C. Duchi.
\newblock \emph{Lecture Notes for {S}tatistics 311/{E}lectrical {E}ngineering
  377, {S}tanford University.}
\newblock 2021.

\bibitem[Duchi and Wainwright(2013)]{duchi2013distance}
John~C. Duchi and Martin~J. Wainwright.
\newblock Distance-based and continuum {F}ano inequalities with applications to
  statistical estimation.
\newblock \emph{arXiv preprint arXiv:1311.2669}, 2013.

\bibitem[Duff(2002)]{duff2002optimal}
Michael~O'Gordon Duff.
\newblock \emph{Optimal Learning: Computational procedures for {B}ayes-adaptive
  {M}arkov decision processes}.
\newblock University of Massachusetts Amherst, 2002.

\bibitem[Dulac-Arnold et~al.(2021)Dulac-Arnold, Levine, Mankowitz, Li,
  Paduraru, Gowal, and Hester]{dulac2021challenges}
Gabriel Dulac-Arnold, Nir Levine, Daniel~J Mankowitz, Jerry Li, Cosmin
  Paduraru, Sven Gowal, and Todd Hester.
\newblock Challenges of real-world reinforcement learning: definitions,
  benchmarks and analysis.
\newblock \emph{Machine Learning}, pages 1--50, 2021.

\bibitem[Dwaracherla et~al.(2020)Dwaracherla, Lu, Ibrahimi, Osband, Wen, and
  Van~Roy]{dwaracherla2020hypermodels}
Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, and
  Benjamin Van~Roy.
\newblock Hypermodels for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Fano(1952)]{fano1952TransInfoLectNotes}
Robert~M. Fano.
\newblock \emph{Class Notes for {MIT} Course 6.574: Transmission of
  Information, MIT, Cambridge, MA.}
\newblock 1952.

\bibitem[Farahmand(2018)]{farahmand2018iterative}
Amir-massoud Farahmand.
\newblock Iterative value-aware model learning.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 9090--9101, 2018.

\bibitem[Farahmand et~al.(2017)Farahmand, Barreto, and
  Nikovski]{farahmand2017value}
Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski.
\newblock Value-aware loss function for model-based reinforcement learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1486--1494.
  PMLR, 2017.

\bibitem[Ferns et~al.(2004)Ferns, Panangaden, and Precup]{ferns2004metrics}
Norm Ferns, Prakash Panangaden, and Doina Precup.
\newblock Metrics for finite {M}arkov {D}ecision {P}rocesses.
\newblock In \emph{Proceedings of the 20th Conference on Uncertainty in
  Artificial Intelligence}, pages 162--169, 2004.

\bibitem[Ferns et~al.(2012)Ferns, Castro, Precup, and
  Panangaden]{ferns2012methods}
Norman Ferns, Pablo~Samuel Castro, Doina Precup, and Prakash Panangaden.
\newblock Methods for computing state similarity in {M}arkov {D}ecision
  {P}rocesses.
\newblock \emph{arXiv preprint arXiv:1206.6836}, 2012.

\bibitem[Flennerhag et~al.(2020)Flennerhag, Wang, Sprechmann, Visin, Galashov,
  Kapturowski, Borsa, Heess, Barreto, and Pascanu]{flennerhag2020temporal}
Sebastian Flennerhag, Jane~X Wang, Pablo Sprechmann, Francesco Visin, Alexandre
  Galashov, Steven Kapturowski, Diana~L Borsa, Nicolas Heess, Andre Barreto,
  and Razvan Pascanu.
\newblock Temporal difference uncertainties as a signal for exploration.
\newblock \emph{arXiv preprint arXiv:2010.02255}, 2020.

\bibitem[Ghavamzadeh et~al.(2015)Ghavamzadeh, Mannor, Pineau, and
  Tamar]{ghavamzadeh2015bayesian}
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar.
\newblock {B}ayesian reinforcement learning: A survey.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (5-6):\penalty0 359--483, 2015.

\bibitem[Gray(2011)]{gray2011entropy}
Robert~M. Gray.
\newblock \emph{Entropy and information theory}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Grimm et~al.(2020)Grimm, Barreto, Singh, and Silver]{grimm2020value}
Christopher Grimm, Andre Barreto, Satinder Singh, and David Silver.
\newblock The value equivalence principle for model-based reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Grimm et~al.(2021)Grimm, Barreto, Farquhar, Silver, and
  Singh]{grimm2021proper}
Christopher Grimm, Andr{\'e} Barreto, Greg Farquhar, David Silver, and Satinder
  Singh.
\newblock Proper value equivalence.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Guez et~al.(2012)Guez, Silver, and Dayan]{guez2012efficient}
Arthur Guez, David Silver, and Peter Dayan.
\newblock Efficient {B}ayes-adaptive reinforcement learning using sample-based
  search.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 1025--1033, 2012.

\bibitem[Guez et~al.(2013)Guez, Silver, and Dayan]{guez2013scalable}
Arthur Guez, David Silver, and Peter Dayan.
\newblock Scalable and efficient {B}ayes-adaptive reinforcement learning based
  on {M}onte-{C}arlo tree search.
\newblock \emph{Journal of Artificial Intelligence Research}, 48:\penalty0
  841--883, 2013.

\bibitem[Guez et~al.(2014)Guez, Heess, Silver, and Dayan]{guez2014bayes}
Arthur Guez, Nicolas Heess, David Silver, and Peter Dayan.
\newblock {B}ayes-adaptive simulation-based search with value function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  451--459, 2014.

\bibitem[Hallak et~al.(2015)Hallak, Di~Castro, and
  Mannor]{hallak2015contextual}
Assaf Hallak, Dotan Di~Castro, and Shie Mannor.
\newblock Contextual {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:1502.02259}, 2015.

\bibitem[Hao and Lattimore(2022)]{hao2022regret}
Botao Hao and Tor Lattimore.
\newblock Regret bounds for information-directed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2206.04640}, 2022.

\bibitem[Harrison and Kontoyiannis(2008)]{harrison2008estimation}
Matthew~T Harrison and Ioannis Kontoyiannis.
\newblock Estimation of the rate--distortion function.
\newblock \emph{IEEE Transactions on Information Theory}, 54\penalty0
  (8):\penalty0 3757--3762, 2008.

\bibitem[Ionescu-Tulcea(1949)]{tulcea1949mesures}
Cassius~T. Ionescu-Tulcea.
\newblock Mesures dans les espaces produits.
\newblock \emph{Atti Acad. Naz. Lincei Rend. Cl Sci. Fis. Mat. Nat}, 8\penalty0
  (7), 1949.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Janz et~al.(2019)Janz, Hron, Mazur, Hofmann, Hern{\'a}ndez-Lobato, and
  Tschiatschek]{janz2019successor}
David Janz, Jiri Hron, Przemys{\l}aw Mazur, Katja Hofmann, Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato, and Sebastian Tschiatschek.
\newblock Successor uncertainties: exploration and uncertainty in temporal
  difference learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, and Singh]{jiang2015abstraction}
Nan Jiang, Alex Kulesza, and Satinder Singh.
\newblock Abstraction selection in model-based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  179--188, 2015.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {$Q$}-learning provably efficient?
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 4868--4878, 2018.

\bibitem[Jong and Stone(2005)]{jong2005state}
Nicholas~K Jong and Peter Stone.
\newblock State abstraction discovery from irrelevant state variables.
\newblock In \emph{Proceedings of the 19th International Joint Conference on
  Artificial Intelligence}, pages 752--757, 2005.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{kaelbling1996reinforcement}
Leslie~Pack Kaelbling, Michael~L Littman, and Andrew~W Moore.
\newblock Reinforcement learning: A survey.
\newblock \emph{Journal of Artificial Intelligence Research}, 4:\penalty0
  237--285, 1996.

\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and
  Cassandra]{kaelbling1998planning}
Leslie~Pack Kaelbling, Michael~L Littman, and Anthony~R Cassandra.
\newblock Planning and acting in partially observable stochastic domains.
\newblock \emph{Artificial intelligence}, 101\penalty0 (1-2):\penalty0 99--134,
  1998.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{Proceedings of the Nineteenth International Conference on
  Machine Learning}, pages 267--274, 2002.

\bibitem[Kakade(2003)]{kakade2003sample}
Sham~Machandranath Kakade.
\newblock \emph{On the {S}ample {C}omplexity of {R}einforcement {L}earning}.
\newblock PhD thesis, Gatsby Computational Neuroscience Unit, University
  College London, 2003.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Kleinberg et~al.(2008)Kleinberg, Slivkins, and
  Upfal]{kleinberg2008multi}
Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal.
\newblock Multi-armed bandits in metric spaces.
\newblock In \emph{Proceedings of the 40th Annual ACM Symposium on Theory of
  Computing}, pages 681--690, 2008.

\bibitem[Kolmogorov and Tikhomirov(1959)]{kolmogorov1959varepsilon}
Andrei~Nikolaevich Kolmogorov and Vladimir~Mikhailovich Tikhomirov.
\newblock $\varepsilon$-entropy and $\varepsilon$-capacity of sets in function
  spaces.
\newblock \emph{Uspekhi Matematicheskikh Nauk}, 14\penalty0 (2):\penalty0
  3--86, 1959.

\bibitem[Kolter and Ng(2009)]{kolter2009near}
J~Zico Kolter and Andrew~Y Ng.
\newblock Near-{B}ayesian exploration in polynomial time.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  machine Learning}, pages 513--520, 2009.

\bibitem[Kostina and Hassibi(2019)]{kostina2019rate}
Victoria Kostina and Babak Hassibi.
\newblock Rate-cost tradeoffs in control.
\newblock \emph{IEEE Transactions on Automatic Control}, 64\penalty0
  (11):\penalty0 4525--4540, 2019.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock {PAC} reinforcement learning with rich observations.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 1848--1856, 2016.

\bibitem[Lai and Robbins(1985)]{lai1985asymptotically}
Tze~Leung Lai and Herbert Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lattimore and Gyorgy(2021)]{lattimore2021mirror}
Tor Lattimore and Andras Gyorgy.
\newblock Mirror descent and the information ratio.
\newblock In \emph{Conference on Learning Theory}, pages 2965--2992. PMLR,
  2021.

\bibitem[Lattimore and Szepesv{\'a}ri(2019)]{lattimore2019information}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock An information-theoretic approach to minimax regret in partial
  monitoring.
\newblock In \emph{Conference on Learning Theory}, pages 2111--2139. PMLR,
  2019.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{li2006towards}
Lihong Li, Thomas~J Walsh, and Michael~L Littman.
\newblock Towards a unified theory of state abstraction for {MDP}s.
\newblock \emph{ISAIM}, 4:\penalty0 5, 2006.

\bibitem[Liu et~al.(2021)Liu, Raghunathan, Liang, and Finn]{liu2021decoupling}
Evan~Z Liu, Aditi Raghunathan, Percy Liang, and Chelsea Finn.
\newblock Decoupling exploration and exploitation for meta-reinforcement
  learning without sacrifices.
\newblock In \emph{International conference on machine learning}, pages
  6925--6935. PMLR, 2021.

\bibitem[Lu and Van~Roy(2017)]{lu2017ensemble}
Xiuyuan Lu and Benjamin Van~Roy.
\newblock Ensemble sampling.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 3260--3268, 2017.

\bibitem[Lu and Van~Roy(2019)]{lu2019information}
Xiuyuan Lu and Benjamin Van~Roy.
\newblock Information-theoretic confidence bounds for reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 2461--2470, 2019.

\bibitem[Lu et~al.(2021)Lu, Van~Roy, Dwaracherla, Ibrahimi, Osband, and
  Wen]{lu2021reinforcement}
Xiuyuan Lu, Benjamin Van~Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian
  Osband, and Zheng Wen.
\newblock {R}einforcement {L}earning, {B}it by {B}it.
\newblock \emph{arXiv preprint arXiv:2103.04047}, 2021.

\bibitem[Matz and Duhamel(2004)]{matz2004information}
Gerald Matz and Pierre Duhamel.
\newblock Information geometric formulation and interpretation of accelerated
  {B}lahut-{A}rimoto-type algorithms.
\newblock In \emph{Information theory workshop}, pages 66--70. IEEE, 2004.

\bibitem[Misra et~al.(2020)Misra, Henaff, Krishnamurthy, and
  Langford]{misra2020kinematic}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  6961--6971. PMLR, 2020.

\bibitem[Mitter and Sahai(1999)]{mitter1999information}
Sanjoy Mitter and Anant Sahai.
\newblock Information and control: {W}itsenhausen revisited.
\newblock In \emph{Learning, Control and Hybrid Systems}, pages 281--293.
  Springer, 1999.

\bibitem[Mitter(2001)]{mitter2001control}
Sanjoy~K Mitter.
\newblock Control with limited information.
\newblock \emph{European Journal of Control}, 7\penalty0 (2-3):\penalty0
  122--131, 2001.

\bibitem[Nair et~al.(2020)Nair, Savarese, and Finn]{nair2020goal}
Suraj Nair, Silvio Savarese, and Chelsea Finn.
\newblock Goal-aware prediction: Learning to model what matters.
\newblock In \emph{International Conference on Machine Learning}, pages
  7207--7219. PMLR, 2020.

\bibitem[Naja et~al.(2009)Naja, Alberge, and Duhamel]{naja2009geometrical}
Ziad Naja, Florence Alberge, and Pierre Duhamel.
\newblock Geometrical interpretation and improvements of the
  {B}lahut-{A}rimoto's algorithm.
\newblock In \emph{2009 IEEE International Conference on Acoustics, Speech and
  Signal Processing}, pages 2505--2508. IEEE, 2009.

\bibitem[Niesen et~al.(2007)Niesen, Shah, and Wornell]{niesen2007adaptive}
Urs Niesen, Devavrat Shah, and Gregory Wornell.
\newblock Adaptive alternating minimization algorithms.
\newblock In \emph{2007 IEEE International Symposium on Information Theory},
  pages 1641--1645. IEEE, 2007.

\bibitem[Nikishin et~al.(2022)Nikishin, Abachi, Agarwal, and
  Bacon]{nikishin2022control}
Evgenii Nikishin, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon.
\newblock Control-oriented model-based reinforcement learning with implicit
  differentiation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2022.

\bibitem[O'Donoghue et~al.(2018)O'Donoghue, Osband, Munos, and
  Mnih]{o2018uncertainty}
Brendan O'Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih.
\newblock The uncertainty {B}ellman equation and exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  3836--3845, 2018.

\bibitem[O'Donoghue et~al.(2020)O'Donoghue, Osband, and Ionescu]{o2020making}
Brendan O'Donoghue, Ian Osband, and Catalin Ionescu.
\newblock Making sense of reinforcement learning and probabilistic inference.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Oh et~al.(2017)Oh, Singh, and Lee]{oh2017value}
Junhyuk Oh, Satinder Singh, and Honglak Lee.
\newblock Value prediction network.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6120--6130, 2017.

\bibitem[Osband(2016)]{osband2016deepthesis}
Ian Osband.
\newblock \emph{Deep Exploration via Randomized Value Functions}.
\newblock PhD thesis, Stanford University, 2016.

\bibitem[Osband and Van~Roy(2014)]{osband2014model}
Ian Osband and Benjamin Van~Roy.
\newblock Model-based reinforcement learning and the {E}luder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Osband and Van~Roy(2017{\natexlab{a}})]{osband2017gaussian}
Ian Osband and Benjamin Van~Roy.
\newblock Gaussian-{D}irichlet posterior dominance in sequential learning.
\newblock \emph{arXiv preprint arXiv:1702.04126}, 2017{\natexlab{a}}.

\bibitem[Osband and Van~Roy(2017{\natexlab{b}})]{osband2017posterior}
Ian Osband and Benjamin Van~Roy.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \emph{International Conference on Machine Learning}, pages
  2701--2710. PMLR, 2017{\natexlab{b}}.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock ({M}ore) efficient reinforcement learning via posterior sampling.
\newblock \emph{Advances in Neural Information Processing Systems},
  26:\penalty0 3003--3011, 2013.

\bibitem[Osband et~al.(2016{\natexlab{a}})Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via {B}ootstrapped {DQN}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4026--4034, 2016{\natexlab{a}}.

\bibitem[Osband et~al.(2016{\natexlab{b}})Osband, Van~Roy, and
  Wen]{osband2016generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  2377--2386, 2016{\natexlab{b}}.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Osband et~al.(2019)Osband, Van~Roy, Russo, and Wen]{osband2019deep}
Ian Osband, Benjamin Van~Roy, Daniel~J Russo, and Zheng Wen.
\newblock Deep exploration via randomized value functions.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (124):\penalty0 1--62, 2019.

\bibitem[Osband et~al.(2021{\natexlab{a}})Osband, Wen, Asghari, Ibrahimi, Lu,
  and Van~Roy]{osband2021epistemic}
Ian Osband, Zheng Wen, Mohammad Asghari, Morteza Ibrahimi, Xiyuan Lu, and
  Benjamin Van~Roy.
\newblock Epistemic neural networks.
\newblock \emph{arXiv preprint arXiv:2107.08924}, 2021{\natexlab{a}}.

\bibitem[Osband et~al.(2021{\natexlab{b}})Osband, Wen, Asghari, Dwaracherla,
  Hao, Ibrahimi, Lawson, Lu, O'Donoghue, and Van~Roy]{osband2021evaluating}
Ian Osband, Zheng Wen, Seyed~Mohammad Asghari, Vikranth Dwaracherla, Botao Hao,
  Morteza Ibrahimi, Dieterich Lawson, Xiuyuan Lu, Brendan O'Donoghue, and
  Benjamin Van~Roy.
\newblock Evaluating predictive distributions: Does {B}ayesian deep learning
  work?
\newblock \emph{arXiv preprint arXiv:2110.04629}, 2021{\natexlab{b}}.

\bibitem[Palaiyanur and Sahai(2008)]{palaiyanur2008uniform}
Hari Palaiyanur and Anant Sahai.
\newblock On the uniform continuity of the rate-distortion function.
\newblock In \emph{2008 IEEE International Symposium on Information Theory},
  pages 857--861. IEEE, 2008.

\bibitem[Polyanskiy and Wu(2019)]{polyanskiy2019lecture}
Yury Polyanskiy and Yihong Wu.
\newblock Lecture notes on information theory.
\newblock 2019.

\bibitem[Puterman(1994)]{Puterman94}
Martin~L. Puterman.
\newblock \emph{{M}arkov Decision Processes---Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, 1994.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and
  Quillen]{rakelly2019efficient}
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In \emph{International conference on machine learning}, pages
  5331--5340. PMLR, 2019.

\bibitem[Rose(1994)]{rose1994mapping}
Kenneth Rose.
\newblock A mapping approach to rate-distortion computation and analysis.
\newblock \emph{IEEE Transactions on Information Theory}, 40\penalty0
  (6):\penalty0 1939--1952, 1994.

\bibitem[Rubin et~al.(2012)Rubin, Shamir, and Tishby]{rubin2012trading}
Jonathan Rubin, Ohad Shamir, and Naftali Tishby.
\newblock Trading value and information in {MDP}s.
\newblock In \emph{Decision Making with Imperfect Decision Makers}, pages
  57--74. Springer, 2012.

\bibitem[Rusmevichientong and Tsitsiklis(2010)]{rusmevichientong2010linearly}
Paat Rusmevichientong and John~N Tsitsiklis.
\newblock Linearly parameterized bandits.
\newblock \emph{Mathematics of Operations Research}, 35\penalty0 (2):\penalty0
  395--411, 2010.

\bibitem[Russo and Van~Roy(2014)]{russo2014learning}
Daniel Russo and Benjamin Van~Roy.
\newblock Learning to optimize via information-directed sampling.
\newblock \emph{Advances in Neural Information Processing Systems},
  27:\penalty0 1583--1591, 2014.

\bibitem[Russo and Van~Roy(2016)]{russo2016information}
Daniel Russo and Benjamin Van~Roy.
\newblock An information-theoretic analysis of {T}hompson sampling.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2442--2471, 2016.

\bibitem[Russo and Van~Roy(2018{\natexlab{a}})]{russo2018learning}
Daniel Russo and Benjamin Van~Roy.
\newblock Learning to optimize via information-directed sampling.
\newblock \emph{Operations Research}, 66\penalty0 (1):\penalty0 230--252,
  2018{\natexlab{a}}.

\bibitem[Russo and Van~Roy(2018{\natexlab{b}})]{russo2018satisficing}
Daniel Russo and Benjamin Van~Roy.
\newblock Satisficing in time-sensitive bandit learning.
\newblock \emph{arXiv preprint arXiv:1803.02855}, 2018{\natexlab{b}}.

\bibitem[Russo and Van~Roy(2022)]{russo2022satisficing}
Daniel Russo and Benjamin Van~Roy.
\newblock Satisficing in time-sensitive bandit learning.
\newblock \emph{Mathematics of Operations Research}, 2022.

\bibitem[Russo et~al.(2017)Russo, Tse, and Van~Roy]{russo2017time}
Daniel Russo, David Tse, and Benjamin Van~Roy.
\newblock Time-sensitive bandit learning and satisficing {T}hompson sampling.
\newblock \emph{arXiv preprint arXiv:1704.09028}, 2017.

\bibitem[Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband, and
  Wen]{russo2018tutorial}
Daniel~J Russo, Benjamin Van~Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen.
\newblock A tutorial on {T}hompson sampling.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (1):\penalty0 1--96, 2018.

\bibitem[Ryzhov et~al.(2012)Ryzhov, Powell, and Frazier]{ryzhov2012knowledge}
Ilya~O Ryzhov, Warren~B Powell, and Peter~I Frazier.
\newblock The knowledge gradient algorithm for a general class of online
  learning problems.
\newblock \emph{Operations Research}, 60\penalty0 (1):\penalty0 180--195, 2012.

\bibitem[Sayir(2000)]{sayir2000iterating}
Jossy Sayir.
\newblock Iterating the {A}rimoto-{B}lahut algorithm for faster convergence.
\newblock In \emph{2000 IEEE International Symposium on Information Theory
  (Cat. No. 00CH37060)}, page 235. IEEE, 2000.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock Mastering {A}tari, {G}o, {C}hess and {S}hogi by planning with a
  learned model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Shafieepoorfard et~al.(2016)Shafieepoorfard, Raginsky, and
  Meyn]{shafieepoorfard2016rationally}
Ehsan Shafieepoorfard, Maxim Raginsky, and Sean~P Meyn.
\newblock Rationally inattentive control of {M}arkov processes.
\newblock \emph{SIAM Journal on Control and Optimization}, 54\penalty0
  (2):\penalty0 987--1016, 2016.

\bibitem[Shannon(1959)]{shannon1959coding}
Claude~E. Shannon.
\newblock Coding theorems for a discrete source with a fidelity criterion.
\newblock \emph{IRE Nat. Conv. Rec., March 1959}, 4:\penalty0 142--163, 1959.

\bibitem[Silver et~al.(2017)Silver, Hasselt, Hessel, Schaul, Guez, Harley,
  Dulac-Arnold, Reichert, Rabinowitz, Barreto, et~al.]{silver2017predictron}
David Silver, Hado Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley,
  Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, et~al.
\newblock The {P}redictron: End-to-end learning and planning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3191--3199. PMLR, 2017.

\bibitem[Simon(1982)]{simon1982models}
Herbert~A. Simon.
\newblock Models of bounded rationality.
\newblock \emph{Economic Analysis and Public Policy, MIT Press, Cambridge,
  Mass}, 1982.

\bibitem[Sorg et~al.(2010)Sorg, Singh, and Lewis]{sorg2010variance}
Jonathan Sorg, Satinder Singh, and Richard~L Lewis.
\newblock Variance-based rewards for approximate {B}ayesian reinforcement
  learning.
\newblock In \emph{Proceedings of the Twenty-Sixth Conference on Uncertainty in
  Artificial Intelligence}, pages 564--571, 2010.

\bibitem[Stjernvall(1983)]{stjernvall1983dominance}
Jan-Erik Stjernvall.
\newblock Dominance--a relation between distortion measures.
\newblock \emph{IEEE Transactions on Information Theory}, 29\penalty0
  (6):\penalty0 798--807, 1983.

\bibitem[Strehl et~al.(2009)Strehl, Li, and Littman]{strehl2009reinforcement}
Alexander~L Strehl, Lihong Li, and Michael~L Littman.
\newblock Reinforcement learning in finite {MDP}s: {PAC} analysis.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Nov):\penalty0 2413--2444, 2009.

\bibitem[Strens(2000)]{strens2000bayesian}
Malcolm~JA Strens.
\newblock A {B}ayesian framework for reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, pages 943--950, 2000.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on Learning Theory}, pages 2898--2933. PMLR,
  2019.

\bibitem[Sutton and Barto(1998)]{sutton1998introduction}
Richard~S Sutton and Andrew~G Barto.
\newblock Introduction to reinforcement learning.
\newblock 1998.

\bibitem[Tatikonda and Mitter(2004)]{tatikonda2004control}
Sekhar Tatikonda and Sanjoy Mitter.
\newblock Control under communication constraints.
\newblock \emph{IEEE Transactions on Automatic Control}, 49\penalty0
  (7):\penalty0 1056--1068, 2004.

\bibitem[Thompson(1933)]{thompson1933likelihood}
William~R Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Tishby and Polani(2011)]{tishby2011information}
Naftali Tishby and Daniel Polani.
\newblock Information theory of decisions and actions.
\newblock In \emph{Perception-action cycle}, pages 601--636. Springer, 2011.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Naftali Tishby, Fernando~C Pereira, and William Bialek.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[Van~Roy(2006)]{van2006performance}
Benjamin Van~Roy.
\newblock Performance loss bounds for approximate value iteration with state
  aggregation.
\newblock \emph{Mathematics of Operations Research}, 31\penalty0 (2):\penalty0
  234--244, 2006.

\bibitem[Verd{\'u} et~al.(1994)]{verdu1994generalizing}
Sergio Verd{\'u} et~al.
\newblock Generalizing the {F}ano inequality.
\newblock \emph{IEEE Transactions on Information Theory}, 40\penalty0
  (4):\penalty0 1247--1251, 1994.

\bibitem[Voelcker et~al.(2022)Voelcker, Liao, Garg, and
  Farahmand]{voelcker2022value}
Claas~A Voelcker, Victor Liao, Animesh Garg, and Amir-massoud Farahmand.
\newblock Value gradient weighted model-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Vontobel et~al.(2008)Vontobel, Kavcic, Arnold, and
  Loeliger]{vontobel2008generalization}
Pascal~O Vontobel, Aleksandar Kavcic, Dieter~M Arnold, and Hans-Andrea
  Loeliger.
\newblock A generalization of the {B}lahut--{A}rimoto algorithm to finite-state
  channels.
\newblock \emph{IEEE Transactions on Information Theory}, 54\penalty0
  (5):\penalty0 1887--1918, 2008.

\bibitem[Wang et~al.(2005)Wang, Lizotte, Bowling, and
  Schuurmans]{wang2005bayesian}
Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans.
\newblock {B}ayesian sparse sampling for on-line reward optimization.
\newblock In \emph{Proceedings of the 22nd International Conference on Machine
  Learning}, pages 956--963, 2005.

\bibitem[Wang et~al.(2008)Wang, Audibert, and Munos]{wang2008algorithms}
Yizao Wang, Jean-Yves Audibert, and R{\'e}mi Munos.
\newblock Algorithms for infinitely many-armed bandits.
\newblock In \emph{Proceedings of the 21st International Conference on Neural
  Information Processing Systems}, pages 1729--1736, 2008.

\bibitem[Whitt(1978)]{whitt1978approximations}
Ward Whitt.
\newblock Approximations of dynamic programs, {I}.
\newblock \emph{Mathematics of Operations Research}, 3\penalty0 (3):\penalty0
  231--243, 1978.

\bibitem[Witsenhausen(1971)]{witsenhausen1971separation}
Hans~S Witsenhausen.
\newblock Separation of estimation and control for discrete time systems.
\newblock \emph{Proceedings of the IEEE}, 59\penalty0 (11):\penalty0
  1557--1566, 1971.

\bibitem[Yu(2010)]{yu2010squeezing}
Yaming Yu.
\newblock Squeezing the {A}rimoto--{B}lahut algorithm for faster convergence.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (7):\penalty0 3149--3157, 2010.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312. PMLR, 2019.

\bibitem[Zimmert and Lattimore(2019)]{zimmert2019connections}
Julian Zimmert and Tor Lattimore.
\newblock Connections between mirror descent, {T}hompson sampling and the
  information ratio.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11973--11982, 2019.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2019varibad}
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin
  Gal, Katja Hofmann, and Shimon Whiteson.
\newblock Vari{BAD}: {A} {V}ery {G}ood {M}ethod for {B}ayes-{A}daptive {D}eep
  {RL} via {M}eta-{L}earning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
