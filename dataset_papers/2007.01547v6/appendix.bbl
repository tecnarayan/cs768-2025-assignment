\begin{thebibliography}{160}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aitchison(2020)]{aitchison2018bayesian}
Aitchison, L.
\newblock {Bayesian filtering unifies adaptive and non-adaptive neural network
  optimization methods}.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020.

\bibitem[Almeida et~al.(2021)Almeida, Winter, Tang, and Zaremba]{Almeida2021}
Almeida, D., Winter, C., Tang, J., and Zaremba, W.
\newblock {A Generalizable Approach to Learning Optimizers}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2106.00958}{{\ttfamily
  \emph{2106.00958}}}, 2021.

\bibitem[Anil et~al.(2019)Anil, Gupta, Koren, and Singer]{Anil2019}
Anil, R., Gupta, V., Koren, T., and Singer, Y.
\newblock {Memory Efficient Adaptive Optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems 32,
  NeurIPS}, 2019.

\bibitem[Anil et~al.(2020)Anil, Gupta, Koren, Regan, and Singer]{Anil2020}
Anil, R., Gupta, V., Koren, T., Regan, K., and Singer, Y.
\newblock {Second Order Optimization Made Practical}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2002.09018}{{\ttfamily
  \emph{2002.09018}}}, 2020.

\bibitem[Ayadi \& Turinici(2020)Ayadi and Turinici]{ayadi2020}
Ayadi, I. and Turinici, G.
\newblock {Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic
  gradient descent}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2002.09304}{{\ttfamily \emph{2002.09304}}}, 2020.

\bibitem[Bae et~al.(2019)Bae, Ryu, and Shin]{bae2019does}
Bae, K., Ryu, H., and Shin, H.
\newblock {Does Adam optimizer keep close to the optimal point?}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1911.00289}{{\ttfamily \emph{1911.00289}}}, 2019.

\bibitem[Bahrami \& Zadeh(2021)Bahrami and Zadeh]{Bahrami2021}
Bahrami, D. and Zadeh, S.~P.
\newblock {Gravity Optimizer: a Kinematic Approach on Optimization in Deep
  Learning}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2101.09192}{{\ttfamily \emph{2101.09192}}}, 2021.

\bibitem[Bai \& Zhang(2019)Bai and Zhang]{bai2019bgadam}
Bai, J. and Zhang, J.
\newblock {BGADAM: Boosting based Genetic-Evolutionary ADAM for Convolutional
  Neural Network Optimization}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1908.08015}{{\ttfamily \emph{1908.08015}}}, 2019.

\bibitem[Balles \& Hennig(2018)Balles and Hennig]{Balles2018}
Balles, L. and Hennig, P.
\newblock {Dissecting Adam: The Sign, Magnitude and Variance of Stochastic
  Gradients}.
\newblock In \emph{35th International Conference on Machine Learning, ICML},
  2018.

\bibitem[Bello et~al.(2017)Bello, Zoph, Vasudevan, and Le]{Bello2017}
Bello, I., Zoph, B., Vasudevan, V., and Le, Q.~V.
\newblock {Neural Optimizer Search with Reinforcement Learning}.
\newblock In \emph{34th International Conference on Machine Learning, {ICML}},
  2017.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{BernsteinWAA18}
Bernstein, J., Wang, Y., Azizzadenesheli, K., and Anandkumar, A.
\newblock {SIGNSGD: Compressed Optimisation for Non-Convex Problems}.
\newblock In \emph{35th International Conference on Machine Learning, {ICML}},
  2018.

\bibitem[Berrada et~al.(2020)Berrada, Zisserman, and
  Kumar]{berrada2019training}
Berrada, L., Zisserman, A., and Kumar, M.~P.
\newblock {Training Neural Networks for and by Interpolation}.
\newblock In \emph{37th International Conference on Machine Learning, {ICML}},
  2020.

\bibitem[Borysenko \& Byshkin(2020)Borysenko and
  Byshkin]{oleks2020coolmomentum}
Borysenko, O. and Byshkin, M.
\newblock {CoolMomentum: A Method for Stochastic Optimization by Langevin
  Dynamics with Simulated Annealing}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2005.14605}{{\ttfamily \emph{2005.14605}}}, 2020.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{Botev2017}
Botev, A., Ritter, H., and Barber, D.
\newblock {Practical Gauss-Newton Optimisation for Deep Learning}.
\newblock In \emph{34th International Conference on Machine Learning, ICML},
  2017.

\bibitem[Bottou(2012)]{Bottou2012}
Bottou, L.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}. Springer, 2012.

\bibitem[Castera et~al.(2021)Castera, Bolte, Févotte, and
  Pauwels]{Castera2021}
Castera, C., Bolte, J., Févotte, C., and Pauwels, E.
\newblock {Second-order step-size tuning of SGD for non-convex optimization}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2103.03570}{{\ttfamily \emph{2103.03570}}}, 2021.

\bibitem[Chae et~al.(2021)Chae, Wilke, and Kafka]{Chae2021}
Chae, Y., Wilke, D.~N., and Kafka, D.
\newblock {GOALS: Gradient-Only Approximations for Line Searches Towards Robust
  and Consistent Training of Deep Neural Networks}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2105.10915}{{\ttfamily
  \emph{2105.10915}}}, 2021.

\bibitem[Chakrabarti \& Chopra(2021)Chakrabarti and Chopra]{Chakrabarti2021}
Chakrabarti, K. and Chopra, N.
\newblock {Generalized AdaGrad (G-AdaGrad) and Adam: A State-Space
  Perspective}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2106.00092}{{\ttfamily \emph{2106.00092}}}, 2021.

\bibitem[Chen et~al.(2018)Chen, Choi, Brand, Agrawal, Zhang, and
  Gopalakrishnan]{ChenCBAZG18}
Chen, C., Choi, J., Brand, D., Agrawal, A., Zhang, W., and Gopalakrishnan, K.
\newblock {AdaComp: Adaptive Residual Gradient Compression for Data-Parallel
  Distributed Training}.
\newblock In \emph{32nd {AAAI} Conference on Artificial Intelligence, {AAAI}},
  2018.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Zhou, Tang, Yang, Cao, and
  Gu]{ChenZTYCG20}
Chen, J., Zhou, D., Tang, Y., Yang, Z., Cao, Y., and Gu, Q.
\newblock {Closing the Generalization Gap of Adaptive Gradient Methods in
  Training Deep Neural Networks}.
\newblock In \emph{29th International Joint Conference on Artificial
  Intelligence, {IJCAI}}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Choi, Balles, Duvenaud, and
  Hennig]{Chen2020a}
Chen, R. T.~Q., Choi, D., Balles, L., Duvenaud, D., and Hennig, P.
\newblock {Self-Tuning Stochastic Optimization with Curvature-Aware Gradient
  Filtering}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2011.04803}{{\ttfamily \emph{2011.04803}}},
  2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Guo, Sun, and Yin]{Chen2021}
Chen, T., Guo, Z., Sun, Y., and Yin, W.
\newblock {CADA: Communication-Adaptive Distributed Adam}.
\newblock In \emph{24th International Conference on Artificial Intelligence and
  Statistics, {AISTATS}}, 2021.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Liu, Sun, and
  Hong]{chen2018convergence}
Chen, X., Liu, S., Sun, R., and Hong, M.
\newblock {On the Convergence of A Class of Adam-Type Algorithms for Non-Convex
  Optimization}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR}}, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Jing, Zhao, Liu, Li, Qiao, Xue,
  Fu, and Yang]{chen2019adaptive}
Chen, Y., Jing, H., Zhao, W., Liu, Z., Li, O., Qiao, L., Xue, W., Fu, H., and
  Yang, G.
\newblock {An Adaptive Remote Stochastic Gradient Method for Training Neural
  Networks}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1905.01422}{{\ttfamily \emph{1905.01422}}},
  2019{\natexlab{b}}.

\bibitem[Chen et~al.(2019{\natexlab{c}})Chen, Jing, Zhao, Liu, Qiao, Xue, Fu,
  and Yang]{chen2019}
Chen, Y., Jing, H., Zhao, W., Liu, Z., Qiao, L., Xue, W., Fu, H., and Yang, G.
\newblock {NAMSG: An Efficient Method For Training Neural Networks}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1905.01422}{{\ttfamily \emph{1905.01422}}},
  2019{\natexlab{c}}.

\bibitem[Chen \& Zhou(2020)Chen and Zhou]{chen2020momentum}
Chen, Z. and Zhou, Y.
\newblock {Momentum with Variance Reduction for Nonconvex Composition
  Optimization}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2005.07755}{{\ttfamily \emph{2005.07755}}}, 2020.

\bibitem[Choi et~al.(2019)Choi, Shallue, Nado, Lee, Maddison, and
  Dahl]{Choi2019}
Choi, D., Shallue, C.~J., Nado, Z., Lee, J., Maddison, C.~J., and Dahl, G.~E.
\newblock {On Empirical Comparisons of Optimizers for Deep Learning}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1910.05446}{{\ttfamily \emph{1910.05446}}}, 2019.

\bibitem[Daley \& Amato(2020)Daley and Amato]{Daley2020}
Daley, B. and Amato, C.
\newblock {Expectigrad: Fast Stochastic Optimization with Robust Convergence
  Properties}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2010.01356}{{\ttfamily \emph{2010.01356}}}, 2020.

\bibitem[de~Roos et~al.(2021)de~Roos, Jidling, Wills, Schön, and
  Hennig]{Roos2021}
de~Roos, F., Jidling, C., Wills, A., Schön, T., and Hennig, P.
\newblock {A Probabilistically Motivated Learning Rate Adaptation for
  Stochastic Optimization}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2102.10880}{{\ttfamily \emph{2102.10880}}}, 2021.

\bibitem[Defazio \& Jelassi(2021)Defazio and Jelassi]{Defazio2021}
Defazio, A. and Jelassi, S.
\newblock {Adaptivity without Compromise: A Momentumized, Adaptive, Dual
  Averaged Gradient Method for Stochastic Optimization}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2101.11075}{{\ttfamily
  \emph{2101.11075}}}, 2021.

\bibitem[Dellaferrera et~al.(2021)Dellaferrera, Wozniak, Indiveri, Pantazi, and
  Eleftheriou]{Dellaferrera2021}
Dellaferrera, G., Wozniak, S., Indiveri, G., Pantazi, A., and Eleftheriou, E.
\newblock {Learning in Deep Neural Networks Using a Biologically Inspired
  Optimizer}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2104.11604}{{\ttfamily \emph{2104.11604}}}, 2021.

\bibitem[Devarakonda et~al.(2017)Devarakonda, Naumov, and
  Garland]{devarakonda2017adabatch}
Devarakonda, A., Naumov, M., and Garland, M.
\newblock {AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1712.02029}{{\ttfamily \emph{1712.02029}}}, 2017.

\bibitem[Ding et~al.(2019)Ding, Ren, Luo, and Sun]{ding2019adaptive}
Ding, J., Ren, X., Luo, R., and Sun, X.
\newblock {An Adaptive and Momental Bound Method for Stochastic Learning}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1910.12249}{{\ttfamily \emph{1910.12249}}}, 2019.

\bibitem[Dozat(2016)]{Dozat2016IncorporatingNM}
Dozat, T.
\newblock {Incorporating Nesterov Momentum into Adam}.
\newblock In \emph{4th International Conference on Learning Representations,
  {ICLR}}, 2016.

\bibitem[Dubey et~al.(2020)Dubey, Chakraborty, Roy, Mukherjee, Singh, and
  Chaudhuri]{Dubey2020}
Dubey, S.~R., Chakraborty, S., Roy, S.~K., Mukherjee, S., Singh, S.~K., and
  Chaudhuri, B.~B.
\newblock {diffGrad: An Optimization Method for Convolutional Neural Networks}.
\newblock \emph{{IEEE} Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Duchi2011}
Duchi, J., Hazan, E., and Singer, Y.
\newblock {Adaptive Subgradient Methods for Online Learning and Stochastic
  Optimization}.
\newblock \emph{Journal of Machine Learning Research, JMLR}, 12, 2011.

\bibitem[Eliyahu(2020)]{ADASRepo}
Eliyahu, Y.
\newblock {ADAS Optimzier}.
\newblock \url{https://github.com/YanaiEliyahu/AdasOptimizer}, 2020.

\bibitem[Fetterman et~al.(2019)Fetterman, Kim, and Albrecht]{fetterman2019}
Fetterman, A.~J., Kim, C.~H., and Albrecht, J.
\newblock {SoftAdam: Unifying SGD and Adam for better stochastic gradient
  descent}.
\newblock \url{https://openreview.net/forum?id=Skgfr1rYDH}, 2019.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{Foret2021}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock {Sharpness-aware Minimization for Efficiently Improving
  Generalization}.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[Gao et~al.(2020)Gao, Liu, Huang, Wang, Wang, Wang, Xu, and
  Yu]{Gao2020}
Gao, K.-X., Liu, X.-L., Huang, Z.-H., Wang, M., Wang, S., Wang, Z., Xu, D., and
  Yu, F.
\newblock {Eigenvalue-corrected Natural Gradient Based on a New Approximation}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2011.13609}{{\ttfamily \emph{2011.13609}}}, 2020.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and
  Vincent]{George2018}
George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P.
\newblock {Fast Approximate Natural Gradient Descent in a Kronecker Factored
  Eigenbasis}.
\newblock In \emph{Advances in Neural Information Processing Systems 31,
  NeurIPS}, 2018.

\bibitem[Ginsburg et~al.(2019)Ginsburg, Castonguay, Hrinchuk, Kuchaiev,
  Lavrukhin, Leary, Li, Nguyen, and Cohen]{Ginsburg2019}
Ginsburg, B., Castonguay, P., Hrinchuk, O., Kuchaiev, O., Lavrukhin, V., Leary,
  R., Li, J., Nguyen, H., and Cohen, J.~M.
\newblock {Stochastic Gradient Methods with Layer-wise Adaptive Moments for
  Training of Deep Networks}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1905.11286}{{\ttfamily \emph{1905.11286}}}, 2019.

\bibitem[Goldfarb et~al.(2020)Goldfarb, Ren, and
  Bahamou]{goldfarb2020practical}
Goldfarb, D., Ren, Y., and Bahamou, A.
\newblock {Practical Quasi-Newton Methods for Training Deep Neural Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow2016}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{{Deep Learning}}.
\newblock MIT Press, 2016.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1706.02677}{{\ttfamily \emph{1706.02677}}}, 2017.

\bibitem[Grankin(2020)]{RangerLars}
Grankin, M.
\newblock {RangerLars}.
\newblock \url{https://github.com/mgrankin/over9000}, 2020.

\bibitem[Granziol et~al.(2020)Granziol, Wan, and Roberts]{Granziol2020}
Granziol, D., Wan, X., and Roberts, S.
\newblock {Gadam: Combining Adaptivity with Iterate Averaging Gives Greater
  Generalisation}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2003.01247}{{\ttfamily \emph{2003.01247}}}, 2020.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{0001KS18}
Gupta, V., Koren, T., and Singer, Y.
\newblock {Shampoo: Preconditioned Stochastic Tensor Optimization}.
\newblock In \emph{35th International Conference on Machine Learning, {ICML}},
  2018.

\bibitem[Hayashi et~al.(2018)Hayashi, Koushik, and Neubig]{Hayashi2018}
Hayashi, H., Koushik, J., and Neubig, G.
\newblock {Eve: A Gradient Based Optimization Method with Locally and Globally
  Adaptive Learning Rates}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1611.01505}{{\ttfamily \emph{1611.01505}}}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In \emph{IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Henriques et~al.(2019)Henriques, Ehrhardt, Albanie, and
  Vedaldi]{Henriques2018}
Henriques, J.~F., Ehrhardt, S., Albanie, S., and Vedaldi, A.
\newblock {Small Steps and Giant Leaps: Minimal Newton Solvers for Deep
  Learning}.
\newblock In \emph{{IEEE/CVF} International Conference on Computer Vision,
  {ICCV}}, 2019.

\bibitem[Heo et~al.(2021)Heo, Chun, Oh, Han, Yun, Uh, and Ha]{Heo2021}
Heo, B., Chun, S., Oh, S.~J., Han, D., Yun, S., Uh, Y., and Ha, J.-W.
\newblock {AdamP: Slowing Down the Weight Norm Increase in Momentum-based
  Optimizers}.
\newblock In \emph{7th International Conference on Learning Representations,
  ICLR}, 2021.

\bibitem[Hosseini \& Plataniotis(2020)Hosseini and Plataniotis]{Hosseini2020}
Hosseini, M.~S. and Plataniotis, K.~N.
\newblock {AdaS: Adaptive Scheduling of Stochastic Gradients}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.06587}{{\ttfamily \emph{2006.06587}}}, 2020.

\bibitem[Howard \& Ruder(2018)Howard and Ruder]{Howard2018}
Howard, J. and Ruder, S.
\newblock {Universal Language Model Fine-tuning for Text Classification}.
\newblock In \emph{56th Annual Meeting of the Association for Computational
  Linguistics}, 2018.

\bibitem[Hu et~al.(2019)Hu, Lin, and Tang]{hu2019secondorder}
Hu, Y., Lin, L., and Tang, S.
\newblock {Second-order Information in First-order Optimization Methods}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1912.09926}{{\ttfamily \emph{1912.09926}}}, 2019.

\bibitem[Hu et~al.(2020)Hu, Zhang, Chen, and He]{hu2020biased}
Hu, Y., Zhang, S., Chen, X., and He, N.
\newblock {Biased Stochastic First-Order Methods for Conditional Stochastic
  Optimization and Applications in Meta Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020.

\bibitem[Huang et~al.(2019)Huang, Wang, and Dong]{HuangWD19}
Huang, H., Wang, C., and Dong, B.
\newblock {Nostalgic Adam: Weighting More of the Past Gradients When Designing
  the Adaptive Learning Rate}.
\newblock In \emph{28th International Joint Conference on Artificial
  Intelligence, {IJCAI}}, 2019.

\bibitem[Huang et~al.(2020)Huang, Zhou, Xu, Wang, and Li]{huang2020adaptive}
Huang, X., Zhou, H., Xu, R., Wang, Z., and Li, L.
\newblock {Adaptive Gradient Methods Can Be Provably Faster than SGD after
  Finite Epochs}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.07037}{{\ttfamily \emph{2006.07037}}}, 2020.

\bibitem[Ida et~al.(2017)Ida, Fujiwara, and Iwamura]{IdaFI17}
Ida, Y., Fujiwara, Y., and Iwamura, S.
\newblock {Adaptive Learning Rate via Covariance Matrix Based Preconditioning
  for Deep Neural Networks}.
\newblock In \emph{26th International Joint Conference on Artificial
  Intelligence, {IJCAI}}, 2017.

\bibitem[Ilboudo et~al.(2020)Ilboudo, Kobayashi, and Sugimoto]{ilboudo2020}
Ilboudo, W. E.~L., Kobayashi, T., and Sugimoto, K.
\newblock {TAdam: A Robust Stochastic Gradient Optimizer}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2003.00179}{{\ttfamily
  \emph{2003.00179}}}, 2020.

\bibitem[Jiang et~al.(2019)Jiang, Balu, Tan, Lee, Hegde, and
  Sarkar]{jiang2019higherorder}
Jiang, Z., Balu, A., Tan, S.~Y., Lee, Y.~M., Hegde, C., and Sarkar, S.
\newblock {On Higher-order Moments in Adam}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1910.06878}{{\ttfamily \emph{1910.06878}}}, 2019.

\bibitem[Jin et~al.(2021)Jin, Zhou, Zhao, Zhu, Guo, Canini, and
  Krishnamurthy]{Jin2021}
Jin, Y., Zhou, T., Zhao, L., Zhu, Y., Guo, C., Canini, M., and Krishnamurthy,
  A.
\newblock {AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization
  on the Fly}.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.
\newblock URL \url{https://openreview.net/forum?id=SlrqM9_lyju}.

\bibitem[Johnson et~al.(2020)Johnson, Agrawal, Gu, and
  Guestrin]{johnson2020adascale}
Johnson, T.~B., Agrawal, P., Gu, H., and Guestrin, C.
\newblock {AdaScale SGD: A User-Friendly Algorithm for Distributed Training}.
\newblock In \emph{37th International Conference on Machine Learning, {ICML}},
  2020.

\bibitem[Kafka \& Wilke(2019)Kafka and Wilke]{kafka2019gradientonly}
Kafka, D. and Wilke, D.
\newblock {Gradient-only line searches: An Alternative to Probabilistic Line
  Searches}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1903.09383}{{\ttfamily \emph{1903.09383}}}, 2019.

\bibitem[Kelterborn et~al.(2020)Kelterborn, Mazur, and
  Petrenko]{kelterborn2020gravilon}
Kelterborn, C., Mazur, M., and Petrenko, B.~V.
\newblock {Gravilon: Applications of a New Gradient Descent Method to Machine
  Learning}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2008.11370}{{\ttfamily \emph{2008.11370}}}, 2020.

\bibitem[Keskar \& Socher(2017)Keskar and Socher]{Keskar2017}
Keskar, N.~S. and Socher, R.
\newblock {Improving Generalization Performance by Switching from Adam to SGD}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1712.07628}{{\ttfamily \emph{1712.07628}}}, 2017.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{KhanNTLGS18}
Khan, M.~E., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A.
\newblock {Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in
  Adam}.
\newblock In \emph{35th International Conference on Machine Learning, {ICML}},
  2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma2015}
Kingma, D.~P. and Ba, J.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR}}, 2015.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{Kwon2021}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock {ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant
  Learning of Deep Neural Networks}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2102.11600}{{\ttfamily \emph{2102.11600}}}, 2021.

\bibitem[Landro et~al.(2020)Landro, Gallo, and Grassa]{Landro2020}
Landro, N., Gallo, I., and Grassa, R.~L.
\newblock {Mixing ADAM and SGD: a Combined Optimization Method}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2011.08042}{{\ttfamily \emph{2011.08042}}}, 2020.

\bibitem[Levy et~al.(2018)Levy, Yurtsever, and Cevher]{accelegrad}
Levy, K.~Y., Yurtsever, A., and Cevher, V.
\newblock {Online Adaptive Methods, Universality and Acceleration}.
\newblock In \emph{Advances in Neural Information Processing Systems 31,
  NeurIPS}, 2018.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Zhang, Wang, and Luo]{li2020adax}
Li, W., Zhang, Z., Wang, X., and Luo, P.
\newblock {AdaX: Adaptive Gradient Descent with Exponential Long Term Memory}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2004.09740}{{\ttfamily \emph{2004.09740}}},
  2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Bao, Zhang, and
  Richtárik]{li2020page}
Li, Z., Bao, H., Zhang, X., and Richtárik, P.
\newblock {PAGE: A Simple and Optimal Probabilistic Gradient Estimator for
  Nonconvex Optimization}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2008.10898}{{\ttfamily \emph{2008.10898}}},
  2020{\natexlab{b}}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Chen, and Theodorou]{Liu2021}
Liu, G.-H., Chen, T., and Theodorou, E.~A.
\newblock Dynamic game theoretic neural optimizer. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2105.03788}{{\ttfamily
  \emph{2105.03788}}}, 2021{\natexlab{a}}.

\bibitem[Liu \& Tian(2020)Liu and Tian]{Liu2020c}
Liu, H. and Tian, X.
\newblock {AEGD: Adaptive Gradient Decent with Energy}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2010.05109}{{\ttfamily
  \emph{2010.05109}}}, 2020.

\bibitem[Liu \& Luo(2020)Liu and Luo]{liu2020new}
Liu, L. and Luo, X.
\newblock {A New Accelerated Stochastic Gradient Method with Momentum}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.00423}{{\ttfamily \emph{2006.00423}}}, 2020.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Jiang, He, Chen, Liu, Gao, and
  Han]{Liu2019}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock {On the Variance of the Adaptive Learning Rate and Beyond}.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR}}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhang, Orabona, and Yang]{Liu2020a}
Liu, M., Zhang, W., Orabona, F., and Yang, T.
\newblock {Adam$^+$: A Stochastic Method with Adaptive Variance Reduction}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2011.11985}{{\ttfamily \emph{2011.11985}}},
  2020{\natexlab{b}}.

\bibitem[Liu et~al.(2020{\natexlab{c}})Liu, Wu, and Mozafari]{Liu2020b}
Liu, R., Wu, T., and Mozafari, B.
\newblock {Adam with Bandit Sampling for Deep Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020{\natexlab{c}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Bernstein, Meister, and
  Yue]{Liu2021a}
Liu, Y., Bernstein, J., Meister, M., and Yue, Y.
\newblock {Learning by Turning: Neural Architecture Aware Optimisation},
  \href{http://arxiv.org/abs/arXiv:2102.07227}{{\ttfamily
  \emph{arXiv:2102.07227}}}, 2021{\natexlab{b}}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{Loshchilov2017}
Loshchilov, I. and Hutter, F.
\newblock {SGDR: Stochastic Gradient Descent with Warm Restarts}.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR}, 2017.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{Loshchilov2019a}
Loshchilov, I. and Hutter, F.
\newblock {Decoupled weight decay regularization}.
\newblock In \emph{7th International Conference on Learning Representations,
  ICLR}, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{Luo2019}
Luo, L., Xiong, Y., Liu, Y., and Sun, X.
\newblock {Adaptive Gradient Methods with Dynamic Bound of Learning Rate}.
\newblock In \emph{7th International Conference on Learning Representations,
  ICLR}, 2019.

\bibitem[Ma \& Yarats(2019)Ma and Yarats]{MaY19}
Ma, J. and Yarats, D.
\newblock {Quasi-hyperbolic momentum and Adam for deep learning}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR}}, 2019.

\bibitem[Mahsereci \& Hennig(2017)Mahsereci and Hennig]{Mahsereci2017}
Mahsereci, M. and Hennig, P.
\newblock {Probabilistic Line Searches for Stochastic Optimization}.
\newblock In \emph{Journal of Machine Learning Research, JMLR}, volume~18,
  2017.

\bibitem[Malkiel \& Wolf(2020)Malkiel and Wolf]{malkiel2020mtadam}
Malkiel, I. and Wolf, L.
\newblock {MTAdam: Automatic Balancing of Multiple Training Loss Terms}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.14683}{{\ttfamily \emph{2006.14683}}}, 2020.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{Martens2015}
Martens, J. and Grosse, R.
\newblock {Optimizing Neural Networks with Kronecker-Factored Approximate
  Curvature}.
\newblock In \emph{32nd International Conference on Machine Learning, ICML},
  2015.

\bibitem[Mukkamala \& Hein(2017)Mukkamala and Hein]{MukkamalaH17}
Mukkamala, M.~C. and Hein, M.
\newblock {Variants of RMSProp and Adagrad with Logarithmic Regret Bounds}.
\newblock In \emph{34th International Conference on Machine Learning, {ICML}},
  2017.

\bibitem[Mutschler \& Zell(2020)Mutschler and Zell]{mutschler2019parabolic}
Mutschler, M. and Zell, A.
\newblock {Parabolic Approximation Line Search for DNNs}.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Nazari, P., Tarzanagh, D.~A., and Michailidis, G.
\newblock {DADAM: A Consensus-based Distributed Adaptive Gradient Method for
  Online Optimization}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1901.09109}{{\ttfamily \emph{1901.09109}}}, 2019.

\bibitem[Nesterov(1983)]{Nesterov1983}
Nesterov, Y.
\newblock {A method for solving the convex programming problem with convergence
  rate $O(1/k^2)$}.
\newblock \emph{Soviet Mathematics Doklady}, 27, 1983.

\bibitem[Orabona \& P{\'{a}}l(2015)Orabona and P{\'{a}}l]{Orabona2015}
Orabona, F. and P{\'{a}}l, D.
\newblock {Scale-Free Algorithms for Online Linear Optimization}.
\newblock In \emph{Algorithmic Learning Theory - 26th International Conference,
  {ALT}}, 2015.

\bibitem[Orvieto et~al.(2019)Orvieto, K{\"{o}}hler, and Lucchi]{OrvietoKL19}
Orvieto, A., K{\"{o}}hler, J., and Lucchi, A.
\newblock {The Role of Memory in Stochastic Optimization}.
\newblock In \emph{35th Conference on Uncertainty in Artificial Intelligence,
  {UAI}}, 2019.

\bibitem[Polyak(1964)]{Polyak1964}
Polyak, B.~T.
\newblock {Some methods of speeding up the convergence of iteration methods}.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5), 1964.

\bibitem[Preechakul \& Kijsirikul(2019)Preechakul and
  Kijsirikul]{Preechakul2019}
Preechakul, K. and Kijsirikul, B.
\newblock {CProp: Adaptive Learning Rate Scaling from Past Gradient
  Conformity}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1912.11493}{{\ttfamily \emph{1912.11493}}}, 2019.

\bibitem[Purkayastha \& Purkayastha(2020)Purkayastha and
  Purkayastha]{Purkayastha2020}
Purkayastha, S. and Purkayastha, S.
\newblock {A Variant of Gradient Descent Algorithm Based on Gradient
  Averaging}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2012.02387}{{\ttfamily \emph{2012.02387}}}, 2020.

\bibitem[Ramezani-Kebrya et~al.(2021)Ramezani-Kebrya, Khisti, and
  Liang]{RamezaniKebrya2021}
Ramezani-Kebrya, A., Khisti, A., and Liang, B.
\newblock {On the Generalization of Stochastic Gradient Descent with Momentum}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2102.13653}{{\ttfamily \emph{2102.13653}}}, 2021.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{Reddi2018}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock {On the Convergence of Adam and Beyond}.
\newblock In \emph{6th International Conference on Learning Representations,
  ICLR}, 2018.

\bibitem[Ren \& Goldfarb(2021)Ren and Goldfarb]{Ren2021}
Ren, Y. and Goldfarb, D.
\newblock {Kronecker-factored Quasi-Newton Methods for Convolutional Neural
  Networks}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2102.06737}{{\ttfamily \emph{2102.06737}}}, 2021.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{Robbins1951}
Robbins, H. and Monro, S.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0 (3), 1951.

\bibitem[Rol{\'{i}}nek \& Martius(2018)Rol{\'{i}}nek and Martius]{Rolinek2018}
Rol{\'{i}}nek, M. and Martius, G.
\newblock {L4: Practical loss-based stepsize adaptation for deep learning}.
\newblock In \emph{Advances in Neural Information Processing Systems 31,
  NeurIPS}, 2018.

\bibitem[Roy et~al.(2021)Roy, Paoletti, Haut, Dubey, Kar, Plaza, and
  Chaudhuri]{Roy2021}
Roy, S.~K., Paoletti, M.~E., Haut, J.~M., Dubey, S.~R., Kar, P., Plaza, A., and
  Chaudhuri, B.~B.
\newblock {AngularGrad: A New Optimization Technique for Angular Convergence of
  Convolutional Neural Networks}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2105.10190}{{\ttfamily \emph{2105.10190}}}, 2021.

\bibitem[Salas et~al.(2018)Salas, Kessler, Zohren, and
  Roberts]{salas2018practical}
Salas, A., Kessler, S., Zohren, S., and Roberts, S.
\newblock {Practical Bayesian Learning of Neural Networks via Adaptive
  Subgradient Methods}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1811.03679}{{\ttfamily \emph{1811.03679}}}, 2018.

\bibitem[Savarese et~al.(2019)Savarese, McAllester, Babu, and
  Maire]{savarese2019domainindependent}
Savarese, P., McAllester, D., Babu, S., and Maire, M.
\newblock {Domain-independent Dominance of Adaptive Methods}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1912.01823}{{\ttfamily \emph{1912.01823}}}, 2019.

\bibitem[Schaul \& LeCun(2013)Schaul and LeCun]{Schaull3}
Schaul, T. and LeCun, Y.
\newblock {Adaptive learning rates and parallelization for stochastic, sparse,
  non-smooth gradients}.
\newblock In \emph{1st International Conference on Learning Representations,
  {ICLR}}, 2013.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{SchaulZL13}
Schaul, T., Zhang, S., and LeCun, Y.
\newblock {No more pesky learning rates}.
\newblock In \emph{30th International Conference on Machine Learning, {ICML}},
  2013.

\bibitem[Shang et~al.(2020)Shang, Zhou, Liu, Cheng, Tsang, Zhang, Tao, and
  Jiao]{ShangZLCTZTJ20}
Shang, F., Zhou, K., Liu, H., Cheng, J., Tsang, I.~W., Zhang, L., Tao, D., and
  Jiao, L.
\newblock {VR-SGD: A Simple Stochastic Variance Reduction Method for Machine
  Learning}.
\newblock \emph{{IEEE} Trans. Knowl. Data Eng.}, 32\penalty0 (1), 2020.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{Shazeer2018}
Shazeer, N. and Stern, M.
\newblock {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}.
\newblock In \emph{35th International Conference on Machine Learning, ICML},
  2018.

\bibitem[Smith(2017)]{Smith2017}
Smith, L.~N.
\newblock {Cyclical Learning Rates for Training Neural Networks}.
\newblock In \emph{{IEEE} Winter Conference on Applications of Computer Vision,
  {WACV}}, 2017.

\bibitem[Smith \& Topin(2017)Smith and Topin]{Smith2017super}
Smith, L.~N. and Topin, N.
\newblock {Super-Convergence: Very Fast Training of Neural Networks Using Large
  Learning Rates}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1708.07120}{{\ttfamily \emph{1708.07120}}}, 2017.

\bibitem[Sun et~al.(2019)Sun, Gu, and Sun]{Sun2019}
Sun, H., Gu, L., and Sun, B.
\newblock {Adathm: Adaptive Gradient Method Based on Estimates of Third-Order
  Moments}.
\newblock In \emph{4th {IEEE} International Conference on Data Science in
  Cyberspace, {DSC}}, 2019.

\bibitem[Sung et~al.(2020)Sung, Choi, Park, Choi, and Shin]{Sung2020}
Sung, W., Choi, I., Park, J., Choi, S., and Shin, S.
\newblock {S-SGD: Symmetrical Stochastic Gradient Descent with Weight Noise
  Injection for Reaching Flat Minima}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2009.02479}{{\ttfamily \emph{2009.02479}}}, 2020.

\bibitem[Tan et~al.(2016)Tan, Ma, Dai, and Qian]{TanMDQ16}
Tan, C., Ma, S., Dai, Y., and Qian, Y.
\newblock {Barzilai-Borwein Step Size for Stochastic Gradient Descent}.
\newblock In \emph{Advances in Neural Information Processing Systems 29, NIPS},
  2016.

\bibitem[Tao et~al.(2019)Tao, Xia, and Li]{tao2019}
Tao, Z., Xia, Q., and Li, Q.
\newblock {A new perspective in understanding of Adam-Type algorithms and
  beyond}.
\newblock \url{https://openreview.net/forum?id=SyxM51BYPB}, 2019.

\bibitem[Teixeira et~al.(2019)Teixeira, Tamersoy, Singh, and
  Kapoor]{teixeira2019adaloss}
Teixeira, B., Tamersoy, B., Singh, V., and Kapoor, A.
\newblock {Adaloss: Adaptive Loss Function for Landmark Localization}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1908.01070}{{\ttfamily \emph{1908.01070}}}, 2019.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{Tieleman2012}
Tieleman, T. and Hinton, G.
\newblock {Lecture 6.5---RMSProp: Divide the gradient by a running average of
  its recent magnitude}, 2012.

\bibitem[Tong et~al.(2019)Tong, Liang, and Bi]{tong2019calibrating}
Tong, Q., Liang, G., and Bi, J.
\newblock {Calibrating the Adaptive Learning Rate to Improve Convergence of
  ADAM}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1908.00700}{{\ttfamily \emph{1908.00700}}}, 2019.

\bibitem[Tran \& Cutkosky(2021)Tran and Cutkosky]{Tran2021}
Tran, H. and Cutkosky, A.
\newblock {Correcting Momentum with Second-order Information}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2103.03265}{{\ttfamily \emph{2103.03265}}}, 2021.

\bibitem[Tran \& Phong(2019)Tran and Phong]{Tran_2019}
Tran, P.~T. and Phong, L.~T.
\newblock {On the Convergence Proof of AMSGrad and a New Version}.
\newblock \emph{IEEE Access}, 7, 2019.

\bibitem[Tran et~al.(2020)Tran, Nguyen, and Tran-Dinh]{Tran2020}
Tran, T.~H., Nguyen, L.~M., and Tran-Dinh, Q.
\newblock {Shuffling Gradient-Based Methods with Momentum}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2011.11884}{{\ttfamily
  \emph{2011.11884}}}, 2020.

\bibitem[Tutunov et~al.(2020)Tutunov, Li, Cowen-Rivers, Wang, and
  Bou-Ammar]{tutunov2020compositional}
Tutunov, R., Li, M., Cowen-Rivers, A.~I., Wang, J., and Bou-Ammar, H.
\newblock {Compositional ADAM: An Adaptive Compositional Solver}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2002.03755}{{\ttfamily \emph{2002.03755}}}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock {Attention Is All You Need}.
\newblock In \emph{Advances in Neural Information Processing Systems 30, NIPS},
  2017.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste{-}Julien]{Vaswani2019a}
Vaswani, S., Mishkin, A., Laradji, I.~H., Schmidt, M., Gidel, G., and
  Lacoste{-}Julien, S.
\newblock {Painless Stochastic Gradient: Interpolation, Line-Search, and
  Convergence Rates}.
\newblock In \emph{Advances in Neural Information Processing Systems 32,
  NeurIPS}, 2019.

\bibitem[Vogels et~al.(2019)Vogels, Karimireddy, and Jaggi]{VogelsKJ19}
Vogels, T., Karimireddy, S.~P., and Jaggi, M.
\newblock {PowerSGD: Practical Low-Rank Gradient Compression for Distributed
  Optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems 32,
  NeurIPS}, 2019.

\bibitem[Wang \& Ye(2020)Wang and Ye]{Wang2020}
Wang, B. and Ye, Q.
\newblock {Stochastic Gradient Descent with Nonlinear Conjugate Gradient-Style
  Adaptive Momentum}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2012.02188}{{\ttfamily \emph{2012.02188}}}, 2020.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Nguyen, Bertozzi, Baraniuk, and
  Osher]{wang2020scheduled}
Wang, B., Nguyen, T.~M., Bertozzi, A.~L., Baraniuk, R.~G., and Osher, S.~J.
\newblock {Scheduled Restart Momentum for Accelerated Stochastic Gradient
  Descent}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2002.10583}{{\ttfamily \emph{2002.10583}}},
  2020{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Liu, Tang, Shang, Liu, Sun, and
  Jiao]{WangLTSLSJ19}
Wang, D., Liu, Y., Tang, W., Shang, F., Liu, H., Sun, Q., and Jiao, L.
\newblock {signADAM++: Learning Confidences for Deep Neural Networks}.
\newblock In \emph{International Conference on Data Mining Workshops, {ICDM}},
  2019{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Lu, Cheng, Tu, and
  Zhang]{Wang2019}
Wang, G., Lu, S., Cheng, Q., Tu, W., and Zhang, L.
\newblock {SAdam: A Variant of Adam for Strongly Convex Functions}.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR}}, 2020{\natexlab{b}}.

\bibitem[Wang \& Wiens(2020)Wang and Wiens]{wang2020adasgd}
Wang, J. and Wiens, J.
\newblock {AdaSGD: Bridging the gap between SGD and Adam}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2006.16541}{{\ttfamily
  \emph{2006.16541}}}, 2020.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Sun, and Xu]{wang2018hyperadam}
Wang, S., Sun, J., and Xu, Z.
\newblock {HyperAdam: A Learnable Task-Adaptive Adam for Network Training}.
\newblock In \emph{33rd {AAAI} Conference on Artificial Intelligence, {AAAI}},
  2019{\natexlab{b}}.

\bibitem[Wright(2020{\natexlab{a}})]{DeepMemory}
Wright, L.
\newblock {Deep Memory}.
\newblock
  \url{https://github.com/lessw2020/Best-Deep-Learning-Optimizers/tree/master/DeepMemory},
  2020{\natexlab{a}}.

\bibitem[Wright(2020{\natexlab{b}})]{Ranger}
Wright, L.
\newblock {Ranger}.
\newblock \url{https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer},
  2020{\natexlab{b}}.

\bibitem[Wu et~al.(2018)Wu, Ward, and Bottou]{wu18}
Wu, X., Ward, R., and Bottou, L.
\newblock {WNGrad: Learn the Learning Rate in Gradient Descent}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1803.02865}{{\ttfamily \emph{1803.02865}}}, 2018.

\bibitem[Xie et~al.(2019)Xie, Koyejo, Gupta, and Lin]{xie2019local}
Xie, C., Koyejo, O., Gupta, I., and Lin, H.
\newblock {Local AdaAlter: Communication-Efficient Stochastic Gradient Descent
  with Adaptive Learning Rates}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1911.09030}{{\ttfamily \emph{1911.09030}}}, 2019.

\bibitem[Xie et~al.(2020)Xie, Wang, Zhang, Sato, and Sugiyama]{Xie2020}
Xie, Z., Wang, X., Zhang, H., Sato, I., and Sugiyama, M.
\newblock {Adai: Separating the Effects of Adaptive Learning Rate and Momentum
  Inertia}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.15815}{{\ttfamily \emph{2006.15815}}}, 2020.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{Xing2018}
Xing, C., Arpit, D., Tsirigotis, C., and Bengio, Y.
\newblock {A Walk with SGD}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1802.08770}{{\ttfamily \emph{1802.08770}}}, 2018.

\bibitem[Xu(2020)]{xu2020momentumbased}
Xu, Y.
\newblock {Momentum-based variance-reduced proximal stochastic gradient method
  for composite nonconvex stochastic optimization}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2006.00425}{{\ttfamily
  \emph{2006.00425}}}, 2020.

\bibitem[Yang et~al.(2020)Yang, Xu, Li, Wen, and Chen]{yang2020structured}
Yang, M., Xu, D., Li, Y., Wen, Z., and Chen, M.
\newblock {Structured Stochastic Quasi-Newton Methods for Large-Scale
  Optimization Problems}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.09606}{{\ttfamily \emph{2006.09606}}}, 2020.

\bibitem[Yao et~al.(2020)Yao, Gholami, Shen, Keutzer, and
  Mahoney]{yao2020adahessian}
Yao, Z., Gholami, A., Shen, S., Keutzer, K., and Mahoney, M.~W.
\newblock {ADAHESSIAN: An Adaptive Second Order Optimizer for Machine
  Learning}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2006.00719}{{\ttfamily \emph{2006.00719}}}, 2020.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
You, Y., Gitman, I., and Ginsburg, B.
\newblock {Large Batch Training of Convolutional Networks}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/1708.03888}{{\ttfamily
  \emph{1708.03888}}}, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock {Large Batch Optimization for Deep Learning: Training BERT in 76
  minutes}.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR}}, 2020.

\bibitem[Yuan \& Gao(2020)Yuan and Gao]{Yuan2020}
Yuan, W. and Gao, K.-X.
\newblock {EAdam Optimizer: How $\epsilon$ Impact Adam}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/2011.02150}{{\ttfamily
  \emph{2011.02150}}}, 2020.

\bibitem[Yue et~al.(2020)Yue, Nouiehed, and Kontar]{Yue2020}
Yue, X., Nouiehed, M., and Kontar, R.~A.
\newblock {SALR: Sharpness-aware Learning Rates for Improved Generalization}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2011.05348}{{\ttfamily \emph{2011.05348}}}, 2020.

\bibitem[Yun et~al.(2019)Yun, Lozano, and Yang]{yun2019stochastic}
Yun, J., Lozano, A.~C., and Yang, E.
\newblock {Stochastic Gradient Methods with Block Diagonal Matrix Adaptation}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1905.10757}{{\ttfamily \emph{1905.10757}}}, 2019.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{ZaheerRSKK18}
Zaheer, M., Reddi, S.~J., Sachan, D.~S., Kale, S., and Kumar, S.
\newblock {Adaptive Methods for Nonconvex Optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems 31,
  NeurIPS}, 2018.

\bibitem[Zeiler(2012)]{Zeiler2012}
Zeiler, M.~D.
\newblock {ADADELTA: An Adaptive Learning Rate Method}. \emph{arXiv}\emph{
  preprint:} \href{http://arxiv.org/abs/1212.5701}{{\ttfamily
  \emph{1212.5701}}}, 2012.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and Grosse]{Zhang2018}
Zhang, G., Sun, S., Duvenaud, D., and Grosse, R.
\newblock {Noisy Natural Gradient as Variational Inference}.
\newblock In \emph{35th International Conference on Machine Learning, ICML},
  2018.

\bibitem[Zhang \& Gouza(2018)Zhang and Gouza]{zhang2018gadam}
Zhang, J. and Gouza, F.~B.
\newblock {GADAM: Genetic-Evolutionary ADAM for Deep Neural Network
  Optimization}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1805.07500}{{\ttfamily \emph{1805.07500}}}, 2018.

\bibitem[Zhang \& Mitliagkas(2019)Zhang and Mitliagkas]{ZhangMR17}
Zhang, J. and Mitliagkas, I.
\newblock {YellowFin and the Art of Momentum Tuning}.
\newblock In \emph{Machine Learning and Systems, MLSys}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S.~J., Kumar, S., and
  Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Lucas, Hinton, and Ba]{Zhang2019a}
Zhang, M.~R., Lucas, J., Hinton, G., and Ba, J.
\newblock {Lookahead Optimizer: k steps forward, 1 step back}.
\newblock \emph{Advances in Neural Information Processing Systems 32, NeurIPS},
  2019.

\bibitem[Zhang et~al.(2017{\natexlab{a}})Zhang, Ma, Li, and
  Wu]{zhang2017normalized}
Zhang, Z., Ma, L., Li, Z., and Wu, C.
\newblock {Normalized Direction-preserving Adam}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1709.04546}{{\ttfamily \emph{1709.04546}}},
  2017{\natexlab{a}}.

\bibitem[Zhang et~al.(2017{\natexlab{b}})Zhang, Wu, and Wang]{Zhang2017}
Zhang, Z., Wu, Y., and Wang, G.
\newblock {BPGrad: Towards Global Optimality in Deep Learning via Branch and
  Pruning}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1711.06959}{{\ttfamily \emph{1711.06959}}},
  2017{\natexlab{b}}.

\bibitem[Zhao et~al.(2020)Zhao, Xie, and Li]{zhao2020stochastic}
Zhao, S.-Y., Xie, Y.-P., and Li, W.-J.
\newblock {Stochastic Normalized Gradient Descent with Momentum for Large Batch
  Training}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2007.13985}{{\ttfamily \emph{2007.13985}}}, 2020.

\bibitem[Zhou et~al.(2020)Zhou, Zheng, and Gao]{Zhou2020}
Zhou, B., Zheng, X., and Gao, J.
\newblock {On the Trend-corrected Variant of Adaptive Stochastic Optimization
  Methods}.
\newblock In \emph{International Joint Conference on Neural Networks, {IJCNN}},
  2020.

\bibitem[Zhou et~al.(2021{\natexlab{a}})Zhou, Huang, Cheng, Wang, and
  Liu]{Zhou2021}
Zhou, Y., Huang, K., Cheng, C., Wang, X., and Liu, X.
\newblock {FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive
  Optimizer by Strong Convexity}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2104.13790}{{\ttfamily \emph{2104.13790}}},
  2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2021{\natexlab{b}})Zhou, Li, and Banerjee]{Zhou2021a}
Zhou, Y., Li, X., and Banerjee, A.
\newblock {Noisy Truncated SGD: Optimization and Generalization}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2103.00075}{{\ttfamily \emph{2103.00075}}},
  2021{\natexlab{b}}.

\bibitem[Zhou et~al.(2019)Zhou, Zhang, Lu, Wang, Zhang, and
  Yu]{zhou2018adashift}
Zhou, Z., Zhang, Q., Lu, G., Wang, H., Zhang, W., and Yu, Y.
\newblock {AdaShift: Decorrelation and Convergence of Adaptive Learning Rate
  Methods}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR}}, 2019.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{Zhuang2020}
Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X.,
  and Duncan, J.~S.
\newblock {AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed
  Gradients}.
\newblock In \emph{Advances in Neural Information Processing Systems 33,
  NeurIPS}, 2020.

\bibitem[Ziyin et~al.(2020)Ziyin, Wang, and Ueda]{ziyin2020laprop}
Ziyin, L., Wang, Z.~T., and Ueda, M.
\newblock {LaProp: a Better Way to Combine Momentum with Adaptive Gradient}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2002.04839}{{\ttfamily \emph{2002.04839}}}, 2020.

\end{thebibliography}
