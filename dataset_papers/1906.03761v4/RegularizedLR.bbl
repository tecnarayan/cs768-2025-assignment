\begin{thebibliography}{10}

\bibitem{abbasi2019performance}
Ehsan Abbasi, Fariborz Salehi, and Babak Hassibi.
\newblock Performance analysis of convex data detection in mimo.
\newblock In {\em ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 4554--4558. IEEE, 2019.

\bibitem{atitallah2017ber}
Ismail~Ben Atitallah, Christos Thrampoulidis, Abla Kammoun, Tareq~Y
  Al-Naffouri, Babak Hassibi, and Mohamed-Slim Alouini.
\newblock Ber analysis of regularized least squares for bpsk recovery.
\newblock In {\em 2017 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4262--4266. IEEE, 2017.

\bibitem{boyd1987evaluating}
Carl~R Boyd, Mary~Ann Tolson, and Wayne~S Copes.
\newblock Evaluating trauma care: the triss method. trauma score and the injury
  severity score.
\newblock {\em The Journal of trauma}, 27(4):370--378, 1987.

\bibitem{bunea2008honest}
Florentina Bunea et~al.
\newblock Honest variable selection in linear and logistic regression models
  via l1 and l1+ l2 penalization.
\newblock {\em Electronic Journal of Statistics}, 2:1153--1194, 2008.

\bibitem{candes2018phase}
Emmanuel~J Cand{\`e}s and Pragya Sur.
\newblock The phase transition for the existence of the maximum likelihood
  estimate in high-dimensional logistic regression.
\newblock {\em arXiv preprint arXiv:1804.09753}, 2018.

\bibitem{dhifallah2018phase}
Oussama Dhifallah, Christos Thrampoulidis, and Yue~M Lu.
\newblock Phase retrieval via polytope optimization: Geometry, phase
  transitions, and new algorithms.
\newblock {\em arXiv preprint arXiv:1805.09555}, 2018.

\bibitem{donoho2016high}
David Donoho and Andrea Montanari.
\newblock High dimensional robust m-estimation: Asymptotic variance via
  approximate message passing.
\newblock {\em Probability Theory and Related Fields}, 166(3-4):935--969, 2016.

\bibitem{el2013robust}
Noureddine El~Karoui, Derek Bean, Peter~J Bickel, Chinghway Lim, and Bin Yu.
\newblock On robust regression with high-dimensional predictors.
\newblock {\em Proceedings of the National Academy of Sciences},
  110(36):14557--14562, 2013.

\bibitem{friedman2010regularization}
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock {\em Journal of statistical software}, 33(1):1, 2010.

\bibitem{gordon1985some}
Yehoram Gordon.
\newblock Some inequalities for gaussian processes and applications.
\newblock {\em Israel Journal of Mathematics}, 50(4):265--289, 1985.

\bibitem{hosmer2013applied}
David~W Hosmer~Jr, Stanley Lemeshow, and Rodney~X Sturdivant.
\newblock {\em Applied logistic regression}, volume 398.
\newblock John Wiley \& Sons, 2013.

\bibitem{james2013introduction}
Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
\newblock {\em An introduction to statistical learning}, volume 112.
\newblock Springer, 2013.

\bibitem{jourani2014differential}
Abderrahim Jourani, Lionel Thibault, and Dariusz Zagrodny.
\newblock Differential properties of the moreau envelope.
\newblock {\em Journal of Functional Analysis}, 266(3):1185--1237, 2014.

\bibitem{kakade2010learning}
Sham Kakade, Ohad Shamir, Karthik Sindharan, and Ambuj Tewari.
\newblock Learning exponential families in high-dimensions: Strong convexity
  and sparsity.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 381--388, 2010.

\bibitem{king2001logistic}
Gary King and Langche Zeng.
\newblock Logistic regression in rare events data.
\newblock {\em Political analysis}, 9(2):137--163, 2001.

\bibitem{koh2007interior}
Kwangmoo Koh, Seung-Jean Kim, and Stephen Boyd.
\newblock An interior-point method for large-scale l1-regularized logistic
  regression.
\newblock {\em Journal of Machine learning research}, 8(Jul):1519--1555, 2007.

\bibitem{krishnapuram2005sparse}
Balaji Krishnapuram, Lawrence Carin, Mario~AT Figueiredo, and Alexander~J
  Hartemink.
\newblock Sparse multinomial logistic regression: Fast algorithms and
  generalization bounds.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  27(6):957--968, 2005.

\bibitem{lehmann2006testing}
Erich~L Lehmann and Joseph~P Romano.
\newblock {\em Testing statistical hypotheses}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{miolane2018distribution}
L{\'e}o Miolane and Andrea Montanari.
\newblock The distribution of the lasso: Uniform control over sparse balls and
  adaptive parameter tuning.
\newblock {\em arXiv preprint arXiv:1811.01212}, 2018.

\bibitem{nelder1972generalized}
John~Ashworth Nelder and Robert~WM Wedderburn.
\newblock Generalized linear models.
\newblock {\em Journal of the Royal Statistical Society: Series A (General)},
  135(3):370--384, 1972.

\bibitem{oymak2017universality}
Samet Oymak and Joel~A Tropp.
\newblock Universality laws for randomized dimension reduction, with
  applications.
\newblock {\em Information and Inference: A Journal of the IMA}, 7(3):337--446,
  2017.

\bibitem{panahi2017universal}
Ashkan Panahi and Babak Hassibi.
\newblock A universal analysis of large-scale regularized least squares
  solutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3381--3390, 2017.

\bibitem{salehi2018learning}
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi.
\newblock Learning without the phase: Regularized phasemax achieves optimal
  sample complexity.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8641--8652, 2018.

\bibitem{salehi2018precise}
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi.
\newblock A precise analysis of phasemax in phase retrieval.
\newblock In {\em 2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 976--980. IEEE, 2018.

\bibitem{shevade2003simple}
Shirish~Krishnaj Shevade and S~Sathiya Keerthi.
\newblock A simple and efficient algorithm for gene selection using sparse
  logistic regression.
\newblock {\em Bioinformatics}, 19(17):2246--2253, 2003.

\bibitem{stojnic2013framework}
Mihailo Stojnic.
\newblock A framework to characterize performance of lasso algorithms.
\newblock {\em arXiv preprint arXiv:1303.7291}, 2013.

\bibitem{sur2018modern}
Pragya Sur and Emmanuel~J Cand{\`e}s.
\newblock A modern maximum-likelihood theory for high-dimensional logistic
  regression.
\newblock {\em arXiv preprint arXiv:1803.06964}, 2018.

\bibitem{sur2017likelihood}
Pragya Sur, Yuxin Chen, and Emmanuel~J Cand{\`e}s.
\newblock The likelihood ratio test in high-dimensional logistic regression is
  asymptotically a rescaled chi-square.
\newblock {\em Probability Theory and Related Fields}, pages 1--72, 2017.

\bibitem{thrampoulidis2016recovering}
Christos Thrampoulidis.
\newblock {\em Recovering structured signals in high dimensions via non-smooth
  convex optimization: Precise performance analysis}.
\newblock PhD thesis, California Institute of Technology, 2016.

\bibitem{thrampoulidis2018precise}
Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi.
\newblock Precise error analysis of regularized $ m $-estimators in high
  dimensions.
\newblock {\em IEEE Transactions on Information Theory}, 64(8):5592--5628,
  2018.

\bibitem{thrampoulidis2015regularized}
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi.
\newblock Regularized linear regression: A precise analysis of the estimation
  error.
\newblock In {\em Conference on Learning Theory}, pages 1683--1709, 2015.

\bibitem{thrampoulidis2019simple}
Christos Thrampoulidis, Ilias Zadik, and Yury Polyanskiy.
\newblock A simple bound on the ber of the map decoder for massive mimo
  systems.
\newblock {\em arXiv preprint arXiv:1903.03949}, 2019.

\bibitem{tibshirani1996regression}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 267--288, 1996.

\bibitem{tu1996advantages}
Jack~V Tu.
\newblock Advantages and disadvantages of using artificial neural networks
  versus logistic regression for predicting medical outcomes.
\newblock {\em Journal of clinical epidemiology}, 49(11):1225--1231, 1996.

\bibitem{van2008high}
Sara~A Van~de Geer et~al.
\newblock High-dimensional generalized linear models and the lasso.
\newblock {\em The Annals of Statistics}, 36(2):614--645, 2008.

\bibitem{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock {\em Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge University Press, 2018.

\end{thebibliography}
