\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, Luo, Neyshabur, and
  Schapire]{agarwal2017corralling}
A.~Agarwal, H.~Luo, B.~Neyshabur, and R.~E. Schapire.
\newblock Corralling a band of bandit algorithms.
\newblock In \emph{Conference on Learning Theory}, pages 12--38. PMLR, 2017.

\bibitem[Arbel et~al.(2019)Arbel, Gretton, Li, and Mont{\'u}far]{Arbel:2019b}
M.~Arbel, A.~Gretton, W.~Li, and G.~Mont{\'u}far.
\newblock Kernelized wasserstein natural gradient.
\newblock \emph{arXiv preprint arXiv:1910.09652}, 2019.

\bibitem[Audibert et~al.(2007)Audibert, Munos, and Szepesv{\'{a}}ri]{audibert}
J.~Audibert, R.~Munos, and C.~Szepesv{\'{a}}ri.
\newblock Tuning bandit algorithms in stochastic environments.
\newblock In \emph{Algorithmic Learning Theory, 18th International Conference,
  {ALT} 2007, Sendai, Japan, October 1-4, 2007, Proceedings}, volume 4754 of
  \emph{Lecture Notes in Computer Science}, pages 150--165. Springer, 2007.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{osband}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML}}, volume~70 of \emph{Proceedings of Machine Learning
  Research}, pages 263--272, 2017.

\bibitem[Badia et~al.(2020)Badia, Piot, Kapturowski, Sprechmann, Vitvitskyi,
  Guo, and Blundell]{agent57}
A.~P. Badia, B.~Piot, S.~Kapturowski, P.~Sprechmann, A.~Vitvitskyi, D.~Guo, and
  C.~Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}. ICML, 2020.

\bibitem[Ball et~al.(2020)Ball, Parker-Holder, Pacchiano, Choromanski, and
  Roberts]{Ball:20_RP1}
P.~Ball, J.~Parker-Holder, A.~Pacchiano, K.~Choromanski, and S.~Roberts.
\newblock Ready policy one: World building through active learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 591--601, 13--18 Jul 2020.

\bibitem[Ball and Roberts(2021)]{Ball:21_offcon}
P.~J. Ball and S.~J. Roberts.
\newblock Offcon$^3$: What is state of the art anyway?, 2021.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  TB, Muldal, Heess, and Lillicrap]{Barth-Maron:18_D4PG}
G.~Barth-Maron, M.~W. Hoffman, D.~Budden, W.~Dabney, D.~Horgan, D.~TB,
  A.~Muldal, N.~Heess, and T.~Lillicrap.
\newblock Distributional policy gradients.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Bartlett and Tewari(2012)]{bartlett_1}
P.~L. Bartlett and A.~Tewari.
\newblock {REGAL:} {A} regularization based algorithm for reinforcement
  learning in weakly communicating {MDP}s.
\newblock \emph{CoRR}, abs/1205.2661, 2012.

\bibitem[Bellemare et~al.(2012)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare:2012_arcade}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{CoRR}, abs/1207.4708, 2012.
\newblock URL \url{http://arxiv.org/abs/1207.4708}.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{Bellemare:17_distRL}
M.~G. Bellemare, W.~Dabney, and R.~Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, page 449–458. JMLR.org, 2017.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{Brockman:16_gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock \emph{CoRR}, 2016.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{Cesa-Bianchi:2006_bandit}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock \emph{Prediction, learning, and games.}
\newblock Cambridge University Press, 2006.

\bibitem[Ciosek et~al.(2019)Ciosek, Vuong, Loftin, and
  Hofmann]{Ciosek:2019_OAC}
K.~Ciosek, Q.~Vuong, R.~Loftin, and K.~Hofmann.
\newblock Better exploration with optimistic actor-critic.
\newblock In \emph{Advances in Neural Information Processing Systems}. NeurIPS,
  2019.

\bibitem[Co-Reyes et~al.(2021)Co-Reyes, Miao, Peng, Le, Levine, Lee, and
  Faust]{evolving_algos}
J.~D. Co-Reyes, Y.~Miao, D.~Peng, Q.~V. Le, S.~Levine, H.~Lee, and A.~Faust.
\newblock Evolving reinforcement learning algorithms.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{Dabney:18_iqn}
W.~Dabney, G.~Ostrovski, D.~Silver, and R.~Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 1096--1105, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/dabney18a.html}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{Dabney:17_qrdqn}
W.~Dabney, M.~Rowland, M.~G. Bellemare, and R.~Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{AAAI}. AAAI, 2018{\natexlab{b}}.

\bibitem[{Filippi} et~al.(2010){Filippi}, {Cappé}, and {Garivier}]{filippi}
S.~{Filippi}, O.~{Cappé}, and A.~{Garivier}.
\newblock Optimism in reinforcement learning and kullback-leibler divergence.
\newblock In \emph{2010 48th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 115--122, 2010.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and Ortner]{fruit}
R.~Fruit, M.~Pirotta, A.~Lazaric, and R.~Ortner.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML}}, volume~80, pages 1573--1581, 2018.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{Fujimoto:2018_TD3}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{ICML}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{Haarnoja:2018_SAC}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{CoRR}, abs/1801.01290, 2018.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{Hafner:19_planet}
D.~Hafner, T.~Lillicrap, I.~Fischer, R.~Villegas, D.~Ha, H.~Lee, and
  J.~Davidson.
\newblock Learning latent dynamics for planning from pixels.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pages 2555--2565. PMLR,
  09--15 Jun 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/hafner19a.html}.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and
  Norouzi]{Hafner:20_dreamer}
D.~Hafner, T.~Lillicrap, J.~Ba, and M.~Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1lOTC4tDS}.

\bibitem[Hasselt(2010)]{Hasselt:2010_doubleQ}
H.~Hasselt.
\newblock Double q-learning.
\newblock In J.~Lafferty, C.~Williams, J.~Shawe-Taylor, R.~Zemel, and
  A.~Culotta, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~23, pages 2613--2621, 2010.

\bibitem[Huber(1964)]{Huber:1964_qrloss}
P.~J. Huber.
\newblock Robust estimation of a location parameter.
\newblock \emph{Annals of Mathematical Statistics}, 35\penalty0 (1):\penalty0
  73--101, 1964.
\newblock ISSN 0003-4851.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{ucrl2}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 1563–1600, Aug. 2010.
\newblock ISSN 1532-4435.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In S.~Vishwanathan, H.~Wallach, S.~Larochelle, K.~Grauman, and
  N.~Cesa-Bianchi, editors, \emph{Advances in Neural Information Processing},
  volume~31. Curran Associates, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020.

\bibitem[Kirsch et~al.(2020)Kirsch, van Steenkiste, and
  Schmidhuber]{Kirsch:2020_lilbitch}
L.~Kirsch, S.~van Steenkiste, and J.~Schmidhuber.
\newblock Improving generalization in meta reinforcement learning using learned
  objectives.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1evHerYPr}.

\bibitem[Kocsis and Szepesv{\'{a}}ri(2006)]{kocsis}
L.~Kocsis and C.~Szepesv{\'{a}}ri.
\newblock Bandit based {M}onte-{C}arlo planning.
\newblock In \emph{Machine Learning: {ECML} 2006, 17th European Conference on
  Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings},
  volume 4212 of \emph{Lecture Notes in Computer Science}, pages 282--293.
  Springer, 2006.

\bibitem[Laskin et~al.(2020{\natexlab{a}})Laskin, Lee, Stooke, Pinto, Abbeel,
  and Srinivas]{Laskin:20_rad}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 19884--19895. Curran Associates, Inc., 2020{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf}.

\bibitem[Laskin et~al.(2020{\natexlab{b}})Laskin, Srinivas, and
  Abbeel]{Laskin:20_curl}
M.~Laskin, A.~Srinivas, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock \emph{Proceedings of the 37th International Conference on Machine
  Learning, Vienna, Austria, PMLR 119}, 2020{\natexlab{b}}.
\newblock arXiv:2004.04136.

\bibitem[Lee et~al.(2020)Lee, Fischer, Liu, Guo, Lee, Canny, and
  Guadarrama]{Lee:20_pisac}
K.-H. Lee, I.~Fischer, A.~Liu, Y.~Guo, H.~Lee, J.~Canny, and S.~Guadarrama.
\newblock Predictive information accelerates learning in rl.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 11890--11901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/89b9e0a6f6d1505fe13dea0f18a2dcfa-Paper.pdf}.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{ICLR}, 2016.

\bibitem[Lin(1992)]{Lin:92_buffer}
L.-J. Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Mach. Learn.}, 8\penalty0 (3–4):\penalty0 293–321, May
  1992.
\newblock ISSN 0885-6125.
\newblock \doi{10.1007/BF00992699}.
\newblock URL \url{https://doi.org/10.1007/BF00992699}.

\bibitem[Ma et~al.(2019)Ma, Yu, and Satir]{Ma:2019_bs}
S.~Ma, J.~Y. Yu, and A.~Satir.
\newblock A scheme for dynamic risk-sensitive sequential decision making, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih:2015_dqn}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, S.~Petersen,
  C.~Beattie, A.~Sadik, I.~Antonoglou, H.~King, D.~Kumaran, D.~Wierstra,
  S.~Legg, and D.~Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, Feb. 2015.
\newblock ISSN 00280836.

\bibitem[Moskovitz et~al.(2021)Moskovitz, Arbel, Huszar, and
  Gretton]{Moskovitz:2020_WNG}
T.~Moskovitz, M.~Arbel, F.~Huszar, and A.~Gretton.
\newblock Efficient wasserstein natural gradients for reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}. ICLR,
  2021.

\bibitem[Oh et~al.(2020)Oh, Hessel, Czarnecki, Xu, van Hasselt, Singh, and
  Silver]{learnedPG}
J.~Oh, M.~Hessel, W.~M. Czarnecki, Z.~Xu, H.~van Hasselt, S.~Singh, and
  D.~Silver.
\newblock Discovering reinforcement learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 33}.
  NeurIPS, 2020.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{bootstrapped_dqn}
I.~Osband, C.~Blundell, A.~Pritzel, and B.~Van~Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29,
  pages 4026--4034, 2016.

\bibitem[Pacchiano et~al.(2020{\natexlab{a}})Pacchiano, Ball, Parker-Holder,
  Choromanski, and Roberts]{narl}
A.~Pacchiano, P.~Ball, J.~Parker-Holder, K.~Choromanski, and S.~Roberts.
\newblock On optimism in model-based reinforcement learning.
\newblock \emph{CoRR}, 2020{\natexlab{a}}.

\bibitem[Pacchiano et~al.(2020{\natexlab{b}})Pacchiano, Dann, Gentile, and
  Bartlett]{pacchiano2020regret}
A.~Pacchiano, C.~Dann, C.~Gentile, and P.~Bartlett.
\newblock Regret bound balancing and elimination for model selection in bandits
  and rl.
\newblock \emph{arXiv preprint arXiv:2012.13045}, 2020{\natexlab{b}}.

\bibitem[Pacchiano et~al.(2020{\natexlab{c}})Pacchiano, Parker-Holder, Tang,
  Choromanski, Choromanska, and Jordan]{Pacchiano:2020_BGRL}
A.~Pacchiano, J.~Parker-Holder, Y.~Tang, K.~Choromanski, A.~Choromanska, and
  M.~Jordan.
\newblock Learning to score behaviors for guided policy optimization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 7445--7454, 13--18 Jul 2020{\natexlab{c}}.

\bibitem[Pacchiano et~al.(2020{\natexlab{d}})Pacchiano, Phan, Abbasi-Yadkori,
  Rao, Zimmert, Lattimore, and Szepesvari]{pacchiano2020model}
A.~Pacchiano, M.~Phan, Y.~Abbasi-Yadkori, A.~Rao, J.~Zimmert, T.~Lattimore, and
  C.~Szepesvari.
\newblock Model selection in contextual stochastic bandit problems.
\newblock \emph{arXiv preprint arXiv:2003.01704}, 2020{\natexlab{d}}.

\bibitem[Parker{-}Holder et~al.(2020)Parker{-}Holder, Pacchiano, Choromanski,
  and Roberts]{parkerholder2020effective}
J.~Parker{-}Holder, A.~Pacchiano, K.~Choromanski, and S.~Roberts.
\newblock Effective diversity in population-based reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 34}.
  NeurIPS, 2020.

\bibitem[Penedones et~al.(2019)Penedones, Riquelme, Vincent, Maennel, Mann,
  Barreto, Gelly, and Neu]{adaptive_mc_td}
H.~Penedones, C.~Riquelme, D.~Vincent, H.~Maennel, T.~A. Mann, A.~Barreto,
  S.~Gelly, and G.~Neu.
\newblock Adaptive temporal-difference learning for policy evaluation with
  per-state uncertainty estimates.
\newblock In \emph{Advances in Neural Information Processing Systems}. NeurIPS,
  2019.

\bibitem[Rowland et~al.(2019)Rowland, Dadashi, Kumar, Munos, Bellemare, and
  Dabney]{Rowland:19_erdqn}
M.~Rowland, R.~Dadashi, S.~Kumar, R.~Munos, M.~G. Bellemare, and W.~Dabney.
\newblock Statistics and samples in distributional reinforcement learning,
  2019.

\bibitem[Schaul et~al.(2019)Schaul, Borsa, Ding, Szepesvari, Ostrovski, Dabney,
  and Osindero]{schaul2019adapting}
T.~Schaul, D.~Borsa, D.~Ding, D.~Szepesvari, G.~Ostrovski, W.~Dabney, and
  S.~Osindero.
\newblock Adapting behaviour for learning progress.
\newblock \emph{CoRR}, abs/1912.06910, 2019.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{dpg}
D.~Silver, G.~Lever, N.~Heess, T.~Degris, D.~Wierstra, and M.~Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, pages 387--395, 2014.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{Silver:2016_go}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~van~den Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, S.~Dieleman,
  D.~Grewe, J.~Nham, N.~Kalchbrenner, I.~Sutskever, T.~Lillicrap, M.~Leach,
  K.~Kavukcuoglu, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.
\newblock ISSN 0028-0836.

\bibitem[Sutton and Barto(2018)]{Sutton:98_rl}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock The MIT Press, second edition, 2018.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, de~Las~Casas,
  Budden, Abdolmaleki, Merel, Lefrancq, Lillicrap, and
  Riedmiller]{Tassa:20_dmc}
Y.~Tassa, Y.~Doron, A.~Muldal, T.~Erez, Y.~Li, D.~de~Las~Casas, D.~Budden,
  A.~Abdolmaleki, J.~Merel, A.~Lefrancq, T.~Lillicrap, and M.~Riedmiller.
\newblock Deepmind control suite, 2018.

\bibitem[Thrun and Schwartz(1993)]{Thrun:93_approx}
S.~Thrun and A.~Schwartz.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{In Proceedings of the Fourth Connectionist Models Summer
  School}. Erlbaum, 1993.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{Todorov:12_mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IROS}, pages 5026--5033. IEEE, 2012.

\bibitem[Tossou et~al.(2019)Tossou, Basu, and Dimitrakakis]{tossou}
A.~C.~Y. Tossou, D.~Basu, and C.~Dimitrakakis.
\newblock Near-optimal optimistic reinforcement learning using empirical
  bernstein inequalities.
\newblock \emph{CoRR}, abs/1905.12425, 2019.

\bibitem[Watkins and Dayan(1992)]{Watkins:92_Q}
C.~J. C.~H. Watkins and P.~Dayan.
\newblock Q-learning.
\newblock \emph{Machine Learning}, 8:\penalty0 279--292, 1992.

\bibitem[Yang and Wang(2020)]{yang2020reinforcement}
L.~Yang and M.~Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning}, pages
  10746--10756. PMLR, 2020.

\bibitem[Yarats et~al.(2021)Yarats, Kostrikov, and Fergus]{Yarats:21_drq}
D.~Yarats, I.~Kostrikov, and R.~Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=GY6-6sTvGaf}.

\bibitem[Zhang and Yao(2019)]{Zhang:2019_quota}
S.~Zhang and H.~Yao.
\newblock Quota: The quantile option architecture for reinforcement learning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33\penalty0 (01):\penalty0 5797--5804, 2019.
\newblock \doi{10.1609/aaai.v33i01.33015797}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/4527}.

\end{thebibliography}
