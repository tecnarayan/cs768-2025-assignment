\begin{thebibliography}{10}

\bibitem{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem{baluja2017adversarial}
Shumeet Baluja and Ian Fischer.
\newblock Adversarial transformation networks: Learning to generate adversarial
  examples.
\newblock {\em arXiv preprint arXiv:1703.09387}, 2017.

\bibitem{bassily2018exponential}
Raef Bassily, Mikhail Belkin, and Siyuan Ma.
\newblock On exponential convergence of sgd in non-convex over-parametrized
  learning.
\newblock {\em arXiv preprint arXiv:1811.02564}, 2018.

\bibitem{bishop1995training}
Chris~M Bishop.
\newblock Training with noise is equivalent to tikhonov regularization.
\newblock {\em Neural computation}, 7(1):108--116, 1995.

\bibitem{bowles2018gansfer}
Christopher Bowles, Roger Gunn, Alexander Hammers, and Daniel Rueckert.
\newblock Gansfer learning: Combining labelled and unlabelled data for gan
  based data augmentation.
\newblock {\em arXiv preprint arXiv:1811.10669}, 2018.

\bibitem{chen2019invariance}
Shuxiao Chen, Edgar Dobriban, and Jane~H Lee.
\newblock Invariance reduces variance: Understanding data augmentation in deep
  learning and beyond.
\newblock {\em arXiv preprint arXiv:1907.10905}, 2019.

\bibitem{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 113--123, 2019.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{dao2019kernel}
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De~Sa, and
  Christopher R{\'e}.
\newblock A kernel theory of modern data augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  1528--1537. PMLR, 2019.

\bibitem{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{fort2020deep}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
  Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{golatkar2019time}
Aditya~Sharad Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Time matters in regularizing deep networks: Weight decay and data
  augmentation affect early learning dynamics, matter little near convergence.
\newblock {\em Advances in Neural Information Processing Systems},
  32:10678--10688, 2019.

\bibitem{griffin2007caltech}
Gregory Griffin, Alex Holub, and Pietro Perona.
\newblock Caltech-256 object category dataset.
\newblock 2007.

\bibitem{hendrycks2019augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock {\em arXiv preprint arXiv:1912.02781}, 2019.

\bibitem{jackson2019style}
Philip~TG Jackson, Amir~Atapour Abarghouei, Stephen Bonner, Toby~P Breckon, and
  Boguslaw Obara.
\newblock Style augmentation: data augmentation via style randomization.
\newblock In {\em CVPR Workshops}, volume~6, pages 10--11, 2019.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:1806.07572}, 2018.

\bibitem{katharopoulos2018not}
Angelos Katharopoulos and Fran{\c{c}}ois Fleuret.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In {\em International conference on machine learning}, pages
  2525--2534. PMLR, 2018.

\bibitem{kaufman1987clustering}
L~Kaufman, PJ~Rousseeuw, and Y~Dodge.
\newblock Clustering by means of medoids in statistical data analysis based on
  the, 1987.

\bibitem{kim2020adjusting}
Byungju Kim and Junmo Kim.
\newblock Adjusting decision boundary for class imbalanced learning.
\newblock {\em IEEE Access}, 8:81674--81685, 2020.

\bibitem{kuchnik2018efficient}
Michael Kuchnik and Virginia Smith.
\newblock Efficient augmentation via data subsampling.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em NeurIPS}, 2019.

\bibitem{lemley2017smart}
Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran.
\newblock Smart augmentation learning an optimal data augmentation strategy.
\newblock {\em Ieee Access}, 5:5858--5869, 2017.

\bibitem{li1993performance}
Fu~Li, Hui Liu, and Richard~J Vaccaro.
\newblock Performance analysis for doa estimation algorithms: unification,
  simplification, and observations.
\newblock {\em IEEE Transactions on Aerospace and Electronic Systems},
  29(4):1170--1184, 1993.

\bibitem{luo2020data}
Calvin Luo, Hossein Mobahi, and Samy Bengio.
\newblock Data augmentation via structured adversarial perturbations.
\newblock {\em arXiv preprint arXiv:2011.03010}, 2020.

\bibitem{minoux1978accelerated}
Michel Minoux.
\newblock Accelerated greedy algorithms for maximizing submodular set
  functions.
\newblock In {\em Optimization techniques}, pages 234--243. Springer, 1978.

\bibitem{mirsky1960symmetric}
Leon Mirsky.
\newblock Symmetric gauge functions and unitarily invariant norms.
\newblock {\em The quarterly journal of mathematics}, 11(1):50--59, 1960.

\bibitem{mirza2014conditional}
Mehdi Mirza and Simon Osindero.
\newblock Conditional generative adversarial nets.
\newblock {\em arXiv preprint arXiv:1411.1784}, 2014.

\bibitem{mirzasoleiman2015lazier}
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondr{\'a}k,
  and Andreas Krause.
\newblock Lazier than lazy greedy.
\newblock In {\em Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem{mirzasoleiman2020coresets}
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In {\em International Conference on Machine Learning}, pages
  6950--6960. PMLR, 2020.

\bibitem{mirzasoleiman2013distributed}
Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause.
\newblock Distributed submodular maximization: Identifying representative
  elements in massive data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2049--2057, 2013.

\bibitem{oymak2019generalization}
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi.
\newblock Generalization guarantees for neural networks via harnessing the
  low-rank structure of the jacobian.
\newblock {\em arXiv preprint arXiv:1906.05392}, 2019.

\bibitem{rajput2019does}
Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris
  Papailiopoulos.
\newblock Does data augmentation lead to positive margin?
\newblock In {\em International Conference on Machine Learning}, pages
  5321--5330. PMLR, 2019.

\bibitem{ratner2017learning}
Alexander~J Ratner, Henry~R Ehrenberg, Zeshan Hussain, Jared Dunnmon, and
  Christopher R{\'e}.
\newblock Learning to compose domain-specific transformations for data
  augmentation.
\newblock {\em Advances in neural information processing systems}, 30:3239,
  2017.

\bibitem{shen2022data}
Ruoqi Shen, S{\'e}bastien Bubeck, and Suriya Gunasekar.
\newblock Data augmentation as feature manipulation: a story of desert cows and
  grass cows.
\newblock {\em arXiv preprint arXiv:2203.01572}, 2022.

\bibitem{shorten2019survey}
Connor Shorten and Taghi~M Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock {\em Journal of Big Data}, 6(1):1--48, 2019.

\bibitem{stewart1979note}
GW~Stewart.
\newblock A note on the perturbation of singular values.
\newblock {\em Linear Algebra and Its Applications}, 28:213--216, 1979.

\bibitem{wager2013dropout}
Stefan Wager, Sida Wang, and Percy Liang.
\newblock Dropout training as adaptive regularization.
\newblock {\em arXiv preprint arXiv:1307.1493}, 2013.

\bibitem{wedin1972perturbation}
Per-{\AA}ke Wedin.
\newblock Perturbation bounds in connection with singular value decomposition.
\newblock {\em BIT Numerical Mathematics}, 12(1):99--111, 1972.

\bibitem{weyl1912asymptotic}
Hermann Weyl.
\newblock The asymptotic distribution law of the eigenvalues of linear partial
  differential equations (with an application to the theory of cavity
  radiation).
\newblock {\em mathematical annals}, 71(4):441--479, 1912.

\bibitem{wu2020generalization}
Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher R{\'e}.
\newblock On the generalization effects of linear transformations in data
  augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  10410--10420. PMLR, 2020.

\bibitem{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em British Machine Vision Conference 2016}. British Machine
  Vision Association, 2016.

\end{thebibliography}
