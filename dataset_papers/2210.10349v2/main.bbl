% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Salas et~al.(2011)Salas, Gelbukh, Calvo, and
  Soria]{garcia2011automatic}
H.~A.~G. Salas, A.~F. Gelbukh, H.~Calvo, and F.~G. Soria, ``Automatic music
  composition with simple probabilistic generative grammars,'' \emph{Polibits},
  vol.~44, pp. 59--65, 2011.

\bibitem[Yanchenko and Mukherjee(2017)]{yanchenko2017classical}
A.~K. Yanchenko and S.~Mukherjee, ``Classical music composition using state
  space models,'' \emph{arXiv preprint arXiv:1708.03822}, 2017.

\bibitem[Huang et~al.(2018)Huang, Vaswani, Uszkoreit, Simon, Hawthorne,
  Shazeer, Dai, Hoffman, Dinculescu, and Eck]{huang2018music}
C.-Z.~A. Huang, A.~Vaswani, J.~Uszkoreit, I.~Simon, C.~Hawthorne, N.~Shazeer,
  A.~M. Dai, M.~D. Hoffman, M.~Dinculescu, and D.~Eck, ``Music transformer:
  Generating music with long-term structure,'' in \emph{Proceedings of
  International Conference on Learning Representations (ICLR)}, 2018.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' \emph{OpenAI}, 2018.

\bibitem[Li et~al.(2021)Li, Tang, Zhao, and Wen]{ijcai2021p612}
J.~Li, T.~Tang, W.~X. Zhao, and J.-R. Wen, ``Pretrained language model for text
  generation: A survey,'' in \emph{Proceedings of International Joint
  Conference on Artificial Intelligence (IJCAI)}, 2021, pp. 4492--4499.

\bibitem[Donahue et~al.(2019)Donahue, Mao, Li, Cottrell, and
  McAuley]{donahue2019lakhnes}
C.~Donahue, H.~H. Mao, Y.~E. Li, G.~W. Cottrell, and J.~J. McAuley, ``Lakhnes:
  Improving multi-instrumental music generation with cross-domain
  pre-training,'' in \emph{Proceedings of International Society for Music
  Information Retrieval Conference (ISMIR)}, 2019, pp. 685--692.

\bibitem[Muhamed et~al.(2021)Muhamed, Li, Shi, Yaddanapudi, Chi, Jackson,
  Suresh, Lipton, and Smola]{muhamed2021symbolic}
A.~Muhamed, L.~Li, X.~Shi, S.~Yaddanapudi, W.~Chi, D.~Jackson, R.~Suresh, Z.~C.
  Lipton, and A.~J. Smola, ``Symbolic music generation with transformer-gans,''
  in \emph{Proceedings of the AAAI Conference on Artificial Intelligence
  (AAAI)}, vol.~35, 2021, pp. 408--417.

\bibitem[Roberts et~al.(2018)Roberts, Engel, Raffel, Hawthorne, and
  Eck]{roberts2018hierarchical}
A.~Roberts, J.~Engel, C.~Raffel, C.~Hawthorne, and D.~Eck, ``A hierarchical
  latent vector model for learning long-term structure in music,'' in
  \emph{Proceedings of International Conference on Machine Learning (ICML)},
  vol.~80, 2018, pp. 4364--4373.

\bibitem[Ju et~al.(2021)Ju, Lu, Tan, Wang, Zhang, Wu, Zhang, Li, Qin, and
  Liu]{ju2021telemelody}
Z.~Ju, P.~Lu, X.~Tan, R.~Wang, C.~Zhang, S.~Wu, K.~Zhang, X.~Li, T.~Qin, and
  T.-Y. Liu, ``Telemelody: Lyric-to-melody generation with a template-based
  two-stage method,'' \emph{arXiv preprint arXiv:2109.09617}, 2021.

\bibitem[Ren et~al.(2020)Ren, He, Tan, Qin, Zhao, and Liu]{ren2020popmag}
Y.~Ren, J.~He, X.~Tan, T.~Qin, Z.~Zhao, and T.-Y. Liu, ``Popmag: Pop music
  accompaniment generation,'' in \emph{Proceedings of ACM International
  Conference on Multimedia (MM)}, 2020, pp. 1198--1206.

\bibitem[Sheng et~al.(2021)Sheng, Song, Tan, Ren, Ye, Zhang, and
  Qin]{sheng2021songmass}
Z.~Sheng, K.~Song, X.~Tan, Y.~Ren, W.~Ye, S.~Zhang, and T.~Qin, ``Songmass:
  Automatic song writing with pre-training and alignment constraint,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  2021, pp. 13\,798--13\,805.

\bibitem[Lin et~al.(2021)Lin, Wang, Liu, and Qiu]{lin2021survey}
T.~Lin, Y.~Wang, X.~Liu, and X.~Qiu, ``A survey of transformers,'' \emph{arXiv
  preprint arXiv:2106.04554}, 2021.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019xl}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~Le, and R.~Salakhutdinov,
  ``Transformer-{XL}: Attentive language models beyond a fixed-length
  context,'' in \emph{Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2019, pp. 2978--2988.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan, ``Longformer: The long-document
  transformer,'' \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret, ``Transformers are
  {RNN}s: Fast autoregressive transformers with linear attention,'' in
  \emph{Proceedings of International Conference on Machine Learning (ICML)},
  2020, pp. 5156--5165.

\bibitem[Ye et~al.(2019)Ye, Guo, Gan, Qiu, and Zhang]{ye2019bp}
Z.~Ye, Q.~Guo, Q.~Gan, X.~Qiu, and Z.~Zhang, ``Bp-transformer: Modelling
  long-range context via binary partitioning,'' \emph{arXiv preprint
  arXiv:1911.04070}, 2019.

\bibitem[Farbood and Sch{\"{o}}ner(2001)]{farbood2001analysis}
M.~M. Farbood and B.~Sch{\"{o}}ner, ``Analysis and synthesis of
  palestrina-style counterpoint using markov chains,'' in \emph{Proceedings of
  International Computer Music Conference (ICMC)}, 2001.

\bibitem[Allan(2002)]{allan2002harmonising}
M.~Allan, ``Harmonising chorales in the style of johann sebastian bach,''
  \emph{Master's}, 2002.

\bibitem[Hsiao et~al.(2021)Hsiao, Liu, Yeh, and Yang]{hsiao2021compound}
W.-Y. Hsiao, J.-Y. Liu, Y.-C. Yeh, and Y.-H. Yang, ``Compound word transformer:
  Learning to compose full-song music over dynamic directed hypergraphs,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  vol.~35, 2021, pp. 178--186.

\bibitem[Zeng et~al.(2021)Zeng, Tan, Wang, Ju, Qin, and Liu]{musicbert}
M.~Zeng, X.~Tan, R.~Wang, Z.~Ju, T.~Qin, and T.-Y. Liu, ``{M}usic{BERT}:
  Symbolic music understanding with large-scale pre-training,'' in
  \emph{Proceedings of Findings of the Association for Computational
  Linguistics (ACL Findings)}, 2021, pp. 791--800.

\bibitem[Huang and Yang(2020)]{huang2020pop}
Y.-S. Huang and Y.-H. Yang, ``Pop music transformer: Beat-based modeling and
  generation of expressive pop piano compositions,'' in \emph{Proceedings of
  ACM International Conference on Multimedia (MM)}, 2020, pp. 1180--1188.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Hillier, and
  Lillicrap]{rae2019compressive}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, C.~Hillier, and T.~P. Lillicrap,
  ``Compressive transformers for long-range sequence modelling,'' in
  \emph{Proceedings of International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Ding et~al.(2021)Ding, Shang, Wang, Sun, Tian, Wu, and
  Wang]{ding2021ernie}
S.~Ding, J.~Shang, S.~Wang, Y.~Sun, H.~Tian, H.~Wu, and H.~Wang,
  ``{ERNIE}-{D}oc: A retrospective long-document modeling transformer,'' in
  \emph{Proceedings of Annual Meeting of the Association for Computational
  Linguistics and International Joint Conference on Natural Language Processing
  (ACL-IJCNLP)}, 2021, pp. 2914--2927.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever, ``Generating long sequences
  with sparse transformers,'' \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed]{zaheer2020big}
M.~Zaheer, G.~Guruganesh, K.~A. Dubey, J.~Ainslie, C.~Alberti, S.~Ontanon,
  P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed, ``Big bird: Transformers
  for longer sequences,'' in \emph{Proceedings of Advances in Neural
  Information Processing Systems (NeurIPS)}, vol.~33, 2020, pp.
  17\,283--17\,297.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar2018imagetfm}
N.~Parmar, A.~Vaswani, J.~Uszkoreit, L.~Kaiser, N.~Shazeer, A.~Ku, and D.~Tran,
  ``Image transformer,'' in \emph{Proceedings of International Conference on
  Machine Learning (ICML)}, vol.~80, 2018, pp. 4055--4064.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{Kitaev2020Reformer}
N.~Kitaev, L.~Kaiser, and A.~Levskaya, ``Reformer: The efficient transformer,''
  in \emph{Proceedings of International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{routingtfm}
A.~Roy, M.~Saffar, A.~Vaswani, and D.~Grangier, ``Efficient content-based
  sparse attention with routing transformers,'' \emph{Transactions of the
  Association for Computational Linguistics (TACL)}, vol.~9, pp. 53--68, 2021.

\bibitem[Li et~al.(2020)Li, Meng, Zhou, Han, Wu, and Li]{sac}
X.~Li, Y.~Meng, M.~Zhou, Q.~Han, F.~Wu, and J.~Li, ``Sac: Accelerating and
  structuring self-attention via sparse adaptive connection,'' in
  \emph{Proceedings of Advances in Neural Information Processing Systems
  (NeurIPS)}, H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  Eds., vol.~33, 2020, pp. 16\,997--17\,008.

\bibitem[Tay et~al.(2020)Tay, Bahri, Yang, Metzler, and Juan]{ssa}
Y.~Tay, D.~Bahri, L.~Yang, D.~Metzler, and D.-C. Juan, ``Sparse {S}inkhorn
  attention,'' in \emph{Proceedings of International Conference on Machine
  Learning (ICML)}, vol. 119, 2020, pp. 9438--9447.

\bibitem[Wu et~al.(2021)Wu, Wu, Qi, Jiao, Jiang, Huang, and Xie]{wu2021smart}
C.~Wu, F.~Wu, T.~Qi, B.~Jiao, D.~Jiang, Y.~Huang, and X.~Xie, ``Smart bird:
  Learnable sparse attention for efficient and effective transformer,''
  \emph{arXiv preprint arXiv:2108.09193}, 2021.

\bibitem[Choromanski et~al.(2020{\natexlab{a}})Choromanski, Likhosherstov,
  Dohan, Song, Gane, Sarlos, Hawkins, Davis, Belanger, Colwell,
  et~al.]{choromanski2020masked}
K.~Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos,
  P.~Hawkins, J.~Davis, D.~Belanger, L.~Colwell \emph{et~al.}, ``Masked
  language modeling for proteins via linearly scalable long-context
  transformers,'' \emph{arXiv preprint arXiv:2006.03555}, 2020.

\bibitem[Choromanski et~al.(2020{\natexlab{b}})Choromanski, Likhosherstov,
  Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser,
  et~al.]{choromanski2020rethinking}
K.~Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos,
  P.~Hawkins, J.~Davis, A.~Mohiuddin, L.~Kaiser \emph{et~al.}, ``Rethinking
  attention with performers,'' \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{peng2021random}
H.~Peng, N.~Pappas, D.~Yogatama, R.~Schwartz, N.~Smith, and L.~Kong, ``Random
  feature attention,'' in \emph{Proceedings of International Conference on
  Learning Representations (ICLR)}, 2021.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma, ``Linformer: Self-attention
  with linear complexity,'' \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Gong, Shen, Li, Lv, Duan, and
  Chen]{poolingformer}
H.~Zhang, Y.~Gong, Y.~Shen, W.~Li, J.~Lv, N.~Duan, and W.~Chen,
  ``Poolingformer: Long document modeling with pooling attention,'' in
  \emph{Proceedings of International Conference on Machine Learning (ICML)},
  vol. 139, 2021, pp. 12\,437--12\,446.

\bibitem[Vyas et~al.(2020)Vyas, Katharopoulos, and Fleuret]{cluster}
A.~Vyas, A.~Katharopoulos, and F.~Fleuret, ``Fast transformers with clustered
  attention,'' in \emph{Proceedings of Advances in Neural Information
  Processing Systems (NeurIPS)}, H.~Larochelle, M.~Ranzato, R.~Hadsell,
  M.~Balcan, and H.~Lin, Eds., vol.~33, 2020, pp. 21\,665--21\,674.

\bibitem[Zhu et~al.(2021)Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and
  Catanzaro]{chen2021tfmls}
C.~Zhu, W.~Ping, C.~Xiao, M.~Shoeybi, T.~Goldstein, A.~Anandkumar, and
  B.~Catanzaro, ``Long-short transformer: Efficient transformers for language
  and vision,'' in \emph{Proceedings of Advances in Neural Information
  Processing Systems (NeurIPS)}, vol.~34, 2021, pp. 17\,723--17\,736.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  ukasz Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~ukasz Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Proceedings of Advances in Neural Information Processing Systems
  (NIPS)}, 2017, pp. 5998--6008.

\bibitem[Raffel(2016)]{raffel2016learning}
C.~Raffel, \emph{Learning-based methods for comparing sequences, with
  applications to audio-to-midi alignment and matching}.\hskip 1em plus 0.5em
  minus 0.4em\relax Columbia University, 2016.

\bibitem[Zheng et~al.(2022)Zheng, Lin, Zhang, Ma, Yang, Yang, Wang, Yang, and
  Zhou]{SparTA2022}
N.~Zheng, B.~Lin, Q.~Zhang, L.~Ma, Y.~Yang, F.~Yang, Y.~Wang, M.~Yang, and
  L.~Zhou, ``Sparta: Deep-learning model sparsity via
  tensor-with-sparsity-attribute,'' in \emph{Proceedings of USENIX Symposium on
  Operating Systems Design and Implementation (OSDI)}, 2022, pp. 213--232.

\bibitem[Liu et~al.(2022)Liu, Dong, Cheng, Zhang, Li, Yu, and
  Sun]{liu2022symphony}
J.~Liu, Y.~Dong, Z.~Cheng, X.~Zhang, X.~Li, F.~Yu, and M.~Sun, ``Symphony
  generation with permutation invariant language model,'' \emph{arXiv preprint
  arXiv:2205.05448}, 2022.

\end{thebibliography}
