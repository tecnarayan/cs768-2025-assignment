\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Yao, Jia, Casper, Baracaldo, Hase, Xu, Yao, Li, Varshney, et~al.]{liu2024rethinking}
Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush~R Varshney, et~al.
\newblock Rethinking machine unlearning for large language models.
\newblock \emph{arXiv preprint arXiv:2402.08787}, 2024{\natexlab{a}}.

\bibitem[Cao and Yang(2015)]{cao2015towards}
Yinzhi Cao and Junfeng Yang.
\newblock Towards making systems forget with machine unlearning.
\newblock In \emph{2015 IEEE symposium on security and privacy}, pages 463--480. IEEE, 2015.

\bibitem[Bourtoule et~al.(2021)Bourtoule, Chandrasekaran, Choquette-Choo, Jia, Travers, Zhang, Lie, and Papernot]{bourtoule2021machine}
Lucas Bourtoule, Varun Chandrasekaran, Christopher~A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot.
\newblock Machine unlearning.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pages 141--159. IEEE, 2021.

\bibitem[Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and Nguyen]{nguyen2022survey}
Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet~Hung Nguyen.
\newblock A survey of machine unlearning.
\newblock \emph{arXiv preprint arXiv:2209.02299}, 2022.

\bibitem[Hoofnagle et~al.(2019)Hoofnagle, van~der Sloot, and Borgesius]{hoofnagle2019european}
Chris~Jay Hoofnagle, Bart van~der Sloot, and Frederik~Zuiderveen Borgesius.
\newblock The european union general data protection regulation: what it is and what it means.
\newblock \emph{Information \& Communications Technology Law}, 28\penalty0 (1):\penalty0 65--98, 2019.

\bibitem[Ginart et~al.(2019)Ginart, Guan, Valiant, and Zou]{ginart2019making}
Antonio Ginart, Melody Guan, Gregory Valiant, and James~Y Zou.
\newblock Making ai forget you: Data deletion in machine learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Thudi et~al.(2022{\natexlab{a}})Thudi, Deza, Chandrasekaran, and Papernot]{thudi2022unrolling}
Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot.
\newblock Unrolling sgd: Understanding factors influencing machine unlearning.
\newblock In \emph{2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P)}, pages 303--319. IEEE, 2022{\natexlab{a}}.

\bibitem[Ullah et~al.(2021)Ullah, Mai, Rao, Rossi, and Arora]{ullah2021machine}
Enayat Ullah, Tung Mai, Anup Rao, Ryan~A Rossi, and Raman Arora.
\newblock Machine unlearning via algorithmic stability.
\newblock In \emph{Conference on Learning Theory}, pages 4126--4142. PMLR, 2021.

\bibitem[Sekhari et~al.(2021)Sekhari, Acharya, Kamath, and Suresh]{sekhari2021remember}
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda~Theertha Suresh.
\newblock Remember what you want to forget: Algorithms for machine unlearning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 18075--18086, 2021.

\bibitem[Golatkar et~al.(2020)Golatkar, Achille, and Soatto]{golatkar2020eternal}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Eternal sunshine of the spotless net: Selective forgetting in deep networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9304--9312, 2020.

\bibitem[Jia et~al.(2023)Jia, Liu, Ram, Yao, Liu, Liu, Sharma, and Liu]{jia2023model}
Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu.
\newblock Model sparsity can simplify machine unlearning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Fan et~al.(2024{\natexlab{a}})Fan, Liu, Zhang, Wei, Wong, and Liu]{fan2023salun}
Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu.
\newblock Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation.
\newblock In \emph{International Conference on Learning Representations}, 2024{\natexlab{a}}.

\bibitem[Fan et~al.(2024{\natexlab{b}})Fan, Liu, Hero, and Liu]{fan2024challenging}
Chongyu Fan, Jiancheng Liu, Alfred Hero, and Sijia Liu.
\newblock Challenging forgets: Unveiling the worst-case forget sets in machine unlearning.
\newblock \emph{arXiv preprint arXiv:2403.07362}, 2024{\natexlab{b}}.

\bibitem[Yao et~al.(2023)Yao, Xu, and Liu]{yao2023large}
Yuanshun Yao, Xiaojun Xu, and Yang Liu.
\newblock Large language model unlearning.
\newblock \emph{arXiv preprint arXiv:2310.10683}, 2023.

\bibitem[Maini et~al.(2024)Maini, Feng, Schwarzschild, Lipton, and Kolter]{maini2024tofu}
Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary~C. Lipton, and J.~Zico Kolter.
\newblock Tofu: A task of fictitious unlearning for llms, 2024.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Liu, and Stone]{liu2022continual}
Bo~Liu, Qiang Liu, and Peter Stone.
\newblock Continual learning and private unlearning.
\newblock In \emph{Conference on Lifelong Learning Agents}, pages 243--254. PMLR, 2022{\natexlab{a}}.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Lin, Bai, and Mei]{zhang2024negative}
Ruiqi Zhang, Licong Lin, Yu~Bai, and Song Mei.
\newblock Negative preference optimization: From catastrophic collapse to effective unlearning.
\newblock \emph{arXiv preprint arXiv:2404.05868}, 2024{\natexlab{a}}.

\bibitem[Jia et~al.(2024)Jia, Zhang, Zhang, Liu, Runwal, Diffenderfer, Kailkhura, and Liu]{jia2024soul}
Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, and Sijia Liu.
\newblock Soul: Unlocking the power of second-order optimization for llm unlearning.
\newblock \emph{arXiv preprint arXiv:2404.18239}, 2024.

\bibitem[Madaan et~al.(2022)Madaan, Tandon, Clark, and Yang]{madaan2022memory}
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang.
\newblock Memory-assisted prompt editing to improve gpt-3 after deployment.
\newblock \emph{arXiv preprint arXiv:2201.06009}, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Li, Dong, Fan, Wu, Xu, and Chang]{zheng2023can}
Ce~Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang.
\newblock Can we edit factual knowledge by in-context learning?
\newblock \emph{arXiv preprint arXiv:2305.12740}, 2023.

\bibitem[Pawelczyk et~al.(2023)Pawelczyk, Neel, and Lakkaraju]{pawelczyk2023context}
Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju.
\newblock In-context unlearning: Language models as few shot unlearners.
\newblock \emph{arXiv preprint arXiv:2310.07579}, 2023.

\bibitem[Thaker et~al.(2024)Thaker, Maurya, and Smith]{thaker2024guardrail}
Pratiksha Thaker, Yash Maurya, and Virginia Smith.
\newblock Guardrail baselines for unlearning in llms.
\newblock \emph{arXiv preprint arXiv:2403.03329}, 2024.

\bibitem[Li et~al.(2024)Li, Pan, Gopal, Yue, Berrios, Gatti, Li, Dombrowski, Goel, Phan, et~al.]{li2024wmdp}
Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin~D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et~al.
\newblock The wmdp benchmark: Measuring and reducing malicious use with unlearning.
\newblock \emph{arXiv preprint arXiv:2403.03218}, 2024.

\bibitem[Eldan and Russinovich(2023)]{eldan2023whos}
Ronen Eldan and Mark Russinovich.
\newblock Who's harry potter? approximate unlearning in llms, 2023.

\bibitem[Lu et~al.(2022)Lu, Welleck, Hessel, Jiang, Qin, West, Ammanabrolu, and Choi]{lu2022quark}
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27591--27609, 2022.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
\newblock \emph{arXiv preprint arXiv:2009.11462}, 2020.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17359--17372, 2022.

\bibitem[Hase et~al.(2024)Hase, Bansal, Kim, and Ghandeharioun]{hase2024does}
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.
\newblock Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yu et~al.(2023)Yu, Jeoung, Kasi, Yu, and Ji]{yu2023unlearning}
Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji.
\newblock Unlearning bias in language models by partitioning gradients.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 6032--6048, 2023.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Li, Xu, Dong, Wu, Bian, and Xiong]{wu2023depn}
Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong.
\newblock Depn: Detecting and editing privacy neurons in pretrained language models.
\newblock \emph{arXiv preprint arXiv:2310.20138}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Fan, Chen, Liu, Ma, Wang, and Ma]{liu2022backdoor}
Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li~Wang, and Jianfeng Ma.
\newblock Backdoor defense with machine unlearning.
\newblock In \emph{IEEE INFOCOM 2022-IEEE Conference on Computer Communications}, pages 280--289. IEEE, 2022{\natexlab{b}}.

\bibitem[Kurmanji et~al.(2023)Kurmanji, Triantafillou, and Triantafillou]{kurmanji2023towards}
Meghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou.
\newblock Towards unbounded machine unlearning.
\newblock \emph{arXiv preprint arXiv:2302.09880}, 2023.

\bibitem[Gandikota et~al.(2023)Gandikota, Materzynska, Fiotto-Kaufman, and Bau]{gandikota2023erasing}
Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau.
\newblock Erasing concepts from diffusion models.
\newblock \emph{arXiv preprint arXiv:2303.07345}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Xu, Wang, and Shi]{zhang2023forget}
Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi.
\newblock Forget-me-not: Learning to forget in text-to-image diffusion models.
\newblock \emph{arXiv preprint arXiv:2303.17591}, 2023.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Zhang, Yao, Jia, Liu, Liu, and Liu]{zhang2024unlearncanvas}
Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, and Sijia Liu.
\newblock Unlearncanvas: A stylized image dataset to benchmark machine unlearning for diffusion models.
\newblock \emph{arXiv preprint arXiv:2402.11846}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Dou, Tan, Tian, and Jiang]{liu2024towards}
Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang.
\newblock Towards safer large language models through machine unlearning.
\newblock \emph{arXiv preprint arXiv:2402.10058}, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2024)Chen, Wang, Mi, Liu, Wang, Ren, and Shen]{chen2024machine}
Kongyang Chen, Zixin Wang, Bing Mi, Waixi Liu, Shaowei Wang, Xiaojun Ren, and Jiaxing Shen.
\newblock Machine unlearning in large language models.
\newblock \emph{arXiv preprint arXiv:2404.16841}, 2024.

\bibitem[Yao et~al.(2024)Yao, Chien, Du, Niu, Wang, Cheng, and Yue]{yao2024machine}
Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue.
\newblock Machine unlearning of pre-trained large language models.
\newblock \emph{arXiv preprint arXiv:2402.15159}, 2024.

\bibitem[Chen et~al.(2022)Chen, Zhang, Wang, Backes, Humbert, and Zhang]{chen2022graph}
Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang.
\newblock Graph unlearning.
\newblock In \emph{Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security}, pages 499--513, 2022.

\bibitem[Chien et~al.(2022)Chien, Pan, and Milenkovic]{chien2022certified}
Eli Chien, Chao Pan, and Olgica Milenkovic.
\newblock Certified graph unlearning.
\newblock \emph{arXiv preprint arXiv:2206.09140}, 2022.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Shen, Ning, Wang, and Wang]{wu2023certified}
Kun Wu, Jie Shen, Yue Ning, Ting Wang, and Wendy~Hui Wang.
\newblock Certified edge unlearning for graph neural networks.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 2606--2617, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2022{\natexlab{c}})Liu, Xu, Yuan, Wang, and Li]{liu2022right}
Yi~Liu, Lei Xu, Xingliang Yuan, Cong Wang, and Bo~Li.
\newblock The right to be forgotten in federated learning: An efficient realization with rapid retraining.
\newblock In \emph{IEEE INFOCOM 2022-IEEE Conference on Computer Communications}, pages 1749--1758. IEEE, 2022{\natexlab{c}}.

\bibitem[Halimi et~al.(2022)Halimi, Kadhe, Rawat, and Baracaldo]{halimi2022federated}
Anisa Halimi, Swanand Kadhe, Ambrish Rawat, and Nathalie Baracaldo.
\newblock Federated unlearning: How to efficiently erase a client in fl?
\newblock \emph{arXiv preprint arXiv:2207.05521}, 2022.

\bibitem[Jin et~al.(2023)Jin, Chen, Zhang, and Li]{jin2023forgettable}
Ruinan Jin, Minghui Chen, Qiong Zhang, and Xiaoxiao Li.
\newblock Forgettable federated linear learning with certified data removal.
\newblock \emph{arXiv preprint arXiv:2306.02216}, 2023.

\bibitem[Thudi et~al.(2022{\natexlab{b}})Thudi, Jia, Shumailov, and Papernot]{thudi2022necessity}
Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot.
\newblock On the necessity of auditable algorithmic definitions for machine unlearning.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pages 4007--4022, 2022{\natexlab{b}}.

\bibitem[Guo et~al.(2019)Guo, Goldstein, Hannun, and Van Der~Maaten]{guo2019certified}
Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der~Maaten.
\newblock Certified data removal from machine learning models.
\newblock \emph{arXiv preprint arXiv:1911.03030}, 2019.

\bibitem[Becker and Liebig(2022)]{becker2022evaluating}
Alexander Becker and Thomas Liebig.
\newblock Evaluating machine unlearning via epistemic uncertainty.
\newblock \emph{arXiv preprint arXiv:2208.10836}, 2022.

\bibitem[Chen et~al.(2023)Chen, Gao, Liu, Peng, and Wang]{chen2023boundary}
Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang.
\newblock Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7766--7775, 2023.

\bibitem[Warnecke et~al.(2021)Warnecke, Pirch, Wressnegger, and Rieck]{warnecke2021machine}
Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck.
\newblock Machine unlearning of features and labels.
\newblock \emph{arXiv preprint arXiv:2108.11577}, 2021.

\bibitem[Patil et~al.(2024)Patil, Hase, and Bansal]{patil2023can}
Vaidehi Patil, Peter Hase, and Mohit Bansal.
\newblock Can sensitive information be deleted from llms? objectives for defending against extraction attacks.
\newblock \emph{ICLR}, 2024.

\bibitem[Lynch et~al.(2024)Lynch, Guo, Ewart, Casper, and Hadfield-Menell]{lynch2024eight}
Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell.
\newblock Eight methods to evaluate robust unlearning in llms.
\newblock \emph{arXiv preprint arXiv:2402.16835}, 2024.

\bibitem[Jang et~al.(2022)Jang, Yoon, Yang, Cha, Lee, Logeswaran, and Seo]{jang2022knowledge}
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo.
\newblock Knowledge unlearning for mitigating privacy risks in language models.
\newblock \emph{arXiv preprint arXiv:2210.01504}, 2022.

\bibitem[Barrett et~al.(2023)Barrett, Boyd, Bursztein, Carlini, Chen, Choi, Chowdhury, Christodorescu, Datta, Feizi, et~al.]{barrett2023identifying}
Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita~Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et~al.
\newblock Identifying and mitigating the security risks of generative ai.
\newblock \emph{Foundations and Trends{\textregistered} in Privacy and Security}, 6\penalty0 (1):\penalty0 1--52, 2023.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pages 1885--1894. PMLR, 2017.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch, Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot, Sebastian Farquhar, et~al.
\newblock Prioritized training on points that are learnable, worth learning, and not yet learnt.
\newblock In \emph{International Conference on Machine Learning}, pages 15630--15649. PMLR, 2022.

\bibitem[Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez, et~al.]{grosse2023studying}
Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et~al.
\newblock Studying large language model generalization with influence functions.
\newblock \emph{arXiv preprint arXiv:2308.03296}, 2023.

\bibitem[Yang et~al.(2022)Yang, Xie, Peng, Xu, Sun, and Li]{yang2022dataset}
Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li.
\newblock Dataset pruning: Reducing training data by examining generalization influence.
\newblock \emph{arXiv preprint arXiv:2205.09329}, 2022.

\bibitem[Guo et~al.(2022)Guo, Zhao, and Bai]{guo2022deepcore}
Chengcheng Guo, Bo~Zhao, and Yanbing Bai.
\newblock Deepcore: A comprehensive library for coreset selection in deep learning.
\newblock In \emph{International Conference on Database and Expert Systems Applications}, pages 181--195. Springer, 2022.

\bibitem[Yoon et~al.(2021)Yoon, Madaan, Yang, and Hwang]{yoon2021online}
Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung~Ju Hwang.
\newblock Online coreset selection for rehearsal-based continual learning.
\newblock \emph{arXiv preprint arXiv:2106.01085}, 2021.

\bibitem[Zhang et~al.(2024{\natexlab{c}})Zhang, Zhang, Chen, Liu, Liu, Hong, Chang, Liu, et~al.]{zhang2024selectivity}
Yihua Zhang, Yimeng Zhang, Aochuan Chen, Jiancheng Liu, Gaowen Liu, Mingyi Hong, Shiyu Chang, Sijia Liu, et~al.
\newblock Selectivity drives productivity: Efficient dataset pruning for enhanced transfer learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{c}}.

\bibitem[Marion et~al.(2023)Marion, {\"U}st{\"u}n, Pozzobon, Wang, Fadaee, and Hooker]{marion2023less}
Max Marion, Ahmet {\"U}st{\"u}n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker.
\newblock When less is more: Investigating data pruning for pretraining llms at scale.
\newblock \emph{arXiv preprint arXiv:2309.04564}, 2023.

\bibitem[Hase et~al.(2023)Hase, Bansal, Kim, and Ghandeharioun]{hase2023does}
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.
\newblock Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Liu, Bair, and Kolter]{sun2023wanda}
Mingjie Sun, Zhuang Liu, Anna Bair, and J.~Zico Kolter.
\newblock A simple and effective pruning approach for large language models.
\newblock \emph{arXiv preprint arXiv:2306.11695}, 2023{\natexlab{a}}.

\bibitem[Ma et~al.(2023)Ma, Fang, and Wang]{ma2023llm}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Llm-pruner: On the structural pruning of large language models.
\newblock \emph{Advances in neural information processing systems}, 36:\penalty0 21702--21720, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Wang, Dao, Zhou, Yuan, Song, Shrivastava, Zhang, Tian, Re, et~al.]{liu2023deja}
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce~Zhang, Yuandong Tian, Christopher Re, et~al.
\newblock Deja vu: Contextual sparsity for efficient llms at inference time.
\newblock In \emph{International Conference on Machine Learning}, pages 22137--22176. PMLR, 2023{\natexlab{a}}.

\bibitem[Jaiswal et~al.(2023)Jaiswal, Gan, Du, Zhang, Wang, and Yang]{jaiswal2023compressing}
Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang.
\newblock Compressing llms: The truth is rarely pure and never simple.
\newblock \emph{arXiv preprint arXiv:2310.01382}, 2023.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and Carbin]{chen2020the}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pages 15834--15846, 2020.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Liu, Bair, and Kolter]{sun2023simple}
Mingjie Sun, Zhuang Liu, Anna Bair, and J~Zico Kolter.
\newblock A simple and effective pruning approach for large language models.
\newblock \emph{arXiv preprint arXiv:2306.11695}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{d}})Zhang, Khanduri, Tsaknakis, Yao, Hong, and Liu]{zhang2024introduction}
Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and Sijia Liu.
\newblock An introduction to bilevel optimization: Foundations and applications in signal processing and machine learning.
\newblock \emph{IEEE Signal Processing Magazine}, 41\penalty0 (1):\penalty0 38--59, 2024{\natexlab{d}}.

\bibitem[Gould et~al.(2016)Gould, Fernando, Cherian, Anderson, Cruz, and Guo]{gould2016differentiating}
Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo~Santa Cruz, and Edison Guo.
\newblock On differentiating parameterized argmin and argmax problems with application to bi-level optimization.
\newblock \emph{arXiv preprint arXiv:1607.05447}, 2016.

\bibitem[Zhang et~al.(2022)Zhang, Yao, Ram, Zhao, Chen, Hong, Wang, and Liu]{zhang2022advancing}
Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu~Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu.
\newblock Advancing model pruning via bi-level optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Hall, Liang, and Ma]{liu2023sophia}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2305.14342}, 2023{\natexlab{b}}.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
Elad Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization}, 2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Ji et~al.(2024)Ji, Liu, Dai, Pan, Zhang, Bian, Chen, Sun, Wang, and Yang]{ji2024beavertails}
Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce~Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, Sarrazin, Sanseviero, Rush, and Wolf]{tunstall2023zephyr}
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl√©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander~M. Rush, and Thomas Wolf.
\newblock Zephyr: Direct distillation of lm alignment, 2023.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer]{shi2023detecting}
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer.
\newblock Detecting pretraining data from large language models.
\newblock \emph{arXiv preprint arXiv:2310.16789}, 2023.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Hanu and {Unitary team}(2020)]{Detoxify}
Laura Hanu and {Unitary team}.
\newblock Detoxify.
\newblock Github. https://github.com/unitaryai/detoxify, 2020.

\bibitem[Lermen et~al.(2023)Lermen, Rogers-Smith, and Ladish]{lermen2023lora}
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
\newblock Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.
\newblock \emph{arXiv preprint arXiv:2310.20624}, 2023.

\bibitem[Geva et~al.(2020)Geva, Schuster, Berant, and Levy]{geva2020transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock \emph{arXiv preprint arXiv:2012.14913}, 2020.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark-etal-2019-boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no questions.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 2924--2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1300}.
\newblock URL \url{https://aclanthology.org/N19-1300}.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine learning challenges workshop}, pages 177--190. Springer, 2005.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Chollet(2019)]{chollet2019measure}
Fran{\c{c}}ois Chollet.
\newblock On the measure of intelligence.
\newblock \emph{arXiv preprint arXiv:1911.01547}, 2019.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pages 7432--7439, 2020.

\end{thebibliography}
