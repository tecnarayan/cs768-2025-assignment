@inproceedings{ir_evaluation_methods, author = {J\"{a}rvelin, Kalervo and Kek\"{a}l\"{a}inen, Jaana}, title = {IR Evaluation Methods for Retrieving Highly Relevant Documents}, year = {2000}, isbn = {1581132263}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, doi = {10.1145/345508.345545}, abstract = {This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query1) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.}, booktitle = {Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages = {41–48}, series = {SIGIR '00} }


@book{introduction_to_information_retrieval, author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich}, title = {Introduction to Information Retrieval}, year = {2008}, isbn = {0521865719}, publisher = {Cambridge University Press}, address = {USA}, abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.} }



@article{fairness_in_recommendation_ranking_through_pairwise_comparisons,
	title={Fairness in Recommendation Ranking through Pairwise Comparisons},
	author={Alex Beutel and J. Chen and T. Doshi and H. Qian and L. Wei and Y. Wu and L. Heldt and Zhe Zhao and L. Hong and Ed Huai-hsin Chi and Cristos Goodrow},
	journal={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	year={2019}
}

@article{causal_intersectionality_for_fair_ranking,
	title={Causal intersectionality for fair ranking},
	author={Ke Yang and Joshua R. Loftus and Julia Stoyanovich},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.08688}
}

@misc{compas_propublica,
	author = "Julia Angwin and
	 Jeff Larson and
	 Surya Mattu and 
	 Lauren Kirchner",
	year = "2016",
	title = "Machine Bias",
	publisher = "ProPublica" }



@misc{german_credit ,
	author = "Dua, Dheeru and Graff, Casey",
	year = "2017",
	title = "{UCI} Machine Learning Repository",
	institution = "University of California, Irvine, School of Information and Computer Sciences" }

@InProceedings{two_player_games_for_efficient_non_convex_constrained_optimization,
  title = 	 {Two-Player Games for Efficient Non-Convex Constrained Optimization},
  author = 	 {Andrew Cotter and Heinrich Jiang and Karthik Sridharan},
  booktitle = 	 {Proceedings of the 30th International Conference on Algorithmic Learning Theory},
  pages = 	 {300--332},
  year = 	 {2019},
  editor = 	 {Aurélien Garivier and Satyen Kale},
  volume = 	 {98},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chicago, Illinois},
  month = 	 {22--24 Mar},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v98/cotter19a/cotter19a.pdf},
  abstract = 	 {In recent years, constrained optimization has become increasingly relevant to the machine learning community, with applications including Neyman-Pearson classification, robust optimization, and fair machine learning. A natural approach to constrained optimization is to optimize the Lagrangian, but this is not guaranteed to work in the non-convex setting, and, if using a first-order method, cannot cope with non-differentiable constraints (e.g. constraints on rates or proportions).
 The Lagrangian can be interpreted as a two-player game played between a player who seeks to optimize over the model parameters, and a player who wishes to maximize over the Lagrange multipliers. We propose a non-zero-sum variant of the Lagrangian formulation that can cope with non-differentiable—even discontinuous—constraints, which we call the “proxy-Lagrangian”. The first player minimizes external regret in terms of easy-to-optimize “proxy constraints”, while the second player enforces the \emph{original} constraints by minimizing swap regret.
 For this new formulation, as for the Lagrangian in the non-convex setting, the result is a stochastic classifier. For both the proxy-Lagrangian and Lagrangian formulations, however, we prove that this classifier, instead of having unbounded size, can be taken to be a distribution over no more than $m+1$ models (where $m$ is the number of constraints). This is a significant improvement in practical terms.}
}

@article{learning_to_rank_for_information_retrieval,
author = {Liu, Tie-Yan},
title = {Learning to Rank for Information Retrieval},
year = {2009},
issue_date = {March 2009},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {3},
number = {3},
issn = {1554-0669},
doi = {10.1561/1500000016},
journal = {Found. Trends Inf. Retr.},
month = mar,
pages = {225–331},
numpages = {107}
}
  



@inproceedings{individual_fairness_in_pipelines,
  title={Individual Fairness in Pipelines},
  author={Cynthia Dwork and Christina Ilvento and Meena Jagadeesan},
  booktitle={FORC},
  year={2020}
}

@inproceedings{the_price_of_local_fairness_in_multistage_selection,
  TITLE = {{The Price of Local Fairness in Multistage Selection}},
  AUTHOR = {Emelianov, Vitalii and Arvanitakis, George and Gast, Nicolas and Gummadi, Krishna P and Loiseau, Patrick},
  BOOKTITLE = {{IJCAI-2019 - Twenty-Eighth International Joint Conference on Artificial Intelligence}},
  ADDRESS = {Macao, France},
  PUBLISHER = {{International Joint Conferences on Artificial Intelligence Organization}},
  PAGES = {5836-5842},
  YEAR = {2019},
  MONTH = Aug,
  DOI = {10.24963/ijcai.2019/809},
  PDF = {https://hal.inria.fr/hal-02145071/file/main.pdf},
  HAL_ID = {hal-02145071},
  HAL_VERSION = {v1},
}

@inproceedings{learning_to_rank_from_pairwise_approach_to_listwise_approach,
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
title = {Learning to Rank: From Pairwise Approach to Listwise Approach},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1273496.1273513},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {129–136},
location = {Corvalis, Oregon, USA},
series = {ICML ’07}
}
  

@inproceedings{policy_learning_for_fairness_in_ranking,
	author = {Singh, Ashudeep and Joachims, Thorsten},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {5426--5436},
	publisher = {Curran Associates, Inc.},
	title = {Policy Learning for Fairness in Ranking},
	volume = {32},
	year = {2019}
}



@inbook{reducing_disparate_exposure_in_ranking,
	author = {Zehlike, Meike and Castillo, Carlos},
	title = {Reducing Disparate Exposure in Ranking: A Learning To Rank Approach},
	year = {2020},
	isbn = {9781450370233},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	abstract = {Ranked search results have become the main mechanism by which we find content, products, places, and people online. Thus their ordering contributes not only to the satisfaction of the searcher, but also to career and business opportunities, educational placement, and even social success of those being ranked. Researchers have become increasingly concerned with systematic biases in data-driven ranking models, and various post-processing methods have been proposed to mitigate discrimination and inequality of opportunity. This approach, however, has the disadvantage that it still allows an unfair ranking model to be trained. In this paper we explore a new in-processing approach: DELTR, a learning-to-rank framework that addresses potential issues of discrimination and unequal opportunity in rankings at training time. We measure these problems in terms of discrepancies in the average group exposure and design a ranker that optimizes search results in terms of relevance and in terms of reducing such discrepancies. We perform an extensive experimental study showing that being “colorblind” can be among the best or the worst choices from the perspective of relevance and exposure, depending on how much and which kind of bias is present in the training set. We show that our in-processing method performs better in terms of relevance and exposure than a pre-processing and a post-processing method across all tested scenarios.},
	booktitle = {Proceedings of The Web Conference 2020},
	pages = {2849–2855},
	numpages = {7}
}

@article{pairwise_fairness_for_ranking_and_regression, title={Pairwise Fairness for Ranking and Regression}, volume={34}, DOI={10.1609/aaai.v34i04.5970}, abstractNote={&lt;p&gt;We present pairwise fairness metrics for ranking models and regression models that form analogues of statistical fairness notions such as equal opportunity, equal accuracy, and statistical parity. Our pairwise formulation supports both discrete protected groups, and continuous protected attributes. We show that the resulting training problems can be efficiently and effectively solved using existing constrained optimization and robust optimization techniques developed for fair classification. Experiments illustrate the broad applicability and trade-offs of these methods.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Wang, Serena}, year={2020}, month={Apr.}, pages={5248-5255} }

@inproceedings{fa_ir_a_fair_top_k_ranking_algorithm,
author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
title = {FA*IR: A Fair Top-k Ranking Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3132847.3132938},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1569–1578},
numpages = {10},
keywords = {top-k selection, ranking, algorithmic fairness, bias in computer systems},
location = {Singapore, Singapore},
series = {CIKM ’17}
}



@inproceedings{bias_in_faculty_hiring,
author = { Damani K.   White-Lewis },
title = {The Facade of Fit in Faculty Search Processes},
booktitle = {The Journal of Higher Education},
volume = {0},
number = {0},
pages = {1-25},
year  = {2020},
publisher = {Routledge},
doi = {10.1080/00221546.2020.1775058},
}

@inproceedings{ranking_with_fairness_constraints,
  author    = {L. Elisa Celis and
               Damian Straszak and
               Nisheeth K. Vishnoi},
  title     = {Ranking with Fairness Constraints},
  booktitle = {45th International Colloquium on Automata, Languages, and Programming,
               {ICALP} 2018, July 9-13, 2018, Prague, Czech Republic},
  series    = {LIPIcs},
  volume    = {107},
  pages     = {28:1--28:15},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  year      = {2018},
  doi       = {10.4230/LIPIcs.ICALP.2018.28},
 }


@InProceedings{learning_fair_representations,
  title = 	 {Learning Fair Representations},
  author = 	 {Rich Zemel and Yu Wu and Kevin Swersky and Toni Pitassi and Cynthia Dwork},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}

@inproceedings{measuring_fairness_in_ranked_outputs,
author = {Yang, Ke and Stoyanovich, Julia},
title = {Measuring Fairness in Ranked Outputs},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3085504.3085526},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {22},
keywords = {Accountability, Data, Data Ethics, Data Science for Social Good, Fairness, Responsibly, Transparency},
series = {SSDBM ’17}
}

@article{ChouldechovaFrontiers2018,
  author    = {Alexandra Chouldechova and
               Aaron Roth},
  title     = {The Frontiers of Fairness in Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1810.08810},
  year      = {2018},
  archivePrefix = {arXiv},
 }

@inproceedings{fairness_cynthia,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2090236.2090255},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
series = {ITCS ’12}
}
  


@inproceedings{Mitchell2018PredictionBasedDA,
  title={Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions},
  author={Shira Mitchell and Eric Potash and Solon Barocas},
  year={2018}
}


@inproceedings{Binns2020OnTA,
	author = {Binns, Reuben},
	title = {On the Apparent Conflict between Individual and Group Fairness},
	year = {2020},
	isbn = {9781450369367},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3351095.3372864},
	booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages = {514–524},
	keywords = {discrimination, individual fairness, machine learning, statistical parity, fairness, justice},
	series = {FAT* ’20}
}

@InProceedings{pmlr-v80-kearns18a,
	title = 	 {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	author = 	 {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {2564--2572},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
	abstract = 	 {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.}
}

@book{barocas-hardt-narayanan,
	title = {Fairness and Machine Learning},
	author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
	publisher = {fairmlbook.org},
	year = {2019}
}

@inproceedings{Kleinberg2017InherentTI,
	title={Inherent Trade-Offs in the Fair Determination of Risk Scores},
	author={Jon M. Kleinberg and Sendhil Mullainathan and Manish Raghavan},
	booktitle={ITCS},
	year={2017}
}

@book{Noble2018,
 ISBN = {9781479849949},
 author = {Safiya Umoja Noble},
 publisher = {NYU Press},
 title = {Algorithms of Oppression: How Search Engines Reinforce Racism},
 year = {2018}
}

@book{ONeil2016,
author = {O'Neil, Cathy},
title = {Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy},
year = {2016},
isbn = {0553418815},
publisher = {Crown Publishing Group},
address = {USA},
}

@inproceedings{BrinPage1998,
author = {Brin, Sergey and Page, Lawrence},
title = {The Anatomy of a Large-Scale Hypertextual Web Search Engine},
year = {1998},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
booktitle = {Proceedings of the Seventh International Conference on World Wide Web 7},
pages = {107–117},
numpages = {11},
keywords = {World Wide Web, Google, PageRank, information retrieval, search engines},
location = {Brisbane, Australia}
}

@ARTICLE{Adomavicius2005,
  author={G. {Adomavicius} and A. {Tuzhilin}},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions}, 
  year={2005},
  volume={17},
  pages={734-749},
}

@article{Kofler2016,
author = {Kofler, Christoph and Larson, Martha and Hanjalic, Alan},
title = {User Intent in Multimedia Search: A Survey of the State of the Art and Future Challenges},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0360-0300},
doi = {10.1145/2954930},
journal = {ACM Comput. Surv.},
articleno = {36},
}

@inproceedings{Pei2019,
author = {Pei, Changhua and Zhang, Yi and Zhang, Yongfeng and Sun, Fei and Lin, Xiao and Sun, Hanxiao and Wu, Jian and Jiang, Peng and Ge, Junfeng and Ou, Wenwu and Pei, Dan},
title = {Personalized Re-Ranking for Recommendation},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3298689.3347000},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {3–11},
series = {RecSys '19}
}

@book{Pariser2011,
author = {Pariser, Eli},
title = {The Filter Bubble: What the Internet Is Hiding from You},
year = {2011},
isbn = {1594203008},
publisher = {Penguin Group , The},
}

@InCollection{Tavani2020,
	author       =	{Tavani, Herman},
	title        =	{{Search Engines and Ethics}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	edition      =	{Fall 2020},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{Barocas2016,
 ISSN = {00081221},
 author = {Solon Barocas and Andrew D. Selbst},
 journal = {California Law Review},
 number = {3},
 pages = {671--732},
 publisher = {California Law Review, Inc.},
 title = {Big Data's Disparate Impact},
 volume = {104},
 year = {2016}
}

@book{Barocas2019,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  year = {2019}
}

@inproceedings{Dwork2012,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2090236.2090255},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{CorbettDavies2017,
author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
title = {Algorithmic Decision Making and the Cost of Fairness},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3097983.3098095},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {797–806},
keywords = {disparate impact, pretrial detention, discrimination, algorithmic fairness, risk assessment},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{Jones1977,
	pages = {211--226},
	author = {Hardy Jones},
	title = {Fairness, Meritocracy, and Reverse Discrimination},
	doi = {10.5840/soctheorpract19774217},
	year = {1977},
	volume = {4},
	journal = {Social Theory and Practice},
	publisher = {Department of Philosophy, Florida State University},
	number = {2}
}

@inproceedings{Hardt2016,
	author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
	title = {Equality of Opportunity in Supervised Learning},
	year = {2016},
	isbn = {9781510838819},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	pages = {3323–3331},
	numpages = {9},
	location = {Barcelona, Spain},
	series = {NIPS'16}
}


@book{Crosby2004,
  title={Affirmative Action is Dead: Long Live Affirmative Action},
  author={Crosby, F.J.},
  isbn={9780300101294},
  lccn={2003019187},
  series={Current perspectives in psychology},
  year={2004},
  publisher={Yale University Press}
}

@InProceedings{Kearns2017,
  title = 	 {Meritocratic Fairness for Cross-Population Selection},
  author =   {Michael Kearns and Aaron Roth and Zhiwei Steven Wu},
  pages = 	 {1828--1836},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/kearns17a/kearns17a.pdf},
}

@inproceedings{Biega2018,
author = {Biega, Asia J. and Gummadi, Krishna P. and Weikum, Gerhard},
title = {Equity of Attention: Amortizing Individual Fairness in Rankings},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3209978.3210063}, 
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {405–414},
numpages = {10},
keywords = {individual fairness, attention, algorithmic fairness, exposure, position bias, amortized fairness, fair ranking},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{Binns2020,
  author    = {Reuben Binns},
  title     = {On the apparent conflict between individual and group fairness},
  booktitle = {FAT* '20: Conference on Fairness, Accountability, and Transparency,
               Barcelona, Spain, January 27-30, 2020},
  pages     = {514--524},
  publisher = {{ACM}},
  year      = {2020},
  doi       = {10.1145/3351095.3372864},
  timestamp = {Thu, 26 Mar 2020 12:29:40 +0100},
  biburl    = {https://dblp.org/rec/conf/fat/Binns20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Asudeh2019,
author = {Asudeh, Abolfazl and Jagadish, H. V. and Stoyanovich, Julia and Das, Gautam},
title = {Designing Fair Ranking Schemes},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3299869.3300079},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1259–1276},
numpages = {18},
keywords = {responsible data management, top-k, linear evaluators, fairness, data ethics},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{Yang2017,
author = {Yang, Ke and Stoyanovich, Julia},
title = {Measuring Fairness in Ranked Outputs},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3085504.3085526},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {22},
numpages = {6},
keywords = {Data, Responsibly, Fairness, Accountability, Data Science for Social Good, Data Ethics, Transparency},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@article{Castillo2019survey,
author = {Castillo, Carlos},
title = {Fairness and Transparency in Ranking},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0163-5840},
doi = {10.1145/3308774.3308783},
journal = {Special Interest Group on Information Retrieval Forum},
month = jan,
pages = {64–71},
numpages = {8}
}

@inproceedings{Geyik2019,
author = {Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram},
title = {Fairness-Aware Ranking in Search \& Recommendation Systems with Application to LinkedIn Talent Search},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3292500.3330691},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2221–2231},
numpages = {11},
keywords = {talent search \& recommendation systems, fairness-aware ranking},
location = {Anchorage, AK, USA},
series = {KDD '19}
}