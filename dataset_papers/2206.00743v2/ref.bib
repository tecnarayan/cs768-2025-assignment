@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={ICML},
  pages={214--223},
  year={2017},
  organization={PMLR}
}



@article{antonakopoulos2021adaptive,
  title={Adaptive First-Order Methods Revisited: Convex Minimization without Lipschitz Requirements},
  author={Antonakopoulos, Kimon and Mertikopoulos, Panayotis},
  journal={NeurIPS},
  volume={34},
  year={2021}
}


@article{antonakopoulos2019adaptive,
  title={An adaptive mirror-prox method for variational inequalities with singular operators},
  author={Antonakopoulos, Kimon and Belmega, Veronica and Mertikopoulos, Panayotis},
  journal={NeurIPS},
  volume={32},
  year={2019}
}



@inproceedings{antonakopoulos2021adaptive2,
  title={Adaptive Extra-Gradient Methods for Min-Max Optimization and Games},
  author={Antonakopoulos, K},
  booktitle={ICLR},
  volume={3},
  pages={7},
  year={2021}
}



@article{auer2002adaptive,
 title={Adaptive and self-confident on-line learning algorithms},
 author={Auer, Peter and Cesa-Bianchi, Nicolo and Gentile, Claudio},
 journal={Journal of Computer and System Sciences},
 volume={64},
 number={1},
 pages={48--75},
 year={2002},
 publisher={Elsevier}
}


@article{adler2018banach,
  title={Banach wasserstein gan},
  author={Adler, Jonas and Lunz, Sebastian},
  journal={NeurIPS},
  volume={31},
  year={2018}
}



@book{beck2017first,
  title={First-order methods in optimization},
  author={Beck, Amir},
  year={2017},
  publisher={SIAM}
}



@article{boct2020alternating,
  title={Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems},
  author={Bo{\c{t}}, Radu Ioan and B{\"o}hm, Axel},
  journal={arXiv preprint arXiv:2007.13605},
  year={2020}
}




@inproceedings{bach2019universal,
  title={A universal algorithm for variational inequalities adaptive to smoothness and noise},
  author={Bach, Francis and Levy, Kfir Y},
  booktitle={COLT},
  pages={164--194},
  year={2019},
  organization={PMLR}
}



@article{carmon2022making,
  title={Making SGD Parameter-Free},
  author={Carmon, Yair and Hinder, Oliver},
  journal={arXiv preprint arXiv:2205.02160},
  year={2022}
}



@inproceedings{cutkosky2018black,
  title={Black-box reductions for parameter-free online learning in banach spaces},
  author={Cutkosky, Ashok and Orabona, Francesco},
  booktitle={COLT},
  pages={1493--1529},
  year={2018},
  organization={PMLR}
}


@inproceedings{NIPS2017_6aed000a,
 author = {Cutkosky, Ashok and Boahen, Kwabena A},
 booktitle = {NeurIPS},
 title = {Stochastic and Adversarial Online Learning without Hyperparameters},
 volume = {30},
 year = {2017}
}


@inproceedings{chen2018sadagrad,
  title={Sadagrad: Strongly adaptive stochastic gradient methods},
  author={Chen, Zaiyi and Xu, Yi and Chen, Enhong and Yang, Tianbao},
  booktitle={ICML},
  pages={913--921},
  year={2018},
  organization={PMLR}
}


@article{chen2021closing,
  title={Closing the Gap: Tighter Analysis of Alternating Stochastic Gradient Methods for Bilevel Problems},
  author={Chen, Tianyi and Sun, Yuejiao and Yin, Wotao},
  journal={NeurIPS},
  volume={34},
  year={2021}
}





@inproceedings{chen2018convergence,
  title={On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={ICLR},
  year={2019}
}



@inproceedings{dai2017learning,
  title={Learning from conditional distributions via dual embeddings},
  author={Dai, Bo and He, Niao and Pan, Yunpeng and Boots, Byron and Song, Le},
  booktitle={AISTATS},
  pages={1458--1467},
  year={2017},
  organization={PMLR}
}



@inproceedings{diakonikolas2021efficient,
  title={Efficient methods for structured nonconvex-nonconcave min-max optimization},
  author={Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael I},
  booktitle={AISTATS},
  pages={2746--2754},
  year={2021},
  organization={PMLR}
}



@inproceedings{diakonikolas2020halpern,
  title={Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities},
  author={Diakonikolas, Jelena},
  booktitle={COLT},
  pages={1428--1451},
  year={2020},
  organization={PMLR}
}



@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}




@article{defossez2020simple,
  title={A Simple Convergence Proof of Adam and Adagrad},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}



@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}



@article{dou2021one,
  title={On the One-sided Convergence of Adam-type Algorithms in Non-convex Non-concave Min-max Optimization},
  author={Dou, Zehao and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2109.14213},
  year={2021}
}



@article{ene2020adaptive,
  title={Adaptive and universal algorithms for variational inequalities with optimal convergence},
  author={Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2010.07799},
  year={2020}
}


@inproceedings{fontaine2021convergence,
  title={Convergence rates and approximation results for SGD and its continuous-time counterpart},
  author={Fontaine, Xavier and De Bortoli, Valentin and Durmus, Alain},
  booktitle={COLT},
  pages={1965--2058},
  year={2021},
  organization={PMLR}
}


@article{fiez2021global,
  title={Global Convergence to Local Minmax Equilibrium in Classes of Nonconvex Zero-Sum Games},
  author={Fiez, Tanner and Ratliff, Lillian and Mazumdar, Eric and Faulkner, Evan and Narang, Adhyyan},
  journal={NeurIPS},
  volume={34},
  year={2021}
}


@article{gasnikov2019adaptive,
  title={An adaptive proximal method for variational inequalities},
  author={Gasnikov, Alexander Vladimirovich and Dvurechensky, PE and Stonyakin, Fedor Sergeevich and Titov, Aleksandr Aleksandrovich},
  journal={Computational Mathematics and Mathematical Physics},
  volume={59},
  number={5},
  pages={836--841},
  year={2019},
  publisher={Springer}
}



@article{guo2021novel,
  title={A Novel Convergence Analysis for Algorithms of the Adam Family and Beyond},
  author={Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  journal={arXiv preprint arXiv:2104.14840},
  year={2021}
}


@article{gulrajani2017improved,
  title={Improved training of Wasserstein GANs},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={NeurIPS},
  volume={30},
  year={2017}
}






@inproceedings{gidel2018variational,
  title={A variational inequality perspective on generative adversarial networks},
  author={Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  booktitle={ICLR},
  year={2019}
}




@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}



@article{goodfellow2016nips,
  title={Nips 2016 tutorial: Generative adversarial networks},
  author={Goodfellow, Ian},
  journal={arXiv preprint arXiv:1701.00160},
  year={2016}
}



@article{goodfellow2014explaining,
 title={Explaining and harnessing adversarial examples},
 author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
 journal={arXiv preprint arXiv:1412.6572},
 year={2014}
}



@article{hazan2019revisiting,
  title={Revisiting the polyak step size},
  author={Hazan, Elad and Kakade, Sham},
  journal={arXiv preprint arXiv:1905.00313},
  year={2019}
}



@article{harvey2019simple,
  title={Simple and optimal high-probability bounds for strongly-convex stochastic gradient descent},
  author={Harvey, Nicholas JA and Liaw, Christopher and Randhawa, Sikander},
  journal={arXiv preprint arXiv:1909.00843},
  year={2019}
}




@article{huang2021efficient,
  title={Efficient mirror descent ascent methods for nonsmooth minimax problems},
  author={Huang, Feihu and Wu, Xidong and Huang, Heng},
  journal={NeurIPS},
  volume={34},
  year={2021}
}



@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year={2012}
}



@article{huang2021adagda,
  title={Adagda: Faster adaptive gradient descent ascent methods for minimax optimization},
  author={Huang, Feihu and Huang, Heng},
  journal={arXiv preprint arXiv:2106.16101},
  year={2021}
}





@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={NeurIPS},
  volume={29},
  year={2016}
}




@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={NeurIPS},
  volume={27},
  year={2014}
}



@inproceedings{jain2019making,
  title={Making the last iterate of sgd information theoretically optimal},
  author={Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
  booktitle={COLT},
  pages={1752--1755},
  year={2019},
  organization={PMLR}
}



@article{jin2019short,
  title={A short note on concentration inequalities for random vectors with subgaussian norm},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.03736},
  year={2019}
}


@article{jin2021nonconvex,
  title={On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={2},
  pages={1--29},
  year={2021},
  publisher={ACM New York, NY, USA}
}



@article{kavis2019unixgrad,
  title={Unixgrad: A universal, adaptive algorithm with optimal guarantees for constrained optimization},
  author={Kavis, Ali and Levy, Kfir Y and Bach, Francis and Cevher, Volkan},
  journal={NeurIPS},
  volume={32},
  year={2019}
}



@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}





@inproceedings{kurakin2016adversarial,
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  title={Adversarial machine learning at scale},
  booktitle={ICLR},
  year={2017}
}




@inproceedings{kingma2015adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  booktitle={ICLR},
  year={2015}
}


@article{li2021complexity,
  title={Complexity lower bounds for nonconvex-strongly-concave min-max optimization},
  author={Li, Haochuan and Tian, Yi and Zhang, Jingzhao and Jadbabaie, Ali},
  journal={NeurIPS},
  volume={34},
  year={2021}
}



@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={ICML},
  pages={6083--6093},
  year={2020},
  organization={PMLR}
}



@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  journal={http://yann. lecun. com/exdb/mnist/},
  year={1998}
}


@article{khaled2020better,
  title={Better theory for SGD in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}


@article{kakade2008generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  journal={NeurIPS},
  volume={21},
  year={2008}
}


@article{levy2018online,
  title={Online adaptive methods, universality and acceleration},
  author={Levy, Kfir Y and Yurtsever, Alp and Cevher, Volkan},
  journal={NeurIPS},
  volume={31},
  year={2018}
}


@inproceedings{li2022convergence,
  title={On Convergence of Gradient Descent Ascent: A Tight Local Analysis},
  author={Li, Haochuan and Farnia, Farzan and Das, Subhro and Jadbabaie, Ali},
  booktitle={ICML},
  pages={12717--12740},
  year={2022},
  organization={PMLR}
}



@article{levy2017online,
  title={Online to offline conversions, universality and adaptive minibatch sizes},
  author={Levy, Kfir},
  journal={NeurIPS},
  volume={30},
  year={2017}
}


@inproceedings{lin2020near,
  title={Near-optimal algorithms for minimax optimization},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I},
  booktitle={COLT},
  pages={2738--2779},
  year={2020},
  organization={PMLR}
}



@article{lacoste2012simpler,
  title={A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}




@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={AISTATS},
  pages={983--992},
  year={2019},
  organization={PMLR}
}

@article{DBLP:journals/corr/abs-2007-14294,
  author    = {Xiaoyu Li and
               Francesco Orabona},
  title     = {A High Probability Analysis of Adaptive {SGD} with Momentum},
  journal   = {CoRR},
  volume    = {abs/2007.14294},
  year      = {2020}
}



@article{liu2021first,
  title={First-order convergence theory for weakly-convex-weakly-concave min-max problems},
  author={Liu, Mingrui and Rafique, Hassan and Lin, Qihang and Yang, Tianbao},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={169},
  pages={1--34},
  year={2021}
}







@inproceedings{li2021second,
  title={A second look at exponential and cosine step sizes: Simplicity, adaptivity, and performance},
  author={Li, Xiaoyu and Zhuang, Zhenxun and Orabona, Francesco},
  booktitle={ICML},
  pages={6553--6564},
  year={2021},
  organization={PMLR}
}



@inproceedings{liu2020stochastic,
  title={Stochastic auc maximization with deep neural networks},
  author={Liu, Mingrui and Yuan, Zhuoning and Ying, Yiming and Yang, Tianbao},
  booktitle={ICLR},
  year={2020}
}



@article{lu2020hybrid,
  title={Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications},
  author={Lu, Songtao and Tsaknakis, Ioannis and Hong, Mingyi and Chen, Yongxin},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={3676--3691},
  year={2020},
  publisher={IEEE}
}





@inproceedings{liu2019towards,
  title={Towards better understanding of adaptive gradient algorithms in generative adversarial nets},
  author={Liu, Mingrui and Mroueh, Youssef and Ross, Jerret and Zhang, Wei and Cui, Xiaodong and Das, Payel and Yang, Tianbao},
  booktitle={ICLR},
  year={2020}
}



@inproceedings{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  booktitle={ICLR},
  year={2020}
}




@inproceedings{Luo2019AdaBound,
  author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  booktitle = {ICLR},
  year = {2019},
}



@article{madden2020high,
  title={High-probability Convergence Bounds for Non-convex Stochastic Gradient Descent},
  author={Madden, Liam and Dallâ€™Anese, Emiliano and Becker, Stephen},
  journal={arXiv preprint arXiv:2006.05610},
  year={2020}
}



@inproceedings{mukkamala2017variants,
  title={Variants of rmsprop and adagrad with logarithmic regret bounds},
  author={Mukkamala, Mahesh Chandra and Hein, Matthias},
  booktitle={ICML},
  pages={2545--2553},
  year={2017},
  organization={PMLR}
}



@article{malitsky2020golden,
  title={Golden ratio algorithms for variational inequalities},
  author={Malitsky, Yura},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={383--410},
  year={2020},
  publisher={Springer}
}




@article{mcmahan2010adaptive,
  title={Adaptive bound optimization for online convex optimization},
  author={McMahan, H Brendan and Streeter, Matthew},
  journal={arXiv preprint arXiv:1002.4908},
  year={2010}
}



@article{modi2021model,
  title={Model-free representation learning and exploration in low-rank mdps},
  author={Modi, Aditya and Chen, Jinglin and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2102.07035},
  year={2021}
}


@article{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis},
  journal={NeurIPS},
  volume={24},
  year={2011}
}





@article{nouiehed2019solving,
  title={Solving a class of non-convex min-max games using iterative first order methods},
  author={Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
  journal={NeurIPS},
  volume={32},
  year={2019}
}



@article{nemirovski2004prox,
  title={Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  author={Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={229--251},
  year={2004},
  publisher={SIAM}
}



@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}




@article{orabona2018scale,
  title={Scale-free online learning},
  author={Orabona, Francesco and P{\'a}l, D{\'a}vid},
  journal={Theoretical Computer Science},
  volume={716},
  pages={50--69},
  year={2018},
  publisher={Elsevier}
}



@article{orvieto2022dynamics,
  title={Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution},
  author={Orvieto, Antonio and Lacoste-Julien, Simon and Loizou, Nicolas},
  journal={arXiv preprint arXiv:2205.04583},
  year={2022}
}



@inproceedings{loizou2021stochastic,
  title={Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence},
  author={Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam Hadj and Lacoste-Julien, Simon},
  booktitle={AISTATS},
  pages={1306--1314},
  year={2021},
  organization={PMLR}
}


@article{pang1987posteriori,
  title={A posteriori error bounds for the linearly-constrained variational inequality problem},
  author={Pang, Jong-Shi},
  journal={Mathematics of Operations Research},
  volume={12},
  number={3},
  pages={474--484},
  year={1987},
  publisher={INFORMS}
}



@article{patel2021stopping,
  title={Stopping criteria for, and strong convergence of, stochastic gradient descent on Bottou-Curtis-Nocedal functions},
  author={Patel, Vivak},
  journal={Mathematical Programming},
  pages={1--42},
  year={2021},
  publisher={Springer}
}



@inproceedings{reddi2018convergence,
  title={On the Convergence of Adam and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={ICLR},
  year={2018}
}





@inproceedings{rakhlin2012making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={ICML},
  pages={1571--1578},
  year={2012}
}




@inproceedings{sebbouh2022randomized,
  title={Randomized Stochastic Gradient Descent Ascent},
  author={Sebbouh, Othmane and Cuturi, Marco and Peyr{\'e}, Gabriel},
  booktitle={AISTATS},
  pages={2941--2969},
  year={2022},
  organization={PMLR}
}




@article{stonyakin2018generalized,
  title={Generalized mirror prox for monotone variational inequalities: Universality and inexact oracle},
  author={Stonyakin, Fedor and Gasnikov, Alexander and Dvurechensky, Pavel and Alkousa, Mohammad and Titov, Alexander},
  journal={arXiv preprint arXiv:1806.05140},
  year={2018}
}



@article{tieleman2012rmsprop,
  title={Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA Neural Networks Mach. Learn},
  year={2012}
}




@inproceedings{tramer2018ensemble,
  title={Ensemble Adversarial Training: Attacks and Defenses},
  author={Tram{\`e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  booktitle={ICLR},
  year={2018}
}


@article{vaswani2021towards,
  title={Towards Noise-adaptive, Problem-adaptive Stochastic Gradient Descent},
  author={Vaswani, Sharan and Dubois-Taine, Benjamin and Babanezhad, Reza},
  journal={arXiv preprint arXiv:2110.11442},
  year={2021}
}


@article{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={NeurIPS},
  volume={32},
  year={2019}
}




@article{Vaswani2020adaptive,
  title={Adaptive Gradient Methods Converge Faster with Over-Parameterization (and you can do a line-search)},
  author={Sharan Vaswani and Frederik Kunstner and Issam H. Laradji and Si Yi Meng and Mark W. Schmidt and Simon Lacoste-Julien},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.06835}
}




@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={ICML},
  pages={6677--6686},
  year={2019},
  organization={PMLR}
}





@inproceedings{wang2019sadam,
  title={SAdam: A Variant of Adam for Strongly Convex Functions},
  author={Wang, Guanghui and Lu, Shiyin and Cheng, Quan and Tu, Wei-wei and Zhang, Lijun},
  booktitle={ICLR},
  year={2020}
}




@inproceedings{DBLP:conf/iclr/SinhaND18,
  author    = {Aman Sinha and
               Hongseok Namkoong and
               John C. Duchi},
  title     = {Certifying Some Distributional Robustness with Principled Adversarial
               Training},
  booktitle = {{ICLR}},
  year      = {2018}
}


@inproceedings{xie2020linear,
  title={Linear convergence of adaptive stochastic gradient descent},
  author={Xie, Yuege and Wu, Xiaoxia and Ward, Rachel},
  booktitle={AISTATS},
  pages={1475--1485},
  year={2020},
  organization={PMLR}
}





@inproceedings{yang2021faster,
  title={Faster Single-loop Algorithms for Minimax Optimization without Strong Concavity},
  author={Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
  booktitle={AISTATS},
  pages={5485--5517},
  year={2022},
  organization={PMLR}
}



@article{yang2020global,
	title={Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems},
	author={Yang, Junchi and Kiyavash, Negar and He, Niao},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={1153--1165},
	year={2020}
}


@article{yang2020catalyst,
  title={A catalyst framework for minimax optimization},
  author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
  journal={NeurIPS},
  volume={33},
  pages={5667--5678},
  year={2020}
}






@inproceedings{zhang2021complexity,
  title={The complexity of nonconvex-strongly-concave minimax optimization},
  author={Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
  booktitle={UAI},
  pages={482--492},
  year={2021},
  organization={PMLR}
}



@article{zhang2020single,
  title={A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems},
  author={Zhang, Jiawei and Xiao, Peijun and Sun, Ruoyu and Luo, Zhiquan},
  journal={NeurIPS},
  volume={33},
  pages={7377--7389},
  year={2020}
}




@inproceedings{kavis2022high,
  title={High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize},
  author={Ali Kavis and Kfir Yehuda Levy and Volkan Cevher},
  booktitle={ICLR},
  year={2022},
}



@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}




@article{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  journal={NeurIPS},
  volume={31},
  year={2018}
}


@misc{githubcode,
  author = {Lv, Louis},
  title = {Reproducing "Certifying Some Distributional Robustness with Principled Adversarial Training"},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Louis-udm/Reproducing-certifiable-distributional-robustness}},
}
