\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In {\em 12th $\{$USENIX$\}$ symposium on operating systems design and
  implementation ($\{$OSDI$\}$ 16)}, pages 265--283, 2016.

\bibitem{an2018pid}
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang.
\newblock A pid controller approach for stochastic optimization of deep
  networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8522--8531, 2018.

\bibitem{aybat2019universally}
Necdet~Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar.
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8523--8534, 2019.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em Siam Review}, 60(2):223--311, 2018.

\bibitem{davis2019stochastic}
Damek Davis, Dmitriy Drusvyatskiy, and Vasileios Charisopoulos.
\newblock Stochastic algorithms with geometric step decay converge linearly on
  sharp functions.
\newblock {\em arXiv preprint arXiv:1907.09547}, 2019.

\bibitem{ge2019step}
Rong Ge, Sham~M Kakade, Rahul Kidambi, and Praneeth Netrapalli.
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14951--14962, 2019.

\bibitem{ghadimi2015global}
Euhanna Ghadimi, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock In {\em 2015 European Control Conference (ECC)}, pages 310--315.
  IEEE, 2015.

\bibitem{ghadimi2013optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization, ii: shrinking procedures and optimal
  algorithms.
\newblock {\em SIAM Journal on Optimization}, 23(4):2061--2089, 2013.

\bibitem{gitman2019understanding}
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao.
\newblock Understanding the role of momentum in stochastic gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9630--9640, 2019.

\bibitem{hazan2014beyond}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: optimal algorithms for
  stochastic strongly-convex optimization.
\newblock {\em The Journal of Machine Learning Research}, 15(1):2489--2512,
  2014.

\bibitem{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock {\em IEEE Signal processing magazine}, 29(6):82--97, 2012.

\bibitem{kidambi2018insufficiency}
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock In {\em 2018 Information Theory and Applications Workshop (ITA)},
  pages 1--9. IEEE, 2018.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{kulunchakov2019generic}
Andrei Kulunchakov and Julien Mairal.
\newblock A generic acceleration framework for stochastic composite
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12556--12567, 2019.

\bibitem{loizou2017linearly}
Nicolas Loizou and Peter Richt{\'a}rik.
\newblock Linearly convergent stochastic heavy ball method for minimizing
  generalization error.
\newblock {\em arXiv preprint arXiv:1710.10737}, 2017.

\bibitem{loizou2017momentum}
Nicolas Loizou and Peter Richt{\'a}rik.
\newblock Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods.
\newblock {\em arXiv preprint arXiv:1712.09677}, 2017.

\bibitem{ma2019qh}
Jerry Ma and Denis Yarats.
\newblock Quasi-hyperbolic momentum and adam for deep learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in neural information processing systems}, pages
  8026--8037, 2019.

\bibitem{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{sebbouh2020convergence}
Othmane Sebbouh, Robert~M Gower, and Aaron Defazio.
\newblock On the convergence of the stochastic heavy ball method.
\newblock {\em arXiv preprint arXiv:2006.07867}, 2020.

\bibitem{sun2019optimization}
Ruoyu Sun.
\newblock Optimization for deep learning: theory and algorithms.
\newblock {\em arXiv preprint arXiv:1912.08957}, 2019.

\bibitem{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem{yan2018unified}
Y~Yan, T~Yang, Z~Li, Q~Lin, and Y~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In {\em IJCAI International Joint Conference on Artificial
  Intelligence}, 2018.

\bibitem{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  7184--7193, 2019.

\bibitem{yuan2019stagewise}
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang.
\newblock Stagewise training accelerates convergence of testing error over sgd.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2604--2614, 2019.

\bibitem{zhang2017yellowfin}
Jian Zhang and Ioannis Mitliagkas.
\newblock Yellowfin and the art of momentum tuning.
\newblock {\em arXiv preprint arXiv:1706.03471}, 2017.

\end{thebibliography}
