\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Benton et~al.(2021)Benton, Maddox, Lotfi, and
  Wilson]{bentonLossSurfaceSimplexes2021}
Gregory~W. Benton, Wesley~J. Maddox, Sanae Lotfi, and Andrew~Gordon Wilson.
\newblock Loss {{Surface Simplexes}} for {{Mode Connecting Volumes}} and {{Fast
  Ensembling}}.
\newblock In \emph{{{PMLR}}}, 2021.

\bibitem[Blum and Rivest(1988)]{blumTraining3NodeNeural1988}
Avrim Blum and Ronald~L Rivest.
\newblock Training a 3-{{Node Neural Network}} is {{NP-Complete}}.
\newblock In \emph{{{NIPS}}}, page~8, 1988.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, {Herbert-Voss}, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brownLanguageModelsAre2020}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel {Herbert-Voss}, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language {{Models}} are {{Few-Shot Learners}}, July 2020.

\bibitem[Cazenavette et~al.(2022)Cazenavette, Wang, Torralba, Efros, and
  Zhu]{cazenavetteDatasetDistillationMatching2022}
George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei~A Efros, and
  Jun-Yan Zhu.
\newblock Dataset {{Distillation}} by {{Matching Training Trajectories}}.
\newblock In \emph{Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer
  Vision}} and {{Pattern Recognition}} ({{CVPR}})}, page~10, 2022.

\bibitem[Coates et~al.(2011)Coates, Lee, and
  Ng]{coatesAnalysisSingleLayerNetworks2011}
Adam Coates, Honglak Lee, and Andrew~Y Ng.
\newblock An {{Analysis}} of {{Single-Layer Networks}} in {{Unsupervised
  Feature Learning}}.
\newblock In \emph{Proceedings of the 14th {{International Con-}} Ference on
  {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})}, page~9, 2011.

\bibitem[Corneanu et~al.(2020)Corneanu, Escalera, and
  Martinez]{corneanuComputingTestingError2020}
Ciprian~A. Corneanu, Sergio Escalera, and Aleix~M. Martinez.
\newblock Computing the {{Testing Error Without}} a {{Testing Set}}.
\newblock In \emph{2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and
  {{Pattern Recognition}} ({{CVPR}})}, pages 2674--2682, {Seattle, WA, USA},
  June 2020. {IEEE}.
\newblock ISBN 978-1-72817-168-5.
\newblock \doi{10.1109/CVPR42600.2020.00275}.

\bibitem[Dabkowski and Gal(2017)]{dabkowskiRealTimeImage2017}
Piotr Dabkowski and Yarin Gal.
\newblock Real {{Time Image Saliency}} for {{Black Box Classifiers}}.
\newblock \emph{arXiv:1705.07857 [stat]}, May 2017.

\bibitem[Dauphin and Schoenholz(2019)]{dauphinMetaInitInitializingLearning2019}
Yann~N Dauphin and Samuel Schoenholz.
\newblock {{MetaInit}}: {{Initializing}} learning by learning to initialize.
\newblock In \emph{Neural {{Information Processing Systems}}}, page~13, 2019.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphinIdentifyingAttackingSaddle2014}
Yann~N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
  and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{{{NIPS}}}, page~9, 2014.

\bibitem[Deng et~al.()Deng, Dong, Socher, Li, Li, and
  {Fei-Fei}]{dengImageNetLargeScaleHierarchical}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~{Fei-Fei}.
\newblock {{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}.
\newblock page~8.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, and
  Ranzato]{denilPredictingParametersDeep2013}
Misha Denil, Babak Shakibi, Laurent Dinh, and Marc'Aurelio Ranzato.
\newblock Predicting {{Parameters}} in {{Deep Learning}}.
\newblock In \emph{Neural {{Information Processing Systems}} ({{NeurIPS}})},
  page~9, 2013.

\bibitem[Eilertsen et~al.(2020)Eilertsen, J{\"o}nsson, Ropinski, Unger, and
  Ynnerman]{eilertsenClassifyingClassifierDissecting2020}
Gabriel Eilertsen, Daniel J{\"o}nsson, Timo Ropinski, Jonas Unger, and Anders
  Ynnerman.
\newblock Classifying the classifier: Dissecting the weight space of neural
  networks.
\newblock \emph{arXiv:2002.05688 [cs]}, February 2020.

\bibitem[Fei et~al.()Fei, Deng, Do, Su, and Li]{feiConstructionAnalysisLarge}
Li~Fei Fei, Jia Deng, Minh Do, Hao Su, and Kai Li.
\newblock Construction and {{Analysis}} of a {{Large Scale Image Ontology}}.
\newblock page~1.

\bibitem[Feng et~al.(2020)Feng, Zhai, He, Wang, and
  Dong]{fengTransferredDiscrepancyQuantifying2020}
Yunzhen Feng, Runtian Zhai, Di~He, Liwei Wang, and Bin Dong.
\newblock Transferred {{Discrepancy}}: {{Quantifying}} the {{Difference Between
  Representations}}.
\newblock \emph{arXiv:2007.12446 [cs, stat]}, July 2020.

\bibitem[Finnoff et~al.(1993)Finnoff, Hergert, and
  Zimmermann]{FinnoffImprovingModelSelection1993}
William Finnoff, Ferdinand Hergert, and Hans~Georg Zimmermann.
\newblock Improving model selection by nonconvergent methods.
\newblock \emph{Neural Networks}, 6\penalty0 (6):\penalty0 771--783, January
  1993.
\newblock ISSN 0893-6080.
\newblock \doi{10.1016/S0893-6080(05)80122-4}.

\bibitem[Gavrikov and Keuper(2022)]{gavrikovCNNFilterDB2022}
Paul Gavrikov and Janis Keuper.
\newblock {{CNN Filter DB}}: {{An Empirical Investigation}} of {{Trained
  Convolutional Filters}}.
\newblock In \emph{Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer
  Vision}} and {{Pattern Recognition}} ({{CVPR}})}, page~11, 2022.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and
  Saxe]{goodfellowQualitativelyCharacterizingNeural2015}
Ian~J. Goodfellow, Oriol Vinyals, and Andrew~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{arXiv:1412.6544 [cs, stat]}, May 2015.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{haHyperNetworks2016}
David Ha, Andrew Dai, and Quoc~V. Le.
\newblock {{HyperNetworks}}.
\newblock In \emph{{{arXiv}}:1609.09106 [Cs]}, 2016.

\bibitem[Hanin and Rolnick(2018)]{haninHowStartTraining2018}
Boris Hanin and David Rolnick.
\newblock How to {{Start Training}}: {{The Effect}} of {{Initialization}} and
  {{Architecture}}.
\newblock \emph{arXiv:1803.01719 [cs, stat]}, November 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{heDeepResidualLearning2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep {{Residual Learning}} for {{Image Recognition}}.
\newblock In \emph{{{IEEE Conference}} on {{Computer Vision}} and {{Pattern
  Recognition}} ({{CVPR}})}, 2016.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, {Ben-Nun}, Dryden, and
  Peste]{hoeflerSparsityDeepLearning2021}
Torsten Hoefler, Dan Alistarh, Tal {Ben-Nun}, Nikoli Dryden, and Alexandra
  Peste.
\newblock Sparsity in {{Deep Learning}}: {{Pruning}} and growth for efficient
  inference and training in neural networks, January 2021.

\bibitem[Hull(1994)]{hullDatabaseHandwrittenText1994}
J.J. Hull.
\newblock A database for handwritten text recognition research.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 16\penalty0 (5):\penalty0 550--554, May 1994.
\newblock ISSN 1939-3539.
\newblock \doi{10.1109/34.291440}.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,
  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan, Fernando, and
  Kavukcuoglu]{jaderbergPopulationBasedTraining2017}
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M. Czarnecki, Jeff
  Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan,
  Chrisantha Fernando, and Koray Kavukcuoglu.
\newblock Population {{Based Training}} of {{Neural Networks}}.
\newblock \emph{arXiv:1711.09846 [cs]}, November 2017.

\bibitem[Jiang et~al.(2019)Jiang, Krishnan, Mobahi, and
  Bengio]{jiangPredictingGeneralizationGap2019}
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio.
\newblock Predicting the {{Generalization Gap}} in {{Deep Networks}} with
  {{Margin Distributions}}.
\newblock \emph{arXiv:1810.00113 [cs, stat]}, June 2019.

\bibitem[Karpathy et~al.(2015)Karpathy, Johnson, and
  {Fei-Fei}]{karpathyVisualizingUnderstandingRecurrent2015}
Andrej Karpathy, Justin Johnson, and Li~{Fei-Fei}.
\newblock Visualizing and {{Understanding Recurrent Networks}}.
\newblock \emph{arXiv:1506.02078 [cs]}, June 2015.

\bibitem[Knyazev et~al.(2021)Knyazev, Drozdzal, Taylor, and
  {Romero-Soriano}]{knyazevParameterPredictionUnseen2021}
Boris Knyazev, Michal Drozdzal, Graham~W. Taylor, and Adriana {Romero-Soriano}.
\newblock Parameter {{Prediction}} for {{Unseen Deep Architectures}}.
\newblock In \emph{Conference on {{Neural Information Processing Systems}}
  ({{NeurIPS}})}, 2021.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblithSimilarityNeuralNetwork2019}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of {{Neural Network Representations Revisited}}.
\newblock \emph{arXiv:1905.00414 [cs, q-bio, stat]}, May 2019.

\bibitem[Krizhevsky(2009)]{krizhevskyLearningMultipleLayers2009}
Alex Krizhevsky.
\newblock Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}.
\newblock page~60, 2009.

\bibitem[Le and Yang()]{leTinyImageNetVisual}
Ya~Le and Xuan Yang.
\newblock Tiny {{ImageNet Visual Recognition Challenge}}.
\newblock page~6.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecunGradientbasedLearningApplied1998}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-{{Based Learning Applied}} to {{Document Recognition}}.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, November 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecunDeepLearning2015}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, May 2015.
\newblock ISSN 0028-0836, 1476-4687.
\newblock \doi{10.1038/nature14539}.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Xu, Taylor, Studer, and
  Goldstein]{liVisualizingLossLandscape2018}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the {{Loss Landscape}} of {{Neural Nets}}.
\newblock In \emph{{{NIPS}}}, page~11, 2018{\natexlab{a}}.

\bibitem[Li et~al.(2020)Li, Jamieson, Rostamizadeh, Gonina, Hardt, Recht, and
  Talwalkar]{liSystemMassivelyParallel2020}
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt,
  Benjamin Recht, and Ameet Talwalkar.
\newblock A {{System}} for {{Massively Parallel Hyperparameter Tuning}}.
\newblock In \emph{Proceedings of the 3 Rd {{MLSys Conference}}}, {Austin, TX,
  USA}, 2020. {arXiv}.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Jamieson, DeSalvo, Rostamizadeh, and
  Talwalkar]{liHyperbandNovelBanditBased2018}
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet
  Talwalkar.
\newblock Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter
  Optimization}}.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 18, June
  2018{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Peng, and Schwing]{liuKnowledgeFlowImprove2019}
Iou-Jen Liu, Jian Peng, and Alexander~G. Schwing.
\newblock Knowledge {{Flow}}: {{Improve Upon Your Teachers}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}, April 2019.

\bibitem[Liu et~al.(2021)Liu, Yin, Mocanu, and
  Pechenizkiy]{liuWeActuallyNeed2021}
Shiwei Liu, Lu~Yin, Decebal~Constantin Mocanu, and Mykola Pechenizkiy.
\newblock Do {{We Actually Need Dense Over-Parameterization}}? {{In-Time
  Over-Parameterization}} in {{Sparse Training}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages
  6989--7000. {PMLR}, July 2021.

\bibitem[Lucas et~al.()Lucas, Bae, Zhang, Fort, Zemel, and
  Grosse]{lucasAnalyzingMonotonicLinear}
James Lucas, Juhan Bae, Michael~R Zhang, Stanislav Fort, Richard Zemel, and
  Roger Grosse.
\newblock Analyzing {{Monotonic Linear Interpolation}} in {{Neural Network Loss
  Landscapes}}.
\newblock page~12.

\bibitem[Lucas et~al.(2021)Lucas, Bae, Zhang, Fort, Zemel, and
  Grosse]{lucasMonotonicLinearInterpolation2021}
James~R. Lucas, Juhan Bae, Michael~R. Zhang, Stanislav Fort, Richard Zemel, and
  Roger~B. Grosse.
\newblock On {{Monotonic Linear Interpolation}} of {{Neural Network
  Parameters}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages
  7168--7179. {PMLR}, July 2021.

\bibitem[Martin and Mahoney(2019)]{martinTraditionalHeavyTailedSelf2019}
Charles~H. Martin and Michael~W. Mahoney.
\newblock Traditional and {{Heavy-Tailed Self Regularization}} in {{Neural
  Network Models}}.
\newblock \emph{arXiv:1901.08276 [cs, stat]}, January 2019.

\bibitem[McInnes et~al.(2018)McInnes, Healy, and
  Saul]{mcinnesUMAPUniformManifold2018a}
Leland McInnes, John Healy, and Nathaniel Saul.
\newblock {{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}}.
\newblock 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanovVariationalDropoutSparsifies2017}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational {{Dropout Sparsifies Deep Neural Networks}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}
  ({{ICML}})}, page~10, 2017.

\bibitem[Morcos et~al.(2018)Morcos, Raghu, and
  Bengio]{morcosInsightsRepresentationalSimilarity2018}
Ari~S. Morcos, Maithra Raghu, and Samy Bengio.
\newblock Insights on representational similarity in neural networks with
  canonical correlation.
\newblock \emph{arXiv:1806.05759 [cs, stat]}, June 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzerReadingDigitsNatural2011}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading {{Digits}} in {{Natural Images}} with {{Unsupervised Feature
  Learning}}.
\newblock In \emph{{{NIPS Workshop}} on {{Deep Learning}} and {{Unsupervised
  Feature Learning}} 2011}, page~9, 2011.

\bibitem[Nguyen et~al.(2020)Nguyen, Raghu, and
  Kornblith]{nguyenWideDeepNetworks2020}
Thao Nguyen, Maithra Raghu, and Simon Kornblith.
\newblock Do {{Wide}} and {{Deep Networks Learn}} the {{Same Things}}?
  {{Uncovering How Neural Network Representations Vary}} with {{Width}} and
  {{Depth}}.
\newblock \emph{arXiv:2010.15327 [cs]}, October 2020.

\bibitem[Peebles et~al.(2022)Peebles, Radosavovic, Brooks, Efros, and
  Malik]{peeblesLearningLearnGenerative2022}
William Peebles, Ilija Radosavovic, Tim Brooks, Alexei~A. Efros, and Jitendra
  Malik.
\newblock Learning to {{Learn}} with {{Generative Models}} of {{Neural Network
  Checkpoints}}, September 2022.

\bibitem[Raghu et~al.(2017)Raghu, Gilmer, Yosinski, and
  {Sohl-Dickstein}]{raghuSVCCASingularVector2017}
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha {Sohl-Dickstein}.
\newblock {{SVCCA}}: {{Singular Vector Canonical Correlation Analysis}} for
  {{Deep Learning Dynamics}} and {{Interpretability}}.
\newblock \emph{arXiv:1706.05806 [cs, stat]}, June 2017.

\bibitem[Ramesh and Chaudhari(2022)]{rameshModelZooGrowing2022}
Rahul Ramesh and Pratik Chaudhari.
\newblock Model {{Zoo}}: {{A Growing}} "{{Brain}}" {{That Learns Continually}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations
  ICLR}}}, 2022.

\bibitem[Ratzlaff and Fuxin(2019)]{ratzlaffHyperGANGenerativeModel2019}
Neale Ratzlaff and Li~Fuxin.
\newblock {{HyperGAN}}: {{A Generative Model}} for {{Diverse}}, {{Performant
  Neural Networks}}.
\newblock In \emph{Proceedings of the 36th {{International Conference}} on
  {{Machine Learning}}}, pages 5361--5369. {PMLR}, May 2019.

\bibitem[Sch{\"u}rholt and Borth(2021)]{schurholtInvestigationWeightSpace2021}
Konstantin Sch{\"u}rholt and Damian Borth.
\newblock An {{Investigation}} of the {{Weight Space}} to {{Monitor}} the
  {{Training Progress}} of {{Neural Networks}}, March 2021.

\bibitem[Sch{\"u}rholt et~al.(2021)Sch{\"u}rholt, Kostadinov, and
  Borth]{schurholtSelfSupervisedRepresentationLearning2021}
Konstantin Sch{\"u}rholt, Dimche Kostadinov, and Damian Borth.
\newblock Self-{{Supervised Representation Learning}} on {{Neural Network
  Weights}} for {{Model Characteristic Prediction}}.
\newblock In \emph{Conference on {{Neural Information Processing Systems}}
  ({{NeurIPS}})}, volume~35, 2021.

\bibitem[Sch{\"u}rholt et~al.(2022{\natexlab{a}})Sch{\"u}rholt, Knyazev,
  {Gir{\'o}-i-Nieto}, and
  Borth]{schurholtHyperRepresentationsGenerativeModels2022}
Konstantin Sch{\"u}rholt, Boris Knyazev, Xavier {Gir{\'o}-i-Nieto}, and Damian
  Borth.
\newblock Hyper-{{Representations}} as {{Generative Models}}: {{Sampling Unseen
  Neural Network Weights}}.
\newblock In \emph{Thirty-Sixth {{Conference}} on {{Neural Information
  Processing Systems}} ({{NeurIPS}})}, September 2022{\natexlab{a}}.

\bibitem[Sch{\"u}rholt et~al.(2022{\natexlab{b}})Sch{\"u}rholt, Knyazev,
  {Gir{\'o}-i-Nieto}, and
  Borth]{schurholtHyperRepresentationsPreTrainingTransfer2022}
Konstantin Sch{\"u}rholt, Boris Knyazev, Xavier {Gir{\'o}-i-Nieto}, and Damian
  Borth.
\newblock Hyper-{{Representations}} for {{Pre-Training}} and {{Transfer
  Learning}}.
\newblock In \emph{First {{Workshop}} of {{Pre-training}}: {{Perspectives}},
  {{Pitfalls}}, and {{Paths Forward}} at {{ICML}} 2022}, 2022{\natexlab{b}}.

\bibitem[Shu et~al.(2021)Shu, Kou, Cao, Wang, and
  Long]{shuZooTuningAdaptiveTransfer2021}
Yang Shu, Zhi Kou, Zhangjie Cao, Jianmin Wang, and Mingsheng Long.
\newblock Zoo-{{Tuning}}: {{Adaptive Transfer}} from a {{Zoo}} of {{Models}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}
  ({{ICML}})}, page~12, 2021.

\bibitem[Such et~al.(2019)Such, Madhavan, Liu, Wang, Castro, Li, Zhi, Schubert,
  Bellemare, Clune, and Lehman]{suchAtariModelZoo2019}
Felipe~Petroski Such, Vashisht Madhavan, Rosanne Liu, Rui Wang, Pablo~Samuel
  Castro, Yulun Li, Jiale Zhi, Ludwig Schubert, Marc~G. Bellemare, Jeff Clune,
  and Joel Lehman.
\newblock An {{Atari Model Zoo}} for {{Analyzing}}, {{Visualizing}}, and
  {{Comparing Deep Reinforcement Learning Agents}}, May 2019.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and
  Tolstikhin]{unterthinerPredictingNeuralNetwork2020}
Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya
  Tolstikhin.
\newblock Predicting {{Neural Network Accuracy}} from {{Weights}}.
\newblock \emph{arXiv:2002.11448 [cs, stat]}, February 2020.

\bibitem[Wortsman et~al.(2021)Wortsman, Horton, Guestrin, Farhadi, and
  Rastegari]{wortsmanLearningNeuralNetwork2021}
Mitchell Wortsman, Maxwell~C. Horton, Carlos Guestrin, Ali Farhadi, and
  Mohammad Rastegari.
\newblock Learning {{Neural Network Subspaces}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages
  11217--11227. {PMLR}, July 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and
  Vollgraf]{xiaoFashionMNISTNovelImage2017}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking
  Machine Learning Algorithms}}, September 2017.

\bibitem[Yak et~al.(2019)Yak, Gonzalvo, and
  Mazzawi]{yakTaskArchitectureIndependentGeneralization2019}
Scott Yak, Javier Gonzalvo, and Hanna Mazzawi.
\newblock Towards {{Task}} and {{Architecture-Independent Generalization Gap
  Predictors}}.
\newblock \emph{arXiv:1906.01550 [cs, stat]}, June 2019.

\bibitem[Ying et~al.(2019)Ying, Klein, Real, Christiansen, Murphy, and
  Hutter]{yingNASBench101ReproducibleNeural2019}
Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, and
  Frank Hutter.
\newblock {{NAS-Bench-101}}: {{Towards Reproducible Neural Architecture
  Search}}.
\newblock In \emph{{{PMLR}}}, volume~97. {arXiv}, 2019.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinskiHowTransferableAre2014}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock How transferable are features in deep neural networks?
\newblock In \emph{Neural {{Information Processing Systems}} ({{NeurIPS}})},
  November 2014.

\bibitem[Yosinski et~al.(2015)Yosinski, Clune, Nguyen, Fuchs, and
  Lipson]{yosinskiUnderstandingNeuralNetworks2015}
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.
\newblock Understanding {{Neural Networks Through Deep Visualization}}.
\newblock \emph{arXiv:1506.06579 [cs]}, June 2015.

\bibitem[Zeiler and
  Fergus(2014)]{zeilerVisualizingUnderstandingConvolutional2014}
Matthew~D. Zeiler and Rob Fergus.
\newblock Visualizing and {{Understanding Convolutional Networks}}.
\newblock In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,
  editors, \emph{Computer {{Vision}} \textendash{} {{ECCV}} 2014}, volume 8689,
  pages 818--833. {Springer International Publishing}, {Cham}, 2014.
\newblock ISBN 978-3-319-10589-5 978-3-319-10590-1.
\newblock \doi{10.1007/978-3-319-10590-1_53}.

\bibitem[Zhang et~al.(2019)Zhang, Ren, and
  Urtasun]{zhangGraphHyperNetworksNeural2019}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph {{HyperNetworks}} for {{Neural Architecture Search}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}, 2019.

\bibitem[Zhmoginov et~al.(2022)Zhmoginov, Sandler, and
  Vladymyrov]{zhmoginovHyperTransformerModelGeneration2022}
Andrey Zhmoginov, Mark Sandler, and Max Vladymyrov.
\newblock {{HyperTransformer}}: {{Model Generation}} for {{Supervised}} and
  {{Semi-Supervised Few-Shot Learning}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}
  ({{ICML}})}, January 2022.

\bibitem[Zhou et~al.(2021)Zhou, Yang, and
  Hu]{zhouJittorGANFasttrainingGenerative2021}
Wen-Yang Zhou, Guo-Wei Yang, and Shi-Min Hu.
\newblock Jittor-{{GAN}}: {{A}} fast-training generative adversarial network
  model zoo based on {{Jittor}}.
\newblock \emph{Computational Visual Media}, 7\penalty0 (1):\penalty0 153--157,
  March 2021.
\newblock ISSN 2096-0433, 2096-0662.
\newblock \doi{10.1007/s41095-021-0203-2}.

\bibitem[Zintgraf et~al.(2017)Zintgraf, Cohen, Adel, and
  Welling]{zintgrafVisualizingDeepNeural2017}
Luisa~M. Zintgraf, Taco~S. Cohen, Tameem Adel, and Max Welling.
\newblock Visualizing {{Deep Neural Network Decisions}}: {{Prediction
  Difference Analysis}}.
\newblock \emph{arXiv:1702.04595 [cs]}, February 2017.

\end{thebibliography}
