\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani \& Ganguli(2016)Advani and Ganguli]{advani2016statistical}
Advani, M. and Ganguli, S.
\newblock Statistical mechanics of optimal convex inference in high dimensions.
\newblock \emph{Physical Review X}, 6\penalty0 (3):\penalty0 031034, 2016.

\bibitem[Ariosto et~al.(2022)Ariosto, Pacelli, Pastore, Ginelli, Gherardi, and
  Rotondo]{Ariosto2022StatisticalMO}
Ariosto, S., Pacelli, R., Pastore, M., Ginelli, F., Gherardi, M., and Rotondo,
  P.
\newblock Statistical mechanics of deep learning beyond the infinite-width
  limit.
\newblock \emph{ArXiv}, abs/2209.04882, 2022.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{Arora2019HarnessingTP}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock \emph{Proc. Int. Conf. Learning Rep. (ICLR)}, 2020.

\bibitem[Aubin et~al.(2018)Aubin, Maillard, Barbier, Krzakala, Macris, and
  Zdeborov{\'a}]{Aubin2018TheCM}
Aubin, B., Maillard, A., Barbier, J., Krzakala, F., Macris, N., and
  Zdeborov{\'a}, L.
\newblock The committee machine: computational to statistical gaps in learning
  a two-layers neural network.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019,
  2018.

\bibitem[Aubin et~al.(2020)Aubin, Krzakala, Lu, and
  Zdeborov\'a]{Aubin2020GeneralizationEI}
Aubin, B., Krzakala, F., Lu, Y.~M., and Zdeborov\'a, L.
\newblock Generalization error in high-dimensional perceptrons: Approaching
  bayes error with convex optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12199–12210, 2020.

\bibitem[Barbier et~al.(2017)Barbier, Krzakala, Macris, Miolane, and
  Zdeborov{\'a}]{Barbier2017OptimalEA}
Barbier, J., Krzakala, F., Macris, N., Miolane, L., and Zdeborov{\'a}, L.
\newblock Optimal errors and phase transitions in high-dimensional generalized
  linear models.
\newblock \emph{Proceedings of the National Academy of Sciences of the United
  States of America}, 116:\penalty0 5451 -- 5460, 2017.

\bibitem[Bosch et~al.(2023)Bosch, Panahi, and Hassibi]{Bosch2023PreciseAA}
Bosch, D., Panahi, A., and Hassibi, B.
\newblock Precise asymptotic analysis of deep random feature models.
\newblock \emph{ArXiv}, abs/2302.06210, 2023.

\bibitem[Canatar et~al.(2020)Canatar, Bordelon, and
  Pehlevan]{Canatar2020SpectralBA}
Canatar, A., Bordelon, B., and Pehlevan, C.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock \emph{Nature Communications}, 12, 2020.

\bibitem[Clart\'e et~al.(2022)Clart\'e, Loureiro, Krzakala, and
  Zdeborov\'a]{Clarte2022ASO}
Clart\'e, L., Loureiro, B., Krzakala, F., and Zdeborov\'a, L.
\newblock A study of uncertainty quantification in overparametrized
  high-dimensional models.
\newblock \emph{ArXiv}, abs/2210.12760, 2022.

\bibitem[Cui et~al.(2019)Cui, Saglietti, and Zdeborov\'a]{Cui2019LargeDF}
Cui, H., Saglietti, L., and Zdeborov\'a, L.
\newblock Large deviations for the perceptron model and consequences for active
  learning.
\newblock \emph{Mach. Learn. Sci. Technol.}, 2:\penalty0 45001, 2019.

\bibitem[Cui et~al.(2021)Cui, Loureiro, Krzakala, and
  Zdeborov\'a]{Cui2021GeneralizationER}
Cui, H., Loureiro, B., Krzakala, F., and Zdeborov\'a, L.
\newblock Generalization error rates in kernel regression: The crossover from
  the noiseless to noisy regime.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Cui et~al.(2022)Cui, Loureiro, Krzakala, and
  Zdeborov\'a]{Cui2022ErrorRF}
Cui, H., Loureiro, B., Krzakala, F., and Zdeborov\'a, L.
\newblock Error rates for kernel classification under source and capacity
  conditions.
\newblock \emph{ArXiv}, abs/2201.12655, 2022.

\bibitem[d'Ascoli et~al.(2021)d'Ascoli, Gabri\'e, Sagun, and
  Biroli]{dAscoli2021OnTI}
d'Ascoli, S., Gabri\'e, M., Sagun, L., and Biroli, G.
\newblock On the interplay between data structure and loss function in
  classification problems.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[de~G.~Matthews et~al.(2018)de~G.~Matthews, Rowland, Hron, Turner, and
  Ghahramani]{Matthews2018GaussianPB}
de~G.~Matthews, A.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{NeurIPS Workshop on Advances in Approximate Bayesian
  Inference}, 2018.

\bibitem[El~Karoui(2008)]{el2008spectrum}
El~Karoui, N.
\newblock Spectrum estimation for large dimensional covariance matrices using
  random matrix theory.
\newblock \emph{The Annals of Statistics}, 36\penalty0 (6):\penalty0
  2757--2790, 2008.

\bibitem[Fan \& Wang(2020)Fan and Wang]{Fan2020SpectraOT}
Fan, Z. and Wang, Z.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock \emph{ArXiv}, abs/2005.11879, 2020.

\bibitem[Fischer et~al.(2022)Fischer, Ren'e, Keup, Layer, Dahmen, and
  Helias]{Fischer2022DecomposingNN}
Fischer, K., Ren'e, A., Keup, C., Layer, M., Dahmen, D., and Helias, M.
\newblock Decomposing neural networks as mappings of correlation functions.
\newblock \emph{Physical Review Research}, 2022.

\bibitem[Gabri\'e(2019)]{Gabrie2019MeanfieldIM}
Gabri\'e, M.
\newblock Mean-field inference methods for neural networks.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical}, 53, 2019.

\bibitem[Gerace et~al.(2020)Gerace, Loureiro, Krzakala, M\'ezard, and
  Zdeborov\'a]{Gerace2020GeneralisationEI}
Gerace, F., Loureiro, B., Krzakala, F., M\'ezard, M., and Zdeborov\'a, L.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock \emph{Proceedings of Machine Learning Research}, 37:\penalty0
  3452–3462, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{Ghorbani2019LinearizedTN}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{ArXiv}, abs/1904.12191, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{Ghorbani2020WhenDN}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock When do neural networks outperform kernel methods?
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2021,
  2020.

\bibitem[Goldt et~al.(2020)Goldt, M{\'e}zard, Krzakala, and
  Zdeborov{\'a}]{Goldt2020ModellingTI}
Goldt, S., M{\'e}zard, M., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Modelling the influence of data structure on learning in neural
  networks.
\newblock \emph{Physical Review X}, 4:\penalty0 041044, 2020.

\bibitem[Goldt et~al.(2021)Goldt, Loureiro, Reeves, Krzakala, M\'ezard, and
  Zdeborov\'a]{Goldt2021TheGE}
Goldt, S., Loureiro, B., Reeves, G., Krzakala, F., M\'ezard, M., and
  Zdeborov\'a, L.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock In \emph{MSML}, 2021.

\bibitem[Goldt et~al.(2022)Goldt, Loureiro, Reeves, Krzakala, M{\'e}zard, and
  Zdeborov{\'a}]{goldt2022gaussian}
Goldt, S., Loureiro, B., Reeves, G., Krzakala, F., M{\'e}zard, M., and
  Zdeborov{\'a}, L.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pp.\
  426--471. PMLR, 2022.

\bibitem[Hanin(2022)]{Hanin2022CorrelationFI}
Hanin, B.
\newblock Correlation functions in random fully connected neural networks at
  finite width.
\newblock \emph{ArXiv}, abs/2204.01058, 2022.

\bibitem[Hanin \& Zlokapa(2022)Hanin and Zlokapa]{Hanin2022BayesianIW}
Hanin, B. and Zlokapa, A.
\newblock Bayesian interpolation with deep linear networks.
\newblock \emph{ArXiv}, abs/2212.14457, 2022.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{Hastie2019SurprisesIH}
Hastie, T.~J., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{Annals of statistics}, 50 2:\penalty0 949--986, 2019.

\bibitem[Hron et~al.(2020)Hron, Bahri, Novak, Pennington, and
  Sohl-Dickstein]{Hron2020ExactPD}
Hron, J., Bahri, Y., Novak, R., Pennington, J., and Sohl-Dickstein, J.~N.
\newblock Exact posterior distributions of wide bayesian neural networks.
\newblock \emph{ArXiv}, abs/2006.10541, 2020.

\bibitem[Hu \& Lu(2022{\natexlab{a}})Hu and Lu]{Hu2020UniversalityLF}
Hu, H. and Lu, Y.~M.
\newblock Universality laws for high-dimensional learning with random features.
\newblock \emph{IEEE Transactions on Information Theory}, 2022{\natexlab{a}}.

\bibitem[Hu \& Lu(2022{\natexlab{b}})Hu and Lu]{Hu2022SharpAO}
Hu, H. and Lu, Y.~M.
\newblock Sharp asymptotics of kernel ridge regression beyond the linear
  regime.
\newblock \emph{ArXiv}, abs/2205.06798, 2022{\natexlab{b}}.

\bibitem[Iba(1998)]{Iba1998TheNL}
Iba, Y.
\newblock The nishimori line and bayesian statistics.
\newblock \emph{Journal of Physics A}, 32:\penalty0 3875--3888, 1998.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{Lee2017DeepNN}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.~N.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{Lee2019WideNN}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl-Dickstein,
  J.~N., and Pennington, J.~S.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2020,
  2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{Lee2020FiniteVI}
Lee, J., Schoenholz, S.~S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.~N.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Li \& Sompolinsky(2021)Li and Sompolinsky]{Li2021StatisticalMO}
Li, Q. and Sompolinsky, H.
\newblock Statistical mechanics of deep linear neural networks: The
  backpropagating kernel renormalization.
\newblock \emph{Physical Review X}, 2021.

\bibitem[Louart et~al.(2017)Louart, Liao, and Couillet]{Louart2017ARM}
Louart, C., Liao, Z., and Couillet, R.
\newblock A random matrix approach to neural networks.
\newblock \emph{The Annals of Applied Probability}, 28:\penalty0 1190--1248,
  2017.

\bibitem[Loureiro et~al.(2021)Loureiro, Gerbelot, Cui, Goldt, Krzakala,
  M\'ezard, and Zdeborov\'a]{Loureiro2021CapturingTL}
Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., M\'ezard, M., and
  Zdeborov\'a, L.
\newblock Learning curves of generic features maps for realistic datasets with
  a teacher-student model.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18137--18151, 2021.

\bibitem[Maillard et~al.(2020)Maillard, Loureiro, Krzakala, and
  Zdeborov\'a]{Maillard2020PhaseRI}
Maillard, A., Loureiro, B., Krzakala, F., and Zdeborov\'a, L.
\newblock Phase retrieval in high dimensions: Statistical and computational
  phase transitions.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11071--11082, 2020.

\bibitem[Mei \& Montanari(2019)Mei and Montanari]{Mei2019TheGE}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 75, 2019.

\bibitem[Mei et~al.(2021)Mei, Misiakiewicz, and
  Montanari]{Mei2021GeneralizationEO}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Generalization error of random feature and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2021.

\bibitem[M\'ezard \& Montanari(2002)M\'ezard and
  Montanari]{Mzard2009InformationPA}
M\'ezard, M. and Montanari, A.
\newblock \emph{Information,Physics and computation}.
\newblock Oxford University Press, 2002.

\bibitem[Misiakiewicz(2022)]{Misiakiewicz2022SpectrumOI}
Misiakiewicz, T.
\newblock Spectrum of inner-product kernel matrices in the polynomial regime
  and multiple descent phenomenon in kernel ridge regression.
\newblock \emph{ArXiv}, abs/2204.10425, 2022.

\bibitem[Montanari \& Saeed(2022)Montanari and
  Saeed]{Montanari2022UniversalityOE}
Montanari, A. and Saeed, B.
\newblock Universality of empirical risk minimization.
\newblock \emph{Conference on Learning Theory}, pp.\  4310--4312, 2022.

\bibitem[Montanari et~al.(2019)Montanari, Ruan, Sohn, and
  Yan]{montanari2019generalization}
Montanari, A., Ruan, F., Sohn, Y., and Yan, J.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock \emph{ArXiv}, abs/1911.01544, 2019.

\bibitem[Neal(1994)]{Neal1996PriorsFI}
Neal, R.~M.
\newblock Priors for infinite networks (tech. rep. no. crg-tr-94-1).
\newblock \emph{University of Toronto}, 1994.

\bibitem[Nishimori(2001)]{Nishimori2001StatisticalPO}
Nishimori, H.
\newblock \emph{Statistical Physics of Spin Glasses and Information
  Processing}.
\newblock Oxford:Clarendon, 2001.

\bibitem[Opper \& Haussler(1991)Opper and Haussler]{Opper1991GeneralizationPO}
Opper and Haussler.
\newblock Generalization performance of bayes optimal classification algorithm
  for learning a perceptron.
\newblock \emph{Physical review letters}, 66 20:\penalty0 2677--2680, 1991.

\bibitem[Parisi(1979)]{Replica}
Parisi, G.
\newblock Towards a mean field theory for spin glasses.
\newblock \emph{Phys. Lett}, 73\penalty0 (A):\penalty0 203--205, 1979.

\bibitem[Parisi(1983)]{Replica2}
Parisi, G.
\newblock Order parameter for spin glasses.
\newblock \emph{Phys. Rev. Lett}, 50:\penalty0 1946--1948, 1983.

\bibitem[Pennington \& Worah(2019)Pennington and
  Worah]{Pennington2019NonlinearRM}
Pennington, J. and Worah, P.
\newblock Nonlinear random matrix theory for deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019,
  2019.

\bibitem[Piccioli et~al.(2023)Piccioli, Troiani, and
  Zdeborov{\'a}]{piccioli2023gibbs}
Piccioli, G., Troiani, E., and Zdeborov{\'a}, L.
\newblock Gibbs sampling the posterior of neural networks.
\newblock \emph{arXiv preprint arXiv:2306.02729}, 2023.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{Rahimi2007RandomFF}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{NIPS}, 2007.

\bibitem[Roberts et~al.(2021)Roberts, Yaida, and Hanin]{Roberts2021ThePO}
Roberts, D.~A., Yaida, S., and Hanin, B.
\newblock The principles of deep learning theory.
\newblock \emph{ArXiv}, abs/2106.10165, 2021.

\bibitem[Sahraee-Ardakan et~al.(2022)Sahraee-Ardakan, Emami, Pandit, Rangan,
  and Fletcher]{SahraeeArdakan2022KernelMA}
Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A.~K.
\newblock Kernel methods and multi-layer perceptrons learn linear models in
  high dimensions.
\newblock \emph{ArXiv}, abs/2201.08082, 2022.

\bibitem[Schr\"oder et~al.(2023)Schr\"oder, Cui, Dmitriev, and
  Loureiro]{DRF2023}
Schr\"oder, D., Cui, H., Dmitriev, D., and Loureiro, B.
\newblock Deterministic equivalent and error universality of deep random
  features learning.
\newblock \emph{ArXiv}, abs/2302.00401, 2023.

\bibitem[Schwarze(1993)]{schwarze1993learning}
Schwarze, H.
\newblock Learning a rule in a multilayer neural network.
\newblock \emph{Journal of Physics A: Mathematical and General}, 26\penalty0
  (21):\penalty0 5781, 1993.

\bibitem[Seung et~al.(1992)Seung, Sompolinsky, and
  Tishby]{seung1992statistical}
Seung, H.~S., Sompolinsky, H., and Tishby, N.
\newblock Statistical mechanics of learning from examples.
\newblock \emph{Physical review A}, 45\penalty0 (8):\penalty0 6056, 1992.

\bibitem[Talagrand(2006)]{talagrand2006parisi}
Talagrand, M.
\newblock The parisi formula.
\newblock \emph{Annals of mathematics}, pp.\  221--263, 2006.

\bibitem[Thrampoulidis et~al.(2018)Thrampoulidis, Abbasi, and
  Hassibi]{thrampoulidis2018precise}
Thrampoulidis, C., Abbasi, E., and Hassibi, B.
\newblock Precise error analysis of regularized $ m $-estimators in high
  dimensions.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (8):\penalty0 5592--5628, 2018.

\bibitem[Watkin et~al.(1993)Watkin, Rau, and Biehl]{watkin1993statistical}
Watkin, T.~L., Rau, A., and Biehl, M.
\newblock The statistical mechanics of learning a rule.
\newblock \emph{Reviews of Modern Physics}, 65\penalty0 (2):\penalty0 499,
  1993.

\bibitem[Wu \& Xu(2020)Wu and Xu]{Wu2020OnTO}
Wu, D. and Xu, J.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10112--10123, 2020.

\bibitem[Xiao \& Pennington(2022)Xiao and Pennington]{xiao2022precise}
Xiao, L. and Pennington, J.
\newblock Precise learning curves and higher-order scaling limits for dot
  product kernel regression.
\newblock \emph{ArXiv}, abs/2205.14846, 2022.

\bibitem[Yaida(2019)]{Yaida2019NonGaussianPA}
Yaida, S.
\newblock Non-gaussian processes and neural networks at finite widths.
\newblock In \emph{Mathematical and Scientific Machine Learning}, 2019.

\bibitem[Zavatone-Veth et~al.(2021)Zavatone-Veth, Canatar, and
  Pehlevan]{ZavatoneVeth2021AsymptoticsOR}
Zavatone-Veth, J.~A., Canatar, A., and Pehlevan, C.
\newblock Asymptotics of representation learning in finite bayesian neural
  networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2022,
  2021.

\bibitem[Zavatone-Veth et~al.(2022)Zavatone-Veth, Tong, and
  Pehlevan]{ZavatoneVeth2022ContrastingRA}
Zavatone-Veth, J.~A., Tong, W., and Pehlevan, C.
\newblock Contrasting random and learned features in deep bayesian linear
  regression.
\newblock \emph{Physical review. E}, 105 6-1:\penalty0 064118, 2022.

\bibitem[Zdeborov{\'a} \& Krzakala(2015)Zdeborov{\'a} and
  Krzakala]{Zdeborov2015StatisticalPO}
Zdeborov{\'a}, L. and Krzakala, F.
\newblock Statistical physics of inference: thresholds and algorithms.
\newblock \emph{Advances in Physics}, 65:\penalty0 453 -- 552, 2015.

\end{thebibliography}
