@inproceedings{el2020genuinely,
  title={Genuinely distributed byzantine machine learning},
  author={El-Mhamdi, El-Mahdi and Guerraoui, Rachid and Guirguis, Arsany and Hoang, L{\^e} Nguy{\^e}n and Rouault, S{\'e}bastien},
  booktitle={Proceedings of the 39th Symposium on Principles of Distributed Computing},
  pages={355--364},
  year={2020}
}

@article{he2020byzantine,
  title={Byzantine-robust learning on heterogeneous datasets via resampling},
  author={He, Lie and Karimireddy, Sai Praneeth and Jaggi, Martin},
  journal={arXiv preprint arXiv:2006.09365v3},
  year={2020}
}

@article{gupta2021byzantine2,
  title={Byzantine Fault-Tolerance in Peer-to-Peer Distributed Gradient-Descent},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  journal={arXiv preprint arXiv:2101.12316},
  year={2021}
}

@inproceedings{gupta2021byzantine,
  title={Byzantine Fault-Tolerance in Decentralized Optimization under 2f-Redundancy},
  author={Gupta, Nirupam and Doan, Thinh T and Vaidya, Nitin H},
  booktitle={2021 American Control Conference (ACC)},
  pages={3632--3637},
  year={2021},
  organization={IEEE}
}

@inproceedings{bulusu2020distributed,
  title={On distributed stochastic gradient descent for nonconvex functions in the presence of byzantines},
  author={Bulusu, Saikiran and Khanduri, Prashant and Sharma, Pranay and Varshney, Pramod K},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3137--3141},
  year={2020},
  organization={IEEE}
}

@article{gorbunov2021marina,
  title={MARINA: Faster Non-Convex Distributed Learning with Compression},
  author={Gorbunov, Eduard and Burlachenko, Konstantin and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2102.07845},
  year={2021}
}

@inproceedings{NEURIPS2020_ef9280fb,
 author = {Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richtarik, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20889--20900},
 publisher = {Curran Associates, Inc.},
 title = {Linearly Converging Error Compensated SGD},
 url = {https://proceedings.neurips.cc/paper/2020/file/ef9280fbc5317f17d480e4d4f61b3751-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v130-gorbunov21a,
  title = 	 { Local SGD: Unified Theory and New Efficient Methods },
  author =       {Gorbunov, Eduard and Hanzely, Filip and Richtarik, Peter},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3556--3564},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/gorbunov21a/gorbunov21a.pdf},
  url = 	 {http://proceedings.mlr.press/v130/gorbunov21a.html},
  abstract = 	 { We present a unified framework for analyzing local SGD methods in the convex and strongly convex regimes for distributed/federated training of supervised machine learning models. We recover several known methods as a special case of our general framework, including Local SGD/FedAvg, SCAFFOLD, and several variants of SGD not originally designed for federated learning. Our framework covers both the identical and heterogeneous data settings, supports both random and deterministic number of local steps, and can work with a wide array of local stochastic gradient estimators, including shifted estimators which are able to adjust the fixed points of local iterations for faster convergence. As an application of our framework, we develop multiple novel FL optimizers which are superior to existing methods. In particular, we develop the first linearly converging local SGD method which does not require any data homogeneity or other strong assumptions. }
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@inproceedings{defazio2014saga,
  title={SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances In Neural Information Processing Systems},
  year={2014}
}

@inproceedings{zhang2020why,
 author = {Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15383--15393},
 publisher = {Curran Associates, Inc.},
 title = {Why are Adaptive Methods Good for Attention Models?},
 url = {https://proceedings.neurips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{lyu2020privacy,
  title={Privacy and Robustness in Federated Learning: Attacks and Defenses},
  author={Lyu, Lingjuan and Yu, Han and Ma, Xingjun and Sun, Lichao and Zhao, Jun and Yang, Qiang and Yu, Philip S},
  journal={arXiv preprint arXiv:2012.06337},
  year={2020}
}

@article{xu2020towards,
  title={Towards Building a Robust and Fair Federated Learning System},
  author={Xu, Xinyi and Lyu, Lingjuan},
  journal={arXiv preprint arXiv:2011.10464},
  year={2020}
}

@article{yang2019bridge,
  title={BRIDGE: Byzantine-resilient decentralized gradient descent},
  author={Yang, Zhixiong and Bajwa, Waheed U},
  journal={arXiv preprint arXiv:1908.08098},
  year={2019}
}

@article{yang2019byrdie,
  title={ByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning},
  author={Yang, Zhixiong and Bajwa, Waheed U},
  journal={IEEE Transactions on Signal and Information Processing over Networks},
  volume={5},
  number={4},
  pages={611--627},
  year={2019},
  publisher={IEEE}
}

@article{regatti2020bygars,
  title={ByGARS: Byzantine SGD with Arbitrary Number of Attackers},
  author={Regatti, Jayanth and Chen, Hao and Gupta, Abhishek},
  journal={arXiv preprint arXiv:2006.13421},
  year={2020}
}

@article{rodriguez2020dynamic,
  title={Dynamic Federated Learning Model for Identifying Adversarial Clients},
  author={Rodr{\'\i}guez-Barroso, Nuria and Mart{\'\i}nez-C{\'a}mara, Eugenio and Luz{\'o}n, M and Seco, Gerardo Gonz{\'a}lez and Veganzones, Miguel {\'A}ngel and Herrera, Francisco},
  journal={arXiv preprint arXiv:2007.15030},
  year={2020}
}

@inproceedings{li2019rsa,
  title={RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets},
  author={Li, Liping and Xu, Wei and Chen, Tianyi and Giannakis, Georgios B and Ling, Qing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1544--1551},
  year={2019}
}

@article{ben2013robust,
  title={Robust Consensus in Distributed Networks using Total Variation},
  author={Ben-Ameur, Walid and Bianchi, Pascal and Jakubowicz, J{\'e}r{\'e}mie},
  journal={arXiv preprint arXiv:1309.7264},
  year={2013}
}

@article{peng2021byzantine,
  title={Byzantine-robust decentralized stochastic optimization over static and time-varying networks},
  author={Peng, Jie and Li, Weiyu and Ling, Qing},
  journal={Signal Processing},
  volume={183},
  pages={108020},
  year={2021},
  publisher={Elsevier}
}

@article{karimireddy2020learning,
  title={Learning from History for Byzantine Robust Optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  journal={arXiv preprint arXiv:2012.10333v3},
  year={2020}
}

@inproceedings{
    allen2020byzantine,
    title={Byzantine-Resilient Non-Convex Stochastic Gradient Descent},
    author={Zeyuan Allen-Zhu and Faeze Ebrahimianghazani and Jerry Li and Dan Alistarh},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=PbEHqvFtcS}
}

@article{wu2020federated,
  title={Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks},
  author={Wu, Zhaoxian and Ling, Qing and Chen, Tianyi and Giannakis, Georgios B},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={4583--4596},
  year={2020},
  publisher={IEEE}
}

@article{zhu2021broadcast,
  title={BROADCAST: Reducing Both Stochastic and Compression Noise to Robustify Communication-Efficient Federated Learning},
  author={Zhu, Heng and Ling, Qing},
  journal={arXiv preprint arXiv:2104.06685},
  year={2021}
}

@inproceedings{blanchard2017machine,
  title={Machine learning with adversaries: Byzantine tolerant gradient descent},
  author={Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={118--128},
  year={2017}
}

@inproceedings{yin2018byzantine,
  title={Byzantine-robust distributed learning: Towards optimal statistical rates},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5650--5659},
  year={2018},
  organization={PMLR}
}

@inproceedings{damaskinos2019aggregathor,
  title={Aggregathor: Byzantine machine learning via robust gradient aggregation},
  author={Damaskinos, Georgios and El Mhamdi, El Mahdi and Guerraoui, Rachid and Guirguis, Arsany Hany Abdelmessih and Rouault, S{\'e}bastien Louis Alexandre},
  booktitle={The Conference on Systems and Machine Learning (SysML), 2019},
  year={2019}
}

@InProceedings{mhamdi2018hidden,
  title = 	 {The Hidden Vulnerability of Distributed Learning in {B}yzantium},
  author =       {El Mhamdi, El Mahdi and Guerraoui, Rachid and Rouault, S{\'e}bastien},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3521--3530},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/mhamdi18a/mhamdi18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/mhamdi18a.html},
}

@inproceedings{defazio2018ineffectiveness,
 author = {Defazio, Aaron and Bottou, Leon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Ineffectiveness of Variance Reduced Optimization for Deep Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{pillutla2019robust,
  title={Robust aggregation for federated learning},
  author={Pillutla, Krishna and Kakade, Sham M and Harchaoui, Zaid},
  journal={arXiv preprint arXiv:1912.13445},
  year={2019}
}

@inproceedings{chen2018draco,
  title={Draco: Byzantine-resilient distributed training via redundant gradients},
  author={Chen, Lingjiao and Wang, Hongyi and Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={903--912},
  year={2018},
  organization={PMLR}
}

@inproceedings{rajput2019detox,
 author = {Rajput, Shashank and Wang, Hongyi and Charles, Zachary and Papailiopoulos, Dimitris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation},
 url = {https://proceedings.neurips.cc/paper/2019/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{alistarh2018byzantine,
  title={Byzantine stochastic gradient descent},
  author={Alistarh, Dan and Allen-Zhu, Zeyuan and Li, Jerry},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4618--4628},
  year={2018}
}

@inproceedings{baruch2019little,
 author = {Baruch, Gilad and Baruch, Moran and Goldberg, Yoav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Little Is Enough: Circumventing Defenses For Distributed Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{xie2020fall,
  title={Fall of empires: Breaking Byzantine-tolerant SGD by inner product manipulation},
  author={Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={261--270},
  year={2020},
  organization={PMLR}
}

@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}

@article{Dettmers20158BitAF,
  title={8-Bit Approximations for Parallelism in Deep Learning},
  author={Tim Dettmers},
  journal={ICLR},
  year={2015}
}

@inproceedings{Trask2015ModelingOI,
  title={Modeling Order in Neural Word Embeddings at Scale},
  author={Andrew Trask and David Gilmore and Matthew Russell},
  booktitle={ICML},
  year={2015}
}

@inproceedings{gross_folding,
    author={Michael Gross},
    year={2012},
    title={Folding research recruits unconventional help},
    booktitle={Current Biology. 22 (2): R35–R38},
    doi={10.1016/j.cub.2012.01.008},
    pmid={PMID 22389910}
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{resnet,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}

@INPROCEEDINGS{loshchilov2016sgdr,
 author    = {I. Loshchilov and F. Hutter},
 title     = {SGDR: Stochastic Gradient Descent with Warm Restarts},
 booktitle = {International Conference on Learning Representations (ICLR) 2017 Conference Track},
 year      = {2017},
 month 	= apr,
}

@inproceedings{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={103--112},
  year={2019}
}

@article{jft300data,
  title={Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
  author={Baoyuan Wu and Weidong Chen and Yanbo Fan and Yong Zhang and Jinlong Hou and Jie Liu and Tong Zhang},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={172683-172693}
}

@article{kolesnikovlarge,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  archivePrefix = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1912-11370},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bert,
  title={{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}

@article{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@article{xlnet,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11942}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using gpu model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{adam2015atlas,
  title={ATLAS@ Home: harnessing volunteer computing for HEP},
  author={Adam-Bourdarios, C and Cameron, D and Filip{\v{c}}i{\v{c}}, A and Lancon, E and Wu, Wenjing and others},
  booktitle={Journal of Physics: Conference Series},
  volume={664},
  number={2},
  pages={022009},
  year={2015},
  organization={IOP Publishing}
}

@article{larson_crowd,
author = {Larson, Stefan and Snow, Christopher and Shirts, Michael and Pande, Vijay},
year = {2009},
month = {02},
pages = {},
title = {Folding@Home and Genome@Home: Using distributed computing to tackle previously intractable problems in computational biology},
journal = {arXiv}
}

@article{li2017case,
  title={A Case Study of IPv6 Network Performance: Packet Delay, Loss, and Reordering},
  author={Li, Fuliang and Wang, Xingwei and Pan, Tian and Yang, Jiahai},
  journal={Mathematical Problems in Engineering},
  volume={2017},
  year={2017},
  publisher={Hindawi}
}

@article{Sun2019OptimizingNP,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Peng Sun and Wansen Feng and Ruobing Han and Shengen Yan and Yonggang Wen},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06855}
}

@inproceedings{anderson2004boinc,
  title={Boinc: A system for public-resource computing and storage},
  author={Anderson, David P},
  booktitle={Fifth IEEE/ACM international workshop on grid computing},
  pages={4--10},
  year={2004},
  organization={IEEE}
}

@inproceedings{recht2011hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in neural information processing systems},
  pages={693--701},
  year={2011}
}

@article{zhang2015staleness,
  title={Staleness-aware async-sgd for distributed deep learning},
  author={Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
  journal={arXiv preprint arXiv:1511.05950},
  year={2015}
}


@article{volunteer_dl_async,
author = {Kijsipongse, Ekasit and Piyatumrong, Apivadee and U-ruekolan, Suriya},
year = {2018},
month = {04},
pages = {},
title = {A hybrid GPU cluster and volunteer computing platform for scalable deep learning},
journal = {The Journal of Supercomputing},
doi = {10.1007/s11227-018-2375-9}
}

@article{lc0,
  author = {{Pascutto, Gian-Carlo and Linscott, Gary}},
  title = {Leela Chess Zero},
  url = {http://lczero.org/},
  version = {0.21.0},
  year = {2019},
} 

@INPROCEEDINGS{vc_evolve,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}


@article{moe_first,
author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
title = {Adaptive Mixtures of Local Experts},
year = {1991},
issue_date = {March 1991},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {3},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1991.3.1.79},
doi = {10.1162/neco.1991.3.1.79},
journal = {Neural Computation},
month = mar,
pages = {79–87},
numpages = {9}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{yao2009hierarchical,
  title={Hierarchical mixture of classification experts uncovers interactions between brain regions},
  author={Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-Fei, Li},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2178--2186},
  year={2009}
}

@inproceedings{rasmussen2002infinite,
  title={Infinite mixtures of Gaussian process experts},
  author={Rasmussen, Carl E and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={881--888},
  year={2002}
}


@inproceedings{moe_lifelong,
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
year = {2017},
month = {07},
pages = {7120-7129},
title = {Expert Gate: Lifelong Learning with a Network of Experts},
doi = {10.1109/CVPR.2017.753}
}

@inproceedings{moe_svm,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={633--640},
  year={2002}
}

@article{moe_dirichlet,
  title={Nonlinear models using Dirichlet process mixtures},
  author={Shahbaba, Babak and Neal, Radford},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Aug},
  pages={1829--1850},
  year={2009}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@incollection{pkm,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8546--8557},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}

@inproceedings{can,
  title={A scalable content-addressable network},
  author={Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
  booktitle={Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
  pages={161--172},
  year={2001}
}


@article{chord,
  title={Looking up data in P2P systems},
  author={Balakrishnan, Hari and Kaashoek, M Frans and Karger, David and Morris, Robert and Stoica, Ion},
  journal={Communications of the ACM},
  volume={46},
  number={2},
  pages={43--48},
  year={2003},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pastry,
  title={Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems},
  author={Rowstron, Antony and Druschel, Peter},
  booktitle={IFIP/ACM International Conference on Distributed Systems Platforms and Open Distributed Processing},
  pages={329--350},
  year={2001},
  organization={Springer}
}

@article{tapestry,
author = {Zhao, Ben and Huang, Ling and Stribling, Jeremy and Rhea, Sean and Joseph, Anthony and Kubiatowicz, John},
year = {2003},
month = {07},
pages = {},
title = {Tapestry: A Resilient Global-Scale Overlay for Service Deployment},
volume = {22},
journal = {IEEE Journal on Selected Areas in Communications},
doi = {10.1109/JSAC.2003.818784}
}

@techreport{tewari1998beyond,
  title={Beyond hierarchies: Design considerations for distributed caching on the internet},
  author={Tewari, Renu and Dahlin, Michael and Vin, Harrick and Kay, John},
  institution={Citeseer}
}

@inproceedings{kademlia,
  title={Kademlia: A peer-to-peer information system based on the xor metric},
  author={Maymounkov, Petar and Mazieres, David},
  booktitle={International Workshop on Peer-to-Peer Systems},
  pages={53--65},
  year={2002},
  organization={Springer}
}

@inproceedings{kaashoek2003koorde,
  title={Koorde: A simple degree-optimal distributed hash table},
  author={Kaashoek, M Frans and Karger, David R},
  booktitle={International Workshop on Peer-to-Peer Systems},
  pages={98--107},
  year={2003},
  organization={Springer}
}

@inproceedings{mcmahan2017communication,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
}

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{stale_gradients_can_win,
author = {Dutta, Sanghamitra and Joshi, Gauri and Ghosh, Soumyadip and Dube, Parijat and Nagpurkar, Priya},
year = {2018},
month = {03},
pages = {},
title = {Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD}
}

@article{gradient_checkpointing_dl,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{gradient_checkpointing_autograd,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9051--9062},
  year={2019}
}

@incollection{NIPS2019_8736,
title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4901--4910},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf}
}

@inproceedings{sybil_attacks_dht,
  title={Real-world sybil attacks in BitTorrent mainline DHT},
  author={Wang, Liang and Kangasharju, Jussi},
  booktitle={2012 IEEE Global Communications Conference (GLOBECOM)},
  pages={826--832},
  year={2012},
  organization={IEEE}
}

@article{sybil_nodes,
  title={Sybil nodes as a mitigation strategy against sybil attack},
  author={Trifa, Zied and Khemakhem, Maher},
  journal={Procedia Computer Science},
  volume={32},
  pages={1135--1140},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{dos_resistance,
  title={A denial-of-service resistant DHT},
  author={Awerbuch, Baruch and Scheideler, Christian},
  booktitle={International Symposium on Distributed Computing},
  pages={33--47},
  year={2007},
  organization={Springer}
}

@article{urdaneta2011survey,
  title={A survey of DHT security techniques},
  author={Urdaneta, Guido and Pierre, Guillaume and Steen, Maarten Van},
  journal={ACM Computing Surveys (CSUR)},
  volume={43},
  number={2},
  pages={1--49},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@article{bagdasaryan2018backdoor,
  title={How to backdoor federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:1807.00459},
  year={2018}
}

@article{bhagoji2018analyzing,
  title={Analyzing federated learning through an adversarial lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  journal={arXiv preprint arXiv:1811.12470},
  year={2018}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@ARTICLE{seq2seqvis,
author={H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
year={2019},
volume={25},
number={1},
pages={353-363},
keywords={data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP},
doi={10.1109/TVCG.2018.2865044},
ISSN={2160-9306},
month={Jan},}

@article{carter2019activation,
  title={Activation atlas},
  author={Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  journal={Distill},
  volume={4},
  number={3},
  pages={e15},
  year={2019}
}

@inproceedings{pipedream,
author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
title = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359646},
doi = {10.1145/3341301.3359646},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {1–15},
numpages = {15},
location = {Huntsville, Ontario, Canada},
series = {SOSP ’19}
}

@article{pipemare,
  title={PipeMare: Asynchronous Pipeline Parallel DNN Training},
  author={Bowen Yang and Jian Zhang and Jonathan Li and Christopher R{\'e} and Christopher R. Aberger and Christopher De Sa},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.05124}
}

@article{valiant1990bridging,
  title={A bridging model for parallel computation},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={33},
  number={8},
  pages={103--111},
  year={1990},
  publisher={ACM New York, NY, USA}
}
@misc{goyal2017accurate,
    title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
    author={Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
    year={2017},
    eprint={1706.02677},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{
lamb,
title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Syx4wnEtvH}
}


@article{natural_compression,
  author    = {Samuel Horvath and
               Chen{-}Yu Ho and
               Ludovit Horvath and
               Atal Narayan Sahu and
               Marco Canini and
               Peter Richt{\'{a}}rik},
  title     = {Natural Compression for Distributed Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1905.10988},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10988},
  archivePrefix = {arXiv},
  eprint    = {1905.10988},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sukhov2016generating,
  title={Generating a function for network delay},
  author={Sukhov, Andrei M and Astrakhantseva, MA and Pervitsky, AK and Boldyrev, SS and Bukatov, AA},
  journal={Journal of High Speed Networks},
  volume={22},
  number={4},
  pages={321--333},
  year={2016},
  publisher={IOS Press}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1627--1635},
  year={2017},
  organization={JMLR. org}
}

@misc{ma2019hsic,
    title={The HSIC Bottleneck: Deep Learning without Back-Propagation},
    author={Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
    year={2019},
    eprint={1908.01580},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2902--2911},
  year={2017},
  organization={JMLR. org}
}

@misc{hendrycks2016gaussian,
    title={Gaussian Error Linear Units (GELUs)},
    author={Dan Hendrycks and Kevin Gimpel},
    year={2016},
    eprint={1606.08415},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{zero,
  title={ZeRO: Memory optimizations Toward Training Trillion Parameter Models},
  author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
  journal={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2020},
  pages={1-16}
}

@misc{tnlg,
  author = {Corby Rosset},
  title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
  howpublished = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
  note = {Accessed: 2020-2-10}
}

@misc{gpt3cost,
  author = {Elliot Turner},
  note = {Estimate of GPT-3 training cost based on public cloud GPU/TPU cost models, from Elliot Turner's personal page (accessed on May 29, 2020)}
}

@misc{lambdabenchmarks,
  author = {Stephen Balaban , Chuan Li},
  title = {Deep Learning GPU benchmarks, Lambda Labs website, 2018/10/08}
}

@manual{folding_timeline,
      title  = "Folding@home project timeline",
      note   = "\url{https://foldingathome.org/project-timeline}(accessed on May 30, 2020)",
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}

@INPROCEEDINGS{desell2017,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}

@book{hammurabi,
  title={The Code of Hammurabi, King of Babylon: About 2250 BC: Autographed Text, Transliteration, Translation, Glossary Index of Subjects, Lists of Proper Names, Signs, Numuerals...},
  author={Hammurabi, King of Babylon and Harper, Robert Francis},
  year={1904},
  publisher={University of Chicago Press},
  note={Page 11, \textsection 1},
  url={https://books.google.ru/books?id=jeLz_BYUoeQC&pg=PA11},
}

@inproceedings{sybil,
  title={The Sybil Attack},
  author={John R. Douceur},
  booktitle={IPTPS},
  year={2002}
}


@inproceedings{decker2013information,
  title={Information propagation in the {B}itcoin network},
  author={Decker, Christian and Wattenhofer, Roger},
  booktitle={IEEE P2P 2013 Proceedings},
  pages={1--10},
  year={2013},
  organization={IEEE}
}

@article{vyzovitis2020gossipsub,
  title={Gossip{S}ub: Attack-resilient message propagation in the {F}ilecoin and {ETH2.0} networks},
  author={Vyzovitis, Dimitris and Napora, Yusef and McCormick, Dirk and Dias, David and Psaras, Yiannis},
  journal={arXiv preprint arXiv:2007.02754},
  year={2020}
}


@article{pease1980reaching,
  title={Reaching agreement in the presence of faults},
  author={Pease, Marshall and Shostak, Robert and Lamport, Leslie},
  journal={Journal of the ACM (JACM)},
  volume={27},
  number={2},
  pages={228--234},
  year={1980},
  publisher={ACM New York, NY, USA}
}

@article{dolev1983authenticated,
  title={Authenticated algorithms for Byzantine agreement},
  author={Dolev, Danny and Strong, H. Raymond},
  journal={SIAM Journal on Computing},
  volume={12},
  number={4},
  pages={656--666},
  year={1983},
  publisher={SIAM}
}

@article{rivest1978method,
  title={A method for obtaining digital signatures and public-key cryptosystems},
  author={Rivest, Ronald L and Shamir, Adi and Adleman, Leonard},
  journal={Communications of the ACM},
  volume={21},
  number={2},
  pages={120--126},
  year={1978},
  publisher={ACM New York, NY, USA}
}

@article{goldwasser1988digital,
  title={A digital signature scheme secure against adaptive chosen-message attacks},
  author={Goldwasser, Shafi and Micali, Silvio and Rivest, Ronald L},
  journal={SIAM Journal on computing},
  volume={17},
  number={2},
  pages={281--308},
  year={1988},
  publisher={SIAM}
}

@inproceedings{hirt2014multi,
  title={Multi-valued byzantine broadcast: The t< n case},
  author={Hirt, Martin and Raykov, Pavel},
  booktitle={International Conference on the Theory and Application of Cryptology and Information Security},
  pages={448--465},
  year={2014},
  organization={Springer}
}

@inproceedings{abraham2019communication,
  title={Communication complexity of byzantine agreement, revisited},
  author={Abraham, Ittai and Chan, TH Hubert and Dolev, Danny and Nayak, Kartik and Pass, Rafael and Ren, Ling and Shi, Elaine},
  booktitle={Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing},
  pages={317--326},
  year={2019}
}

@article{blum1983coin,
  title={Coin flipping by telephone a protocol for solving impossible problems},
  author={Blum, Manuel},
  journal={ACM SIGACT News},
  volume={15},
  number={1},
  pages={23--27},
  year={1983},
  publisher={ACM New York, NY, USA}
}

@inproceedings{cleve1986limits,
  title={Limits on the security of coin flips when half the processors are faulty},
  author={Cleve, Richard},
  booktitle={Proceedings of the eighteenth annual ACM symposium on Theory of computing},
  pages={364--369},
  year={1986}
}

@inproceedings{rabin1989verifiable,
  title={Verifiable secret sharing and multiparty protocols with honest majority},
  author={Rabin, Tal and Ben-Or, Michael},
  booktitle={Proceedings of the twenty-first annual ACM symposium on Theory of computing},
  pages={73--85},
  year={1989}
}

@inproceedings{moran2009optimally,
  title={An optimally fair coin toss},
  author={Moran, Tal and Naor, Moni and Segev, Gil},
  booktitle={Theory of Cryptography Conference},
  pages={1--18},
  year={2009},
  organization={Springer}
}

@inproceedings{beimel2010protocols,
  title={Protocols for multiparty coin toss with dishonest majority},
  author={Beimel, Amos and Omri, Eran and Orlov, Ilan},
  booktitle={Annual Cryptology Conference},
  pages={538--557},
  year={2010},
  organization={Springer}
}

@inproceedings{zhang2019efficient,
  title={Efficient multi-party private set intersection against malicious adversaries},
  author={Zhang, En and Liu, Feng-Hao and Lai, Qiqi and Jin, Ganggang and Li, Yu},
  booktitle={Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
  pages={93--104},
  year={2019}
}


@misc{vissl,
  author =       {Priya Goyal and Quentin Duval and Jeremy Reizenstein and Matthew Leavitt and Min Xu and
                  Benjamin Lefaudeux and Mannat Singh and Vinicius Reis and Mathilde Caron and Piotr Bojanowski and
                  Armand Joulin and Ishan Misra},
  title =        {VISSL},
  url = {https://github.com/facebookresearch/vissl},
  year =         {2021}
}

@misc{horovod,
      title={Horovod: fast and easy distributed deep learning in TensorFlow}, 
      author={Alexander Sergeev and Mike Del Balso},
      year={2018},
      eprint={1802.05799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}










%% COPYPASTE %%

@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}

@article{Dettmers20158BitAF,
  title={8-Bit Approximations for Parallelism in Deep Learning},
  author={Tim Dettmers},
  journal={ICLR},
  year={2015}
}

@misc{TURN,
	series =	{Request for Comments},
	number =	8656,
	howpublished =	{RFC 8656},
	publisher =	{RFC Editor},
	doi =		{10.17487/RFC8656},
	url =		{https://rfc-editor.org/rfc/rfc8656.txt},
        author =	{Tirumaleswar Reddy.K and Alan Johnston and Philip Matthews and Jonathan Rosenberg},
	title =		{{Traversal Using Relays around NAT (TURN): Relay Extensions to Session Traversal Utilities for NAT (STUN)}},
	pagetotal =	79,
	year =		2020,
	month =		feb,
}

@article{STUN,
author = {Weinberger, J. and Huitema, C. and Mahy, R.},
year = {2003},
month = {01},
pages = {},
title = {STUN—Simple traversal of user datagram protocol (UDP) through network address translators (NATs)},
journal = {IETF RFC 3489}
}

@inproceedings{NATFord,
  title={Peer-to-Peer Communication Across Network Address Translators},
  author={B. Ford and P. Srisuresh and Dan Kegel},
  booktitle={USENIX Annual Technical Conference, General Track},
  year={2005}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Misc from intro section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{protein2vec,
author = {Gao, Jianliang and Tian, Ling and Lv, Tengfei and Wang, Jianxin and Song, Bo and Hu, Xiaohua},
year = {2019},
month = {08},
pages = {1-1},
title = {Protein2Vec: Aligning Multiple PPI Networks with Representation Learning},
volume = {PP},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
doi = {10.1109/TCBB.2019.2937771}
}

@misc{bertologybiology,
    title={BERTology Meets Biology: Interpreting Attention in Protein Language Models},
    author={Jesse Vig and Ali Madani and Lav R. Varshney and Caiming Xiong and Richard Socher and Nazneen Fatema Rajani},
    year={2020},
    eprint={2006.15222},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2006.15222}
}

 @misc{torrent, 
 title={{The BitTorrent Protocol Specification}}, 
     author={Bram Cohen},
    year={2008},

 howpublished={\url{http://www.bittorrent.org/beps/bep_0003.html}}} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Federated Learning section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{FedLearningOriginal,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
}

@inproceedings{FedLearningAtScale,
title	= {Towards Federated Learning at Scale: System Design},
author	= {K. A. Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloé M Kiddon and Jakub Konečný and Stefano Mazzocchi and Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
year	= {2019},
URL	= {https://arxiv.org/abs/1902.01046},
note	= {To appear},
booktitle	= {SysML 2019}
}

@inproceedings{PracticalSecureAggregation,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@article{FedLearningDiffPrivacy,
  author    = {Kang Wei and
               Jun Li and
               Ming Ding and
               Chuan Ma and
               Howard H. Yang and
               Farhad Farokhi and
               Shi Jin and
               Tony Q. S. Quek and
               H. Vincent Poor},
  title     = {Federated Learning with Differential Privacy: Algorithms and Performance
               Analysis},
  journal   = {CoRR},
  volume    = {abs/1911.00222},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.00222},
  archivePrefix = {arXiv},
  eprint    = {1911.00222},
  timestamp = {Thu, 14 Jan 2021 15:41:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-00222.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{FedLearningAdvancesProblems,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}

@misc{FedLearningDecentralized,
      title={Decentralized Federated Learning Preserves Model and Data Privacy}, 
      author={Thorsten Wittkopp and Alexander Acker},
      year={2021},
      eprint={2102.00880},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Federated Learning section end %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Volunteer computing section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@manual{folding_timeline,
      title  = "Folding@home project timeline",
      note   = "\url{https://foldingathome.org/project-timeline}(accessed on May 30, 2020)",
}

% https://folding.typepad.com/news/2007/09/crossing-the-pe.html
@article{folding_petaflop,
    author = {Gross, Michael},
    year = {2012},
    month = {01},
    pages = {R35-8},
    title = {Folding research recruits unconventional help},
    volume = {22},
    journal = {Current biology : CB},
    doi = {10.1016/j.cub.2012.01.008}
}

@manual{folding_exaflop_1,
      title  = "Folding@home active CPU and GPU by OS",
      note   = "\url{https://archive.md/20200412111010/https://stats.foldingathome.org/os}(accessed on Apr 29, 2021)",
}

@manual{folding_exaflop_2,
    author = {Merritt, Rick},
    year = {2020},
    month = {04},
    title  = "Folding@home gets 1.5+ Exaflops to Fight COVID-19",
    note   = "\url{https://blogs.nvidia.com/blog/2020/04/01/foldingathome-exaflop-coronavirus/}(accessed on Apr 29, 2021)",
}

@article{folding_ps3,
    author = {Narumi, Tetsu and Kameoka, Shun and Taiji, Makoto and Yasuoka, Kenji},
    year = {2008},
    month = {01},
    pages = {3108-3125},
    title = {Accelerating Molecular Dynamics Simulations on PlayStation 3 Platform Using Virtual-GRAPE Programming Model},
    volume = {30},
    journal = {SIAM J. Scientific Computing},
    doi = {10.1137/070692054}
}

@article{larson_crowd,
author = {Larson, Stefan and Snow, Christopher and Shirts, Michael and Pande, Vijay},
year = {2009},
month = {02},
pages = {},
title = {Folding@Home and Genome@Home: Using distributed computing to tackle previously intractable problems in computational biology},
journal = {arXiv}
}


@article{lhc_at_home,
author = {Barranco, Javier and Cai, Yunhi and Cameron, David and Crouch, Matthew and De Maria, Riccardo and Field, Laurence and Giovannozzi, M. and Hermes, Pascal and Høimyr, Nils and Kaltchev, Dobrin and Karastathis, Nikos and Luzzi, Cinzia and Maclean, Ewen and Mcintosh, Eric and Mereghetti, Alessio and Molson, James and Nosochkov, Yuri and Pieloni, Tatiana and Reid, Ivan and Zacharov, Igor},
year = {2017},
month = {12},
pages = {},
title = {LHC@Home: a BOINC-based volunteer computing infrastructure for physics studies at CERN},
volume = {7},
journal = {Open Engineering},
doi = {10.1515/eng-2017-0042}
}

@article{seti_at_home,
    author = {Anderson, David and Cobb, Jeff and Korpela, Eric and Lebofsky, Matt and Werthimer, Dan},
    year = {2002},
    month = {11},
    pages = {56-61},
    title = {SETI@home: An Experiment in Public-Resource Computing},
    volume = {45},
    journal = {Commun. ACM},
    doi = {10.1145/581571.581573}
}

@misc{clemens2021mlds,
  title={MLDS: A Dataset for Weight-Space Analysis of Neural Networks}, 
  author={John Clemens},
  year={2021},
  eprint={2104.10555},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@manual{vastai,
      title  = "vast.ai ",
      note   = "\url{https://vast.ai}",
}


@misc{atre2021distributed,
  title={Distributed Deep Learning Using Volunteer Computing-Like Paradigm}, 
  author={Medha Atre and Birendra Jha and Ashwini Rao},
  year={2021},
  eprint={2103.08894},
  archivePrefix={arXiv},
  primaryClass={cs.DC}
}

@article{qmc_at_home,
	doi = {10.1088/1361-648x/aab9c3},
	url = {https://doi.org/10.1088/1361-648x/aab9c3},
	year = 2018,
	month = {apr},
	publisher = {{IOP} Publishing},
	volume = {30},
	number = {19},
	pages = {195901},
	author = {Jeongnim Kim and Andrew D Baczewski and Todd D Beaudet and Anouar Benali and M Chandler Bennett and Mark A Berrill and Nick S Blunt and Edgar Josu{\'{e}} Landinez Borda and Michele Casula and David M Ceperley and Simone Chiesa and Bryan K Clark and Raymond C Clay and Kris T Delaney and Mark Dewing and Kenneth P Esler and Hongxia Hao and Olle Heinonen and Paul R C Kent and Jaron T Krogel and Ilkka Kylänpää and Ying Wai Li and M Graham Lopez and Ye Luo and Fionn D Malone and Richard M Martin and Amrita Mathuriya and Jeremy McMinis and Cody A Melton and Lubos Mitas and Miguel A Morales and Eric Neuscamman and William D Parker and Sergio D Pineda Flores and Nichols A Romero and Brenda M Rubenstein and Jacqueline A R Shea and Hyeondeok Shin and Luke Shulenburger and Andreas F Tillack and Joshua P Townsend and Norm M Tubman and Brett Van Der Goetz and Jordan E Vincent and D ChangMo Yang and Yubo Yang and Shuai Zhang and Luning Zhao},
	title = {{QMCPACK}: an open sourceab initioquantum Monte Carlo package for the electronic structure of atoms, molecules and solids},
	journal = {Journal of Physics: Condensed Matter},
}

@inproceedings{hivemind_dmoe,
 author = {Ryabinin, Max and Gusev, Anton},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3659--3672},
 publisher = {Curran Associates, Inc.},
 title = {Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts},
 url = {https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{dedloc,
  author    = {Michael Diskin and
               Alexey Bukhtiyarov and
               Max Ryabinin and
               Lucile Saulnier and
               Quentin Lhoest and
               Anton Sinitsin and
               Dmitriy Popov and
               Dmitry Pyrkin and
               Maxim Kashirin and
               Alexander Borzunov and
               Albert Villanova del Moral and
               Denis Mazur and
               Ilia Kobelev and
               Yacine Jernite and
               Thomas Wolf and
               Gennady Pekhimenko},
  title     = {Distributed Deep Learning in Open Collaborations},
  journal   = {CoRR},
  volume    = {abs/2106.10207},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.10207},
  eprinttype = {arXiv},
  eprint    = {2106.10207},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-10207.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{hivemind_moshpit,
  title={Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices}, 
  author={Max Ryabinin and Eduard Gorbunov and Vsevolod Plokhotnyuk and Gennady Pekhimenko},
  year={2021},
  eprint={2103.03239},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Volunteer computing section end %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Trask2015ModelingOI,
  title={Modeling Order in Neural Word Embeddings at Scale},
  author={Andrew Trask and David Gilmore and Matthew Russell},
  booktitle={ICML},
  year={2015}
}

@inproceedings{gross_folding,
    author={Michael Gross},
    year={2012},
    title={Folding research recruits unconventional help},
    booktitle={Current Biology. 22 (2): R35–R38},
    doi={10.1016/j.cub.2012.01.008},
    pmid={PMID 22389910}
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@inproceedings{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={103--112},
  year={2019}
}

@article{megatron2,
  title={Efficient Large-Scale Language Model Training on GPU Clusters},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  journal={arXiv preprint arXiv:2104.04473},
  year={2021}
}

@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{jft300data,
  title={Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
  author={Baoyuan Wu and Weidong Chen and Yanbo Fan and Yong Zhang and Jinlong Hou and Jie Liu and Tong Zhang},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={172683-172693}
}

@article{kolesnikovlarge,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  archivePrefix = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1912-11370},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mpich_rabenseifner,
author = {Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
title = {Optimization of Collective Communication Operations in MPICH},
year = {2005},
issue_date = {February  2005},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {19},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342005051521},
doi = {10.1177/1094342005051521},
abstract = {We describe our work on improving the performance of collective communication operations in MPICH for clusters connected by switched networks. For each collective operation, we use multiple algorithms depending on the message size, with the goal of minimizing latency for short messages and minimizing bandwidth use for long messages. Although we have implemented new algorithms for all MPI Message Passing Interface collective operations, because of limited space we describe only the algorithms for allgather, broadcast, all-to-all, reduce-scatter, reduce, and allreduce. Performance results on a Myrinet-connected Linux cluster and an IBM SP indicate that, in all cases, the new algorithms significantly outperform the old algorithms used in MPICH on the Myrinet cluster, and, in many cases, they outperform the algorithms used in IBM's MPI on the SP. We also explore in further detail the optimization of two of the most commonly used collective operations, allreduce and reduce, particularly for long messages and nonpower-of-two numbers of processes. The optimized algorithms for these operations perform several times better than the native algorithms on a Myrinet cluster, IBM SP, and Cray T3E. Our results indicate that to achieve the best performance for a collective communication operation, one needs to use a number of different algorithms and select the right algorithm for a particular message size and number of processes.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {49–66},
numpages = {18},
keywords = {reduction, MPI, message passing, Collective communication}
}

@article{nesterov,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Y. Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547}
}
@inproceedings{adam2015atlas,
  title={ATLAS@ Home: harnessing volunteer computing for HEP},
  author={Adam-Bourdarios, C and Cameron, D and Filip{\v{c}}i{\v{c}}, A and Lancon, E and Wu, Wenjing and others},
  booktitle={Journal of Physics: Conference Series},
  volume={664},
  number={2},
  pages={022009},
  year={2015},
  organization={IOP Publishing}
}

@article{li2017case,
  title={A Case Study of IPv6 Network Performance: Packet Delay, Loss, and Reordering},
  author={Li, Fuliang and Wang, Xingwei and Pan, Tian and Yang, Jiahai},
  journal={Mathematical Problems in Engineering},
  volume={2017},
  year={2017},
  publisher={Hindawi}
}

@article{Sun2019OptimizingNP,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Peng Sun and Wansen Feng and Ruobing Han and Shengen Yan and Yonggang Wen},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06855}
}


@article{zhang2015staleness,
  title={Staleness-aware async-sgd for distributed deep learning},
  author={Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
  journal={arXiv preprint arXiv:1511.05950},
  year={2015}
}

@misc{lc0,
  author = {{Pascutto, Gian-Carlo and Linscott, Gary}},
  title = {Leela Chess Zero},
  url = {http://lczero.org/},
  version = {0.21.0},
  year = {2019},
} 

@INPROCEEDINGS{vc_evolve,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}


@article{moe_first,
author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
title = {Adaptive Mixtures of Local Experts},
year = {1991},
issue_date = {March 1991},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {3},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1991.3.1.79},
doi = {10.1162/neco.1991.3.1.79},
journal = {Neural Computation},
month = mar,
pages = {79–87},
numpages = {9}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{yao2009hierarchical,
  title={Hierarchical mixture of classification experts uncovers interactions between brain regions},
  author={Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-Fei, Li},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2178--2186},
  year={2009}
}

@inproceedings{rasmussen2002infinite,
  title={Infinite mixtures of Gaussian process experts},
  author={Rasmussen, Carl E and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={881--888},
  year={2002}
}


@inproceedings{moe_lifelong,
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
year = {2017},
month = {07},
pages = {7120-7129},
title = {Expert Gate: Lifelong Learning with a Network of Experts},
doi = {10.1109/CVPR.2017.753}
}

@inproceedings{moe_svm,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={633--640},
  year={2002}
}

@article{moe_dirichlet,
  title={Nonlinear models using Dirichlet process mixtures},
  author={Shahbaba, Babak and Neal, Radford},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Aug},
  pages={1829--1850},
  year={2009}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@incollection{pkm,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8546--8557},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}

@inproceedings{can,
  title={A scalable content-addressable network},
  author={Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
  booktitle={Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
  pages={161--172},
  year={2001}
}





@techreport{tewari1998beyond,
  title={Beyond hierarchies: Design considerations for distributed caching on the internet},
  author={Tewari, Renu and Dahlin, Michael and Vin, Harrick and Kay, John},
  institution={Citeseer}
}


@misc{i2p,
  title = {Invisible Internet Project (I2P) Project Overview}, 
  author = {jrandom (Pseudonym)}, 
  year = {2003}, 
  month = {August}, 
  howpublished = {\url{https://geti2p.net/_static/pdf/i2p_philosophy.pdf}},
}



@inproceedings{mcmahan2017communication,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
}

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{stale_gradients_can_win,
author = {Dutta, Sanghamitra and Joshi, Gauri and Ghosh, Soumyadip and Dube, Parijat and Nagpurkar, Priya},
year = {2018},
month = {03},
pages = {},
title = {Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD}
}

@article{gradient_checkpointing_dl,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{gradient_checkpointing_autograd,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9051--9062},
  year={2019}
}

@incollection{NIPS2019_8736,
title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4901--4910},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf}
}



@inproceedings{dos_resistance,
  title={A denial-of-service resistant DHT},
  author={Awerbuch, Baruch and Scheideler, Christian},
  booktitle={International Symposium on Distributed Computing},
  pages={33--47},
  year={2007},
  organization={Springer}
}


@article{bagdasaryan2018backdoor,
  title={How to backdoor federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:1807.00459},
  year={2018}
}

@article{bhagoji2018analyzing,
  title={Analyzing federated learning through an adversarial lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  journal={arXiv preprint arXiv:1811.12470},
  year={2018}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@ARTICLE{seq2seqvis,
author={H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
year={2019},
volume={25},
number={1},
pages={353-363},
keywords={data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP},
doi={10.1109/TVCG.2018.2865044},
ISSN={2160-9306},
month={Jan},}

@article{carter2019activation,
  title={Activation atlas},
  author={Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  journal={Distill},
  volume={4},
  number={3},
  pages={e15},
  year={2019}
}


@article{pipemare,
  title={PipeMare: Asynchronous Pipeline Parallel DNN Training},
  author={Bowen Yang and Jian Zhang and Jonathan Li and Christopher R{\'e} and Christopher R. Aberger and Christopher De Sa},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.05124}
}

@article{valiant1990bridging,
  title={A bridging model for parallel computation},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={33},
  number={8},
  pages={103--111},
  year={1990},
  publisher={ACM New York, NY, USA}
}


@article{natural_compression,
  author    = {Samuel Horvath and
               Chen{-}Yu Ho and
               Ludovit Horvath and
               Atal Narayan Sahu and
               Marco Canini and
               Peter Richt{\'{a}}rik},
  title     = {Natural Compression for Distributed Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1905.10988},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10988},
  archivePrefix = {arXiv},
  eprint    = {1905.10988},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sukhov2016generating,
  title={Generating a function for network delay},
  author={Sukhov, Andrei M and Astrakhantseva, MA and Pervitsky, AK and Boldyrev, SS and Bukatov, AA},
  journal={Journal of High Speed Networks},
  volume={22},
  number={4},
  pages={321--333},
  year={2016},
  publisher={IOS Press}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1627--1635},
  year={2017},
  organization={JMLR. org}
}

@misc{ma2019hsic,
    title={The HSIC Bottleneck: Deep Learning without Back-Propagation},
    author={Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
    year={2019},
    eprint={1908.01580},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2902--2911},
  year={2017},
  organization={JMLR. org}
}

@misc{hendrycks2016gaussian,
    title={Gaussian Error Linear Units (GELUs)},
    author={Dan Hendrycks and Kevin Gimpel},
    year={2016},
    eprint={1606.08415},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}



@misc{tnlg,
  author = {Corby Rosset},
  title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
  howpublished = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
  note = {Accessed: 2020-2-10}
}

@misc{gpt3cost,
  author = {Elliot Turner},
  note = {Estimate of GPT-3 training cost based on public cloud GPU/TPU cost models, from Elliot Turner's personal page (accessed on May 29, 2020)}
}

@misc{lambdabenchmarks,
  author = {Stephen Balaban , Chuan Li},
  title = {Deep Learning GPU benchmarks, Lambda Labs website, 2018/10/08}
}


@INPROCEEDINGS{desell2017,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}


@inproceedings{deepgradientcompression,
  title={{Deep Gradient Compression: Reducing the communication bandwidth for distributed training}},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  booktitle={The International Conference on Learning Representations},
  year={2018}
}


 @InProceedings{li2020acceleration, title = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization}, author = {Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richtarik, Peter}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {5895--5904}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/li20g/li20g.pdf}, url = { http://proceedings.mlr.press/v119/li20g.html }} 
 
 @inproceedings{
koloskova2020decentralized,
title={Decentralized Deep Learning with Arbitrary Communication Compression},
author={Anastasia Koloskova and Tao Lin and Sebastian U Stich and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgGCkrKvH}
}

@article{wagma,
   title={Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging},
   ISSN={2161-9883},
   url={http://dx.doi.org/10.1109/TPDS.2020.3040606},
   DOI={10.1109/tpds.2020.3040606},
   journal={IEEE Transactions on Parallel and Distributed Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Li, Shigang and Ben-Nun, Tal and Nadiradze, Giorgi and Digirolamo, Salvatore and Dryden, Nikoli and Alistarh, Dan and Hoefler, Torsten},
   year={2020},
   pages={1–1}
}

@misc{moshpit,
      title={Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices}, 
      author={Max Ryabinin and Eduard Gorbunov and Vsevolod Plokhotnyuk and Gennady Pekhimenko},
      year={2021},
      eprint={2103.03239},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{ps,
  title={Scaling Distributed Machine Learning with the Parameter Server},
  author={Mu Li and D. Andersen and J. Park and Alex Smola and Amr Ahmed and V. Josifovski and J. Long and E. Shekita and Bor-Yiing Su},
  booktitle={BigDataScience '14},
  year={2014}
}


@misc{zeroinfinity,
      title={ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning}, 
      author={Samyam Rajbhandari and Olatunji Ruwase and Jeff Rasley and Shaden Smith and Yuxiong He},
      year={2021},
      eprint={2104.07857},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{zerooffload,
      title={ZeRO-Offload: Democratizing Billion-Scale Model Training}, 
      author={Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
      year={2021},
      eprint={2101.06840},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings {byteps,
author = {Yimin Jiang and Yibo Zhu and Chang Lan and Bairen Yi and Yong Cui and Chuanxiong Guo},
title = {A Unified Architecture for Accelerating Distributed {DNN} Training in Heterogeneous GPU/CPU Clusters},
booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {463--479},
url = {https://www.usenix.org/conference/osdi20/presentation/jiang},
publisher = {{USENIX} Association},
month = nov,
}

@misc{horvath2019natural,
    title={Natural Compression for Distributed Deep Learning},
    author={Samuel Horvath and Chen-Yu Ho and Ludovit Horvath and Atal Narayan Sahu and Marco Canini and Peter Richtarik},
    year={2019},
    eprint={1905.10988},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

 @InProceedings{sgp, title = {Stochastic Gradient Push for Distributed Deep Learning}, author = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {344--353}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/assran19a/assran19a.pdf}, url = { http://proceedings.mlr.press/v97/assran19a.html }} 
 
 @inproceedings{
slowmo,
title={SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum},
author={Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkxJ8REYPH}
}

@misc{mikami2019massively,
      title={Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash}, 
      author={Hiroaki Mikami and Hisahiro Suganuma and Pongsakorn U-chupala and Yoshiki Tanaka and Yuichi Kageyama},
      year={2019},
      eprint={1811.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

 @misc{pytorch_elastic, title={{PyTorch Elastic}}, howpublished={\url{https://pytorch.org/elastic}}} 
 
 @misc{elastic_horovod, title={{Elastic Horovod}}, howpublished={\url{ https://horovod.readthedocs.io/en/stable/elastic_include.html}}}
 
 @inproceedings{parameter_server_first,
author = {Li, Mu},
title = {Scaling Distributed Machine Learning with the Parameter Server},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644155},
doi = {10.1145/2640087.2644155},
abstract = {Big data may contain big values, but also brings lots of challenges to the computing theory, architecture, framework, knowledge discovery algorithms, and domain specific tools and applications. Beyond the 4-V or 5-V characters of big datasets, the data processing shows the features like inexact, incremental, and inductive manner. This brings new research opportunities to research community across theory, systems, algorithms, and applications. Is there some new "theory" for the big data? How to handle the data computing algorithms in an operatable manner? This report shares some view on new challenges identified, and covers some of the application scenarios such as micro-blog data analysis and data processing in building next generation search engines.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {3},
numpages = {1},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{Biggadike05natblaster:establishing,
    author = {Andrew Biggadike and Daniel Ferullo and Geoffrey Wilson and Adrian Perrig},
    title = { NATBLASTER: Establishing TCP connections between hosts behind NATs},
    booktitle = {IN PROCEEDINGS OF ACM SIGCOMM ASIA WORKSHOP},
    year = {2005},
    publisher = {}
}

@article{DBLP:journals/corr/abs-cs-0603074,
  author    = {Bryan Ford and
               Pyda Srisuresh and
               Dan Kegel},
  title     = {Peer-to-Peer Communication Across Network Address Translators},
  journal   = {CoRR},
  volume    = {abs/cs/0603074},
  year      = {2006},
  url       = {http://arxiv.org/abs/cs/0603074},
  archivePrefix = {arXiv},
  eprint    = {cs/0603074},
  timestamp = {Mon, 13 Aug 2018 16:48:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-cs-0603074.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Oscar,
    title = "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
    author = "Ortiz Su{\'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.156",
    pages = "1703--1714",
    abstract = "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",
}

@article{kudo2018subword,
  title={Subword regularization: Improving neural network translation models with multiple subword candidates},
  author={Kudo, Taku},
  journal={arXiv preprint arXiv:1804.10959},
  year={2018}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{feyzmahdavian2016asynchronous,
  title={An asynchronous mini-batch algorithm for regularized stochastic optimization},
  author={Feyzmahdavian, Hamid Reza and Aytekin, Arda and Johansson, Mikael},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={12},
  pages={3740--3754},
  year={2016},
  publisher={IEEE}
}

@inproceedings{arjevani2020tight,
  title={A tight convergence analysis for stochastic gradient descent with delayed updates},
  author={Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan},
  booktitle={Algorithmic Learning Theory},
  pages={111--132},
  year={2020},
  organization={PMLR}
}

@inproceedings{MLSYS2019_d09bf415,
 author = {Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {A. Talwalkar and V. Smith and M. Zaharia},
 pages = {132--145},
 title = {Priority-based Parameter Propagation for Distributed DNN Training},
 url = {https://proceedings.mlsys.org/paper/2019/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf},
 volume = {1},
 year = {2019}
}


@inproceedings{agarwal2011distributed,
  title={Distributed delayed stochastic optimization},
  author={Agarwal, Alekh and Duchi, John C},
  booktitle={Proceedings of the 24th International Conference on Neural Information Processing Systems},
  pages={873--881},
  year={2011}
}

@inproceedings{mishchenko2018delay,
  title={A delay-tolerant proximal-gradient algorithm for distributed learning},
  author={Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Amini, Massih-Reza},
  booktitle={International Conference on Machine Learning},
  pages={3587--3595},
  year={2018},
  organization={PMLR}
}

@article{peng2016arock,
  title={Arock: an algorithmic framework for asynchronous parallel coordinate updates},
  author={Peng, Zhimin and Xu, Yangyang and Yan, Ming and Yin, Wotao},
  journal={SIAM Journal on Scientific Computing},
  volume={38},
  number={5},
  pages={A2851--A2879},
  year={2016},
  publisher={SIAM}
}

@inproceedings{leblond2017asaga,
  title={ASAGA: asynchronous parallel SAGA},
  author={Leblond, R{\'e}mi and Pedregosa, Fabian and Lacoste-Julien, Simon},
  booktitle={Artificial Intelligence and Statistics},
  pages={46--54},
  year={2017},
  organization={PMLR}
}

@inproceedings{zhao2016fast,
  title={Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee},
  author={Zhao, Shen-Yi and Li, Wu-Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{assran2020advances,
  title={Advances in Asynchronous Parallel and Distributed Optimization},
  author={Assran, Mahmoud and Aytekin, Arda and Feyzmahdavian, Hamid Reza and Johansson, Mikael and Rabbat, Michael G},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={2013--2031},
  year={2020},
  publisher={IEEE}
}

@inproceedings{basu2019qsparse,
  title={Qsparse-local-{SGD}: Distributed {SGD} with Quantization, Sparsification and Local Computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14668--14679},
  year={2019}
}

@article{yuan2020federated_comp,
  title={Federated Composite Optimization},
  author={Yuan, Honglin and Zaheer, Manzil and Reddi, Sashank},
  journal={arXiv preprint arXiv:2011.08474},
  year={2020}
}

@article{yuan2020federated,
  title={Federated Accelerated Stochastic Gradient Descent},
  author={Yuan, Honglin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{woodworth2020minibatch,
  title={Minibatch vs local sgd for heterogeneous distributed learning},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Srebro, Nathan},
  journal={arXiv preprint arXiv:2006.04735},
  year={2020}
}

@Article{LinSPJ2018local,
  author  = {Tao Lin and Sebastian Urban Stich and Kumar Kshitij Patel and Martin Jaggi},
  title   = {Don't Use Large Mini-Batches, Use Local {SGD}},
  journal = {ICLR},
  year    = {2020},
  pages   = {arXiv:1808.07217},
  url     = {https://arxiv.org/abs/1808.07217},
}

@Article{Stich18local,
  author  = {Sebastian Urban Stich},
  title   = {Local {SGD} Converges Fast and Communicates Little},
  journal = {International Conference on Learning Representations (ICLR)},
  year    = {2019},
  pages   = {arXiv:1805.09767},
  url     = {https://arxiv.org/abs/1805.09767 },
}


@article{kairouz2019advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{kovalev2020linearly,
  title={A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!},
  author={Kovalev, Dmitry and Koloskova, Anastasia and Jaggi, Martin and Richtarik, Peter and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2011.01697},
  year={2020}
}

@article{reisizadeh2019exact,
  title={An exact quantized decentralized gradient descent algorithm},
  author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Pedarsani, Ramtin},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={19},
  pages={4934--4947},
  year={2019},
  publisher={IEEE}
}

@article{qian2020error,
  title={Error Compensated Distributed SGD Can Be Accelerated},
  author={Qian, Xun and Richt{\'a}rik, Peter and Zhang, Tong},
  journal={arXiv preprint arXiv:2010.00091},
  year={2020}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4452--4463},
  year={2018}
}

@article{das2020improved,
  title={Improved Convergence Rates for Non-Convex Federated Learning with Compression},
  author={Das, Rudrajit and Hashemi, Abolfazl and Sanghavi, Sujay and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:2012.04061},
  year={2020}
}

@article{haddadpour2020federated,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:2007.01154},
  year={2020}
}

@article{li2020unified,
  title={A unified analysis of stochastic gradient methods for nonconvex federated optimization},
  author={Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.07013},
  year={2020}
}

@article{philippenko2020artemis,
  title={Artemis: tight convergence guarantees for bidirectional compression in federated learning},
  author={Philippenko, Constantin and Dieuleveut, Aymeric},
  journal={arXiv preprint arXiv:2006.14591},
  year={2020}
}

@article{gorbunov2020linearly,
  title={Linearly Converging Error Compensated SGD},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{horvath2019stochastic,
  title={Stochastic distributed learning with gradient quantization and variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1904.05115},
  year={2019}
}

@article{mishchenko2019distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}

@inproceedings{wen2017terngrad,
  title={TernGrad: ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={1508--1518},
  year={2017}
}

@article{beznosikov2020biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={arXiv preprint arXiv:2002.12410},
  year={2020}
}

@article{horvath2019natural,
  title={Natural compression for distributed deep learning},
  author={Horvath, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richtarik, Peter},
  journal={arXiv preprint arXiv:1905.10988},
  year={2019}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry Z and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={1707--1718},
  year={2017}
}

@inproceedings{suresh2017distributed,
  title={Distributed mean estimation with limited communication},
  author={Suresh, Ananda Theertha and Felix, X Yu and Kumar, Sanjiv and McMahan, H Brendan},
  booktitle={International Conference on Machine Learning},
  pages={3329--3337},
  year={2017},
  organization={PMLR}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@article{li2019communication,
  title={Communication efficient decentralized training with multiple local updates},
  author={Li, Xiang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1910.09126},
  volume={5},
  year={2019}
}

@inproceedings{karimireddy2020scaffold,
  title={SCAFFOLD: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized SGD with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@inproceedings{khaled2020tighter,
  title={Tighter theory for local SGD on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@inproceedings{woodworth2020local,
  title={Is local SGD better than minibatch SGD?},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={10334--10343},
  year={2020},
  organization={PMLR}
}

@article{gorbunov2020local,
  title={Local sgd: Unified theory and new efficient methods},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2011.02828},
  year={2020}
}

@inproceedings{gower2019sgd,
  title={SGD: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5200--5209},
  year={2019},
  organization={PMLR}
}


@inproceedings{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5330--5340},
  year={2017}
}

@inproceedings{scaman2017optimal,
  title={Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={International Conference on Machine Learning},
  pages={3027--3036},
  year={2017}
}

@inproceedings{scaman2018optimal,
  title={Optimal algorithms for non-smooth distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2740--2749},
  year={2018}
}

@inproceedings{assran2019stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  booktitle={International Conference on Machine Learning},
  pages={344--353},
  year={2019},
  organization={PMLR}
}

@article{xiao2004fast,
  title={Fast linear iterations for distributed averaging},
  author={Xiao, Lin and Boyd, Stephen},
  journal={Systems \& Control Letters},
  volume={53},
  number={1},
  pages={65--78},
  year={2004},
  publisher={Elsevier}
}

@article{boyd2006randomized,
  title={Randomized gossip algorithms},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  journal={IEEE transactions on information theory},
  volume={52},
  number={6},
  pages={2508--2530},
  year={2006},
  publisher={IEEE}
}

@article{merris1994laplacian,
  title={Laplacian matrices of graphs: a survey},
  author={Merris, Russell},
  journal={Linear algebra and its applications},
  volume={197},
  pages={143--176},
  year={1994},
  publisher={Elsevier}
}

@article{uribe2020dual,
  title={A dual approach for optimal algorithms in distributed optimization over networks},
  author={Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  journal={Optimization Methods and Software},
  pages={1--40},
  year={2020},
  publisher={Taylor \& Francis}
}

@techreport{tsitsiklis1984problems,
  title={Problems in decentralized decision making and computation.},
  author={Tsitsiklis, John Nikolas},
  year={1984},
  institution={Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems}
}

@article{scaman2019optimal,
  title={Optimal convergence rates for convex distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin and Massouli{\'e}, Laurent},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={1--31},
  year={2019}
}

@article{xu2020distributed,
  title={Distributed Algorithms for Composite Optimization: Unified and Tight Convergence Analysis},
  author={Xu, Jinming and Tian, Ye and Sun, Ying and Scutari, Gesualdo},
  journal={arXiv preprint arXiv:2002.11534},
  year={2020}
}

@article{kovalev2020optimal,
  title={Optimal and practical algorithms for smooth and strongly convex decentralized optimization},
  author={Kovalev, Dmitry and Salim, Adil and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1756--1764},
  year={2015}
}

@article{fallah2019robust,
  title={Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks},
  author={Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asu and Simsekli, Umut and Zhu, Lingjiong},
  journal={arXiv preprint arXiv:1910.08701},
  year={2019}
}

@article{nedic2014distributed,
  title={Distributed optimization over time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={60},
  number={3},
  pages={601--615},
  year={2014},
  publisher={IEEE}
}

@article{nedic2016stochastic,
  title={Stochastic gradient-push for strongly convex functions on time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={12},
  pages={3936--3947},
  year={2016},
  publisher={IEEE}
}

@article{nedic2018network,
  title={Network topology and communication-computation tradeoffs in decentralized optimization},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex and Rabbat, Michael G},
  journal={Proceedings of the IEEE},
  volume={106},
  number={5},
  pages={953--976},
  year={2018},
  publisher={IEEE}
}

@article{rogozin2019projected,
  title={Projected gradient method for decentralized optimization over time-varying networks},
  author={Rogozin, Alexander and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:1911.08527},
  year={2019}
}

@inproceedings{ram2009asynchronous,
  title={Asynchronous gossip algorithms for stochastic optimization},
  author={Ram, S Sundhar and Nedi{\'c}, A and Veeravalli, Venugopal V},
  booktitle={Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference},
  pages={3581--3586},
  year={2009},
  organization={IEEE}
}

@article{yan2012distributed,
  title={Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties},
  author={Yan, Feng and Sundaram, Shreyas and Vishwanathan, SVN and Qi, Yuan},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={25},
  number={11},
  pages={2483--2493},
  year={2012},
  publisher={IEEE}
}

@article{yuan2016convergence,
  title={On the convergence of decentralized gradient descent},
  author={Yuan, Kun and Ling, Qing and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={3},
  pages={1835--1854},
  year={2016},
  publisher={SIAM}
}

@inproceedings{squad,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}

@misc{aldous2002reversible,
  title={Reversible markov chains and random walks on graphs, 2002. Unfinished monograph, recompiled 2014},
  author={Aldous, David and Fill, James Allen},
  year={2002}
}

@misc{minimax,
  title={SA305 – Linear Programming, Lesson 32. Maximin and Minimax Objectives},
  author={David Phillips},
  year={2015}
}

@incollection{andersen,
	doi = {10.1007/978-1-4757-3216-0_8},
	url = {https://doi.org/10.1007%2F978-1-4757-3216-0_8},
	year = 2000,
	publisher = {Springer {US}},
	pages = {197--232},
	author = {Erling D. Andersen and Knud D. Andersen},
	title = {The Mosek Interior Point Optimizer for Linear Programming: An Implementation of the Homogeneous Algorithm},
	booktitle = {Applied Optimization}
}

@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}

@article{Dettmers20158BitAF,
  title={8-Bit Approximations for Parallelism in Deep Learning},
  author={Tim Dettmers},
  journal={ICLR},
  year={2015}
}

@inproceedings{Trask2015ModelingOI,
  title={Modeling Order in Neural Word Embeddings at Scale},
  author={Andrew Trask and David Gilmore and Matthew Russell},
  booktitle={ICML},
  year={2015}
}

@inproceedings{gross_folding,
    author={Michael Gross},
    year={2012},
    title={Folding research recruits unconventional help},
    booktitle={Current Biology. 22 (2): R35–R38},
    doi={10.1016/j.cub.2012.01.008},
    pmid={PMID 22389910}
}


@inproceedings{jft-300m,
title	= {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
author	= {Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta},
year	= {2017},
URL	= {https://arxiv.org/abs/1707.02968},
booktitle	= {ICCV}
}


@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-\\with-deep-convolutional-neural-networks.pdf}
}



@article{jft300data,
  title={Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
  author={Baoyuan Wu and Weidong Chen and Yanbo Fan and Yong Zhang and Jinlong Hou and Jie Liu and Tong Zhang},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={172683-172693}
}

@inproceedings{Kolesnikov2020BigT,
  title={Big Transfer (BiT): General Visual Representation Learning},
  author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and S. Gelly and N. Houlsby},
  booktitle={ECCV},
  year={2020}
}

@article{kolesnikovlarge,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  archivePrefix = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1912-11370},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}


@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{ringallreduce,
author = {Patarasuk, Pitch and Yuan, Xin},
year = {2009},
month = {02},
pages = {117-124},
title = {Bandwidth optimal all-reduce algorithms for clusters of workstations},
volume = {69},
journal = {Journal of Parallel and Distributed Computing},
doi = {10.1016/j.jpdc.2008.09.002}
}


@article{bandwidth_optimal_allreduce,
author = {Patarasuk, Pitch and Yuan, Xin},
title = {Bandwidth Optimal All-Reduce Algorithms for Clusters of Workstations},
year = {2009},
issue_date = {February, 2009},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {69},
number = {2},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2008.09.002},
doi = {10.1016/j.jpdc.2008.09.002},
abstract = {We consider an efficient realization of the all-reduce operation with large data sizes in cluster environments, under the assumption that the reduce operator is associative and commutative. We derive a tight lower bound of the amount of data that must be communicated in order to complete this operation and propose a ring-based algorithm that only requires tree connectivity to achieve bandwidth optimality. Unlike the widely used butterfly-like all-reduce algorithm that incurs network contention in SMP/multi-core clusters, the proposed algorithm can achieve contention-free communication in almost all contemporary clusters, including SMP/multi-core clusters and Ethernet switched clusters with multiple switches. We demonstrate that the proposed algorithm is more efficient than other algorithms on clusters with different nodal architectures and networking technologies when the data size is sufficiently large.},
journal = {J. Parallel Distrib. Comput.},
month = feb,
pages = {117–124},
numpages = {8},
keywords = {Tree topology, Collective communication, All-reduce, Cluster of workstations}
}

@article{torus_allreduce,
author = {Sack, Paul and Gropp, William},
title = {Collective Algorithms for Multiported Torus Networks},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/2686882},
doi = {10.1145/2686882},
abstract = {Modern supercomputers with torus networks allow each node to simultaneously pass messages on all of its links. However, most collective algorithms are designed to only use one link at a time. In this work, we present novel multiported algorithms for the scatter, gather, all-gather, and reduce-scatter operations. Our algorithms can be combined to create multiported reduce, all-reduce, and broadcast algorithms. Several of these algorithms involve a new technique where we relax the MPI message-ordering constraints to achieve high performance and restore the correctordering using an additional stage of redundant communication.According to our models, on an n-dimensional torus, our algorithms should allow for nearly a 2n-fold improvement in communication performance compared to known, single-ported torus algorithms. In practice, we have achieved nearly 6x better performance on a 32k-node 3-dimensional torus.},
journal = {ACM Trans. Parallel Comput.},
month = feb,
articleno = {12},
numpages = {33},
keywords = {collective algorithms, Message-passing}
}

@inproceedings{adam2015atlas,
  title={ATLAS@ Home: harnessing volunteer computing for HEP},
  author={Adam-Bourdarios, C and Cameron, D and Filip{\v{c}}i{\v{c}}, A and Lancon, E and Wu, Wenjing and others},
  booktitle={Journal of Physics: Conference Series},
  volume={664},
  number={2},
  pages={022009},
  year={2015},
  organization={IOP Publishing}
}

@misc{speedtest,
  title = {Speedtest Global Index for Fixed Broadband},
  note = {\url{https://www.speedtest.net/global-index} (accessed on 11.08.2020, bandwidth for top countries and general trend)},
  
}

@article{li2017case,
  title={A Case Study of IPv6 Network Performance: Packet Delay, Loss, and Reordering},
  author={Li, Fuliang and Wang, Xingwei and Pan, Tian and Yang, Jiahai},
  journal={Mathematical Problems in Engineering},
  volume={2017},
  year={2017},
  publisher={Hindawi}
}

@article{Sun2019OptimizingNP,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Peng Sun and Wansen Feng and Ruobing Han and Shengen Yan and Yonggang Wen},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06855}
}

@inproceedings{survey_distributed2,
author = {Alqahtani, Salem and Demirbas, Murat},
year = {2019},
month = {07},
pages = {},
title = {Performance Analysis and Comparison of Distributed Machine Learning Systems}
}

@inproceedings{sharded_ps_first,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {1223--1231},
 publisher = {Curran Associates, Inc.},
 title = {Large Scale Distributed Deep Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
 volume = {25},
 year = {2012}
}


@InProceedings{pmlr-v97-koloskova19a, title = {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication}, author = {Koloskova, Anastasia and Stich, Sebastian and Jaggi, Martin}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {3478--3487}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/koloskova19a/koloskova19a.pdf}, url = { http://proceedings.mlr.press/v97/koloskova19a.html }, abstract = {We consider decentralized stochastic optimization with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a fixed communication graph. To address the communication bottleneck, the nodes compress (e.g. quantize or sparsify) their model updates. We cover both unbiased and biased compression operators with quality denoted by \delta <= 1 (\delta=1 meaning no compression). We (i) propose a novel gossip-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1/(nT) + 1/(T \rho^2 \delta)^2) for strongly convex objectives, where T denotes the number of iterations and \rho the eigengap of the connectivity matrix. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the average consensus problem that converges in time O(1/(\rho^2\delta) \log (1/\epsilon)) for accuracy \epsilon > 0. This is (up to our knowledge) the first gossip algorithm that supports arbitrary compressed messages for \delta > 0 and still exhibits linear convergence. We (iii) show in experiments that both of our algorithms do outperform the respective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes.} }

@inproceedings{
lin2018deep,
title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
author={Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SkhQHMW0W},
}

@inproceedings{localsgd_first,
 author = {Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {2595--2603},
 publisher = {Curran Associates, Inc.},
 title = {Parallelized Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
 volume = {23},
 year = {2010}
}

@article{survey_distributed,
author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
title = {A Survey on Distributed Machine Learning},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3377454},
doi = {10.1145/3377454},
abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {30},
numpages = {33},
keywords = {Distributed machine learning, distributed systems}
}


@article{zhang2015staleness,
  title={Staleness-aware async-sgd for distributed deep learning},
  author={Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
  journal={arXiv preprint arXiv:1511.05950},
  year={2015}
}


@INPROCEEDINGS{vc_evolve,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}


@article{moe_first,
author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
title = {Adaptive Mixtures of Local Experts},
year = {1991},
issue_date = {March 1991},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {3},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1991.3.1.79},
doi = {10.1162/neco.1991.3.1.79},
journal = {Neural Computation},
month = mar,
pages = {79–87},
numpages = {9}
}

@inproceedings{proteus,
author = {Harlap, Aaron and Tumanov, Alexey and Chung, Andrew and Ganger, Gregory R. and Gibbons, Phillip B.},
title = {Proteus: Agile ML Elasticity through Tiered Reliability in Dynamic Resource Markets},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064182},
doi = {10.1145/3064176.3064182},
abstract = {Many shared computing clusters allow users to utilize excess idle resources at lower cost or priority, with the proviso that some or all may be taken away at any time. But, exploiting such dynamic resource availability and the often fluctuating markets for them requires agile elasticity and effective acquisition strategies. Proteus aggressively exploits such transient revocable resources to do machine learning (ML) cheaper and/or faster. Its parameter server framework, AgileML, efficiently adapts to bulk additions and revocations of transient machines, through a novel 3-stage active-backup approach, with minimal use of more costly non-transient resources. Its BidBrain component adaptively allocates resources from multiple EC2 spot markets to minimize average cost per work as transient resource availability and cost change over time. Our evaluations show that Proteus reduces cost by 85% relative to non-transient pricing, and by 43% relative to previous approaches, while simultaneously reducing runtimes by up to 37%.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {589–604},
numpages = {16},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@misc{lin2020multinode,
      title={Multi-node Bert-pretraining: Cost-efficient Approach}, 
      author={Jiahuang Lin and Xin Li and Gennady Pekhimenko},
      year={2020},
      eprint={2008.00177},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fedus2021switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2021},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{yao2009hierarchical,
  title={Hierarchical mixture of classification experts uncovers interactions between brain regions},
  author={Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-Fei, Li},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2178--2186},
  year={2009}
}

@inproceedings{rasmussen2002infinite,
  title={Infinite mixtures of Gaussian process experts},
  author={Rasmussen, Carl E and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={881--888},
  year={2002}
}


@inproceedings{moe_lifelong,
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
year = {2017},
month = {07},
pages = {7120-7129},
title = {Expert Gate: Lifelong Learning with a Network of Experts},
doi = {10.1109/CVPR.2017.753}
}

@inproceedings{moe_svm,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={633--640},
  year={2002}
}

@article{moe_dirichlet,
  title={Nonlinear models using Dirichlet process mixtures},
  author={Shahbaba, Babak and Neal, Radford},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Aug},
  pages={1829--1850},
  year={2009}
}


@article{Lepikhin2020GShardSG,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Dmitry Lepikhin and H. Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Y. Huang and M. Krikun and Noam Shazeer and Z. Chen},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.16668}
}

@incollection{pkm,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8546--8557},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}

@inproceedings{can,
  title={A scalable content-addressable network},
  author={Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
  booktitle={Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
  pages={161--172},
  year={2001}
}





@techreport{tewari1998beyond,
  title={Beyond hierarchies: Design considerations for distributed caching on the internet},
  author={Tewari, Renu and Dahlin, Michael and Vin, Harrick and Kay, John},
  institution={Citeseer}
}


@inproceedings{mcmahan2017communication,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
}

@InProceedings{sgpush, title = {Stochastic Gradient Push for Distributed Deep Learning}, author = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {344--353}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/assran19a/assran19a.pdf}, url = { http://proceedings.mlr.press/v97/assran19a.html }, abstract = {Distributed data-parallel algorithms aim to accelerate the training of deep neural networks by parallelizing the computation of large mini-batch gradient updates across multiple nodes. Approaches that synchronize nodes using exact distributed averaging (e.g., via AllReduce) are sensitive to stragglers and communication delays. The PushSum gossip algorithm is robust to these issues, but only performs approximate distributed averaging. This paper studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient updates. We prove that SGP converges to a stationary point of smooth, non-convex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We empirically validate the performance of SGP on image classification (ResNet-50, ImageNet) and machine translation (Transformer, WMT’16 En-De) workloads.} }

@InProceedings{zeno, title = {Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance}, author = {Xie, Cong and Koyejo, Sanmi and Gupta, Indranil}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6893--6901}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/xie19b/xie19b.pdf}, url = { http://proceedings.mlr.press/v97/xie19b.html }, abstract = {We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes previous results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to suspect workers that are potentially defective. Since this is likely to lead to false positives, we use a ranking-based preference mechanism. We prove the convergence of SGD for non-convex problems under these scenarios. Experimental results show that Zeno outperforms existing approaches.} }

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{stale_gradients_can_win,
author = {Dutta, Sanghamitra and Joshi, Gauri and Ghosh, Soumyadip and Dube, Parijat and Nagpurkar, Priya},
year = {2018},
month = {03},
pages = {},
title = {Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD}
}

@article{gradient_checkpointing_dl,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{gradient_checkpointing_autograd,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{puigcerver2020scalable,
  title={Scalable transfer learning with expert models},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Renggli, Cedric and Pinto, Andr{\'e} Susano and Gelly, Sylvain and Keysers, Daniel and Houlsby, Neil},
  journal={arXiv preprint arXiv:2009.13239},
  year={2020}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9051--9062},
  year={2019}
}

@incollection{NIPS2019_8736,
title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4901--4910},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@ARTICLE{seq2seqvis,
author={H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
year={2019},
volume={25},
number={1},
pages={353-363},
keywords={data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP},
doi={10.1109/TVCG.2018.2865044},
ISSN={2160-9306},
month={Jan},}

@article{carter2019activation,
  title={Activation atlas},
  author={Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  journal={Distill},
  volume={4},
  number={3},
  pages={e15},
  year={2019}
}


@article{pipemare,
  title={PipeMare: Asynchronous Pipeline Parallel DNN Training},
  author={Bowen Yang and Jian Zhang and Jonathan Li and Christopher R{\'e} and Christopher R. Aberger and Christopher De Sa},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.05124}
}

@article{valiant1990bridging,
  title={A bridging model for parallel computation},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={33},
  number={8},
  pages={103--111},
  year={1990},
  publisher={ACM New York, NY, USA}
}


@article{natural_compression,
  author    = {Samuel Horvath and
               Chen{-}Yu Ho and
               Ludovit Horvath and
               Atal Narayan Sahu and
               Marco Canini and
               Peter Richt{\'{a}}rik},
  title     = {Natural Compression for Distributed Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1905.10988},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10988},
  archivePrefix = {arXiv},
  eprint    = {1905.10988},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sukhov2016generating,
  title={Generating a function for network delay},
  author={Sukhov, Andrei M and Astrakhantseva, MA and Pervitsky, AK and Boldyrev, SS and Bukatov, AA},
  journal={Journal of High Speed Networks},
  volume={22},
  number={4},
  pages={321--333},
  year={2016},
  publisher={IOS Press}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1627--1635},
  year={2017},
  organization={JMLR. org}
}

@misc{ma2019hsic,
    title={The HSIC Bottleneck: Deep Learning without Back-Propagation},
    author={Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
    year={2019},
    eprint={1908.01580},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2902--2911},
  year={2017},
  organization={JMLR. org}
}

@misc{hendrycks2016gaussian,
    title={Gaussian Error Linear Units (GELUs)},
    author={Dan Hendrycks and Kevin Gimpel},
    year={2016},
    eprint={1606.08415},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


@inproceedings{secure_aggregation,
title	= {Practical Secure Aggregation  for Privacy-Preserving Machine Learning},
author	= {Aaron Segal and Antonio Marcedone and Benjamin Kreuter and Daniel Ramage and H. Brendan McMahan and Karn Seth and K. A. Bonawitz and Sarvar Patel and Vladimir Ivanov},
year	= {2017},
URL	= {https://eprint.iacr.org/2017/281.pdf},
booktitle	= {CCS}
}

@misc{fed_google1,
title	= {Federated Learning for Mobile Keyboard Prediction},
author	= {Andrew Hard and Chloé M Kiddon and Daniel Ramage and Francoise Beaufays and Hubert Eichner and Kanishka Rao and Rajiv Mathews and Sean Augenstein},
year	= {2018},
URL	= {https://arxiv.org/abs/1811.03604}
}

@misc{fed_google2,
title	= {Applied Federated Learning: Improving Google Keyboard Query Suggestions},
author	= {Timothy Yang and Galen Andrew and Hubert Eichner and Haicheng Sun and Wei Li and Nicholas Kong and Daniel Ramage and Françoise Beaufays},
year	= {2018},
URL	= {https://arxiv.org/abs/1812.02903}
}





﻿@Article{fed_intel,
author={Sheller, Micah J.
and Edwards, Brandon
and Reina, G. Anthony
and Martin, Jason
and Pati, Sarthak
and Kotrotsou, Aikaterini
and Milchenko, Mikhail
and Xu, Weilin
and Marcus, Daniel
and Colen, Rivka R.
and Bakas, Spyridon},
title={Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data},
journal={Scientific Reports},
year={2020},
month={Jul},
day={28},
volume={10},
number={1},
pages={12598},
abstract={Several studies underscore the potential of deep learning in identifying complex patterns, leading to diagnostic and prognostic biomarkers. Identifying sufficiently large and diverse datasets, required for training, is a significant challenge in medicine and can rarely be found in individual institutions. Multi-institutional collaborations based on centrally-shared patient data face privacy and ownership challenges. Federated learning is a novel paradigm for data-private multi-institutional collaborations, where model-learning leverages all available data without sharing data between institutions, by distributing the model-training to the data-owners and aggregating their results. We show that federated learning among 10 institutions results in models reaching 99{\%} of the model quality achieved with centralized data, and evaluate generalizability on data from institutions outside the federation. We further investigate the effects of data distribution across collaborating institutions on model quality and learning patterns, indicating that increased access to data through data private multi-institutional collaborations can benefit model quality more than the errors introduced by the collaborative method. Finally, we compare with other collaborative-learning approaches demonstrating the superiority of federated learning, and discuss practical implementation considerations. Clinical adoption of federated learning is expected to lead to models trained on datasets of unprecedented size, hence have a catalytic impact towards precision/personalized medicine.},
issn={2045-2322},
doi={10.1038/s41598-020-69250-1},
url={https://doi.org/10.1038/s41598-020-69250-1}
}

@article{Popel2018TrainingTF,
  title={Training Tips for the Transformer Model},
  author={M. Popel and Ondrej Bojar},
  journal={The Prague Bulletin of Mathematical Linguistics},
  year={2018},
  volume={110},
  pages={43 - 70}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inbook{fed_nvidia,
title = "Privacy-Preserving Federated Brain Tumour Segmentation",
abstract = "Due to medical data privacy regulations, it is often infeasible to collect and share patient data in a centralised data lake. This poses challenges for training machine learning algorithms, such as deep convolutional networks, which often require large numbers of diverse training examples. Federated learning sidesteps this difficulty by bringing code to the patient data owners and only sharing intermediate model training updates among them. Although a high-accuracy model could be achieved by appropriately aggregating these model updates, the model shared could indirectly leak the local training examples. In this paper, we investigate the feasibility of applying differential-privacy techniques to protect the patient data in a federated learning setup. We implement and evaluate practical federated learning systems for brain tumour segmentation on the BraTS dataset. The experimental results show that there is a trade-off between model performance and privacy protection costs.",
author = "Wenqi Li and Fausto Milletar{\`i} and Daguang Xu and Nicola Rieke and Jonny Hancox and Wentao Zhu and Maximilian Baust and Yan Cheng and S{\'e}bastien Ourselin and Cardoso, {M. Jorge} and Andrew Feng",
year = "2019",
month = jan,
day = "1",
doi = "10.1007/978-3-030-32692-0_16",
language = "English",
isbn = "9783030326913",
series = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
publisher = "SPRINGER",
pages = "133--141",
editor = "Heung-Il Suk and Mingxia Liu and Chunfeng Lian and Pingkun Yan",
booktitle = "Machine Learning in Medical Imaging - 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Proceedings",
note = "10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019 held in conjunction with the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019 ; Conference date: 13-10-2019 Through 13-10-2019",
}


@inproceedings{federatedlearningatscale,
title	= {Towards Federated Learning at Scale: System Design},
author	= {K. A. Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloé M Kiddon and Jakub Konečný and Stefano Mazzocchi and Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
year	= {2019},
URL	= {https://arxiv.org/abs/1902.01046},
note	= {To appear},
booktitle	= {SysML 2019}
}



@misc{tnlg,
  author = {Corby Rosset},
  title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
  howpublished = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
  note = {Accessed: 2020-2-10}
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@article{wikitext103,
  title={Pointer Sentinel Mixture Models},
  author={Stephen Merity and Caiming Xiong and James Bradbury and R. Socher},
  journal={ArXiv},
  year={2017},
  volume={abs/1609.07843}
}

@article{datasets,
  title={Datasets},
  author={Thomas Wolf and Quentin Lhoest and Patrick von Platen and Yacine Jernite and Mariama Drame and Julien Plu and Julien Chaumond and Clement Delangue and Clara Ma and Abhishek Thakur and Suraj Patil and Joe Davison and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angie McMillan-Major and Simon Brandeis and Sylvain Gugger and François Lagunas and Lysandre Debut and Morgan Funtowicz and Anthony Moi and Sasha Rush and Philipp Schmidd and Pierric Cistac and Victor Muštar and Jeff Boudier and Anna Tordjmann},
  journal={GitHub. Note: https://github.com/huggingface/datasets},
  volume={1},
  year={2020}
}

@article{wikitext2,
title= {Wikitext-2},
keywords= {fastai},
journal= {},
author= {Stephen Merity et al., 2016},
year= {},
url= {https://arxiv.org/abs/1609.07843},
license= {},
abstract= {A subset of Wikitext-103; useful for testing language model training on smaller datasets.},
superseded= {},
terms= {}
}


@misc{gpt3costlambda,
  author = {Chuan Li},
  title = {Demystifying GPT-3 Language Model: A Technical Overview},
  note = {"\url{https://lambdalabs.com/blog/demystifying-gpt-3}"},
  year = {2020},
}

@misc{nvidia_perf,
   author = {NVIDIA},
   title = {NVIDIA Data Center Deep Learning Product Performance},
   note = {"\url{https://developer.nvidia.com/deep-learning-performance-training-inference}", accessed at 2021.02.03}
}

@misc{dettmerswikitext2,
  author = {Tim Dettmers},
  note = {https://github.com/TimDettmers/transformer-xl/tree/wikitext2}
}

@article{mlperf-arxiv,
  author    = {Peter Mattson and
               Christine Cheng and
               Cody Coleman and
               Greg Diamos and
               Paulius Micikevicius and
               David A. Patterson and
               Hanlin Tang and
               Gu{-}Yeon Wei and
               Peter Bailis and
               Victor Bittorf and
               David Brooks and
               Dehao Chen and
               Debojyoti Dutta and
               Udit Gupta and
               Kim M. Hazelwood and
               Andrew Hock and
               Xinyuan Huang and
               Bill Jia and
               Daniel Kang and
               David Kanter and
               Naveen Kumar and
               Jeffery Liao and
               Guokai Ma and
               Deepak Narayanan and
               Tayo Oguntebi and
               Gennady Pekhimenko and
               Lillian Pentecost and
               Vijay Janapa Reddi and
               Taylor Robie and
               Tom St. John and
               Carole{-}Jean Wu and
               Lingjie Xu and
               Cliff Young and
               Matei Zaharia},
  title     = {MLPerf Training Benchmark},
  journal   = {CoRR},
  volume    = {abs/1910.01500},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01500},
  archivePrefix = {arXiv},
  eprint    = {1910.01500},
  timestamp = {Mon, 04 Nov 2019 08:16:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01500.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{variability_azure,
  author={V. {Persico} and P. {Marchetta} and A. {Botta} and A. {Pescape}},
  booktitle={2015 IEEE Global Communications Conference (GLOBECOM)}, 
  title={On Network Throughput Variability in Microsoft Azure Cloud}, 
  year={2015},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/GLOCOM.2015.7416997}}

@article{variability_aws,
title = "Measuring network throughput in the cloud: The case of Amazon EC2",
journal = "Computer Networks",
volume = "93",
pages = "408 - 422",
year = "2015",
note = "Cloud Networking and Communications II",
issn = "1389-1286",
doi = "https://doi.org/10.1016/j.comnet.2015.09.037",
url = "http://www.sciencedirect.com/science/article/pii/S138912861500362X",
author = "Valerio Persico and Pietro Marchetta and Alessio Botta and Antonio Pescapè",
keywords = "Cloud networking monitoring and measurement, Cloud networking performance, Cloud network throughput",
abstract = "Cloud providers employ sophisticated virtualization techniques and strategies for sharing resources among a large number of largely uncoordinated and mutually untrusted customers. The shared networking environment, in particular, dictates the need for mechanisms to partition network resources among virtual machines. At the same time, the performance of applications deployed over these virtual machines may be heavily impacted by the performance of the underlying network, and therefore by such mechanisms. Nevertheless, due to security and commercial reasons, providers rarely provide detailed information on network organization, performance, and mechanisms employed to regulate it. In addition, the scientific literature only provides a blurred image of the network performance inside the cloud. The few available pioneer works marginally focus on this aspect, use different methodologies, operate in few limited scenarios, or report conflicting results. In this paper, we present a detailed analysis of the performance of the internal network of Amazon EC2, performed by adopting a non-cooperative experimental evaluation approach (i.e. not relying on provider support). Our aim is to provide a quantitative assessment of the networking performance as a function of the several variables available, such as geographic region, resource price or size. We propose a detailed methodology to perform this kind of analysis, which we believe is essential in a such complex and dynamic environment. During this analysis we have discovered and analyzed the limitations enforced by Amazon over customer traffic in terms of maximum throughput allowed. Thanks to our work it is possible to understand how the complex mechanisms enforced by the provider in order to manage its infrastructure impact the performance perceived by the cloud customers and potentially tamper with monitoring and controlling approaches previously proposed in literature. Leveraging our knowledge of the bandwidth-limiting mechanisms, we then present a clear picture of the maximum throughput achievable in Amazon EC2 network, shedding light on when and how such maximum throughput can be achieved and at which cost."
}


@misc{gpt3cost,
  author = {Elliot Turner},
  note = {Estimate of GPT-3 training cost based on public cloud GPU/TPU cost models, from Elliot Turner's personal page (accessed on May 29, 2020)}
}

@misc{lambdabenchmarks,
  author = {Stephen Balaban , Chuan Li},
  title = {Deep Learning GPU benchmarks, Lambda Labs website, 2018/10/08}
}

@INPROCEEDINGS{desell2017,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}
  
  
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{Hendrycks2019NaturalAE,
  title={Natural Adversarial Examples},
  author={Dan Hendrycks and Kevin Keliang Zhao and Steven Basart and Jacob Steinhardt and Dawn Xiaodong Song},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.07174}
}

@inproceedings{Ott2019fairseqAF,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle={NAACL-HLT},
  year={2019}
}

@article{hern2018facebook,
  title={Facebook translates ``good morning'' into ``attack them'', leading to arrest},
  author={Alex Hern},
  journal={The Guardian},
  year={2018}
}

@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}

@article{Papernot2018DeepKN,
  title={Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning},
  author={Nicolas Papernot and Patrick D. McDaniel},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.04765}
}

@article{Garnelo2018ConditionalNP,
  title={Conditional Neural Processes},
  author={Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo Jimenez Rezende and S. M. Ali Eslami},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.01613}
}

@inproceedings{Wallace2019UniversalAT,
  title={Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author={Eric Wallace and Feng Shi and Nikhil Kandpal and Matt Gardner and Sameer Singh},
  year={2019}
}

@article{Szegedy2013IntriguingPO,
  title={Intriguing properties of neural networks},
  author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6199}
}

@article{Yuan2017AdversarialEA,
  title={Adversarial Examples: Attacks and Defenses for Deep Learning},
  author={Xiaoyong Yuan and Pan He and Qile Zhu and Xiaolin Li},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2017},
  volume={30},
  pages={2805-2824}
}
@inproceedings{Ebrahimi2017HotFlipWA,
  title={HotFlip: White-Box Adversarial Examples for Text Classification},
  author={Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
  booktitle={ACL},
  year={2017}
}

@article{schmidthuber,
  title={First superhuman visual pattern recognition},
  author={D. C. Ciresan, U. Meier, J. Schmidhuber},
  year={2011},
  journal={IJCNN}
}

@article{microsoft_mt_parity,
  title={Achieving Human Parity on Automatic Chinese to English News Translation},
  author={Hany Hassan and Anthony Aue and Chang Chen and Vishal Chowdhary and Jonathan R. Clark and Christian Federmann and Xuedong Huang and Marcin Junczys-Dowmunt and William Lewis and Mu Li and Shujie Liu and T. M. Liu and Renqian Luo and Arul Menezes and Tao Qin and Frank Seide and Xu Tan and Fei Tian and Lijun Wu and Shuangzhi Wu and Yingce Xia and Dongdong Zhang and Zhirui Zhang and Ming Zhou},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.05567}
}

@article{Silver2016MasteringTG,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={529},
  pages={484-489}
}

@inproceedings{Singh2013OpticalCR,
  title={Optical Character Recognition Techniques: A survey},
  author={Sukhpreet Singh},
  year={2013}
}

@inproceedings{maml,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle={ICML},
  year={2017}
}

@article{reptile,
  title={On First-Order Meta-Learning Algorithms},
  author={Alex Nichol and Joshua Achiam and John Schulman},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.02999}
}

@article{learning2learn,
  title={Learning to learn by gradient descent by gradient descent},
  author={Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.04474}
}

@article{elasticweightcolsolidation,
  title={Overcoming catastrophic forgetting in neural networks},
  author={James Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2016},
  volume={114 13},
  pages={
          3521-3526
        }
}

@article{rehearsal,
  title={Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
  author={Anthony V. Robins},
  journal={Connect. Sci.},
  year={1995},
  volume={7},
  pages={123-146}
}

@article{Houthooft2018EvolvedPG,
  title={Evolved Policy Gradients},
  author={Rein Houthooft and Yuhua Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.04821}
}

@article{withoutgd,
  title={Learning to learn by gradient descent by gradient descent},
  author={Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.04474}
}

@article{Lin2019ConditionalCF,
  title={Conditional Computation for Continual Learning},
  author={Min Lin and Jie Fu and Yoshua Bengio},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.06635}
}

@article{clmetalearning1,
  title={Task Agnostic Continual Learning via Meta Learning},
  author={Xu He and Jakub Sygnowski and Alexandre Galashov and Andrei A. Rusu and Yee Whye Teh and Razvan Pascanu},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.05201}
}

@article{clmetalearning2,
  title={Meta-Learning Representations for Continual Learning},
  author={Khurram Javed and Martha White},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.12588}
}

@article{densenet,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={2261-2269}
}

@article{adagrad,
  title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author={John C. Duchi and Elad Hazan and Yoram Singer},
  journal={J. Mach. Learn. Res.},
  year={2010},
  volume={12},
  pages={2121-2159}
}


@article{adadelta,
  title={ADADELTA: An Adaptive Learning Rate Method},
  author={Matthew D. Zeiler},
  journal={ArXiv},
  year={2012},
  volume={abs/1212.5701}
}

@article{superconvergence,
  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},
  author={Leslie N. Smith and Nicholay Topin},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.07120}
}

@inproceedings{adversarial_training,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}

@inproceedings{signsgd,
  added-at = {2019-04-03T00:00:00.000+0200},
  author = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  biburl = {https://www.bibsonomy.org/bibtex/2409dcbd9da24821e21edb290debe9944/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2018},
  editor = {Dy, Jennifer G. and Krause, Andreas},
  ee = {http://proceedings.mlr.press/v80/bernstein18a.html},
  interhash = {a579bb8583ea2fdba022637317600f88},
  intrahash = {409dcbd9da24821e21edb290debe9944},
  keywords = {dblp},
  pages = {559-568},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  timestamp = {2019-04-04T11:43:21.000+0200},
  title = {SIGNSGD: Compressed Optimisation for Non-Convex Problems.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2018.html#BernsteinWAA18},
  volume = 80,
  year = 2018
}



@misc{rmsprop,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@misc{cifar,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}


@inproceedings{GAN,
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Nets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2672–2680},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}


@article{trainingtips,
author = {Popel, Martin and Bojar, Ondřej},
year = {2018},
month = {03},
pages = {},
title = {Training Tips for the Transformer Model},
volume = {110},
journal = {The Prague Bulletin of Mathematical Linguistics},
doi = {10.2478/pralin-2018-0002}
}


@inproceedings{Li2017VisualizingTL,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{tsne,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  year={2008}
}

@article{elu,
  title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  author={Djork-Arn{\'e} Clevert and Thomas Unterthiner and Sepp Hochreiter},
  journal={CoRR},
  year={2015},
  volume={abs/1511.07289}
}

@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{iwslt14,
  title={Report on the 11 th IWSLT Evaluation Campaign , IWSLT 2014},
  author={Mauro Cettolo and Jan Niehues and Sebastian St{\"u}ker and Luisa Bentivogli and Marcello Federico},
  year={2015}
}

@inproceedings{koehn-etal-2007-moses,
    title = "{M}oses: Open Source Toolkit for Statistical Machine Translation",
    author = "Koehn, Philipp  and
      Hoang, Hieu  and
      Birch, Alexandra  and
      Callison-Burch, Chris  and
      Federico, Marcello  and
      Bertoldi, Nicola  and
      Cowan, Brooke  and
      Shen, Wade  and
      Moran, Christine  and
      Zens, Richard  and
      Dyer, Chris  and
      Bojar, Ond{\v{r}}ej  and
      Constantin, Alexandra  and
      Herbst, Evan",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-2045",
    pages = "177--180",
}

@incollection{transformer,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@article{ratcliff1990connectionist,
  title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={97},
  number={2},
  pages={285},
  year={1990},
  publisher={American Psychological Association}
}

@inproceedings{Furlanello2018BornAgainNN,
  title={Born-Again Neural Networks},
  author={Tommaso Furlanello and Zachary Chase Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar},
  booktitle={ICML},
  year={2018}
}

@article{inception_v3,
  title={Rethinking the Inception Architecture for Computer Vision},
  author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jon Shlens and Zbigniew Wojna},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={2818-2826}
}


@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}

@article{Dettmers20158BitAF,
  title={8-Bit Approximations for Parallelism in Deep Learning},
  author={Tim Dettmers},
  journal={ICLR},
  year={2015}
}

@inproceedings{gross_folding,
    author={Michael Gross},
    year={2012},
    title={Folding research recruits unconventional help},
    booktitle={Current Biology. 22 (2): R35–R38},
    doi={10.1016/j.cub.2012.01.008},
    pmid={PMID 22389910}
}


@article{jft300data,
  title={Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
  author={Baoyuan Wu and Weidong Chen and Yanbo Fan and Yong Zhang and Jinlong Hou and Jie Liu and Tong Zhang},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={172683-172693}
}

@article{kolesnikovlarge,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  archivePrefix = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1912-11370},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{adam2015atlas,
  title={ATLAS@ Home: harnessing volunteer computing for HEP},
  author={Adam-Bourdarios, C and Cameron, D and Filip{\v{c}}i{\v{c}}, A and Lancon, E and Wu, Wenjing and others},
  booktitle={Journal of Physics: Conference Series},
  volume={664},
  number={2},
  pages={022009},
  year={2015},
  organization={IOP Publishing}
}

@article{li2017case,
  title={A Case Study of IPv6 Network Performance: Packet Delay, Loss, and Reordering},
  author={Li, Fuliang and Wang, Xingwei and Pan, Tian and Yang, Jiahai},
  journal={Mathematical Problems in Engineering},
  volume={2017},
  year={2017},
  publisher={Hindawi}
}

@article{Sun2019OptimizingNP,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Peng Sun and Wansen Feng and Ruobing Han and Shengen Yan and Yonggang Wen},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06855}
}

@article{zhang2015staleness,
  title={Staleness-aware async-sgd for distributed deep learning},
  author={Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
  journal={arXiv preprint arXiv:1511.05950},
  year={2015}
}



@article{moe_first,
author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
title = {Adaptive Mixtures of Local Experts},
year = {1991},
issue_date = {March 1991},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {3},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1991.3.1.79},
doi = {10.1162/neco.1991.3.1.79},
journal = {Neural Computation},
month = mar,
pages = {79–87},
numpages = {9}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{yao2009hierarchical,
  title={Hierarchical mixture of classification experts uncovers interactions between brain regions},
  author={Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-Fei, Li},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2178--2186},
  year={2009}
}

@inproceedings{rasmussen2002infinite,
  title={Infinite mixtures of Gaussian process experts},
  author={Rasmussen, Carl E and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={881--888},
  year={2002}
}


@inproceedings{moe_lifelong,
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
year = {2017},
month = {07},
pages = {7120-7129},
title = {Expert Gate: Lifelong Learning with a Network of Experts},
doi = {10.1109/CVPR.2017.753}
}

@inproceedings{moe_svm,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={633--640},
  year={2002}
}

@article{moe_dirichlet,
  title={Nonlinear models using Dirichlet process mixtures},
  author={Shahbaba, Babak and Neal, Radford},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Aug},
  pages={1829--1850},
  year={2009}
}

@incollection{pkm,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8546--8557},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}

@inproceedings{can,
  title={A scalable content-addressable network},
  author={Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
  booktitle={Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
  pages={161--172},
  year={2001}
}



@techreport{tewari1998beyond,
  title={Beyond hierarchies: Design considerations for distributed caching on the internet},
  author={Tewari, Renu and Dahlin, Michael and Vin, Harrick and Kay, John},
  institution={Citeseer}
}



@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{stale_gradients_can_win,
author = {Dutta, Sanghamitra and Joshi, Gauri and Ghosh, Soumyadip and Dube, Parijat and Nagpurkar, Priya},
year = {2018},
month = {03},
pages = {},
title = {Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD}
}

@article{gradient_checkpointing_dl,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{gradient_checkpointing_autograd,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9051--9062},
  year={2019}
}

@incollection{NIPS2019_8736,
title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4901--4910},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@ARTICLE{seq2seqvis,
author={H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
year={2019},
volume={25},
number={1},
pages={353-363},
keywords={data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP},
doi={10.1109/TVCG.2018.2865044},
ISSN={2160-9306},
month={Jan},}

@article{carter2019activation,
  title={Activation atlas},
  author={Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  journal={Distill},
  volume={4},
  number={3},
  pages={e15},
  year={2019}
}


@article{pipemare,
  title={PipeMare: Asynchronous Pipeline Parallel DNN Training},
  author={Bowen Yang and Jian Zhang and Jonathan Li and Christopher R{\'e} and Christopher R. Aberger and Christopher De Sa},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.05124}
}


@article{natural_compression,
  author    = {Samuel Horvath and
               Chen{-}Yu Ho and
               Ludovit Horvath and
               Atal Narayan Sahu and
               Marco Canini and
               Peter Richt{\'{a}}rik},
  title     = {Natural Compression for Distributed Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1905.10988},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10988},
  archivePrefix = {arXiv},
  eprint    = {1905.10988},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1627--1635},
  year={2017},
  organization={JMLR. org}
}

@misc{ma2019hsic,
    title={The HSIC Bottleneck: Deep Learning without Back-Propagation},
    author={Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
    year={2019},
    eprint={1908.01580},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2902--2911},
  year={2017},
  organization={JMLR. org}
}

@misc{hendrycks2016gaussian,
    title={Gaussian Error Linear Units (GELUs)},
    author={Dan Hendrycks and Kevin Gimpel},
    year={2016},
    eprint={1606.08415},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{mlperf,
  title={{MLPerf Training Benchmark}},
  author={Peter Mattson and Christine Cheng and Cody Coleman and Greg Diamos
    and Paulius Micikevicius and David Patterson and Hanlin Tang and Gu-Yeon
      Wei and Peter Bailis and Victor Bittorf and David Brooks and Dehao Chen
      and Debojyoti Dutta and Udit Gupta and Kim Hazelwood and Andrew Hock and
      Xinyuan Huang and Bill Jia and Daniel Kang and David Kanter and Naveen
      Kumar and Jeffery Liao and Guokai Ma and Deepak Narayanan and Tayo
      Oguntebi and Gennady Pekhimenko and Lillian Pentecost and Vijay Janapa
      Reddi and Taylor Robie and Tom St. John and Carole-Jean Wu and Lingjie Xu
      and Cliff Young and Matei Zaharia},
  year={2020},
  booktitle = {{Proceedings of the 3rd Conference on Machine Learning and
    Systems (MLSys'20)}},
}

@article{sgd_with_momentum,
 author = {Qian, Ning},
 title = {On the Momentum Term in Gradient Descent Learning Algorithms},
 journal = {Neural Netw.},
 issue_date = {Jan. 1999},
 volume = {12},
 number = {1},
 month = jan,
 year = {1999},
 pages = {145--151},
 numpages = {7},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
 keywords = {critical damping, damped harmonic oscillator, gradient descent learning algorithm, learning rate, momentum, speed of convergence}
} 

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year      = {2015},
  timestamp = {Fri, 29 Mar 2019 10:36:36 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bookcorpus,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@inproceedings{
reddi2021adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=LkFG3lB13U5}
}

@inproceedings{chen2020toward,
author = {Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
title = {Toward Communication Efficient Adaptive Gradient Method},
year = {2020},
isbn = {9781450381031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412815.3416891},
doi = {10.1145/3412815.3416891},
booktitle = {Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
pages = {119–128},
numpages = {10},
keywords = {adaptive method, federated learning, convergence analysis},
location = {Virtual Event, USA},
series = {FODS '20}
}

 @InProceedings{pmlr-v97-karimireddy19a, title = {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes}, author = {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {3252--3261}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf}, url = { http://proceedings.mlr.press/v97/karimireddy19a.html }, abstract = {Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the biased nature of the sign compression operator. We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory.} } 
 
@article{scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{kaplan1974application,
  title={Application of programs with maximin objective functions to problems of optimal resource allocation},
  author={Kaplan, Seymour},
  journal={Operations Research},
  volume={22},
  number={4},
  pages={802--807},
  year={1974},
  publisher={INFORMS}
}

@article{Joshi2020TheSA,
  title={The State and Fate of Linguistic Diversity and Inclusion in the NLP World},
  author={Pratik M. Joshi and Sebastin Santy and A. Budhiraja and K. Bali and M. Choudhury},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.09095}
}

@article{Schwartz2020GreenA,
  title={Green AI},
  author={Roy Schwartz and Jesse Dodge and N. A. Smith and Oren Etzioni},
  journal={Communications of the ACM},
  year={2020},
  volume={63},
  pages={54 - 63}
}

@article{Strubell2019EnergyAP,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Emma Strubell and Ananya Ganesh and A. McCallum},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.02243}
}

@article{Henderson2020TowardsTS,
  title={Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning},
  author={Peter Henderson and Jie-Ru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05651}
}

@article{Anthony2020CarbontrackerTA,
  title={Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author={Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.03051}
}

@article{Kline2016HolisticallyET,
  title={Holistically evaluating the environmental impacts in modern computing systems},
  author={Donald Kline and Nikolas Parshook and Xiaoyu Ge and E. Brunvand and R. Melhem and Panos K. Chrysanthis and A. Jones},
  journal={2016 Seventh International Green and Sustainable Computing Conference (IGSC)},
  year={2016},
  pages={1-8}
}

@article{Bashroush2018ACR,
  title={A Comprehensive Reasoning Framework for Hardware Refresh in Data Centers},
  author={R. Bashroush},
  journal={IEEE Transactions on Sustainable Computing},
  year={2018},
  volume={3},
  pages={209-220}
}

@article{Qiu2020CanFL,
  title={Can Federated Learning Save The Planet},
  author={Xinchi Qiu and Titouan Parcollet and Daniel J. Beutel and Taner Topal and Akhil Mathur and N. Lane},
  journal={arXiv: Learning},
  year={2020}
}

@misc{baevski2020wav2vec,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xlmr,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
}

@misc{switch,
    title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
    author={William Fedus and Barret Zoph and Noam Shazeer},
    year={2021},
    eprint={2101.03961},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lars,
      title={Large Batch Training of Convolutional Networks}, 
      author={Yang You and Igor Gitman and Boris Ginsburg},
      year={2017},
      eprint={1708.03888},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

% Evaluation:
@inproceedings{rahimi-etal-2019-massively,
    title = "Massively Multilingual Transfer for {NER}",
    author = "Rahimi, Afshin  and
      Li, Yuan  and
      Cohn, Trevor",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1015",
    pages = "151--164",
}

@inproceedings{pan-etal-2017-cross,
    title = "Cross-lingual Name Tagging and Linking for 282 Languages",
    author = "Pan, Xiaoman  and
      Zhang, Boliang  and
      May, Jonathan  and
      Nothman, Joel  and
      Knight, Kevin  and
      Ji, Heng",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1178",
    doi = "10.18653/v1/P17-1178",
    pages = "1946--1958",
}

@InProceedings{pmlr-v119-hu20b,
  title = 	 {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  author =       {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4411--4421},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/hu20b/hu20b.pdf},
  url = 	 {
http://proceedings.mlr.press/v119/hu20b.html
},
}

# IndicGLUE
@inproceedings{kakwani-etal-2020-indicnlpsuite,
    title = "{I}ndic{NLPS}uite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for {I}ndian Languages",
    author = "Kakwani, Divyanshu  and
      Kunchukuttan, Anoop  and
      Golla, Satish  and
      N.C., Gokul  and
      Bhattacharyya, Avik  and
      Khapra, Mitesh M.  and
      Kumar, Pratyush",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.445",
    doi = "10.18653/v1/2020.findings-emnlp.445",
    pages = "4948--4961",
}


@inproceedings{conneau-etal-2020-unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
}

@misc{Sagor_2020,
  title   = {BanglaBERT: Bengali Mask Language Model for Bengali Language Understading},
  author  = {Sagor Sarker},
  year    = {2020},
  url    = {https://github.com/sagorbrur/bangla-bert}
}

@inproceedings{kakwani2020indicnlpsuite,
    title={{IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages}},
    author={Divyanshu Kakwani and Anoop Kunchukuttan and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},
    year={2020},
    booktitle={Findings of EMNLP},
}

% HF ecosystem
@misc{hftokenizers2019,
  author = {Anthony MOI and
                  Pierric Cistac and
                  Nicolas Patry and
                  Evan P. Walsh and
                  Funtowicz Morgan and
                  Sebastian Pütz and
                  Thomas Wolf and
                  Sylvain Gugger and
                  Clément Delangue and
                  Julien Chaumond and
                  Lysandre Debut and
                  Patrick von Platen},
  title = {Hugging Face Tokenizers library},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4784271},
  howpublished = {\url{https://github.com/huggingface/tokenizers}}
}

@inproceedings{butterfly_arsgd,
author = {Li, Zhenyu and Davis, James and Jarvis, Stephen},
title = {An Efficient Task-Based All-Reduce for Machine Learning Applications},
year = {2017},
isbn = {9781450351379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3146347.3146350},
doi = {10.1145/3146347.3146350},
booktitle = {Proceedings of the Machine Learning on HPC Environments},
articleno = {2},
numpages = {8},
keywords = {Data-flow Frameworks, Apache Spark, Synchronous Model Training, Butterfly All-Reduce},
location = {Denver, CO, USA},
series = {MLHPC'17}
}

  
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}
@article{2020HuggingFace-datasets,
  title={Datasets},
  author={Thomas Wolf and Quentin Lhoest and Patrick von Platen and Yacine Jernite and Mariama Drame and Julien Plu and Julien Chaumond and Clement Delangue and Clara Ma and Abhishek Thakur and Suraj Patil and Joe Davison and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angie McMillan-Major and Simon Brandeis and Sylvain Gugger and François Lagunas and Lysandre Debut and Morgan Funtowicz and Anthony Moi and Sasha Rush and Philipp Schmidd and Pierric Cistac and Victor Muštar and Jeff Boudier and Anna Tordjmann},
  journal={GitHub. Note: https://github.com/huggingface/datasets},
  volume={1},
  year={2020}
}

@INPROCEEDINGS {dlhub,
author = {R. Chard and Z. Li and K. Chard and L. Ward and Y. Babuji and A. Woodard and S. Tuecke and B. Blaiszik and M. J. Franklin and I. Foster},
booktitle = {2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
title = {DLHub: Model and Data Serving for Science},
year = {2019},
volume = {},
issn = {},
pages = {283-292},
keywords = {computational modeling;metadata;adaptation models;biological system modeling;training;learning systems},
doi = {10.1109/IPDPS.2019.00038},
url = {https://doi.ieeecomputersociety.org/10.1109/IPDPS.2019.00038},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@inproceedings{dp_sgd,
 author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
 volume = {30},
 year = {2017}
}





@inproceedings{hole_punching,
author = {Ford, Bryan and Srisuresh, Pyda and Kegel, Dan},
title = {Peer-to-Peer Communication across Network Address Translators},
year = {2005},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the Annual Conference on USENIX Annual Technical Conference},
pages = {13},
numpages = {1},
location = {Anaheim, CA},
series = {ATEC '05}
}

@inproceedings{ott2018scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
}

@inproceedings{swav,
 author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {9912--9924},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
 url = {https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{
mixed_precision,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@article{foldingathome,
  title={Folding@home: Lessons from eight years of volunteer distributed computing},
  author={A. L. Beberg and D. Ensign and G. Jayachandran and S. Khaliq and V. Pande},
  journal={2009 IEEE International Symposium on Parallel \& Distributed Processing},
  year={2009},
  pages={1-8}
}
@article{einstein_at_home,
   title={Einstein@Home All-sky Search for Continuous Gravitational Waves in LIGO O2 Public Data},
   volume={909},
   ISSN={1538-4357},
   url={http://dx.doi.org/10.3847/1538-4357/abc7c9},
   DOI={10.3847/1538-4357/abc7c9},
   number={1},
   journal={The Astrophysical Journal},
   publisher={American Astronomical Society},
   author={Steltner, B. and Papa, M. A. and Eggenstein, H.-B. and Allen, B. and Dergachev, V. and Prix, R. and Machenschalk, B. and Walsh, S. and Zhu, S. J. and Behnke, O. and et al.},
   year={2021},
   month={Mar},
   pages={79}
}

@article{pytorch_distributed,
author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
title = {PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415530},
doi = {10.14778/3415478.3415530},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3005–3018},
numpages = {14}
}

@misc{zhu2021transfer,
      title={Transfer Learning in Deep Reinforcement Learning: A Survey}, 
      author={Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou},
      year={2021},
      eprint={2009.07888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{7298965,

  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},

  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Fully convolutional networks for semantic segmentation}, 

  year={2015},

  volume={},

  number={},

  pages={3431-3440},

  doi={10.1109/CVPR.2015.7298965}}

@inproceedings{10.1109/CVPR.2014.81,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
year = {2014},
isbn = {9781479951185},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CVPR.2014.81},
doi = {10.1109/CVPR.2014.81},
booktitle = {Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {580–587},
numpages = {8},
series = {CVPR '14}
}
@inproceedings{pmlr-v32-donahue14,
  title={DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  author={J. Donahue and Y. Jia and Oriol Vinyals and Judy Hoffman and Ning Zhang and Eric Tzeng and Trevor Darrell},
  booktitle={ICML},
  year={2014}
}

 @misc{johnson2016perceptual,
      title={Perceptual Losses for Real-Time Style Transfer and Super-Resolution}, 
      author={Justin Johnson and Alexandre Alahi and Li Fei-Fei},
      year={2016},
      eprint={1603.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{honda2019smiles,
      title={SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery}, 
      author={Shion Honda and Shoi Shi and Hiroki R. Ueda},
      year={2019},
      eprint={1911.04738},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article {Lu2020.09.04.283929,
	author = {Lu, Amy X. and Zhang, Haoran and Ghassemi, Marzyeh and Moses, Alan},
	title = {Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization},
	elocation-id = {2020.09.04.283929},
	year = {2020},
	doi = {10.1101/2020.09.04.283929},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929},
	eprint = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929.full.pdf},
	journal = {bioRxiv}
}

@inproceedings{NEURIPS2020_92d1e1eb,
 author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12449--12460},
 publisher = {Curran Associates, Inc.},
 title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
 url = {https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{tfhub,
  title = {{TensorFlow} {H}ub},
  howpublished = {\url{https://www.tensorflow.org/hub}},
  note = {Accessed: 2021-05-20}
}

@misc{torchhub,
  title = {{PyTorch} {H}ub},
  howpublished = {\url{https://pytorch.org/hub/}},
  note = {Accessed: 2021-05-20}
}

@misc{hfhub,
  title = {{Hugging Face} {H}ub},
  howpublished = {\url{https://huggingface.co/models}},
  note = {Accessed: 2021-05-20}
}

@inproceedings{Tapparello2016VolunteerCO,
  title={Volunteer Computing on Mobile Devices: State of the Art and Future Research Directions},
  author={C. Tapparello and Colin Funai and Shurouq Hijazi and Abner Aquino and Bora Karaoglu and H. Ba and J. Shi and W. Heinzelman},
  year={2016},
  booktitle={Enabling Real-Time Mobile Cloud Computing through Emerging Technologies},
  pages = {153-181}
}

@misc{shi2018performance,
      title={Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs}, 
      author={Shaohuai Shi and Qiang Wang and Xiaowen Chu},
      year={2018},
      eprint={1711.05979},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{sergeev2018horovod,
      title={Horovod: fast and easy distributed deep learning in TensorFlow}, 
      author={Alexander Sergeev and Mike Del Balso},
      year={2018},
      eprint={1802.05799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{jft300m,
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}


@article{gpipe,
  title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author={Y. Huang and Yonglong Cheng and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Z. Chen},
  journal={ArXiv},
  year={2019},
  volume={abs/1811.06965}
}


@article{Collaboration2016ObservationOG,
  author={Abbott, Benjamin and LIGO Scientific Collaboration and Virgo Collaboration},
    year = {2016},
    month = {02},
    pages = {},
    title = {{Observation of Gravitational Waves from a Binary Black Hole Merger}},
    volume = {116},
    journal = {Physical Review Letters},
    doi = {10.1103/PhysRevLett.116.061102}
}
@article{Collaboration2012ObservationOA,
  author={Aad, Georges and Abajyan, Tatevik and The ATLAS Collaboration},
year = {2012},
month = {09},
pages = {1–29},
title = {{Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC}},
volume = {716},
journal = {Physics Letters B}
}


@article{ISS,
author = {Ruttley, Tara and Robinson, Julie and Gerstenmaier, William},
year = {2017},
month = {12},
pages = {1160-1174},
title = {The International Space Station: Collaboration, Utilization, and Commercialization*: The International Space Station},
volume = {98},
journal = {Social Science Quarterly},
doi = {10.1111/ssqu.12469}
}

@misc{huang2019patient,
      title={Patient Clustering Improves Efficiency of Federated Machine Learning to predict mortality and hospital stay time using distributed Electronic Medical Records}, 
      author={Li Huang and Dianbo Liu},
      year={2019},
      eprint={1903.09296},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{setihomeamdfail,
  title = {Flakey AMD/ATI GPUs, including RX 5700 XT, Cross Validating, polluting the Database},
  author = {Bob Smith},
  url = {https://setiathome.berkeley.edu/forum_thread.php?id=84508},
  note = {Accessed: 2021-05-20},
  year = {2019},
}



@INPROCEEDINGS{4568164,
  author={Chor, Benny and Goldwasser, Shafi and Micali, Silvio and Awerbuch, Baruch},
  booktitle={26th Annual Symposium on Foundations of Computer Science (sfcs 1985)}, 
  title={Verifiable secret sharing and achieving simultaneity in the presence of faults}, 
  year={1985},
  volume={},
  number={},
  pages={383-395},
  doi={10.1109/SFCS.1985.64}}
  
@inproceedings{tolpegin2020data,
  title={Data Poisoning Attacks Against Federated Learning Systems},
  author={Vale Tolpegin and Stacey Truex and Mehmet Emre Gursoy and Ling Liu},
  booktitle={ESORICS},
  year={2020}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}