@book{durrett2010probability,
  title={Probability: Theory and Examples},
  author={Durrett, Rick},
  edition={Fourth},
  year={2010},
  publisher={Cambridge university press}
}

%------------------------------%
% Additional suggested citations:
%------------------------------%
@article{biau2010rates,
  title={Rates of convergence of the functional $ k $-nearest neighbor estimate},
  author={Biau, G{\'e}rard and C{\'e}rou, Fr{\'e}d{\'e}ric and Guyader, Arnaud},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={4},
  pages={2034--2040},
  year={2010},
  publisher={IEEE}
}
% Suggested by Laszlo.

@article{doring2018rate,
  title={Rate of convergence of k-nearest-neighbor classification rule},
  author={D{\"o}ring, Maik and Gy{\"o}rfi, L{\'a}szl{\'o} and Walk, Harro},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={227},
  pages={1--16},
  year={2018},
  publisher={JMLR. org}
}
% Suggested by Laszlo.

@article{chzhen2019minimax,
  title={Minimax semi-supervised confidence sets for multi-class classification},
  author={Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed},
  journal={arXiv preprint arXiv:1904.12527},
  year={2019}
}
% Suggested by Sasha Tsybakov.
% Some strange multi-class margin condition--will have to investigate further.


@article{puchkin2020adaptive,
  title={An adaptive multiclass nearest neighbor classifier},
  author={Puchkin, Nikita and Spokoiny, Vladimir},
  journal={ESAIM: Probability and Statistics},
  volume={24},
  pages={69--99},
  year={2020},
  publisher={EDP Sciences}
}
% Suggested by Laszlo, contains multiclass margin.
% Aggregate across different values of k.
% D. Belomestny and V. Spokoiny, Spatial aggregation of local likelihood estimates with applications to classification. Ann. Stat. 35 (2007).
%------------------------------%
% Additional citations:
%------------------------------%

@article{gadat2016classification,
  title={Classification in general finite dimensional spaces with the k-nearest neighbor rule},
  author={Gadat, S{\'e}bastien and Klein, Thierry and Marteau, Cl{\'e}ment},
  journal={The Annals of Statistics},
  volume={44},
  number={3},
  pages={982--1009},
  year={2016},
  publisher={Institute of Mathematical Statistics}
}

@article{cannings2019local,
  title={Local nearest neighbour classification with applications to semi-supervised learning},
  author={Cannings, Timothy I and Berrett, Thomas B and Samworth, Richard J},
  journal={arXiv preprint arXiv:1704.00642 v3},
  year={2019}
}


%------------------------------%
% Surveys on Imbalanced Classification
%------------------------------%
@article{he2009learning,
  title={Learning from imbalanced data},
  author={He, Haibo and Garcia, Edwardo A},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={21},
  number={9},
  pages={1263--1284},
  year={2009},
  publisher={Ieee}
}

@book{fernandez2018learning,
  title={Learning from Imbalanced Data Sets},
  author={Fern{\'a}ndez, Alberto and Garc{\'\i}a, Salvador and Galar, Mikel and Prati, Ronaldo C and Krawczyk, Bartosz and Herrera, Francisco},
  year={2018},
  publisher={Springer}
}


%------------------------------%
% kNN: Historical
%------------------------------%
@techreport{fix1951discriminatory,
  title={Discriminatory analysis-nonparametric discrimination: consistency properties},
  author={Fix, Evelyn and Hodges, Joseph L},
  year={1951},
  institution={USAF School of Aviation Medicine, Randolph Field, Texas}
}

@article{cover1967nearest,
  title={Nearest neighbor pattern classification},
  author={Cover, Thomas and Hart, Peter},
  journal={IEEE Transactions on Information Theory},
  volume={13},
  number={1},
  pages={21--27},
  year={1967},
  publisher={IEEE}
}

@article{stone1977consistent,
  title={Consistent nonparametric regression},
  author={Stone, Charles J},
  journal={The Annals of Statistics},
  pages={595--620},
  year={1977},
  publisher={JSTOR}
}


%------------------------------%
% kNN: Statistical
%------------------------------%
@article{samworth2012optimal,
  title={Optimal weighted nearest neighbour classifiers},
  author={Samworth, Richard J},
  journal={Annals of Statistics},
  volume={40},
  number={5},
  pages={2733--2763},
  year={2012},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{chaudhuri2014rates,
  title={Rates of convergence for nearest neighbor classification},
  author={Chaudhuri, Kamalika and Dasgupta, Sanjoy},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3437--3445},
  year={2014}
}

@book{biau2015lectures,
  title={Lectures on the Nearest Neighbor Method},
  author={Biau, G{\'e}rard and Devroye, Luc},
  year={2015},
  publisher={Springer}
}

%------------------------------%
% kNN: Approximation/Computational
%------------------------------%
@article{hart1968condensed,
  title={The condensed nearest neighbor rule},
  author={Hart, Peter},
  journal={IEEE Transactions on Information Theory},
  volume={14},
  number={3},
  pages={515--516},
  year={1968}
}

@inproceedings{kontorovich2017nearest,
  title={Nearest-neighbor sample compression: Efficiency, consistency, infinite dimensions},
  author={Kontorovich, Aryeh and Sabato, Sivan and Weiss, Roi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1573--1583},
  year={2017}
}
% Also need to look at this.

@article{gottlieb2018near, 
  author={L. {Gottlieb} and A. {Kontorovich} and P. {Nisnevitch}}, 
  journal={IEEE Transactions on Information Theory}, 
  title={Near-Optimal Sample Compression for Nearest Neighbors}, 
  year={2018}, 
  volume={64}, 
  number={6}, 
  pages={4120-4128}
}
% Also from Aryeh.

@inproceedings{efremenko2020fast,
  title={Fast and {B}ayes-consistent nearest neighbors},
  author={Efremenko, Klim and Kontorovich, Aryeh and Noivirt, Moshe},
  booktitle={Proceedings of the 23rd International Concerence on Artificial Intelligence and Statistics},
  publisher={PMLR},
  year={2020}
}
% Need to look at this.


%------------------------------%
% kNN: Imbalanced Classification
%------------------------------%
%% Prototypes again
@inproceedings{liu2011class,
  title={Class confidence weighted knn algorithms for imbalanced data sets},
  author={Liu, Wei and Chawla, Sanjay},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={345--356},
  year={2011},
  organization={Springer}
}
% Weight the prototypes by a prior on prob(x|y), which seems weird.
% Again, mostly empirical. Tries to tie things to Bayesian networks.

%% Prototype slection.
@article{lopez2014addressing,
  title={Addressing imbalanced classification with instance generation techniques: {IPADE-ID}},
  author={L{\'o}pez, Victoria and Triguero, Isaac and Carmona, Crist{\'o}bal J and Garc{\'\i}a, Salvador and Herrera, Francisco},
  journal={Neurocomputing},
  volume={126},
  pages={15--28},
  year={2014},
  publisher={Elsevier}
}
% Dataset manipulation approach + knn, i.e., modify data points. They call this prototype selection. 
% Sort of a compression.
% Algorithmic, experimental. No theory.

@article{vluymans2016eprennid,
  title={EPRENNID: An evolutionary prototype reduction based ensemble for nearest neighbor classification of imbalanced data},
  author={Vluymans, Sarah and Triguero, Isaac and Cornelis, Chris and Saeys, Yvan},
  journal={Neurocomputing},
  volume={216},
  pages={596--610},
  year={2016},
  publisher={Elsevier}
}
% Same sort of deal, prototype selection + knn.
% Algorithmic, experimental, no theory.


%% Gravitational methods.
@article{cano2013weighted,
  title={Weighted data gravitation classification for standard and imbalanced data},
  author={Cano, Alberto and Zafra, Amelia and Ventura, Sebasti{\'a}n},
  journal={IEEE Transactions on Cybernetics},
  volume={43},
  number={6},
  pages={1672--1687},
  year={2013},
  publisher={IEEE}
}
% Combine gravitation ideas with nearest neighbors, adjust the distance function essentially.
% No theory; algorithmic and experimental.

@article{zhu2015gravitational,
  title={Gravitational fixed radius nearest neighbor for imbalanced problem},
  author={Zhu, Yujin and Wang, Zhe and Gao, Daqi},
  journal={Knowledge-Based Systems},
  volume={90},
  pages={224--238},
  year={2015},
  publisher={Elsevier}
}
% Gravitational nearest neighbors classifier with a fixed radius.
% No theory.




%------------------------------%
% Multiclass Methods
%------------------------------%
@article{tewari2007consistency,
  title={On the consistency of multiclass classification methods},
  author={Tewari, Ambuj and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={May},
  pages={1007--1025},
  year={2007}
}
% Try to prove consistency for losses in multiclass classification, i.e., the multiclass analogue of Bartlett, Jordan, McAuliff.

@inproceedings{liu2007fisher,
  title={Fisher consistency of multicategory support vector machines},
  author={Liu, Yufeng},
  booktitle={Artificial Intelligence and Statistics},
  pages={291--298},
  year={2007}
}
% Fisher consistency = classification calibration.
% Shows 3 extensions are inconsistent, 1 is consistent.

@article{wang2008probability,
  title={Probability estimation for large-margin classifiers},
  author={Wang, Junhui and Shen, Xiaotong and Liu, Yufeng},
  journal={Biometrika},
  volume={95},
  number={1},
  pages={149--167},
  year={2008},
  publisher={Oxford University Press}
}
% Basis for wu2010robust.
% Essentially use SVMs to estimate class probabilities (in binary classification).
% Also considers \Psi-estimation.
% Also looks at rates of convergence for classification versus probability estimation in \Psi-learning, finding that classification is easier.

@article{wu2010robust,
  title={Robust model-free multiclass probability estimation},
  author={Wu, Yichao and Zhang, Hao Helen and Liu, Yufeng},
  journal={Journal of the American Statistical Association},
  volume={105},
  number={489},
  pages={424--436},
  year={2010},
  publisher={Taylor \& Francis}
}
% An extension of the Wang, Shen, Lin 2008 Biometrika paper on estimating class probabilities (regression function) using an SVM. It uses the fact that SVMs are classification-consistent and therefore estimate the boundary sign(p(x) - 1/2\), and the 1/2 can be altered to any threshold t by weighting the losses.
% Border weights key--a weighting such that all weighted probabilities are equal at a given point. Gives two algorithms for finding the weights (may be computationally intensive with a large number of classes). Can estimate class probabilities from the border weights.
% Related to our work because of weighting losses by class, although here it is to estimate class probabilities rather than to optimize some other metric in a plug-in setting.

@article{wang2019multiclass,
  title={Multiclass Probability Estimation with Support Vector Machines},
  author={Wang, Xin and Helen Zhang, Hao and Wu, Yichao},
  journal={Journal of Computational and Graphical Statistics},
  pages={1--18},
  year={2019},
  publisher={Taylor \& Francis}
}
% New algorithm for estimating class probabilities in the multiclass setting, again using the weighted SVM trick.
% Algorithm is O(k^2), can handle up to k = 10 or so.
% Some consistency too.


%------------------------------%
% Data Augmentation
%------------------------------%
@article{chen2019invariance,
  title={Invariance reduces Variance: Understanding Data Augmentation in Deep Learning and Beyond},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={arXiv preprint arXiv:1907.10905},
  year={2019}
}

%------------------------------%
% Classical Classification
%------------------------------%
@article{fisher1936use,
  title={The use of multiple measurements in taxonomic problems},
  author={Fisher, Ronald A},
  journal={Annals of Eugenics},
  volume={7},
  number={2},
  pages={179--188},
  year={1936},
  publisher={Wiley Online Library}
}

@article{fisher1938statistical,
  title={The statistical utilization of multiple measurements},
  author={Fisher, Ronald A},
  journal={Annals of Eugenics},
  volume={8},
  number={4},
  pages={376--386},
  year={1938},
  publisher={Wiley Online Library}
}

%------------------------------%
% Nonparametric Regression
%------------------------------%
@book{gyorfi2002distribution,
  title={A distribution-free theory of nonparametric regression},
  author={Gy{\"o}rfi, L{\'a}szl{\'o} and Kohler, Michael and Krzyzak, Adam and Walk, Harro},
  year={2002},
  publisher={Springer Science \& Business Media}
}

%------------------------------%
% Nonparametric Classification
%------------------------------%

@book{devroye1996probabilistic,
  title={A probabilistic theory of pattern recognition},
  author={Devroye, Luc and Gy{\"o}rfi, L{\'a}szl{\'o} and Lugosi, G{\'a}bor},
  year={1996},
  publisher={Springer Science \& Business Media}
}


@article{mammen1999smooth,
  title={Smooth discrimination analysis},
  author={Mammen, Enno and Tsybakov, Alexandre B},
  journal={The Annals of Statistics},
  volume={27},
  number={6},
  pages={1808--1829},
  year={1999},
  publisher={Institute of Mathematical Statistics}
}

@article{audibert2007fast,
  title={Fast learning rates for plug-in classifiers},
  author={Audibert, Jean-Yves and Tsybakov, Alexandre B},
  journal={The Annals of Statistics},
  volume={35},
  number={2},
  pages={608--633},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}
%------------------------------%
% Saddle-point/Minimax
%------------------------------%

@article{ben2011simpler,
  title={A simpler proof of the Von Neumann minimax theorem},
  author={Ben-El-Mechaiekh, Hichem and Dimand, Robert W},
  journal={The American Mathematical Monthly},
  volume={118},
  number={7},
  pages={636--641},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{hazan2016introduction,
  title={Introduction to online convex optimization},
  author={Hazan, Elad},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@book{plg2006,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, Gabor},
  year={2006},
  publisher={Cambridge university press}
}


%------------------------------%
% Neural Network Minimax/GANs
%------------------------------%

@article{daskalakis2017training,
  title={Training {GAN}s with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal={arXiv preprint arXiv:1711.00141},
  year={2017}
}

@article{liang2018interaction,
  title={Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks},
  author={Liang, Tengyuan and Stokes, James},
  journal={arXiv preprint arXiv:1802.06132},
  year={2018}
}

%------------------------------%
% Stochastic Programming
%------------------------------%

@article{ahmadi2012entropic,
  title={Entropic value-at-risk: A new coherent risk measure},
  author={Ahmadi-Javid, Amir},
  journal={Journal of Optimization Theory and Applications},
  volume={155},
  number={3},
  pages={1105--1123},
  year={2012},
  publisher={Springer}
}

@book{shapiro2009lectures,
  title={Lectures on stochastic programming: modeling and theory},
  author={Shapiro, Alexander and Dentcheva, Darinka and Ruszczy{\'n}ski, Andrzej},
  year={2009},
  publisher={SIAM}
}
% Chapter 6: Risk Averse Optimization.
%--6.2 Mean risk models. Value at risk (VaR), average value at risk (AVaR) = conditional value at risk (CVaR). Conservative convex approximation of VaR.
%--6.3 Coherent risk measures. Coherence = convex, monotone, translation equivariant, and positive 1-homogeneous. 
%--Average (Conditional) Value at Risk is coherent.
%--Some differentiability properties.
%--Dual formulations. Dual of AVaR (CVaR).


@article{rockafellar2000optimization,
  title={Optimization of conditional value-at-risk},
  author={Rockafellar, R Tyrrell and Uryasev, Stanislav},
  journal={Journal of risk},
  volume={2},
  pages={21--42},
  year={2000}
}
% Another optimization paper.
% Mainly about demonstrating that CVaR is good for optimization.
% Portfolio examples.
% The starting point for CVaR here is the "dual" formulation, i.e., the one that makes sense for portfolio optimization.
% Proposes an empirical objective based on samples.
% Contains a numerical portfolio optimization example. 3 assets.

%------------------------------%
% Robust Optimization
%------------------------------%
@article{ben1999robust,
  title={Robust solutions of uncertain linear programs},
  author={Ben-Tal, Aharon and Nemirovski, Arkadi},
  journal={Operations research letters},
  volume={25},
  number={1},
  pages={1--13},
  year={1999},
  publisher={Elsevier}
}

@article{ben2003robust,
  title={Robust solutions of linear programming problems contaminated with uncertain data},
  author={Ben-Tal, Aharon and Nemirovski, Arkadi},
  journal={Mathematical programming},
  volume={88},
  number={3},
  pages={411--424},
  year={2003},
  publisher={Springer}
}

@article{ben2004adjustable,
  title={Adjustable robust solutions of uncertain linear programs},
  author={Ben-Tal, Aharon and Goryashko, Alexander and Guslitzer, Elana and Nemirovski, Arkadi},
  journal={Mathematical Programming},
  volume={99},
  number={2},
  pages={351--376},
  year={2004},
  publisher={Springer}
}

@book{ben2009,
  title={Robust Optimization},
  author={Ben-Tal, Aharon and El Ghaoui, Laurent and Nemirovski, Arkadi},
  year={2009},
  publisher={Princeton University Press}
}

%------------------------------%
% Distributionally Robust Optimization
%------------------------------%

@article{ben2013,
  title={Robust solutions of optimization problems affected by uncertain probabilities},
  author={Ben-Tal, Aharon and Den Hertog, Dick and De Waegenaere, Anja and Melenberg, Bertrand and Rennen, Gijs},
  journal={Management Science},
  volume={59},
  number={2},
  pages={341--357},
  year={2013},
  publisher={INFORMS}
}

@article{bertsimas2014,
  title={Robust sample average approximation},
  author={Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
  journal={Mathematical Programming},
  pages={1--66},
  year={2014},
  publisher={Springer}
}

@article{bertsimas2018,
  title={Data-driven robust optimization},
  author={Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
  journal={Mathematical Programming},
  volume={167},
  number={2},
  pages={235--292},
  year={2018},
  publisher={Springer}
}

@article{goh2010distributionally,
  title={Distributionally robust optimization and its tractable approximations},
  author={Goh, Joel and Sim, Melvyn},
  journal={Operations research},
  volume={58},
  number={4-part-1},
  pages={902--917},
  year={2010},
  publisher={INFORMS}
}
% Want to solve linear multistage stochastic optimization problems. CVaR is an example.

%------------------------------%
% Statistical Imbalanced classification
%------------------------------%
@book{zipf1936psycho,
  title={The Psycho-Biology of Language: an Introduction to Dynamic Philology},
  author={Zipf, George Kingsley},
  year={1936},
  publisher={George Routledge \& Sons, Ltd.}
}
% Origin of the Zipf distribution for language.

@inproceedings{salakhutdinov2011learning,
  title={Learning to share visual appearance for multiclass object detection},
  author={Salakhutdinov, Ruslan and Torralba, Antonio and Tenenbaum, Josh},
  booktitle={CVPR 2011},
  pages={1481--1488},
  year={2011},
  organization={IEEE}
}
% Hierarchical classification because of little data.
% For images.
% Also notes that text data follows Zipf's law, i.e., power law decay.

@inproceedings{zhu2014capturing,
  title={Capturing long-tail distributions of object subcategories},
  author={Zhu, Xiangxin and Anguelov, Dragomir and Ramanan, Deva},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={915--922},
  year={2014}
}
% References salakhutdinov2011learning because of long tail.
% Again a mixture thing to share examples across subcategories.

@article{feldman2019does,
  title={Does Learning Require Memorization? A Short Tale about a Long Tail},
  author={Feldman, Vitaly},
  journal={arXiv preprint arXiv:1906.05271},
  year={2019}
}
% An interpolation/model/differential privacy paper.
% Proposes a heavy-tailed model for the data-generating process. Apparently does some statistical learning to show when interpolating is necessary.
% Shows interpolation is at odds with differential privacy. Sort of to be expected from a stability point of view (since differential privacy is really a stability condition).
% Need to read more carefully, but essentially under this model, need to learn labels of classes with 1/n frequency (because there are many), and this requires memorization.
% Assumes disjoint support for main example.


@article{fithian2014local,
  title={Local case-control sampling: Efficient subsampling in imbalanced data sets},
  author={Fithian, William and Hastie, Trevor},
  journal={Annals of Statistics},
  volume={42},
  number={5},
  pages={1693},
  year={2014}
}

%------------------------------%
% More cost-weighting: margins
%------------------------------%
@article{lin2002support,
  title={Support vector machines for classification in nonstandard situations},
  author={Lin, Yi and Lee, Yoonkyung and Wahba, Grace},
  journal={Machine learning},
  volume={46},
  number={1-3},
  pages={191--202},
  year={2002},
  publisher={Springer}
}
%% Referenced by Joachims 2005 paper, presumably because both are SVM papers.
% Considers weights for different classes and transfer learning.
% For the SVM, slack variables are modified depending on the costs.
% (Lemma 2?) clearly aware that the Bayes rule changes to be \sgn(p_{s} - L_{0}/(L_{0} + L_{1}).


@article{scott2012calibrated,
  title={Calibrated asymmetric surrogate losses},
  author={Scott, Clayton},
  journal={Electronic Journal of Statistics},
  volume={6},
  pages={958--992},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}
% Want consistency/calibration for losses when training data is imbalanced or misclassification costs differ.
% Still binary classification.
% Lots of example losses, including uneven margins.
% Old news that asymmetric losses improve performance in imbalanced data.
% Calibrated: surrogate loss excess risk -> 0 implies target loss excess risk -> 0.
% An extension of Bartlett, Jordan, McAuliffe, etc.
% Steinwart also considers cost-sensitive classification.
% Zhang; Tewari and Bartlett study calibration for the multiclass case... but not for uneven data.
% Notes that in some cases, Fisher consistent, admissible, and classification-calibrated are all the same thing.

@article{cao2019learning,
  title={Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss},
  author={Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  journal={arXiv preprint arXiv:1906.07413},
  year={2019}
}
% First derive a margin bound where the margins are different for each class.
% Then show that the optimal bound has heterogeneous margins, the size of which depend on the number of examples in a class.
% Propose Label-distribution-aware margin (LDAM) objective to try to enforce that margin.
% Propose a two-step algorithm for optimization. Step 1, ERM on LDAM. Step 2, reweight according to class sizes, and optimize.

%------------------------------%
% General Classification Metrics: Information Retrieval
%------------------------------%
@article{van1974foundation,
  title={Foundation of evaluation},
  author={Van Rijsbergen, Cornelis Joost},
  journal={Journal of Documentation},
  volume={30},
  number={4},
  pages={365--373},
  year={1974},
  publisher={MCB UP Ltd}
}
%% Origin of F-measure.

@book{van1979information,
  title={Information Retrieval},
  author={Van Rijsbergen, Cornelis Joost},
  year={1979},
  edition={2nd},
  address={London},
  publisher={Butterworth-Heinemann}
}
% More readily-accessible book where F-measure first appears (or is derived from, anyways).
% Reprints much of the Foundation of evaluation article, according to the footnote.


@inproceedings{lewis1995evaluating,
  title={Evaluating and optimizing autonomous text classification systems},
  author={Lewis, David D},
  booktitle={SIGIR},
  volume={95},
  pages={246--254},
  year={1995},
  organization={Citeseer}
}
% An information retrieval paper.
% Binary text classification.
% Wants to replace rankings because they require human supervision. Formulate and optimize a metric instead.
% Suggests thresholding class probabilities, which requires estimating class probabilities.
% References F measure as being from van Rijsbergen.

%------------------------------%
% General Classification Metrics: Machine Learning
%------------------------------%

@inproceedings{joachims2005support,
  title={A support vector method for multivariate performance measures},
  author={Joachims, Thorsten},
  booktitle={Proceedings of the 22nd International Conference on Machine Learning},
  pages={377--384},
  year={2005},
  organization={ACM}
}
% Method for SVM to optimize metrics like F1 score.
% Gives an algorithm that works in polynomial time sometimes.
% Imbalanced classification is in mind.
% First part of the paper reformulated the problem to consider all the data at once, for non-decomposable metrics.
% This leads to an optimization problem with an exponential number of constraints, but some algorithm seems to work.
% No statistical results.



@inproceedings{ye2012optimizing,
  title={Optimizing {F}-measure: A tale of two approaches},
  author={Nan Ye and Chai, Kian Ming and Lee, Wee Sun and Chieu, Hai Leong},
  booktitle = 	 {Proceedings of the 29th International Conference on Machine Learning},
  year={2012}
}
%% Abstract
% F-measures
% EUM (Empirical Utility Maximization)
% DTA (Decision-Theoretic Approach, i.e., plug-in)
% Lots of empirical evaluation.
%% Paper
% DTA not commonly used for F-measure.
% Understand the convergence for a single classifier in EUM. Not uniform over classifiers.
% DTA analysis is limited: assume that P(X, Y) is known, and then the DTA approach is the optimal. So, compare against the Bayes optimal.
% Uniform convergence over thresholds, but no rates, and only of expected empirical F-measure, not high-probability empirical, under the EUM approach.
% Some O(n^2) algorithms for maximizaing F-measure.
% Lots of experiments.


@inproceedings{dembczynski2013optimizing,
  title = 	 {Optimizing the {F}-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization},
  year = {2013},
  author = 	 {Krzysztof Dembczynski and Arkadiusz Jachnik and Wojciech Kotlowski and Willem Waegeman and Eyke Huellermeier},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  publisher = 	 {PMLR}
}
%% Abstract
% Plug-in approach for optimizing F-measure in multilabel classification.
% Compares against Structured SVM (SSVM).
% Computational considerations, also statistical consistency.
% Plug-in is consistent, SSVM is not.
%% Paper
% Prior references: Lewis 1995, Chai 2005, Jansche 2007, etc.
% Main theoretical result is consistency.
% SSVM from Petterson & Caetano 2010, 2011.
% F\beta measure Bayes classifier: inner and outer maximization.
% Organization is very algorithmic.
% Decompose multi-label to binary, which simplifies things computationally but may lead to incompatible estimates.
% COnsistency here = convergence to best optimal, even using surrogate losses. Inconsistency of SSVM on a simple example. Consistency for EFP (estimate F plug-in method).

@inproceedings{menon2013statistical,
  title={On the statistical consistency of algorithms for binary classification under class imbalance},
  author={Menon, Aditya and Narasimhan, Harikrishna and Agarwal, Shivani and Chawla, Sanjay},
  booktitle={International Conference on Machine Learning},
  pages={603--611},
  year={2013}
}
% Arithmetic Mean (AM) = balance weighted risk.
% A nonparametric statistics-type paper, less on the learning theory side. Consistency but not rates.
% Consistency of plug-in rule type stuff. Also consistency of ERM. Weak results because weak assumptions; also not doing statistical learning theory (or full-on nonparametric stuff).

@inproceedings{narasimhan2014statistical,
  title={On the statistical consistency of plug-in classifiers for non-decomposable performance measures},
  author={Narasimhan, Harikrishna and Vaish, Rohit and Agarwal, Shivani},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1493--1501},
  year={2014}
}
%% Abstract.
% Looking at consistency for non-decomposable metrics.
% Plug-in methods.
% Performance metrics that are functions of TPR and TNR.
%% Paper
% Ye et al. gave consistent algorithms when the regression function is known (strong assumption).
% p = \prob(Y = 1).
% \Psi-consistency: would like convergence to optimal.
% CPE (Class Probability Estimation) algorithm, e.g. regression function estimator.
% L_{1} consistency: \expect_{X}|\hat{\eta}(X) -\eta(X)| \probto 0.
% Assumption A: Continuous CDF for \eta(X)| Y at the threshold, i.e., the threshold is not an atom for the distribution of the regression function. In our case, we need to have this for every class, which is entirely analogous.
% Lemma 5: Uniform consistency over threshold classifiers, used because the estimated and actual thresholds are not the same (but may be close).
% Proofs are similar in spirit, but the analysis is quite qualitative in that it uses consistency rather than rates.
% Consider some slightly restricted class of performance measures in TPR and TNR, including non-decomposable metrics.

@incollection{koyejo2014consistent,
	title = {Consistent {{Binary Classification}} with {{Generalized Performance Metrics}}},
	booktitle = {Advances in {{Neural Information Processing Systems}} 27},
	publisher = {{Curran Associates, Inc.}},
	year = {2014},
	pages = {2744--2752},
	author = {Koyejo, Oluwasanmi O and Natarajan, Nagarajan and Ravikumar, Pradeep K and Dhillon, Inderjit S},
}
%% From the abstract.
% Performance metrics that are linear fractional metrics of the confusion matrix.
% Binary classification.
% Optimal method is to threshold a plug-in estimator.
% Two algorithms, statistical consistency.
%% From the paper
% It was previously known that \eta(x) - \delta was the optimal decision threshold for F1 score
% Unifies prior analysis on AM measure (Menon et al.) and F\beta measure by Ye et al.
% Consistency with respect to the optimal metric, i.e., converging to best F1 or whatever else.
% Algorithms essentially try to select the optimal threshold.
% Scott


@article{fathony2019genericMetrics,
  title={{AP}-Perf: Incorporating Generic Performance Metrics in Differentiable Learning},
  author={Fathony, Rizal and Kolter, J Zico},
  journal={arXiv preprint arXiv:1912.00965},
  year={2019}
}
% Adversarial prediction framework to optimize metrics in a differentiable pipeline. Examples include precision, recall, F1, etc.
% Goal of the paper is essentially to replace cross-entropy loss with something better.
% The consistency result is that the method is Fisher consistent, which is a very weak notion.

%------------------------------%
% Classification Applications
%------------------------------%

@article{khan2001classification,
  title={Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks},
  author={Khan, Javed and Wei, Jun S and Ringner, Markus and Saal, Lao H and Ladanyi, Marc and Westermann, Frank and Berthold, Frank and Schwab, Manfred and Antonescu, Cristina R and Peterson, Carsten and others},
  journal={Nature medicine},
  volume={7},
  number={6},
  pages={673},
  year={2001},
  publisher={Nature Publishing Group}
}
% Classify cancers based on gene expression signatures (round blue-cell tumors used here).

@article{lee2004cloud,
  title={Cloud classification of satellite radiance data by multicategory support vector machines},
  author={Lee, Yoonkyung and Wahba, Grace and Ackerman, Steven A},
  journal={Journal of Atmospheric and Oceanic Technology},
  volume={21},
  number={2},
  pages={159--169},
  year={2004}
}
% Application to cloud classification with satellite radiance profiles.

@article{lee2004multicategory,
  title={Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data},
  author={Lee, Yoonkyung and Lin, Yi and Wahba, Grace},
  journal={Journal of the American Statistical Association},
  volume={99},
  number={465},
  pages={67--81},
  year={2004},
  publisher={Taylor \& Francis}
}


@online{moon2012clinical,
  author = {Moon, Sungrim and Pakhomov, Serguei and Melton, Genevieve},
  title = {Clinical Abbreviation Sense Inventory},
  year = 2012,
  url = {http://purl.umn.edu/137703},
  urldate = {2019-12-05}
}
% UMN dataset.


@inproceedings{lin2018overview,
  title={{Overview of the SIGIR 2018 eCom Rakuten Data Challenge}},
  author={Lin, Yiu-Chang and Das, Pradipto and Datta, Ankur},
  booktitle={eCOM@ SIGIR},
  year={2018}
}
% 1 million products; 800,000 in training and 200,000 in test. Like Amazon of Japan.
% Predict category, defined as full path in taxonomy tree.
% 800,000 titles, 3,008 leaf nodes. Uneven distribution. Top 40 categories ~ 50% of data.
% Some high-level description of submissions.
% Still don't know what precision, recall, and F1 scores are.


%------------------------------%
% Imbalanced Classification
%------------------------------%
@article{chawla2002smote,
  title={{SMOTE}: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of Artificial Intelligence Research},
  volume={16},
  pages={321--357},
  year={2002}
}
% Previous approaches: under-sample majority, over-sample minority.
% Here, the over-sampling of the minority occurs by interpolating minority class points (in feature space).


@inproceedings{domingos1999metacost,
  title={Metacost: A general method for making classifiers cost-sensitive},
  author={Domingos, Pedro},
  booktitle={KDD},
  volume={99},
  pages={155--164},
  year={1999}
}
% Metacost: a wrapper on classifiers, or something like that.
% Cost-sensitive classification. Cost C(i, j) for guessing i when the true class is j. 
% All empirical.


@article{mariani2018bagan,
  title={Bagan: Data augmentation with balancing {GAN}},
  author={Mariani, Giovanni and Scheidegger, Florian and Istrate, Roxana and Bekas, Costas and Malossi, Cristiano},
  journal={arXiv preprint arXiv:1803.09655},
  year={2018}
}
% Use GANs to create more minority class images.

@incollection{pazzani1994reducing,
  title={Reducing misclassification costs},
  author={Pazzani, Michael and Merz, Christopher and Murphy, Patrick and Ali, Kamal and Hume, Timothy and Brunk, Clifford},
  booktitle={Machine Learning Proceedings 1994},
  pages={217--225},
  year={1994},
  publisher={Elsevier}
}


@article{zhou2006training,
  title={Training cost-sensitive neural networks with methods addressing the class imbalance problem},
  author={Zhou, Zhi-Hua and Liu, Xu-Ying},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={18},
  number={1},
  pages={63--77},
  year={2006},
  publisher={IEEE}
}
% Over-sampling, under-sampling, threshold moving.
% Uses neural networks.
% Mostly just empirical studies.
% Works okay on 2 classes but poorly on multiclass.

%------------------------------%
% Computational Extreme Classification
%------------------------------%
@article{joulin2016bag,
  title={Bag of tricks for efficient text classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}
% Linear models are faster. 
% Use a linear classifier that has two parts, i.e. BAx rather than Ax. The Ax gives the word representation, and then B maps that to the output space. Seems like a sparsity and/or rank constraint thing.
% Minimize the negative log-likelihood -\frac{1}{N}\sum_{n = 1}}^{N} y_{n} \log(f(BAx_{n})).
% Hierarchical softmax for many features. Drops traning time from O(kh) to O(h\log(k)), where k is the number of classes and h is the dimension of the hidden layer/representation/latent features. How to get a good tree though? Unclear. Why not use a decision tree or random forest type thing?
% Use n-grams instead of just bag of words to get better performance. Bigrams and trigrams in practice.

@inproceedings{yen2016pd,
  title={Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification},
  author={Yen, Ian En-Hsu and Huang, Xiangru and Ravikumar, Pradeep and Zhong, Kai and Dhillon, Inderjit},
  booktitle={International Conference on Machine Learning},
  pages={3069--3077},
  year={2016}
}
% Use an \ell_{1} penalty to encourage sparsity in some way; propose and algorithm to do this efficiently (based on Frank-Wolfe).
% Previously: embeddings into low-dimensional space (mostly low-rank); trees.
% Separation ranking loss. For multiclass classification, just the SVM. We're doing linear classification anyways.
% Mostly an optimization paper; no statistics.

@inproceedings{yen2017ppdsparse,
  title={Ppdsparse: A parallel primal-dual sparse method for extreme classification},
  author={Yen, Ian EH and Huang, Xiangru and Dai, Wei and Ravikumar, Pradeep and Dhillon, Inderjit and Xing, Eric},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={545--553},
  year={2017},
  organization={ACM}
}
% Use a separable hinge loss instead of the usual hinge loss; show that it still has enough sparsity; parallelize.
% Really an optimization-ty@article{blake1998uci,
	title={UCI repository of machine learning datasets},
	author={Blake, C and Merz, CJ},
	journal={University of California, Irvine, Dept. of Information and Computer Sciences},
	year={1998}
}pe paper; not statistical.

@inproceedings{yen2018loss,
  title={Loss decomposition for fast learning in large output spaces},
  author={Yen, Ian En-Hsu and Kale, Satyen and Yu, Felix and Holtmann-Rice, Daniel and Kumar, Sanjiv and Ravikumar, Pradeep},
  booktitle={International Conference on Machine Learning},
  pages={5626--5635},
  year={2018}
}
% Previously: data structures for problem. Maximum Inner Product Search (MIPS), Nearest Neighbor Search (NNS).
% Use MIPS plus a decomposition of the loss function to parallelize.


%------------------------------%
% Neyman-Pearson Classification
%------------------------------%
@article{rigollet2011neyman,
  title={{N}eyman-{P}earson classification, convexity and stochastic constraints},
  author={Rigollet, Philippe and Tong, Xin},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Oct},
  pages={2831--2855},
  year={2011}
}
% Cannon et al. 2002 began studying NP classification.
% Look at convexification. Bound convex surrogate excess risk. Is the convex class assumption key? Yes! This convex combination thing permeates all their results via M, and it's quite a strong assumption. Okay for linear, but not for neural networks, nn, etc.
% Stupid Theorem 7 shows a surrogate risk upper bounds the regular risk.
% Contrived example involving 0/1 loss showing that excess risk for a more conservative risk is bounded below by alpha (Proposition 8).
% Something something chance-constrained optimization. Okay, so here they just do chance-constrained optimization, but with some of the assumptions for their setting.

@article{tong2013plug,
  title={A plug-in approach to {N}eyman-{P}earson classification},
  author={Tong, Xin},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={3011--3040},
  year={2013},
  publisher={JMLR. org}
}



@article{tong2016survey,
  title={A survey on {N}eyman-{P}earson classification and suggestions for future research},
  author={Tong, Xin and Feng, Yang and Zhao, Anqi},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={8},
  number={2},
  pages={64--81},
  year={2016},
  publisher={Wiley Online Library}
}
% Survey on Neyman-Pearson classification, i.e. fixing an alpha level for misclassification for a given class that we do not wish to violate and then optimizing the misclassification rate of the other class (in binary classification).
% Leverages a lot of other domains, e.g. statistical learning (VC theory), non-parametric classification (Audibert & Tsybakov), etc.
% Doesn't seem fundamental.

%------------------------------%
% Distributional Robustness
%------------------------------%
@inproceedings{namkoong2017variance,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2971--2980},
  year={2017}
}
% Study distributionally robust optimization with the Chi^2-divergence.
% Mostly do some learning theory on the resulting robust ERM problem. Shows it beats ERM in some cases; loses in others. Lots of examples.
% Also some simulations.

@article{duchi2018mixture,
  title={Distributionally Robust Losses Against Mixture Covariate Shifts},
  author={Duchi, John C and Hashimoto, Tatsunori and Namkoong, Hongseok},
  journal={Arxiv},
  year={2018}
}
% This paper has some CVaR stuff.
% Emphasizes relations to causal inference, distributional robustness, and fairness. No explicit results for fairness though.
% Idea is to control sup over marginal risks to get good subpopulation performance. The main duality tool is the use of the dual form of CVaR. Actually, the DRO problem is already the dual version, but it is made tractable (convex, etc.) by converting back to the original formulation.
% Also a lot on controlling marginal distribution versus joint, because joint is more conservative.
% Does not seem to immediately give any insight for human-aligned decision making.

@article{duchi2018learning,
  title={Learning Models with Uniform Performance via Distributionally Robust Optimization},
  author={Duchi, John and Namkoong, Hongseok},
  journal={arXiv preprint arXiv:1810.08750},
  year={2018}
}
% Another long paper. Motivation is to provide good sub-population performance (e.g. race, gender), but that's not actually studied directly.
% Lots of simulations, but honestly more of the same.
% Explicitly look at the Cressie-Read family of divergences. Considers minimax upper and lower bounds, although the analysis is not tight. Also some consistency and asymptotic normality results. 
% CVaR is an example in this paper. Corresponds to a particular choice of f-divergence where k = \infty. Need to double check Shapiro example 6.19.

%------------------------------%
% Probability
%------------------------------%
@article{ledoux1989,
  title={Comparison theorems, random geometry and some limit theorems for empirical processes},
  author={Ledoux, M and Talagrand, M},
  journal={The Annals of Probability},
  pages={596--631},
  year={1989},
  publisher={JSTOR}
}


@book{ledoux1991,
  title={Probability in Banach Spaces: Isoperimetry and Processes},
  author={Ledoux, Michel and Talagrand, Michel},
  year={1991},
  publisher={Springer Science \& Business Media}
}

@book{boucheron2013,
  title={Concentration Inequalities: A Nonasymptotic Theory of Independence},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year={2013},
  publisher={Oxford University Press}
}

@article{slud1977distribution,
	title={Distribution inequalities for the binomial law},
	author={Slud, Eric V},
	journal={The Annals of Probability},
	pages={404--412},
	year={1977},
	publisher={JSTOR}
}

%------------------------------%
% Neural Networks
%------------------------------%

@ARTICLE{bartlett1998, 
author={P. L. Bartlett}, 
journal={IEEE Transactions on Information Theory}, 
title={The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network}, 
year={1998}, 
volume={44}, 
number={2}, 
pages={525-536}, 
doi={10.1109/18.661502}, 
ISSN={0018-9448}, 
month={March},}
% Two layer neural network generalization error via sizes of weights.

@inproceedings{bartlett2017,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6240--6249},
  year={2017}
}
@book{goodfellow2016,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press Cambridge}
}


@inproceedings{golowich2018,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018}
}

@inproceedings{neyshabur2015,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}
% Bounds on Rademacher complexity, exponential in depth.


%------------------------------%
% Optimization/Robustness Generally
%------------------------------%



@article{esfahani2015,
  title={Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations},
  author={Esfahani, Peyman Mohajerin and Kuhn, Daniel},
  journal={Mathematical Programming},
  pages={1--52},
  year={2015},
  publisher={Springer}
}

@article{gao2017,
  title={Distributional Robustness and Regularization in Statistical Learning},
  author={Gao, Rui and Chen, Xi and Kleywegt, Anton J},
  journal={arXiv preprint arXiv:1712.06050},
  year={2017}
}


@article{trafalis2007,
  title={Robust support vector machines for classification and computational issues},
  author={Trafalis, Theodore B and Gilbert, Robin C},
  journal={Optimisation Methods and Software},
  volume={22},
  number={1},
  pages={187--198},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{xu2009lasso,
  title={Robust regression and {L}asso},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1801--1808},
  year={2009}
}

@article{xu2009svm,
  title={Robustness and regularization of support vector machines},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Jul},
  pages={1485--1510},
  year={2009}
}
%------------------------------%
% Adversarial Robustness
%------------------------------%
@inproceedings{athalye2018,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {July},
  publisher = 	 {PMLR}
}

@article{bubeck2018,
  title={Adversarial examples from computational constraints},
  author={Bubeck, S{\'e}bastien and Price, Eric and Razenshteyn, Ilya},
  journal={arXiv preprint arXiv:1805.10204},
  year={2018}
}

@inproceedings{carlini2017,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Security and Privacy (SP), 2017 IEEE Symposium on},
  pages={39--57},
  year={2017},
  organization={IEEE}
}


@article{fawzi2018,
  title={Analysis of classifiers’ robustness to adversarial perturbations},
  author={Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  journal={Machine Learning},
  volume={107},
  number={3},
  pages={481--508},
  year={2018},
  publisher={Springer}
}

@article{goodfellow2014,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{ilyas2018,
  title={Black-box Adversarial Attacks with Limited Queries and Information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  journal={arXiv preprint arXiv:1804.08598},
  year={2018}
}

@article{schmidt2018,
  title={Adversarially Robust Generalization Requires More Data},
  author={Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and M{\k{a}}dry, Aleksander},
  journal={arXiv preprint arXiv:1804.11285},
  year={2018}
}
% Upper and lower bounds for sample size required to robustly learn in two contrived cases (opposite mean Gaussian mixture, opposite mean binary mixture). Upper bounds are for linear classifiers (usually the mean).

@InProceedings{wong2018,
  title = 	 {Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope},
  author = 	 {Wong, Eric and Kolter, Zico},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5286--5295},
  year = 	 {2018},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {July},
  publisher = 	 {PMLR}
}
% Convex outer polytope. Basically construct an adversarially polytope of all possible neural network outputs. Then, optimize over this using some convex duality argument. Does not consider generalization.


@inproceedings{madry2018,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJzIBfZAb},
}

@inproceedings{raghunathan2018,
title={Certified Defenses against Adversarial Examples },
author={Aditi Raghunathan and Jacob Steinhardt and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Bys4ob-Rb},
}
% Provides an upper bound on the adversarial loss via semidefinite programming for neural networks with one hidden layer. Does not provide generalization guarantees.

@inproceedings{papernot2016,
  title={The limitations of deep learning in adversarial settings},
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Security and Privacy (EuroS\&P), 2016 IEEE European Symposium on},
  pages={372--387},
  year={2016},
  organization={IEEE}
}

@article{shafahi2018,
  title={Are adversarial examples inevitable?},
  author={Shafahi, Ali and Huang, W Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
  journal={arXiv preprint arXiv:1809.02104},
  year={2018}
}
% Looks at adversarial examples as a phenomenon arising from isoperimetry. Shows that they may be unavoidable in some cases. Assumes a generative model.

@article{suggala2018,
  title={On Adversarial Risk and Training},
  author={Suggala, Arun Sai and Prasad, Adarsh and Nagarajan, Vaishnavh and Ravikumar, Pradeep},
  journal={arXiv preprint arXiv:1806.02924},
  year={2018}
}
% Examines adversarial risk. Sees that it provides regularization. In some cases, one can optimize usual and adversarial risk simulatenously. Does some analysis with mixtures of normals.

@article{szegedy2013,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{taran2018,
  title={Bridging machine learning and cryptography in defence against adversarial attacks},
  author={Taran, Olga and Rezaeifar, Shideh and Voloshynovskiy, Slava},
  journal={arXiv preprint arXiv:1809.01715},
  year={2018}
}
% Just a discussion and defense of gray-box analysis. Provides some simulations with one algorithm. Not wonderful.

@article{xu2018,
  title={Robust {GAN}s against Dishonest Adversaries},
  author={Xu, Zhi and Li, Chengtao and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1802.09700},
  year={2018}
}


@article{yin2018,
  title={Rademacher Complexity for Adversarially Robust Generalization},
  author={Yin, Dong and Ramchandran, Kannan and Bartlett, Peter},
  journal={arXiv preprint arXiv:1810.11914},
  year={2018}
}

%------------------------------%
% Distributional Robustness
%------------------------------%

@article{BlaKan17,
  title={Semi-supervised Learning based on Distributionally Robust Optimization},
  author={Blanchet, Jose and Kang, Yang},
  journal={arXiv preprint arXiv:1702.08848},
  year={2017}
}



@article{cranko2018,
  title={Lipschitz Networks and Distributional Robustness},
  author={Cranko, Zac and Kornblith, Simon and Shi, Zhan and Nock, Richard},
  journal={arXiv preprint arXiv:1809.01129},
  year={2018}
}
% Treats distributional robustness with neural networks as a Lipschitz problem, ie wants to look at the layer norms. Doesn't make much progress.
% Does know that adversarial risk is bounded by Wasserstein risk.

@inproceedings{namkoong2017,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2971--2980},
  year={2017}
}

@inproceedings{namkoong2016,
  title={Stochastic gradient methods for distributionally robust optimization with $f$-divergences},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2208--2216},
  year={2016}
}

@inproceedings{sinha2018,
title={Certifiable Distributional Robustness with Principled Adversarial Training},
author={Aman Sinha and Hongseok Namkoong and John Duchi},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hk6kPgZA-},
}

%------------------------------%
% Fairness
%------------------------------%

@article{kearns2017,
  title={Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author={Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  journal={arXiv preprint arXiv:1711.05144},
  year={2017}
}

%------------------------------%
% Standard Machine Learning
%------------------------------%

@inproceedings{kuznetsov2015,
  title={Rademacher complexity margin bounds for learning with a large number of classes},
  author={Kuznetsov, Vitaly and Mohri, Mehryar and Syed, U},
  booktitle={ICML Workshop on Extreme Classification: Learning with a Very Large Number of Labels},
  year={2015}
}

@book{mohri2018foundations,
  title={Foundations of Machine Learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

%------------------------------%
% Other Books
%------------------------------%
@book{boyd2004,
  title={Convex {O}ptimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge University Press}
}

@article{godsil2001,
  title={Algebraic {G}raph {T}heory},
  author={Godsil, Chris and Royle, Gordon},
  journal={Graduate Texts in Mathematics, Springer, New York},
  year={2001}
}

%------------------------------%
% Shashank's Refs
%------------------------------%
@inproceedings{tagami2017annexml,
  title={AnnexML: Approximate nearest neighbor search for extreme multi-label classification},
  author={Tagami, Yukihiro},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={455--464},
  year={2017},
  organization={ACM}
}

@inproceedings{bhatia2015sparse,
  title={Sparse local embeddings for extreme multi-label classification},
  author={Bhatia, Kush and Jain, Himanshu and Kar, Purushottam and Varma, Manik and Jain, Prateek},
  booktitle={Advances in Neural Information Processing Systems},
  pages={730--738},
  year={2015}
}



@article{dong2019scalable,
  title={Scalable nearest neighbor search for optimal transport},
  author={Dong, Yihe and Indyk, Piotr and Razenshteyn, Ilya and Wagner, Tal},
  journal={arXiv preprint arXiv:1910.04126},
  year={2019}
}

@inproceedings{andoni2006near,
  title={Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions},
  author={Andoni, Alexandr and Indyk, Piotr},
  booktitle={2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)},
  pages={459--468},
  year={2006},
  organization={IEEE}
}

@article{andoni2018approximate,
  title={Approximate nearest neighbor search in high dimensions},
  author={Andoni, Alexandr and Indyk, Piotr and Razenshteyn, Ilya},
  journal={arXiv preprint arXiv:1806.09823},
  year={2018}
}

@article{indyk2018approximate,
  title={Approximate nearest neighbors in limited space},
  author={Indyk, Piotr and Wagner, Tal},
  journal={arXiv preprint arXiv:1807.00112},
  year={2018}
}


@book{tsybakov2009introduction,
  title={Introduction to Nonparametric Estimation. Revised and extended from the 2004 French original. Translated by Vladimir Zaiats},
  author={Tsybakov, Alexandre B},
  year={2009},
  publisher={Springer Series in Statistics. Springer, New York}
}
%------------%
% Datasets
%------------%

@article{blackard1999comparative,
	title={Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables},
	author={Blackard, Jock A and Dean, Denis J},
	journal={Computers and electronics in agriculture},
	volume={24},
	number={3},
	pages={131--151},
	year={1999},
	publisher={Elsevier}
}

@article{zhao2013beyond,
  title={Beyond Fano's inequality: Bounds on the optimal F-score, BER, and cost-sensitive risk and their implications},
  author={Zhao, Ming-Jie and Edakunni, Narayanan and Pocock, Adam and Brown, Gavin},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={1033--1090},
  year={2013},
  publisher={JMLR. org}
}

@book{szeliski2010computer,
  title={Computer vision: algorithms and applications},
  author={Szeliski, Richard},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@inproceedings{goutte2005probabilistic,
  title={A probabilistic interpretation of precision, recall and F-score, with implication for evaluation},
  author={Goutte, Cyril and Gaussier, Eric},
  booktitle={European conference on information retrieval},
  pages={345--359},
  year={2005},
  organization={Springer}
}

@article{cortes2004auc,
  title={{AUC} optimization vs. error rate minimization},
  author={Cortes, Corinna and Mohri, Mehryar},
  journal={Advances in Neural Information Processing Systems},
  volume={16},
  number={16},
  pages={313--320},
  year={2004},
  publisher={MIT Press}
}

@book{he2013imbalanced,
  title={Imbalanced Learning: Foundations, Algorithms, and Applications},
  author={He, Haibo and Ma, Yunqian},
  year={2013},
  publisher={John Wiley \& Sons}
}

@article{scholkopf2019causality,
  title={Causality for machine learning},
  author={Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1911.10500},
  year={2019}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}

@article{cotter2019making,
  title={On making stochastic classifiers deterministic},
  author={Cotter, Andrew and Narasimhan, Harikrishna and Gupta, Maya R},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@book{mitchell1997machine,
  title={Machine Learning},
  author={Mitchell, Tom M},
  year={1997},
  publisher={McGraw-hill New York}
}

@book{james2013introduction,
  title={An Introduction to Statistical Learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}

@article{krawczyk2016learning,
  title={Learning from imbalanced data: open challenges and future directions},
  author={Krawczyk, Bartosz},
  journal={Progress in Artificial Intelligence},
  volume={5},
  number={4},
  pages={221--232},
  year={2016},
  publisher={Springer}
}

@article{arlot2011margin,
  title={Margin-adaptive model selection in statistical learning},
  author={Arlot, Sylvain and Bartlett, Peter L},
  journal={Bernoulli},
  volume={17},
  number={2},
  pages={687--713},
  year={2011},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@book{lehmann2006testing,
  title={Testing Statistical Hypotheses},
  author={Lehmann, Erich L and Romano, Joseph P},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{bennett1962probability,
  title={Probability inequalities for the sum of independent random variables},
  author={Bennett, George},
  journal={Journal of the American Statistical Association},
  volume={57},
  number={297},
  pages={33--45},
  year={1962},
  publisher={Taylor \& Francis Group}
}

@inproceedings{bousquet2003introduction,
  title={Introduction to statistical learning theory},
  author={Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi, G{\'a}bor},
  booktitle={Summer School on Machine Learning},
  pages={169--207},
  year={2003},
  organization={Springer}
}

@incollection{mcdiarmid1998concentration,
  title={Concentration},
  author={McDiarmid, Colin},
  booktitle={Probabilistic Methods for Algorithmic Discrete Mathematics},
  pages={195--248},
  year={1998},
  publisher={Springer}
}

@incollection{vapnik2015uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, A Ya},
  booktitle={Measures of Complexity},
  pages={11--30},
  year={2015},
  publisher={Springer}
}

@article{wang2019consistent,
  title={Consistent classification with generalized metrics},
  author={Wang, Xiaoyan and Li, Ran and Yan, Bowei and Koyejo, Oluwasanmi},
  journal={arXiv preprint arXiv:1908.09057},
  year={2019}
}

@inproceedings{yan2018binary,
  title={Binary classification with karmic, threshold-quasi-concave metrics},
  author={Yan, Bowei and Koyejo, Sanmi and Zhong, Kai and Ravikumar, Pradeep},
  booktitle={International Conference on Machine Learning},
  pages={5531--5540},
  year={2018},
  organization={PMLR}
}

@inproceedings{dembczynski2017consistency,
  title={Consistency analysis for binary classification revisited},
  author={Dembczy{\'n}ski, Krzysztof and Kot{\l}owski, Wojciech and Koyejo, Oluwasanmi and Natarajan, Nagarajan},
  booktitle={International Conference on Machine Learning},
  pages={961--969},
  year={2017},
  organization={PMLR}
}

@article{noe2020machine,
  title={Machine learning for protein folding and dynamics},
  author={No{\'e}, Frank and De Fabritiis, Gianni and Clementi, Cecilia},
  journal={Current Opinion in Structural Biology},
  volume={60},
  pages={77--84},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{narasimhan2015consistent,
  title={Consistent multiclass algorithms for complex performance measures},
  author={Narasimhan, Harikrishna and Ramaswamy, Harish and Saha, Aadirupa and Agarwal, Shivani},
  booktitle={International Conference on Machine Learning},
  pages={2398--2407},
  year={2015},
  organization={PMLR}
}

@article{schaefer2020use,
  title={The use of machine learning in rare diseases: a scoping review},
  author={Schaefer, Julia and Lehne, Moritz and Schepers, Josef and Prasser, Fabian and Thun, Sylvia},
  journal={Orphanet Journal of Rare Diseases},
  volume={15},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Springer}
}

@inproceedings{awoyemi2017credit,
  title={Credit card fraud detection using machine learning techniques: A comparative analysis},
  author={Awoyemi, John O and Adetunmbi, Adebayo O and Oluwadare, Samuel A},
  booktitle={2017 International Conference on Computing Networking and Informatics (ICCNI)},
  pages={1--9},
  year={2017},
  organization={IEEE}
}

@inproceedings{wang2020logistic,
  title={Logistic Regression for Massive Data with Rare Events},
  author={Wang, HaiYing},
  booktitle={International Conference on Machine Learning},
  pages={9829--9836},
  year={2020},
  organization={PMLR}
}

@article{gottlieb2014efficient,
  title={Efficient classification for metric data},
  author={Gottlieb, Lee-Ad and Kontorovich, Aryeh and Krauthgamer, Robert},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={9},
  pages={5750--5759},
  year={2014},
  publisher={IEEE}
}

@inproceedings{kontorovich2015bayes,
  title={A {B}ayes consistent 1-{NN} classifier},
  author={Kontorovich, Aryeh and Weiss, Roi},
  booktitle={Artificial Intelligence and Statistics},
  pages={480--488},
  year={2015},
  organization={PMLR}
}

@inproceedings{hanneke2020universal,
  title={Universal {B}ayes consistency in metric spaces},
  author={Hanneke, Steve and Kontorovich, Aryeh and Sabato, Sivan and Weiss, Roi},
  booktitle={2020 Information Theory and Applications Workshop (ITA)},
  pages={1--33},
  year={2020},
  organization={IEEE}
}

@book{billingsley2008probability,
  title={Probability and measure},
  author={Billingsley, Patrick},
  year={2008},
  publisher={John Wiley \& Sons}
}

@inproceedings{flach2003geometry,
  title={The geometry of {ROC} space: understanding machine learning metrics through ROC isometrics},
  author={Flach, Peter A},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={194--201},
  year={2003}
}

@inproceedings{wu2022metric,
  title={Metric-Fair Classifier Derandomization},
  author={Wu, Jimmy and Chen, Yatong and Liu, Yang},
  booktitle={International Conference on Machine Learning},
  pages={23999--24016},
  year={2022},
  organization={PMLR}
}

@article{pinot2022robustness,
  title={On the robustness of randomized classifiers to adversarial examples},
  author={Pinot, Rafael and Meunier, Laurent and Yger, Florian and Gouy-Pailler, C{\'e}dric and Chevaleyre, Yann and Atif, Jamal},
  journal={Machine Learning},
  pages={1--33},
  year={2022},
  publisher={Springer}
}

@inproceedings{lu2020stochastic,
  title={Stochastic classifiers for unsupervised domain adaptation},
  author={Lu, Zhihe and Yang, Yongxin and Zhu, Xiatian and Liu, Cong and Song, Yi-Zhe and Xiang, Tao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9111--9120},
  year={2020}
}

@inproceedings{cotter2019two,
  title={Two-player games for efficient non-convex constrained optimization},
  author={Cotter, Andrew and Jiang, Heinrich and Sridharan, Karthik},
  booktitle={Algorithmic Learning Theory},
  pages={300--332},
  year={2019},
  organization={PMLR}
}

@inproceedings{lipton2014optimal,
  title={Optimal thresholding of classifiers to maximize F1 measure},
  author={Lipton, Zachary C and Elkan, Charles and Naryanaswamy, Balakrishnan},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={225--239},
  year={2014},
  organization={Springer}
}

@article{hernandez2013roc,
  title={ROC curves in cost space},
  author={Hern{\'a}ndez-Orallo, Jos{\'e} and Flach, Peter and Ferri, C{\'e}sar},
  journal={Machine learning},
  volume={93},
  number={1},
  pages={71--91},
  year={2013},
  publisher={Springer}
}

@incollection{flach2016classifier,
  title={Classifier calibration},
  author={Flach, Peter A},
  booktitle={Encyclopedia of Machine Learning and Data Mining},
  year={2016},
  publisher={Springer US}
}