


@article{liu2023seeing,
  title={Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability},
  author={Liu, Ziming and Gan, Eric and Tegmark, Max},
  journal={arXiv preprint arXiv:2305.08746},
  year={2023}
}

@article{schaeffer2023emergent,
  title={Are Emergent Abilities of Large Language Models a Mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}


@article{
wei2022emergent,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=yzkSU5zdwD},
note={Survey Certification}
}


@article{davies2021advancing,
  title={Advancing mathematics by guiding human intuition with AI},
  author={Davies, Alex and Veli{\v{c}}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v{s}}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and others},
  journal={Nature},
  volume={600},
  number={7887},
  pages={70--74},
  year={2021},
  publisher={Nature Publishing Group}
}


@article{gukov2021learning,
  title={Learning to unknot},
  author={Gukov, Sergei and Halverson, James and Ruehle, Fabian and Su{\l}kowski, Piotr},
  journal={Machine Learning: Science and Technology},
  volume={2},
  number={2},
  pages={025035},
  year={2021},
  publisher={IOP Publishing}
}

@article{he2021machine,
  title={Machine-learning mathematical structures},
  author={He, Yang-Hui},
  journal={International Journal of Data Science in the Mathematical Sciences},
  volume={1},
  number={01},
  pages={23--47},
  year={2023},
  publisher={World Scientific}
}



@article{barak2022hidden,
  title={Hidden progress in deep learning: S{G}{D} learns parities near the computational limit},
  author={Barak, Boaz and Edelman, Benjamin and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21750--21764},
  year={2022}
}


@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}


@article{kim2020integration,
  title={Integration of neural network-based symbolic regression in deep learning for scientific discovery},
  author={Kim, Samuel and Lu, Peter Y and Mukherjee, Srijon and Gilbert, Michael and Jing, Li and {\v{C}}eperi{\'c}, Vladimir and Solja{\v{c}}i{\'c}, Marin},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  number={9},
  pages={4166--4177},
  year={2020},
  publisher={IEEE}
}


@inproceedings{hoshen2016visual,
  title={Visual learning of arithmetic operation},
  author={Hoshen, Yedid and Peleg, Shmuel},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2016}
}


@inproceedings{lubana2022mechanistic,
  title={Mechanistic mode connectivity},
  author={Lubana, Ekdeep Singh and Bigelow, Eric J and Dick, Robert P and Krueger, David and Tanaka, Hidenori},
  booktitle={International Conference on Machine Learning},
  pages={22965--23004},
  year={2023},
  organization={PMLR}
}


@book{saitta2011phase,
  title={Phase transitions in machine learning},
  author={Saitta, Lorenza and Giordana, Attilio and Cornuejols, Antoine},
  year={2011},
  publisher={Cambridge University Press}
}


@inproceedings{
liu2023omnigrok,
title={Omnigrok: Grokking Beyond Algorithmic Data},
author={Ziming Liu and Eric J Michaud and Max Tegmark},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zDiHoIWa0q1}
}


@inproceedings{chughtai2023toy,
  title={A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations},
  author={Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  booktitle={
 The Fortieth International Conference on Machine Learning},
  year={2023}
}



@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread}
}


@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}





@article{michaud2023quantization,
  title={The Quantization Model of Neural Scaling},
  author={Michaud, Eric J and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  journal={arXiv preprint arXiv:2303.13506},
  year={2023}
}


@inproceedings{
akyurek2022learning,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0g0X4H8yN4I}
}


@book{hartmann2006phase,
  title={Phase transitions in combinatorial optimization problems: basics, algorithms and statistical mechanics},
  author={Hartmann, Alexander K and Weigt, Martin},
  year={2006},
  publisher={John Wiley \& Sons}
}


@article{conmy2023towards,
  title={Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine N and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={arXiv preprint arXiv:2304.14997},
  year={2023}
}


@article{gurnee2023finding,
  title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}


@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}


@inproceedings{
wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}


@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}


@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}


@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}


@inproceedings{
loshchilov2017decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@inproceedings{hassid-etal-2022-much,
    title = "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    author = "Hassid, Michael  and
      Peng, Hao  and
      Rotem, Daniel  and
      Kasai, Jungo  and
      Montero, Ivan  and
      Smith, Noah A.  and
      Schwartz, Roy",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.101",
    pages = "1403--1416",
    abstract = "The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones{---}the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance{---}an average relative drop of only 8{\%} from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@inproceedings{
frankle2018lottery,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}
