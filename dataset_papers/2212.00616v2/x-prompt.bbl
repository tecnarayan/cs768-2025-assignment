\begin{thebibliography}{38}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Ben-David et~al.(2021)Ben-David, Oved, and
  Reichart}]{BenDavid2021PADAEP}
Eyal Ben-David, Nadav Oved, and Roi Reichart. 2021.
\newblock Pada: Example-based prompt learning for on-the-fly adaptation to
  unseen domains.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:414--433.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Davison et~al.(2019)Davison, Feldman, and
  Rush}]{davison-etal-2019-commonsense}
Joe Davison, Joshua Feldman, and Alexander Rush. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1109} {Commonsense knowledge
  mining from pretrained models}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 1173--1178, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Gal et~al.(2022)Gal, Alaluf, Atzmon, Patashnik, Bermano, Chechik, and
  Cohen-or}]{gal2022image}
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or~Patashnik, Amit~Haim Bermano, Gal
  Chechik, and Daniel Cohen-or. 2022.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Gao et~al.(2021)Gao, Fisch, and Chen}]{gao-etal-2021-making}
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.295} {Making
  pre-trained language models better few-shot learners}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 3816--3830,
  Online. Association for Computational Linguistics.

\bibitem[{Ge et~al.(2018)Ge, Cui, Chang, Sui, Wei, and Zhou}]{ge2018eventwiki}
Tao Ge, Lei Cui, Baobao Chang, Zhifang Sui, Furu Wei, and Ming Zhou. 2018.
\newblock Eventwiki: a knowledge base of major events.
\newblock In \emph{Proceedings of the Eleventh International Conference on
  Language Resources and Evaluation (LREC 2018)}.

\bibitem[{Ge et~al.(2023)Ge, Hu, Wang, Wang, Chen, and Wei}]{ge2023incontext}
Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
\newblock \href {http://arxiv.org/abs/2307.06945} {In-context autoencoder for
  context compression in a large language model}.
\newblock \emph{arXiv preprint arXiv:2307.06945}.

\bibitem[{Gonen et~al.(2022)Gonen, Iyer, Blevins, Smith, and
  Zettlemoyer}]{gonen2022demystifying}
Hila Gonen, Srini Iyer, Terra Blevins, Noah~A Smith, and Luke Zettlemoyer.
  2022.
\newblock Demystifying prompts in language models via perplexity estimation.
\newblock \emph{arXiv preprint arXiv:2212.04037}.

\bibitem[{Han et~al.(2022)Han, Zhao, Ding, Liu, and Sun}]{han2022ptr}
Xu~Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2022.
\newblock Ptr: Prompt tuning with rules for text classification.
\newblock \emph{AI Open}, 3:182--192.

\bibitem[{Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi}]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 2019.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}.

\bibitem[{Hu et~al.(2021)Hu, Ding, Wang, Liu, Li, and
  Sun}]{hu2021knowledgeable}
Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun.
  2021.
\newblock Knowledgeable prompt-tuning: Incorporating knowledge into prompt
  verbalizer for text classification.
\newblock \emph{arXiv preprint arXiv:2108.02035}.

\bibitem[{Jiang et~al.(2019)Jiang, Xu, Araki, and Neubig}]{Jiang2019HowCW}
Zhengbao Jiang, Frank~F. Xu, J.~Araki, and Graham Neubig. 2019.
\newblock How can we know what language models know?
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:423--438.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}.

\bibitem[{Kumari et~al.(2022)Kumari, Zhang, Zhang, Shechtman, and
  Zhu}]{kumari2022multi}
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.
  2022.
\newblock Multi-concept customization of text-to-image diffusion.
\newblock \emph{arXiv preprint arXiv:2212.04488}.

\bibitem[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}.

\bibitem[{Lester et~al.(2022)Lester, Yurtsever, Shakeri, and
  Constant}]{lester2022reducing}
Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah Constant. 2022.
\newblock Reducing retraining by recycling parameter-efficient prompts.
\newblock \emph{arXiv preprint arXiv:2208.05577}.

\bibitem[{Li et~al.(2022)Li, Li, Ge, King, and Lyu}]{li2022text}
Jingjing Li, Zichao Li, Tao Ge, Irwin King, and Michael~R Lyu. 2022.
\newblock Text revision by on-the-fly representation optimization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 10956--10964.

\bibitem[{Li and Liang(2021)}]{li2021prefix}
Xiang~Lisa Li and Percy Liang. 2021.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}.

\bibitem[{Liu et~al.(2022)Liu, Kumar, Liang, and Jia}]{liu2022sample}
Nelson~F Liu, Ananya Kumar, Percy Liang, and Robin Jia. 2022.
\newblock Are sample-efficient nlp models more robust?
\newblock \emph{arXiv preprint arXiv:2210.06456}.

\bibitem[{Liu et~al.(2021)Liu, Zheng, Du, Ding, Qian, Yang, and
  Tang}]{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
  Jie Tang. 2021.
\newblock Gpt understands, too.
\newblock \emph{arXiv preprint arXiv:2103.10385}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
  2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:27730--27744.

\bibitem[{Qin and Eisner(2021)}]{qin2021learning}
Guanghui Qin and Jason Eisner. 2021.
\newblock Learning how to ask: Querying lms with mixtures of soft prompts.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 5203--5212.

\bibitem[{Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He}]{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--16. IEEE.

\bibitem[{Rao and Tetreault(2018)}]{rao2018dear}
Sudha Rao and Joel Tetreault. 2018.
\newblock Dear sir or madam, may i introduce the gyafc dataset: Corpus,
  benchmarks and metrics for formality style transfer.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 129--140.

\bibitem[{Schick and Sch{\"u}tze(2020)}]{schick2020exploiting}
Timo Schick and Hinrich Sch{\"u}tze. 2020.
\newblock Exploiting cloze questions for few shot text classification and
  natural language inference.
\newblock \emph{arXiv preprint arXiv:2001.07676}.

\bibitem[{Su et~al.(2021)Su, Wang, Qin, Chan, Lin, Wang, Wen, Liu, Li, Li
  et~al.}]{su2021transferability}
Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang,
  Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et~al. 2021.
\newblock On transferability of prompt tuning for natural language processing.
\newblock \emph{arXiv preprint arXiv:2111.06719}.

\bibitem[{Vu et~al.(2022)Vu, Lester, Constant, Al-Rfou, and Cer}]{vu2022spot}
Tu~Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2022.
\newblock Spot: Better frozen model adaptation through soft prompt transfer.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 5039--5059.

\bibitem[{Wang et~al.(2022)Wang, Ge, Mao, Li, Wei, and Chen}]{wang2022pay}
Xun Wang, Tao Ge, Allen Mao, Yuki Li, Furu Wei, and Si-Qing Chen. 2022.
\newblock Pay attention to your tone: Introducing a new dataset for polite
  language rewrite.
\newblock \emph{arXiv preprint arXiv:2212.10190}.

\bibitem[{Wang et~al.(2023)Wang, Mao, Wu, Ge, Wei, and Ji}]{wang2023unleashing}
Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji.
  2023.
\newblock Unleashing cognitive synergy in large language models: A task-solving
  agent through multi-persona self-collaboration.
\newblock \emph{arXiv preprint arXiv:2307.05300}.

\bibitem[{Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
  2022{\natexlab{a}}.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}.

\bibitem[{Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi,
  Le, Zhou et~al.}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al. 2022{\natexlab{b}}.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:24824--24837.

\bibitem[{Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor,
  Hendricks, Anderson, Kohli, Coppin, and Huang}]{welbl2021challenges}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang. 2021.
\newblock Challenges in detoxifying language models.
\newblock \emph{arXiv preprint arXiv:2109.07445}.

\bibitem[{Xu et~al.(2019)Xu, Ge, and Wei}]{xu2019formality}
Ruochen Xu, Tao Ge, and Furu Wei. 2019.
\newblock Formality style transfer with hybrid textual annotations.
\newblock \emph{arXiv preprint arXiv:1903.06353}.

\bibitem[{Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and
  Cao}]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao. 2022.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}.

\bibitem[{Zaporojets et~al.(2022)Zaporojets, Kaffee, Deleu, Demeester,
  Develder, and Augenstein}]{zaporojets2022tempel}
Klim Zaporojets, Lucie-Aim{\'e}e Kaffee, Johannes Deleu, Thomas Demeester,
  Chris Develder, and Isabelle Augenstein. 2022.
\newblock Tempel: Linking dynamically evolving and newly emerging entities.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:1850--1866.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin et~al.}]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}.

\bibitem[{Zhang et~al.(2020)Zhang, Ge, and Sun}]{zhang2020parallel}
Yi~Zhang, Tao Ge, and Xu~Sun. 2020.
\newblock Parallel data augmentation for formality style transfer.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 3221--3228.

\end{thebibliography}
