\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida, J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.
\newblock {GPT-4} technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.

\bibitem{almazrouei2023falcon}
E.~Almazrouei, H.~Alobeidli, A.~Alshamsi, A.~Cappelli, R.~Cojocaru, M.~Debbah, {\'E}.~Goffinet, D.~Hesslow, J.~Launay, Q.~Malartic, et~al.
\newblock The falcon series of open language models.
\newblock {\em arXiv preprint arXiv:2311.16867}, 2023.

\bibitem{bajaj2018ms}
P.~Bajaj, D.~Campos, N.~Craswell, L.~Deng, J.~Gao, X.~Liu, R.~Majumder, A.~McNamara, B.~Mitra, T.~Nguyen, M.~Rosenberg, X.~Song, A.~Stoica, S.~Tiwary, and T.~Wang.
\newblock {MS MARCO}: A human generated machine reading comprehension dataset, 2018.

\bibitem{brave24}
{Brave Software}.
\newblock Brave {S}earch {API}.

\bibitem{chen2023benchmarking}
J.~Chen, H.~Lin, X.~Han, and L.~Sun.
\newblock Benchmarking large language models in retrieval-augmented generation.
\newblock {\em arXiv preprint arXiv:2309.01431}, 2023.

\bibitem{chen-etal-2022-murag}
W.~Chen, H.~Hu, X.~Chen, P.~Verga, and W.~Cohen.
\newblock {M}u{RAG}: Multimodal retrieval-augmented generator for open question answering over images and text.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, Dec. 2022.

\bibitem{symphony}
Z.~Chen, Z.~Gu, L.~Cao, J.~Fan, S.~Madden, and N.~Tang.
\newblock Symphony: Towards natural language query answering over multi-modal data lakes.
\newblock In {\em {CIDR}}, 2023.

\bibitem{chung2024scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, Y.~Li, X.~Wang, M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em Journal of Machine Learning Research}, 25(70):1--53, 2024.

\bibitem{xin2024journey}
X.~L. Dong.
\newblock The journey to a knowledgeable assistant with retrieval-augmented generation (rag).
\newblock In {\em Companion of the 2024 International Conference on Management of Data}, SIGMOD/PODS '24, page~3, New York, NY, USA, 2024. Association for Computing Machinery.

\bibitem{gao2024llm}
M.~Gao, X.~Hu, J.~Ruan, X.~Pu, and X.~Wan.
\newblock Llm-based nlg evaluation: Current status and challenges.
\newblock {\em arXiv preprint arXiv:2402.01383}, 2024.

\bibitem{gao2024ragsurvey}
Y.~Gao, Y.~Xiong, X.~Gao, K.~Jia, J.~Pan, Y.~Bi, Y.~Dai, J.~Sun, Q.~Guo, M.~Wang, and H.~Wang.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock 2024.

\bibitem{huang2023survey}
L.~Huang, W.~Yu, W.~Ma, W.~Zhong, Z.~Feng, H.~Wang, Q.~Chen, W.~Peng, X.~Feng, B.~Qin, et~al.
\newblock A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.
\newblock {\em arXiv preprint arXiv:2311.05232}, 2023.

\bibitem{10.1145/3571730}
Z.~Ji, N.~Lee, R.~Frieske, T.~Yu, D.~Su, Y.~Xu, E.~Ishii, Y.~J. Bang, A.~Madotto, and P.~Fung.
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Comput. Surv.}, 55(12), mar 2023.

\bibitem{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~d.~l. Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{joshi-etal-2017-triviaqa}
M.~Joshi, E.~Choi, D.~Weld, and L.~Zettlemoyer.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock Association for Computational Linguistics, July 2017.

\bibitem{kandpal2023large}
N.~Kandpal, H.~Deng, A.~Roberts, E.~Wallace, and C.~Raffel.
\newblock Large language models struggle to learn long-tail knowledge.
\newblock In {\em International Conference on Machine Learning}, pages 15696--15707. PMLR, 2023.

\bibitem{kwiatkowski2019natural}
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~Parikh, C.~Alberti, D.~Epstein, I.~Polosukhin, M.~Kelcey, J.~Devlin, K.~Lee, K.~N. Toutanova, L.~Jones, M.-W. Chang, A.~Dai, J.~Uszkoreit, Q.~Le, and S.~Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock {\em Transactions of the Association of Computational Linguistics}, 2019.

\bibitem{lewis2021retrievalaugmented}
P.~Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal, H.~Küttler, M.~Lewis, W.~tau Yih, T.~Rocktäschel, S.~Riedel, and D.~Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

\bibitem{lievin2024can}
V.~Li{\'e}vin, C.~E. Hother, A.~G. Motzfeldt, and O.~Winther.
\newblock Can large language models reason about medical questions?
\newblock {\em Patterns}, 5(3), 2024.

\bibitem{lin2004rouge}
C.-Y. Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In {\em Text summarization branches out}, pages 74--81, 2004.

\bibitem{liu2023pre}
P.~Liu, W.~Yuan, J.~Fu, Z.~Jiang, H.~Hayashi, and G.~Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55(9):1--35, 2023.

\bibitem{mallen2023}
A.~Mallen, A.~Asai, V.~Zhong, R.~Das, D.~Khashabi, and H.~Hajishirzi.
\newblock When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.
\newblock In {\em ACL}, 2023.

\bibitem{chatgpt2023}
{OpenAI}.
\newblock Chat{GPT}.
\newblock \url{https://openai.com/index/chatgpt/}, 2023.
\newblock Accessed: 2024-06-04.

\bibitem{panickssery2024llm}
A.~Panickssery, S.~R. Bowman, and S.~Feng.
\newblock Llm evaluators recognize and favor their own generations.
\newblock {\em arXiv preprint arXiv:2404.13076}, 2024.

\bibitem{pradeep2024ragnar}
R.~Pradeep, N.~Thakur, S.~Sharifymoghaddam, E.~Zhang, R.~Nguyen, D.~Campos, N.~Craswell, and J.~Lin.
\newblock Ragnar$\backslash$" ok: A reusable rag framework and baselines for trec 2024 retrieval-augmented generation track.
\newblock {\em arXiv preprint arXiv:2406.16828}, 2024.

\bibitem{rawte2023troubling}
V.~Rawte, S.~Chakraborty, A.~Pathak, A.~Sarkar, S.~Tonmoy, A.~Chadha, A.~P. Sheth, and A.~Das.
\newblock The troubling emergence of hallucination in large language models--an extensive definition, quantification, and prescriptive remediations.
\newblock {\em arXiv preprint arXiv:2310.04988}, 2023.

\bibitem{Schick2023ToolFormer}
T.~Schick, J.~Dwivedi-Yu, R.~Dessi, R.~Raileanu, M.~Lomeli, L.~Zettlemoyer, N.~Cancedda, and T.~Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock {\em arXiv}, 2023.

\bibitem{sun2023head}
K.~Sun, Y.~E. Xu, H.~Zha, Y.~Liu, and X.~L. Dong.
\newblock {Head-to-Tail}: How knowledgeable are large language models (llms)? a.k.a. will llms replace knowledge graphs?
\newblock {\em arXiv preprint arXiv:2308.10168}, 2024.

\bibitem{sun2024large}
Y.~Sun, H.~Xin, K.~Sun, Y.~E. Xu, X.~Yang, X.~L. Dong, N.~Tang, and L.~Chen.
\newblock Are large language models a good replacement of taxonomies?
\newblock {\em Proc. VLDB Endow.}, 17(11):2919–2932, aug 2024.

\bibitem{talmor2018web}
A.~Talmor and J.~Berant.
\newblock The web as a knowledge-base for answering complex questions, 2018.

\bibitem{verifai}
N.~Tang, C.~Yang, J.~Fan, L.~Cao, Y.~Luo, and A.~Y. Halevy.
\newblock Verifai: Verified generative {AI}.
\newblock In {\em {CIDR}}, 2024.

\bibitem{tang2024multihop}
Y.~Tang and Y.~Yang.
\newblock Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries.
\newblock {\em arXiv preprint arXiv:2401.15391}, 2024.

\bibitem{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C. Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem{usbeck2023qald}
R.~Usbeck, X.~Yan, A.~Perevalov, L.~Jiang, J.~Schulz, A.~Kraft, C.~M{\"o}ller, J.~Huang, J.~Reineke, A.-C. Ngonga~Ngomo, et~al.
\newblock {QALD-10}--the 10th challenge on question answering over linked data.
\newblock {\em Semantic Web}, (Preprint):1--15, 2023.

\bibitem{vu2023freshllms}
T.~Vu, M.~Iyyer, X.~Wang, N.~Constant, J.~Wei, J.~Wei, C.~Tar, Y.-H. Sung, D.~Zhou, Q.~Le, and T.~Luong.
\newblock {FreshLLMs}: Refreshing large language models with search engine augmentation, 2023.

\bibitem{lfqa23}
F.~Xu, Y.~Song, M.~Iyyer, and E.~Choi.
\newblock A critical evaluation of evaluations for long-form question answering.
\newblock In {\em Association of Computational Linguistics}, 2023.

\bibitem{yasunaga-etal-2021-qa}
M.~Yasunaga, H.~Ren, A.~Bosselut, P.~Liang, and J.~Leskovec.
\newblock {QA}-{GNN}: Reasoning with language models and knowledge graphs for question answering.
\newblock Association for Computational Linguistics, 2021.

\bibitem{statsqa}
Y.~Zhu, S.~Du, B.~Li, Y.~Luo, and N.~Tang.
\newblock Are large language models good statisticians?
\newblock In {\em NeurIPS}, 2024.

\end{thebibliography}
