\begin{thebibliography}{10}

\bibitem{precup2001off}
Doina Precup, Richard~S Sutton, and Sanjoy Dasgupta.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In {\em ICML}, pages 417--424, 2001.

\bibitem{harutyunyan2016q}
Anna Harutyunyan, Marc~G Bellemare, Tom Stepleton, and R{\'e}mi Munos.
\newblock Q (lambda) with off-policy corrections.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 305--320. Springer, 2016.

\bibitem{munos2016safe}
R{\'e}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1054--1062, 2016.

\bibitem{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{barth2018distributed}
Gabriel Barth-Maron, Matthew~W Hoffman, David Budden, Will Dabney, Dan Horgan,
  Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap.
\newblock Distributed distributional deterministic policy gradients.
\newblock {\em arXiv preprint arXiv:1804.08617}, 2018.

\bibitem{kapturowski2018recurrent}
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock 2018.

\bibitem{rowland2019adaptive}
Mark Rowland, Will Dabney, and R{\'e}mi Munos.
\newblock Adaptive trade-offs in off-policy learning.
\newblock {\em arXiv preprint arXiv:1910.07478}, 2019.

\bibitem{oh2018self}
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.
\newblock Self-imitation learning.
\newblock {\em arXiv preprint arXiv:1806.05635}, 2018.

\bibitem{gangwani2018learning}
Tanmay Gangwani, Qiang Liu, and Jian Peng.
\newblock Learning self-imitating diverse policies.
\newblock {\em arXiv preprint arXiv:1805.10309}, 2018.

\bibitem{guo2019efficient}
Yijie Guo, Jongwook Choi, Marcin Moczulski, Samy Bengio, Mohammad Norouzi, and
  Honglak Lee.
\newblock Efficient exploration with self-imitation learning via
  trajectory-conditioned policy.
\newblock {\em arXiv preprint arXiv:1907.10247}, 2019.

\bibitem{he2016learning}
Frank~S He, Yang Liu, Alexander~G Schwing, and Jian Peng.
\newblock Learning to play in a day: Faster deep reinforcement learning by
  optimality tightening.
\newblock {\em arXiv preprint arXiv:1611.01606}, 2016.

\bibitem{bellman1957markovian}
Richard Bellman.
\newblock A markovian decision process.
\newblock {\em Journal of mathematics and mechanics}, pages 679--684, 1957.

\bibitem{sutton1999}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem{mnih2013}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{schulman2015}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem{wang2016}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van~Hasselt, Marc Lanctot, and Nando
  De~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1511.06581}, 2015.

\bibitem{schulman2017}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{ziebart2010}
Brian~D Ziebart.
\newblock {\em Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\bibitem{fox2015taming}
Roy Fox, Ari Pakman, and Naftali Tishby.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock {\em arXiv preprint arXiv:1512.08562}, 2015.

\bibitem{asadi2017}
Kavosh Asadi and Michael~L Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  243--252, 2017.

\bibitem{nachum2017bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2775--2785, 2017.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em Machine Learning Proceedings 1995}, pages 30--37. Elsevier,
  1995.

\bibitem{mnih2016}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1928--1937, 2016.

\bibitem{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock {\em arXiv preprint arXiv:1802.01561}, 2018.

\bibitem{fujimoto2018addressing}
Scott Fujimoto, Herke Van~Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock {\em arXiv preprint arXiv:1802.09477}, 2018.

\bibitem{schaul2016}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock {\em arXiv preprint arXiv:1511.05952}, 2015.

\bibitem{horgan2018distributed}
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado
  Van~Hasselt, and David Silver.
\newblock Distributed prioritized experience replay.
\newblock {\em arXiv preprint arXiv:1803.00933}, 2018.

\bibitem{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em Thirtieth AAAI conference on artificial intelligence}, 2016.

\bibitem{brockman2016}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{tassa2018deepmind}
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de~Las
  Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et~al.
\newblock Deepmind control suite.
\newblock {\em arXiv preprint arXiv:1801.00690}, 2018.

\bibitem{coumans2010bullet}
Erwin Coumans.
\newblock Bullet physics engine.
\newblock {\em Open Source Software: http://bulletphysics. org}, 1(3):84, 2010.

\bibitem{hasselt2010double}
Hado~V Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in neural information processing systems}, pages
  2613--2621, 2010.

\bibitem{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\bibitem{bertsekas1995neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock Neuro-dynamic programming: an overview.
\newblock In {\em Proceedings of 1995 34th IEEE Conference on Decision and
  Control}, volume~1, pages 560--564. IEEE, 1995.

\bibitem{lee2013bias}
Donghun Lee, Boris Defourny, and Warren~B Powell.
\newblock Bias-corrected q-learning to control max-operator bias in q-learning.
\newblock In {\em 2013 IEEE Symposium on Adaptive Dynamic Programming and
  Reinforcement Learning (ADPRL)}, pages 93--99. IEEE, 2013.

\bibitem{zhang2017weighted}
Zongzhang Zhang, Zhiyuan Pan, and Mykel~J Kochenderfer.
\newblock Weighted double q-learning.
\newblock In {\em IJCAI}, pages 3455--3461, 2017.

\bibitem{anschel2017averaged}
Oron Anschel, Nir Baram, and Nahum Shimkin.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  176--185. PMLR, 2017.

\bibitem{lan2020maxmin}
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White.
\newblock Maxmin q-learning: Controlling the estimation bias of q-learning.
\newblock {\em arXiv preprint arXiv:2002.06487}, 2020.

\bibitem{sutton1998}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem{szepesvari2010algorithms}
Csaba Szepesv{\'a}ri.
\newblock Algorithms for reinforcement learning.
\newblock {\em Synthesis lectures on artificial intelligence and machine
  learning}, 4(1):1--103, 2010.

\bibitem{dudik2014doubly}
Miroslav Dud{\'\i}k, Dumitru Erhan, John Langford, Lihong Li, et~al.
\newblock Doubly robust policy evaluation and optimization.
\newblock {\em Statistical Science}, 29(4):485--511, 2014.

\bibitem{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2139--2148, 2016.

\bibitem{mahmood2017multi}
Ashique~Rupam Mahmood, Huizhen Yu, and Richard~S Sutton.
\newblock Multi-step off-policy learning without importance sampling ratios.
\newblock {\em arXiv preprint arXiv:1702.03006}, 2017.

\bibitem{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock {\em arXiv preprint arXiv:1802.03493}, 2018.

\bibitem{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635, 2011.

\bibitem{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em Proceedings of the twenty-first international conference on
  Machine learning}, page~1, 2004.

\bibitem{blundell2016model}
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman,
  Joel~Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis.
\newblock Model-free episodic control.
\newblock {\em arXiv preprint arXiv:1606.04460}, 2016.

\bibitem{pritzel2017neural}
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol
  Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell.
\newblock Neural episodic control.
\newblock {\em arXiv preprint arXiv:1703.01988}, 2017.

\bibitem{achiam2018openai}
Joshua Achiam.
\newblock Openai spinning up.
\newblock {\em GitHub, GitHub repository}, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{schaul2015prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock {\em arXiv preprint arXiv:1511.05952}, 2015.

\bibitem{baselines}
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias
  Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\end{thebibliography}
