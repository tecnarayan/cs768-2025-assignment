% Encoding: UTF-8

@article{neumann1928theorie,
  title={Zur theorie der gesellschaftsspiele},
  author={Neumann, J v},
  journal={Mathematische annalen},
  volume={100},
  number={1},
  pages={295--320},
  year={1928},
  publisher={Springer}
}

@article{sion1958general,
  title={On general minimax theorems.},
  author={Sion, Maurice},
  journal={Pacific Journal of mathematics},
  volume={8},
  number={1},
  pages={171--176},
  year={1958},
  publisher={Pacific Journal of Mathematics, A Non-profit Corporation}
}



@article{qiu20single_timescale_ncsc,
  title={Single-timescale stochastic nonconvex-concave optimization for smooth nonlinear {TD} learning},
  author={Qiu, Shuang and Yang, Zhuoran and Wei, Xiaohan and Ye, Jieping and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2008.10103},
  year={2020}
}

@article{fiez21NC_PL_SC_neurips,
  title={Global Convergence to Local Minmax Equilibrium in Classes of Nonconvex Zero-Sum Games},
  author={Fiez, Tanner and Ratliff, Lillian and Mazumdar, Eric and Faulkner, Evan and Narang, Adhyyan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{xie21NC_PL_FL_arxiv,
  title={A Federated Learning Framework for Nonconvex-PL Minimax Problems},
  author={Xie, Jiahao and Zhang, Chao and Zhang, Yunsong and Shen, Zebang and Qian, Hui},
  journal={arXiv preprint arXiv:2105.14216},
  year={2021}
}



@article{haddadpour19conv_FL_arxiv,
  title={On the convergence of local descent methods in federated learning},
  author={Haddadpour, Farzin and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:1910.14425},
  year={2019}
}

@article{haddadpour19local_SHD_neurips,
  title={Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={11082--11094},
  year={2019}
}

@inproceedings{yoon21acc_ncc_icml,
  title={Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with O (1/k\^{}2) Rate on Squared Gradient Norm},
  author={Yoon, TaeHo and Ryu, Ernest K},
  booktitle={International Conference on Machine Learning},
  pages={12098--12109},
  year={2021},
  organization={PMLR}
}

@article{ouyang21lower_cc_bilinear_mathprog,
  title={Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  author={Ouyang, Yuyuan and Xu, Yangyang},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={1--35},
  year={2021},
  publisher={Springer}
}

@inproceedings{wang20improved_cc_neurips,
  title={Improved Algorithms for Convex-Concave Minimax Optimization},
  author={Wang, Yuanhao and Li, Jian},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {4800--4810},
  volume={33},
  year={2020}
}

@inproceedings{li21lower_bd_NCSC_neurips,
  title={Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max Optimization},
  author={Li, Haochuan and Tian, Yi and Zhang, Jingzhao and Jadbabaie, Ali},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{yang20NCNC_VR_neurips,
  title={Global Convergence and Variance Reduction for a Class of Nonconvex-Nonconcave Minimax Problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {1153--1165},
  volume={33},
  year={2020}
}

@inproceedings{lei21stability_Minimax_icml,
  title={Stability and Generalization of Stochastic Gradient Methods for Minimax Problems},
  author={Lei, Yunwen and Yang, Zhenhuan and Yang, Tianbao and Ying, Yiming},
  booktitle={International Conference on Machine Learning},
  pages={6175--6186},
  year={2021},
  organization={PMLR}
}


@inproceedings{fiez20GDA_finite_iclr,
  title={Local convergence analysis of gradient descent ascent with finite timescale separation},
  author={Fiez, Tanner and Ratliff, Lillian J},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zhang21NCSC_uai,
  title={The complexity of nonconvex-strongly-concave minimax optimization},
  author={Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={482--492},
  year={2021},
  organization={PMLR}
}

@article{yang21NCPL_arxiv,
  title={Faster Single-loop Algorithms for Minimax Optimization without Strong Concavity},
  author={Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
  journal={arXiv preprint arXiv:2112.05604},
  year={2021}
}

@inproceedings{srivastava11distributed_minmax_dsp,
  title={Distributed min-max optimization in networks},
  author={Srivastava, Kunal and Nedi{\'c}, Angelia and Stipanovi{\'c}, Du{\v{s}}an},
  booktitle={2011 17th International Conference on Digital Signal Processing (DSP)},
  pages={1--8},
  year={2011},
  organization={IEEE}
}

@inproceedings{kiyavash20catalyst_neurips,
  title={A catalyst framework for minimax optimization},
  author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {5667--5678},
  volume={33},
  year={2020}
}

@inproceedings{lee21NCNC_structured_neurips,
  title={Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems},
  author={Lee, Sucheol and Kim, Donghwan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{lu20HiBSA_NC_C_ieee,
  title={Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications},
  author={Lu, Songtao and Tsaknakis, Ioannis and Hong, Mingyi and Chen, Yongxin},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={3676--3691},
  year={2020},
  publisher={IEEE}
}

@inproceedings{diakonikolas21NC_NC_aistats,
  title={Efficient methods for structured nonconvex-nonconcave min-max optimization},
  author={Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2746--2754},
  year={2021},
  organization={PMLR}
}

@article{leaute13protecting,
  title={Protecting privacy through distributed computation in multi-agent decision making},
  author={L{\'e}aut{\'e}, Thomas and Faltings, Boi},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={649--695},
  year={2013}
}

@article{xing2016strategies,
  title={Strategies and principles of distributed machine learning on big data},
  author={Xing, Eric P and Ho, Qirong and Xie, Pengtao and Wei, Dai},
  journal={Engineering},
  volume={2},
  number={2},
  pages={179--195},
  year={2016},
  publisher={Elsevier}
}

@article{bottou18optML_siam,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@book{borkar09stoch_approx_book,
  title={Stochastic approximation: a dynamical systems viewpoint},
  author={Borkar, Vivek S},
  volume={48},
  year={2009},
  publisher={Springer}
}

@inproceedings{tran20hybrid_NCLin_neurips,
  title={Hybrid Variance-Reduced SGD Algorithms For Minimax Problems with Nonconvex-Linear Function.},
  author={Tran-Dinh, Quoc and Liu, Deyi and Nguyen, Lam M},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {11096--11107},
  volume = {33},
  year={2020}
}

@inproceedings{lin_GDA_icml20,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6083--6093},
  year={2020},
  organization={PMLR}
}

@inproceedings{luo20SREDA_ncsc_neurips,
  title={Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems},
  author={Luo, Luo and Ye, Haishan and Huang, Zhichao and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {20566--20577},
  volume={33},
  year={2020}
}

@inproceedings{jin20local_opt_NCNC_icml,
  title={What is local optimality in nonconvex-nonconcave minimax optimization?},
  author={Jin, Chi and Netrapalli, Praneeth and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4880--4889},
  year={2020},
  organization={PMLR}
}

@inproceedings{thekumparampil19NC_C_neurips,
  title={Efficient Algorithms for Smooth Minimax Optimization},
  author={Thekumparampil, Kiran K and Jain, Prateek and Netrapalli, Praneeth and Oh, Sewoong},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{tomluo_1_loop_ncc_neurips20,
  title={A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems},
  author={Zhang, Jiawei and Xiao, Peijun and Sun, Ruoyu and Luo, Zhiquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7377--7389},
  volume={33},
  year={2020}
}

@article{lan_unified_ncc_arxiv20,
  title={A unified single-loop alternating gradient projection algorithm for nonconvex-concave and convex-nonconcave minimax problems},
  author={Xu, Zi and Zhang, Huiling and Xu, Yang and Lan, Guanghui},
  journal={arXiv preprint arXiv:2006.02032},
  year={2020}
}

@inproceedings{lin20near_opt_det_colt,
  title={Near-optimal algorithms for minimax optimization},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2738--2779},
  year={2020},
  organization={PMLR}
}

@article{zhang_nc_linear_cons_ADMM_SIAM20,
  title={A proximal alternating direction method of multiplier for linearly constrained nonconvex minimization},
  author={Zhang, Jiawei and Luo, Zhi-Quan},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={3},
  pages={2272--2302},
  year={2020},
  publisher={SIAM}
}

@inproceedings{liang20proxGDA_KL_iclr,
  title={Proximal Gradient Descent-Ascent: Variable Convergence under K{\L} Geometry},
  author={Chen, Ziyi and Zhou, Yi and Xu, Tengyu and Liang, Yingbin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{luo21near_opt_FS_cc_arxiv,
  title={Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced Convex-Concave Minimax Optimization},
  author={Luo, Luo and Xie, Guangzeng and Zhang, Tong and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2106.01761},
  year={2021}
}

@inproceedings{xie20lower_FS_cc_icml,
  title={Lower complexity bounds for finite-sum convex-concave minimax optimization problems},
  author={Xie, Guangzeng and Luo, Luo and Lian, Yijiang and Zhang, Zhihua},
  booktitle={International Conference on Machine Learning},
  pages={10504--10513},
  year={2020},
  organization={PMLR}
}

@article{hardt16fairness_neurips,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3315--3323},
  year={2016}
}

@article{rafique18WCC_oms,
  title={Weakly-convex--concave min--max optimization: provable algorithms and applications in machine learning},
  author={Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
  journal={Optimization Methods and Software},
  pages={1--35},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{wang21coopSGD_jmlr,
  title={Cooperative {SGD}: A unified framework for the design and analysis of local-update sgd algorithms},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={213},
  pages={1--50},
  year={2021}
}

@article{liu21wcwc_jmlr,
  title={First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems},
  author={Liu, Mingrui and Rafique, Hassan and Lin, Qihang and Yang, Tianbao},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={169},
  pages={1--34},
  year={2021}
}





@inproceedings{mahdavi21localSGDA_aistats,
  title={Local stochastic gradient descent ascent: Convergence analysis and communication efficiency},
  author={Deng, Yuyang and Mahdavi, Mehrdad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1387--1395},
  year={2021},
  organization={PMLR}
}

@article{liao21local_AdaGrad_CC_arxiv,
  title={Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems},
  author={Liao, Luofeng and Shen, Li and Duan, Jia and Kolar, Mladen and Tao, Dacheng},
  journal={arXiv preprint arXiv:2106.10022},
  year={2021}
}

@inproceedings{mahdavi20dist_robustfl_neurips,
  title={Distributionally Robust Federated Averaging},
  author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {15111--15122},
  volume={33},
  year={2020}
}


@inproceedings{reisizadeh20robustfl_neurips,
  title={Robust federated learning: The case of affine distribution shifts},
  author={Reisizadeh, Amirhossein and Farnia, Farzan and Pedarsani, Ramtin and Jadbabaie, Ali},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {21554--21565},
  volume={33},
  year={2020}
}

@inproceedings{yuan21FedDeepAUC_icml,
  title={Federated Deep {AUC} Maximization for Heterogeneous Data with a Constant Communication Complexity},
  author={Yuan, Zhuoning and Guo, Zhishuai and Xu, Yi and Ying, Yiming and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={12219--12229},
  year={2021},
  organization={PMLR}
}

@article{yang21DeepAUC_arxiv,
  title={Deep {AUC} Maximization for Medical Image Classification: Challenges and Opportunities},
  author={Yang, Tianbao},
  journal={arXiv preprint arXiv:2111.02400},
  year={2021}
}

@inproceedings{guo20DeepAUC_icml,
  title={Communication-efficient distributed stochastic {AUC} maximization with deep neural networks},
  author={Guo, Zhishuai and Liu, Mingrui and Yuan, Zhuoning and Shen, Li and Liu, Wei and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={3864--3874},
  year={2020},
  organization={PMLR}
}

@article{gasnikov21dec_stoch_EG_VI_arxiv,
  title={Decentralized Local Stochastic Extra-Gradient for Variational Inequalities},
  author={Beznosikov, Aleksandr and Dvurechensky, Pavel and Koloskova, Anastasia and Samokhin, Valentin and Stich, Sebastian U and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2106.08315},
  year={2021}
}

@inproceedings{liu20dec_GANs_neurips,
  title={Decentralized Parallel Algorithm for Training Generative Adversarial Nets},
  author={Liu, Mingrui Liu and Mroueh, Youssef and Zhang, Wei and Cui, Xiaodong and Ross, Jerret and Das, Payel},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {11056--11070},
  volume={33},
  year={2020}
}

@inproceedings{gasnikov21decen_deter_cc_icoa,
  title={Near-Optimal Decentralized Algorithms for Saddle Point Problems over Time-Varying Networks},
  author={Beznosikov, Aleksandr and Rogozin, Alexander and Kovalev, Dmitry and Gasnikov, Alexander},
  booktitle={International Conference on Optimization and Applications},
  pages={246--257},
  year={2021},
  organization={Springer}
}

@article{rogozin21dec_local_global_var_cc_arxiv,
  title={Decentralized distributed optimization for saddle point problems},
  author={Rogozin, Alexander and Beznosikov, Alexander and Dvinskikh, Darina and Kovalev, Dmitry and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2102.07758},
  year={2021}
}

@inproceedings{xian21dec_ncsc_storm_neurips,
  title={A faster decentralized algorithm for nonconvex minimax problems},
  author={Xian, Wenhan and Huang, Feihu and Zhang, Yanfu and Huang, Heng},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ozdaglar19dec_prox_sp_arxiv,
  title={A decentralized proximal point-type method for saddle point problems},
  author={Liu, Weijie and Mokhtari, Aryan and Ozdaglar, Asuman and Pattathil, Sarath and Shen, Zebang and Zheng, Nenggan},
  journal={arXiv preprint arXiv:1910.14380},
  year={2019}
}

@inproceedings{beznosikov21dist_sp_neurips,
  title={Distributed saddle-point problems under similarity},
  author={Beznosikov, Aleksandr and Scutari, Gesualdo and Rogozin, Alexander and Gasnikov, Alexander},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{beznosikov20dist_SP_arxiv,
  title={Distributed Saddle-Point Problems: Lower Bounds, Optimal Algorithms and Federated GANs},
  author={Beznosikov, Aleksandr and Samokhin, Valentin and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2010.13112},
  year={2020}
}

@article{chen20dist_GAN_quantize_arxiv,
  title={A Distributed Training Algorithm of Generative Adversarial Networks with Quantized Gradients},
  author={Chen, Xiaojun and Yang, Shu and Shen, Li and Pang, Xuanrong},
  journal={arXiv preprint arXiv:2010.13359},
  year={2020}
}

@article{hou21FedSP_arxiv,
  title={Efficient Algorithms for Federated Saddle Point Optimization},
  author={Hou, Charlie and Thekumparampil, Kiran K and Fanti, Giulia and Oh, Sewoong},
  journal={arXiv preprint arXiv:2102.06333},
  year={2021}
}

@article{kurakin16adversarial_arxiv,
  title={Adversarial machine learning at scale},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.01236},
  year={2016}
}

@inproceedings{bach19universal_VI_Colt,
  title={A universal algorithm for variational inequalities adaptive to smoothness and noise},
  author={Bach, Francis and Levy, Kfir Y},
  booktitle={Conference on Learning Theory},
  pages={164--194},
  year={2019},
  organization={PMLR}
}

@article{richtarik21dist_VI_comm_arxiv,
  title={Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees},
  author={Beznosikov, Aleksandr and Richt{\'a}rik, Peter and Diskin, Michael and Ryabinin, Max and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2110.03313},
  year={2021}
}

@article{gasnikov21dec_person_FL_arxiv,
  title={Decentralized Personalized Federated Min-Max Problems},
  author={Beznosikov, Aleksandr and Sushko, Vadim and Sadiev, Abdurakhmon and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2106.07289},
  year={2021}
}

@article{barazandeh21decen_minmax_arxiv,
  title={A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems},
  author={Barazandeh, Babak and Huang, Tianjian and Michailidis, George},
  journal={arXiv preprint arXiv:2106.06075},
  year={2021}
}




@inproceedings{spiridonoff21comm_eff_SGD_neurips,
  title={Communication-efficient SGD: From Local SGD to One-Shot Averaging},
  author={Spiridonoff, Artin and Olshevsky, Alex and Paschalidis, Ioannis Ch},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{khaled20localSGD_aistats,
  title={Tighter theory for local SGD on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@inproceedings{koloskova20unified_localSGD_icml,
  title={A unified theory of decentralized SGD with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@inproceedings{ma20fedaccSGD_neurips20,
  title={Federated Accelerated Stochastic Gradient Descent},
  author={Yuan, Honglin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {5332--5344},
  volume={33},
  year={2020}
}

@article{stich20error_fb_jmlr,
  title={The error-feedback framework: Better rates for SGD with delayed gradients and compressed updates},
  author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--36},
  year={2020}
}

@inproceedings{zhou18localSGD_ijcai,
  title={On the convergence properties of a K-step averaging stochastic gradient descent algorithm for nonconvex optimization},
  author={Zhou, Fan and Cong, Guojing},
  booktitle={Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  pages={3219--3227},
  year={2018}
}

@inproceedings{scaffold20icml,
  title={SCAFFOLD: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@inproceedings{joshi20fednova_neurips,
  title={Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization},
  author={Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H Vincent},
  booktitle={Advances in Neural Information Processing Systems},
  pages = {7611--7623},
  volume={33},
  year={2020}
}

@inproceedings{mohri19agnosticFL_icml,
  title={Agnostic federated learning},
  author={Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={4615--4625},
  year={2019},
  organization={PMLR}
}

@article{bonawitz19towardsFL_arxiv,
  title={Towards federated learning at scale: System design},
  author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Kone{\v{c}}n{\`y}, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and others},
  journal={arXiv preprint arXiv:1902.01046},
  year={2019}
}

@article{shen21fedmm_arxiv,
  title={Fed{MM}: Saddle Point Optimization for Federated Adversarial Domain Adaptation},
  author={Shen, Yan and Du, Jian and Zhao, Han and Zhang, Benyu and Ji, Zhanghexuan and Gao, Mingchen},
  journal={arXiv preprint arXiv:2110.08477},
  year={2021}
}

@article{yang18FL_google_arxiv,
  title={Applied federated learning: Improving google keyboard query suggestions},
  author={Yang, Timothy and Andrew, Galen and Eichner, Hubert and Sun, Haicheng and Li, Wei and Kong, Nicholas and Ramage, Daniel and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:1812.02903},
  year={2018}
}

@article{kairouz19advancesFL_arxiv,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}

@article{smith20FL_SPmag,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

@inproceedings{tomluo21comm_ncns_aistats,
  title={Communication Efficient Primal-Dual Algorithm for Nonconvex Nonsmooth Distributed Optimization},
  author={Chen, Congliang and Zhang, Jiawei and Shen, Li and Zhao, Peilin and Luo, Zhiquan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1594--1602},
  year={2021},
  organization={PMLR}
}

@article{shahram21dist_WC_tac,
  title={On Distributed Non-convex Optimization: Projected Subgradient Method For Weakly Convex Problems in Networks},
  author={Chen, Shixiang and Garcia, Alfredo and Shahrampour, Shahin},
  journal={IEEE Transactions on Automatic Control},
  year={2021},
  publisher={IEEE}
}

@inproceedings{hinder20near_opt_star_convex_colt,
  title={Near-optimal methods for minimizing star-convex functions and beyond},
  author={Hinder, Oliver and Sidford, Aaron and Sohoni, Nimit},
  booktitle={Conference on Learning Theory},
  pages={1894--1938},
  year={2020},
  organization={PMLR}
}

@article{jin20quasar_convex_arxiv,
  title={On The Convergence of First Order Methods for Quasar-Convex Optimization},
  author={Jin, Jikai},
  journal={arXiv preprint arXiv:2010.04937},
  year={2020}
}

@article{gasnikov17acc_quasar_convex_arxiv,
  title={Accelerated Methods for $\alpha$-Weakly-Quasi-Convex Problems},
  author={Guminov, Sergey and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:1710.00797},
  year={2017}
}

@article{nesterov06cubic_mathprog,
  title={Cubic regularization of Newton method and its global performance},
  author={Nesterov, Yurii and Polyak, Boris T},
  journal={Mathematical Programming},
  volume={108},
  number={1},
  pages={177--205},
  year={2006},
  publisher={Springer}
}



@article{nemirovski09robustSA_siam,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@inproceedings{johansson20sg_momentum_ncns_icml,
  title={Convergence of a stochastic gradient method with momentum for non-smooth non-convex optimization},
  author={Mai, Vien and Johansson, Mikael},
  booktitle={International Conference on Machine Learning},
  pages={6630--6639},
  year={2020},
  organization={PMLR}
}

@article{gasnikov20recent_nc_arxiv,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  journal={arXiv preprint arXiv:2012.06188},
  year={2020}
}

@inproceedings{schmidt16lin_conv_PL_kdd,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@inproceedings{mertikopoulos18optMD_SP_iclr,
  title={Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile},
  author={Mertikopoulos, Panayotis and Lecouat, Bruno and Zenati, Houssam and Foo, Chuan-Sheng and Chandrasekhar, Vijay and Piliouras, Georgios},
  booktitle={International Conference on Learning Representations},
  year={2018}
}



@inproceedings{kleinberg18icml,
  title={An alternative view: When does SGD escape local minima?},
  author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={2698--2707},
  year={2018},
  organization={PMLR}
}

@inproceedings{li17relu_neurips,
  title={Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{shamir21oracle_ncns_arxiv,
  title={Oracle Complexity in Nonsmooth Nonconvex Optimization},
  author={Kornowski, Guy and Shamir, Ohad},
  journal={arXiv preprint arXiv:2104.06763},
  year={2021}
}



@book{nesterov18book,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{zhang16arxiv,
  title={Parallel {SGD}: When does averaging help?},
  author={Zhang, Jian and De Sa, Christopher and Mitliagkas, Ioannis and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1606.07365},
  year={2016}
}

@article{davis19wc_siam,
  title={Stochastic model-based minimization of weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={207--239},
  year={2019},
  publisher={SIAM}
}

@article{drusvyatskiy19wc_mathprog,
  title={Efficiency of minimizing compositions of convex functions and smooth maps},
  author={Drusvyatskiy, Dmitriy and Paquette, Courtney},
  journal={Mathematical Programming},
  volume={178},
  number={1},
  pages={503--558},
  year={2019},
  publisher={Springer}
}


@article{konevcny16federated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}

@article{jaggi18localSGD,
  title={Don't Use Large Mini-Batches, Use Local {SGD}},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@inproceedings{cadambe19icml,
  title={Trading Redundancy for Communication: Speeding up Distributed {SGD} for Non-convex Optimization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={International Conference on Machine Learning},
  pages={2545--2554},
  year={2019},
  organization={PMLR}
}

@inproceedings{chen18neurips_lag,
  title={{LAG}: Lazily aggregated gradient for communication-efficient distributed learning},
  author={Chen, Tianyi and Giannakis, Georgios and Sun, Tao and Yin, Wotao},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{wang18arxiv,
  title={Adaptive communication strategies to achieve the best error-runtime trade-off in local-update {SGD}},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={arXiv preprint arXiv:1810.08313},
  year={2018}
}

@inproceedings{yu19icml_momentum,
  title={On the Linear Speedup Analysis of Communication Efficient Momentum {SGD} for Distributed Non-Convex Optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle={International Conference on Machine Learning},
  pages={7184--7193},
  year={2019},
  organization={PMLR}
}

@inproceedings{ruan21device_part_FL_aistats,
  title={Towards Flexible Device Participation in Federated Learning},
  author={Ruan, Yichen and Zhang, Xiaoxi and Liang, Shu-Che and Joe-Wong, Carlee},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3403--3411},
  year={2021},
  organization={PMLR}
}

@inproceedings{gu21mifa_neurips,
  title={Fast Federated Learning in the Presence of Arbitrary Device Unavailability},
  author={Gu, Xinran and Huang, Kaixuan and Zhang, Jingzhao and Huang, Longbo},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{yang21partial_client_iclr,
  title={Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning},
  author={Yang, Haibo and Fang, Minghong and Liu, Jia},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{heusel17gans_neurips,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{xiao17fashionMNIST,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@inproceedings{daskalakis18GANs_iclr,
  title={Training GANs with Optimism.},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  booktitle={International Conference on Learning Representations (ICLR 2018)},
  year={2018}
}

@inproceedings{wang19FL_iclr,
  title={Federated Learning with Matched Averaging},
  author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{stich18localSGD_iclr,
  title={Local SGD Converges Fast and Communicates Little},
  author={Stich, Sebastian U},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{restarted_sgd19aaai,
  title={Parallel restarted {SGD} with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5693--5700},
  year={2019}
}

@article{ghadimi13siam,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@inproceedings{nouiehed19minimax_neurips19,
  title={Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods},
  author={Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14934--14942},
  volume={32},
  year={2019}
}


@inproceedings{cutkosky19momentum_neurips,
 author = {Cutkosky, Ashok and Orabona, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Momentum-Based Variance Reduction in Non-Convex SGD},
 volume = {32},
 year = {2019}
}

@article{polyak63PL,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}

@article{liu22overparameter_NN_elsevier,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{charles18generalization_icml,
  title={Stability and generalization of learning algorithms that converge to global optima},
  author={Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={745--754},
  year={2018},
  organization={PMLR}
}


@article{nedic09subgradient_jota,
  title={Subgradient methods for saddle-point problems},
  author={Nedi{\'c}, Angelia and Ozdaglar, Asuman},
  journal={Journal of optimization theory and applications},
  volume={142},
  number={1},
  pages={205--228},
  year={2009},
  publisher={Springer}
}

@article{nemirovski04prox_siam,
  title={Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  author={Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={229--251},
  year={2004},
  publisher={SIAM}
}


@article{chen21distWC_tac21,
  title={On Distributed Non-convex Optimization: Projected Subgradient Method For Weakly Convex Problems in Networks},
  author={Chen, Shixiang and Garcia, Alfredo and Shahrampour, Shahin},
  journal={IEEE Transactions on Automatic Control},
  year={2021},
  publisher={IEEE}
}

@inproceedings{adam15iclr,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@article{zhang20fedpd_arxiv,
  title={Fedpd: A federated learning framework with optimal rates and adaptivity to non-iid data},
  author={Zhang, Xinwei and Hong, Mingyi and Dhople, Sairaj and Yin, Wotao and Liu, Yang},
  journal={arXiv preprint arXiv:2005.11418},
  year={2020}
}

@article{zhao18FL_noniid_arxiv,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}

@inproceedings{fedpaq20icml,
  title={Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization},
  author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2021--2031},
  year={2020},
  organization={PMLR}
}

@inproceedings{fedavg17aistats,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{mokhtari20fedcomgate,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:2007.01154},
  year={2020}
}

@article{karimireddy20mime,
  title={Mime: Mimicking centralized stochastic algorithms in federated learning},
  author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:2008.03606},
  year={2020}
}

@inproceedings{resnet16cvpr,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{cifar09dataset,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{arjevani19lower_stoch_NC_arxiv,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={arXiv preprint arXiv:1912.02365},
  year={2019}
}

@article{mnist98lecun,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


% Applications:
% 1. Adversarial Example Generation
@inproceedings{madry18adversarial_iclr,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{sinha17certifiable_robust_iclr,
  title={Certifiable distributional robustness with principled adversarial training},
  author={Sinha, Aman and Namkoong, Hongseok and Duchi, John},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{wang21adversarial_minmax_neurips,
  title={Adversarial attack generation empowered by min-max optimization},
  author={Wang, Jingkang and Zhang, Tianyun and Liu, Sijia and Chen, Pin-Yu and Xu, Jiacen and Fardad, Makan and Li, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{jacot18NTK_neurips,
  title={Neural Tangent Kernel: convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8580--8589},
  volume={31},
  year={2018}
}

% 2. GANs
@inproceedings{goodfellow14GANs_neurips,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{gulrajani17improved_WGANs_neurips,
  title={Improved Training of Wasserstein GANs},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Mart{\'\i}n and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{arjovsky17WGANs_icml,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

% 3. Reinforcement Learning
@inproceedings{dai18sbeed_RL_nonlin_FA_icml,
  title={Sbeed: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  booktitle={International Conference on Machine Learning},
  pages={1125--1134},
  year={2018},
  organization={PMLR}
}

@inproceedings{dai17learning_aistats,
  title={Learning from conditional distributions via dual embeddings},
  author={Dai, Bo and He, Niao and Pan, Yunpeng and Boots, Byron and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1458--1467},
  year={2017},
  organization={PMLR}
}

@inproceedings{jiang18linear_neurips,
  title={A linear speedup analysis of distributed deep learning with sparse and quantized communication},
  author={Jiang, Peng and Agrawal, Gagan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2530--2541},
  year={2018}
}

% 4. Robust Optimization
@inproceedings{namkoong16SG_DRO_neurips,
  title={Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{hsieh21limits_minmax_icml,
  title={The limits of min-max optimization algorithms: Convergence to spurious non-critical sets},
  author={Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={4337--4348},
  year={2021},
  organization={PMLR}
}

@inproceedings{daskalakis21constr_minmax_sigact,
  title={The complexity of constrained min-max optimization},
  author={Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1466--1478},
  year={2021}
}

@inproceedings{namkoong17var_regular_neurips,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@Comment{jabref-meta: databaseType:bibtex;}

