
@article{yu2023gptfuzzer,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{liu2023autodan,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{gcg,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{lapid2023open,
  title={Open sesame! universal black box jailbreaking of large language models},
  author={Lapid, Raz and Langberg, Ron and Sipper, Moshe},
  journal={arXiv preprint arXiv:2309.01446},
  year={2023}
}

@article{xiao2023c,
  title={C-pack: Packaged resources to advance general chinese embedding},
  author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},
  journal={arXiv preprint arXiv:2309.07597},
  year={2023}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{falcon40b,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@inproceedings{
xue2023trojllm,
title={Troj{LLM}: A Black-box Trojan Prompt Attack on Large Language Models},
author={Jiaqi Xue and Mengxin Zheng and Ting Hua and Yilin Shen and Yepeng Liu and Ladislau B{\"o}l{\"o}ni and Qian Lou},
booktitle={NeurIPS},
year={2023}
}

@article{deng2023jailbreaker,
  title={Jailbreaker: Automated jailbreak across multiple large language model chatbots},
  author={Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2307.08715},
  year={2023}
}

@inproceedings{
wei2023jailbroken,
title={Jailbroken: How Does {LLM} Safety Training Fail?},
author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
booktitle={NeurIPS},
year={2023}
}

@inproceedings{
li2023multistep,
title={Multi-step Jailbreaking Privacy Attacks on Chat{GPT}},
author={Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Fanpu Meng and Yangqiu Song},
booktitle={EMNLP},
year={2023}
}

@article{liu2023jailbreaking,
  title={Jailbreaking chatgpt via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023}
}

@article{shen2023anything,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@article{li2023deepinception,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}

@article{bhardwaj2023red,
  title={Red-teaming large language models using chain of utterances for safety-alignment},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2308.09662},
  year={2023}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@inproceedings{rlprompt,
    title = {{RLP}rompt: Optimizing Discrete Text Prompts with Reinforcement Learning},
    author = {Deng, Mingkai  and
      Wang, Jianyu  and
      Hsieh, Cheng-Ping  and
      Wang, Yihan  and
      Guo, Han  and
      Shu, Tianmin  and
      Song, Meng  and
      Xing, Eric  and
      Hu, Zhiting},
    booktitle = {EMNLP},
    year = {2022},}

@inproceedings{perez-red,
    title = {Red Teaming Language Models with Language Models},
    author = {Perez, Ethan  and
      Huang, Saffron  and
      Song, Francis  and
      Cai, Trevor  and
      Ring, Roman  and
      Aslanides, John  and
      Glaese, Amelia  and
      McAleese, Nat  and
      Irving, Geoffrey},
    booktitle = {EMNLP},
    year = {2022},
}

@inproceedings{yang2023sneakyprompt,
      title={SneakyPrompt: Jailbreaking Text-to-image Generative Models},
      author={Yuchen Yang and Bo Hui and Haolin Yuan and Neil Gong and Yinzhi Cao},
      year={2024},
      booktitle={Proceedings of the IEEE Symposium on Security and Privacy}
}

@inproceedings{
yuan2024gpt,
title={{GPT}-4 Is Too Smart To Be Safe: Stealthy Chat with {LLM}s via Cipher},
author={Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu},
booktitle={ICLR},
year={2024},
}

@inproceedings{
hong2024curiositydriven,
title={Curiosity-driven Red-teaming for Large Language Models},
author={Zhang-Wei Hong and Idan Shenfeld and Tsun-Hsuan Wang and Yung-Sung Chuang and Aldo Pareja and James R. Glass and Akash Srivastava and Pulkit Agrawal},
booktitle={ICLR},
year={2024},
}

@article{guo2021efficient,
  title={Efficient (soft) q-learning for text generation with limited good data},
  author={Guo, Han and Tan, Bowen and Liu, Zhengzhong and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2106.07704},
  year={2021}
}


@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {NeurIPS},
 title = {Attention is All you Need},
 year = {2017}
}

@misc{openai2023chatgpt,
  title={Introducing ChatGPT},
  author={OpenAI},
  year={2023},
  url={https://openai.com/blog/chatgpt},
}

@misc{bard_website,
  author = {Google AI},
  title = {Bard: A Large Language Model by Google AI},
  year = {2023},
  url = {https://gemini.google.com/app}
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle = {NeurIPS},
  year={2022}
}

@article{wang2023decodingtrust,
  title={Decodingtrust: A comprehensive assessment of trustworthiness in gpt models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  journal={arXiv preprint arXiv:2306.11698},
  year={2023}
}

@article{sun2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Sun, Lichao and Huang, Yue and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and others},
  journal={arXiv preprint arXiv:2401.05561},
  year={2024}
}

@inproceedings{
shah2023scalable,
title={Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation},
author={Rusheb Shah and Quentin Feuillade Montixi and Soroush Pour and Arush Tagade and Javier Rando},
booktitle={NeurIPS workshop SoLaR},
year={2023},
}

@article{shen2024rapid,
  title={Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia},
  author={Shen, Guangyu and Cheng, Siyuan and Zhang, Kaiyuan and Tao, Guanhong and An, Shengwei and Yan, Lu and Zhang, Zhuo and Ma, Shiqing and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2402.05467},
  year={2024}
}

@article{mehrotra2023tree,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@article{zeng2024johnny,
  title={How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
  author={Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
  journal={arXiv preprint arXiv:2401.06373},
  year={2024}
}

@article{kumar2023certifying,
  title={Certifying llm safety against adversarial prompting},
  author={Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Feizi, Soheil and Lakkaraju, Hima},
  journal={arXiv preprint arXiv:2309.02705},
  year={2023}
}

@inproceedings{robey2023smoothllm,
    title={Smooth{LLM}: Defending Large Language Models Against Jailbreaking Attacks},
    author={Alexander Robey and Eric Wong and Hamed Hassani and George Pappas},
    booktitle={NeurIPS workshop R0-FoMo},
    year={2023},
  
}

@article{cao2023defending,
  title={Defending against alignment-breaking attacks via robustly aligned llm},
  author={Cao, Bochuan and Cao, Yuanpu and Lin, Lu and Chen, Jinghui},
  journal={arXiv preprint arXiv:2309.14348},
  year={2023}
}

@inproceedings{li2024rain,
title={{RAIN}: Your Language Models Can Align Themselves without Finetuning},
author={Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang and Hongyang Zhang},
booktitle={ICLR},
year={2024},
}

@article{helbling2023llm,
  title={Llm self defense: By self examination, llms know they are being tricked},
  author={Helbling, Alec and Phute, Mansi and Hull, Matthew and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2308.07308},
  year={2023}
}

@inproceedings{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      booktitle = {NeurIPS},
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={NeurIPS},
  year={2023}
}

@article{zhu2023multilingual,
  title={Multilingual machine translation with large language models: Empirical results and analysis},
  author={Zhu, Wenhao and Liu, Hongyi and Dong, Qingxiu and Xu, Jingjing and Kong, Lingpeng and Chen, Jiajun and Li, Lei and Huang, Shujian},
  journal={arXiv preprint arXiv:2304.04675},
  year={2023}
}

@inproceedings{
wei2022finetuned,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={ICLR},
year={2022},
}

@inproceedings{markov2023holistic,
  title={A holistic approach to undesired content detection in the real world},
  author={Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Nekoul, Florentine Eloundou and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian},
  booktitle={AAAI},
  year={2023}
}

@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@article{wang2023investigating,
  title={Investigating the Existence of" Secret Language''in Language Models},
  author={Wang, Yimu and Shi, Peng and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2307.12507},
  year={2023}
}

@article{xie2023reminders,
  title={Defending ChatGPT against jailbreak attack via self-reminders},
  author={Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao},
  journal={Nature Machine Intelligence},
  year={2023}
}

@article{xu2024safedecoding,
  title={SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha},
  journal={arXiv preprint arXiv:2402.08983},
  year={2024}
}

@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  year={2022}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@inproceedings{
zhang2023tempera,
title={{TEMPERA}: Test-Time Prompt Editing via Reinforcement Learning},
author={Tianjun Zhang and Xuezhi Wang and Denny Zhou and Dale Schuurmans and Joseph E. Gonzalez},
booktitle={ICLR},
year={2023},
}

@misc{Detoxify,
  title={Detoxify},
  author={Hanu, Laura and {Unitary team}},
  howpublished={Github. https://github.com/unitaryai/detoxify},
  year={2020}
}

@inproceedings{
Holtzman2020The,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={ICLR},
year={2020},
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{zhao2024weak,
  title={Weak-to-strong jailbreaking on large language models},
  author={Zhao, Xuandong and Yang, Xianjun and Pang, Tianyu and Du, Chao and Li, Lei and Wang, Yu-Xiang and Wang, William Yang},
  journal={arXiv preprint arXiv:2401.17256},
  year={2024}
}

@book{Holland1975,  
  publisher = {University of Michigan Press},
  title = {Adaptation in Natural and Artificial Systems},
  year = 1975
}

@article{moscato1989evolution,
  title={On evolution, search, optimization, genetic algorithms and martial arts: Towards memetic algorithms},
  author={Moscato, Pablo and others}
}

@article{ant,
  author={Dorigo, M. and Maniezzo, V. and Colorni, A.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
  title={Ant system: optimization by a colony of cooperating agents}, 
  year={1996}
  }

@article{holland1992genetic,
  title={Genetic algorithms},
  author={Holland, John H},
  journal={Scientific american},
  year={1992},
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@book{goldberg2013genetic,
  title={Genetic algorithms},
  author={Goldberg, David E},
  year={2013},
  publisher={pearson education India}
}


@inproceedings{Kennedy,
  author={Kennedy, J. and Eberhart, R.},
  booktitle={Proceedings of ICNN'95 - International Conference on Neural Networks}, 
  title={Particle swarm optimization}, 
  year={1995},
  }

@inproceedings{riedmiller2018learning,
  title={Learning by playing solving sparse reward tasks from scratch},
  author={Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle={ICML},
  year={2018},
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  year={2015},
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={ICML},
  year={2016},
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={ICML},
  year={2015},
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@incollection{hoos2018stochastic,
  title={Stochastic local search},
  author={Hoos, Holger H and St$\ddot{\nu}$tzle, Thomas},
  booktitle={Handbook of Approximation Algorithms and Metaheuristics},
  year={2018},
}

@software{openai-gpt35turbo1106,
  author       = {{OpenAI}},
  title        = {{gpt-3.5-turbo-1106}},
  year         = {2023},
  url          = {https://openai.com/blog/new-models-and-developer-products-announced-at-devday},
}

@article{ramamurthy2022reinforcement,
  title={Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.01241},
  year={2022}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{miller1990empirical,
  title={An empirical study of the reliability of UNIX utilities},
  author={Miller, Barton P and Fredriksen, Lars and So, Bryan},
  journal={Communications of the ACM},
  year={1990},
}

@misc{liu2023llava,
title={Visual Instruction Tuning}, 
author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
publisher={NeurIPS},
year={2023},
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@misc{sora_website,
  author = {OpenAI},
  title = {Creating video from text.},
  year = {2024},
  howpublished = {\url{https://openai.com/sora}}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{xu2023languagerl,
  title={Language agents with reinforcement learning for strategic play in the werewolf game},
  author={Xu, Zelai and Yu, Chao and Fang, Fei and Wang, Yu and Wu, Yi},
  journal={arXiv preprint arXiv:2310.18940},
  year={2023}
}

@article{ataallah2024minigpt4,
  title={MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens},
  author={Ataallah, Kirolos and Shen, Xiaoqian and Abdelrahman, Eslam and Sleiman, Essam and Zhu, Deyao and Ding, Jian and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2404.03413},
  year={2024}
}

@article{zhang2023defending,
  title={Defending large language models against jailbreaking attacks through goal prioritization},
  author={Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Huang, Minlie},
  journal={arXiv preprint arXiv:2311.09096},
  year={2023}
}

@article{yuan2024rigorllm,
  title={RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content},
  author={Yuan, Zhuowen and Xiong, Zidi and Zeng, Yi and Yu, Ning and Jia, Ruoxi and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2403.13031},
  year={2024}
}

@article{zeng2024autodefense,
  title={AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks},
  author={Zeng, Yifan and Wu, Yiran and Zhang, Xiao and Wang, Huazheng and Wu, Qingyun},
  journal={arXiv preprint arXiv:2403.04783},
  year={2024}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{ding2023wolf,
  title={A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian},
  journal={arXiv preprint arXiv:2311.08268},
  year={2023}
}

@article{lv2024codechameleon,
  title={CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models},
  author={Lv, Huijie and Wang, Xiao and Zhang, Yuansen and Huang, Caishuang and Dou, Shihan and Ye, Junjie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.16717},
  year={2024}
}

@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={AAAI},
  year={2016}
}

@article{ruder2016gd,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@article{guo2024cold,
  title={Cold-attack: Jailbreaking llms with stealthiness and controllability},
  author={Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
  journal={arXiv preprint arXiv:2402.08679},
  year={2024}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  year={2024},
}

@article{chang2024play,
  title={Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues},
  author={Chang, Zhiyuan and Li, Mingyang and Liu, Yi and Wang, Junjie and Wang, Qing and Liu, Yang},
  journal={arXiv preprint arXiv:2402.09091},
  year={2024}
}

@article{huang2023catastrophic,
  title={Catastrophic jailbreak of open-source llms via exploiting generation},
  author={Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06987},
  year={2023}
}

@article{wang2023adversarial,
  title={Adversarial demonstration attacks on large language models},
  author={Wang, Jiongxiao and Liu, Zichen and Park, Keun Hee and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2305.14950},
  year={2023}
}

@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{uncensored,
  title = {{TheBloke/WizardLM-7B-uncensored-GPTQ}},
  howpublished = {\url{https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ}},
}

@misc{gpt4_0409,
    author={OpenAI},
  title={GPT-4 Turbo},
  year={2024},
  howpublished ={\url{https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4}},
}

@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={ICML},
  year={2023},
}

@article{zhou2024bileve,
  title={Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature},
  author={Zhou, Tong and Zhao, Xuandong and Xu, Xiaolin and Ren, Shaolei},
  journal={arXiv preprint arXiv:2406.01946},
  year={2024}
}

@article{lin2024pathseeker,
  title={PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach},
  author={Lin, Zhihao and Ma, Wei and Zhou, Mingyi and Zhao, Yanjie and Wang, Haoyu and Liu, Yang and Wang, Jun and Li, Li},
  journal={arXiv preprint arXiv:2409.14177},
  year={2024}
}

@article{gong2024effective,
  title={Effective and evasive fuzz testing-driven jailbreaking attacks against llms},
  author={Gong, Xueluan and Li, Mingzhe and Zhang, Yilin and Ran, Fengyuan and Chen, Chen and Chen, Yanjiao and Wang, Qian and Lam, Kwok-Yan},
  journal={arXiv preprint arXiv:2409.14866},
  year={2024}
}

@article{yan2024parafuzz,
  title={Parafuzz: An interpretability-driven technique for detecting poisoned samples in nlp},
  author={Yan, Lu and Zhang, Zhuo and Tao, Guanhong and Zhang, Kaiyuan and Chen, Xuan and Shen, Guangyu and Zhang, Xiangyu},
  journal={NeurIPS},
  year={2024}
}