\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Atanov et~al.(2019)Atanov, Volokhova, Ashukha, Sosnovik, and
  Vetrov]{atanov2019semi}
Andrei Atanov, Alexandra Volokhova, Arsenii Ashukha, Ivan Sosnovik, and Dmitry
  Vetrov.
\newblock Semi-conditional normalizing flows for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.00505}, 2019.

\bibitem[Baldi et~al.(2016)Baldi, Cranmer, Faucett, Sadowski, and
  Whiteson]{baldi2016parameterized}
Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, and Daniel
  Whiteson.
\newblock Parameterized machine learning for high-energy physics.
\newblock \emph{arXiv preprint arXiv:1601.07913}, 2016.

\bibitem[Behrmann et~al.(2018)Behrmann, Duvenaud, and
  Jacobsen]{behrmann2018invertible}
Jens Behrmann, David Duvenaud, and J{\"o}rn-Henrik Jacobsen.
\newblock Invertible residual networks.
\newblock \emph{arXiv preprint arXiv:1811.00995}, 2018.

\bibitem[Bhattacharyya et~al.(2020)Bhattacharyya, Mahajan, Fritz, Schiele, and
  Roth]{bhattacharyya2020normalizing}
Apratim Bhattacharyya, Shweta Mahajan, Mario Fritz, Bernt Schiele, and Stefan
  Roth.
\newblock Normalizing flows with multi-scale autoregressive priors.
\newblock In \emph{Proceedings of the Conference on Computer Vision and Pattern
  Recognition}, pages 8415--8424, 2020.

\bibitem[Chen et~al.(2020)Chen, Lu, Chenli, Zhu, and Tian]{chen2020vflow}
Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, and Tian Tian.
\newblock Vflow: More expressive generative flows with variational data
  augmentation.
\newblock \emph{arXiv preprint arXiv:2002.09741}, 2020.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and
  Jacobsen]{chen2019residual}
Ricky~TQ Chen, Jens Behrmann, David Duvenaud, and J{\"o}rn-Henrik Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock \emph{arXiv preprint arXiv:1906.02735}, 2019.

\bibitem[Choi et~al.(2018)Choi, Jang, and Alemi]{choi2018waic}
Hyunsun Choi, Eric Jang, and Alexander~A Alemi.
\newblock Waic, but why? generative ensembles for robust anomaly detection.
\newblock \emph{arXiv preprint arXiv:1810.01392}, 2018.

\bibitem[De~Cao et~al.(2019)De~Cao, Titov, and Aziz]{de2019block}
Nicola De~Cao, Ivan Titov, and Wilker Aziz.
\newblock Block neural autoregressive flow.
\newblock \emph{arXiv preprint arXiv:1904.04676}, 2019.

\bibitem[Dinh et~al.(2014)Dinh, Krueger, and Bengio]{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock {NICE}: Non-linear independent components estimation.
\newblock \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using {Real NVP}.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and
  Papamakarios]{durkan2019neural}
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
\newblock Neural spline flows.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7509--7520, 2019.

\bibitem[Finzi et~al.(2019)Finzi, Izmailov, Maddox, Kirichenko, and
  Wilson]{finzi2019invertible}
Marc Finzi, Pavel Izmailov, Wesley Maddox, Polina Kirichenko, and Andrew~Gordon
  Wilson.
\newblock Invertible convolutional networks.
\newblock In \emph{Workshop on Invertible Neural Nets and Normalizing Flows,
  International Conference on Machine Learning}, 2019.

\bibitem[Grathwohl et~al.(2018)Grathwohl, Chen, Betterncourt, Sutskever, and
  Duvenaud]{grathwohl2018ffjord}
Will Grathwohl, Ricky~TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David
  Duvenaud.
\newblock Ffjord: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock \emph{arXiv preprint arXiv:1810.01367}, 2018.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, and
  Dietterich]{hendrycks2018deep}
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.
\newblock Deep anomaly detection with outlier exposure.
\newblock \emph{arXiv preprint arXiv:1812.04606}, 2018.

\bibitem[Ho et~al.(2019)Ho, Chen, Srinivas, Duan, and Abbeel]{ho2019flow++}
Jonathan Ho, Xi~Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock \emph{arXiv preprint arXiv:1902.00275}, 2019.

\bibitem[Hoogeboom et~al.(2019)Hoogeboom, Berg, and
  Welling]{hoogeboom2019emerging}
Emiel Hoogeboom, Rianne van~den Berg, and Max Welling.
\newblock Emerging convolutions for generative normalizing flows.
\newblock \emph{arXiv preprint arXiv:1901.11137}, 2019.

\bibitem[Huang et~al.(2018)Huang, Krueger, Lacoste, and
  Courville]{huang2018neural}
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville.
\newblock Neural autoregressive flows.
\newblock \emph{arXiv preprint arXiv:1804.00779}, 2018.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Izmailov et~al.(2020)Izmailov, Kirichenko, Finzi, and
  Wilson]{izmailov2019semi}
Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew~Gordon Wilson.
\newblock Semi-supervised learning with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Karami et~al.(2019)Karami, Schuurmans, Sohl-Dickstein, Dinh, and
  Duckworth]{karami2019icf}
Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, and Daniel
  Duckworth.
\newblock Invertible convolutional flow.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 5635--5645. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.nips.cc/paper/8801-invertible-convolutional-flow.pdf}.

\bibitem[Kim et~al.(2018)Kim, Lee, Song, Kim, and Yoon]{kim2018flowavenet}
Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon.
\newblock Flowavenet: A generative flow for raw audio.
\newblock \emph{arXiv preprint arXiv:1811.02155}, 2018.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1$\times$1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10215--10224, 2018.

\bibitem[Kobyzev et~al.(2019)Kobyzev, Prince, and
  Brubaker]{kobyzev2019normalizing}
Ivan Kobyzev, Simon Prince, and Marcus~A Brubaker.
\newblock Normalizing flows: Introduction and ideas.
\newblock \emph{arXiv preprint arXiv:1908.09257}, 2019.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Ma et~al.(2019)Ma, Kong, Zhang, and Hovy]{ma2019macow}
Xuezhe Ma, Xiang Kong, Shanghang Zhang, and Eduard Hovy.
\newblock Macow: Masked convolutional generative flow.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5891--5900, 2019.

\bibitem[Mitchell(1980)]{mitchell1980need}
Tom~M Mitchell.
\newblock \emph{The need for biases in learning generalizations}.
\newblock Department of Computer Science, Laboratory for Computer Science
  Research~â€¦, 1980.

\bibitem[Nalisnick et~al.(2018)Nalisnick, Matsukawa, Teh, Gorur, and
  Lakshminarayanan]{nalisnick2018deep}
Eric Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, Dilan Gorur, and Balaji
  Lakshminarayanan.
\newblock Do deep generative models know what they don't know?
\newblock \emph{arXiv preprint arXiv:1810.09136}, 2018.

\bibitem[Nalisnick et~al.(2019)Nalisnick, Matsukawa, Teh, and
  Lakshminarayanan]{nalisnick2019detecting}
Eric Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, and Balaji Lakshminarayanan.
\newblock Detecting out-of-distribution inputs to deep generative models using
  a test for typicality.
\newblock \emph{arXiv preprint arXiv:1906.02994}, 2019.

\bibitem[Oord et~al.(2016)Oord, Kalchbrenner, and Kavukcuoglu]{oord2016pixel}
Aaron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1601.06759}, 2016.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2338--2347, 2017.

\bibitem[Papamakarios et~al.(2019)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2019normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{arXiv preprint arXiv:1912.02762}, 2019.

\bibitem[Prenger et~al.(2019)Prenger, Valle, and
  Catanzaro]{prenger2019waveglow}
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
\newblock Waveglow: A flow-based generative network for speech synthesis.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3617--3621. IEEE, 2019.

\bibitem[Ren et~al.(2019)Ren, Liu, Fertig, Snoek, Poplin, Depristo, Dillon, and
  Lakshminarayanan]{ren2019likelihood}
Jie Ren, Peter~J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo,
  Joshua Dillon, and Balaji Lakshminarayanan.
\newblock Likelihood ratios for out-of-distribution detection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14680--14691, 2019.

\bibitem[Roe et~al.(2005)Roe, Yang, Zhu, Liu, Stancu, and
  McGregor]{roe2005boosted}
Byron~P Roe, Hai-Jun Yang, Ji~Zhu, Yong Liu, Ion Stancu, and Gordon McGregor.
\newblock Boosted decision trees as an alternative to artificial neural
  networks for particle identification.
\newblock \emph{Nuclear Instruments and Methods in Physics Research Section A:
  Accelerators, Spectrometers, Detectors and Associated Equipment},
  543\penalty0 (2-3):\penalty0 577--584, 2005.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Serr{\`a} et~al.(2019)Serr{\`a}, {\'A}lvarez, G{\'o}mez, Slizovskaia,
  N{\'u}{\~n}ez, and Luque]{serra2019input}
Joan Serr{\`a}, David {\'A}lvarez, Vicen{\c{c}} G{\'o}mez, Olga Slizovskaia,
  Jos{\'e}~F N{\'u}{\~n}ez, and Jordi Luque.
\newblock Input complexity and out-of-distribution detection with
  likelihood-based generative models.
\newblock \emph{arXiv preprint arXiv:1909.11480}, 2019.

\bibitem[Song et~al.(2019{\natexlab{a}})Song, Song, and
  Ermon]{song2019unsupervised}
Jiaming Song, Yang Song, and Stefano Ermon.
\newblock Unsupervised out-of-distribution detection with batch normalization.
\newblock \emph{arXiv preprint arXiv:1910.09115}, 2019{\natexlab{a}}.

\bibitem[Song et~al.(2019{\natexlab{b}})Song, Meng, and Ermon]{song2019mintnet}
Yang Song, Chenlin Meng, and Stefano Ermon.
\newblock Mintnet: Building invertible neural networks with masked
  convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11002--11012, 2019{\natexlab{b}}.

\bibitem[Tabak and Turner(2013)]{tabak2013family}
Esteban~G Tabak and Cristina~V Turner.
\newblock A family of nonparametric density estimation algorithms.
\newblock \emph{Communications on Pure and Applied Mathematics}, 66\penalty0
  (2):\penalty0 145--164, 2013.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc~V Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.11946}, 2019.

\bibitem[Theis et~al.(2015)Theis, Oord, and Bethge]{theis2015note}
Lucas Theis, A{\"a}ron van~den Oord, and Matthias Bethge.
\newblock A note on the evaluation of generative models.
\newblock \emph{arXiv preprint arXiv:1511.01844}, 2015.

\bibitem[Torralba et~al.(2008)Torralba, Fergus, and Freeman]{torralba200880}
Antonio Torralba, Rob Fergus, and William~T Freeman.
\newblock 80 million tiny images: A large data set for nonparametric object and
  scene recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 30\penalty0 (11):\penalty0 1958--1970, 2008.

\bibitem[Uria et~al.(2013)Uria, Murray, and Larochelle]{uria2013rnade}
Benigno Uria, Iain Murray, and Hugo Larochelle.
\newblock Rnade: The real-valued neural autoregressive density-estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2175--2183, 2013.

\bibitem[Wilson and Izmailov(2020)]{wilson2020bayesian}
Andrew~Gordon Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{arXiv preprint arXiv:2002.08791}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Liu, Chen, Wang, Liu, Li, Wei, and
  Chen]{zhang2020out}
Yufeng Zhang, Wanwei Liu, Zhenbang Chen, Ji~Wang, Zhiming Liu, Kenli Li,
  Hongmei Wei, and Zuoning Chen.
\newblock Out-of-distribution detection with distance guarantee in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:2002.03328}, 2020.

\end{thebibliography}
