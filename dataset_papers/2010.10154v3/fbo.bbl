\begin{thebibliography}{10}

\bibitem{anguita2013public}
D.~Anguita, A.~Ghio, L.~Oneto, X.~Parra, and J.~L. Reyes-Ortiz.
\newblock A public domain dataset for human activity recognition using
  smartphones.
\newblock In {\em Proc. {ESANN}}, 2013.

\bibitem{bogunovic2018adversarially}
I.~Bogunovic, J.~Scarlett, S.~Jegelka, and V.~Cevher.
\newblock Adversarially robust optimization with gaussian processes.
\newblock In {\em Proc. {NeurIPS}}, pages 5760--5770, 2018.

\bibitem{chang2019cronus}
H.~Chang, V.~Shejwalkar, R.~Shokri, and A.~Houmansadr.
\newblock Cronus: Robust and heterogeneous collaborative learning with
  black-box knowledge transfer.
\newblock {arXiv}:1912.11279, 2019.

\bibitem{chapelle2011empirical}
O.~Chapelle and L.~Li.
\newblock An empirical evaluation of {Thompson} sampling.
\newblock In {\em Proc. {NeurIPS}}, pages 2249--2257, 2011.

\bibitem{Chen13}
J.~Chen, N.~Cao, B.~K.~H. Low, R.~Ouyang, C.~K.-Y. Tan, and P.~Jaillet.
\newblock Parallel {Gaussian} process regression with low-rank covariance
  matrix approximations.
\newblock In {\em Proc. UAI}, pages 152--161, 2013.

\bibitem{LowTASE15}
J.~Chen, B.~K.~H. Low, P.~Jaillet, and Y.~Yao.
\newblock Gaussian process decentralized data fusion and active sensing for
  spatiotemporal traffic modeling and prediction in mobility-on-demand systems.
\newblock {\em {IEEE} Trans. Autom. Sci. Eng.}, 12:901--921, 2015.

\bibitem{LowRSS13}
J.~Chen, B.~K.~H. Low, and C.~K.-Y. Tan.
\newblock {Gaussian} process-based decentralized data fusion and active sensing
  for mobility-on-demand system.
\newblock In {\em Proc. RSS}, 2013.

\bibitem{LowUAI12}
J.~Chen, B.~K.~H. Low, C.~K.-Y. Tan, A.~Oran, P.~Jaillet, J.~M. Dolan, and
  G.~S. Sukhatme.
\newblock Decentralized data fusion and active sensing with mobile sensors for
  modeling and predicting spatiotemporal traffic phenomena.
\newblock In {\em Proc. UAI}, pages 163--173, 2012.

\bibitem{chowdhury2017kernelized}
S.~R. Chowdhury and A.~Gopalan.
\newblock On kernelized multi-armed bandits.
\newblock In {\em Proc. {ICML}}, pages 844--853, 2017.

\bibitem{contal2013parallel}
E.~Contal, D.~Buffoni, A.~Robicquet, and N.~Vayatis.
\newblock Parallel {Gaussian} process optimization with upper confidence bound
  and pure exploration.
\newblock In {\em Proc. {ECML/PKDD}}, pages 225--240, 2013.

\bibitem{dai2020r2}
Z.~Dai, Y.~Chen, K.~H. Low, P.~Jaillet, and T.-H. Ho.
\newblock {R2-B2}: Recursive reasoning-based {Bayesian} optimization for
  no-regret learning in games.
\newblock In {\em Proc. {ICML}}, 2020.

\bibitem{dai2019bayesian}
Z.~Dai, H.~Yu, B.~K.~H. Low, and P.~Jaillet.
\newblock Bayesian optimization meets {Bayesian} optimal stopping.
\newblock In {\em Proc. {ICML}}, pages 1496--1506, 2019.

\bibitem{daxberger2017distributed}
E.~A. Daxberger and B.~K.~H. Low.
\newblock Distributed batch {Gaussian} process optimization.
\newblock In {\em Proc. {ICML}}, pages 951--960, 2017.

\bibitem{desautels2014parallelizing}
T.~Desautels, A.~Krause, and J.~W. Burdick.
\newblock Parallelizing exploration-exploitation tradeoffs in {Gaussian}
  process bandit optimization.
\newblock {\em Journal of Machine Learning Research}, 15:3873--3923, 2014.

\bibitem{dwork2006calibrating}
C.~Dwork, F.~McSherry, K.~Nissim, and A.~Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In {\em Proc. {TCC}}, pages 265--284. Springer, 2006.

\bibitem{feurer2018scalable}
M.~Feurer, B.~Letham, and E.~Bakshy.
\newblock Scalable meta-learning for {Bayesian} optimization using
  ranking-weighted {Gaussian} process ensembles.
\newblock In {\em Proc. {ICML} Workshop on Automatic Machine Learning}, 2018.

\bibitem{hernandez2017parallel}
J.~M. Hern{\'a}ndez-Lobato, J.~Requeima, E.~O. Pyzer-Knapp, and
  A.~Aspuru-Guzik.
\newblock Parallel and distributed {Thompson} sampling for large-scale
  accelerated exploration of chemical space.
\newblock In {\em Proc. {ICML}}, 2017.

\bibitem{MinhAAAI17}
Q.~M. Hoang, T.~N. Hoang, and B.~K.~H. Low.
\newblock A generalized stochastic variational {Bayesian} hyperparameter
  learning framework for sparse spectrum {Gaussian} process regression.
\newblock In {\em Proc. {AAAI}}, pages 2007--2014, 2017.

\bibitem{HoangICML19}
Q.~M. Hoang, T.~N. Hoang, B.~K.~H. Low, and C.~Kingsford.
\newblock Collective model fusion for multiple black-box experts.
\newblock In {\em Proc. ICML}, pages 2742--2750, 2019.

\bibitem{NghiaICML16}
T.~N. Hoang, Q.~M. Hoang, and B.~K.~H. Low.
\newblock A unifying framework of anytime sparse {Gaussian} process regression
  models with stochastic variational inference for big data.
\newblock In {\em Proc. {ICML}}, pages 569--578, 2015.

\bibitem{HoangICML16}
T.~N. Hoang, Q.~M. Hoang, and B.~K.~H. Low.
\newblock A distributed variational inference framework for unifying parallel
  sparse {Gaussian} process regression models.
\newblock In {\em Proc. ICML}, pages 382--391, 2016.

\bibitem{NghiaAAAI18}
T.~N. Hoang, Q.~M. Hoang, and B.~K.~H. Low.
\newblock Decentralized high-dimensional {Bayesian} optimization with factor
  graphs.
\newblock In {\em Proc. {AAAI}}, pages 3231--3238, 2018.

\bibitem{NghiaAAAI19}
T.~N. Hoang, Q.~M. Hoang, B.~K.~H. Low, and J.~P. How.
\newblock Collective online learning of {Gaussian} processes in massive
  multi-agent systems.
\newblock In {\em Proc. {AAAI}}, pages 7850--7857, 2019.

\bibitem{kairouz2019advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock {arXiv}:1912.04977, 2019.

\bibitem{kandasamy2016gaussian}
K.~Kandasamy, G.~Dasarathy, J.~B. Oliva, J.~Schneider, and B.~P{\'o}czos.
\newblock {Gaussian} process bandit optimisation with multi-fidelity
  evaluations.
\newblock In {\em Proc. {NeurIPS}}, pages 992--1000, 2016.

\bibitem{kandasamy2018parallelised}
K.~Kandasamy, A.~Krishnamurthy, J.~Schneider, and B.~P{\'o}czos.
\newblock Parallelised {Bayesian} optimisation via {Thompson} sampling.
\newblock In {\em Proc. {AISTATS}}, pages 133--142, 2018.

\bibitem{kharkovskii2020private}
D.~Kharkovskii, Z.~Dai, and B.~K.~H. Low.
\newblock Private outsourced {Bayesian} optimization.
\newblock In {\em Proc. {ICML}}, 2020.

\bibitem{dmitrii20a}
D.~Kharkovskii, C.~K. Ling, and B.~K.~H. Low.
\newblock Nonmyopic {Gaussian} process optimization with macro-actions.
\newblock In {\em Proc. AISTATS}, pages 4593--4604, 2020.

\bibitem{kusner2015differentially}
M.~Kusner, J.~Gardner, R.~Garnett, and K.~Weinberger.
\newblock Differentially private {Bayesian} optimization.
\newblock In {\em Proc. {ICML}}, pages 918--927, 2015.

\bibitem{li2019federated2}
Q.~Li, Z.~Wen, and B.~He.
\newblock Federated learning systems: Vision, hype and reality for data privacy
  and protection.
\newblock {arXiv}:1907.09693, 2019.

\bibitem{li2020practical}
Q.~Li, Z.~Wen, and B.~He.
\newblock Practical federated gradient boosting decision trees.
\newblock In {\em Proc. {AAAI}}, pages 4642--4649, 2020.

\bibitem{li2020privacy}
Q.~Li, Z.~Wu, Z.~Wen, and B.~He.
\newblock Privacy-preserving gradient boosting decision trees.
\newblock In {\em Proc. {AAAI}}, pages 784--791, 2020.

\bibitem{li2019federated}
T.~Li, A.~K. Sahu, A.~Talwalkar, and V.~Smith.
\newblock Federated learning: {Challenges}, methods, and future directions.
\newblock {arXiv}:1908.07873, 2019.

\bibitem{li2018federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock {arXiv}:1812.06127, 2018.

\bibitem{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang.
\newblock On the convergence of {FedAvg} on non-iid data.
\newblock In {\em Proc. {ICLR}}, 2020.

\bibitem{ling16}
C.~K. Ling, K.~H. Low, and P.~Jaillet.
\newblock {Gaussian} process planning with {Lipschitz} continuous reward
  functions: Towards unifying {Bayesian} optimization, active learning, and
  beyond.
\newblock In {\em Proc. {AAAI}}, pages 1860--1866, 2016.

\bibitem{LowECML14a}
B.~K.~H. Low, N.~Xu, J.~Chen, K.~K. Lim, and E.~B. {\"{O}zg\"{u}l}.
\newblock Generalized online sparse {Gaussian} processes with application to
  persistent mobile robot localization.
\newblock In {\em Proc. {ECML/PKDD Nectar Track}}, pages 499--503, 2014.

\bibitem{low15}
B.~K.~H. Low, J.~Yu, J.~Chen, and P.~Jaillet.
\newblock Parallel {Gaussian} process regression for big data: Low-rank
  representation meets {M}arkov approximation.
\newblock In {\em Proc. {AAAI}}, pages 2821--2827, 2015.

\bibitem{mcmahan2016communication}
H.~B. McMahan, E.~Moore, D.~Ramage, S.~Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Proc. {AISTATS}}, 2017.

\bibitem{mutny2018efficient}
M.~Mutny and A.~Krause.
\newblock Efficient high dimensional {Bayesian} optimization with additivity
  and quadrature {Fourier} features.
\newblock In {\em Proc. {NeurIPS}}, pages 9005--9016, 2018.

\bibitem{Ruofei18}
R.~Ouyang and B.~K.~H. Low.
\newblock Gaussian process decentralized data fusion meets transfer learning in
  large-scale distributed cooperative perception.
\newblock In {\em Proc. AAAI}, pages 3876--3883, 2018.

\bibitem{poloczek2017multi}
M.~Poloczek, J.~Wang, and P.~Frazier.
\newblock Multi-information source optimization.
\newblock In {\em Proc. {NeurIPS}}, pages 4288--4298, 2017.

\bibitem{rahimi2008random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Proc. {NeurIPS}}, pages 1177--1184, 2008.

\bibitem{rahman2015unintrusive}
S.~A. Rahman, C.~Merck, Y.~Huang, and S.~Kleinberg.
\newblock Unintrusive eating recognition using {Google} glass.
\newblock In {\em Proc. {PervasiveHealth}}, pages 108--111, 2015.

\bibitem{rasmussen2004gaussian}
C.~E. Rasmussen and C.~K.~I. Williams.
\newblock {\em {Gaussian Processes} for {Machine Learning}}.
\newblock MIT Press, 2006.

\bibitem{russo2014learning}
D.~Russo and B.~Van~Roy.
\newblock Learning to optimize via posterior sampling.
\newblock {\em Mathematics of Operations Research}, 39(4):1221--1243, 2014.

\bibitem{russo2017tutorial}
D.~Russo, B.~Van~Roy, A.~Kazerouni, I.~Osband, and Z.~Wen.
\newblock A tutorial on {Thompson} sampling.
\newblock {arXiv}:1707.02038, 2017.

\bibitem{sessa2019no}
P.~G. Sessa, I.~Bogunovic, M.~Kamgarpour, and A.~Krause.
\newblock No-regret learning in unknown games with correlated payoffs.
\newblock In {\em Proc. {NeurIPS}}, 2019.

\bibitem{shahriari2016taking}
B.~Shahriari, K.~Swersky, Z.~Wang, R.~P. Adams, and N.~{de Freitas}.
\newblock Taking the human out of the loop: A review of {Bayesian}
  optimization.
\newblock {\em Proceedings of the IEEE}, 104(1):148--175, 2016.

\bibitem{Rachael20}
R.~H.~L. Sim, Y.~Zhang, M.~C. Chan, and B.~K.~H. Low.
\newblock Collaborative machine learning with incentive-aware model rewards.
\newblock In {\em Proc. ICML}, 2020.

\bibitem{smith2017federated}
V.~Smith, C.-K. Chiang, M.~Sanjabi, and A.~S. Talwalkar.
\newblock Federated multi-task learning.
\newblock In {\em Proc. {NeurIPS}}, pages 4424--4434, 2017.

\bibitem{srinivas2009gaussian}
N.~Srinivas, A.~Krause, S.~M. Kakade, and M.~Seeger.
\newblock {Gaussian} process optimization in the bandit setting: No regret and
  experimental design.
\newblock In {\em Proc. {ICML}}, pages 1015--1022, 2010.

\bibitem{teng20}
T.~Teng, J.~Chen, Y.~Zhang, and B.~K.~H. Low.
\newblock Scalable variational {Bayesian} kernel selection for sparse
  {Gaussian} process regression.
\newblock In {\em Proc. {AAAI}}, pages 5997--6004, 2020.

\bibitem{thompson1933likelihood}
W.~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3/4):285--294, 1933.

\bibitem{wistuba2018scalable}
M.~Wistuba, N.~Schilling, and L.~Schmidt-Thieme.
\newblock Scalable {Gaussian} process-based transfer surrogates for
  hyperparameter optimization.
\newblock {\em Machine Learning}, 107(1):43--78, 2018.

\bibitem{wu2020practical}
J.~Wu, S.~Toscano-Palmerin, P.~I. Frazier, and A.~G. Wilson.
\newblock Practical multi-fidelity {Bayesian} optimization for hyperparameter
  tuning.
\newblock In {\em Proc. {UAI}}, pages 788--798, 2020.

\bibitem{LowAAAI14}
N.~Xu, B.~K.~H. Low, J.~Chen, K.~K. Lim, and E.~B. {\"{O}zg\"{u}l}.
\newblock {GP-Localize}: Persistent mobile robot localization using online
  sparse {Gaussian} process observation model.
\newblock In {\em Proc. {AAAI}}, pages 2585--2592, 2014.

\bibitem{xue2007multi}
Y.~Xue, X.~Liao, L.~Carin, and B.~Krishnapuram.
\newblock Multi-task learning for classification with dirichlet process priors.
\newblock {\em Journal of Machine Learning Research}, 8(Jan):35--63, 2007.

\bibitem{Haibin19}
H.~Yu, Y.~Chen, Z.~Dai, B.~K.~H. Low, and P.~Jaillet.
\newblock Implicit posterior variational inference for deep {Gaussian}
  processes.
\newblock In {\em Proc. {NeurIPS}}, pages 14475--14486, 2019.

\bibitem{HaibinAPP}
H.~Yu, T.~N. Hoang, B.~K.~H. Low, and P.~Jaillet.
\newblock Stochastic variational inference for {Bayesian} sparse {Gaussian}
  process regression.
\newblock In {\em Proc. {IJCNN}}, 2019.

\bibitem{yu2015predicting}
S.~Yu, F.~Farooq, A.~Van~Esbroeck, G.~Fung, V.~Anand, and B.~Krishnapuram.
\newblock Predicting readmission risk with institution-specific prediction
  models.
\newblock {\em Artificial Intelligence in Medicine}, 65(2):89--96, 2015.

\bibitem{zhang2020bayesian}
Y.~Zhang, Z.~Dai, and B.~K.~H. Low.
\newblock Bayesian optimization with binary auxiliary information.
\newblock In {\em Proc. {UAI}}, pages 1222--1232, 2020.

\bibitem{zhang2017information}
Y.~Zhang, T.~N. Hoang, B.~K.~H. Low, and M.~Kankanhalli.
\newblock Information-based multi-fidelity {Bayesian} optimization.
\newblock In {\em Proc. {NeurIPS} Workshop on {Bayesian} Optimization}, 2017.

\end{thebibliography}
