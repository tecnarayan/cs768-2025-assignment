\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CQsmMYmlP5T}.

\bibitem[Aitchison et~al.(2021)Aitchison, Yang, and Ober]{aitchison2021deep}
Laurence Aitchison, Adam Yang, and Sebastian~W Ober.
\newblock Deep kernel processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  130--140. PMLR, 2021.

\bibitem[Altintas et~al.(2023)Altintas, Bachmann, Noci, and
  Hofmann]{altintas2023disentangling}
Gul~Sena Altintas, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann.
\newblock Disentangling linear mode-connectivity.
\newblock \emph{arXiv preprint arXiv:2312.09832}, 2023.

\bibitem[Ashmore and Gashler(2015)]{ashmore2015method}
Stephen Ashmore and Michael Gashler.
\newblock A method for finding similarity between multi-layer perceptrons by
  forward bipartite alignment.
\newblock In \emph{2015 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--7. IEEE, 2015.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Badrinarayanan et~al.(2015)Badrinarayanan, Mishra, and
  Cipolla]{badrinarayanan2015understanding}
Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla.
\newblock Understanding symmetries in deep networks.
\newblock \emph{arXiv preprint arXiv:1511.01029}, 2015.

\bibitem[B{\"o}kman and Kahl(2023)]{bokman2023investigating}
Georg B{\"o}kman and Fredrik Kahl.
\newblock Investigating how re{LU}-networks encode symmetries.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=8lbFwpebeu}.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  933--941. PMLR, 2017.

\bibitem[DeVries and Taylor(2017)]{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1019--1028. PMLR, 2017.

\bibitem[Eilertsen et~al.(2020)Eilertsen, J{\"o}nsson, Ropinski, Unger, and
  Ynnerman]{eilertsen2020classifying}
Gabriel Eilertsen, Daniel J{\"o}nsson, Timo Ropinski, Jonas Unger, and Anders
  Ynnerman.
\newblock Classifying the classifier: dissecting the weight space of neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.05688}, 2020.

\bibitem[Entezari et~al.(2022)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2022the}
Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=dNigytemkL}.

\bibitem[Ferbach et~al.(2024)Ferbach, Goujaud, Gidel, and
  Dieuleveut]{ferbach2024proving}
Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, and Aymeric Dieuleveut.
\newblock Proving linear mode connectivity of neural networks via optimal
  transport.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3853--3861. PMLR, 2024.

\bibitem[Fey and Lenssen(2019)]{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock \emph{arXiv preprint arXiv:1903.02428}, 2019.

\bibitem[Finzi et~al.(2021)Finzi, Welling, and Wilson]{finzi2021practical}
Marc Finzi, Max Welling, and Andrew~Gordon Wilson.
\newblock A practical method for constructing equivariant multilayer
  perceptrons for arbitrary matrix groups.
\newblock In \emph{International conference on machine learning}, pages
  3318--3328. PMLR, 2021.

\bibitem[Frankle(2020)]{frankle2020revisiting}
Jonathan Frankle.
\newblock Revisiting "qualitatively characterizing neural network optimization
  problems", 2020.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020.

\bibitem[Gegout et~al.(1995)Gegout, Girau, and Rossi]{gegout1995mathematical}
Cedric Gegout, Bernard Girau, and Fabrice Rossi.
\newblock \emph{A mathematical model for feed-forward neural networks:
  theoretical description and parallel applications.}
\newblock PhD thesis, Laboratoire de l'informatique du parall{\'e}lisme, 1995.

\bibitem[Godfrey et~al.(2022)Godfrey, Brown, Emerson, and
  Kvinge]{godfrey2022symmetries}
Charles Godfrey, Davis Brown, Tegan Emerson, and Henry Kvinge.
\newblock On the symmetries of deep learning models and their internal
  representations.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 11893--11905, 2022.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Ian~J Goodfellow, Oriol Vinyals, and Andrew~M Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{ICLR}, 2015.

\bibitem[Hamilton(2020)]{hamilton2020graph}
William~L Hamilton.
\newblock \emph{Graph representation learning}.
\newblock Morgan \& Claypool Publishers, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hecht-Nielsen(1990)]{hecht1990algebraic}
Robert Hecht-Nielsen.
\newblock On the algebraic structure of feedforward network weight spaces.
\newblock In \emph{Advanced Neural Computers}, pages 129--135. Elsevier, 1990.

\bibitem[Hecht-Nielsen(1992)]{hecht1992theory}
Robert Hecht-Nielsen.
\newblock Theory of the backpropagation neural network.
\newblock In \emph{Neural networks for perception}, pages 65--93. Elsevier,
  1992.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 22118--22133, 2020.

\bibitem[Imfeld et~al.(2023)Imfeld, Graldi, Giordano, Hofmann, Anagnostidis,
  and Singh]{imfeld2023transformer}
Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris
  Anagnostidis, and Sidak~Pal Singh.
\newblock Transformer fusion with optimal transport.
\newblock \emph{arXiv preprint arXiv:2310.05719}, 2023.

\bibitem[Jordan et~al.(2023)Jordan, Sedghi, Saukh, Entezari, and
  Neyshabur]{jordan2023repair}
Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur.
\newblock {REPAIR}: {RE}normalizing permuted activations for interpolation
  repair.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=gU5sJ6ZggcX}.

\bibitem[Jospin et~al.(2022)Jospin, Laga, Boussaid, Buntine, and
  Bennamoun]{jospin2022hands}
Laurent~Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed
  Bennamoun.
\newblock Hands-on bayesian neural networksâ€”a tutorial for deep learning
  users.
\newblock \emph{IEEE Computational Intelligence Magazine}, 17\penalty0
  (2):\penalty0 29--48, 2022.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kofinas et~al.(2024)Kofinas, Knyazev, Zhang, Chen, Burghouts, Gavves,
  Snoek, and Zhang]{kofinas2024graph}
Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan~J Burghouts,
  Efstratios Gavves, Cees~GM Snoek, and David~W Zhang.
\newblock Graph neural networks for learning equivariant representations of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2403.12143}, 2024.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Kurle et~al.(2022)Kurle, Herbrich, Januschowski, Wang, and
  Gasthaus]{kurle2022detrimental}
Richard Kurle, Ralf Herbrich, Tim Januschowski, Yuyang~Bernie Wang, and Jan
  Gasthaus.
\newblock On the detrimental effect of invariances in the likelihood for
  variational inference.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 4531--4542, 2022.

\bibitem[Langosco et~al.(2024)Langosco, Alex, Baker, Quarel, Bradley, and
  Krueger]{langosco2024towards}
Lauro Langosco, Neel Alex, William Baker, David~John Quarel, Herbie Bradley,
  and David Krueger.
\newblock Towards meta-models for automated interpretability, 2024.
\newblock URL \url{https://openreview.net/forum?id=fM1ETm3ssl}.

\bibitem[Laurent et~al.(2024)Laurent, Aldea, and Franchi]{laurent2024a}
Olivier Laurent, Emanuel Aldea, and Gianni Franchi.
\newblock A symmetry-aware exploration of bayesian neural network posteriors.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=FOSBQuXgAq}.

\bibitem[Leclerc et~al.(2023)Leclerc, Ilyas, Engstrom, Park, Salman, and
  M{\k{a}}dry]{leclerc2023ffcv}
Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung~Min Park, Hadi Salman,
  and Aleksander M{\k{a}}dry.
\newblock Ffcv: Accelerating training by removing data bottlenecks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12011--12020, 2023.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lim et~al.(2024)Lim, Maron, Law, Lorraine, and Lucas]{lim2023graph}
Derek Lim, Haggai Maron, Marc~T. Law, Jonathan Lorraine, and James Lucas.
\newblock Graph metanetworks for processing diverse neural architectures.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=ijK5hyxs0n}.

\bibitem[Loshchilov and Hutter(2018)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lucas et~al.(2021)Lucas, Bae, Zhang, Fort, Zemel, and
  Grosse]{lucas2021analyzing}
James Lucas, Juhan Bae, Michael~R Zhang, Stanislav Fort, Richard Zemel, and
  Roger Grosse.
\newblock Analyzing monotonic linear interpolation in neural network loss
  landscapes.
\newblock \emph{arXiv preprint arXiv:2104.11044}, 2021.

\bibitem[Maron et~al.(2019)Maron, Fetaya, Segol, and
  Lipman]{maron2019universality}
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman.
\newblock On the universality of invariant networks.
\newblock In \emph{International conference on machine learning}, pages
  4363--4371. PMLR, 2019.

\bibitem[Milsom et~al.(2024)Milsom, Anson, and
  Aitchison]{milsom2024convolutional}
Edward Milsom, Ben Anson, and Laurence Aitchison.
\newblock Convolutional deep kernel machines.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=1oqedRt6Z7}.

\bibitem[Mirzadeh et~al.(2021)Mirzadeh, Farajtabar, Gorur, Pascanu, and
  Ghasemzadeh]{mirzadeh2021linear}
Seyed~Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and
  Hassan Ghasemzadeh.
\newblock Linear mode connectivity in multitask and continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Fmg_fQYUejf}.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~29, 2015.

\bibitem[Navon et~al.(2023{\natexlab{a}})Navon, Shamsian, Achituve, Fetaya,
  Chechik, and Maron]{navon2023equivariant}
Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, and Haggai
  Maron.
\newblock Equivariant architectures for learning in deep weight spaces.
\newblock In \emph{International Conference on Machine Learning}, pages
  25790--25816. PMLR, 2023{\natexlab{a}}.

\bibitem[Navon et~al.(2023{\natexlab{b}})Navon, Shamsian, Fetaya, Chechik, Dym,
  and Maron]{navon2023equivariant_alignment}
Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, and Haggai
  Maron.
\newblock Equivariant deep weight space alignment.
\newblock \emph{arXiv preprint arXiv:2310.13397}, 2023{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{a}})Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
Behnam Neyshabur, Russ~R Salakhutdinov, and Nati Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 28,
  2015{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{b}})Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on learning theory}, pages 1376--1401. PMLR,
  2015{\natexlab{b}}.

\bibitem[Papamarkou et~al.(2022)Papamarkou, Hinkle, Young, and
  Womble]{papamarkou2022challenges}
Theodore Papamarkou, Jacob Hinkle, M~Todd Young, and David Womble.
\newblock Challenges in markov chain monte carlo for bayesian neural networks.
\newblock \emph{Statistical Science}, 37\penalty0 (3):\penalty0 425--442, 2022.

\bibitem[Papamarkou et~al.(2024)Papamarkou, Skoularidou, Palla, Aitchison,
  Arbel, Dunson, Filippone, Fortuin, Hennig, Hubin,
  et~al.]{papamarkou2024position}
Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison,
  Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp
  Hennig, Aliaksandr Hubin, et~al.
\newblock Position paper: Bayesian deep learning in the age of large-scale ai.
\newblock \emph{arXiv preprint arXiv:2402.00809}, 2024.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pe{\~n}a et~al.(2023)Pe{\~n}a, Medeiros, Dubail, Aminbeidokhti,
  Granger, and Pedersoli]{pena2023re}
Fidel A~Guerrero Pe{\~n}a, Heitor~Rapela Medeiros, Thomas Dubail, Masih
  Aminbeidokhti, Eric Granger, and Marco Pedersoli.
\newblock Re-basin via implicit sinkhorn differentiation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 20237--20246, 2023.

\bibitem[Pittorino et~al.(2022)Pittorino, Ferraro, Perugini, Feinauer,
  Baldassi, and Zecchina]{pittorino2022deep}
Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer,
  Carlo Baldassi, and Riccardo Zecchina.
\newblock Deep networks on toroids: removing symmetries reveals the structure
  of flat regions in the landscape geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  17759--17781. PMLR, 2022.

\bibitem[Pourzanjani et~al.(2017)Pourzanjani, Jiang, and
  Petzold]{pourzanjani2017improving}
Arya~A Pourzanjani, Richard~M Jiang, and Linda~R Petzold.
\newblock Improving the identifiability of neural networks for bayesian
  inference.
\newblock In \emph{NIPS workshop on bayesian deep learning}, volume~4, page~31,
  2017.

\bibitem[Qu and Horvath(2024)]{qu2024rethink}
Xingyu Qu and Samuel Horvath.
\newblock Rethink model re-basin and the linear mode connectivity.
\newblock \emph{arXiv preprint arXiv:2402.05966}, 2024.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le]{ramachandran2017searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V. Le.
\newblock Searching for activation functions, 2017.

\bibitem[Shamsian et~al.(2024)Shamsian, Navon, Zhang, Zhang, Fetaya, Chechik,
  and Maron]{shamsian2024improved}
Aviv Shamsian, Aviv Navon, David~W Zhang, Yan Zhang, Ethan Fetaya, Gal Chechik,
  and Haggai Maron.
\newblock Improved generalization of weight space networks via augmentations.
\newblock \emph{arXiv preprint arXiv:2402.04081}, 2024.

\bibitem[Singh and Jaggi(2020)]{singh2020model}
Sidak~Pal Singh and Martin Jaggi.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 22045--22055, 2020.

\bibitem[Stoica et~al.(2024)Stoica, Bolya, Bjorner, Ramesh, Hearn, and
  Hoffman]{stoica2024zipit}
George Stoica, Daniel Bolya, Jakob~Brandt Bjorner, Pratik Ramesh, Taylor Hearn,
  and Judy Hoffman.
\newblock Zipit! merging models from different tasks without training.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=LEYUkvdUhq}.

\bibitem[Sussmann(1992)]{sussmann1992uniqueness}
H{\'e}ctor~J Sussmann.
\newblock Uniqueness of the weights for minimal feedforward nets with a given
  input-output map.
\newblock \emph{Neural networks}, 5\penalty0 (4):\penalty0 589--593, 1992.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and
  Lai]{tatro2020optimizing}
Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and
  Rongjie Lai.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15300--15311, 2020.

\bibitem[Tomczak et~al.(2020)Tomczak, Swaroop, and Turner]{tomczak2020elrg}
Marcin Tomczak, Siddharth Swaroop, and Richard Turner.
\newblock Efficient low rank gaussian variational inference for neural
  networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 4610--4622. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/310cc7ca5a76a446f85c1a0d641ba96d-Paper.pdf}.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and
  Tolstikhin]{unterthiner2020predicting}
Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya
  Tolstikhin.
\newblock Predicting neural network accuracy from weights.
\newblock \emph{arXiv preprint arXiv:2002.11448}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Verma and Elbayad(2024)]{verma2024merging}
Neha Verma and Maha Elbayad.
\newblock Merging text transformer models from different initializations.
\newblock \emph{arXiv preprint arXiv:2403.00986}, 2024.

\bibitem[Vlaar and Frankle(2022)]{vlaar2022what}
Tiffany~J. Vlaar and Jonathan Frankle.
\newblock What can linear interpolation of neural network loss landscapes tell
  us?
\newblock \emph{ICML}, 2022.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{wang2020federated}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
  Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock \emph{arXiv preprint arXiv:2002.06440}, 2020.

\bibitem[Wang et~al.(2023)Wang, Wang, Zhou, and Ge]{wang2023plateau}
Xiang Wang, Annie~N. Wang, Mo~Zhou, and Rong Ge.
\newblock Plateau in monotonic linear interpolation --- a ''biased'' view of
  loss landscape for deep networks.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=z289SIQOQna}.

\bibitem[Wiese et~al.(2023)Wiese, Wimmer, Papamarkou, Bischl, G{\"u}nnemann,
  and R{\"u}gamer]{wiese2023towards}
Jonas~Gregor Wiese, Lisa Wimmer, Theodore Papamarkou, Bernd Bischl, Stephan
  G{\"u}nnemann, and David R{\"u}gamer.
\newblock Towards efficient mcmc sampling in bayesian neural networks by
  exploiting symmetry.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 459--474. Springer, 2023.

\bibitem[Xiao et~al.(2023)Xiao, Liu, and Bamler]{xiao2023compact}
Tim~Z Xiao, Weiyang Liu, and Robert Bamler.
\newblock A compact representation for bayesian neural networks by removing
  permutation symmetry.
\newblock \emph{arXiv preprint arXiv:2401.00611}, 2023.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2019how}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryGs6iA5Km}.

\bibitem[Yang et~al.(2023)Yang, Robeyns, Milsom, Anson, Schoots, and
  Aitchison]{yang2023theory}
Adam~X Yang, Maxime Robeyns, Edward Milsom, Ben Anson, Nandi Schoots, and
  Laurence Aitchison.
\newblock A theory of representation learning gives a deep generalisation of
  kernel methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  39380--39415. PMLR, 2023.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{Yun2020Are}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Yunis et~al.(2022)Yunis, Patel, Savarese, Vardi, Frankle, Walter,
  Livescu, and Maire]{yunis2022convexity}
David Yunis, Kumar~Kshitij Patel, Pedro Henrique~Pamplona Savarese, Gal Vardi,
  Jonathan Frankle, Matthew Walter, Karen Livescu, and Michael Maire.
\newblock On convexity and linear mode connectivity in neural networks.
\newblock In \emph{OPT 2022: Optimization for Machine Learning (NeurIPS 2022
  Workshop)}, 2022.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{zaheer2017deep}
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ~R
  Salakhutdinov, and Alexander~J Smola.
\newblock Deep sets.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zhao et~al.(2022)Zhao, Ganev, Walters, Yu, and
  Dehmamy]{zhao2022symmetries}
Bo~Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmamy.
\newblock Symmetries, flat minima, and the conserved quantities of gradient
  flow.
\newblock \emph{arXiv preprint arXiv:2210.17216}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Dehmamy, Walters, and
  Yu]{zhao2023understanding}
Bo~Zhao, Nima Dehmamy, Robin Walters, and Rose Yu.
\newblock Understanding mode connectivity via parameter space symmetry.
\newblock In \emph{UniReps: the First Workshop on Unifying Representations in
  Neural Models}, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Gower, Walters, and Yu]{zhao2024improving}
Bo~Zhao, Robert~M. Gower, Robin Walters, and Rose Yu.
\newblock Improving convergence and generalization using parameter symmetries.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=L0r0GphlIL}.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Yang, Burns, Cardace, Jiang,
  Sokota, Kolter, and Finn]{zhou2023permutation}
Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace, Yiding Jiang, Samuel
  Sokota, J~Zico Kolter, and Chelsea Finn.
\newblock Permutation equivariant neural functionals.
\newblock \emph{Advances in Neural Information Processing Systems}, 36,
  2023{\natexlab{a}}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Yang, Jiang, Burns, Xu, Sokota,
  Kolter, and Finn]{zhou2023neural}
Allan Zhou, Kaien Yang, Yiding Jiang, Kaylee Burns, Winnie Xu, Samuel Sokota,
  J~Zico Kolter, and Chelsea Finn.
\newblock Neural functional transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36,
  2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2024)Zhou, Finn, and Harrison]{zhou2024universal}
Allan Zhou, Chelsea Finn, and James Harrison.
\newblock Universal neural functionals.
\newblock \emph{arXiv preprint arXiv:2402.05232}, 2024.

\bibitem[Ziyin(2023)]{ziyin2023symmetry}
Liu Ziyin.
\newblock Symmetry leads to structured constraint of learning.
\newblock \emph{arXiv preprint arXiv:2309.16932}, 2023.

\end{thebibliography}
