\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acun et~al.(2021)Acun, Murphy, Wang, Nie, Wu, and
  Hazelwood]{acun2021understanding}
B.~Acun, M.~Murphy, X.~Wang, J.~Nie, C.-J. Wu, and K.~Hazelwood.
\newblock Understanding training efficiency of deep learning recommendation
  models at scale.
\newblock In \emph{2021 IEEE International Symposium on High-Performance
  Computer Architecture (HPCA)}, pages 802--814. IEEE, 2021.

\bibitem[Chen et~al.(2021)Chen, Wang, Lu, Han, Lv, Min, Cai, Zhang, Fan, Li,
  et~al.]{chen2021fangorn}
Y.~Chen, J.~Wang, Y.~Lu, Y.~Han, Z.~Lv, X.~Min, H.~Cai, W.~Zhang, H.~Fan,
  C.~Li, et~al.
\newblock Fangorn: adaptive execution framework for heterogeneous workloads on
  shared clusters.
\newblock \emph{Proceedings of the VLDB Endowment}, 14\penalty0 (12):\penalty0
  2972--2985, 2021.

\bibitem[Cheng et~al.(2016)Cheng, Koc, Harmsen, Shaked, Chandra, Aradhye,
  Anderson, Corrado, Chai, Ispir, et~al.]{cheng2016wide}
H.-T. Cheng, L.~Koc, J.~Harmsen, T.~Shaked, T.~Chandra, H.~Aradhye,
  G.~Anderson, G.~Corrado, W.~Chai, M.~Ispir, et~al.
\newblock Wide \& deep learning for recommender systems.
\newblock In \emph{Proceedings of the 1st workshop on deep learning for
  recommender systems}, pages 7--10, 2016.

\bibitem[community()]{deeprec2022}
D.~community.
\newblock Deeprec.
\newblock \url{https://github.com/alibaba/DeepRec} 2022.5.11.

\bibitem[Covington et~al.(2016)Covington, Adams, and Sargin]{covington2016deep}
P.~Covington, J.~Adams, and E.~Sargin.
\newblock Deep neural networks for youtube recommendations.
\newblock In \emph{Proceedings of the 10th ACM conference on recommender
  systems}, pages 191--198, 2016.

\bibitem[Dutta et~al.(2018)Dutta, Joshi, Ghosh, Dube, and
  Nagpurkar]{dutta2018slow}
S.~Dutta, G.~Joshi, S.~Ghosh, P.~Dube, and P.~Nagpurkar.
\newblock Slow and stale gradients can win the race: Error-runtime trade-offs
  in distributed {SGD}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 803--812. PMLR, 2018.

\bibitem[Eliad et~al.(2021)Eliad, Hakimi, De~Jagger, Silberstein, and
  Schuster]{eliad2021fine}
S.~Eliad, I.~Hakimi, A.~De~Jagger, M.~Silberstein, and A.~Schuster.
\newblock Fine-tuning giant neural networks on commodity hardware with
  automatic pipeline model parallelism.
\newblock In \emph{2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages 381--396, 2021.

\bibitem[Group()]{alimama2019}
A.~Group.
\newblock Ad display/click data on taobao.com.
\newblock
  \url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=56&lang=en-us}
  under CC BY-NC-SA 4.0, visited on 2022.5.11.

\bibitem[Guo et~al.(2020)Guo, Liu, Yang, and Rosing]{guo2020improved}
Y.~Guo, M.~Liu, T.~Yang, and T.~Rosing.
\newblock Improved schemes for episodic memory-based lifelong learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1023--1035, 2020.

\bibitem[He et~al.(2019)He, Liu, and Tao]{he2019control}
F.~He, T.~Liu, and D.~Tao.
\newblock Control batch size and learning rate to generalize well: Theoretical
  and empirical evidence.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ho et~al.(2013)Ho, Cipar, Cui, Lee, Kim, Gibbons, Gibson, Ganger, and
  Xing]{ho2013more}
Q.~Ho, J.~Cipar, H.~Cui, S.~Lee, J.~K. Kim, P.~B. Gibbons, G.~A. Gibson,
  G.~Ganger, and E.~P. Xing.
\newblock More effective distributed ml via a stale synchronous parallel
  parameter server.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Hron et~al.(2021)Hron, Krauth, Jordan, and
  Kilbertus]{hron2021component}
J.~Hron, K.~Krauth, M.~Jordan, and N.~Kilbertus.
\newblock On component interactions in two-stage recommender systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Inc.({\natexlab{a}})]{criteo1tb}
C.~Inc.
\newblock Criteo 1tb click logs dataset.
\newblock
  \url{https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/}
  visited on 2022.5.11, {\natexlab{a}}.

\bibitem[Inc.({\natexlab{b}})]{criteo2014kaggle}
C.~Inc.
\newblock Kaggle display advertising challenge dataset.
\newblock
  \url{http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/}
  visited on 2020.1.10, {\natexlab{b}}.

\bibitem[Jiang et~al.(2020)Jiang, Zhu, Lan, Yi, Cui, and Guo]{jiang2020unified}
Y.~Jiang, Y.~Zhu, C.~Lan, B.~Yi, Y.~Cui, and C.~Guo.
\newblock A unified architecture for accelerating distributed $\{$DNN$\}$
  training in heterogeneous $\{$GPU/CPU$\}$ clusters.
\newblock In \emph{14th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 20)}, pages 463--479, 2020.

\bibitem[Kim et~al.(2019)Kim, Yu, Park, Cho, Jeong, Ha, Lee, Jeong, and
  Chun]{kim2019parallax}
S.~Kim, G.-I. Yu, H.~Park, S.~Cho, E.~Jeong, H.~Ha, S.~Lee, J.~S. Jeong, and
  B.-G. Chun.
\newblock Parallax: Sparsity-aware data parallel training of deep neural
  networks.
\newblock In \emph{Proceedings of the Fourteenth EuroSys Conference 2019},
  pages 1--15, 2019.

\bibitem[Kumar et~al.(2014)Kumar, Beutel, Ho, and Xing]{kumar2014fugue}
A.~Kumar, A.~Beutel, Q.~Ho, and E.~Xing.
\newblock Fugue: Slow-worker-agnostic distributed learning for big models on
  big data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 531--539.
  PMLR, 2014.

\bibitem[Li et~al.(2020)Li, Jamieson, Rostamizadeh, Gonina, Ben-Tzur, Hardt,
  Recht, and Talwalkar]{li2020system}
L.~Li, K.~Jamieson, A.~Rostamizadeh, E.~Gonina, J.~Ben-Tzur, M.~Hardt,
  B.~Recht, and A.~Talwalkar.
\newblock A system for massively parallel hyperparameter tuning.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  230--246, 2020.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{11th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 14)}, pages 583--598, 2014.

\bibitem[Li et~al.(2021)Li, Mangoubi, Xu, and Guo]{li2021sync}
S.~Li, O.~Mangoubi, L.~Xu, and T.~Guo.
\newblock Sync-switch: Hybrid parameter synchronization for distributed deep
  learning.
\newblock In \emph{2021 IEEE 41st International Conference on Distributed
  Computing Systems (ICDCS)}, pages 528--538. IEEE, 2021.

\bibitem[Lian et~al.(2018)Lian, Zhou, Zhang, Chen, Xie, and
  Sun]{lian2018xdeepfm}
J.~Lian, X.~Zhou, F.~Zhang, Z.~Chen, X.~Xie, and G.~Sun.
\newblock xdeepfm: Combining explicit and implicit feature interactions for
  recommender systems.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1754--1763, 2018.

\bibitem[Luo et~al.(2019)Luo, Lin, Zhuo, and Qian]{luo2019hop}
Q.~Luo, J.~Lin, Y.~Zhuo, and X.~Qian.
\newblock Hop: Heterogeneity-aware decentralized training.
\newblock In \emph{Proceedings of the Twenty-Fourth International Conference on
  Architectural Support for Programming Languages and Operating Systems}, pages
  893--907, 2019.

\bibitem[Luo et~al.(2020)Luo, He, Zhuo, and Qian]{luo2020prague}
Q.~Luo, J.~He, Y.~Zhuo, and X.~Qian.
\newblock Prague: High-performance heterogeneity-aware asynchronous
  decentralized training.
\newblock In \emph{Proceedings of the Twenty-Fifth International Conference on
  Architectural Support for Programming Languages and Operating Systems}, pages
  401--416, 2020.

\bibitem[Nadiradze et~al.(2021)Nadiradze, Sabour, Davies, Li, and
  Alistarh]{nadiradze2021asynchronous}
G.~Nadiradze, A.~Sabour, P.~Davies, S.~Li, and D.~Alistarh.
\newblock Asynchronous decentralized sgd with quantized and local updates.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wu et~al.(2020)Wu, Hu, Xiong, Huan, Braverman, and Zhu]{wu2020noisy}
J.~Wu, W.~Hu, H.~Xiong, J.~Huan, V.~Braverman, and Z.~Zhu.
\newblock On the noisy gradient descent that generalizes as sgd.
\newblock In \emph{International Conference on Machine Learning}, pages
  10367--10376. PMLR, 2020.

\bibitem[Ying et~al.(2021)Ying, Yuan, Chen, Hu, Pan, and
  Yin]{ying2021exponential}
B.~Ying, K.~Yuan, Y.~Chen, H.~Hu, P.~Pan, and W.~Yin.
\newblock Exponential graph is provably efficient for decentralized deep
  training.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
Y.~You, J.~Li, S.~Reddi, J.~Hseu, S.~Kumar, S.~Bhojanapalli, X.~Song,
  J.~Demmel, K.~Keutzer, and C.-J. Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{ICLR}, 2020.

\bibitem[Yu and Jin(2019)]{yu2019computation}
H.~Yu and R.~Jin.
\newblock On the computation and communication complexity of parallel sgd with
  dynamic batch sizes for stochastic non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7174--7183. PMLR, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Chen, Yang, Yuan, Yi, Zhang, Wang, Dong, Xu,
  Song, et~al.]{zhang2022picasso}
Y.~Zhang, L.~Chen, S.~Yang, M.~Yuan, H.~Yi, J.~Zhang, J.~Wang, J.~Dong, Y.~Xu,
  Y.~Song, et~al.
\newblock Picasso: Unleashing the potential of gpu-centric training for
  wide-and-deep recommender systems.
\newblock In \emph{2022 IEEE International Conference on Data Engineering
  (ICDE)}, 2022.

\bibitem[Zheng et~al.(2017)Zheng, Meng, Wang, Chen, Yu, Ma, and
  Liu]{zheng2017asynchronous}
S.~Zheng, Q.~Meng, T.~Wang, W.~Chen, N.~Yu, Z.-M. Ma, and T.-Y. Liu.
\newblock Asynchronous stochastic gradient descent with delay compensation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4120--4129. PMLR, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Zhu, Song, Fan, Zhu, Ma, Yan, Jin, Li, and
  Gai]{zhou2018deep}
G.~Zhou, X.~Zhu, C.~Song, Y.~Fan, H.~Zhu, X.~Ma, Y.~Yan, J.~Jin, H.~Li, and
  K.~Gai.
\newblock Deep interest network for click-through rate prediction.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 1059--1068, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Mou, Fan, Pi, Bian, Zhou, Zhu, and
  Gai]{zhou2019deep}
G.~Zhou, N.~Mou, Y.~Fan, Q.~Pi, W.~Bian, C.~Zhou, X.~Zhu, and K.~Gai.
\newblock Deep interest evolution network for click-through rate prediction.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 5941--5948, 2019.

\bibitem[Zhou et~al.(2016)Zhou, Yu, Dai, Liang, and Xing]{zhou2016convergence}
Y.~Zhou, Y.~Yu, W.~Dai, Y.~Liang, and E.~Xing.
\newblock On convergence of model parallel proximal gradient algorithm for
  stale synchronous parallel system.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 713--722.
  PMLR, 2016.

\end{thebibliography}
