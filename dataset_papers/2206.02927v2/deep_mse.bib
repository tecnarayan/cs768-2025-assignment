@book{wainwright_2019, 
place={Cambridge}, 
series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint}, 
DOI={10.1017/9781108627771}, 
publisher={Cambridge University Press}, 
author={Wainwright, Martin J.}, 
year={2019}, 
collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@article{JMLR:v21:17-678,
  author  = {James Martens},
  title   = {New Insights and Perspectives on the Natural Gradient Method},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {146},
  pages   = {1-76},
  url     = {http://jmlr.org/papers/v21/17-678.html}
} 

@INPROCEEDINGS{Pascanu14revisitingnatural,
    author = {Razvan Pascanu and Universite ́ De Montréal and Yoshua Bengio},
    title = {Revisiting natural gradient for deep networks},
    booktitle = {In International Conference on Learning Representations},
    year = {2014}
}

@article{10.1162/089976600300015637,
    author = {Heskes, Tom},
    title = "{On “Natural” Learning and Pruning in Multilayered Perceptrons}",
    journal = {Neural Computation},
    volume = {12},
    number = {4},
    pages = {881-901},
    year = {2000},
    month = {04},
    abstract = "{Several studies have shown that natural gradient descent for on-line learning is much more efficient than standard gradient descent. In this article, we derive natural gradients in a slightly different manner and discuss implications for batch-mode learning and pruning, linking them to existing algorithms such as Levenberg-Marquardt optimization and optimal brain surgeon.The Fisher matrix plays an important role in all these algorithms. The second half of the article discusses a layered approximation of the Fisher matrix specific to multilayered perceptrons. Using this approximation rather than the exact Fisher matrix, we arrive at much faster “natural” learning algorithms and more robust pruning procedures.}",
    issn = {0899-7667},
    doi = {10.1162/089976600300015637},
    url = {https://doi.org/10.1162/089976600300015637},
    eprint = {https://direct.mit.edu/neco/article-pdf/12/4/881/814455/089976600300015637.pdf},
}


@inproceedings{zhang2017understanding,
  author    = {Chiyuan Zhang and
               Samy Bengio and
               Moritz Hardt and
               Benjamin Recht and
               Oriol Vinyals},
  title     = {Understanding deep learning requires rethinking generalization},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Sy8gdB9xx},
  timestamp = {Thu, 04 Apr 2019 13:20:08 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangBHRV17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jacot2020neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{dumer_ellipsoid,
author = {Dumer, Ilya},
title = {Covering an Ellipsoid with Equal Balls},
year = {2006},
issue_date = {November 2006},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {113},
number = {8},
issn = {0097-3165},
url = {https://doi.org/10.1016/j.jcta.2006.03.021},
doi = {10.1016/j.jcta.2006.03.021},
abstract = {The thinnest coverings of ellipsoids are studied in the Euclidean spaces of an arbitrary dimension n. Given any ellipsoid, our goal is to find the minimum number of unit balls needed to cover this ellipsoid. A tight asymptotic bound on the logarithm of this number is obtained.},
journal = {J. Comb. Theory Ser. A},
month = {nov},
pages = {1667–1676},
numpages = {10},
keywords = {spherical covering, Euclidean space, ellipsoid, unit ball}
}

@book{pisier_1989, place={Cambridge}, series={Cambridge Tracts in Mathematics}, title={The Volume of Convex Bodies and Banach Space Geometry}, DOI={10.1017/CBO9780511662454}, publisher={Cambridge University Press}, author={Pisier, Gilles}, year={1989}, collection={Cambridge Tracts in Mathematics}}

@article{Gordon1987GeometricAP,
  title={Geometric and probabilistic estimates for entropy and approximation numbers of operators},
  author={Y. Gordon and Hermann K{\"o}nig and Carsten Sch{\"u}tt},
  journal={Journal of Approximation Theory},
  year={1987},
  volume={49},
  pages={219-239}
}

@inproceedings{liuonlinearity,
 author = {Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15954--15964},
 publisher = {Curran Associates, Inc.},
 title = {On the linearity of large non-linear models: when and why the tangent kernel is constant},
 url = {https://proceedings.neurips.cc/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf},
 volume = {33},
 year = {2020}
}

@InProceedings{huang2019dynamics,
  title = 	 {Dynamics of Deep Neural Networks and Neural Tangent Hierarchy},
  author =       {Huang, Jiaoyang and Yau, Horng-Tzer},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4542--4551},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/huang20l/huang20l.pdf},
  url = 	 {https://proceedings.mlr.press/v119/huang20l.html},
  abstract = 	 {The evolution of a deep neural network trained by the gradient descent in the overparametrization regime can be described by its neural tangent kernel (NTK) \cite{jacot2018neural, du2018gradient1,du2018gradient2,arora2019fine}. It was observed \cite{arora2019exact} that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. We study the dynamic of neural networks of finite width and derive an infinite hierarchy of differential equations, the neural tangent hierarchy (NTH). We prove that the NTH hierarchy truncated at the level $p\geq 2$ approximates the dynamic of the NTK up to arbitrary precision under certain conditions on the neural network width and the data set dimension. The assumptions needed for these approximations become weaker as $p$ increases. Finally, NTH can be viewed as higher order extensions of NTK. In particular, the NTH truncated at $p=2$ recovers the NTK dynamics.}
}

@inproceedings{
bowman2022implicit,
title={Implicit Bias of {MSE} Gradient Optimization in Underparameterized Neural Networks},
author={Benjamin Bowman and Guido Mont\'ufar},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=VLgmhQDVBV}
}

@article{rosasco10a,
  author  = {Lorenzo Rosasco and Mikhail Belkin and Ernesto De Vito},
  title   = {On Learning with Integral Operators},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {30},
  pages   = {905-934},
  url     = {http://jmlr.org/papers/v11/rosasco10a.html}
}

@misc{mjt_dlt,
       author = {Matus Telgarsky},
        title = {Deep learning theory lecture notes},
 howpublished = {\url{https://mjt.cs.illinois.edu/dlt/}},
         year = {2021},
         note = {Version: 2021-10-27 v0.0-e7150f2d (alpha)},
}

@misc{neyshabur2017geometry,
      title={Geometry of Optimization and Implicit Regularization in Deep Learning}, 
      author={Behnam Neyshabur and Ryota Tomioka and Ruslan Salakhutdinov and Nathan Srebro},
      year={2017},
      eprint={1705.03071},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note ={arXiv:1705.03071}
}


@inproceedings{neyshabur2015search,
  author    = {Behnam Neyshabur and
               Ryota Tomioka and
               Nathan Srebro},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {In Search of the Real Inductive Bias: On the Role of Implicit Regularization
               in Deep Learning},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6614},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NeyshaburTS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2017_58191d2a,
 author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization in Matrix Factorization},
 url = {https://proceedings.neurips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf},
 volume = {30},
 year = {2017}
}

%% Couldn't find published reference
@article{DBLP:journals/corr/WuZE17,
  author    = {Lei Wu and
               Zhanxing Zhu and
               Weinan E},
  title     = {Towards Understanding Generalization of Deep Learning: Perspective
               of Loss Landscapes},
  journal   = {CoRR},
  volume    = {abs/1706.10239},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.10239},
  eprinttype = {arXiv},
  eprint    = {1706.10239},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WuZE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{soudry2018implicit,
  author  = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  title   = {The Implicit Bias of Gradient Descent on Separable Data},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {70},
  pages   = {1-57},
  url     = {http://jmlr.org/papers/v19/18-188.html}
}

@inproceedings{NEURIPS2018_0e98aeeb,
 author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Bias of Gradient Descent on Linear Convolutional Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf},
 volume = {31},
 year = {2018}
}

@InProceedings{rahaman2019spectral,
  title = 	 {On the Spectral Bias of Neural Networks},
  author =       {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5301--5310},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rahaman19a.html},
  abstract = 	 {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.}
}


@inproceedings{basri2019convergence,
 author = {Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies},
 url = {https://proceedings.neurips.cc/paper/2019/file/5ac8bb8a7d745102a978c5f8ccdb61b8-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{basri2020frequency,
  title = 	 {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
  author =       {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {685--694},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/basri20a/basri20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/basri20a.html},
  abstract = 	 {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias – networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency $\kappa$, convergence at a point $x \in \S^{d-1}$ occurs in time $O(\kappa^d/p(x))$ where $p(x)$ denotes the local density at $x$. Specifically, for data in $\S^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.}
}

@InProceedings{xu2019training,
author="Xu, Zhi-Qin John
and Zhang, Yaoyu
and Xiao, Yanyang",
editor="Gedeon, Tom
and Wong, Kok Wai
and Lee, Minho",
title="Training Behavior of Deep Neural Network in Frequency Domain",
booktitle="Neural Information Processing",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="264--274",
abstract="Why deep neural networks (DNNs) capable of overfitting often generalize well in practice is a mystery [24]. To find a potential mechanism, we focus on the study of implicit biases underlying the training process of DNNs. In this work, for both real and synthetic datasets, we empirically find that a DNN with common settings first quickly captures the dominant low-frequency components, and then relatively slowly captures the high-frequency ones. We call this phenomenon Frequency Principle (F-Principle). The F-Principle can be observed over DNNs of various structures, activation functions, and training algorithms in our experiments. We also illustrate how the F-Principle helps understand the effect of early-stopping as well as the generalization of DNNs. This F-Principle potentially provides insight into a general principle underlying DNN optimization and generalization.",
noisbn ="978-3-030-36708-4"
}


@inproceedings{
yang2022overcoming,
title={Overcoming The Spectral Bias of Neural Value Approximation},
author={Ge Yang and Anurag Ajay and Pulkit Agrawal},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vIC-xLFuM6}
}

@InProceedings{telgarskynonseparable,
  title = 	 {The implicit bias of gradient descent on nonseparable data},
  author =       {Ji, Ziwei and Telgarsky, Matus},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {1772--1798},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/ji19a/ji19a.pdf},
  url = 	 {https://proceedings.mlr.press/v99/ji19a.html},
  abstract = 	 {Gradient descent, when applied to the task of logistic regression, outputs iterates which are biased to follow a unique ray defined by the data. The direction of this ray is the maximum margin predictor of a maximal linearly separable subset of the data; the gradient descent iterates converge to this ray in direction at the rate $\cO(\nicefrac{\ln\ln t }{\ln t})$. The ray does not pass through the origin in general, and its offset is the bounded global optimum of the risk over the remaining data; gradient descent recovers this offset at a rate $\cO(\nicefrac{(\ln t)^2}{\sqrt{t}})$.}
}

@inproceedings{NEURIPS2020_f21e255f,
 author = {Razin, Noam and Cohen, Nadav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21174--21187},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization in Deep Learning May Not Be Explainable by Norms},
 url = {https://proceedings.neurips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v75-li18a,
  title = 	 {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
  author =       {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {2--47},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/li18a/li18a.pdf},
  url = 	 {https://proceedings.mlr.press/v75/li18a.html},
  abstract = 	 {We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.’17 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.}
}

@inproceedings{Nacson2019ConvergenceOG,
  title={Convergence of Gradient Descent on Separable Data},
  author={Mor Shpigel Nacson and J. Lee and Suriya Gunasekar and Nathan Srebro and Daniel Soudry},
  booktitle={AISTATS},
  year={2019}
}

@InProceedings{generalizationinducedbyinit,
  title = 	 {A type of generalization error induced by initialization in deep neural networks},
  author =       {Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
  booktitle = 	 {Proceedings of The First Mathematical and Scientific Machine Learning Conference},
  pages = 	 {144--164},
  year = 	 {2020},
  editor = 	 {Lu, Jianfeng and Ward, Rachel},
  volume = 	 {107},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v107/zhang20a/zhang20a.pdf},
  url = 	 {https://proceedings.mlr.press/v107/zhang20a.html},
  abstract = 	 { How initialization and loss function affect the learning of a deep neural network (DNN), specifically its generalization error, is an important problem in practice. In this work, by exploiting the linearity of DNN training dynamics in the NTK regime \citep{jacot2018neural,lee2019wide}, we provide an explicit and quantitative answer to this problem. Focusing on regression problem, we prove that, in the NTK regime, for any loss in a general class of functions, the DNN finds the same \emph{global} minima—the one that is nearest to the initial value in the parameter space, or equivalently, the one that is closest to the initial DNN output in the corresponding reproducing kernel Hilbert space. Using these optimization problems, we quantify the impact of initial output and prove that a random non-zero one increases the generalization error. We further propose an antisymmetrical initialization (ASI) trick that eliminates this type of error and accelerates the training. To understand whether the above results hold in general, we also perform experiments for DNNs in the non-NTK regime, which demonstrate the effectiveness of our theoretical results and the ASI trick in a qualitative sense. Overall, our work serves as a baseline for the further investigation of the impact of initialization and loss function on the generalization of DNNs, which can potentially guide and improve the training of DNNs in practice.  }
}

@inproceedings{williams2019gradient,
 author = {Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Dynamics of Shallow Univariate ReLU Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/1f6419b1cbe79c71410cb320fc094775-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{jin2021implicit,
      title={Implicit bias of gradient descent for mean squared error regression with wide neural networks}, 
      author={Hui Jin and Guido Montúfar},
      year={2021},
      eprint={2006.07356},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      note={arXiv:2006.07356}
}

@InProceedings{li2019gradient,
  title = 	 {Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks},
  author =       {Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4313--4324},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/li20j/li20j.pdf},
  url = 	 {https://proceedings.mlr.press/v108/li20j.html},
  abstract = 	 {Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including significantly corrupted ones. Despite this (over)fitting capacity in this paper we demonstrate that such overparameterized networks have an intriguing robustness capability: they are surprisingly robust to label noise when first order methods with early stopping is used to train them. This paper also takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy labels network must stray rather far from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.}
}

@misc{
oymak2020generalization,
title={Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the {J}acobian},
author={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},
year={2020},
url={https://openreview.net/forum?id=ryl5CJSFPS}
}

@article{Yang2021DoesTD,
  title={Does the Data Induce Capacity Control in Deep Learning?},
  author={Rubing Yang and Jialin Mao and Pratik Chaudhari},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.14163}
}

@inproceedings{yarotskyasymptotics,
  author    = {Maksim Velikanov and
               Dmitry Yarotsky},
  editor    = {Marc'Aurelio Ranzato and
               Alina Beygelzimer and
               Yann N. Dauphin and
               Percy Liang and
               Jennifer Wortman Vaughan},
  title     = {Explicit loss asymptotics in the gradient descent training of neural
               networks},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {2570--2582},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/14faf969228fc18fcd4fcf59437b0c97-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:46 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/VelikanovY21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bartlettmendelson2003,
author = {Bartlett, Peter L. and Mendelson, Shahar},
title = {Rademacher and {G}aussian Complexities: Risk Bounds and Structural Results},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {463–482},
numpages = {20},
keywords = {Rademacher averages, error bounds, data-dependent complexity, maximum discrepancy}
}

@InProceedings{arora2019finegrained,
  title = 	 {Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author =       {Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {322--332},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/arora19a/arora19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/arora19a.html},
  abstract = 	 {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.}
}

@inproceedings{caounderstanding,
  title     = {Towards Understanding the Spectral Bias of Deep Learning},
  author    = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {2205--2211},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/304},
  url       = {https://doi.org/10.24963/ijcai.2021/304},
}

@inproceedings{aroraexact,
 author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Exact Computation with an Infinitely Wide Neural Net},
 url = {https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{
Hanin2020Finite,
title={Finite Depth and Width Corrections to the Neural Tangent Kernel},
author={Boris Hanin and Mihai Nica},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgndT4KwB}
}

@inproceedings{du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@ARTICLE{oymak2019moderate,
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks}, 
  year={2020},
  volume={1},
  number={1},
  pages={84-105},
  nodoi={10.1109/JSAIT.2020.2991332}}
  
@InProceedings{du2019gradient,
  title = 	 {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author =       {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1675--1685},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/du19c/du19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/du19c.html},
  abstract = 	 {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.}
}

@InProceedings{allenzhu2019convergence,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  abstract = 	 {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps &nbsp; e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
}

@inproceedings{qyunhpyramidal,
 author = {Nguyen, Quynh and Mondelli, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {11961--11972},
 publisher = {Curran Associates, Inc.},
 title = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
 url = {https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{nguyenrelu,
  title = 	 {On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths},
  author =       {Nguyen, Quynh},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8056--8062},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21a/nguyen21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21a.html},
  abstract = 	 {We give a simple proof for the global convergence of gradient descent in training deep ReLU networks with the standard square loss, and show some of its improvements over the state-of-the-art. In particular, while prior works require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$ being the number of training samples), we require a single wide layer of linear, quadratic or cubic width depending on the type of initialization. Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof need not track the evolution of the entire NTK matrix, or more generally, any quantities related to the changes of activation patterns during training. Instead, we only need to track the evolution of the output at the last hidden layer, which can be done much more easily thanks to the Lipschitz property of ReLU. Some highlights of our setting: (i) all the layers are trained with standard gradient descent, (ii) the network has standard parameterization as opposed to the NTK one, and (iii) the network has a single wide layer as opposed to having all wide hidden layers as in most of NTK-related results.}
}


@article{zou2018stochastic,
  title={Gradient descent optimizes over-parameterized deep {ReLU} networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal={Machine learning},
  volume={109},
  pages={467–492},
  year={2020},
  publisher={Springer}
}


@inproceedings{zou2019improved,
 author = {Zou, Difan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An Improved Analysis of Training Over-parameterized Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{
Ji2020Polylogarithmic,
title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow {ReLU} networks},
author={Ziwei Ji and Matus Telgarsky},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HygegyrYwH}
}


@article{Weinan2020ACA,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics},
  author={E, Weinan and Chao Ma and Lei Wu},
  journal={Science China Mathematics},
  year={2020},
  volume={63},
  pages={1235-1258}
}

@inproceedings{su2019learning,
 author = {Su, Lili and Yang, Pengkun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
 url = {https://proceedings.neurips.cc/paper/2019/file/253f7b5d921338af34da817c00f42753-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Lee2019WideNN-SHORT,
  title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
  author={Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and Roman Novak and Jascha Sohl-Dickstein and Jascha Sohl-Dickstein},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Lee2019WideNN,
 author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf},
 volume = {32},
 year = {2019}
}





@article{papyantraces,
  author  = {Vardan Papyan},
  title   = {Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {252},
  pages   = {1-64},
  url     = {http://jmlr.org/papers/v21/20-933.html}
}

@misc{gregyangtp2,
  doi = {10.48550/ARXIV.2006.14548},
  
  url = {https://arxiv.org/abs/2006.14548},
  
  author = {Yang, Greg},
  
  keywords = {Machine Learning (stat.ML), Disordered Systems and Neural Networks (cond-mat.dis-nn), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Tensor Programs {II}: Neural Tangent Kernel for Any Architecture},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
buchanan2021deep,
title={Deep Networks and the Multiple Manifold Problem},
author={Sam Buchanan and Dar Gilboa and John Wright},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=O-6Pm_d_Q-}
}


@InProceedings{kernelalignment,
  title = 	 {Implicit Regularization via Neural Feature Alignment },
  author =       {Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Devon Hjelm, R and Lajoie, Guillaume and Vincent, Pascal and Lacoste-Julien, Simon},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2269--2277},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/baratin21a/baratin21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/baratin21a.html},
  abstract = 	 { We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a regularization effect induced by a dynamical alignment ofthe neural tangent features introduced by Jacot et al. (2018), along a small number of task-relevant directions. This can be interpreted as a combined mechanism of feature selection and compression. By extrapolating a new analysis of Rademacher complexity bounds for linear models, we motivate and study a heuristic complexity measure that captures this phenomenon, in terms of sequences of tangent kernel classes along optimization paths. The code for our experiments is available as https://github.com/tfjgeorge/ntk_alignment. }
}

@ARTICLE{lenetpaper,
  author={LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}
  
@inbook{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12}
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@Article{harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@Article{Hunter:2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

@conference{Kluyver2016jupyter,
Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
Editor = {F. Loizides and B. Schmidt},
Organization = {IOS Press},
Pages = {87 - 90},
Year = {2016}
}

@Article{ipython,
  Author    = {P\'erez, Fernando and Granger, Brian E.},
  Title     = {{IP}ython: a System for Interactive Scientific Computing},
  Journal   = {Computing in Science and Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {21--29},
  month     = may,
  year      = 2007,
  url       = "https://ipython.org",
  ISSN      = "1521-9615",
  doi       = {10.1109/MCSE.2007.53},
  publisher = {IEEE Computer Society},
}

@article{liubelkinarxiv,
  author    = {Chaoyue Liu and
               Libin Zhu and
               Mikhail Belkin},
  title     = {On the linearity of large non-linear models: when and why the tangent
               kernel is constant},
  journal   = {CoRR},
  volume    = {abs/2010.01092},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.01092},
  eprinttype = {arXiv},
  eprint    = {2010.01092},
  timestamp = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-01092.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{vershynin2011introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Roman Vershynin},
  booktitle={Compressed Sensing},
  publisher={Cambridge University Press}, 
  year={2012},
  chapter={5},
}

@article{karakida_pathological_2021,
	title = {Pathological {Spectra} of the {Fisher} {Information} {Metric} and {Its} {Variants} in {Deep} {Neural} {Networks}},
	volume = {33},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco\_a\_01411},
	doi = {10.1162/neco_a_01411},
	abstract = {The Fisher information matrix (FIM) plays an essential role in statistics and machine learning as a Riemannian metric tensor or a component of the Hessian matrix of loss functions. Focusing on the FIM and its variants in deep neural networks (DNNs), we reveal their characteristic scale dependence on the network width, depth, and sample size when the network has random weights and is sufficiently wide. This study covers two widely used FIMs for regression with linear output and for classification with softmax output. Both FIMs asymptotically show pathological eigenvalue spectra in the sense that a small number of eigenvalues become large outliers depending on the width or sample size, while the others are much smaller. It implies that the local shape of the parameter space or loss landscape is very sharp in a few specific directions while almost flat in the other directions. In particular, the softmax output disperses the outliers and makes a tail of the eigenvalue density spread from the bulk. We also show that pathological spectra appear in other variants of FIMs: one is the neural tangent kernel; another is a metric for the input signal and feature space that arises from feedforward signal propagation. Thus, we provide a unified perspective on the FIM and its variants that will lead to more quantitative understanding of learning in large-scale DNNs.},
	number = {8},
	journal = {Neural Computation},
	author = {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
	month = jul,
	year = {2021},
	note = {\_eprint: https://direct.mit.edu/neco/article-pdf/33/8/2274/1930880/neco\_a\_01411.pdf},
	pages = {2274--2307},
}

@inproceedings{NEURIPS2018_18bb68e2,
 author = {Pennington, Jeffrey and Worah, Pratik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network},
 url = {https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf},
 volume = {31},
 year = {2018}
}


@InProceedings{pmlr-v70-pennington17a,
  title = 	 {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
  author =       {Jeffrey Pennington and Yasaman Bahri},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2798--2806},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pennington17a.html},
  abstract = 	 {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, $\phi$, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function $1/2(1-\phi)^2$.}
}

@inproceedings{10.5555/3495724.3496370,
author = {Fan, Zhou and Wang, Zhichao},
title = {Spectra of the Conjugate Kernel and Neural Tangent Kernel for Linear-Width Neural Networks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the eigenvalue distributions of the Conjugate Kernel and Neural Tangent Kernel associated to multi-layer feedforward neural networks. In an asymptotic regime where network width is increasing linearly in sample size, under random initialization of the weights, and for input samples satisfying a notion of approximate pairwise orthogonality, we show that the eigenvalue distributions of the CK and NTK converge to deterministic limits. The limit for the CK is described by iterating the Marcenko-Pastur map across the hidden layers. The limit for the NTK is equivalent to that of a linear combination of the CK matrices across layers, and may be described by recursive fixed-point equations that extend this Marcenko-Pastur map. We demonstrate the agreement of these asymptotic predictions with the observed spectra for both synthetic and CIFAR-10 training data, and we perform a small simulation to investigate the evolutions of these spectra over training.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {646},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@misc{https://doi.org/10.48550/arxiv.1907.10599,
  doi = {10.48550/ARXIV.1907.10599},
  
  url = {https://arxiv.org/abs/1907.10599},
  
  author = {Yang, Greg and Salman, Hadi},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Fine-Grained Spectral Perspective on Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
atanasov2022neural,
title={Neural Networks as Kernel Learners: The Silent Alignment Effect},
author={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=1NvflqAdoom}
}

@misc{https://doi.org/10.48550/arxiv.2010.08153,
  doi = {10.48550/ARXIV.2010.08153},
  
  url = {https://arxiv.org/abs/2010.08153},
  
  author = {Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the exact computation of linear frequency principle dynamics and its generalization},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
