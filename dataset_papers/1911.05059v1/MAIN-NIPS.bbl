\begin{thebibliography}{45}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li and
  Liang}]{allen2018generalization}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Liang, Y.}
  (2019{\natexlab{a}}).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li and
  Song}]{allen2018convergence}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Song, Z.}
  (2019{\natexlab{b}}).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li and
  Wang}]{arora2019fine}
\textsc{Arora, S.}, \textsc{Du, S.}, \textsc{Hu, W.}, \textsc{Li, Z.} and
  \textsc{Wang, R.} (2019{\natexlab{a}}).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov and
  Wang}]{arora2019exact}
\textsc{Arora, S.}, \textsc{Du, S.~S.}, \textsc{Hu, W.}, \textsc{Li, Z.},
  \textsc{Salakhutdinov, R.} and \textsc{Wang, R.} (2019{\natexlab{b}}).
\newblock On exact computation with an infinitely wide neural net.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Baum(1990)}]{baum1990polynomial}
\textsc{Baum, E.~B.} (1990).
\newblock A polynomial time algorithm that learns two hidden unit nets.
\newblock \textit{Neural Computation} \textbf{2} 510--522.

\bibitem[{Blum and Rivest(1989)}]{blum1989training}
\textsc{Blum, A.} and \textsc{Rivest, R.~L.} (1989).
\newblock Training a 3-node neural network is np-complete.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Brutzkus and Globerson(2017)}]{brutzkus2017globally}
\textsc{Brutzkus, A.} and \textsc{Globerson, A.} (2017).
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Cao and Gu(2019)}]{cao2019generalizationsgd}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cao and Gu(2020)}]{cao2019generalization}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2020).
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock In \textit{the Thirty-Fourth AAAI Conference on Artificial
  Intelligence}.

\bibitem[{Cohen and Shashua(2016)}]{cohen2016convolutional}
\textsc{Cohen, N.} and \textsc{Shashua, A.} (2016).
\newblock Convolutional rectifier networks as generalized tensor
  decompositions.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Cuadras(2002)}]{cuadras2002covariance}
\textsc{Cuadras, C.~M.} (2002).
\newblock On the covariance between functions.
\newblock \textit{Journal of Multivariate Analysis} \textbf{81} 19--27.

\bibitem[{Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang and
  Zhai}]{du2018gradientdeep}
\textsc{Du, S.}, \textsc{Lee, J.}, \textsc{Li, H.}, \textsc{Wang, L.} and
  \textsc{Zhai, X.} (2019{\natexlab{a}}).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Du and Goel(2018)}]{du2018improved}
\textsc{Du, S.~S.} and \textsc{Goel, S.} (2018).
\newblock Improved learning of one-hidden-layer convolutional neural networks
  with overlaps.
\newblock \textit{arXiv preprint arXiv:1805.07798} .

\bibitem[{Du et~al.(2018{\natexlab{a}})Du, Lee and Tian}]{du2017convolutional}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.} and \textsc{Tian, Y.}
  (2018{\natexlab{a}}).
\newblock When is a convolutional filter easy to learn?
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Du et~al.(2018{\natexlab{b}})Du, Lee, Tian, Singh and
  Poczos}]{du2017gradient}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.}, \textsc{Tian, Y.}, \textsc{Singh, A.}
  and \textsc{Poczos, B.} (2018{\natexlab{b}}).
\newblock Gradient descent learns one-hidden-layer cnn: Don’t be afraid of
  spurious local minima.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Du et~al.(2018{\natexlab{c}})Du, Wang, Zhai, Balakrishnan,
  Salakhutdinov and Singh}]{du2018many}
\textsc{Du, S.~S.}, \textsc{Wang, Y.}, \textsc{Zhai, X.}, \textsc{Balakrishnan,
  S.}, \textsc{Salakhutdinov, R.} and \textsc{Singh, A.} (2018{\natexlab{c}}).
\newblock How many samples are needed to learn a convolutional neural network?
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos and
  Singh}]{du2018gradient}
\textsc{Du, S.~S.}, \textsc{Zhai, X.}, \textsc{Poczos, B.} and \textsc{Singh,
  A.} (2019{\natexlab{b}}).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Fu et~al.(2019)Fu, Chi and Liang}]{fu2018local}
\textsc{Fu, H.}, \textsc{Chi, Y.} and \textsc{Liang, Y.} (2019).
\newblock Local geometry of cross entropy loss in learning one-hidden-layer
  neural networks.
\newblock In \textit{2019 IEEE International Symposium on Information Theory
  (ISIT)}. IEEE.

\bibitem[{Ge et~al.(2017)Ge, Lee and Ma}]{ge2017learning}
\textsc{Ge, R.}, \textsc{Lee, J.~D.} and \textsc{Ma, T.} (2017).
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Goel et~al.(2018)Goel, Klivans and Meka}]{goel2018learning}
\textsc{Goel, S.}, \textsc{Klivans, A.} and \textsc{Meka, R.} (2018).
\newblock Learning one convolutional layer with overlapping patches.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Gunasekar et~al.(2018)Gunasekar, Lee, Soudry and
  Srebro}]{gunasekar2018implicit}
\textsc{Gunasekar, S.}, \textsc{Lee, J.~D.}, \textsc{Soudry, D.} and
  \textsc{Srebro, N.} (2018).
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath et~al.}]{hinton2012deep}
\textsc{Hinton, G.}, \textsc{Deng, L.}, \textsc{Yu, D.}, \textsc{Dahl, G.~E.},
  \textsc{Mohamed, A.-r.}, \textsc{Jaitly, N.}, \textsc{Senior, A.},
  \textsc{Vanhoucke, V.}, \textsc{Nguyen, P.}, \textsc{Sainath, T.~N.}
  \textsc{et~al.} (2012).
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \textit{IEEE Signal Processing Magazine} \textbf{29} 82--97.

\bibitem[{Hoeffding(1940)}]{hoeffding1940masstabinvariante}
\textsc{Hoeffding, W.} (1940).
\newblock Masstabinvariante korrelationtheorie, schriften des mathematis chen
  instituts und des instituts f{\"u}r angewandte mathematik der universit{\"a}t
  berlin 5, 181\# 233.(translated in fisher, ni and pk sen (1994). the
  collected works of wassily hoeffding, new york.

\bibitem[{Hornik(1991)}]{hornik1991approximation}
\textsc{Hornik, K.} (1991).
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \textit{Neural networks} \textbf{4} 251--257.

\bibitem[{Janzamin et~al.(2015)Janzamin, Sedghi and
  Anandkumar}]{janzamin2015beating}
\textsc{Janzamin, M.}, \textsc{Sedghi, H.} and \textsc{Anandkumar, A.} (2015).
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock \textit{arXiv preprint arXiv:1506.08473} .

\bibitem[{Klivans et~al.(2009)Klivans, Long and Tang}]{klivans2009baum}
\textsc{Klivans, A.~R.}, \textsc{Long, P.~M.} and \textsc{Tang, A.~K.} (2009).
\newblock Baum’s algorithm learns intersections of halfspaces with respect to
  log-concave distributions.
\newblock In \textit{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques}. Springer, 588--600.

\bibitem[{Krizhevsky et~al.(2012)Krizhevsky, Sutskever and
  Hinton}]{krizhevsky2012imagenet}
\textsc{Krizhevsky, A.}, \textsc{Sutskever, I.} and \textsc{Hinton, G.~E.}
  (2012).
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Li and Liang(2018)}]{li2018learning}
\textsc{Li, Y.} and \textsc{Liang, Y.} (2018).
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li and Yuan(2017)}]{li2017convergence}
\textsc{Li, Y.} and \textsc{Yuan, Y.} (2017).
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Mei et~al.(2018{\natexlab{a}})Mei, Bai and
  Montanari}]{mei2016landscape}
\textsc{Mei, S.}, \textsc{Bai, Y.} and \textsc{Montanari, A.}
  (2018{\natexlab{a}}).
\newblock The landscape of empirical risk for non-convex losses.
\newblock In \textit{The Annals of Statistics}.

\bibitem[{Mei et~al.(2018{\natexlab{b}})Mei, Montanari and
  Nguyen}]{mei2018mean}
\textsc{Mei, S.}, \textsc{Montanari, A.} and \textsc{Nguyen, P.-M.}
  (2018{\natexlab{b}}).
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \textit{Proceedings of the National Academy of Sciences} \textbf{115}
  E7665--E7671.

\bibitem[{Nguyen and Hein(2017)}]{nguyen2017bloss}
\textsc{Nguyen, Q.} and \textsc{Hein, M.} (2017).
\newblock The loss surface and expressivity of deep convolutional neural
  networks.
\newblock \textit{arXiv preprint arXiv:1710.10928} .

\bibitem[{Sen(1994)}]{sen1994impact}
\textsc{Sen, P.~K.} (1994).
\newblock The impact of wassily hoeffding’s research on nonparametrics.
\newblock In \textit{The Collected Works of Wassily Hoeffding}. Springer,
  29--55.

\bibitem[{Shamir(2018)}]{shamir2016distribution}
\textsc{Shamir, O.} (2018).
\newblock Distribution-specific hardness of learning neural networks.
\newblock \textit{The Journal of Machine Learning Research} \textbf{19}
  1135--1163.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot
  et~al.}]{silver2016mastering}
\textsc{Silver, D.}, \textsc{Huang, A.}, \textsc{Maddison, C.~J.},
  \textsc{Guez, A.}, \textsc{Sifre, L.}, \textsc{Van Den~Driessche, G.},
  \textsc{Schrittwieser, J.}, \textsc{Antonoglou, I.}, \textsc{Panneershelvam,
  V.}, \textsc{Lanctot, M.} \textsc{et~al.} (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \textit{Nature} \textbf{529} 484--489.

\bibitem[{Slepian(1962)}]{slepian1962one}
\textsc{Slepian, D.} (1962).
\newblock The one-sided barrier problem for gaussian noise.
\newblock \textit{Bell Labs Technical Journal} \textbf{41} 463--501.

\bibitem[{Tian(2016)}]{tian2016symmetry}
\textsc{Tian, Y.} (2016).
\newblock Symmetry-breaking convergence analysis of certain two-layered neural
  networks with relu nonlinearity .

\bibitem[{Vershynin(2010)}]{vershynin2010introduction}
\textsc{Vershynin, R.} (2010).
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \textit{arXiv preprint arXiv:1011.3027} .

\bibitem[{Yi and Caramanis(2015)}]{yi2015regularized}
\textsc{Yi, X.} and \textsc{Caramanis, C.} (2015).
\newblock Regularized em algorithms: A unified framework and statistical
  guarantees.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2019)Zhang, Yu, Wang and Gu}]{zhang2018learning}
\textsc{Zhang, X.}, \textsc{Yu, Y.}, \textsc{Wang, L.} and \textsc{Gu, Q.}
  (2019).
\newblock Learning one-hidden-layer relu networks via gradient descent.
\newblock In \textit{The 22nd International Conference on Artificial
  Intelligence and Statistics}.

\bibitem[{Zhang et~al.(2017)Zhang, Liang and Wainwright}]{zhang2016convexified}
\textsc{Zhang, Y.}, \textsc{Liang, P.} and \textsc{Wainwright, M.~J.} (2017).
\newblock Convexified convolutional neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Zhong et~al.(2017{\natexlab{a}})Zhong, Song and
  Dhillon}]{zhong2017learning}
\textsc{Zhong, K.}, \textsc{Song, Z.} and \textsc{Dhillon, I.~S.}
  (2017{\natexlab{a}}).
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock \textit{arXiv preprint arXiv:1711.03440} .

\bibitem[{Zhong et~al.(2017{\natexlab{b}})Zhong, Song, Jain, Bartlett and
  Dhillon}]{zhong2017recovery}
\textsc{Zhong, K.}, \textsc{Song, Z.}, \textsc{Jain, P.}, \textsc{Bartlett,
  P.~L.} and \textsc{Dhillon, I.~S.} (2017{\natexlab{b}}).
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Zou et~al.(2019)Zou, Cao, Zhou and Gu}]{zou2018stochastic}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Zhou, D.} and \textsc{Gu, Q.}
  (2019).
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock In \textit{Machine Learning Journal}.

\bibitem[{Zou and Gu(2019)}]{zou2019improved}
\textsc{Zou, D.} and \textsc{Gu, Q.} (2019).
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
