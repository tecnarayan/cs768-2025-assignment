@article{duan2020minimax,
  title={Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation},
  author={Duan, Yaqi and Wang, Mengdi},
  journal={arXiv preprint arXiv:2002.09516},
  year={2020}
}

@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018},
  organization={PMLR}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={arXiv preprint arXiv:2005.12900},
  year={2020}
}

@article{zhang2020reinforcement,
  title={Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2009.13503},
  year={2020}
}

@article{zhang2020nearly,
  title={Nearly Minimax Optimal Reward-free Reinforcement Learning},
  author={Zhang, Zihan and Du, Simon S and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2010.05901},
  year={2020}
}

@InProceedings{yin2020asymptotically, 
  title = {Asymptotically Efficient Off-Policy Evaluation for Tabular Reinforcement Learning}, 
  author = {Yin, Ming and Wang, Yu-Xiang}, 
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, 
  pages = {3948--3958}, 
  year = {2020}, 
  editor = {Silvia Chiappa and Roberto Calandra}, 
  volume = {108}, 
  series = {Proceedings of Machine Learning Research}, 
  month = {26--28 Aug}, 
  publisher = {PMLR}, 
}

@article{wang2020long,
  title={Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}

@inproceedings{munos2003error,
  title={Error bounds for approximate policy iteration},
  author={Munos, R{\'e}mi},
  booktitle={ICML},
  volume={3},
  pages={560--567},
  year={2003}
}

@article{yin2020near,
  title={Near optimal provable uniform convergence in off-policy evaluation for reinforcement learning},
  author={Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2007.03760},
  year={2020}
}

@inproceedings{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2318--2328},
  year={2019}
}

@article{freedman1975tail,
  title={On tail probabilities for martingales},
  author={Freedman, David A},
  journal={the Annals of Probability},
  pages={100--118},
  year={1975},
  publisher={JSTOR}
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@InProceedings{pmlr-v124-xie20a, title = {Q* Approximation Schemes for Batch Reinforcement Learning: A Theoretical Comparison}, author = {Xie, Tengyang and Jiang, Nan}, booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)}, pages = {550--559}, year = {2020}, editor = {Jonas Peters and David Sontag}, volume = {124}, series = {Proceedings of Machine Learning Research}, month = {03--06 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v124/xie20a/xie20a.pdf}, url = { http://proceedings.mlr.press/v124/xie20a.html }, abstract = {We prove performance guarantees of two algorithms for approximating Q* in batch reinforcement learning. Compared to classical iterative methods such as Fitted Q-Iteration—whose performance loss incurs quadratic dependence on horizon—these methods estimate (some forms of) the Bellman error and enjoy linear-in-horizon error propagation, a property established for the first time for algorithms that rely solely on batch data and output stationary policies. One of the algorithms uses a novel and explicit importance-weighting correction to overcome the infamous "double sampling" difficulty in Bellman error estimation, and does not use any squared losses. Our analyses reveal its distinct characteristics and potential advantages compared to classical algorithms. } }

@InProceedings{pmlr-v97-chen19e, title = {Information-Theoretic Considerations in Batch Reinforcement Learning}, author = {Chen, Jinglin and Jiang, Nan}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {1042--1051}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/chen19e/chen19e.pdf}, url = { http://proceedings.mlr.press/v97/chen19e.html }, abstract = {Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.} }

@InProceedings{pmlr-v97-le19a, title = {Batch Policy Learning under Constraints}, author = {Le, Hoang and Voloshin, Cameron and Yue, Yisong}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {3703--3712}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/le19a/le19a.pdf}, url = { http://proceedings.mlr.press/v97/le19a.html }, abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. As part of off-policy learning, we propose a simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.} }

@article{liu2020provably,
  title={Provably good batch reinforcement learning without great exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:2007.08202},
  year={2020}
}

@article{cui2020plug,
  title={Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?},
  author={Cui, Qiwen and Yang, Lin F},
  journal={arXiv preprint arXiv:2010.05673},
  year={2020}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={5717--5727},
  year={2017}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}

@article{lattimore2014near,
  title={Near-optimal PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  journal={Theoretical Computer Science},
  volume={558},
  pages={125--143},
  year={2014},
  publisher={Elsevier}
}

@article{pananjady2020instance,
  title={Instance-Dependent l-infinity-Bounds for Policy Evaluation in Tabular Reinforcement Learning},
  author={Pananjady, Ashwin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={1},
  pages={566--585},
  year={2020},
  publisher={IEEE}
}

@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2},
  pages={2818--2826},
  year={2015}
}

@inproceedings{jin2018qlearning,
 author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {4863--4873},
 publisher = {Curran Associates, Inc.},
 title = {Is Q-Learning Provably Efficient?},
 url = {https://proceedings.neurips.cc/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{dai2017learning,
  title={Learning combinatorial optimization algorithms over graphs},
  author={Dai, Hanjun and Khalil, Elias B and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  journal={arXiv preprint arXiv:1704.01665},
  year={2017}
}


@inproceedings{mandel2014offline,
  title={Offline policy evaluation across representations with applications to educational games},
  author={Mandel, Travis and Liu, Yun-En and Levine, Sergey and Brunskill, Emma and Popovic, Zoran},
  booktitle={AAMAS},
  pages={1077--1084},
  year={2014}
}

@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={16--18},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{murphy2001marginal,
  title={Marginal mean models for dynamic regimes},
  author={Murphy, Susan A and van der Laan, Mark J and Robins, James M and Conduct Problems Prevention Research Group},
  journal={Journal of the American Statistical Association},
  volume={96},
  number={456},
  pages={1410--1423},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{swaminathan2016off,
  title={Off-policy evaluation for slate recommendation},
  author={Swaminathan, Adith and Krishnamurthy, Akshay and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Langford, John and Jose, Damien and Zitouni, Imed},
  journal={arXiv preprint arXiv:1605.04812},
  year={2016}
}

@inproceedings{li2011unbiased,
  title={Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms},
  author={Li, Lihong and Chu, Wei and Langford, John and Wang, Xuanhui},
  booktitle={Proceedings of the fourth ACM international conference on Web search and data mining},
  pages={297--306},
  year={2011}
}

@inproceedings{chen2019top,
  title={Top-k off-policy correction for a REINFORCE recommender system},
  author={Chen, Minmin and Beutel, Alex and Covington, Paul and Jain, Sagar and Belletti, Francois and Chi, Ed H},
  booktitle={Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
  pages={456--464},
  year={2019}
}

@article{ghandeharioun2019approximating,
  title={Approximating interactive human evaluation with self-play for open-domain dialog systems},
  author={Ghandeharioun, Asma and Shen, Judy Hanwen and Jaques, Natasha and Ferguson, Craig and Jones, Noah and Lapedriza, Agata and Picard, Rosalind},
  journal={arXiv preprint arXiv:1906.09308},
  year={2019}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@inproceedings{li2015toward,
  title={Toward minimax off-policy value estimation},
  author={Li, Lihong and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  booktitle={Artificial Intelligence and Statistics},
  pages={608--616},
  year={2015},
  organization={PMLR}
}

@article{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  journal={arXiv preprint arXiv:1810.12429},
  year={2018}
}

@inproceedings{uehara2020minimax,
  title={Minimax weight and q-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={9659--9668},
  year={2020},
  organization={PMLR}
}

@article{zhang2020gendice,
  title={Gendice: Generalized offline estimation of stationary values},
  author={Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2002.09072},
  year={2020}
}

@article{yang2020off,
  title={Off-policy evaluation via the regularized lagrangian},
  author={Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2007.03438},
  year={2020}
}

@article{xie2019towards,
  title={Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
  author={Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:1906.03393},
  year={2019}
}

@article{charles2013counterfactual,
  title={Counterfactual reasoning and learning systems: The example of computational advertising},
  author={Charles, Denis and Chickering, Max and Simard, Patrice},
  journal={Journal of Machine Learning Research},
  volume={14},
  year={2013}
}

@inproceedings{gelada2019off,
  title={Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author={Gelada, Carles and Bellemare, Marc G},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3647--3655},
  year={2019}
}

@inproceedings{hallak2017consistent,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={1372--1383},
  year={2017},
  organization={PMLR}
}

@inproceedings{liu2020off,
  title={Off-Policy Policy Gradient with Stationary Distribution Correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1180--1190},
  year={2020},
  organization={PMLR}
}

@article{nachum2019algaedice,
  title={Algaedice: Policy gradient from arbitrary experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}

@article{kallus2019efficiently,
  title={Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={arXiv preprint arXiv:1909.05850},
  year={2019}
}

@InProceedings{pmlr-v75-jiang18a, title = {Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon}, author = {Jiang, Nan and Agarwal, Alekh}, booktitle = {Proceedings of the 31st Conference On Learning Theory}, pages = {3395--3398}, year = {2018}, editor = {Sébastien Bubeck and Vianney Perchet and Philippe Rigollet}, volume = {75}, series = {Proceedings of Machine Learning Research}, address = {}, month = {06--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v75/jiang18a/jiang18a.pdf}, url = {http://proceedings.mlr.press/v75/jiang18a.html}, abstract = {In reinforcement learning (RL), problems with long planning horizons are perceived as very challenging. The recent advances in PAC RL, however, show that the sample complexity of RL does not depend on planning horizon except at a superficial level. How can we explain such a difference? Noting that the technical assumptions in these upper bounds might have hidden away the challenges of long horizons, we ask the question: \emph{can we prove a lower bound with a horizon dependence when such assumptions are removed?} We also provide a few observations on the desired characteristics of the lower bound construction.} }

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{kallus2020double,
  title={Double reinforcement learning for efficient off-policy evaluation in markov decision processes},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={167},
  pages={1--63},
  year={2020}
}

@book{mohri2012book,
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  title = {Foundations of Machine Learning},
  year = {2012},
  isbn = {026201825X},
  publisher = {The MIT Press},
  abstract = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar. }
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@article{xie2020batch,
  title={Batch value-function approximation with only realizability},
  author={Xie, Tengyang and Jiang, Nan},
  journal={arXiv preprint arXiv:2008.04990},
  year={2020}
}

@misc{yin2021nearoptimal,
      title={Near-Optimal Offline Reinforcement Learning via Double Variance Reduction}, 
      author={Ming Yin and Yu Bai and Yu-Xiang Wang},
      year={2021},
      eprint={2102.01748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yang2019sample,
  title={Sample-optimal parametric Q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}

@inproceedings{feng2020accountable,
  title={Accountable off-policy evaluation with kernel bellman statistics},
  author={Feng, Yihao and Ren, Tongzheng and Tang, Ziyang and Liu, Qiang},
  booktitle={International Conference on Machine Learning},
  pages={3102--3111},
  year={2020},
  organization={PMLR}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}

@article{liu2019off,
  title={Off-policy policy gradient with state distribution correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:1904.08473},
  year={2019}
}