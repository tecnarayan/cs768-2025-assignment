\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aitchison(2018)]{aitchison2018unified}
Laurence Aitchison.
\newblock A unified theory of adaptive stochastic gradient descent as bayesian
  filtering.
\newblock \emph{arXiv preprint arXiv:1807.07540}, 2018.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bensoussan(2004)]{bensoussan2004stochastic}
Alain Bensoussan.
\newblock \emph{Stochastic control of partially observable systems}.
\newblock Cambridge University Press, 2004.

\bibitem[Carmona(2016)]{carmona2016lectures}
Ren{\'e} Carmona.
\newblock \emph{Lectures on BSDEs, stochastic control, and stochastic
  differential games with financial applications}, volume~1.
\newblock SIAM, 2016.

\bibitem[Casgrain and Jaimungal(2018{\natexlab{a}})]{casgrain2018algorithmic}
Philippe Casgrain and Sebastian Jaimungal.
\newblock Mean field games with partial information for algorithmic trading.
\newblock \emph{arXiv preprint arXiv:1803.04094}, 2018{\natexlab{a}}.

\bibitem[Casgrain and Jaimungal(2018{\natexlab{b}})]{casgrain2018mean}
Philippe Casgrain and Sebastian Jaimungal.
\newblock Mean-field games with differing beliefs for algorithmic trading.
\newblock \emph{arXiv preprint arXiv:1810.06101}, 2018{\natexlab{b}}.

\bibitem[Casgrain and Jaimungal(2018{\natexlab{c}})]{casgrain2018trading}
Philippe Casgrain and Sebastian Jaimungal.
\newblock Trading algorithms with learning in latent alpha models.
\newblock \emph{arXiv preprint arXiv:1806.04472}, 2018{\natexlab{c}}.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[da~Silva and Gazeau(2018)]{da2018general}
Andr{\'e}~Belotto da~Silva and Maxime Gazeau.
\newblock A general system of differential equations to model first order
  adaptive algorithms.
\newblock \emph{arXiv preprint arXiv:1810.13108}, 2018.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Gupta et~al.(2017)Gupta, Koren, and Singer]{gupta2017unified}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock A unified approach to adaptive regularization in online and
  stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1706.06569}, 2017.

\bibitem[Jacod and Shiryaev(2013)]{jacod2013limit}
Jean Jacod and Albert Shiryaev.
\newblock \emph{Limit theorems for stochastic processes}, volume 288.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Jankovi{\'c} et~al.(2012)Jankovi{\'c}, Jovanovi{\'c}, and
  Djordjevi{\'c}]{jankovic2012perturbed}
Svetlana Jankovi{\'c}, Miljana Jovanovi{\'c}, and Jasmina Djordjevi{\'c}.
\newblock Perturbed backward stochastic differential equations.
\newblock \emph{Mathematical and Computer Modelling}, 55\penalty0
  (5-6):\penalty0 1734--1745, 2012.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krichene and Bartlett(2017)]{krichene2017acceleration}
Walid Krichene and Peter~L Bartlett.
\newblock Acceleration and averaging in stochastic descent dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6796--6806, 2017.

\bibitem[Krichene et~al.(2015)Krichene, Bayen, and
  Bartlett]{krichene2015accelerated}
Walid Krichene, Alexandre Bayen, and Peter~L Bartlett.
\newblock Accelerated mirror descent in continuous and discrete time.
\newblock In \emph{Advances in neural information processing systems}, pages
  2845--2853, 2015.

\bibitem[Lucas et~al.(2018)Lucas, Sun, Zemel, and Grosse]{lucas2018aggregated}
James Lucas, Shengyang Sun, Richard Zemel, and Roger Grosse.
\newblock Aggregated momentum: Stability through passive damping.
\newblock \emph{arXiv preprint arXiv:1804.00325}, 2018.

\bibitem[Ma et~al.(1999)Ma, Morel, and Yong]{ma1999forward}
Jin Ma, J-M Morel, and Jiongmin Yong.
\newblock \emph{Forward-backward stochastic differential equations and their
  applications}.
\newblock Number 1702. Springer Science \& Business Media, 1999.

\bibitem[Mertikopoulos and Staudigl(2018)]{mertikopoulos2018convergence}
Panayotis Mertikopoulos and Mathias Staudigl.
\newblock On the convergence of gradient-like flows with noisy gradient input.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  163--197, 2018.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovsky1983problem}
Arkadii~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov()]{nesterov27method}
Yu~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Sov. Math. Dokl}, volume~27.

\bibitem[Pardoux and Tang(1999)]{pardoux1999forward}
Etienne Pardoux and Shanjian Tang.
\newblock Forward-backward stochastic differential equations and quasilinear
  parabolic pdes.
\newblock \emph{Probability Theory and Related Fields}, 114\penalty0
  (2):\penalty0 123--150, 1999.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Raginsky and Bouvrie(2012)]{raginsky2012continuous}
Maxim Raginsky and Jake Bouvrie.
\newblock Continuous-time stochastic mirror descent on a network: Variance
  reduction, consensus, convergence.
\newblock In \emph{2012 IEEE 51st IEEE Conference on Decision and Control
  (CDC)}, pages 6793--6800. IEEE, 2012.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Ruder(2016)]{ruder2016overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Su et~al.(2014)Su, Boyd, and Candes]{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling nesterov's accelerated gradient
  method: Theory and insights.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem[Van~Handel(2007)]{van2007stochastic}
Ramon Van~Handel.
\newblock Stochastic calculus, filtering, and stochastic control.
\newblock \emph{Course notes., URL http://www. princeton. edu/\~{}
  rvan/acm217/ACM217. pdf}, 2007.

\bibitem[Vuckovic(2018)]{vuckovic2018kalman}
James Vuckovic.
\newblock Kalman gradient descent: Adaptive variance reduction in stochastic
  optimization.
\newblock \emph{arXiv preprint arXiv:1810.12273}, 2018.

\bibitem[Walrand and Dimakis(2006)]{walrand-dimakis-randomProcesses}
Jean Walrand and Antonis Dimakis.
\newblock Random processes in systems - lecture notes.
\newblock Department of Electrical Engineering and Computer Sciences,
  University of California, Berkeley CA 94720, August 2006.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and
  Jordan]{wibisono2016variational}
Andre Wibisono, Ashia~C Wilson, and Michael~I Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 113\penalty0
  (47):\penalty0 E7351--E7358, 2016.

\bibitem[Wilson et~al.(2016)Wilson, Recht, and Jordan]{wilson2016lyapunov}
Ashia~C Wilson, Benjamin Recht, and Michael~I Jordan.
\newblock A lyapunov analysis of momentum methods in optimization.
\newblock \emph{arXiv preprint arXiv:1611.02635}, 2016.

\bibitem[Xu et~al.(2018{\natexlab{a}})Xu, Wang, and Gu]{xu2018accelerated}
Pan Xu, Tianhao Wang, and Quanquan Gu.
\newblock Accelerated stochastic mirror descent: From continuous-time dynamics
  to discrete-time algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1087--1096, 2018{\natexlab{a}}.

\bibitem[Xu et~al.(2018{\natexlab{b}})Xu, Wang, and Gu]{xu2018continuous}
Pan Xu, Tianhao Wang, and Quanquan Gu.
\newblock Continuous and discrete-time accelerated stochastic mirror descent
  for strongly convex functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  5488--5497, 2018{\natexlab{b}}.

\end{thebibliography}
