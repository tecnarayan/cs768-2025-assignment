\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams \& Fournier(2003)Adams and Fournier]{adams2003sobolev}
Adams, R.~A. and Fournier, J.~J.
\newblock \emph{Sobolev spaces}.
\newblock Elsevier, 2003.

\bibitem[Alemohammad et~al.(2023)Alemohammad, Casco-Rodriguez, Luzi, Humayun, Babaei, LeJeune, Siahkoohi, and Baraniuk]{alemohammad2023self}
Alemohammad, S., Casco-Rodriguez, J., Luzi, L., Humayun, A.~I., Babaei, H., LeJeune, D., Siahkoohi, A., and Baraniuk, R.~G.
\newblock Self-consuming generative models go mad.
\newblock \emph{arXiv preprint arXiv:2307.01850}, 2023.

\bibitem[Anderson(1982)]{anderson1982reverse}
Anderson, B.~D.
\newblock Reverse-time diffusion equation models.
\newblock \emph{Stochastic Processes and their Applications}, 12\penalty0 (3):\penalty0 313--326, 1982.

\bibitem[Antoniou et~al.(2017)Antoniou, Storkey, and Edwards]{antoniou2017data}
Antoniou, A., Storkey, A., and Edwards, H.
\newblock Data augmentation generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1711.04340}, 2017.

\bibitem[Azizi et~al.(2023)Azizi, Kornblith, Saharia, Norouzi, and Fleet]{azizi2023synthetic}
Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., and Fleet, D.~J.
\newblock Synthetic data from diffusion models improves imagenet classification.
\newblock \emph{arXiv preprint arXiv:2304.08466}, 2023.

\bibitem[Bertrand et~al.(2023)Bertrand, Bose, Duplessis, Jiralerspong, and Gidel]{bertrand2023stability}
Bertrand, Q., Bose, A.~J., Duplessis, A., Jiralerspong, M., and Gidel, G.
\newblock On the stability of iterative retraining of generative models on their own data.
\newblock \emph{arXiv preprint arXiv:2310.00429}, 2023.

\bibitem[Briesch et~al.(2023)Briesch, Sobania, and Rothlauf]{briesch2023large}
Briesch, M., Sobania, D., and Rothlauf, F.
\newblock Large language models suffer from their own output: An analysis of the self-consuming training loop.
\newblock \emph{arXiv preprint arXiv:2311.16822}, 2023.

\bibitem[Burg et~al.(2023)Burg, Wenzel, Zietlow, Horn, Makansi, Locatello, and Russell]{burg2023data}
Burg, M.~F., Wenzel, F., Zietlow, D., Horn, M., Makansi, O., Locatello, F., and Russell, C.
\newblock A data augmentation perspective on diffusion models and retrieval.
\newblock \emph{arXiv preprint arXiv:2304.10253}, 2023.

\bibitem[Cleanthous et~al.(2019)Cleanthous, Georgiadis, and Porcu]{cleanthous2019minimax}
Cleanthous, G., Georgiadis, A.~G., and Porcu, E.
\newblock Minimax density estimation on sobolev spaces with dominating mixed smoothness.
\newblock \emph{arXiv preprint arXiv:1906.06835}, 2019.

\bibitem[Devroye \& Lugosi(2001)Devroye and Lugosi]{devroye2001combinatorial}
Devroye, L. and Lugosi, G.
\newblock \emph{Combinatorial methods in density estimation}.
\newblock Springer Science \& Business Media, 2001.

\bibitem[DuMont~Sch{\"u}tte et~al.(2021)DuMont~Sch{\"u}tte, Hetzel, Gatidis, Hepp, Dietz, Bauer, and Schwab]{dumont2021overcoming}
DuMont~Sch{\"u}tte, A., Hetzel, J., Gatidis, S., Hepp, T., Dietz, B., Bauer, S., and Schwab, P.
\newblock Overcoming barriers to data sharing with medical image generation: a comprehensive evaluation.
\newblock \emph{NPJ digital medicine}, 4\penalty0 (1):\penalty0 141, 2021.

\bibitem[Fonseca et~al.(2023)Fonseca, Zappala, Caro, and Van~Dijk]{fonseca2023continuous}
Fonseca, A. H. D.~O., Zappala, E., Caro, J.~O., and Van~Dijk, D.
\newblock Continuous spatiotemporal transformer.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7343--7365. PMLR, 2023.

\bibitem[Han et~al.(2021)Han, Hu, and Long]{han2021class}
Han, J., Hu, R., and Long, J.
\newblock A class of dimensionality-free metrics for the convergence of empirical measures.
\newblock \emph{arXiv preprint arXiv:2104.12036}, 196, 2021.

\bibitem[Holmstr{\"o}m \& Klemel{\"a}(1992)Holmstr{\"o}m and Klemel{\"a}]{holmstrom1992asymptotic}
Holmstr{\"o}m, L. and Klemel{\"a}, J.
\newblock Asymptotic bounds for the expected l1 error of a multivariate kernel density estimator.
\newblock \emph{Journal of multivariate analysis}, 42\penalty0 (2):\penalty0 245--266, 1992.

\bibitem[Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han]{huang2022large}
Huang, J., Gu, S.~S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J.
\newblock Large language models can self-improve.
\newblock \emph{arXiv preprint arXiv:2210.11610}, 2022.

\bibitem[Huschens et~al.(2023)Huschens, Briesch, Sobania, and Rothlauf]{huschens2023you}
Huschens, M., Briesch, M., Sobania, D., and Rothlauf, F.
\newblock Do you trust chatgpt?--perceived credibility of human and ai-generated content.
\newblock \emph{arXiv preprint arXiv:2309.02524}, 2023.

\bibitem[Jiang(2017)]{jiang2017uniform}
Jiang, H.
\newblock Uniform convergence rates for kernel density estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1694--1703. PMLR, 2017.

\bibitem[Kontorovich(2014)]{kontorovich2014concentration}
Kontorovich, A.
\newblock Concentration in unbounded metric spaces and algorithmic stability.
\newblock In \emph{International conference on machine learning}, pp.\  28--36. PMLR, 2014.

\bibitem[Kroll(2021)]{kroll2021density}
Kroll, M.
\newblock On density estimation at a fixed point under local differential privacy.
\newblock 2021.

\bibitem[Li et~al.(2023)Li, Li, Zhang, and Bian]{li2023generalization}
Li, P., Li, Z., Zhang, H., and Bian, J.
\newblock On the generalization properties of diffusion models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Ma et~al.(2019)Ma, Wang, et~al.]{ma2019priori}
Ma, C., Wang, Q., et~al.
\newblock A priori estimates of the population risk for residual networks.
\newblock \emph{arXiv preprint arXiv:1903.02154}, 2019.

\bibitem[Mart{\'\i}nez et~al.(2023{\natexlab{a}})Mart{\'\i}nez, Watson, Reviriego, Hern{\'a}ndez, Juarez, and Sarkar]{martinez2023combining}
Mart{\'\i}nez, G., Watson, L., Reviriego, P., Hern{\'a}ndez, J.~A., Juarez, M., and Sarkar, R.
\newblock Combining generative artificial intelligence (ai) and the internet: Heading towards evolution or degradation?
\newblock \emph{arXiv preprint arXiv:2303.01255}, 2023{\natexlab{a}}.

\bibitem[Mart{\'\i}nez et~al.(2023{\natexlab{b}})Mart{\'\i}nez, Watson, Reviriego, Hern{\'a}ndez, Juarez, and Sarkar]{martinez2023towards}
Mart{\'\i}nez, G., Watson, L., Reviriego, P., Hern{\'a}ndez, J.~A., Juarez, M., and Sarkar, R.
\newblock Towards understanding the interplay of generative artificial intelligence and the internet.
\newblock \emph{arXiv preprint arXiv:2306.06130}, 2023{\natexlab{b}}.

\bibitem[Mroueh et~al.(2017)Mroueh, Li, Sercu, Raj, and Cheng]{mroueh2017sobolev}
Mroueh, Y., Li, C.-L., Sercu, T., Raj, A., and Cheng, Y.
\newblock Sobolev gan.
\newblock \emph{arXiv preprint arXiv:1711.04894}, 2017.

\bibitem[OpenAI(2023)]{OpenAI_2023}
OpenAI, O.
\newblock Gpt-4 technical report.
\newblock Mar 2023.

\bibitem[Ramesh et~al.()Ramesh, Dhariwal, Nichol, Chu, and Chen]{ramesh2022hierarchical}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Sadasivan et~al.(2023)Sadasivan, Kumar, Balasubramanian, Wang, and Feizi]{sadasivan2023can}
Sadasivan, V.~S., Kumar, A., Balasubramanian, S., Wang, W., and Feizi, S.
\newblock Can ai-generated text be reliably detected?
\newblock \emph{arXiv preprint arXiv:2303.11156}, 2023.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25278--25294, 2022.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shumailov et~al.(2023)Shumailov, Shumaylov, Zhao, Gal, Papernot, and Anderson]{shumailov2023curse}
Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., and Anderson, R.
\newblock The curse of recursion: Training on generated data makes models forget.
\newblock \emph{arXiv preprint arxiv:2305.17493}, 2023.

\bibitem[Song et~al.(2021)Song, Durkan, Murray, and Ermon]{song2021maximum}
Song, Y., Durkan, C., Murray, I., and Ermon, S.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 1415--1428, 2021.

\bibitem[Stein(1970)]{stein1970singular}
Stein, E.~M.
\newblock \emph{Singular integrals and differentiability properties of functions}.
\newblock Princeton university press, 1970.

\bibitem[Turinici(2019)]{turinici2019x}
Turinici, G.
\newblock X-ray sobolev variational auto-encoders.
\newblock 2019.

\bibitem[Van~Handel(2014)]{van2014probability}
Van~Handel, R.
\newblock Probability in high dimension.
\newblock \emph{Lecture Notes (Princeton University)}, 2014.

\bibitem[Villalobos et~al.(2022)Villalobos, Sevilla, Heim, Besiroglu, Hobbhahn, and Ho]{villalobos2022will}
Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A.
\newblock Will we run out of data? an analysis of the limits of scaling datasets in machine learning.
\newblock \emph{arXiv preprint arXiv:2211.04325}, 2022.

\bibitem[Wu \& Su(2023)Wu and Su]{wu2023implicit}
Wu, L. and Su, W.~J.
\newblock The implicit regularization of dynamical stability in stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:2305.17490}, 2023.

\bibitem[Yang(2022)]{yang2022mathematical}
Yang, H.
\newblock A mathematical framework for learning probability distributions.
\newblock \emph{arXiv preprint arXiv:2212.11481}, 2022.

\bibitem[Yang \& Weinan(2022)Yang and Weinan]{yang2022generalization}
Yang, H. and Weinan, E.
\newblock Generalization and memorization: The bias potential model.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pp.\  1013--1043. PMLR, 2022.

\end{thebibliography}
