\begin{thebibliography}{10}

\bibitem{al2019character}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3159--3166, 2019.

\bibitem{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In {\em International conference on machine learning}, pages
  136--145. PMLR, 2017.

\bibitem{andriushchenko2020square}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock In {\em European conference on computer vision}, pages 484--501.
  Springer, 2020.

\bibitem{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{bai2019deep}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{DBLP:journals/corr/abs-2103-14586}
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas
  Unterthiner, and Andreas Veit.
\newblock Understanding robustness of transformers for image classification.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 10231--10241, 2021.

\bibitem{bishop2006pattern}
Christopher~M Bishop and Nasser~M Nasrabadi.
\newblock {\em Pattern recognition and machine learning}, volume~4.
\newblock Springer, 2006.

\bibitem{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{bui2024revisiting}
Long~Minh Bui, Tho~Tran Huu, Duy Dinh, Tan~Minh Nguyen, and Trong~Nghia Hoang.
\newblock Revisiting kernel attention with correlated gaussian process
  representation.
\newblock In {\em The 40th Conference on Uncertainty in Artificial
  Intelligence}, 2024.

\bibitem{candes2009robust}
Emmanuel~J Cand{\`e}s, Xiaodong Li, Yi~Ma, and John Wright.
\newblock Robust principal component analysis?
\newblock {\em Journal of the ACM (JACM)}, 58(3):1--37, 2011.

\bibitem{DBLP:journals/corr/CarliniW16a}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em 2017 ieee symposium on security and privacy (sp)}, pages
  39--57. Ieee, 2017.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems},
  34:15084--15097, 2021.

\bibitem{chen2023calibrating}
Wenlong Chen and Yingzhen Li.
\newblock Calibrating transformers via sparse gaussian processes.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{chen2023primalattention}
Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan Suykens.
\newblock Primal-attention: Self-attention through asymmetric kernel svd in
  primal representation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{choromanski2021rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J Colwell, and Adrian
  Weller.
\newblock Rethinking attention with performers.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{JMLR:v24:22-1144}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In {\em International conference on machine learning}, pages
  2206--2216. PMLR, 2020.

\bibitem{de2001robust}
Fernando De~la Torre and Michael~J Black.
\newblock Robust principal component analysis for computer vision.
\newblock In {\em Proceedings Eighth IEEE International Conference on Computer
  Vision. ICCV 2001}, volume~1, pages 362--369. IEEE, 2001.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em
  Proceedings of the 2019 Conference of the North {A}merican Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.

\bibitem{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock In {\em International Conference on Machine Learning}, pages
  2793--2803. PMLR, 2021.

\bibitem{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{NEURIPS2021_23937b42}
Prasad Gabbur, Manjot Bilkhu, and Javier Movellan.
\newblock Probabilistic attention for interactive segmentation.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 4448--4460. Curran Associates, Inc., 2021.

\bibitem{gal2023an}
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or~Patashnik, Amit~Haim Bermano, Gal
  Chechik, and Daniel Cohen-or.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{geng2021attention}
Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke~Wei, and Zhouchen Lin.
\newblock Is attention better than matrix decomposition?
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{geshkovski2023mathematical}
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet.
\newblock A mathematical perspective on transformers.
\newblock {\em arXiv preprint arXiv:2312.10794}, 2023.

\bibitem{geshkovski2024emergence}
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet.
\newblock The emergence of clusters in self-attention dynamics.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{43405}
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{han2024designing}
Xing Han, Tongzheng Ren, Tan Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho.
\newblock Designing robust transformers using robust kernel density estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{DBLP:journals/corr/abs-2006-16241}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 8340--8349, 2021.

\bibitem{DBLP:journals/corr/abs-1903-12261}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 15262--15271, 2021.

\bibitem{hewitt-liang-2019-designing}
John Hewitt and Percy Liang.
\newblock Designing and interpreting probes with control tasks.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2733--2743, Hong Kong,
  China, November 2019. Association for Computational Linguistics.

\bibitem{hubert2005robpca}
Mia Hubert, Peter~J Rousseeuw, and Karlien Vanden~Branden.
\newblock Robpca: a new approach to robust principal component analysis.
\newblock {\em Technometrics}, 47(1):64--79, 2005.

\bibitem{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock {\em Advances in neural information processing systems},
  34:1273--1286, 2021.

\bibitem{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596(7873):583--589, 2021.

\bibitem{DBLP:journals/corr/abs-2006-16236}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In {\em International conference on machine learning}, pages
  5156--5165. PMLR, 2020.

\bibitem{khan2022transformers}
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed~Waqas Zamir, Fahad~Shahbaz
  Khan, and Mubarak Shah.
\newblock Transformers in vision: A survey.
\newblock {\em ACM computing surveys (CSUR)}, 54(10s):1--41, 2022.

\bibitem{NEURIPS2022_8bb0d291}
Takeshi Kojima, Shixiang~(Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 22199--22213. Curran Associates, Inc., 2022.

\bibitem{lee2022multi}
Kuang-Huei Lee, Ofir Nachum, Mengjiao~Sherry Yang, Lisa Lee, Daniel Freeman,
  Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski,
  et~al.
\newblock Multi-game decision transformers.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27921--27936, 2022.

\bibitem{lin2010augmented}
Zhouchen Lin, Minming Chen, and Yi~Ma.
\newblock The augmented lagrange multiplier method for exact recovery of
  corrupted low-rank matrices.
\newblock {\em arXiv preprint arXiv:1009.5055}, 2010.

\bibitem{lin2017a}
Zhouhan Lin, Minwei Feng, Cicero~Nogueira dos Santos, Mo~Yu, Bing Xiang, Bowen
  Zhou, and Yoshua Bengio.
\newblock A structured self-attentive sentence embedding.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{lu2019understanding}
Yiping Lu, Zhuohan Li, Di~He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
  Tie-Yan Liu.
\newblock Understanding and improving transformer from a multi-particle dynamic
  system point of view.
\newblock {\em arXiv preprint arXiv:1906.02762}, 2019.

\bibitem{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mao2022robust}
Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan
  He, and Hui Xue.
\newblock Towards robust vision transformer.
\newblock In {\em Proceedings of the IEEE/CVF conference on Computer Vision and
  Pattern Recognition}, pages 12042--12051, 2022.

\bibitem{DBLP:journals/corr/MerityXBS16}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{morris2020textattack}
John Morris, Eli Lifland, Jin~Yong Yoo, Jake Grigsby, Di~Jin, and Yanjun Qi.
\newblock Textattack: A framework for adversarial attacks, data augmentation,
  and adversarial training in nlp.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 119--126, 2020.

\bibitem{nguyen2008robust}
Minh Nguyen and Fernando Torre.
\newblock Robust kernel principal component analysis.
\newblock {\em Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem{nguyen2023mitigating}
Tam Nguyen, Tan Nguyen, and Richard Baraniuk.
\newblock Mitigating over-smoothing in transformers via regularized nonlocal
  functionals.
\newblock {\em Advances in Neural Information Processing Systems},
  36:80233--80256, 2023.

\bibitem{nguyen2024pidformer}
Tam Nguyen, C{\'e}sar~A Uribe, Tan~M Nguyen, and Richard~G Baraniuk.
\newblock Pidformer: Transformer meets control theory.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2024.

\bibitem{nguyen2022improving}
Tam~Minh Nguyen, Tan~Minh Nguyen, Dung~DD Le, Duy~Khuong Nguyen, Viet-Anh Tran,
  Richard Baraniuk, Nhat Ho, and Stanley Osher.
\newblock Improving transformers with probabilistic attention keys.
\newblock In {\em International Conference on Machine Learning}, pages
  16595--16621. PMLR, 2022.

\bibitem{nguyen2022head}
Tan Nguyen, Tam Nguyen, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham,
  Khuong~Duy Nguyen, Nhat Ho, and Stanley Osher.
\newblock Improving transformer with an admixture of attention heads.
\newblock {\em Advances in neural information processing systems},
  35:27937--27952, 2022.

\bibitem{nguyen2022fourierformer}
Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho.
\newblock Fourierformer: Transformer meets generalized fourier integral
  theorem.
\newblock {\em Advances in Neural Information Processing Systems},
  35:29319--29335, 2022.

\bibitem{nguyen2021fmmformer}
Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang.
\newblock Fmmformer: Efficient and flexible transformer via decomposed
  near-field and far-field attention.
\newblock {\em Advances in neural information processing systems},
  34:29449--29463, 2021.

\bibitem{nguyen2023probabilistic}
Tan~M Nguyen, Tam Nguyen, Long Bui, Hai Do, Duy~Khuong Nguyen, Dung~D Le, Hung
  Tran-The, Nhat Ho, Stan~J Osher, and Richard~G Baraniuk.
\newblock A probabilistic framework for pruning transformers via a finite
  admixture of keys.
\newblock In {\em ICASSP 2023-2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 1--5. IEEE, 2023.

\bibitem{nguyen2023a}
Tan~Minh Nguyen, Tam~Minh Nguyen, Nhat Ho, Andrea~L. Bertozzi, Richard
  Baraniuk, and Stanley Osher.
\newblock A primal-dual framework for transformers and neural networks.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{nielsen2024elliptical}
Stefan~K Nielsen, Laziz~U Abdullaev, Rachel Teo, and Tan~M Nguyen.
\newblock Elliptical attention.
\newblock {\em Advances in Neural Information Processing Systems}, 2024.

\bibitem{papernot2018cleverhans}
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben
  Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy,
  Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang,
  Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi
  Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and
  Rujun Long.
\newblock Technical report on the cleverhans v2.1.0 adversarial examples
  library.
\newblock {\em arXiv preprint arXiv:1610.00768}, 2018.

\bibitem{parikh-etal-2016-decomposable}
Ankur Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock In Jian Su, Kevin Duh, and Xavier Carreras, editors, {\em Proceedings
  of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages 2249--2255, Austin, Texas, November 2016. Association for Computational
  Linguistics.

\bibitem{DBLP:journals/corr/abs-2105-07581}
Sayak Paul and Pin-Yu Chen.
\newblock Vision transformers are robust learners.
\newblock In {\em Proceedings of the AAAI conference on Artificial
  Intelligence}, volume~36, pages 2071--2081, 2022.

\bibitem{pearson1901liii}
Karl Pearson.
\newblock Liii. on lines and planes of closest fit to systems of points in
  space.
\newblock {\em The London, Edinburgh, and Dublin philosophical magazine and
  journal of science}, 2(11):559--572, 1901.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock {\em OpenAI report}, 2018.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{JMLR:v21:20-074}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of Machine Learning Research}, 21(140):1--67, 2020.

\bibitem{pmlr-v139-rao21a}
Roshan~M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter
  Abbeel, Tom Sercu, and Alexander Rives.
\newblock Msa transformer.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 8844--8856. PMLR, 18--24 Jul 2021.

\bibitem{rives2021biological}
Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason
  Liu, Demi Guo, Myle Ott, C~Lawrence Zitnick, Jerry Ma, et~al.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(15), 2021.

\bibitem{DBLP:journals/corr/abs-2110-11773}
Michael~E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr{\'e}.
\newblock Sinkformers: Transformers with doubly stochastic attention.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3515--3530. PMLR, 2022.

\bibitem{schlag2021linear}
Imanol Schlag, Kazuki Irie, and J{\"u}rgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In {\em International Conference on Machine Learning}, pages
  9355--9366. PMLR, 2021.

\bibitem{shi2022revisiting}
Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen
  M.~S. Lee, and James Kwok.
\newblock Revisiting over-smoothing in {BERT} from the perspective of graph.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{strudel2021segmenter}
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 7262--7272, 2021.

\bibitem{subramanya2022backdoor}
Akshayvarun Subramanya, Aniruddha Saha, Soroush~Abbasi Koohpayegani, Ajinkya
  Tejankar, and Hamed Pirsiavash.
\newblock Backdoor attacks on vision transformers.
\newblock {\em arXiv preprint arXiv:2206.08477}, 2022.

\bibitem{1189643}
J.A.K. Suykens, T.~Van~Gestel, J.~Vandewalle, and B.~De~Moor.
\newblock A support vector machine formulation to pca analysis and its kernel
  version.
\newblock {\em IEEE Transactions on Neural Networks}, 14(2):447--450, 2003.

\bibitem{tang2021probabilistic}
Binh Tang and David~S. Matteson.
\newblock Probabilistic transformer for time series analysis.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{tarzanagh2023transformers}
Davoud~Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak.
\newblock Transformers as support vector machines.
\newblock {\em arXiv preprint arXiv:2308.16898}, 2023.

\bibitem{tenney-etal-2019-bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick.
\newblock {BERT} rediscovers the classical {NLP} pipeline.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 4593--4601, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{DBLP:journals/corr/abs-2012-12877}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International conference on machine learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{DBLP:journals/corr/abs-1904-13000}
Florian Tramer and Dan Boneh.
\newblock Adversarial training and robustness for multiple perturbations.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{tran2024equivariant}
Viet-Hoang Tran, Thieu~N Vo, An~Nguyen The, Tho~Tran Huu, Minh-Khoi
  Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, and Tan~Minh Nguyen.
\newblock Equivariant neural functional networks for transformers.
\newblock {\em arXiv preprint arXiv:2410.04209}, 2024.

\bibitem{tsai-etal-2019-transformer}
Yao-Hung~Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and
  Ruslan Salakhutdinov.
\newblock Transformer dissection: An unified understanding for transformer{'}s
  attention via the lens of kernel.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
  {\em Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing (EMNLP-IJCNLP)}, pages 4344--4353, Hong Kong, China,
  November 2019. Association for Computational Linguistics.

\bibitem{uesato2018adversarial}
Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron Oord.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock In {\em International Conference on Machine Learning}, pages
  5025--5034. PMLR, 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{vig-belinkov-2019-analyzing}
Jesse Vig and Yonatan Belinkov.
\newblock Analyzing the structure of attention in a transformer language model.
\newblock In {\em Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
  and Interpreting Neural Networks for NLP}, pages 63--76, Florence, Italy,
  August 2019. Association for Computational Linguistics.

\bibitem{voita-etal-2019-analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 5797--5808, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{wang2022antioversmooth}
Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang.
\newblock Anti-oversmoothing in deep vision transformers via the fourier domain
  analysis: From theory to practice.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{DBLP:journals/corr/abs-2006-04768}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{wang2022transtab}
Zifeng Wang and Jimeng Sun.
\newblock Transtab: Learning transferable tabular transformers across tables.
\newblock {\em Advances in Neural Information Processing Systems},
  35:2902--2915, 2022.

\bibitem{yang2021graph}
Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng
  Zhang, Zengfeng Huang, and David Wipf.
\newblock Graph neural networks inspired by classical iterative algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  11773--11783. PMLR, 2021.

\bibitem{NEURIPS2019_dc6a7e65}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{yuan2009sparse}
Xiaoming Yuan and Junfeng Yang.
\newblock Sparse and low-rank matrix decomposition via alternating direction
  method.
\newblock {\em Pacific Journal of Optimization}, 9:167, 2013.

\bibitem{zhang-feng-2021-modeling-concentrated}
Shaolei Zhang and Yang Feng.
\newblock Modeling concentrated cross-attention for neural machine translation
  with {G}aussian mixture model.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
  Wen-tau Yih, editors, {\em Findings of the Association for Computational
  Linguistics: EMNLP 2021}, pages 1401--1411, Punta Cana, Dominican Republic,
  November 2021. Association for Computational Linguistics.

\bibitem{zhang2019deep}
Shuai Zhang, Lina Yao, Aixin Sun, and Yi~Tay.
\newblock Deep learning based recommender system: A survey and new
  perspectives.
\newblock {\em ACM Computing Surveys (CSUR)}, 52(1):1--38, 2019.

\bibitem{zheng2022online}
Qinqing Zheng, Amy Zhang, and Aditya Grover.
\newblock Online decision transformer.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 27042--27059. PMLR, 17--23 Jul 2022.

\bibitem{8100027}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 5122--5130, 2017.

\bibitem{zhou2022understanding}
Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi
  Feng, and Jose~M Alvarez.
\newblock Understanding the robustness in vision transformers.
\newblock In {\em International Conference on Machine Learning}, pages
  27378--27394. PMLR, 2022.

\bibitem{zhuang-etal-2021-robustly}
Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun.
\newblock A robustly optimized {BERT} pre-training approach with post-training.
\newblock In {\em Proceedings of the 20th Chinese National Conference on
  Computational Linguistics}, pages 1218--1227, Huhhot, China, August 2021.
  Chinese Information Processing Society of China.

\end{thebibliography}
