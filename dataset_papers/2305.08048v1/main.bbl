\begin{thebibliography}{98}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bastings et~al.(2017)Bastings, Titov, Aziz, Marcheggiani, and
  Sima'an]{DBLP:conf/emnlp/BastingsTAMS17}
Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., and Sima'an, K.
\newblock Graph convolutional encoders for syntax-aware neural machine
  translation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, pp.\  1957--1967, 2017.

\bibitem[Beck et~al.(2018)Beck, Haffari, and Cohn]{DBLP:conf/acl/CohnHB18}
Beck, D., Haffari, G., and Cohn, T.
\newblock Graph-to-sequence learning using gated graph neural networks.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  273--283, 2018.

\bibitem[Bevilacqua et~al.(2021)Bevilacqua, Zhou, and Ribeiro]{BevilacquaZ021}
Bevilacqua, B., Zhou, Y., and Ribeiro, B.
\newblock Size-invariant graph representations for graph classification
  extrapolations.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pp.\  837--851, 2021.

\bibitem[Chen et~al.(2020)Chen, Wei, Huang, Ding, and
  Li]{DBLP:conf/icml/ChenWHDL20}
Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y.
\newblock Simple and deep graph convolutional networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, pp.\  1725--1735, 2020.

\bibitem[Chen et~al.(2019)Chen, Villar, Chen, and
  Bruna]{DBLP:conf/nips/ChenVCB19}
Chen, Z., Villar, S., Chen, L., and Bruna, J.
\newblock On the equivalence between graph isomorphism testing and function
  approximation with gnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15868--15876, 2019.

\bibitem[Chien et~al.(2021)Chien, Peng, Li, and Milenkovic]{chien2021adaptive}
Chien, E., Peng, J., Li, P., and Milenkovic, O.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Cong et~al.(2021)Cong, Ramezani, and Mahdavi]{cong2021on}
Cong, W., Ramezani, M., and Mahdavi, M.
\newblock On provable benefits of depth in training graph convolutional
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{NIPS2016_04df4d43}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Dehmamy et~al.(2019)Dehmamy, Barabasi, and Yu]{NEURIPS2019_73bf6c41}
Dehmamy, N., Barabasi, A.-L., and Yu, R.
\newblock Understanding the representation power of graph neural networks in
  learning graph topology.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Deng et~al.(2022)Deng, Lian, Wu, and Chen]{deng2022graph}
Deng, L., Lian, D., Wu, C., and Chen, E.
\newblock Graph convolution network based recommender systems: Learning
  guarantee and item mixture powered strategy.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Devroye et~al.(1996)Devroye, Györfi, and Lugosi]{devroye}
Devroye, L., Györfi, L., and Lugosi, G.
\newblock \emph{A Probablistic Theory of Pattern Recognition}.
\newblock Springer, 1996.

\bibitem[Du et~al.(2019)Du, Hou, Salakhutdinov, P{\'{o}}czos, Wang, and
  Xu]{DBLP:conf/nips/DuHSPWX19}
Du, S.~S., Hou, K., Salakhutdinov, R., P{\'{o}}czos, B., Wang, R., and Xu, K.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi11a}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (61):\penalty0 2121--2159, 2011.

\bibitem[El{-}Yaniv \& Pechyony(2006)El{-}Yaniv and Pechyony]{Yaniv06}
El{-}Yaniv, R. and Pechyony, D.
\newblock Stable transductive learning.
\newblock In \emph{Conference on Learning Theory}, pp.\  35--49, 2006.

\bibitem[El{-}Yaniv \& Pechyony(2007)El{-}Yaniv and Pechyony]{yaniv_2007}
El{-}Yaniv, R. and Pechyony, D.
\newblock Transductive rademacher complexity and its applications.
\newblock In \emph{Conference on Learning Theory}, pp.\  157--171, 2007.

\bibitem[Esser et~al.(2021)Esser, Vankadara, and Ghoshdastidar]{esser_2021}
Esser, P.~M., Vankadara, L.~C., and Ghoshdastidar, D.
\newblock Learning theory can (sometimes) explain generalisation in graph
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  27043--27056, 2021.

\bibitem[Fan et~al.(2019)Fan, Ma, Li, He, Zhao, Tang, and
  Yin]{DBLP:conf/www/Fan0LHZTY19}
Fan, W., Ma, Y., Li, Q., He, Y., Zhao, Y.~E., Tang, J., and Yin, D.
\newblock Graph neural networks for social recommendation.
\newblock In \emph{The World Wide Web Conference}, pp.\  417--426, 2019.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and
  Pappas]{fazlyab2019}
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G.
\newblock Efficient and accurate estimation of lipschitz constants for deep
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Federer(1969)]{federer1969geometric}
Federer, H.
\newblock \emph{Geometric Measure Theory}.
\newblock Springer, 1969.

\bibitem[Feng et~al.(2022)Feng, Chen, Li, Sarkar, and Zhang]{feng2022how}
Feng, J., Chen, Y., Li, F., Sarkar, A., and Zhang, M.
\newblock How powerful are k-hop message passing graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{pyg}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with pytorch geometric.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Garg et~al.(2020)Garg, Jegelka, and Jaakkola]{pmlr-v119-garg20c}
Garg, V., Jegelka, S., and Jaakkola, T.
\newblock Generalization and representational limits of graph neural networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  3419--3430, 2020.

\bibitem[Gasteiger et~al.(2019)Gasteiger, Bojchevski, and
  Günnemann]{gasteiger2018combining}
Gasteiger, J., Bojchevski, A., and Günnemann, S.
\newblock Predict then propagate: Graph neural networks meet personalized
  pagerank.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Gin{\'e} \& Pe{\~n}a(1999)Gin{\'e} and Pe{\~n}a]{Gin1998DecouplingFD}
Gin{\'e}, E. and Pe{\~n}a, V.~H.
\newblock \emph{Decoupling: From Dependence to Independence}.
\newblock Springer, 1999.

\bibitem[Gori et~al.(2005)Gori, Monfardini, and Scarselli]{gori}
Gori, M., Monfardini, G., and Scarselli, F.
\newblock A new model for learning in graph domains.
\newblock In \emph{Proceedings. 2005 IEEE International Joint Conference on
  Neural Networks, 2005.}, pp.\  729--734, 2005.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{sage}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Han et~al.(2022)Han, Huang, Ma, Li, Tenenbaum, and
  Gan]{han2022learning}
Han, J., Huang, W., Ma, H., Li, J., Tenenbaum, J.~B., and Gan, C.
\newblock Learning physical dynamics with subequivariant graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{DBLP:conf/icml/HardtRS16}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning}, pp.\  1225--1234, 2016.

\bibitem[He et~al.(2021)He, Wei, Huang, and Xu]{he2021bernnet}
He, M., Wei, Z., Huang, Z., and Xu, H.
\newblock Bernnet: Learning arbitrary graph spectral filters via bernstein
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[He et~al.(2020)He, Deng, Wang, Li, Zhang, and
  Wang]{DBLP:conf/sigir/0001DWLZ020}
He, X., Deng, K., Wang, X., Li, Y., Zhang, Y., and Wang, M.
\newblock Lightgcn: Simplifying and powering graph convolution network for
  recommendation.
\newblock In \emph{Proceedings of the 43rd International {ACM} {SIGIR}
  Conference on Research and Development in Information Retrieval}, pp.\
  639--648, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NEURIPS2018_5a4be1fa}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Johnson et~al.(2018)Johnson, Gupta, and
  Fei{-}Fei]{DBLP:conf/cvpr/JohnsonGF18}
Johnson, J., Gupta, A., and Fei{-}Fei, L.
\newblock Image generation from scene graphs.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, pp.\
   1219--1228, 2018.

\bibitem[Ju et~al.(2023)Ju, Li, Sharma, and Zhang]{ju23generalization}
Ju, H., Li, D., Sharma, A., and Zhang, H.~R.
\newblock Generalization in graph neural networks: Improved pac-bayesian bounds
  on graph diffusion.
\newblock In \emph{Proceedings of The 26th International Conference on
  Artificial Intelligence and Statistics}, pp.\  6314--6341, 2023.

\bibitem[Keriven et~al.(2020)Keriven, Bietti, and Vaiter]{NEURIPS2020_f5a14d49}
Keriven, N., Bietti, A., and Vaiter, S.
\newblock Convergence and stability of graph convolutional networks on large
  random graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{kipf2017semisupervised}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kuzborskij \& Lampert(2018)Kuzborskij and
  Lampert]{pmlr-v80-kuzborskij18a}
Kuzborskij, I. and Lampert, C.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  2815--2824, 2018.

\bibitem[Landrieu \& Simonovsky(2018)Landrieu and
  Simonovsky]{DBLP:conf/cvpr/LandrieuS18}
Landrieu, L. and Simonovsky, M.
\newblock Large-scale point cloud semantic segmentation with superpoint graphs.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, pp.\
   4558--4567, 2018.

\bibitem[Latała \& Oleszkiewicz(1994)Latała and Oleszkiewicz]{latala1994}
Latała, R. and Oleszkiewicz, K.
\newblock On the best constant in the khinchin-kahane inequality.
\newblock \emph{Studia Mathematica}, 109\penalty0 (1):\penalty0 101--104, 1994.

\bibitem[Lei \& Tang(2021)Lei and Tang]{lei}
Lei, Y. and Tang, K.
\newblock Learning rates for stochastic gradient descent with nonconvex
  objectives.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 43\penalty0 (12):\penalty0 4505--4511, 2021.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Wang, Liu, Chen, and
  Xiong]{li22generalization}
Li, H., Wang, M., Liu, S., Chen, P., and Xiong, J.
\newblock Generalization guarantee of training graph convolutional networks
  with graph topology sampling.
\newblock In \emph{Proceedings of The 39th International Conference on Machine
  Learning}, pp.\  13014--13051, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Wang, Zhang, and
  Zhu]{li2022outofdistribution}
Li, H., Wang, X., Zhang, Z., and Zhu, W.
\newblock Out-of-distribution generalization on graphs: A survey.
\newblock \emph{arXiv preprint arXiv:2202.07987}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2018)Li, Han, and Wu]{DBLP:conf/aaai/LiHW18}
Li, Q., Han, Z., and Wu, X.
\newblock Deeper insights into graph convolutional networks for semi-supervised
  learning.
\newblock In \emph{Proceedings of the Thirty-Second {AAAI} Conference on
  Artificial Intelligence}, pp.\  3538--3545, 2018.

\bibitem[Li \& Liu(2021)Li and Liu]{Li2021ImprovedLR}
Li, S. and Liu, Y.
\newblock Improved learning rates for stochastic optimization: Two theoretical
  viewpoints.
\newblock \emph{arXiv preprint arXiv:2107.08686}, 2021.

\bibitem[Liao et~al.(2019)Liao, Zhao, Urtasun, and Zemel]{liao2018lanczosnet}
Liao, R., Zhao, Z., Urtasun, R., and Zemel, R.
\newblock Lanczosnet: Multi-scale deep graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liao et~al.(2021)Liao, Urtasun, and Zemel]{liao2021a}
Liao, R., Urtasun, R., and Zemel, R.
\newblock A {PAC}-bayesian approach to generalization bounds for graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{Liu2020TowardAT}
Liu, C., Zhu, L., and Belkin, M.
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock \emph{arXiv preprint arXiv:2003.00307}, 2020.

\bibitem[Luo et~al.(2020)Luo, Cheng, Xu, Yu, Zong, Chen, and
  Zhang]{DBLP:conf/nips/LuoCXYZC020}
Luo, D., Cheng, W., Xu, D., Yu, W., Zong, B., Chen, H., and Zhang, X.
\newblock Parameterized explainer for graph neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Lv(2021)]{Lv2021GeneralizationBF}
Lv, S.
\newblock Generalization bounds for graph convolutional neural networks via
  rademacher complexity.
\newblock \emph{arXiv preprint arXiv:2102.10234}, 2021.

\bibitem[Maron et~al.(2019)Maron, Ben-Hamu, Shamir, and
  Lipman]{maron2018invariant}
Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y.
\newblock Invariant and equivariant graph networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Oono \& Suzuki(2020{\natexlab{a}})Oono and
  Suzuki]{DBLP:conf/iclr/OonoS20}
Oono, K. and Suzuki, T.
\newblock Graph neural networks exponentially lose expressive power for node
  classification.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Oono \& Suzuki(2020{\natexlab{b}})Oono and Suzuki]{oono_2020}
Oono, K. and Suzuki, T.
\newblock Optimization and generalization analysis of transduction through
  gradient boosting and application to multi-scale graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{b}}.

\bibitem[Pfaff et~al.(2021)Pfaff, Fortunato, Sanchez{-}Gonzalez, and
  Battaglia]{DBLP:conf/iclr/PfaffFSB21}
Pfaff, T., Fortunato, M., Sanchez{-}Gonzalez, A., and Battaglia, P.~W.
\newblock Learning mesh-based simulation with graph networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Pisier(1989)]{pisier_1989}
Pisier, G.
\newblock \emph{The Volume of Convex Bodies and Banach Space Geometry}.
\newblock Cambridge Tracts in Mathematics. Cambridge University Press, 1989.

\bibitem[Qi et~al.(2017)Qi, Liao, Jia, Fidler, and
  Urtasun]{DBLP:conf/iccv/QiLJFU17}
Qi, X., Liao, R., Jia, J., Fidler, S., and Urtasun, R.
\newblock 3d graph neural networks for {RGBD} semantic segmentation.
\newblock In \emph{International Conference on Computer Vision}, pp.\
  5209--5218, 2017.

\bibitem[Rong et~al.(2020)Rong, Huang, Xu, and Huang]{Rong2020DropEdge:}
Rong, Y., Huang, W., Xu, T., and Huang, J.
\newblock Dropedge: Towards deep graph convolutional networks on node
  classification.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sanchez{-}Gonzalez et~al.(2020)Sanchez{-}Gonzalez, Godwin, Pfaff,
  Ying, Leskovec, and Battaglia]{DBLP:conf/icml/Sanchez-Gonzalez20}
Sanchez{-}Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and
  Battaglia, P.~W.
\newblock Learning to simulate complex physics with graph networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pp.\  8459--8468, 2020.

\bibitem[Satorras \& Estrach(2018)Satorras and
  Estrach]{DBLP:conf/iclr/SatorrasE18}
Satorras, V.~G. and Estrach, J.~B.
\newblock Few-shot learning with graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Scarselli et~al.(2009)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{scar}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
\newblock The graph neural network model.
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (1):\penalty0 61--80, 2009.

\bibitem[Scarselli et~al.(2018)Scarselli, Tsoi, and
  Hagenbuchner]{SCARSELLI2018248}
Scarselli, F., Tsoi, A.~C., and Hagenbuchner, M.
\newblock The vapnik–chervonenkis dimension of graph and recursive neural
  networks.
\newblock \emph{Neural Networks}, 108:\penalty0 248--259, 2018.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Gallagher, and
  Eliassi{-}Rad]{DBLP:journals/aim/SenNBGGE08}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B., and Eliassi{-}Rad,
  T.
\newblock Collective classification in network data.
\newblock \emph{{AI} magazine}, 29\penalty0 (3):\penalty0 93--106, 2008.

\bibitem[Shen et~al.(2021)Shen, Luo, Zhou, Yu, and Du]{Shen2021NPIGNNPN}
Shen, Z.-A., Luo, T., Zhou, Y.-K., Yu, H., and Du, P.-F.
\newblock Npi-gnn: Predicting ncrna-protein interactions with deep graph neural
  networks.
\newblock \emph{Briefings in bioinformatics}, 2021.

\bibitem[Song et~al.(2018)Song, Zhang, Wang, and
  Gildea]{DBLP:conf/acl/GildeaWZS18}
Song, L., Zhang, Y., Wang, Z., and Gildea, D.
\newblock A graph-to-sequence model for amr-to-text generation.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  1616--1626, 2018.

\bibitem[Vapnik(1998)]{DBLP:books/daglib/0097035}
Vapnik, V.
\newblock \emph{Statistical learning theory}.
\newblock Wiley, 1998.

\bibitem[Vapnik(2006)]{DBLP:books/sp/Vapnik06}
Vapnik, V.
\newblock \emph{Estimation of Dependences Based on Empirical Data, Second
  Editiontion}.
\newblock Springer, 2006.

\bibitem[Veličković et~al.(2018)Veličković, Cucurull, Casanova, Romero,
  Liò, and Bengio]{gat}
Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., and Bengio,
  Y.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Verma \& Zhang(2019)Verma and Zhang]{verma}
Verma, S. and Zhang, Z.-L.
\newblock Stability and generalization of graph convolutional neural networks.
\newblock In \emph{Proceedings of the 25th {ACM} {SIGKDD} International
  Conference on Knowledge Discovery {\&} Data Mining}, pp.\  1539–1548, 2019.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{virmaux2018}
Virmaux, A. and Scaman, K.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Vu \& Thai(2020)Vu and Thai]{NEURIPS2020_8fb134f2}
Vu, M. and Thai, M.~T.
\newblock Pgm-explainer: Probabilistic graphical model explanations for graph
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12225--12235, 2020.

\bibitem[Wang et~al.(2021)Wang, Wang, Yang, and Lin]{yifei}
Wang, Y., Wang, Y., Yang, J., and Lin, Z.
\newblock Dissecting the diffusion process in linear graph convolutional
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Wu et~al.(2019)Wu, Souza, Zhang, Fifty, Yu, and Weinberger]{sgc}
Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  6861--6871, 2019.

\bibitem[Wu et~al.(2022)Wu, Zhang, Yan, and Wipf]{wu2022handling}
Wu, Q., Zhang, H., Yan, J., and Wipf, D.
\newblock Handling distribution shifts on graphs: An invariance perspective.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wu et~al.(2023)Wu, Chen, Yang, and Yan]{wu2023energybased}
Wu, Q., Chen, Y., Yang, C., and Yan, J.
\newblock Energy-based out-of-distribution detection for graph neural networks.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Wu et~al.(2021)Wu, Pan, Chen, Long, Zhang, and Yu]{9046288}
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P.~S.
\newblock A comprehensive survey on graph neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  32\penalty0 (1):\penalty0 4--24, 2021.

\bibitem[Xu et~al.(2018)Xu, Li, Tian, Sonobe, Kawarabayashi, and
  Jegelka]{DBLP:conf/icml/XuLTSKJ18}
Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K., and Jegelka, S.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pp.\  5449--5458, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018how}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Du, ichi Kawarabayashi, and
  Jegelka]{Xu2020What}
Xu, K., Li, J., Zhang, M., Du, S.~S., ichi Kawarabayashi, K., and Jegelka, S.
\newblock What can neural networks reason about?
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Xu et~al.(2021)Xu, Zhang, Li, Du, Kawarabayashi, and
  Jegelka]{xu2021how}
Xu, K., Zhang, M., Li, J., Du, S.~S., Kawarabayashi, K.-I., and Jegelka, S.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xu \& Zeevi(2020)Xu and Zeevi]{xuyun}
Xu, Y. and Zeevi, A.
\newblock Towards optimal problem dependent generalization error bounds in
  statistical learning theory.
\newblock \emph{arXiv preprint arXiv:2011.06186}, 2020.

\bibitem[Yang et~al.(2023)Yang, Wu, Wang, and Yan]{yang2023graph}
Yang, C., Wu, Q., Wang, J., and Yan, J.
\newblock Graph neural networks are inherently good generalizers: Insights by
  bridging {GNN}s and {MLP}s.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Yang et~al.(2022)Yang, Zeng, Wu, Jia, and Yan]{yang2022learning}
Yang, N., Zeng, K., Wu, Q., Jia, X., and Yan, J.
\newblock Learning substructure invariance for out-of-distribution molecular
  representations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yang et~al.(2016)Yang, Cohen, and
  Salakhutdinov]{DBLP:conf/icml/YangCS16}
Yang, Z., Cohen, W.~W., and Salakhutdinov, R.
\newblock Revisiting semi-supervised learning with graph embeddings.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning}, pp.\  40--48, 2016.

\bibitem[Yehudai et~al.(2021)Yehudai, Fetaya, Meirom, Chechik, and
  Maron]{YehudaiFMCM21}
Yehudai, G., Fetaya, E., Meirom, E.~A., Chechik, G., and Maron, H.
\newblock From local structures to size generalization in graph neural
  networks.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pp.\  11975--11986, 2021.

\bibitem[Ying et~al.(2018)Ying, He, Chen, Eksombatchai, Hamilton, and
  Leskovec]{DBLP:conf/kdd/YingHCEHL18}
Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W.~L., and Leskovec, J.
\newblock Graph convolutional neural networks for web-scale recommender
  systems.
\newblock In \emph{Proceedings of the 24th {ACM} {SIGKDD} International
  Conference on Knowledge Discovery {\&} Data Mining}, pp.\  974--983, 2018.

\bibitem[Ying \& Campbell(2010)Ying and Campbell]{ying}
Ying, Y. and Campbell, C.
\newblock Rademacher chaos complexities for learning the kernel problem.
\newblock \emph{Neural Computation}, 22\penalty0 (11):\penalty0 2858--2886,
  2010.

\bibitem[Ying et~al.(2019)Ying, Bourgeois, You, Zitnik, and
  Leskovec]{DBLP:conf/nips/YingBYZL19}
Ying, Z., Bourgeois, D., You, J., Zitnik, M., and Leskovec, J.
\newblock Gnnexplainer: Generating explanations for graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9240--9251, 2019.

\bibitem[Yuan et~al.(2020)Yuan, Tang, Hu, and Ji]{10.1145/3394486.3403085}
Yuan, H., Tang, J., Hu, X., and Ji, S.
\newblock Xgnn: Towards model-level explanations of graph neural networks.
\newblock In \emph{Proceedings of the 26th {ACM} {SIGKDD} International
  Conference on Knowledge Discovery {\&} Data Mining}, pp.\  430–438, 2020.

\bibitem[Yuan et~al.(2021)Yuan, Yu, Wang, Li, and Ji]{pmlr-v139-yuan21c}
Yuan, H., Yu, H., Wang, J., Li, K., and Ji, S.
\newblock On explainability of graph neural networks via subgraph explorations.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pp.\  12241--12252, 2021.

\bibitem[Zeng et~al.(2020)Zeng, Zhou, Srivastava, Kannan, and
  Prasanna]{Zeng2020GraphSAINT}
Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V.
\newblock Graphsaint: Graph sampling based inductive learning method.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zeng et~al.(2021)Zeng, Zhang, Xia, Srivastava, Malevich, Kannan,
  Prasanna, Jin, and Chen]{DBLP:conf/nips/ZengZXSMKPJC21}
Zeng, H., Zhang, M., Xia, Y., Srivastava, A., Malevich, A., Kannan, R.,
  Prasanna, V.~K., Jin, L., and Chen, R.
\newblock Decoupling the depth and scope of graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  19665--19679, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Wang, Liu, Chen, and
  Xiong]{pmlr-v119-zhang20y}
Zhang, S., Wang, M., Liu, S., Chen, P.-Y., and Xiong, J.
\newblock Fast learning of graph neural networks with guaranteed
  generalizability: One-hidden-layer case.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  11268--11277, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Cui, and Zhu]{9039675}
Zhang, Z., Cui, P., and Zhu, W.
\newblock Deep learning on graphs: A survey.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  34\penalty0 (1):\penalty0 249--270, 2022.

\bibitem[Zhao \& Akoglu(2020)Zhao and Akoglu]{DBLP:conf/iclr/ZhaoA20}
Zhao, L. and Akoglu, L.
\newblock Pairnorm: Tackling oversmoothing in gnns.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Tang, Yang, Cao, and Gu]{dongruo}
Zhou, D., Tang, Y., Yang, Z., Cao, Y., and Gu, Q.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Cui, Hu, Zhang, Yang, Liu, Wang, Li, and
  Sun]{ZHOU202057}
Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and
  Sun, M.
\newblock Graph neural networks: A review of methods and applications.
\newblock \emph{AI Open}, 1:\penalty0 57--81, 2020.

\bibitem[Zhou \& Wang(2021)Zhou and Wang]{zhou2021enlarge}
Zhou, X. and Wang, H.
\newblock The generalization error of graph convolutional networks may enlarge
  with more layers.
\newblock \emph{Neurocomputing}, 424:\penalty0 97--106, 2021.

\bibitem[Zhu \& Koniusz(2021)Zhu and Koniusz]{DBLP:conf/iclr/ZhuK21}
Zhu, H. and Koniusz, P.
\newblock Simple spectral graph convolution.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhu et~al.(2021)Zhu, Xu, Zhang, Du, Zhang, Liu, Yang, and Wu]{yanqiao}
Zhu, Y., Xu, W., Zhang, J., Du, Y., Zhang, J., Liu, Q., Yang, C., and Wu, S.
\newblock A survey on graph structure learning: Progress and opportunities.
\newblock \emph{arXiv preprint arXiv:2103.03036}, 2021.

\end{thebibliography}
