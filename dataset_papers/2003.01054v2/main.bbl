\begin{thebibliography}{10}

\bibitem{geiger2019scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'e}phane d'Ascoli, Giulio Biroli, Cl{\'e}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock {\em arXiv preprint arXiv:1901.01608}, 2019.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444, 2015.

\bibitem{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock {\em IEEE Signal processing magazine}, 29(6):82--97, 2012.

\bibitem{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em arXiv preprint arXiv:1710.03667}, 2017.

\bibitem{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine learning and the bias-variance trade-off.
\newblock {\em arXiv preprint arXiv:1812.11118}, 2018.

\bibitem{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock {\em arXiv preprint arXiv:1810.08591}, 2018.

\bibitem{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}, 2019.

\bibitem{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock {\em arXiv preprint arXiv:1412.6614}, 2014.

\bibitem{spigler2018jamming}
Stefano Spigler, Mario Geiger, St{\'e}phane d'Ascoli, Levent Sagun, Giulio
  Biroli, and Matthieu Wyart.
\newblock A jamming transition from under-to over-parametrization affects loss
  landscape and generalization.
\newblock {\em arXiv preprint arXiv:1810.09665}, 2018.

\bibitem{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock {\em arXiv preprint arXiv:1912.02292}, 2019.

\bibitem{breiman1995reflections}
Leo Breiman.
\newblock Reflections after refereeing papers for nips.
\newblock {\em The Mathematics of Generalization}, pages 11--15, 1995.

\bibitem{opper1996statistical}
Manfred Opper and Wolfgang Kinzel.
\newblock Statistical mechanics of generalization.
\newblock In {\em Models of neural networks III}, pages 151--209. Springer,
  1996.

\bibitem{liu1998jamming}
Andrea~J Liu and Sidney~R Nagel.
\newblock Jamming is not just cool any more.
\newblock {\em Nature}, 396(6706):21--22, 1998.

\bibitem{engel2001statistical}
Andreas Engel and Christian Van~den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem{franz2016simplest}
Silvio Franz and Giorgio Parisi.
\newblock The simplest model of jamming.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  49(14):145001, 2016.

\bibitem{krzakala2007landscape}
Florent Krzakala and Jorge Kurchan.
\newblock Landscape analysis of constraint satisfaction problems.
\newblock {\em Physical Review E}, 76(2):021122, 2007.

\bibitem{zdeborova2007phase}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Phase transitions in the coloring of random graphs.
\newblock {\em Physical Review E}, 76(3):031131, 2007.

\bibitem{rosset2004margin}
Saharon Rosset, Ji~Zhu, and Trevor~J Hastie.
\newblock Margin maximizing loss functions.
\newblock In {\em Advances in neural information processing systems}, pages
  1237--1244, 2004.

\bibitem{chizat2018note}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'Alch\'{e} Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 2933--2943. Curran Associates, Inc., 2019.

\bibitem{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em arXiv preprint arXiv:1904.11955}, 2019.

\bibitem{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock {\em arXiv preprint arXiv:1902.06015}, 2019.

\bibitem{woodworth2019kernel}
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and deep regimes in overparametrized models.
\newblock {\em arXiv preprint arXiv:1906.05827}, 2019.

\bibitem{geiger2019disentangling}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy learning in deep neural networks: an
  empirical study.
\newblock {\em arXiv preprint arXiv:1906.08034}, 2019.

\bibitem{arora2019harnessing}
Sanjeev Arora, Simon~S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and
  Dingli Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock {\em arXiv preprint arXiv:1910.01663}, 2019.

\bibitem{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock {\em arXiv preprint arXiv:1711.00165}, 2017.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock {\em arXiv preprint arXiv:1705.03071}, 2017.

\bibitem{mezard1987spin}
Marc M{\'e}zard, Giorgio Parisi, and Miguel Virasoro.
\newblock {\em Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem{seung1992statistical}
Hyunjune~Sebastian Seung, Haim Sompolinsky, and Naftali Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock {\em Physical review A}, 45(8):6056, 1992.

\bibitem{advani2013statistical}
Madhu Advani, Subhaneil Lahiri, and Surya Ganguli.
\newblock Statistical mechanics of complex neural systems and high dimensional
  data.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2013(03):P03014, 2013.

\bibitem{zdeborova2016statistical}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Statistical physics of inference: Thresholds and algorithms.
\newblock {\em Advances in Physics}, 65(5):453--552, 2016.

\bibitem{livan2018introduction}
Giacomo Livan, Marcel Novaes, and Pierpaolo Vivo.
\newblock {\em Introduction to random matrices: theory and practice},
  volume~26.
\newblock Springer, 2018.

\bibitem{tarquini2016level}
Elena Tarquini, Giulio Biroli, and Marco Tarzia.
\newblock Level statistics and localization transitions of levy matrices.
\newblock {\em Physical review letters}, 116(1):010601, 2016.

\bibitem{aggarwal2018goe}
Amol Aggarwal, Patrick Lopatto, and Horng-Tzer Yau.
\newblock Goe statistics for levy matrices.
\newblock {\em arXiv preprint arXiv:1806.07363}, 2018.

\bibitem{nakkiran2019more}
Preetum Nakkiran.
\newblock More data can hurt for linear regression: Sample-wise double descent.
\newblock {\em arXiv preprint arXiv:1912.07242}, 2019.

\bibitem{belkin2019two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em arXiv preprint arXiv:1903.07571}, 2019.

\bibitem{deng2019model}
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis.
\newblock A model of double descent for high-dimensional binary linear
  classification.
\newblock {\em arXiv preprint arXiv:1911.05822}, 2019.

\bibitem{kini2020analytic}
Ganesh Kini and Christos Thrampoulidis.
\newblock Analytic study of double descent in binary classification: The impact
  of loss.
\newblock {\em arXiv preprint arXiv:2001.11572}, 2020.

\bibitem{replica2020}
Florent Krzakala Marc M\'ezard Lenka~Zdeborov\'a Federica~Gerace,
  Bruno~Loureiro.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock {\em arXiv:2002.09339}, 2020.

\bibitem{drucker1994boosting}
Harris Drucker, Corinna Cortes, Lawrence~D Jackel, Yann LeCun, and Vladimir
  Vapnik.
\newblock Boosting and other ensemble methods.
\newblock {\em Neural Computation}, 6(6):1289--1301, 1994.

\bibitem{zhang2013divide}
Yuchen Zhang, John Duchi, and Martin Wainwright.
\newblock Divide and conquer kernel ridge regression.
\newblock In {\em Conference on Learning Theory}, pages 592--617, 2013.

\bibitem{goldt2019modelling}
Sebastian Goldt, Marc M{\'e}zard, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Modelling the influence of data structure on learning in neural
  networks.
\newblock {\em arXiv preprint arXiv:1909.11500}, 2019.

\bibitem{bun2016cleaning}
Jo{\"e}l Bun, Jean-Philippe Bouchaud, and Marc Potters.
\newblock Cleaning correlation matrices.
\newblock {\em Risk magazine}, 2015, 2016.

\bibitem{jacot2020implicit}
Arthur Jacot, Berfin {\c{S}}im{\c{s}}ek, Francesco Spadaro, Cl{\'e}ment
  Hongler, and Franck Gabriel.
\newblock Implicit regularization of random feature models.
\newblock {\em arXiv preprint arXiv:2002.08404}, 2020.

\bibitem{zhang2015divide}
Yuchen Zhang, John Duchi, and Martin Wainwright.
\newblock Divide and conquer kernel ridge regression: A distributed algorithm
  with minimax optimal rates.
\newblock {\em The Journal of Machine Learning Research}, 16(1):3299--3340,
  2015.

\bibitem{yang2020rethinking}
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi~Ma.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock {\em arXiv preprint arXiv:2002.11328}, 2020.

\bibitem{chawla2003distributed}
Nitesh~V Chawla, Thomas~E Moore, Lawrence~O Hall, Kevin~W Bowyer, W~Philip
  Kegelmeyer, and Clayton Springer.
\newblock Distributed learning with bagging-like performance.
\newblock {\em Pattern recognition letters}, 24(1-3):455--471, 2003.

\end{thebibliography}
