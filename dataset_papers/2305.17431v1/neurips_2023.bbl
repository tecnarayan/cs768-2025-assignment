\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{An2023LatentShiftLD}
An, J., Zhang, S., Yang, H., Gupta, S., Huang, J.B., Luo, J., Yin, X.:
  Latent-shift: Latent diffusion with temporal shift for efficient
  text-to-video generation. ArXiv  \textbf{abs/2304.08477} (2023)

\bibitem{arnab2021vivit}
Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu{\v{c}}i{\'c}, M., Schmid, C.:
  Vivit: A video vision transformer. In: Proceedings of the IEEE/CVF
  international conference on computer vision. pp. 6836--6846 (2021)

\bibitem{layernorm}
Ba, J., Kiros, J.R., Hinton, G.E.: Layer normalization. ArXiv
  \textbf{abs/1607.06450} (2016)

\bibitem{BarTal2022Text2LIVETL}
Bar-Tal, O., Ofri-Amar, D., Fridman, R., Kasten, Y., Dekel, T.: Text2live:
  Text-driven layered image and video editing. ArXiv  \textbf{abs/2204.02491}
  (2022)

\bibitem{bertasius2021space}
Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need
  for video understanding? In: ICML. vol.~2, p.~4 (2021)

\bibitem{ceylan2023pix2video}
Ceylan, D., Huang, C.H.P., Mitra, N.J.: Pix2video: Video editing using image
  diffusion. arXiv preprint arXiv:2303.12688  (2023)

\bibitem{3d-unet}
{\c{C}}i{\c{c}}ek, {\"O}., Abdulkadir, A., Lienkamp, S.S., Brox, T.,
  Ronneberger, O.: 3d u-net: learning dense volumetric segmentation from sparse
  annotation. In: Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2016: 19th International Conference, Athens, Greece,
  October 17-21, 2016, Proceedings, Part II 19. pp. 424--432. Springer (2016)

\bibitem{dhariwal2021diffusion}
Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis.
  Advances in Neural Information Processing Systems  \textbf{34},  8780--8794
  (2021)

\bibitem{esser2023structure}
Esser, P., Chiu, J., Atighehchian, P., Granskog, J., Germanidis, A.: Structure
  and content-guided video synthesis with diffusion models. arXiv preprint
  arXiv:2302.03011  (2023)

\bibitem{gal2022image}
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
  Cohen-Or, D.: An image is worth one word: Personalizing text-to-image
  generation using textual inversion. arXiv preprint arXiv:2208.01618  (2022)

\bibitem{softmax}
Gao, B., Pavel, L.: On the properties of the softmax function with application
  in game theory and reinforcement learning. ArXiv  \textbf{abs/1704.00805}
  (2017)

\bibitem{gur2020hierarchical}
Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: Generating diverse
  videos from a single sample. Advances in Neural Information Processing
  Systems  \textbf{33},  16761--16772 (2020)

\bibitem{hertz2022prompt}
Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.:
  Prompt-to-prompt image editing with cross attention control. arXiv preprint
  arXiv:2208.01626  (2022)

\bibitem{hessel2021clipscore}
Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.: Clipscore: A
  reference-free evaluation metric for image captioning. arXiv preprint
  arXiv:2104.08718  (2021)

\bibitem{ho2022imagen}
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P.,
  Poole, B., Norouzi, M., Fleet, D.J., et~al.: Imagen video: High definition
  video generation with diffusion models. arXiv preprint arXiv:2210.02303
  (2022)

\bibitem{ddpm}
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models (2020).
  \doi{10.48550/ARXIV.2006.11239}, \url{https://arxiv.org/abs/2006.11239}

\bibitem{ho2020denoising}
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models.
  Advances in Neural Information Processing Systems  \textbf{33},  6840--6851
  (2020)

\bibitem{ho2019axial}
Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in
  multidimensional transformers. arXiv preprint arXiv:1912.12180  (2019)

\bibitem{ho2022classifier}
Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
  arXiv:2207.12598  (2022)

\bibitem{ho2022video}
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
  diffusion models. arXiv:2204.03458  (2022)

\bibitem{VDM}
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
  diffusion models (2022). \doi{10.48550/ARXIV.2204.03458},
  \url{https://arxiv.org/abs/2204.03458}

\bibitem{hong2022cogvideo}
Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.: Cogvideo: Large-scale
  pretraining for text-to-video generation via transformers. arXiv preprint
  arXiv:2205.15868  (2022)

\bibitem{Hu2021MakeIM}
Hu, Y., Luo, C., Chen, Z.: Make it move: Controllable image-to-video generation
  with text descriptions. 2022 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) pp. 18198--18207 (2021)

\bibitem{bn}
Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training
  by reducing internal covariate shift. In: ICML (2015)

\bibitem{Karras2019stylegan2}
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.:
  Analyzing and improving the image quality of {StyleGAN}. In: Proc. CVPR
  (2020)

\bibitem{Khachatryan2023Text2VideoZeroTD}
Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z.,
  Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are
  zero-shot video generators. ArXiv  \textbf{abs/2303.13439} (2023)

\bibitem{kingma2021variational}
Kingma, D., Salimans, T., Poole, B., Ho, J.: Variational diffusion models.
  Advances in neural information processing systems  \textbf{34},  21696--21707
  (2021)

\bibitem{Kingma2013AutoEncodingVB}
Kingma, D.P., Welling, M.: Auto-encoding variational bayes. CoRR
  \textbf{abs/1312.6114} (2013)

\bibitem{kumari2022multi}
Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept
  customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488
  (2022)

\bibitem{Li2023BLIP2BL}
Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image
  pre-training with frozen image encoders and large language models. ArXiv
  \textbf{abs/2301.12597} (2023)

\bibitem{liu2023video}
Liu, S., Zhang, Y., Li, W., Lin, Z., Jia, J.: Video-p2p: Video editing with
  cross-attention control. arXiv preprint arXiv:2303.04761  (2023)

\bibitem{Miyato2018SpectralNF}
Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for
  generative adversarial networks. ArXiv  \textbf{abs/1802.05957} (2018)

\bibitem{mokady2022null}
Mokady, R., Hertz, A., Aberman, K., Pritch, Y., Cohen-Or, D.: Null-text
  inversion for editing real images using guided diffusion models. arXiv
  preprint arXiv:2211.09794  (2022)

\bibitem{molad2023dreamix}
Molad, E., Horwitz, E., Valevski, D., Acha, A.R., Matias, Y., Pritch, Y.,
  Leviathan, Y., Hoshen, Y.: Dreamix: Video diffusion models are general video
  editors. arXiv preprint arXiv:2302.01329  (2023)

\bibitem{nichol2021glide}
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
  Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and
  editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741
  (2021)

\bibitem{vqvae}
van~den Oord, A., Vinyals, O., Kavukcuoglu, K.: Neural discrete representation
  learning. In: NeurIPS (2018)

\bibitem{parmar2023zero}
Parmar, G., Singh, K.K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-shot
  image-to-image translation. arXiv preprint arXiv:2302.03027  (2023)

\bibitem{davis}
Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel{\'a}ez, P., Sorkine-Hornung,
  A., Van~Gool, L.: The 2017 davis challenge on video object segmentation.
  arXiv preprint arXiv:1704.00675  (2017)

\bibitem{qi2023fatezero}
Qi, C., Cun, X., Zhang, Y., Lei, C., Wang, X., Shan, Y., Chen, Q.: Fatezero:
  Fusing attentions for zero-shot text-based video editing. arXiv preprint
  arXiv:2303.09535  (2023)

\bibitem{clip}
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning
  transferable visual models from natural language supervision (2021).
  \doi{10.48550/ARXIV.2103.00020}, \url{https://arxiv.org/abs/2103.00020}

\bibitem{radford2021learning}
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.: Learning transferable visual
  models from natural language supervision. In: ICML. pp. 8748--8763. PMLR
  (2021)

\bibitem{2020t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
  unified text-to-text transformer. Journal of Machine Learning Research
  \textbf{21}(140),  1--67 (2020), \url{http://jmlr.org/papers/v21/20-074.html}

\bibitem{dalle2}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical
  text-conditional image generation with clip latents (2022).
  \doi{10.48550/ARXIV.2204.06125}, \url{https://arxiv.org/abs/2204.06125}

\bibitem{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
  image synthesis with latent diffusion models. In: CVPR. pp. 10684--10695
  (2022)

\bibitem{ruiz2022dreambooth}
Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.:
  Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
  generation. arXiv preprint arXiv:2208.12242  (2022)

\bibitem{saharia2022palette}
Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D.,
  Norouzi, M.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH 2022
  Conference Proceedings. pp. 1--10 (2022)

\bibitem{imagen}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
  S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet,
  D.J., Norouzi, M.: Photorealistic text-to-image diffusion models with deep
  language understanding (2022). \doi{10.48550/ARXIV.2205.11487},
  \url{https://arxiv.org/abs/2205.11487}

\bibitem{Salimans2016WeightNA}
Salimans, T., Kingma, D.P.: Weight normalization: A simple reparameterization
  to accelerate training of deep neural networks. In: NIPS (2016)

\bibitem{shin2023edit}
Shin, C., Kim, H., Lee, C.H., Lee, S.g., Yoon, S.: Edit-a-video: Single video
  editing with object-aware consistency. arXiv preprint arXiv:2303.07945
  (2023)

\bibitem{singer2022make}
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang,
  H., Ashual, O., Gafni, O., et~al.: Make-a-video: Text-to-video generation
  without text-video data. arXiv preprint arXiv:2209.14792  (2022)

\bibitem{song2020denoising}
Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv
  preprint arXiv:2010.02502  (2020)

\bibitem{song2017end}
Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An end-to-end spatio-temporal
  attention model for human action recognition from skeleton data. In:
  Proceedings of the AAAI conference on artificial intelligence. vol.~31 (2017)

\bibitem{song2020improved}
Song, Y., Ermon, S.: Improved techniques for training score-based generative
  models. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H.
  (eds.) Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual (2020)

\bibitem{song2021scorebased}
Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.:
  Score-based generative modeling through stochastic differential equations.
  In: International Conference on Learning Representations (2021),
  \url{https://openreview.net/forum?id=PxTIG12RRHS}

\bibitem{tumanyan2022plug}
Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features
  for text-driven image-to-image translation. arXiv preprint arXiv:2211.12572
  (2022)

\bibitem{Tzaban2022StitchII}
Tzaban, R., Mokady, R., Gal, R., Bermano, A.H., Cohen-Or, D.: Stitch it in
  time: Gan-based facial editing of real videos. SIGGRAPH Asia 2022 Conference
  Papers  (2022)

\bibitem{InstanceNT}
Ulyanov, D., Vedaldi, A., Lempitsky, V.S.: Instance normalization: The missing
  ingredient for fast stylization. ArXiv  \textbf{abs/1607.08022} (2016)

\bibitem{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
  Kaiser, L., Polosukhin, I.: Attention is all you need (2017).
  \doi{10.48550/ARXIV.1706.03762}, \url{https://arxiv.org/abs/1706.03762}

\bibitem{villegas2022phenaki}
Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H.,
  Saffar, M.T., Castro, S., Kunze, J., Erhan, D.: Phenaki: Variable length
  video generation from open domain textual description. arXiv preprint
  arXiv:2210.02399  (2022)

\bibitem{villegas2023phenaki}
Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H.,
  Saffar, M.T., Castro, S., Kunze, J., Erhan, D.: Phenaki: Variable length
  video generation from open domain textual descriptions. In: ICLR (2023)

\bibitem{Wang2023ZeroShotVE}
Wang, W., Xie, K., Liu, Z., Chen, H., Cao, Y., Wang, X., Shen, C.: Zero-shot
  video editing using off-the-shelf image diffusion models. ArXiv
  \textbf{abs/2303.17599} (2023)

\bibitem{wu2022tune}
Wu, J.Z., Ge, Y., Wang, X., Lei, W., Gu, Y., Hsu, W., Shan, Y., Qie, X., Shou,
  M.Z.: Tune-a-video: One-shot tuning of image diffusion models for
  text-to-video generation. arXiv preprint arXiv:2212.11565  (2022)

\bibitem{yang2022Vtoonify}
Yang, S., Jiang, L., Liu, Z., Loy, C.C.: Vtoonify: Controllable high-resolution
  portrait video style transfer. ACM Transactions on Graphics (TOG)
  \textbf{41}(6),  1--15 (2022). \doi{10.1145/3550454.3555437}

\end{thebibliography}
