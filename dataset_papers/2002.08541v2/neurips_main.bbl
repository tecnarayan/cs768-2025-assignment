\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Steinhaus(1956)]{steinhaus1956division}
Hugo Steinhaus.
\newblock Sur la division des corp materiels en parties.
\newblock \emph{Bull. Acad. Polon. Sci}, 1\penalty0 (804):\penalty0 801, 1956.

\bibitem[Heller and Ghahramani(2005)]{heller2005bayesian}
Katherine~A Heller and Zoubin Ghahramani.
\newblock Bayesian hierarchical clustering.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 297--304, 2005.

\bibitem[Kulis and Jordan(2011)]{kulis2011revisiting}
Brian Kulis and Michael~I Jordan.
\newblock Revisiting k-means: New algorithms via bayesian nonparametrics.
\newblock \emph{arXiv preprint arXiv:1111.0352}, 2011.

\bibitem[Stephenson et~al.(2020)Stephenson, Herring, and
  Olshan]{stephenson2020robust}
Briana~JK Stephenson, Amy~H Herring, and Andrew Olshan.
\newblock Robust clustering with subpopulation-specific deviations.
\newblock \emph{Journal of the American Statistical Association}, 115\penalty0
  (530):\penalty0 521--537, 2020.

\bibitem[Jing et~al.(2007)Jing, Ng, and Huang]{jing2007entropy}
Liping Jing, Michael~K Ng, and Joshua~Zhexue Huang.
\newblock An entropy weighting k-means algorithm for subspace clustering of
  high-dimensional sparse data.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  19\penalty0 (8):\penalty0 1026--1041, 2007.

\bibitem[Vidal(2011)]{vidal2011subspace}
Ren{\'e} Vidal.
\newblock Subspace clustering.
\newblock \emph{IEEE Signal Processing Magazine}, 28\penalty0 (2):\penalty0
  52--68, 2011.

\bibitem[Chi et~al.(2016)Chi, Chi, and Baraniuk]{chi2016k}
Jocelyn~T Chi, Eric~C Chi, and Richard~G Baraniuk.
\newblock k-pod: {A} method for k-means clustering of missing data.
\newblock \emph{The American Statistician}, 70\penalty0 (1):\penalty0 91--99,
  2016.

\bibitem[Shah and Koltun(2017)]{shah2017robust}
Sohil~Atul Shah and Vladlen Koltun.
\newblock Robust continuous clustering.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (37):\penalty0 9814--9819, 2017.

\bibitem[Xu et~al.(2018)Xu, Chi, Yang, and Lange]{xu2018majorization}
Jason Xu, Eric~C Chi, Meng Yang, and Kenneth Lange.
\newblock A majorization--minimization algorithm for split feasibility
  problems.
\newblock \emph{Computational Optimization and Applications}, 71\penalty0
  (3):\penalty0 795--828, 2018.

\bibitem[Lloyd(1982)]{lloyd1982least}
Stuart Lloyd.
\newblock Least squares quantization in {PCM}.
\newblock \emph{IEEE Transactions on Information Theory}, 28\penalty0
  (2):\penalty0 129--137, 1982.

\bibitem[Mohamad and Usman(2013)]{mohamad2013standardization}
Ismail~Bin Mohamad and Dauda Usman.
\newblock Standardization and its effects on k-means clustering algorithm.
\newblock \emph{Research Journal of Applied Sciences, Engineering and
  Technology}, 6\penalty0 (17):\penalty0 3299--3303, 2013.

\bibitem[Dasgupta(2008)]{dasgupta2008hardness}
Sanjoy Dasgupta.
\newblock \emph{The hardness of k-means clustering}.
\newblock Department of Computer Science and Engineering, University of
  California, 2008.

\bibitem[Vattani(2009)]{vattani2009hardness}
Andrea Vattani.
\newblock The hardness of k-means clustering in the plane.
\newblock \emph{Manuscript, accessible at http://cseweb. ucsd.
  edu/avattani/papers/kmeans\_hardness. pdf}, 617, 2009.

\bibitem[Mahajan et~al.(2012)Mahajan, Nimbhorkar, and
  Varadarajan]{mahajan2012planar}
Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan.
\newblock The planar k-means problem is {NP}-hard.
\newblock \emph{Theoretical Computer Science}, 442:\penalty0 13--21, 2012.

\bibitem[Bradley and Fayyad(1998)]{bradley1998refining}
Paul~S Bradley and Usama~M Fayyad.
\newblock Refining initial points for k-means clustering.
\newblock In \emph{ICML}, volume~98, pages 91--99. Citeseer, 1998.

\bibitem[Al~Hasan et~al.(2009)Al~Hasan, Chaoji, Salem, and Zaki]{al2009robust}
Mohammad Al~Hasan, Vineet Chaoji, Saeed Salem, and Mohammed~J Zaki.
\newblock Robust partitional clustering by outlier and density insensitive
  seeding.
\newblock \emph{Pattern Recognition Letters}, 30\penalty0 (11):\penalty0
  994--1002, 2009.

\bibitem[Celebi et~al.(2013)Celebi, Kingravi, and Vela]{celebi2013comparative}
M~Emre Celebi, Hassan~A Kingravi, and Patricio~A Vela.
\newblock A comparative study of efficient initialization methods for the
  k-means clustering algorithm.
\newblock \emph{Expert Systems with Applications}, 40\penalty0 (1):\penalty0
  200--210, 2013.

\bibitem[Arthur and Vassilvitskii(2007)]{arthur2007k}
David Arthur and Sergei Vassilvitskii.
\newblock k-means++: The advantages of careful seeding.
\newblock In \emph{Proceedings of the eighteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1027--1035. Society for Industrial and Applied
  Mathematics, 2007.

\bibitem[Ostrovsky et~al.(2012)Ostrovsky, Rabani, Schulman, and
  Swamy]{ostrovsky2012effectiveness}
Rafail Ostrovsky, Yuval Rabani, Leonard~J Schulman, and Chaitanya Swamy.
\newblock The effectiveness of {L}loyd-type methods for the k-means problem.
\newblock \emph{Journal of the ACM (JACM)}, 59\penalty0 (6):\penalty0 28, 2012.

\bibitem[Beyer et~al.(1999)Beyer, Goldstein, Ramakrishnan, and
  Shaft]{beyer1999nearest}
Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft.
\newblock When is “nearest neighbor” meaningful?
\newblock In \emph{International Conference on Database Theory}, pages
  217--235. Springer, 1999.

\bibitem[Aggarwal et~al.(2001)Aggarwal, Hinneburg, and
  Keim]{aggarwal2001surprising}
Charu~C Aggarwal, Alexander Hinneburg, and Daniel~A Keim.
\newblock On the surprising behavior of distance metrics in high dimensional
  space.
\newblock In \emph{International Conference on Database Theory}, pages
  420--434. Springer, 2001.

\bibitem[Kohavi and John(1997)]{kohavi1997wrappers}
Ron Kohavi and George~H John.
\newblock Wrappers for feature subset selection.
\newblock \emph{Artificial Intelligence}, 97\penalty0 (1-2):\penalty0 273--324,
  1997.

\bibitem[Alelyani et~al.(2018)Alelyani, Tang, and Liu]{alelyani2018feature}
Salem Alelyani, Jiliang Tang, and Huan Liu.
\newblock Feature selection for clustering: {A} review.
\newblock In \emph{Data Clustering}, pages 29--60. Chapman and Hall/CRC, 2018.

\bibitem[Zhao and Liu(2007)]{zhao2007spectral}
Zheng Zhao and Huan Liu.
\newblock Spectral feature selection for supervised and unsupervised learning.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, pages 1151--1157. ACM, 2007.

\bibitem[Zhao and Liu(2011)]{zhao2011spectral}
Zheng~Alan Zhao and Huan Liu.
\newblock \emph{Spectral {F}eature {S}election for {D}ata {M}ining (Open
  Access)}.
\newblock Chapman and Hall/CRC, 2011.

\bibitem[Talavera(1999)]{talavera1999feature}
Luis Talavera.
\newblock Feature selection as a preprocessing step for hierarchical
  clustering.
\newblock In \emph{ICML}, volume~99, pages 389--397. Citeseer, 1999.

\bibitem[He et~al.(2006)He, Cai, and Niyogi]{he2006laplacian}
Xiaofei He, Deng Cai, and Partha Niyogi.
\newblock {L}aplacian score for feature selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  507--514, 2006.

\bibitem[Dy and Brodley(2004)]{dy2004feature}
Jennifer~G Dy and Carla~E Brodley.
\newblock Feature selection for unsupervised learning.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Aug):\penalty0 845--889, 2004.

\bibitem[Friedman and Meulman(2004)]{friedman2004clustering}
Jerome~H Friedman and Jacqueline~J Meulman.
\newblock Clustering objects on subsets of attributes (with discussion).
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 66\penalty0 (4):\penalty0 815--849, 2004.

\bibitem[Witten and Tibshirani(2010)]{witten2010framework}
Daniela~M Witten and Robert Tibshirani.
\newblock A framework for feature selection in clustering.
\newblock \emph{Journal of the American Statistical Association}, 105\penalty0
  (490):\penalty0 713--726, 2010.

\bibitem[Kondo et~al.(2012)Kondo, Salibian-Barrera, and Zamar]{kondo2012robust}
Yumi Kondo, Matias Salibian-Barrera, and Ruben Zamar.
\newblock A robust and sparse k-means clustering algorithm.
\newblock \emph{arXiv preprint arXiv:1201.6082}, 2012.

\bibitem[Brodinov{\'a} et~al.(2017)Brodinov{\'a}, Filzmoser, Ortner,
  Breiteneder, and Rohm]{brodinova2017robust}
{\v{S}}{\'a}rka Brodinov{\'a}, Peter Filzmoser, Thomas Ortner, Christian
  Breiteneder, and Maia Rohm.
\newblock Robust and sparse k-means clustering for high-dimensional data.
\newblock \emph{Advances in Data Analysis and Classification}, pages 1--28,
  2017.

\bibitem[Huang et~al.(2005)Huang, Ng, Rong, and Li]{huang2005automated}
Joshua~Zhexue Huang, Michael~K Ng, Hongqiang Rong, and Zichen Li.
\newblock Automated variable weighting in k-means type clustering.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 27\penalty0 (5):\penalty0 657--668, 2005.

\bibitem[Chakraborty et~al.(2020)Chakraborty, Paul, Das, and
  Xu]{chakraborty2020entropy}
Saptarshi Chakraborty, Debolina Paul, Swagatam Das, and Jason Xu.
\newblock Entropy weighted power k-means clustering.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 691--701. PMLR, 2020.

\bibitem[Chakraborty and Xu(2020)]{chakraborty2020biconvex}
Saptarshi Chakraborty and Jason Xu.
\newblock Biconvex clustering.
\newblock \emph{arXiv preprint arXiv:2008.01760}, 2020.

\bibitem[Pollard(1981)]{pollard1981strong}
David Pollard.
\newblock Strong consistency of k-means clustering.
\newblock \emph{The Annals of Statistics}, pages 135--140, 1981.

\bibitem[Becker et~al.(1997)Becker, Yang, and Lange]{becker1997algorithms}
Mark~P Becker, Ilsoon Yang, and Kenneth Lange.
\newblock {EM} algorithms without missing data.
\newblock \emph{Statistical Methods in Medical Research}, 6\penalty0
  (1):\penalty0 38--54, 1997.

\bibitem[Lange(2016)]{lange2016mm}
Kenneth Lange.
\newblock \emph{{MM} {O}ptimization {A}lgorithms}, volume 147.
\newblock SIAM, 2016.

\bibitem[Wang and Allen(2019)]{wang2019integrative}
Minjie Wang and Genevera~I Allen.
\newblock Integrative generalized convex clustering optimization and feature
  selection for mixed multi-view data.
\newblock \emph{arXiv preprint arXiv:1912.05449}, 2019.

\bibitem[Tibshirani et~al.(2001)Tibshirani, Walther, and
  Hastie]{tibshirani2001estimating}
Robert Tibshirani, Guenther Walther, and Trevor Hastie.
\newblock Estimating the number of clusters in a data set via the gap
  statistic.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 63\penalty0 (2):\penalty0 411--423, 2001.

\bibitem[Rand(1971)]{rand1971objective}
William~M Rand.
\newblock Objective criteria for the evaluation of clustering methods.
\newblock \emph{Journal of the American Statistical association}, 66\penalty0
  (336):\penalty0 846--850, 1971.

\bibitem[Vinh et~al.(2009)Vinh, Epps, and Bailey]{vinh2009information}
Nguyen~Xuan Vinh, Julien Epps, and James Bailey.
\newblock Information theoretic measures for clusterings comparison: is a
  correction for chance necessary?
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 1073--1080. ACM, 2009.

\bibitem[Vinh et~al.(2010)Vinh, Epps, and Bailey]{vinh2010information}
Nguyen~Xuan Vinh, Julien Epps, and James Bailey.
\newblock Information theoretic measures for clusterings comparison:
  {V}ariants, properties, normalization and correction for chance.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Oct):\penalty0 2837--2854, 2010.

\bibitem[Bezanson et~al.(2017)Bezanson, Edelman, Karpinski, and
  Shah]{bezanson2017julia}
Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral~B Shah.
\newblock Julia: {A} fresh approach to numerical computing.
\newblock \emph{SIAM Review}, 59\penalty0 (1):\penalty0 65--98, 2017.

\bibitem[Cardot et~al.(2012)Cardot, C{\'e}nac, and Monnez]{cardot2012fast}
Herv{\'e} Cardot, Peggy C{\'e}nac, and Jean-Marie Monnez.
\newblock A fast and recursive algorithm for clustering large datasets with
  k-medians.
\newblock \emph{Computational Statistics \& Data Analysis}, 56\penalty0
  (6):\penalty0 1434--1449, 2012.

\bibitem[Zhang(2001)]{zhang2001generalized}
Bin Zhang.
\newblock Generalized k-harmonic means--dynamic weighting of data in
  unsupervised learning.
\newblock In \emph{Proceedings of the 2001 SIAM International Conference on
  Data Mining}, pages 1--13. SIAM, 2001.

\bibitem[Hamerly and Elkan(2002)]{hamerly2002alternatives}
Greg Hamerly and Charles Elkan.
\newblock Alternatives to the k-means algorithm that find better clusterings.
\newblock In \emph{Proceedings of the Eleventh International Conference on
  Information and Knowledge Management}, pages 600--607. ACM, 2002.

\bibitem[Xu and Lange(2019)]{pmlr-v97-xu19a}
Jason Xu and Kenneth Lange.
\newblock Power k-means {C}lustering.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research},
  pages 6921--6931, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Higuera et~al.(2015)Higuera, Gardiner, and Cios]{higuera2015self}
Clara Higuera, Katheleen~J Gardiner, and Krzysztof~J Cios.
\newblock Self-organizing feature maps identify proteins critical to learning
  in a mouse model of down syndrome.
\newblock \emph{PloS One}, 10\penalty0 (6):\penalty0 e0129126, 2015.

\bibitem[Chi et~al.(2014)Chi, Zhou, and Lange]{chi2014distance}
Eric~C Chi, Hua Zhou, and Kenneth Lange.
\newblock Distance majorization and its applications.
\newblock \emph{Mathematical programming}, 146\penalty0 (1-2):\penalty0
  409--436, 2014.

\bibitem[Xu et~al.(2017)Xu, Chi, and Lange]{xu2017generalized}
Jason Xu, Eric Chi, and Kenneth Lange.
\newblock Generalized linear model regression under distance-to-set penalties.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1385--1395, 2017.

\bibitem[Keys et~al.(2019)Keys, Zhou, and Lange]{keys2019proximal}
Kevin~L Keys, Hua Zhou, and Kenneth Lange.
\newblock Proximal distance algorithms: Theory and practice.
\newblock \emph{J. Mach. Learn. Res.}, 20\penalty0 (66):\penalty0 1--38, 2019.

\bibitem[Bauschke and Combettes(2011)]{bauschke2011convex}
Heinz~H Bauschke and Patrick~L Combettes.
\newblock \emph{Convex {A}nalysis and {M}onotone {O}perator {T}heory in
  {H}ilbert {S}paces}, volume 408.
\newblock Springer, 2011.

\bibitem[Beck(2017)]{beck2017}
Amir Beck.
\newblock \emph{First-order {M}ethods in {O}ptimization}.
\newblock SIAM, 2017.

\bibitem[Banerjee et~al.(2005)Banerjee, Merugu, Dhillon, and
  Ghosh]{banerjee2005clustering}
Arindam Banerjee, Srujana Merugu, Inderjit~S Dhillon, and Joydeep Ghosh.
\newblock Clustering with {B}regman divergences.
\newblock \emph{Journal of machine learning research}, 6\penalty0
  (Oct):\penalty0 1705--1749, 2005.

\end{thebibliography}
