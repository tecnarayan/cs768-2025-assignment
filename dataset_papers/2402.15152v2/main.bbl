\begin{thebibliography}{83}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko and Flammarion(2022)]{andriushchenko2022towards}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{ICML}, 2022.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{ICML}, 2018.

\bibitem[Bahri et~al.(2021)Bahri, Mobahi, and Tay]{bahri2021sharpness}
Dara Bahri, Hossein Mobahi, and Yi~Tay.
\newblock Sharpness-aware minimization improves language model generalization.
\newblock \emph{arXiv preprint arXiv:2110.08529}, 2021.

\bibitem[Bai et~al.(2019)Bai, Feng, Wang, Dai, Xia, and Jiang]{bai2019hilbert}
Yang Bai, Yan Feng, Yisen Wang, Tao Dai, Shu-Tao Xia, and Yong Jiang.
\newblock Hilbert-based generative defense for adversarial examples.
\newblock In \emph{ICCV}, 2019.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019.

\bibitem[Chen et~al.(2022)Chen, Shao, Wang, Shang, Chen, Ji, and
  Wu]{chen2022bootstrap}
Huanran Chen, Shitong Shao, Ziyi Wang, Zirui Shang, Jin Chen, Xiaofeng Ji, and
  Xinxiao Wu.
\newblock Bootstrap generalization ability from loss landscape perspective.
\newblock In \emph{European Conference on Computer Vision}, pages 500--517.
  Springer, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Dong, Wang, Yang, Duan, Su, and
  Zhu]{chen2023robust}
Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, and
  Jun Zhu.
\newblock Robust classification via a single diffusion model.
\newblock \emph{arXiv preprint arXiv:2305.15241}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Zhang, Dong, Yang, Su, and
  Zhu]{chen2023rethinking}
Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, and Jun Zhu.
\newblock Rethinking model ensemble in transfer-based adversarial attacks.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2024)Chen, Dong, Shao, Hao, Yang, Su, and
  Zhu]{chen2024your}
Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, and
  Jun Zhu.
\newblock Your diffusion model is secretly a certifiably robust classifier.
\newblock \emph{arXiv preprint arXiv:2402.02316}, 2024.

\bibitem[Chen et~al.(2017)Chen, Papandreou, Schroff, and
  Adam]{chen2017rethinking}
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock \emph{arXiv preprint arXiv:1706.05587}, 2017.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{chrabaszcz2017downsampled}
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Jeremy~M Cohen, Elan Rosenfeld, and J.~Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing, 2019.

\bibitem[Croce and Hein(2020{\natexlab{a}})]{croce2020minimally}
F.~Croce and M.~Hein.
\newblock Minimally distorted adversarial examples with a fast adaptive
  boundary attack.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Croce and Hein(2020{\natexlab{b}})]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Croce and Hein(2021)]{croce2021mind}
Francesco Croce and Matthias Hein.
\newblock Mind the box: $l_1$-apgd for sparse adversarial attacks on image
  classifiers.
\newblock In \emph{ICML}, 2021.

\bibitem[Croce et~al.(2021)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{croce2021robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Dong et~al.(2023)Dong, Chen, Chen, Fang, Yang, Zhang, Tian, Su, and
  Zhu]{dong2023robust}
Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang,
  Yu~Tian, Hang Su, and Jun Zhu.
\newblock How robust is google's bard to adversarial image attacks?
\newblock In \emph{R0-FoMo: Robustness of Few-shot and Zero-shot Learning in
  Large Foundation Models}, 2023.

\bibitem[Du et~al.(2021)Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2021efficient}
Jiawei Du, Hanshu Yan, Jiashi Feng, Joey~Tianyi Zhou, Liangli Zhen, Rick
  Siow~Mong Goh, and Vincent~YF Tan.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.03141}, 2021.

\bibitem[Du et~al.(2022)Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2022efficient}
Jiawei Du, Hanshu Yan, Jiashi Feng, Joey~Tianyi Zhou, Liangli Zhen, Rick
  Siow~Mong Goh, and Vincent Y.~F. Tan.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.03141}, 2022.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Everingham et~al.()Everingham, Van~Gool, Williams, Winn, and
  Zisserman]{voc-2012}
M.~Everingham, L.~Van~Gool, C.~K.~I. Williams, J.~Winn, and A.~Zisserman.
\newblock The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)}
  {R}esults.

\bibitem[Everingham et~al.(2015)Everingham, Eslami, Gool, Williams, Winn, and
  Zisserman]{mIoU}
Mark Everingham, S.~M.~Ali Eslami, Luc~Van Gool, Christopher K.~I. Williams,
  John~M. Winn, and Andrew Zisserman.
\newblock The pascal visual object classes challenge: {A} retrospective.
\newblock \emph{Int. J. Comput. Vis.}, 2015.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2015explaining}
Ian~J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2015.

\bibitem[Gould et~al.()Gould, Fulton, and Koller]{background_dataset}
Stephen Gould, Richard Fulton, and Daphne Koller.
\newblock Decomposing a scene into geometric and semantically consistent
  regions.
\newblock In \emph{ICCV 2009}.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, 2016{\natexlab{b}}.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations, 2019.

\bibitem[Hochreiter and Schmidhuber(1994)]{hochreiter1994simplifying}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Simplifying neural nets by discovering flat minima.
\newblock \emph{NeurIPS}, 1994.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural computation}, 1997.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{Ilyas2019AdversarialEA}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Jain et~al.(2023)Jain, Schwarzschild, Wen, Somepalli, Kirchenbauer,
  yeh Chiang, Goldblum, Saha, Geiping, and Goldstein]{jain2023baseline}
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer,
  Ping yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom
  Goldstein.
\newblock Baseline defenses for adversarial attacks against aligned language
  models, 2023.

\bibitem[Jin et~al.(2020)Jin, Jin, Zhou, and Szolovits]{jin2020bert}
Di~Jin, Zhijing Jin, Joey~Tianyi Zhou, and Peter Szolovits.
\newblock Is bert really robust? a strong baseline for natural language attack
  on text classification and entailment, 2020.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kim(2020)]{kim2020torchattacks}
Hoki Kim.
\newblock Torchattacks: A pytorch repository for adversarial attacks.
\newblock \emph{arXiv preprint arXiv:2010.01950}, 2020.

\bibitem[Kim et~al.(2022)Kim, Li, Hu, and Hospedales]{kim2022fisher}
Minyoung Kim, Da~Li, Shell~X Hu, and Timothy Hospedales.
\newblock Fisher sam: Information geometry and sharpness aware minimisation.
\newblock In \emph{ICML}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{ICML}, 2021.

\bibitem[Liu et~al.(2022)Liu, Mai, Chen, Hsieh, and You]{liu2022towards}
Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You.
\newblock Towards efficient and scalable sharpness-aware minimization.
\newblock In \emph{CVPR}, 2022.

\bibitem[Ma et~al.(2023)Ma, Pan, and massoud Farahmand]{ma2023understanding}
Avery Ma, Yangchen Pan, and Amir massoud Farahmand.
\newblock Understanding the robustness difference between stochastic gradient
  descent and adaptive gradient methods.
\newblock \emph{arXiv preprint arXiv:2308.06703}, 2023.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao]{mi2022make}
Peng Mi, Li~Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and
  Dacheng Tao.
\newblock Make sharpness-aware minimization stronger: A sparsified perturbation
  approach.
\newblock \emph{arXiv preprint arXiv:2210.05177}, 2022.

\bibitem[Mo et~al.(2024)Mo, Wang, Wei, and Wang]{mo2024studious}
Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang.
\newblock Studious bob fight back against jailbreaking via prompt adversarial
  tuning.
\newblock \emph{arXiv preprint arXiv:2402.06255}, 2024.

\bibitem[Morris et~al.(2020)Morris, Lifland, Yoo, Grigsby, Jin, and
  Qi]{morris2020textattack}
John Morris, Eli Lifland, Jin~Yong Yoo, Jake Grigsby, Di~Jin, and Yanjun Qi.
\newblock Textattack: A framework for adversarial attacks, data augmentation,
  and adversarial training in nlp.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Mueller et~al.(2023)Mueller, Vlaar, Rolnick, and
  Hein]{mueller2023normalization}
Maximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein.
\newblock Normalization layers are all that sharpness-aware minimization needs.
\newblock \emph{arXiv preprint arXiv:2306.04226}, 2023.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock \emph{NeurIPS}, 30, 2017.

\bibitem[Pang and Lee(2005)]{rotten}
Bo~Pang and Lillian Lee.
\newblock Seeing stars: Exploiting class relationships for sentiment
  categorization with respect to rating scales.
\newblock In \emph{ACL}, 2005.

\bibitem[Pang et~al.(2020)Pang, Yang, Dong, Su, and Zhu]{pang2020bag}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Bag of tricks for adversarial training.
\newblock \emph{arXiv preprint arXiv:2010.00467}, 2020.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{SP}, 2016.

\bibitem[Piet et~al.(2023)Piet, Alrashed, Sitawarin, Chen, Wei, Sun, Alomair,
  and Wagner]{piet2023jatmo}
Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth
  Sun, Basel Alomair, and David Wagner.
\newblock Jatmo: Prompt injection defense by task-specific finetuning.
\newblock \emph{arXiv preprint arXiv:2312.17673}, 2023.

\bibitem[Pomponi et~al.(2022)Pomponi, Scardapane, and Uncini]{Pomponi_2022}
Jary Pomponi, Simone Scardapane, and Aurelio Uncini.
\newblock Pixle: a fast and effective black-box attack based on rearranging
  pixels.
\newblock In \emph{IJCNN}, 2022.

\bibitem[Ren and Malik(2003)]{ren2003learning}
Ren and Malik.
\newblock Learning a classification model for segmentation.
\newblock In \emph{ICCV}, 2003.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and J.~Zico Kolter.
\newblock Overfitting in adversarially robust deep learning, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{mobilenetv2}
Mark Sandler, Andrew~G. Howard, Menglong Zhu, Andrey Zhmoginov, and
  Liang{-}Chieh Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{DistilBERT}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{ArXiv}, abs/1910.01108, 2019.

\bibitem[Shafahi et~al.(2019)Shafahi, Najibi, Ghiasi, Xu, Dickerson, Studer,
  Davis, Taylor, and Goldstein]{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Mohammad~Amin Ghiasi, Zheng Xu, John Dickerson,
  Christoph Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock \emph{NeurIPS}, 2019.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  Jégou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Hervé Jégou.
\newblock Training data-efficient image transformers distillation through
  attention.
\newblock \emph{arXiv preprint arXiv:2012.12877}, 2021.

\bibitem[Tsipras et~al.(2018)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock \emph{arXiv preprint arXiv:1805.12152}, 2018.

\bibitem[Wang et~al.(2021)Wang, Jin, Yang, and He]{iga}
Xiaosen Wang, Hao Jin, Yichen Yang, and Kun He.
\newblock Natural language adversarial defense through synonym encoding.
\newblock \emph{arXiv preprint arXiv:1909.06723}, 2021.

\bibitem[Wang et~al.(2023)Wang, Li, Yang, Lin, and Wang]{wang2024balance}
Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, and Yisen Wang.
\newblock Balance, imbalance, and rebalance: Understanding robust overfitting
  from a minimax game perspective.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Wang et~al.(2024)Wang, Wu, Wei, Jegelka, and
  Wang]{wang2024theoretical}
Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang.
\newblock A theoretical understanding of self-correction through in-context
  alignment.
\newblock \emph{arXiv preprint arXiv:2405.18634}, 2024.

\bibitem[Wei et~al.(2023{\natexlab{a}})Wei, Guo, and
  Wang]{wei2023characterizing}
Zeming Wei, Yiwen Guo, and Yisen Wang.
\newblock Characterizing robust overfitting in adversarial training via
  cross-class features.
\newblock \emph{OpenReview preprint}, 2023{\natexlab{a}}.

\bibitem[Wei et~al.(2023{\natexlab{b}})Wei, Wang, Guo, and Wang]{wei2023cfa}
Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock Cfa: Class-wise calibrated fair adversarial training.
\newblock In \emph{CVPR}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2023{\natexlab{c}})Wei, Wang, and Wang]{wei2023jailbreak}
Zeming Wei, Yifei Wang, and Yisen Wang.
\newblock Jailbreak and guard aligned language models with only few in-context
  demonstrations.
\newblock \emph{arXiv preprint arXiv:2310.06387}, 2023{\natexlab{c}}.

\bibitem[Wei et~al.(2024)Wei, Zhang, Zhang, and Sun]{wei2024weighted}
Zeming Wei, Xiyue Zhang, Yihao Zhang, and Meng Sun.
\newblock Weighted automata extraction and explanation of recurrent neural
  networks for natural language tasks.
\newblock \emph{Journal of Logical and Algebraic Methods in Programming}, 2024.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{wong2020fast}
Eric Wong, Leslie Rice, and J~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock \emph{arXiv preprint arXiv:2001.03994}, 2020.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xiao et~al.(2018)Xiao, Zhu, Li, He, Liu, and Song]{xiao2018spatially}
Chaowei Xiao, Jun-Yan Zhu, Bo~Li, Warren He, Mingyan Liu, and Dawn Song.
\newblock Spatially transformed adversarial examples.
\newblock \emph{arXiv preprint arXiv:1801.02612}, 2018.

\bibitem[Xie et~al.(2019)Xie, Wu, Maaten, Yuille, and He]{xie2019feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan~L Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock In \emph{CVPR}, 2019.

\bibitem[Xie et~al.(2023)Xie, Yi, Shao, Curl, Lyu, Chen, Xie, and
  Wu]{xie2023defending}
Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen,
  Xing Xie, and Fangzhao Wu.
\newblock Defending chatgpt against jailbreak attack via self-reminders.
\newblock \emph{Nature Machine Intelligence}, 2023.

\bibitem[Xu et~al.(2021)Xu, Liu, Li, Jain, and Tang]{xu2021robust}
Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang.
\newblock To be robust or to be fair: Towards fairness in adversarial training.
\newblock In \emph{ICML}, 2021.

\bibitem[Yu et~al.(2022{\natexlab{a}})Yu, Han, Gong, Shen, Ge, Du, and
  Liu]{yu2022robust}
Chaojian Yu, Bo~Han, Mingming Gong, Li~Shen, Shiming Ge, Bo~Du, and Tongliang
  Liu.
\newblock Robust weight perturbation for adversarial training.
\newblock \emph{arXiv preprint arXiv:2205.14826}, 2022{\natexlab{a}}.

\bibitem[Yu et~al.(2022{\natexlab{b}})Yu, Han, Gong, Shen, Ge, Du, and
  Liu]{yu2022robust2}
Chaojian Yu, Bo~Han, Mingming Gong, Li~Shen, Shiming Ge, Bo~Du, and Tongliang
  Liu.
\newblock Robust weight perturbation for adversarial training.
\newblock \emph{arXiv preprint arXiv:2205.14826}, 2022{\natexlab{b}}.

\bibitem[Zagoruyko and Komodakis(2016)]{DBLP:journals/corr/ZagoruykoK16}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zang et~al.(2020)Zang, Qi, Yang, Liu, Zhang, Liu, and Sun]{pso}
Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and
  Maosong Sun.
\newblock Word-level textual adversarial attacking as combinatorial
  optimization.
\newblock In \emph{ACL}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhang and Wei(2024)]{zhang2024boosting}
Yihao Zhang and Zeming Wei.
\newblock Boosting jailbreak attack with momentum.
\newblock In \emph{ICLR 2024 Workshop on Reliable and Responsible Foundation
  Models}, 2024.

\bibitem[Zhu et~al.(2021)Zhu, Ji, Zhu, Gan, Wu, and Yan]{zhu2021learning}
Lanyun Zhu, Deyi Ji, Shiping Zhu, Weihao Gan, Wei Wu, and Junjie Yan.
\newblock Learning statistical texture for semantic segmentation.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zhu et~al.(2023)Zhu, He, Chen, Song, and Tao]{zhu2023decentralized}
Tongtian Zhu, Fengxiang He, Kaixuan Chen, Mingli Song, and Dacheng Tao.
\newblock Decentralized sgd and average-direction sam are asymptotically
  equivalent.
\newblock \emph{arXiv preprint arXiv:2306.02913}, 2023.

\bibitem[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal}
Andy Zou, Zifan Wang, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language
  models, 2023.

\end{thebibliography}
