\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Aky{\"u}rek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Alberti et~al.(2023)Alberti, Dern, Thesing, and Kutyniok]{alberti2023sumformer}
Alberti, S., Dern, N., Thesing, L., and Kutyniok, G.
\newblock Sumformer: Universal approximation for efficient transformers.
\newblock In \emph{Topological, Algebraic and Geometric Learning Workshops 2023}, pp.\  72--86. PMLR, 2023.

\bibitem[Alman \& Song(2023)Alman and Song]{alman2023fast}
Alman, J. and Song, Z.
\newblock Fast attention requires bounded entries.
\newblock \emph{arXiv preprint arXiv:2302.13214}, 2023.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020Longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv:2004.05150}, 2020.

\bibitem[Bhattamishra et~al.(2020)Bhattamishra, Ahuja, and Goyal]{bhattamishra2020ability}
Bhattamishra, S., Ahuja, K., and Goyal, N.
\newblock On the ability and limitations of transformers to recognize formal languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in neural information processing systems}, volume~33, pp.\  1877--1901, 2020.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2021rethinking}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dai et~al.(2023)Dai, Sun, Dong, Hao, Ma, Sui, and Wei]{dai2023can}
Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F.
\newblock Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers.
\newblock In \emph{ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models}, 2023.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186. Association for Computational Linguistics, 2019.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1, 2021.

\bibitem[Feng et~al.(2023)Feng, Gu, Zhang, Ye, He, and Wang]{feng2023towards}
Feng, G., Gu, Y., Zhang, B., Ye, H., He, D., and Wang, L.
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Furst et~al.(1984)Furst, Saxe, and Sipser]{furst1984parity}
Furst, M., Saxe, J.~B., and Sipser, M.
\newblock Parity, circuits, and the polynomial-time hierarchy.
\newblock \emph{Mathematical systems theory}, 17\penalty0 (1):\penalty0 13--27, 1984.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022what}
Garg, S., Tsipras, D., Liang, P., and Valiant, G.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023mamba}
Gu, A. and Dao, T.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Hahn(2020)]{hahn2020theoretical}
Hahn, M.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 156--171, 2020.

\bibitem[Hao et~al.(2022)Hao, Angluin, and Frank]{hao2022formal}
Hao, Y., Angluin, D., and Frank, R.
\newblock Formal language recognition by hard attention transformers: Perspectives from circuit complexity.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 800--810, 2022.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{International conference on machine learning}, pp.\  5156--5165. PMLR, 2020.

\bibitem[Keles et~al.(2023)Keles, Wijewardena, and Hegde]{keles2023computational}
Keles, F.~D., Wijewardena, P.~M., and Hegde, C.
\newblock On the computational complexity of self-attention.
\newblock In \emph{International Conference on Algorithmic Learning Theory}, pp.\  597--619. PMLR, 2023.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Liu, B., Ash, J.~T., Goel, S., Krishnamurthy, A., and Zhang, C.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Liu et~al.(2023)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2023transformers}
Liu, B., Ash, J.~T., Goel, S., Krishnamurthy, A., and Zhang, C.
\newblock Transformers learn shortcuts to automata.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017fixing}
Loshchilov, I. and Hutter, F.
\newblock Fixing weight decay regularization in adam.
\newblock 2017.

\bibitem[Luo et~al.(2021)Luo, Li, Cai, He, Peng, Zheng, Ke, Wang, and Liu]{luo2021stable}
Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke, G., Wang, L., and Liu, T.-Y.
\newblock Stable, fast and accurate: Kernelized attention with relative positional encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  22795--22807, 2021.

\bibitem[Merrill \& Sabharwal(2023)Merrill and Sabharwal]{merrill2023parallelism}
Merrill, W. and Sabharwal, A.
\newblock The parallelism tradeoff: Limitations of log-precision transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 2023.

\bibitem[Merrill et~al.(2022)Merrill, Sabharwal, and Smith]{merrill2022saturated}
Merrill, W., Sabharwal, A., and Smith, N.~A.
\newblock Saturated transformers are constant-depth threshold circuits.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 843--856, 2022.

\bibitem[Nye et~al.(2022)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{nye2022show}
Nye, M., Andreassen, A.~J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock In \emph{Deep Learning for Code Workshop}, 2022.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez, Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{olsson2022context}
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and Kong]{peng2021random}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.~A., and Kong, L.
\newblock Random feature attention.
\newblock \emph{arXiv preprint arXiv:2103.02143}, 2021.

\bibitem[P{\'e}rez et~al.(2019)P{\'e}rez, Marinkovi{\'c}, and Barcel{\'o}]{perez2019turing}
P{\'e}rez, J., Marinkovi{\'c}, J., and Barcel{\'o}, P.
\newblock On the turing completeness of modern neural network architectures.
\newblock \emph{arXiv preprint arXiv:1901.03429}, 2019.

\bibitem[P{\'e}rez et~al.(2021)P{\'e}rez, Barcel{\'o}, and Marinkovic]{perez2021attention}
P{\'e}rez, J., Barcel{\'o}, P., and Marinkovic, J.
\newblock Attention is turing complete.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 3463--3497, 2021.

\bibitem[Qiu et~al.(2020)Qiu, Ma, Levy, Yih, Wang, and Tang]{qiu2020blockwise}
Qiu, J., Ma, H., Levy, O., Yih, W.-t., Wang, S., and Tang, J.
\newblock Blockwise self-attention for long document understanding.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pp.\  2555--2565, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{roy2021efficient}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 53--68, 2021.

\bibitem[Sanford et~al.(2023)Sanford, Hsu, and Telgarsky]{sanford2023representational}
Sanford, C., Hsu, D., and Telgarsky, M.
\newblock Representational strengths and limitations of transformers.
\newblock \emph{arXiv preprint arXiv:2306.02896}, 2023.

\bibitem[Sun et~al.()Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2307retentive}
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.
\newblock Retentive network: A successor to transformer for large language models (2023).
\newblock \emph{URL http://arxiv. org/abs/2307.08621 v1}.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Yang, Metzler, and Juan]{tay2020sparse}
Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
\newblock Sparse sinkhorn attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9438--9447. PMLR, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{tay2020long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020{\natexlab{b}}.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Von~Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\  35151--35174. PMLR, 2023.

\bibitem[Vyas et~al.(2020)Vyas, Katharopoulos, and Fleuret]{vyas2020fast}
Vyas, A., Katharopoulos, A., and Fleuret, F.
\newblock Fast transformers with clustered attention.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  21665--21674, 2020.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Chen, and Ma]{wei2022statistically}
Wei, C., Chen, Y., and Ma, T.
\newblock Statistically meaningful approximation: a case study on approximating turing machines with transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 12071--12083, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022{\natexlab{b}}.

\bibitem[Weiss et~al.(2021)Weiss, Goldberg, and Yahav]{weiss2021thinking}
Weiss, G., Goldberg, Y., and Yahav, E.
\newblock Thinking like transformers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11080--11090. PMLR, 2021.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Yao, S., Peng, B., Papadimitriou, C., and Narasimhan, K.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  3770--3785, 2021.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{yun2019transformers}
Yun, C., Bhojanapalli, S., Rawat, A.~S., Reddi, S.~J., and Kumar, S.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Zhou et~al.(2023)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le, and Chi]{zhou2023leasttomost}
Zhou, D., Sch{\"a}rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q.~V., and Chi, E.~H.
\newblock Least-to-most prompting enables complex reasoning in large language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\end{thebibliography}
