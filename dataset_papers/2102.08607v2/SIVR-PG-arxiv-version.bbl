\begin{thebibliography}{10}

\bibitem{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In {\em Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In {\em International conference on machine learning}, pages
  699--707, 2016.

\bibitem{azar2011reinforcement}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, Mohammad Ghavamzadeh, and Hilbert
  Kappen.
\newblock Reinforcement learning with a near optimal rate of convergence.
\newblock 2011.

\bibitem{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Hilbert~J Kappen.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock {\em Machine learning}, 91(3):325--349, 2013.

\bibitem{baxter2001infinite}
Jonathan Baxter and Peter~L Bartlett.
\newblock Infinite-horizon policy-gradient estimation.
\newblock {\em Journal of Artificial Intelligence Research}, 15:319--350, 2001.

\bibitem{bhandari2019global}
Jalaj Bhandari and Daniel Russo.
\newblock Global optimality guarantees for policy gradient methods.
\newblock {\em arXiv preprint arXiv:1906.01786}, 2019.

\bibitem{bhandari2020note}
Jalaj Bhandari and Daniel Russo.
\newblock A note on the linear convergence of policy gradient methods.
\newblock {\em arXiv preprint arXiv:2007.11120}, 2020.

\bibitem{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{cen2020fast}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock {\em arXiv preprint arXiv:2007.06558}, 2020.

\bibitem{chen2018scalable}
Yichen Chen, Lihong Li, and Mengdi Wang.
\newblock Scalable bilinear pi learning using state and action features.
\newblock {\em arXiv preprint arXiv:1804.10328}, 2018.

\bibitem{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock {\em Advances in neural information processing systems},
  27:1646--1654, 2014.

\bibitem{even2006action}
Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan.
\newblock Action elimination and stopping conditions for the multi-armed bandit
  and reinforcement learning problems.
\newblock {\em Journal of machine learning research}, 7(6), 2006.

\bibitem{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  689--699, 2018.

\bibitem{fazel2018global}
Maryam Fazel, Rong Ge, Sham~M Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock {\em arXiv preprint arXiv:1801.05039}, 2018.

\bibitem{ghadimi2020single}
Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang.
\newblock A single timescale stochastic approximation method for nested
  stochastic optimization.
\newblock {\em SIAM Journal on Optimization}, 30(1):960--979, 2020.

\bibitem{hazan2019provably}
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2681--2691. PMLR, 2019.

\bibitem{huo2018accelerated}
Zhouyuan Huo, Bin Gu, Ji~Liu, and Heng Huang.
\newblock Accelerated method for stochastic composition optimization with
  nonsmooth regularization.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{j2016proximal}
Sashank J~Reddi, Suvrit Sra, Barnabas Poczos, and Alexander~J Smola.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock {\em Advances in neural information processing systems},
  29:1145--1153, 2016.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock {\em Advances in neural information processing systems}, 26:315--323,
  2013.

\bibitem{kakade2001natural}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock {\em Advances in neural information processing systems},
  14:1531--1538, 2001.

\bibitem{konda2000actor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  1008--1014, 2000.

\bibitem{konda1999actor}
Vijaymohan~R Konda and Vivek~S Borkar.
\newblock Actor-critic--type learning algorithms for markov decision processes.
\newblock {\em SIAM Journal on control and Optimization}, 38(1):94--123, 1999.

\bibitem{lian2017finite}
Xiangru Lian, Mengdi Wang, and Ji~Liu.
\newblock Finite-sum composition optimization via variance reduced gradient
  descent.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1159--1167.
  PMLR, 2017.

\bibitem{liu2019neural}
Boyi Liu, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10565--10576, 2019.

\bibitem{mei2020global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock {\em arXiv preprint arXiv:2005.06392}, 2020.

\bibitem{miryoosefi2019reinforcement}
Sobhan Miryoosefi, Kiant{\'e} Brantley, Hal Daume~III, Miro Dudik, and Robert~E
  Schapire.
\newblock Reinforcement learning with convex constraints.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14093--14102, 2019.

\bibitem{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock {\em arXiv preprint arXiv:1703.00102}, 2017.

\bibitem{nguyen2019finite}
Lam~M Nguyen, Marten van Dijk, Dzung~T Phan, Phuong~Ha Nguyen, Tsui-Wei Weng,
  and Jayant~R Kalagnanam.
\newblock Finite-sum smooth optimization with sarah.
\newblock {\em arXiv preprint arXiv:1901.07648}, 2019.

\bibitem{papini2018stochastic}
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello
  Restelli.
\newblock Stochastic variance-reduced policy gradient.
\newblock {\em arXiv preprint arXiv:1806.05618}, 2018.

\bibitem{peters2008natural}
Jan Peters and Stefan Schaal.
\newblock Natural actor-critic.
\newblock {\em Neurocomputing}, 71(7-9):1180--1190, 2008.

\bibitem{pham2020hybrid}
Nhan~H Pham, Lam~M Nguyen, Dzung~T Phan, Phuong~Ha Nguyen, Marten van Dijk, and
  Quoc Tran-Dinh.
\newblock A hybrid stochastic policy gradient algorithm for reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2003.00430}, 2020.

\bibitem{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International conference on machine learning}, pages
  314--323, 2016.

\bibitem{reddi2016fast}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Fast incremental method for nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1603.06159}, 2016.

\bibitem{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162(1-2):83--112, 2017.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897, 2015.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{shani2020adaptive}
Lior Shani, Yonathan Efroni, and Shie Mannor.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5668--5675, 2020.

\bibitem{shen2019hessian}
Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi.
\newblock Hessian aided policy gradient.
\newblock In {\em International Conference on Machine Learning}, pages
  5729--5738, 2019.

\bibitem{sidford2018near}
Aaron Sidford, Mengdi Wang, Xian Wu, Lin~F Yang, and Yinyu Ye.
\newblock Near-optimal time and sample complexities for solving discounted
  markov decision process with a generative model.
\newblock {\em arXiv preprint arXiv:1806.01492}, 2018.

\bibitem{sidford2018variance}
Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye.
\newblock Variance reduced value iteration and faster algorithms for solving
  markov decision processes.
\newblock In {\em Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 770--787. SIAM, 2018.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem{wang2017stochastic}
Mengdi Wang, Ethan~X Fang, and Han Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock {\em Mathematical Programming}, 161(1-2):419--449, 2017.

\bibitem{wang2017accelerating}
Mengdi Wang, Ji~Liu, and Ethan~X Fang.
\newblock Accelerating stochastic composition optimization.
\newblock {\em The Journal of Machine Learning Research}, 18(1):3721--3743,
  2017.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{xu2019sample}
Pan Xu, Felicia Gao, and Quanquan Gu.
\newblock Sample efficient policy gradient methods with recursive variance
  reduction.
\newblock {\em arXiv preprint arXiv:1909.08610}, 2019.

\bibitem{xu2020improved}
Pan Xu, Felicia Gao, and Quanquan Gu.
\newblock An improved convergence analysis of stochastic variance-reduced
  policy gradient.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 541--551.
  PMLR, 2020.

\bibitem{zhang2020cautious}
Junyu Zhang, Amrit~Singh Bedi, Mengdi Wang, and Alec Koppel.
\newblock Cautious reinforcement learning via distributional risk in the dual
  domain.
\newblock {\em arXiv preprint arXiv:2002.12475}, 2020.

\bibitem{zhang2020variational}
Junyu Zhang, Alec Koppel, Amrit~Singh Bedi, Csaba Szepesvari, and Mengdi Wang.
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock {\em arXiv preprint arXiv:2007.02151}, 2020.

\bibitem{zhang2019composite}
Junyu Zhang and Lin Xiao.
\newblock A composite randomized incremental gradient method.
\newblock In {\em International Conference on Machine Learning}, pages
  7454--7462, 2019.

\bibitem{zhang2019multi}
Junyu Zhang and Lin Xiao.
\newblock Multi-level composite stochastic optimization via nested variance
  reduction.
\newblock {\em arXiv preprint arXiv:1908.11468}, 2019.

\bibitem{zhang2019stochastic}
Junyu Zhang and Lin Xiao.
\newblock A stochastic composite gradient method with incremental variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9078--9088, 2019.

\bibitem{zhang2020sample}
Junzi Zhang, Jongho Kim, Brendan O'Donoghue, and Stephen Boyd.
\newblock Sample efficient reinforcement learning with reinforce.
\newblock {\em arXiv preprint arXiv:2010.11364}, 2020.

\bibitem{zhang2019global}
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Ba{\c{s}}ar.
\newblock Global convergence of policy gradient methods to (almost) locally
  optimal policies.
\newblock {\em arXiv preprint arXiv:1906.08383}, 2019.

\bibitem{zhao2019stochastic}
Mingming Zhao, Yongfeng Li, and Zaiwen Wen.
\newblock A stochastic trust-region framework for policy optimization.
\newblock {\em arXiv preprint arXiv:1911.11640}, 2019.

\bibitem{zhao2012analysis}
Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama.
\newblock Analysis and improvement of policy gradient estimation.
\newblock {\em Neural Networks}, 26:118--129, 2012.

\end{thebibliography}
