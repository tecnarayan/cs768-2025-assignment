\begin{thebibliography}{10}

\bibitem{alain2016understanding}
Guillaume Alain and Yoshua Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock {\em arXiv preprint arXiv:1610.01644}, 2016.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{becker2018interpreting}
S\"oren Becker, Marcel Ackermann, Sebastian Lapuschkin, Klaus-Robert M\"uller,
  and Wojciech Samek.
\newblock Interpreting and explaining deep neural networks for classification
  of audio signals.
\newblock {\em CoRR}, abs/1807.03418, 2018.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alex Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em arXiv preprint arXiv:2105.05233}, 2021.

\bibitem{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{esposito2020blitzbdl}
Piero Esposito.
\newblock Blitz - bayesian layers in torch zoo (a bayesian deep learing library
  for torch).
\newblock \url{https://github.com/piEsposito/blitz-bayesian-deep-learning/},
  2020.

\bibitem{falcon2019pytorch}
et~al. Falcon, WA.
\newblock Pytorch lightning.
\newblock {\em GitHub. Note:
  https://github.com/PyTorchLightning/pytorch-lightning}, 3, 2019.

\bibitem{falcon2020framework}
William Falcon and Kyunghyun Cho.
\newblock A framework for contrastive self-supervised learning and designing a
  new approach.
\newblock {\em arXiv preprint arXiv:2009.00104}, 2020.

\bibitem{feinman2017detecting}
Reuben Feinman, Ryan~R Curtin, Saurabh Shintre, and Andrew~B Gardner.
\newblock Detecting adversarial samples from artifacts.
\newblock {\em arXiv preprint arXiv:1703.00410}, 2017.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059. PMLR, 2016.

\bibitem{geifman2017selective}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selective classification for deep neural networks.
\newblock {\em arXiv preprint arXiv:1705.08500}, 2017.

\bibitem{geifman2019selectivenet}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selectivenet: A deep neural network with an integrated reject option.
\newblock In {\em International Conference on Machine Learning}, pages
  2151--2159. PMLR, 2019.

\bibitem{goodfellow2014generative}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1406.2661}, 2014.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1321--1330. PMLR, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock {\em arXiv preprint arXiv:1610.02136}, 2016.

\bibitem{hendrycks2018deep}
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.
\newblock Deep anomaly detection with outlier exposure.
\newblock {\em arXiv preprint arXiv:1812.04606}, 2018.

\bibitem{hendrycks2019using}
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song.
\newblock Using self-supervised learning can improve model robustness and
  uncertainty.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15663--15674, 2019.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2006.11239}, 2020.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem{lee2018simple}
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7167--7177, 2018.

\bibitem{liang2018enhancing}
Shiyu Liang, Yixuan Li, and R~Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{liang2017enhancing}
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock {\em arXiv preprint arXiv:1706.02690}, 2017.

\bibitem{lugosch2019speech}
Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant~Singh Tomar, and Yoshua
  Bengio.
\newblock Speech model pre-training for end-to-end spoken language
  understanding.
\newblock {\em arXiv preprint arXiv:1904.03670}, 2019.

\bibitem{mahendran2015understanding}
Aravindh Mahendran and Andrea Vedaldi.
\newblock Understanding deep image representations by inverting them.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5188--5196, 2015.

\bibitem{makhzani2015adversarial}
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan
  Frey.
\newblock Adversarial autoencoders.
\newblock {\em arXiv preprint arXiv:1511.05644}, 2015.

\bibitem{metzen2017detecting}
Jan~Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff.
\newblock On detecting adversarial perturbations.
\newblock {\em arXiv preprint arXiv:1702.04267}, 2017.

\bibitem{mu2019mnist}
Norman Mu and Justin Gilmer.
\newblock Mnist-c: A robustness benchmark for computer vision.
\newblock {\em arXiv preprint arXiv:1906.02337}, 2019.

\bibitem{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem{niculescu2005predicting}
Alexandru Niculescu-Mizil and Rich Caruana.
\newblock Predicting good probabilities with supervised learning.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, pages 625--632, 2005.

\bibitem{pang2018towards}
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu.
\newblock Towards robust detection of adversarial examples.
\newblock In {\em NeurIPS}, 2018.

\bibitem{papamakarios2019normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em arXiv preprint arXiv:1912.02762}, 2019.

\bibitem{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock {\em arXiv preprint arXiv:1705.07057}, 2017.

\bibitem{perarnau2016invertible}
Guim Perarnau, Joost Van De~Weijer, Bogdan Raducanu, and Jose~M {\'A}lvarez.
\newblock Invertible conditional gans for image editing.
\newblock {\em arXiv preprint arXiv:1611.06355}, 2016.

\bibitem{platt1999probabilistic}
John Platt et~al.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock {\em Advances in large margin classifiers}, 10(3):61--74, 1999.

\bibitem{rombach2020network}
Robin Rombach, Patrick Esser, and Bjorn Ommer.
\newblock Network-to-network translation with conditional invertible neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{sastry2019detecting}
Chandramouli~Shama Sastry and Sageev Oore.
\newblock Detecting out-of-distribution examples with in-distribution examples
  and gram matrices.
\newblock {\em arXiv preprint arXiv:1912.12510}, 2019.

\bibitem{schmidhuber2015deep}
J{\"u}rgen Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock {\em Neural networks}, 61:85--117, 2015.

\bibitem{song2019learning}
Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano
  Ermon.
\newblock Learning controllable fair representations.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2164--2173. PMLR, 2019.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em arXiv preprint arXiv:1907.05600}, 2019.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock {\em arXiv preprint arXiv:1409.3215}, 2014.

\bibitem{tamkin2020viewmaker}
Alex Tamkin, Mike Wu, and Noah Goodman.
\newblock Viewmaker networks: Learning views for unsupervised representation
  learning.
\newblock {\em arXiv preprint arXiv:2010.07432}, 2020.

\bibitem{tao2019deep}
Sean Tao.
\newblock Deep neural network ensembles.
\newblock In {\em International Conference on Machine Learning, Optimization,
  and Data Science}, pages 1--12. Springer, 2019.

\bibitem{xu2017feature}
Weilin Xu, David Evans, and Yanjun Qi.
\newblock Feature squeezing: Detecting adversarial examples in deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1704.01155}, 2017.

\bibitem{zadrozny2001obtaining}
Bianca Zadrozny and Charles Elkan.
\newblock Obtaining calibrated probability estimates from decision trees and
  naive bayesian classifiers.
\newblock In {\em Icml}, volume~1, pages 609--616. Citeseer, 2001.

\bibitem{zadrozny2002transforming}
Bianca Zadrozny and Charles Elkan.
\newblock Transforming classifier scores into accurate multiclass probability
  estimates.
\newblock In {\em Proceedings of the eighth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 694--699, 2002.

\bibitem{zisselman2020deep}
Ev~Zisselman and Aviv Tamar.
\newblock Deep residual flow for out of distribution detection.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13994--14003, 2020.

\end{thebibliography}
