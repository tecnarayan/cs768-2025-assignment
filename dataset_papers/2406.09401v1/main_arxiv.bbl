\begin{thebibliography}{10}

\bibitem{mp3d-license}
Matterport3d license.
\newblock \url{https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf}, 2017.

\bibitem{scannet-license}
Scannet license.
\newblock \url{https://kaldir.vc.in.tum.de/scannet/ScanNet_TOS.pdf}, 2017.

\bibitem{3rscan-license}
3rscan license.
\newblock \url{https://github.com/WaldJohannaU/3RScan}, 2019.

\bibitem{chatgpt}
Openai chatgpt.
\newblock \url{https://openai.com/gpt-4}, 2023.

\bibitem{embodiedscan-license}
Embodiedscan and mmscan access.
\newblock \url{https://docs.google.com/forms/d/e/1FAIpQLScUXEDTksGiqHZp31j7Zp7zlCNV7p_08uViwP_Nbzfn3g6hhw/viewform}, 2024.

\bibitem{referit3d}
P.~Achlioptas, A.~Abdelreheem, F.~Xia, M.~Elhoseiny, and L.~Guibas.
\newblock Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16}, pages 422--440. Springer, 2020.

\bibitem{scanqa}
D.~Azuma, T.~Miyanishi, S.~Kurita, and M.~Kawanabe.
\newblock Scanqa: 3d question answering for spatial scene understanding.
\newblock In {\em proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 19129--19139, 2022.

\bibitem{Qwen-VL}
J.~Bai, S.~Bai, S.~Yang, S.~Wang, S.~Tan, P.~Wang, J.~Lin, C.~Zhou, and J.~Zhou.
\newblock Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{arkitscenes}
G.~Baruch, Z.~Chen, A.~Dehghan, T.~Dimry, Y.~Feigin, P.~Fu, T.~Gebauer, B.~Joffe, D.~Kurz, A.~Schwartz, and E.~Shulman.
\newblock {ARK}itscenes - a diverse real-world dataset for 3d indoor scene understanding using mobile {RGB}-d data.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem{3djcg}
D.~Cai, L.~Zhao, J.~Zhang, L.~Sheng, and D.~Xu.
\newblock 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 16464--16473, 2022.

\bibitem{mp3d}
A.~Chang, A.~Dai, T.~Funkhouser, M.~Halber, M.~Niessner, M.~Savva, S.~Song, A.~Zeng, and Y.~Zhang.
\newblock {Matterport3D}: Learning from {RGB-D} data in indoor environments.
\newblock {\em International Conference on 3D Vision (3DV)}, 2017.

\bibitem{scanrefer}
D.~Z. Chen, A.~X. Chang, and M.~Nie{\ss}ner.
\newblock Scanrefer: 3d object localization in rgb-d scans using natural language.
\newblock In {\em European conference on computer vision}, pages 202--221. Springer, 2020.

\bibitem{d3net}
D.~Z. Chen, Q.~Wu, M.~Nie√üner, and A.~X. Chang.
\newblock D3net: A speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans, 2021.

\bibitem{ll3da}
S.~Chen, X.~Chen, C.~Zhang, M.~Li, G.~Yu, H.~Fei, H.~Zhu, J.~Fan, and T.~Chen.
\newblock Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning.
\newblock {\em arXiv preprint arXiv:2311.18651}, 2023.

\bibitem{vil3drel}
S.~Chen, P.-L. Guhur, M.~Tapaswi, C.~Schmid, and I.~Laptev.
\newblock Language conditioned spatial relation reasoning for 3d object grounding.
\newblock {\em Advances in Neural Information Processing Systems}, 35:20522--20535, 2022.

\bibitem{vote2cap-detr}
S.~Chen, H.~Zhu, X.~Chen, Y.~Lei, G.~Yu, and T.~Chen.
\newblock End-to-end 3d dense captioning with vote2cap-detr.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11124--11133, 2023.

\bibitem{grounded-3d-llm}
Y.~Chen, S.~Yang, H.~Huang, T.~Wang, R.~Lyu, R.~Xu, D.~Lin, and J.~Pang.
\newblock Grounded 3d-llm with referent tokens.
\newblock {\em arXiv preprint arXiv:2405.10370}, 2024.

\bibitem{scan2cap}
Z.~Chen, A.~Gholami, M.~Nie{\ss}ner, and A.~X. Chang.
\newblock Scan2cap: Context-aware dense captioning in rgb-d scans.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 3193--3203, 2021.

\bibitem{internvl-chat}
Z.~Chen, W.~Wang, H.~Tian, S.~Ye, Z.~Gao, E.~Cui, W.~Tong, K.~Hu, J.~Luo, Z.~Ma, et~al.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock {\em arXiv preprint arXiv:2404.16821}, 2024.

\bibitem{2023opencompass}
O.~Contributors.
\newblock Opencompass: A universal evaluation platform for foundation models.
\newblock \url{https://github.com/open-compass/opencompass}, 2023.

\bibitem{scannet}
A.~Dai, A.~X. Chang, M.~Savva, M.~Halber, T.~Funkhouser, and M.~Nie{\ss}ner.
\newblock Scannet: Richly-annotated 3d reconstructions of indoor scenes.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5828--5839, 2017.

\bibitem{internlmxcomposer2}
X.~Dong, P.~Zhang, Y.~Zang, Y.~Cao, B.~Wang, L.~Ouyang, X.~Wei, S.~Zhang, H.~Duan, M.~Cao, W.~Zhang, Y.~Li, H.~Yan, Y.~Gao, X.~Zhang, W.~Li, J.~Li, K.~Chen, C.~He, X.~Zhang, Y.~Qiao, D.~Lin, and J.~Wang.
\newblock Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.
\newblock {\em arXiv preprint arXiv:2401.16420}, 2024.

\bibitem{pointbindpointllm}
Z.~Guo, R.~Zhang, X.~Zhu, Y.~Tang, X.~Ma, J.~Han, K.~Chen, P.~Gao, X.~Li, H.~Li, et~al.
\newblock Point-bind \& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following.
\newblock {\em arXiv preprint arXiv:2309.00615}, 2023.

\bibitem{3dllm}
Y.~Hong, H.~Zhen, P.~Chen, S.~Zheng, Y.~Du, Z.~Chen, and C.~Gan.
\newblock 3d-llm: Injecting the 3d world into large language models.
\newblock {\em arXiv preprint arXiv:2307.12981}, 2023.

\bibitem{chat3dv2}
H.~Huang, Z.~Wang, R.~Huang, L.~Liu, X.~Cheng, Y.~Zhao, T.~Jin, and Z.~Zhao.
\newblock Chat-3d v2: Bridging 3d scene and large language models with object identifiers.
\newblock {\em arXiv preprint arXiv:2312.08168}, 2023.

\bibitem{embodiedgeneralist}
J.~Huang, S.~Yong, X.~Ma, X.~Linghu, P.~Li, Y.~Wang, Q.~Li, S.-C. Zhu, B.~Jia, and S.~Huang.
\newblock An embodied generalist agent in 3d world.
\newblock {\em arXiv preprint arXiv:2311.12871}, 2023.

\bibitem{butddetr}
A.~Jain, N.~Gkanatsios, I.~Mediratta, and K.~Fragkiadaki.
\newblock Bottom up top down detection transformers for language grounding in images and point clouds.
\newblock In {\em European Conference on Computer Vision}, pages 417--433. Springer, 2022.

\bibitem{sceneverse}
B.~Jia, Y.~Chen, H.~Yu, Y.~Wang, X.~Niu, T.~Liu, Q.~Li, and S.~Huang.
\newblock Sceneverse: Scaling 3d vision-language learning for grounded scene understanding.
\newblock {\em arXiv preprint arXiv:2401.09340}, 2024.

\bibitem{jiao2022more}
Y.~Jiao, S.~Chen, Z.~Jie, J.~Chen, L.~Ma, and Y.-G. Jiang.
\newblock More: Multi-order relation mining for dense captioning in 3d scenes.
\newblock In {\em European Conference on Computer Vision}, pages 528--545. Springer, 2022.

\bibitem{3dvlp}
Z.~Jin, M.~Hayat, Y.~Yang, Y.~Guo, and Y.~Lei.
\newblock Context-aware alignment and mutual masking for 3d-language pre-training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10984--10994, 2023.

\bibitem{llava}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{Cap3D}
T.~Luo, C.~Rockwell, H.~Lee, and J.~Johnson.
\newblock Scalable 3d captioning with pretrained models.
\newblock {\em arXiv preprint arXiv:2306.07279}, 2023.

\bibitem{sqa3d}
X.~Ma, S.~Yong, Z.~Zheng, Q.~Li, Y.~Liang, S.-C. Zhu, and S.~Huang.
\newblock Sqa3d: Situated question answering in 3d scenes.
\newblock {\em arXiv preprint arXiv:2210.07474}, 2022.

\bibitem{clipguided}
M.~Parelli, A.~Delitzas, N.~Hars, G.~Vlassis, S.~Anagnostidis, G.~Bachmann, and T.~Hofmann.
\newblock Clip-guided vision-language pre-training for question answering in 3d scenes.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5606--5611, 2023.

\bibitem{flickr30k}
B.~A. Plummer, L.~Wang, C.~M. Cervantes, J.~C. Caicedo, J.~Hockenmaier, and S.~Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2641--2649, 2015.

\bibitem{votenet}
C.~R. Qi, O.~Litany, K.~He, and L.~J. Guibas.
\newblock Deep hough voting for 3d object detection in point clouds.
\newblock In {\em proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 9277--9286, 2019.

\bibitem{qi2024shapellm}
Z.~Qi, R.~Dong, S.~Zhang, H.~Geng, C.~Han, Z.~Ge, L.~Yi, and K.~Ma.
\newblock Shapellm: Universal 3d object understanding for embodied interaction.
\newblock {\em arXiv preprint arXiv:2402.17766}, 2024.

\bibitem{qi2023gpt4point}
Z.~Qi, Y.~Fang, Z.~Sun, X.~Wu, T.~Wu, J.~Wang, D.~Lin, and H.~Zhao.
\newblock Gpt4point: A unified framework for point-language understanding and generation.
\newblock {\em arXiv preprint arXiv:2312.02980}, 2023.

\bibitem{hm3d}
S.~K. Ramakrishnan, A.~Gokaslan, E.~Wijmans, O.~Maksymets, A.~Clegg, J.~M. Turner, E.~Undersander, W.~Galuba, A.~Westbury, A.~X. Chang, M.~Savva, Y.~Zhao, and D.~Batra.
\newblock Habitat-matterport 3d dataset ({HM}3d): 1000 large-scale 3d environments for embodied {AI}.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem{rasheed2023glamm}
H.~Rasheed, M.~Maaz, S.~Shaji, A.~Shaker, S.~Khan, H.~Cholakkal, R.~M. Anwer, E.~Xing, M.-H. Yang, and F.~S. Khan.
\newblock Glamm: Pixel grounding large multimodal model.
\newblock {\em arXiv preprint arXiv:2311.03356}, 2023.

\bibitem{sunrgbd}
S.~Song, S.~P. Lichtenberg, and J.~Xiao.
\newblock Sun rgb-d: A rgb-d scene understanding benchmark suite.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2015.

\bibitem{concretenet}
O.~Unal, C.~Sakaridis, S.~Saha, F.~Yu, and L.~Van~Gool.
\newblock Three ways to improve verbo-visual fusion for dense 3d visual grounding.
\newblock {\em arXiv preprint arXiv:2309.04561}, 2023.

\bibitem{3rscan}
J.~Wald, A.~Avetisyan, N.~Navab, F.~Tombari, and M.~Nie{\ss}ner.
\newblock Rio: 3d object instance re-localization in changing indoor environments.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 7658--7667, 2019.

\bibitem{wang2023embodiedscan}
T.~Wang, X.~Mao, C.~Zhu, R.~Xu, R.~Lyu, P.~Li, X.~Chen, W.~Zhang, K.~Chen, T.~Xue, X.~Liu, C.~Lu, D.~Lin, and J.~Pang.
\newblock Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024.

\bibitem{cogvlm}
W.~Wang, Q.~Lv, W.~Yu, W.~Hong, J.~Qi, Y.~Wang, J.~Ji, Z.~Yang, L.~Zhao, X.~Song, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock {\em arXiv preprint arXiv:2311.03079}, 2023.

\bibitem{chat3d}
Z.~Wang, H.~Huang, Y.~Zhao, Z.~Zhang, and Z.~Zhao.
\newblock Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes.
\newblock {\em arXiv preprint arXiv:2308.08769}, 2023.

\bibitem{pointllm}
R.~Xu, X.~Wang, T.~Wang, Y.~Chen, J.~Pang, and D.~Lin.
\newblock Pointllm: Empowering large language models to understand point clouds.
\newblock {\em arXiv preprint arXiv:2308.16911}, 2023.

\bibitem{X-trans2cap}
Z.~Yuan, X.~Yan, Y.~Liao, Y.~Guo, G.~Li, S.~Cui, and Z.~Li.
\newblock X-trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8563--8573, 2022.

\bibitem{multi3drefer}
Y.~Zhang, Z.~Gong, and A.~X. Chang.
\newblock Multi3drefer: Grounding text description to multiple 3d objects.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15225--15236, 2023.

\bibitem{3dvg-transformer}
L.~Zhao, D.~Cai, L.~Sheng, and D.~Xu.
\newblock 3dvg-transformer: Relation modeling for visual grounding on point clouds.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2928--2937, 2021.

\bibitem{zhu2023object2scene}
C.~Zhu, W.~Zhang, T.~Wang, X.~Liu, and K.~Chen.
\newblock Object2scene: Putting objects in context for open-vocabulary 3d detection.
\newblock {\em arXiv preprint arXiv:2309.09456}, 2023.

\bibitem{3dvista}
Z.~Zhu, X.~Ma, Y.~Chen, Z.~Deng, S.~Huang, and Q.~Li.
\newblock 3d-vista: Pre-trained transformer for 3d vision and text alignment.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2911--2921, 2023.

\end{thebibliography}
