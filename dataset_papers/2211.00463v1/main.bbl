\begin{thebibliography}{10}

\bibitem{MNIST}
\url{http://yann.lecun.com/exdb/mnist/}.

\bibitem{CIFAR}
\url{https://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem{STL10}
\url{https://cs.stanford.edu/\%7Eacoates/stl10/}.

\bibitem{ACGMMTZ16}
Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock {Deep Learning with Differential Privacy}.
\newblock In {\em {ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}}, pages 308--318. ACM, 2016.

\bibitem{BPS19}
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov.
\newblock {Differential Privacy Has Disparate Impact on Model Accuracy}.
\newblock In {\em {Annual Conference on Neural Information Processing Systems
  (NeurIPS)}}, pages 15453--15462. NeurIPS, 2019.

\bibitem{BST14}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock {Differentially Private Empirical Risk Minimization: Efficient
  Algorithms and Tight Error Bounds}.
\newblock In {\em {Annual Symposium on Foundations of Computer Science
  (FOCS)}}, pages 464--473. IEEE, 2014.

\bibitem{BNL12}
Battista Biggio, Blaine Nelson, and Pavel Laskov.
\newblock {Poisoning Attacks against Support Vector Machines}.
\newblock In {\em {International Conference on Machine Learning (ICML)}}.
  icml.cc / Omnipress, 2012.

\bibitem{BG192}
Aleksandar Bojchevski and Stephan G{\"u}nnemann.
\newblock {Adversarial Attacks on Node Embeddings via Graph Poisoning}.
\newblock In {\em {International Conference on Machine Learning (ICML)}}, pages
  695--704. PMLR, 2019.

\bibitem{CCNSTT22}
Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and
  Florian Tram{\`{e}}r.
\newblock {Membership Inference Attacks From First Principles}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  1897--1914. IEEE, 2022.

\bibitem{CLEKS19}
Nicholas Carlini, Chang Liu, {\'U}lfar Erlingsson, Jernej Kos, and Dawn Song.
\newblock {The Secret Sharer: Evaluating and Testing Unintended Memorization in
  Neural Networks}.
\newblock In {\em {USENIX Security Symposium (USENIX Security)}}, pages
  267--284. USENIX, 2019.

\bibitem{CW17}
Nicholas Carlini and David Wagner.
\newblock {Towards Evaluating the Robustness of Neural Networks}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  39--57. IEEE, 2017.

\bibitem{DMNS06}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock {Calibrating Noise to Sensitivity in Private Data Analysis}.
\newblock In {\em {Theory of Cryptography Conference (TCC)}}, pages 265--284.
  Springer, 2006.

\bibitem{DR14}
Cynthia Dwork and Aaron Roth.
\newblock {\em {The Algorithmic Foundations of Differential Privacy}}.
\newblock Now Publishers Inc., 2014.

\bibitem{GSKGRF19}
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and
  Rog{\'{e}}rio~Schmidt Feris.
\newblock {SpotTune: Transfer Learning Through Adaptive Fine-Tuning}.
\newblock In {\em {IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}}, pages 4805--4814. IEEE, 2019.

\bibitem{HMDC19}
Jamie Hayes, Luca Melis, George Danezis, and Emiliano~De Cristofaro.
\newblock {LOGAN: Evaluating Privacy Leakage of Generative Models Using
  Generative Adversarial Networks}.
\newblock {\em {Privacy Enhancing Technologies Symposium}}, 2019.

\bibitem{HGFTG20}
W.~Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.
\newblock {MetaPoison: Practical General-purpose Clean-label Data Poisoning}.
\newblock In {\em {Annual Conference on Neural Information Processing Systems
  (NeurIPS)}}. NeurIPS, 2020.

\bibitem{HYYBGC21}
Bo~Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil~Zhenqiang Gong, and
  Yinzhi Cao.
\newblock {Practical Blind Membership Inference Attack via Differential
  Comparisons}.
\newblock In {\em {Network and Distributed System Security Symposium (NDSS)}}.
  Internet Society, 2021.

\bibitem{INSTTW19}
Roger Iyengar, Joseph~P. Near, Dawn~Xiaodong Song, Om~Dipakbhai Thakkar,
  Abhradeep Thakurta, and Lun Wang.
\newblock {Towards Practical Differentially Private Convex Optimization}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  299--316. IEEE, 2019.

\bibitem{JOBLNL18}
Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina
  Nita-Rotaru, and Bo~Li.
\newblock {Manipulating Machine Learning: Poisoning Attacks and Countermeasures
  for Regression Learning}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  19--35. IEEE, 2018.

\bibitem{JE19}
Bargav Jayaraman and David Evans.
\newblock {Evaluating Differentially Private Machine Learning in Practice}.
\newblock In {\em {USENIX Security Symposium (USENIX Security)}}, pages
  1895--1912. USENIX, 2019.

\bibitem{KH91}
Anders Krogh and John~A. Hertz.
\newblock {A Simple Weight Decay Can Improve Generalization}.
\newblock In {\em {Annual Conference on Neural Information Processing Systems
  (NIPS)}}, pages 950--957. NIPS, 1991.

\bibitem{LF20}
Klas Leino and Matt Fredrikson.
\newblock {Stolen Memories: Leveraging Model Memorization for Calibrated
  White-Box Membership Inference}.
\newblock In {\em {USENIX Security Symposium (USENIX Security)}}, pages
  1605--1622. USENIX, 2020.

\bibitem{LQSWY13}
Ninghui Li, Wahbeh~H. Qardaji, Dong Su, Yi~Wu, and Weining Yang.
\newblock {Membership Privacy: A Unifying Framework for Privacy Definitions}.
\newblock In {\em {ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}}, pages 889--900. ACM, 2013.

\bibitem{LZ21}
Zheng Li and Yang Zhang.
\newblock {Membership Leakage in Label-Only Exposures}.
\newblock In {\em {ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}}, pages 880--895. ACM, 2021.

\bibitem{LLWT15}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock {Deep Learning Face Attributes in the Wild}.
\newblock In {\em {IEEE International Conference on Computer Vision (ICCV)}},
  pages 3730--3738. IEEE, 2015.

\bibitem{MGC22}
Saeed Mahloujifar, Esha Ghosh, and Melissa Chase.
\newblock {Property Inference from Poisoning}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  1120--1137. IEEE, 2022.

\bibitem{NSH19}
Milad Nasr, Reza Shokri, and Amir Houmansadr.
\newblock {Comprehensive Privacy Analysis of Deep Learning: Passive and Active
  White-box Inference Attacks against Centralized and Federated Learning}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  1021--1035. IEEE, 2019.

\bibitem{NSTPC21}
Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas
  Carlini.
\newblock {Adversary Instantiation: Lower Bounds for Differentially Private
  Machine Learning}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}. IEEE, 2021.

\bibitem{PMSW18}
Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman.
\newblock {SoK: Towards the Science of Security and Privacy in Machine
  Learning}.
\newblock In {\em {IEEE European Symposium on Security and Privacy (Euro
  S\&P)}}, pages 399--414. IEEE, 2018.

\bibitem{RHW21}
Yuji Roh, Geon Heo, and Steven~Euijong Whang.
\newblock {A Survey on Data Collection for Machine Learning: {A} Big Data -
  {AI} Integration Perspective}.
\newblock {\em {IEEE Transactions on Knowledge and Data Engineering}}, 2021.

\bibitem{SDSOJ19}
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and
  Herv{\'e} J{\'e}gou.
\newblock {White-box vs Black-box: Bayes Optimal Strategies for Membership
  Inference}.
\newblock In {\em {International Conference on Machine Learning (ICML)}}, pages
  5558--5567. PMLR, 2019.

\bibitem{SWBMZ22}
Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang.
\newblock {Dynamic Backdoor Attacks Against Machine Learning Models}.
\newblock In {\em {IEEE European Symposium on Security and Privacy (Euro
  S\&P)}}, pages 703--718. IEEE, 2022.

\bibitem{SZHBFB19}
Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
  Michael Backes.
\newblock {ML-Leaks: Model and Data Independent Membership Inference Attacks
  and Defenses on Machine Learning Models}.
\newblock In {\em {Network and Distributed System Security Symposium (NDSS)}}.
  Internet Society, 2019.

\bibitem{SHNSSDG18}
Ali Shafahi, W~Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
  Tudor Dumitras, and Tom Goldstein.
\newblock {Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural
  Networks}.
\newblock In {\em {Annual Conference on Neural Information Processing Systems
  (NeurIPS)}}, pages 6103--6113. NeurIPS, 2018.

\bibitem{SNGXDSDTG19}
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John~P. Dickerson, Christoph
  Studer, Larry~S. Davis, Gavin Taylor, and Tom Goldstein.
\newblock {Adversarial training for free!}
\newblock In {\em {Annual Conference on Neural Information Processing Systems
  (NeurIPS)}}, pages 3353--3364. NeurIPS, 2019.

\bibitem{SSZGSJG20}
Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer,
  David~W. Jacobs, and Tom Goldstein.
\newblock {Adversarially Robust Transfer Learning}.
\newblock In {\em {International Conference on Learning Representations
  (ICLR)}}, 2020.

\bibitem{SSZ20}
Reza Shokri, Martin Strobel, and Yair Zick.
\newblock {Exploiting Transparency Measures for Membership Inference: a
  Cautionary Tale}.
\newblock In {\em {The AAAI Workshop on Privacy-Preserving Artificial
  Intelligence (PPAI)}}. AAAI, 2020.

\bibitem{SSSS17}
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
\newblock {Membership Inference Attacks Against Machine Learning Models}.
\newblock In {\em {IEEE Symposium on Security and Privacy (S\&P)}}, pages
  3--18. IEEE, 2017.

\bibitem{SM21}
Liwei Song and Prateek Mittal.
\newblock {Systematic Evaluation of Privacy Risks of Machine Learning Models}.
\newblock In {\em {USENIX Security Symposium (USENIX Security)}}. USENIX, 2021.

\bibitem{SSM19}
Liwei Song, Reza Shokri, and Prateek Mittal.
\newblock {Privacy Risks of Securing Machine Learning Models against
  Adversarial Examples}.
\newblock In {\em {ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}}, pages 241--257. ACM, 2019.

\bibitem{SCS13}
Shuang Song, Kamalika Chaudhuri, and Anand~D. Sarwate.
\newblock {Stochastic Gradient Descent with Differentially Private Updates}.
\newblock In {\em {IEEE Global Conference on Signal and Information Processing
  (GlobalSIP)}}, pages 245--248. IEEE, 2013.

\bibitem{TTGL20}
Vale Tolpegin, Stacey Truex, Mehmet~Emre Gursoy, and Ling Liu.
\newblock {Data Poisoning Attacks Against Federated Learning Systems}.
\newblock In {\em {European Symposium on Research in Computer Security
  (ESORICS)}}, pages 480--501. Springer, 2020.

\bibitem{TSJLJHC22}
Florian Tram{\`e}, Reza Shokri, Ayrton~San Joaquin, Hoang Le, Matthew
  Jagielski, Sanghyun Hong, and Nicholas Carlini.
\newblock {Truth Serum: Poisoning Machine Learning Models to Reveal Their
  Secrets}.
\newblock In {\em {ACM SIGSAC Conference on Computer and Communications
  Security (CCS)}}. ACM, 2022.

\bibitem{TKPGBM17}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock {Ensemble Adversarial Training: Attacks and Defenses}.
\newblock In {\em {International Conference on Learning Representations
  (ICLR)}}, 2017.

\bibitem{MH08}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock {Visualizing Data using t-SNE}.
\newblock {\em {Journal of Machine Learning Research}}, 2008.

\bibitem{VLWCW18}
Bastiaan~S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling.
\newblock {Rotation Equivariant CNNs for Digital Pathology}.
\newblock In {\em {Medical Image Computing and Computer Assisted Intervention
  (MICCAI)}}, pages 210--218. Springer, 2018.

\bibitem{WYVZZ18}
Bolun Wang, Yuanshun Yao, Bimal Viswanath, Haitao Zheng, and Ben~Y. Zhao.
\newblock {With Great Training Comes Great Vulnerability: Practical Attacks
  against Transfer Learning}.
\newblock In {\em {USENIX Security Symposium (USENIX Security)}}, pages
  1281--1297. USENIX, 2018.

\bibitem{WSRVASLP20}
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
  Agarwal, Jy~yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock {Attack of the Tails: Yes, You Really Can Backdoor Federated
  Learning}.
\newblock In {\em {Annual Conference on Neural Information Processing Systems
  (NeurIPS)}}. NeurIPS, 2020.

\bibitem{WRK20}
Eric Wong, Leslie Rice, and J.~Zico Kolter.
\newblock {Fast Is Better Than Free: Revisiting Adversarial Training}.
\newblock In {\em {International Conference on Learning Representations
  (ICLR)}}, 2020.

\bibitem{XPJW21}
Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang.
\newblock {Graph Backdoor}.
\newblock In {\em {USENIX Security Symposium (USENIX Security)}}. USENIX, 2021.

\bibitem{YGFJ18}
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.
\newblock {Privacy Risk in Machine Learning: Analyzing the Connection to
  Overfitting}.
\newblock In {\em {IEEE Computer Security Foundations Symposium (CSF)}}, pages
  268--282. IEEE, 2018.

\bibitem{ZGS21}
Hengtong Zhang, Jing Gao, and Lu~Su.
\newblock {Data Poisoning Attacks Against Outcome Interpretations of Predictive
  Models}.
\newblock In {\em {ACM Conference on Knowledge Discovery and Data Mining
  (KDD)}}, pages 2165--2173. ACM, 2021.

\bibitem{ZMZBCJ20}
Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang
  Jiang.
\newblock {Clean-Label Backdoor Attacks on Video Recognition Models}.
\newblock In {\em {IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}}, pages 14443--144528. IEEE, 2020.

\bibitem{ZL21}
Tianhang Zheng and Baochun Li.
\newblock {First-Order Efficient General-Purpose Clean-Label Data Poisoning}.
\newblock In {\em {IEEE Conference on Computer Communications (INFOCOM)}},
  pages 1--10. IEEE, 2021.

\bibitem{ZHLTSG19}
Chen Zhu, W~Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom
  Goldstein.
\newblock {Transferable Clean-label Poisoning Attacks on Deep Neural Nets}.
\newblock In {\em {International Conference on Machine Learning (ICML)}}, pages
  7614--7623. JMLR, 2019.

\end{thebibliography}
