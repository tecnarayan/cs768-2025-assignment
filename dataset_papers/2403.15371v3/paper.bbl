\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2023)Abernethy, Agarwal, Marinov, and
  Warmuth]{abernethy2023mechanism}
Jacob Abernethy, Alekh Agarwal, Teodor~V Marinov, and Manfred~K Warmuth.
\newblock A mechanism for sample-efficient in-context learning for sparse
  retrieval tasks.
\newblock \emph{arXiv:2305.17040}, 2023.

\bibitem[Agrawal and Goyal(2012)]{Shipra-colt12}
Shipra Agrawal and Navin Goyal.
\newblock {Analysis of Thompson Sampling for the multi-armed bandit problem}.
\newblock In \emph{Conference on Learning Theory}, 2012.

\bibitem[Agrawal and Goyal(2017)]{Shipra-aistats13-JACM}
Shipra Agrawal and Navin Goyal.
\newblock Near-optimal regret bounds for thompson sampling.
\newblock \emph{Journal of the ACM}, 2017.
\newblock Preliminary version in \emph{AISTATS 2013}.

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Daneshmand, and Sra]{ahn2023transformers}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra.
\newblock Transformers learn to implement preconditioned gradient descent for
  in-context learning.
\newblock \emph{arXiv:2306.00297}, 2023.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu,
  Gopalakrishnan, Hausman, Herzon, Hsu, Ibarz, Ichter, Irpan, Jang, Ruano,
  Jeffrey, Jesmonth, Joshi, Julian, Kalashnikov, Kuang, Lee, Levine, Lu, Luu,
  Parada, Pastor, Quiambao, Rao, Rettinghouse, Reyes, Sermanet, Sievers, Tan,
  Toshev, Vanhoucke, Xia, Xiao, Xu, Xu, Yan, and Zeng]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
  David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
  Daniel Herzon, Alexand~Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex
  Irpan, Eric Jang, Rosario~Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth,
  Nikhil~J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei
  Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell
  Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet,
  Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,
  Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng.
\newblock Do as {I} can, not as {I} say: Grounding language in robotic
  affordances.
\newblock \emph{arXiv:2204.01691}, 2022.

\bibitem[Ahuja et~al.(2023)Ahuja, Panwar, and Goyal]{ahuja2023context}
Kabir Ahuja, Madhur Panwar, and Navin Goyal.
\newblock In-context learning through the bayesian prism.
\newblock \emph{arXiv:2306.04891}, 2023.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? {I}nvestigations with
  linear models.
\newblock \emph{arXiv:2211.15661}, 2022.

\bibitem[Aky{\"u}rek et~al.(2024)Aky{\"u}rek, Wang, Kim, and
  Andreas]{akyurek2024context}
Ekin Aky{\"u}rek, Bailin Wang, Yoon Kim, and Jacob Andreas.
\newblock In-context language learning: Architectures and algorithms.
\newblock \emph{arXiv:2401.12973}, 2024.

\bibitem[Auer et~al.(2002{\natexlab{a}})Auer, Cesa-Bianchi, and
  Fischer]{bandits-ucb1}
Peter Auer, Nicol{\`o} Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine Learning}, 2002{\natexlab{a}}.

\bibitem[Auer et~al.(2002{\natexlab{b}})Auer, Cesa-Bianchi, Freund, and
  Schapire]{bandits-exp3}
Peter Auer, Nicol{\`o} Cesa-Bianchi, Yoav Freund, and Robert~E. Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM J. Comput.}, 32\penalty0 (1):\penalty0 48--77,
  2002{\natexlab{b}}.
\newblock Preliminary version in {\em 36th IEEE FOCS}, 1995.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{bai2023transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with
  in-context algorithm selection.
\newblock \emph{arXiv:2306.04637}, 2023.

\bibitem[Banihashem et~al.(2023)Banihashem, Hajiaghayi, Shin, and
  Slivkins]{BSL-myopic23}
Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, and Aleksandrs
  Slivkins.
\newblock Bandit social learning: Exploration under myopic behavior.
\newblock \emph{arXiv:2302.07425}, 2023.

\bibitem[Bhattamishra et~al.(2023)Bhattamishra, Patel, Blunsom, and
  Kanade]{bhattamishra2023understanding}
Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade.
\newblock Understanding in-context learning in transformers and {LLM}s by
  learning to learn discrete functions.
\newblock \emph{arXiv:2310.03016}, 2023.

\bibitem[Binz and Schulz(2023)]{binz2023using}
Marcel Binz and Eric Schulz.
\newblock Using cognitive psychology to understand {GPT}-3.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2023.

\bibitem[Brooks et~al.(2023)Brooks, Walls, Lewis, and Singh]{brooks2023large}
Ethan Brooks, Logan~A Walls, Richard Lewis, and Satinder Singh.
\newblock Large language models can implement policy iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3-neurips20}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Bubeck and Cesa-Bianchi(2012)]{Bubeck-survey12}
S\'{e}bastien Bubeck and Nicolo Cesa-Bianchi.
\newblock {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
  Problems}.
\newblock \emph{Foundations and Trends in Machine Learning}, 5\penalty0
  (1):\penalty0 1--122, 2012.
\newblock Published with \emph{Now Publishers} (Boston, MA, USA). Also
  available at {\tt https://arxiv.org/abs/1204.5721}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and
  Zhang]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  Harsha Nori, Hamid Palangi, Marco~Tulio Ribeiro, and Yi~Zhang.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv:2303.12712}, 2023.

\bibitem[Cao et~al.(2024)Cao, Zhao, Cheng, Shu, Liu, Liang, Zhao, and
  Li]{cao2024survey}
Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang, Junhua
  Zhao, and Yun Li.
\newblock Survey on large language model-enhanced reinforcement learning:
  Concept, taxonomy, and methods.
\newblock \emph{arXiv:2404.00282}, 2024.

\bibitem[Cheng et~al.(2023)Cheng, Chen, and Sra]{cheng2023transformers}
Xiang Cheng, Yuxin Chen, and Suvrit Sra.
\newblock Transformers implement functional gradient descent to learn
  non-linear functions in context.
\newblock \emph{arXiv:2312.06528}, 2023.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv:2110.14168}, 2021.

\bibitem[Coda-Forno et~al.(2023)Coda-Forno, Binz, Akata, Botvinick, Wang, and
  Schulz]{coda2023meta}
Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and
  Eric Schulz.
\newblock Meta-in-context learning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Coda-Forno et~al.(2024)Coda-Forno, Binz, Wang, and
  Schulz]{coda2024cogbench}
Julian Coda-Forno, Marcel Binz, Jane~X Wang, and Eric Schulz.
\newblock Cogbench: a large language model walks into a psychology lab.
\newblock \emph{arXiv:2402.18225}, 2024.

\bibitem[Dettmers and Zettlemoyer(2023)]{dettmers2023case}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Edwards et~al.(2023)Edwards, Naik, Khot, Burke, Ji, and
  Hope]{edwards2023synergpt}
Carl~N Edwards, Aakanksha Naik, Tushar Khot, Martin~D Burke, Heng Ji, and Tom
  Hope.
\newblock Synergpt: In-context learning for personalized drug synergy
  prediction and drug design.
\newblock \emph{arXiv:2307.11694}, 2023.

\bibitem[Fu et~al.(2023)Fu, Chen, Jia, and Sharan]{fu2023transformers}
Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan.
\newblock Transformers learn higher-order optimization methods for in-context
  learning: A study with linear models.
\newblock \emph{arXiv:2310.17086}, 2023.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and
  Neubig]{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
  Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Guo et~al.(2023)Guo, Hu, Mei, Wang, Xiong, Savarese, and
  Bai]{guo2023transformers}
Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and
  Yu~Bai.
\newblock How do transformers learn in-context beyond simple functions? {A}
  case study on learning with representations.
\newblock \emph{arXiv:2310.10616}, 2023.

\bibitem[Hahn and Goyal(2023)]{hahn2023theory}
Michael Hahn and Navin Goyal.
\newblock A theory of emergent in-context learning as implicit structure
  induction.
\newblock \emph{arXiv:2303.07971}, 2023.

\bibitem[Han et~al.(2023{\natexlab{a}})Han, Wang, Zhao, and
  Ji]{han2023explaining}
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji.
\newblock Explaining emergent in-context learning as kernel regression.
\newblock \emph{arXiv:2305.12766}, 2023{\natexlab{a}}.

\bibitem[Han et~al.(2023{\natexlab{b}})Han, Simig, Mihaylov, Tsvetkov,
  Celikyilmaz, and Wang]{han2023understanding}
Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz,
  and Tianlu Wang.
\newblock Understanding in-context learning via supportive pretraining data.
\newblock \emph{arXiv:2306.15091}, 2023{\natexlab{b}}.

\bibitem[Hayes et~al.(2024)Hayes, Yax, and Palminteri]{hayes2024relative}
William~M Hayes, Nicolas Yax, and Stefano Palminteri.
\newblock Relative value biases in large language models.
\newblock \emph{arXiv:2401.14530}, 2024.

\bibitem[Hendel et~al.(2023)Hendel, Geva, and Globerson]{hendel2023context}
Roee Hendel, Mor Geva, and Amir Globerson.
\newblock In-context learning creates task vectors.
\newblock \emph{arXiv:2310.15916}, 2023.

\bibitem[Ho et~al.(2016)Ho, Slivkins, and Vaughan]{RepeatedPA-ec14}
Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer~Wortman Vaughan.
\newblock Adaptive contract design for crowdsourcing markets: Bandit algorithms
  for repeated principal-agent problems.
\newblock \emph{Journal of Artificial Intelligence Research}, 2016.
\newblock Preliminary version in \emph{ACM EC 2014}.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv:2310.05249}, 2023.

\bibitem[Jeon et~al.(2024)Jeon, Lee, Lei, and Van~Roy]{jeon2024information}
Hong~Jun Jeon, Jason~D Lee, Qi~Lei, and Benjamin Van~Roy.
\newblock An information-theoretic analysis of in-context learning.
\newblock \emph{arXiv:2401.15530}, 2024.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and Munos]{Kaufmann-alt12}
Emilie Kaufmann, Nathaniel Korda, and R{\'{e}}mi Munos.
\newblock Thompson sampling: {A}n asymptotically optimal finite-time analysis.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  2012.

\bibitem[K{\i}c{\i}man et~al.(2023)K{\i}c{\i}man, Ness, Sharma, and
  Tan]{kiciman2023causal}
Emre K{\i}c{\i}man, Robert Ness, Amit Sharma, and Chenhao Tan.
\newblock Causal reasoning and large language models: Opening a new frontier
  for causality.
\newblock \emph{arXiv:2305.00050}, 2023.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl-Dickstein, and
  Metz]{kirsch2022general}
Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz.
\newblock General-purpose in-context learning by meta-learning transformers.
\newblock \emph{arXiv:2212.04458}, 2022.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 2022.

\bibitem[Laskin et~al.(2022)Laskin, Wang, Oh, Parisotto, Spencer, Steigerwald,
  Strouse, Hansen, Filos, Brooks, Gazeau, Sahni, Singh, and
  Mnih]{laskin2022context}
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer,
  Richie Steigerwald, DJ~Strouse, Steven Hansen, Angelos Filos, Ethan Brooks,
  Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih.
\newblock In-context reinforcement learning with algorithm distillation.
\newblock \emph{arXiv:2210.14215}, 2022.

\bibitem[Lattimore and Szepesv\'{a}ri(2020)]{LS19bandit-book}
Tor Lattimore and Csaba Szepesv\'{a}ri.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2023{\natexlab{a}})Lee, Xie, Pacchiano, Chandak, Finn,
  Nachum, and Brunskill]{lee2023supervised}
Jonathan~N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir
  Nachum, and Emma Brunskill.
\newblock Supervised pretraining can learn in-context reinforcement learning.
\newblock \emph{arXiv:2306.14892}, 2023{\natexlab{a}}.

\bibitem[Lee et~al.(2023{\natexlab{b}})Lee, Goldberg, and Kohane]{lee2023ai}
Peter Lee, Carey Goldberg, and Isaac Kohane.
\newblock \emph{The AI revolution in medicine: GPT-4 and beyond}.
\newblock Pearson, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Ildiz, Papailiopoulos, and
  Oymak]{li2023transformers}
Yingcong Li, Muhammed~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
\newblock Transformers as algorithms: Generalization and stability in
  in-context learning.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Lin et~al.(2023)Lin, Bai, and Mei]{lin2023transformers}
Licong Lin, Yu~Bai, and Song Mei.
\newblock Transformers as decision makers: Provable in-context reinforcement
  learning via supervised pretraining.
\newblock \emph{arXiv:2310.08566}, 2023.

\bibitem[Liu et~al.(2024)Liu, Ash, Goel, Krishnamurthy, and
  Zhang]{liu2024exposing}
Bingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Exposing attention glitches with flip-flop language modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang,
  Galley, and Gao]{lu2023mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh
  Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in
  visual contexts.
\newblock \emph{arXiv:2310.02255}, 2023.

\bibitem[Malach(2023)]{malach2023auto}
Eran Malach.
\newblock Auto-regressive next-token predictors are universal learners.
\newblock \emph{arXiv:2309.06979}, 2023.

\bibitem[Momennejad et~al.(2023)Momennejad, Hasanbeig, Vieira, Sharma, Ness,
  Jojic, Palangi, and Larson]{momennejad2023evaluating}
Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert~Osazuwa
  Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson.
\newblock Evaluating cognitive maps and planning in large language models with
  cogeval.
\newblock \emph{arXiv:2309.15129}, 2023.

\bibitem[Monea et~al.(2024)Monea, Bosselut, Brantley, and Artzi]{monea2024llms}
Giovanni Monea, Antoine Bosselut, Kiant'{e} Brantley, and Yoav Artzi.
\newblock {LLM}s are in-context reinforcement learners.
\newblock \emph{arxiv:2410.05362}, 2024.

\bibitem[Nie et~al.(2024)Nie, Su, Lee, Chi, Le, and Minmin]{nie2024evolve}
Allen Nie, Yi~Su, Jonathan~N. Lee, Ed~H. Chi, Quoc~V. Le, and Chen Minmin.
\newblock {EVOL}v{E}: {E}valuating and optimizing {LLM}s for exploration.
\newblock \emph{arxiv:2410.06238}, 2024.

\bibitem[OpenAI(2023)]{achiam2023gpt}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv:2303.08774}, 2023.

\bibitem[Park et~al.(2024)Park, Liu, Ozdaglar, and Zhang]{park2024llm}
Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang.
\newblock Do {LLM} agents have regret? {A} case study in online learning and
  games.
\newblock \emph{arXiv:2403.16843}, 2024.

\bibitem[Park et~al.(2023)Park, O'Brien, Cai, Morris, Liang, and
  Bernstein]{park2023generative}
Joon~Sung Park, Joseph O'Brien, Carrie~Jun Cai, Meredith~Ringel Morris, Percy
  Liang, and Michael~S Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In \emph{Symposium on User Interface Software and Technology}, 2023.

\bibitem[Raparthy et~al.(2023)Raparthy, Hambro, Kirk, Henaff, and
  Raileanu]{raparthy2023generalization}
Sharath~Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, and Roberta
  Raileanu.
\newblock Generalization to new sequential decision making tasks with
  in-context learning.
\newblock \emph{arXiv:2312.03801}, 2023.

\bibitem[Ravent{\'o}s et~al.(2023)Ravent{\'o}s, Paul, Chen, and
  Ganguli]{raventos2023pretraining}
Allan Ravent{\'o}s, Mansheej Paul, Feng Chen, and Surya Ganguli.
\newblock Pretraining task diversity and the emergence of non-bayesian
  in-context learning for regression.
\newblock \emph{arXiv:2306.15063}, 2023.

\bibitem[Russo et~al.(2018)Russo, {Van Roy}, Kazerouni, Osband, and
  Wen]{TS-survey-FTML18}
Daniel Russo, Benjamin {Van Roy}, Abbas Kazerouni, Ian Osband, and Zheng Wen.
\newblock A tutorial on thompson sampling.
\newblock \emph{Foundations and Trends in Machine Learning}, 2018.

\bibitem[Schubert et~al.(2024)Schubert, Jagadish, Binz, and
  Schulz]{schubert2024context}
Johannes~A Schubert, Akshay~K Jagadish, Marcel Binz, and Eric Schulz.
\newblock In-context learning agents are asymmetric belief updaters.
\newblock \emph{arXiv:2402.03969}, 2024.

\bibitem[Sclar et~al.(2023)Sclar, Choi, Tsvetkov, and
  Suhr]{sclar2023quantifying}
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
\newblock Quantifying language models' sensitivity to spurious features in
  prompt design or: How i learned to start worrying about prompt formatting.
\newblock \emph{arXiv:2310.11324}, 2023.

\bibitem[Shen et~al.(2023)Shen, Mishra, and Khashabi]{shen2023pretrained}
Lingfeng Shen, Aayush Mishra, and Daniel Khashabi.
\newblock Do pretrained transformers really learn in-context by gradient
  descent?
\newblock \emph{arXiv:2310.08540}, 2023.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Labash, Gopinath, Narasimhan, and
  Yao]{shinn2023reflexion}
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
  and Shunyu Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{arXiv:2303.11366}, 2023.

\bibitem[Simchowitz et~al.(2021)Simchowitz, Tosh, Krishnamurthy, Hsu, Lykouris,
  Dudik, and Schapire]{simchowitz2021bayesian}
Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel~J Hsu, Thodoris
  Lykouris, Miro Dudik, and Robert~E Schapire.
\newblock Bayesian decision-making under misspecified priors with applications
  to meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Slivkins(2019)]{slivkins-MABbook}
Aleksandrs Slivkins.
\newblock Introduction to multi-armed bandits.
\newblock \emph{Foundations and Trends in Machine Learning}, 2019.

\bibitem[Slivkins et~al.(2013)Slivkins, Radlinski, and
  Gollapudi]{ZoomingRBA-icml10}
Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi.
\newblock Ranked bandits in metric spaces: Learning optimally diverse rankings
  over large document collections.
\newblock \emph{Journal of Machine Learning Research}, 2013.
\newblock Preliminary version in {\em ICML}, 2010.

\bibitem[Som et~al.(2023)Som, Sikka, Gent, Divakaran, Kathol, and
  Vergyri]{som2023demonstrations}
Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol, and
  Dimitra Vergyri.
\newblock Demonstrations are all you need: Advancing offensive content
  paraphrasing using in-context learning.
\newblock \emph{arXiv:2310.10707}, 2023.

\bibitem[Thompson(1933)]{Thompson-1933}
William~R. Thompson.
\newblock {On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.}
\newblock \emph{Biometrika}, 1933.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull,
  Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini,
  Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril,
  Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton,
  Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang,
  Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang,
  Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv:2307.09288}, 2023.

\bibitem[Valmeekam et~al.(2023)Valmeekam, Marquez, Olmo, Sreedharan, and
  Kambhampati]{valmeekam2023planbench}
Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and
  Subbarao Kambhampati.
\newblock Planbench: An extensible benchmark for evaluating large language
  models on planning and reasoning about change.
\newblock In \emph{Advances in Neural Information Processing Systems: Datasets
  and Benchmarks Track}, 2023.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Wang et~al.(2023)Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and
  Anandkumar]{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
  Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv:2305.16291}, 2023.

\bibitem[Weber et~al.(2023)Weber, Bruni, and Hupkes]{weber2023icl}
Lucas Weber, Elia Bruni, and Dieuwke Hupkes.
\newblock The {ICL} consistency test.
\newblock \emph{arXiv:2312.04945}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, and
  Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wies et~al.(2023)Wies, Levine, and Shashua]{wies2023learnability}
Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock The learnability of in-context learning.
\newblock \emph{arXiv:2303.07895}, 2023.

\bibitem[Wu et~al.(2023)Wu, Zou, Chen, Braverman, Gu, and Bartlett]{wu2023many}
Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and
  Peter~L Bartlett.
\newblock How many pretraining tasks are needed for in-context learning of
  linear regression?
\newblock \emph{arXiv:2310.08391}, 2023.

\bibitem[Wu et~al.(2024)Wu, Tang, Mitchell, and Li]{wu2024smartplay}
Yue Wu, Xuan Tang, Tom Mitchell, and Yuanzhi Li.
\newblock Smartplay: {A} benchmark for {LLM}s as intelligent agents.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock \emph{arXiv:2111.02080}, 2021.

\bibitem[Xu et~al.(2022)Xu, Shen, Zhang, Lu, Zhao, Tenenbaum, and
  Gan]{xu2022prompting}
Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and
  Chuang Gan.
\newblock Prompting decision transformer for few-shot policy generalization.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Xu et~al.(2023)Xu, Huang, Yu, Liu, Zhang, Niu, Zhang, Xia, Tan, and
  Zhao]{xu2023creative}
Mengdi Xu, Peide Huang, Wenhao Yu, Shiqi Liu, Xilun Zhang, Yaru Niu, Tingnan
  Zhang, Fei Xia, Jie Tan, and Ding Zhao.
\newblock Creative robot tool use with large language models.
\newblock \emph{arXiv:2310.13065}, 2023.

\bibitem[Yiu et~al.(2023)Yiu, Kosoy, and Gopnik]{yiu2023imitation}
Eunice Yiu, Eliza Kosoy, and Alison Gopnik.
\newblock Imitation versus innovation: What children can do that large language
  and language-and-vision models cannot (yet)?
\newblock \emph{arXiv:2305.07666}, 2023.

\bibitem[Yu et~al.(2023)Yu, Kaur, Gupta, Brown-Cohen, Goyal, and
  Arora]{yu2023skill}
Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and
  Sanjeev Arora.
\newblock Skill-mix: A flexible and expandable family of evaluations for ai
  models.
\newblock \emph{arXiv:2310.17567}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Frei, and
  Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv:2306.09927}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhang, Yang, and
  Wang]{zhang2023and}
Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang.
\newblock What and how does in-context learning learn? bayesian model
  averaging, parameterization, and generalization.
\newblock \emph{arXiv:2305.19420}, 2023{\natexlab{b}}.

\end{thebibliography}
