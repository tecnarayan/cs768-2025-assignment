@article{hoeffding1948class,
  title={A Class of Statistics with Asymptotically Normal Distribution},
  author={Hoeffding, Wassily and others},
  journal={The Annals of Mathematical Statistics},
  volume={19},
  number={3},
  pages={293--325},
  year={1948},
  publisher={Institute of Mathematical Statistics}
}


@book{van2000asymptotic,
  title={Asymptotic statistics},
  author={Van der Vaart, Aad W},
  volume={3},
  year={2000},
  publisher={Cambridge university press}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{huang2019dynamics,
  title={Dynamics of deep neural networks and neural tangent hierarchy},
  author={Huang, Jiaoyang and Yau, Horng-Tzer},
  journal={arXiv preprint arXiv:1909.08156},
  year={2019}
}

@article{cho2010large,
  title={Large-margin classification in infinite neural networks},
  author={Cho, Youngmin and Saul, Lawrence K},
  journal={Neural computation},
  volume={22},
  number={10},
  pages={2678--2697},
  year={2010},
  publisher={MIT Press}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8139--8148},
  year={2019}
}

@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12873--12884},
  year={2019}
}

@article{aomoto1977analytic,
  title={Analytic structure of Schl{\"a}fli function},
  author={Aomoto, Kazuhiko},
  journal={Nagoya Mathematical Journal},
  volume={68},
  pages={1--16},
  year={1977},
  publisher={Cambridge University Press}
}

@article{ribando2006measuring,
  title={Measuring solid angles beyond dimension three},
  author={Ribando, Jason M},
  journal={Discrete \& Computational Geometry},
  volume={36},
  number={3},
  pages={479--487},
  year={2006},
  publisher={Springer}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{du2019graph,
  title={Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
  author={Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5724--5734},
  year={2019}
}

@article{li2019enhanced,
  title={Enhanced Convolutional Neural Tangent Kernels},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1911.00809},
  year={2019}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}




@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2933--2943},
  year={2019}
}

@article{hanin2019finite,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Hanin, Boris and Nica, Mihai},
  journal={arXiv preprint arXiv:1909.05989},
  year={2019}
}


@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1904.12191},
  year={2019}
}

@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6594--6604},
  year={2019}
}


@inproceedings{ghorbani2019limitations,
  title={Limitations of Lazy Training of Two-layers Neural Network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9108--9118},
  year={2019}
}

@article{bai2019beyond,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}

@article{bai2020taylorized,
  title={Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width},
  author={Bai, Yu and Krause, Ben and Wang, Huan and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:2002.04010},
  year={2020}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}


@incollection{neal1996priors,
  title={Priors for infinite networks},
  author={Neal, Radford M},
  booktitle={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996},
  publisher={Springer}
}

@inproceedings{williams1997computing,
  title={Computing with infinite networks},
  author={Williams, Christopher KI},
  booktitle={Advances in neural information processing systems},
  pages={295--301},
  year={1997}
}

@inproceedings{le2007continuous,
  title={Continuous neural networks},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  booktitle={Artificial Intelligence and Statistics},
  pages={404--411},
  year={2007}
}

@article{hazan2015steps,
  title={Steps toward deep kernel methods from infinite neural networks},
  author={Hazan, Tamir and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:1508.05133},
  year={2015}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{novak2018bayesian,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}

@article{garriga2018deep,
  title={Deep convolutional networks as shallow gaussian processes},
  author={Garriga-Alonso, Adri{\`a} and Rasmussen, Carl Edward and Aitchison, Laurence},
  journal={arXiv preprint arXiv:1808.05587},
  year={2018}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8570--8581},
  year={2019}
}

@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}

@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@article{du2018gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}


@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}


@article{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1911.12360},
  year={2019}
}

@article{arora2019harnessing,
  title={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},
  author={Arora, Sanjeev and Du, Simon S and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
  journal={arXiv preprint arXiv:1910.01663},
  year={2019}
}

@inproceedings{cristianini2002kernel,
  title={On kernel-target alignment},
  author={Cristianini, Nello and Shawe-Taylor, John and Elisseeff, Andre and Kandola, Jaz S},
  booktitle={Advances in neural information processing systems},
  pages={367--373},
  year={2002}
}

@article{lanckriet2004learning,
  title={Learning the kernel matrix with semidefinite programming},
  author={Lanckriet, Gert RG and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I},
  journal={Journal of Machine learning research},
  volume={5},
  number={Jan},
  pages={27--72},
  year={2004}
}

@article{cortes2012l2,
  title={L2 regularization for learning kernels},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={arXiv preprint arXiv:1205.2653},
  year={2012}
}

@article{gonen2011multiple,
  title={Multiple kernel learning algorithms},
  author={G{\"o}nen, Mehmet and Alpaydin, Ethem},
  journal={Journal of machine learning research},
  volume={12},
  number={64},
  pages={2211--2268},
  year={2011}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@inproceedings{he2019local,
  title={The Local Elasticity of Neural Networks},
  author={He, Hangfeng and Su, Weijie J.},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{krizhevsky2009learning,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Krizhevsky, A},
  journal={Masterâ€™s thesis, Department of Computer Science, University of Toronto},
  year={2009}
}

@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://github.com/google/neural-tangents}
}

@article{ailon2009fast,
  title={The fast Johnson--Lindenstrauss transform and approximate nearest neighbors},
  author={Ailon, Nir and Chazelle, Bernard},
  journal={SIAM Journal on computing},
  volume={39},
  number={1},
  pages={302--322},
  year={2009},
  publisher={SIAM}
}

@article{fort2019stiffness,
  title={Stiffness: A new perspective on generalization in neural networks},
  author={Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Jastrzebski, Stanislaw and Narayanan, Srini},
  journal={arXiv preprint arXiv:1901.09491},
  year={2019}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={ICLR},
  year={2015}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1321--1330},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9709--9721},
  year={2019}
}

@inproceedings{allen2019can,
  title={What Can ResNet Learn Efficiently, Going Beyond Kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9015--9025},
  year={2019}
}

@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}


@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  year={2018}
}


@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10836--10846},
  year={2019}
}

@article{chen2020generalized,
      title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks}, 
      author={Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
      year={2020},
      journal={arXiv preprint arXiv:2002.04026},
}