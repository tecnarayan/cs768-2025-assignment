\begin{thebibliography}{10}

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622, 2015.

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{burt2019rates}
David Burt, Carl~Edward Rasmussen, and Mark Van Der~Wilk.
\newblock Rates of convergence for sparse variational gaussian process
  regression.
\newblock In {\em International Conference on Machine Learning}, pages
  862--871. PMLR, 2019.

\bibitem{chen2014stochastic}
Tianqi Chen, Emily Fox, and Carlos Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In {\em International Conference on Machine Learning}, pages
  1683--1691. PMLR, 2014.

\bibitem{cortes2010impact}
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
\newblock On the impact of kernel approximation on learning accuracy.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 113--120. JMLR Workshop and Conference Proceedings, 2010.

\bibitem{daxberger2021laplace}
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
  Matthias Bauer, and Philipp Hennig.
\newblock Laplace redux-effortless bayesian deep learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{daxberger2021bayesian}
Erik Daxberger, Eric Nalisnick, James~U Allingham, Javier Antor{\'a}n, and
  Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Bayesian deep learning via subnetwork inference.
\newblock In {\em International Conference on Machine Learning}, pages
  2510--2521. PMLR, 2021.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2009.

\bibitem{deng2022neuralef}
Zhijie Deng, Jiaxin Shi, and Jun Zhu.
\newblock Neuralef: Deconstructing kernels by deep neural networks.
\newblock {\em arXiv preprint arXiv:2205.00165}, 2022.

\bibitem{deng2020bayesadapter}
Zhijie Deng, Hao Zhang, Xiao Yang, Yinpeng Dong, and Jun Zhu.
\newblock Bayesadapter: Being bayesian, inexpensively and reliably, via
  bayesian fine-tuning.
\newblock {\em arXiv preprint arXiv:2010.01979}, 2020.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{drineas2005nystrom}
Petros Drineas, Michael~W Mahoney, and Nello Cristianini.
\newblock On the nystr{\"o}m method for approximating a gram matrix for
  improved kernel-based learning.
\newblock {\em Journal of Machine Learning Research}, 6(12), 2005.

\bibitem{eschenhagen2021mixtures}
Runa Eschenhagen, Erik Daxberger, Philipp Hennig, and Agustinus Kristiadi.
\newblock Mixtures of laplace approximations for improved post-hoc uncertainty
  in deep learning.
\newblock {\em arXiv preprint arXiv:2111.03577}, 2021.

\bibitem{foong2019between}
Andrew~YK Foong, Yingzhen Li, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, and
  Richard~E Turner.
\newblock 'in-between'uncertainty in bayesian neural networks.
\newblock {\em arXiv preprint arXiv:1906.11537}, 2019.

\bibitem{francis2021major}
Deena~P Francis and Kumudha Raimond.
\newblock Major advancements in kernel function approximation.
\newblock {\em Artificial Intelligence Review}, 54(2):843--876, 2021.

\bibitem{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2348--2356, 2011.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock {\em arXiv preprint arXiv:1706.04599}, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}, 2019.

\bibitem{hernandez2015probabilistic}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato and Ryan Adams.
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1869, 2015.

\bibitem{hinton1993keeping}
Geoffrey Hinton and Drew Van~Camp.
\newblock Keeping neural networks simple by minimizing the description length
  of the weights.
\newblock In {\em ACM Conference on Computational Learning Theory}, 1993.

\bibitem{immer2021improving}
Alexander Immer, Maciej Korzepa, and Matthias Bauer.
\newblock Improving predictions of bayesian neural nets via local
  linearization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 703--711. PMLR, 2021.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International Conference on Machine Learning}, pages
  448--456. PMLR, 2015.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:1806.07572}, 2018.

\bibitem{jang2017empirical}
Huisu Jang and Jaewook Lee.
\newblock An empirical study on modeling and prediction of bitcoin prices with
  bayesian neural networks based on blockchain information.
\newblock {\em Ieee Access}, 6:5427--5437, 2017.

\bibitem{jin2013improved}
Rong Jin, Tianbao Yang, Mehrdad Mahdavi, Yu-Feng Li, and Zhi-Hua Zhou.
\newblock Improved bounds for the nystr{\"o}m method with application to kernel
  classification.
\newblock {\em IEEE Transactions on Information Theory}, 59(10):6939--6949,
  2013.

\bibitem{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in {B}ayesian deep learning for
  computer vision?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5574--5584, 2017.

\bibitem{khan2019approximate}
Mohammad~Emtiyaz Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa.
\newblock Approximate inference turns deep networks into gaussian processes.
\newblock {\em arXiv preprint arXiv:1906.01930}, 2019.

\bibitem{khan2018fast}
Mohammad~Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and
  Akash Srivastava.
\newblock Fast and scalable {B}ayesian deep learning by weight-perturbation in
  adam.
\newblock In {\em International Conference on Machine Learning}, pages
  2616--2625, 2018.

\bibitem{kristiadi2020being}
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
\newblock Being bayesian, even just a bit, fixes overconfidence in relu
  networks.
\newblock {\em arXiv preprint arXiv:2002.10118}, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6402--6413, 2017.

\bibitem{lawrence2001variational}
Neil~David Lawrence.
\newblock {\em Variational inference in probabilistic models}.
\newblock PhD thesis, Citeseer, 2001.

\bibitem{leibig2017leveraging}
Christian Leibig, Vaneeda Allken, Murat~Se{\c{c}}kin Ayhan, Philipp Berens, and
  Siegfried Wahl.
\newblock Leveraging uncertainty information from deep neural networks for
  disease detection.
\newblock {\em Scientific Reports}, 7(1):1--14, 2017.

\bibitem{liu2016stein}
Qiang Liu and Dilin Wang.
\newblock Stein variational gradient descent: A general purpose {B}ayesian
  inference algorithm.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2378--2386, 2016.

\bibitem{louizos2016structured}
Christos Louizos and Max Welling.
\newblock Structured and efficient variational deep learning with matrix
  gaussian posteriors.
\newblock In {\em International Conference on Machine Learning}, pages
  1708--1716, 2016.

\bibitem{mackay1992practical}
David~JC MacKay.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock {\em Neural Computation}, 4(3):448--472, 1992.

\bibitem{mackay1992bayesian}
David John~Cameron Mackay.
\newblock {\em Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, 1992.

\bibitem{maddox2019simple}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13153--13164, 2019.

\bibitem{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In {\em International Conference on Machine Learning}, pages
  2408--2417. PMLR, 2015.

\bibitem{masegosa2020learning}
Andres Masegosa.
\newblock Learning under model misspecification: Applications to variational
  and ensemble methods.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5479--5491, 2020.

\bibitem{mercer1909functions}
J~Mercer.
\newblock Functions ofpositive and negativetypeand theircommection with the
  theory ofintegral equations.
\newblock {\em Philosophicol Trinsdictions of the Rogyal Society}, pages
  4--415, 1909.

\bibitem{munkhoeva2018quadrature}
Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, and Ivan Oseledets.
\newblock Quadrature-based features for kernel approximation.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{neal1995bayesian}
Radford~M Neal.
\newblock {\em {B}ayesian Learning for Neural Networks}.
\newblock PhD thesis, {U}niversity of {T}oronto, 1995.

\bibitem{nystrom1930praktische}
Evert~J Nystr{\"o}m.
\newblock {\"U}ber die praktische aufl{\"o}sung von integralgleichungen mit
  anwendungen auf randwertaufgaben.
\newblock {\em Acta Mathematica}, 54:185--204, 1930.

\bibitem{osawa2019practical}
Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard~E
  Turner, Rio Yokota, and Mohammad~Emtiyaz Khan.
\newblock Practical deep learning with {B}ayesian principles.
\newblock {\em arXiv preprint arXiv:1906.02506}, 2019.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8026--8037, 2019.

\bibitem{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock {\em Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem{rahimi2008weighted}
Ali Rahimi and Benjamin Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock {\em Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem{ritter2018scalable}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In {\em 6th International Conference on Learning Representations},
  volume~6. International Conference on Representation Learning, 2018.

\bibitem{seeger2004gaussian}
Matthias Seeger.
\newblock Gaussian processes for machine learning.
\newblock {\em International journal of neural systems}, 14(02):69--106, 2004.

\bibitem{sun2017learning}
Shengyang Sun, Changyou Chen, and Lawrence Carin.
\newblock Learning structured weight uncertainty in {B}ayesian neural networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1283--1292, 2017.

\bibitem{titsias2009variational}
Michalis Titsias.
\newblock Variational learning of inducing variables in sparse gaussian
  processes.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 567--574. PMLR, 2009.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em International Conference on Machine Learning}, pages
  681--688, 2011.

\bibitem{wen2018flipout}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock {\em arXiv preprint arXiv:1803.04386}, 2018.

\bibitem{wild2021connections}
Veit Wild, Motonobu Kanagawa, and Dino Sejdinovic.
\newblock Connections and equivalences between the nystr$\backslash$" om method
  and sparse variational gaussian processes.
\newblock {\em arXiv preprint arXiv:2106.01121}, 2021.

\bibitem{williams2000using}
Christopher Williams and Matthias Seeger.
\newblock Using the nystr{\"o}m method to speed up kernel machines.
\newblock {\em Advances in Neural Information Processing Systems}, 13, 2000.

\bibitem{wilson2020bayesian}
Andrew~Gordon Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock {\em arXiv preprint arXiv:2002.08791}, 2020.

\bibitem{woodbury1950inverting}
Max~A Woodbury.
\newblock {\em Inverting modified matrices}.
\newblock Statistical Research Group, 1950.

\bibitem{yu2016orthogonal}
Felix Xinnan~X Yu, Ananda~Theertha Suresh, Krzysztof~M Choromanski, Daniel~N
  Holtmann-Rice, and Sanjiv Kumar.
\newblock Orthogonal random features.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{zhang2018noisy}
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse.
\newblock Noisy natural gradient as variational inference.
\newblock In {\em International Conference on Machine Learning}, pages
  5847--5856, 2018.

\bibitem{zhang2019cyclical}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock {\em arXiv preprint arXiv:1902.03932}, 2019.

\end{thebibliography}
