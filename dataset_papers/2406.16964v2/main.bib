@inproceedings{merrill2024language,
  title={Language Models Still Struggle to Zero-shot Reason about Time Series},
  author={Merrill, Mike A and Tan, Mingtian and Gupta, Vinayak and Hartvigsen, Thomas and Althoff, Tim},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{jin2024position,
  title={Position Paper: What Can Large Language Models Tell Us about Time Series Analysis},
  author={Jin, Ming and Zhang, Yifan and Chen, Wei and Zhang, Kexin and Liang, Yuxuan and Yang, Bin and Wang, Jindong and Pan, Shirui and Wen, Qingsong},
  booktitle={ICML},
  year={2024}
}

@inproceedings{cheng2024sociodojo,
  title={SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series},
  author={Cheng, Junyan and Chin, Peter},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={AAAI},
  year={2021}
}


#oneFitall
#OFA
@inproceedings{zhou2024one,
  title={One fits all: Power general time series analysis by pretrained lm},
  author={Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others},
  booktitle={NeurIPS},
  year={2023}
}

# Tempo
@inproceedings{cao2023tempo,
  title={Tempo: Prompt-based generative pre-trained transformer for time series forecasting},
  author={Cao, Defu and Jia, Furong and Arik, Sercan O and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
  booktitle={ICLR},
  year={2024}
}

# original LLaTA
@article{liu2024taming,
      title={CALF: Aligning LLMs for Time Series Forecasting via Cross-modal Fine-Tuning}, 
      author={Liu, Peiyuan and Guo, Hang and Dai, Tao and Li, Naiqi and Bao, Jigang and Ren, Xudong and Jiang, Yong and Xia, Shu-Tao},
      journal={arXiv preprint arXiv:2403.07300},
      year={2024},
      arxiv={2403.07300}
}

#RePro
@inproceedings{jin2023time,
  title={Time-llm: Time series forecasting by reprogramming large language models},
  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others},
  booktitle={ICLR},
  year={2024}
}


@article{wimmer2023leveraging,
  title={Leveraging vision-language models for granular market change prediction},
  author={Wimmer, Christopher and Rekabsaz, Navid},
  journal={arXiv preprint arXiv:2301.10166},
  year={2023}
}


@inproceedings{girdhar2023imagebind,
  title={Imagebind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{pan2024textbf,
  title={$S^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting},
  author={Pan, Zijie and Jiang, Yushan and Garg, Sahil and Schneider, Anderson and Nevmyvaka, Yuriy and Song, Dongjin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}


@article{xue2023promptcast,
  title={Promptcast: A new prompt-based learning paradigm for time series forecasting},
  author={Xue, Hao and Salim, Flora D},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

%NYU`s 
@inproceedings{gruver2024large,
  title={Large language models are zero-shot time series forecasters},
  author={Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew G},
  booktitle={NeurIPS},
  year={2023}
}


#Bert
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

#LLama-1 
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models (2023)},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

#GPT2
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

#TEST
@inproceedings{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  booktitle={ICLR},
  year={2024}
}

chang2024llm4ts

@article{chang2024llm4ts,
  title={Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms},
  author={Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu},
  journal={arXiv preprint arXiv:2308.08469},
  year={2023}
}

#10k  FITS 
@inproceedings{xu2024fits,
      title={FITS: Modeling Time Series with $10k$ Parameters}, 
      author={Zhijian Xu and Ailing Zeng and Qiang Xu},
      year={2024},
      booktitle={ICLR}
}

@article{liuLargeLanguageModels2023,
  title={Large language models are few-shot health learners},
  author={Liu, Xin and McDuff, Daniel and Kovacs, Geza and Galatzer-Levy, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  journal={arXiv preprint arXiv:2305.15525},
  year={2023}
}

#AAAI
@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={AAAI},
  year={2023}
}


#Datasets
@article{li2023revisiting,
  title={Revisiting long-term time series forecasting: An investigation on linear mapping},
  author={Li, Zhe and Qi, Shiyi and Li, Yiduo and Xu, Zenglin},
  journal={arXiv preprint arXiv:2305.10721},
  year={2023}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{zhou2022fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={ICML},
  year={2022}
}

@misc{misc_electricityloaddiagrams20112014_321,
  author       = {Trindade,Artur},
  title        = {{ElectricityLoadDiagrams20112014}},
  year         = {2015},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C58C86}
}

intorduciton 
@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}

@article{bose2017probabilistic,
  title={Probabilistic demand forecasting at scale},
  author={B{\"o}se, Joos-Hendrik and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Lange, Dustin and Salinas, David and Schelter, Sebastian and Seeger, Matthias and Wang, Yuyang},
  journal={VLDB},
  year={2017}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@article{sezer2020financial,
  title={Financial time series forecasting with deep learning: A systematic literature review: 2005--2019},
  author={Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
  journal={Applied soft computing},
  volume={90},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{rasul2024lagllama,
  title={Lag-llama: Towards foundation models for time series forecasting},
  author={Rasul, Kashif and Ashok, Arjun and Williams, Andrew Robert and Khorasani, Arian and Adamopoulos, George and Bhagwatkar, Rishika and Bilo{\v{s}}, Marin and Ghonia, Hena and Hassen, Nadhir and Schneider, Anderson and others},
  booktitle={R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
  year={2023}
}


@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@inproceedings{Yuqietal-2023-PatchTST,
  title     = {A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  author    = {Nie, Yuqi and H. Nguyen, Nam and Sinthong, Phanwadee and Kalagnanam, Jayant},
  booktitle = {ICLR},
  year      = {2023}
}

@inproceedings{lee2023learning,
  title={Learning to Embed Time Series Patches Independently},
  author={Lee, Seunghan and Park, Taeyoung and Lee, Kibok},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{LIFT,
title={Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators},
author={Lifan Zhao and Yanyan Shen},
booktitle={ICLR},
year={2024}
}

@article{talukder2023time,
  title={Time Series Modeling at Scale: A Universal Representation Across Tasks and Domains},
  author={Talukder, Sabera J and Gkioxari, Georgia},
  year={2023}
}

@inproceedings{liu2023itransformer,
  title={{iTransformer}: Inverted Transformers Are Effective for Time Series Forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  booktitle={ICLR},
  year={2024}
}

@article{borsos2023audiolm,
  title={Audiolm: a language modeling approach to audio generation},
  author={Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2023},
  publisher={IEEE}
}


@article{jorke2024supporting,
  title={Supporting Physical Activity Behavior Change with LLM-Based Conversational Agents},
  author={J{\"o}rke, Matthew and Sapkota, Shardul and Warkenthien, Lyndsea and Vainio, Niklas and Schmiedmayer, Paul and Brunskill, Emma and Landay, James},
  journal={arXiv preprint arXiv:2405.06061},
  year={2024}
}

@article{moridTimeSeriesPrediction2023,
  author = {Morid, Mohammad Amin and Sheng, Olivia R. Liu and Dunbar, Joseph},
  title = {Time Series Prediction Using Deep Learning Methods in Healthcare},
  year = {2023},
  volume = {14},
  number = {1},
  journal = {ACM Trans. Manage. Inf. Syst.},
  articleno = {2},
  numpages = {29}
}

@article{ansari2024chronos,
  title={Chronos: Learning the language of time series},
  author={Abdul Fatir Ansari and Lorenzo Stella and Caner Turkmen and Xiyuan Zhang and Pedro Mercado and Huibin Shen and Oleksandr Shchur and Syama Sundar Rangapuram and Sebastian Pineda Arango and Shubham Kapoor and Jasper Zschiegner and Danielle C. Maddix and Hao Wang and Michael W. Mahoney and Kari Torkkola and Andrew Gordon Wilson and Michael Bohlke-Schneider and Yuyang Wang},
  journal={arXiv preprint arXiv:2403.07815},
  year={2024}
}

@InProceedings{godahewa2021monash,
    author={Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I. and Hyndman, Rob J. and Montero-Manso, Pablo},
    title={Monash Time Series Forecasting Archive},
    booktitle={NeurIPS},
    year={2021}
}

@inproceedings{lai2018modeling,
  title={Modeling long-and short-term temporal patterns with deep neural networks},
  author={Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  booktitle={SIGIR},
  year={2018}
}


@article{das2023decoder,
  title={A decoder-only foundation model for time-series forecasting},
  author={Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
 booktitle={ICML},
  year={2024}
}

@article{woo2024unified,
  title={Unified Training of Universal Time Series Forecasting Transformers},
  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
   booktitle={ICML},
  year={2024}
}

@misc{beyondforecast,
      title={Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution}, 
      author={Wen Ye and Yizhou Zhang and Wei Yang and Lumingyuan Tang and Defu Cao and Jie Cai and Yan Liu},
      year={2024},
      eprint={2410.04047},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.04047}, 
}

@inproceedings{wang2024news,
   title={From News to Forecast: Iterative Event Reasoning in LLM-Based Time Series Forecasting},
   author={Wang, Xinlei and Feng, Maike and Qiu, Jing and Gu, Jinjin and Zhao, Junhua},
   booktitle={Neural Information Processing Systems},
   year={2024}
}


@article{chow2024towards,
  title={Towards Time Series Reasoning with LLMs},
  author={Chow, Winnie and Gardiner, Lauren and Hallgr{\'\i}msson, Haraldur T and Xu, Maxwell A and Ren, Shirley You},
  journal={arXiv preprint arXiv:2409.11376},
  year={2024}
}


@article{wang2024stocktime,
  title={StockTime: A Time Series Specialized Large Language Model Architecture for Stock Price Prediction},
  author={Wang, Shengkun and Ji, Taoran and Wang, Linhan and Sun, Yanshen and Liu, Shang-Ching and Kumar, Amit and Lu, Chang-Tien},
  journal={arXiv preprint arXiv:2409.08281},
  year={2024}
}

@inproceedings{li2024reflective,
  title={A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading},
  author={Li, Yuan and Luo, Bingqiao and Wang, Qian and Chen, Nuo and Liu, Xu and He, Bingsheng},
  booktitle={EMNLP},
  year={2024}
}

@article{xu2024beyond,
  title={Beyond Trend and Periodicity: Guiding Time Series Forecasting with Textual Cues},
  author={Xu, Zhijian and Bian, Yuxuan and Zhong, Jianyuan and Wen, Xiangyu and Xu, Qiang},
  journal={arXiv preprint arXiv:2405.13522},
  year={2024}
}

@inproceedings{wu2023timesnet,
  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
  booktitle={International Conference on Learning Representations},
  year={2023},
}