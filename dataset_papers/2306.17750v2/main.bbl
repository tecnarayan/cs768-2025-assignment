\begin{thebibliography}{10}

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em {N}ature}, 2015.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em Nature}, 2016.

\bibitem{tsitsiklis1996analysis}
John Tsitsiklis and Benjamin Van~Roy.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock {\em Advances in neural information processing systems}, 1996.

\bibitem{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em Machine Learning}. Elsevier, 1995.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton1988learning}
Richard~S Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Journal of Machine Learning Research}, 1988.

\bibitem{gordon1995stable}
Geoffrey~J Gordon.
\newblock Stable function approximation in dynamic programming.
\newblock In {\em Machine learning}. Elsevier, 1995.

\bibitem{lee2019target}
Donghwan Lee and Niao He.
\newblock Target-based temporal-difference learning.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{puterman2014markov}
Martin~L Puterman.
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem{maei2011gradient}
Hamid~Reza Maei.
\newblock {\em Gradient temporal-difference learning algorithms}.
\newblock PhD thesis, University of Alberta, 2011.

\bibitem{asadi2023resetting}
Kavosh Asadi, Rasool Fakoor, and Shoham Sabach.
\newblock Resetting the optimizer in deep {RL}: An empirical study.
\newblock {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{bas2021logistic}
Joan Bas-Serrano, Sebastian Curi, Andreas Krause, and Gergely Neu.
\newblock Logistic {Q}-learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem{precup1997exponentiated}
Doina Precup and Richard~S Sutton.
\newblock Exponentiated gradient methods for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 1997.

\bibitem{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 2005.

\bibitem{maei2010toward}
Hamid~Reza Maei, Csaba Szepesv{\'a}ri, Shalabh Bhatnagar, and Richard~S Sutton.
\newblock Toward off-policy learning control with function approximation.
\newblock {\em International Conference on Machine Learning}, 2010.

\bibitem{kolter2011fixed}
J~Kolter.
\newblock The fixed points of off-policy td.
\newblock {\em Advances in Neural Information Processing Systems}, 2011.

\bibitem{meyer2000matrix}
Carl~D Meyer.
\newblock {\em Matrix analysis and applied linear algebra}.
\newblock SIAM, 2000.

\bibitem{beck2017first}
Amir Beck.
\newblock {\em First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem{tibshirani1996reg}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, 1996.

\bibitem{loshchilov2019adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{amos2017input}
Brandon Amos, Lei Xu, and J~Zico Kolter.
\newblock Input convex neural networks.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{asadi2017alternative}
Kavosh Asadi and Michael~L Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In {\em Conference on learning theory}, 2018.

\bibitem{melo2008analysis}
Francisco~S Melo, Sean~P Meyn, and M~Isabel Ribeiro.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, 2008.

\bibitem{zou2019finite}
Shaofeng Zou, Tengyu Xu, and Yingbin Liang.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock {\em Advances in neural information processing systems}, 2019.

\bibitem{sutton2016emphatic}
Richard~S Sutton, A~Rupam Mahmood, and Martha White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock {\em Journal of Machine Learning Research}, 2016.

\bibitem{zhang2021breaking}
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson.
\newblock Breaking the deadly triad with a target network.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{diddigi2019convergent}
Raghuram~Bharadwaj Diddigi, Chandramouli Kamanchi, and Shalabh Bhatnagar.
\newblock A convergent off-policy temporal difference algorithm.
\newblock {\em arXiv}, 2019.

\bibitem{lim2022regularized}
Han-Dong Lim, Do~Wan Kim, and Donghwan Lee.
\newblock Regularized {Q}-learning.
\newblock {\em arXiv}, 2022.

\bibitem{ghiassian2020gradient}
Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and
  Martha White.
\newblock Gradient temporal-difference learning with regularized corrections.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{ghiassian2017first}
Sina Ghiassian, Banafsheh Rafiee, and Richard~S Sutton.
\newblock A first empirical study of emphatic temporal difference learning.
\newblock {\em arXiv}, 2017.

\bibitem{manek2022pitfalls}
Gaurav Manek and J~Zico Kolter.
\newblock The pitfalls of regularization in off-policy {TD} learning.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{cai2019neural}
Qi~Cai, Zhuoran Yang, Jason~D Lee, and Zhaoran Wang.
\newblock Neural temporal-difference learning converges to global optima.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{xiao2021understanding}
Chenjun Xiao, Bo~Dai, Jincheng Mei, Oscar~A Ramirez, Ramki Gummadi, Chris
  Harris, and Dale Schuurmans.
\newblock Understanding and leveraging overparameterization in recursive value
  estimation.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{ghosh2020representations}
Dibya Ghosh and Marc~G Bellemare.
\newblock Representations for stable off-policy reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{asadi2022faster}
Kavosh Asadi, Rasool Fakoor, Omer Gottesman, Taesup Kim, Michael Littman, and
  Alexander~J Smola.
\newblock Faster deep reinforcement learning with slower online network.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{lyle2021effect}
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney.
\newblock On the effect of auxiliary tasks on representation dynamics.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem{sutton2009fast}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em International Conference on Machine Learning}, 2009.

\bibitem{liu2012regularized}
Bo~Liu, Sridhar Mahadevan, and Ji~Liu.
\newblock Regularized off-policy {TD}-learning.
\newblock {\em Advances in Neural Information Processing Systems}, 2012.

\bibitem{liu2016proximal}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Proximal gradient temporal difference learning algorithms.
\newblock In {\em International Joint Conferences on Artificial Intelligence},
  2016.

\bibitem{liu2020finite}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient {TD} algorithms.
\newblock {\em arXiv}, 2020.

\bibitem{feng2019kernel}
Yihao Feng, Lihong Li, and Qiang Liu.
\newblock A kernel loss for solving the {B}ellman equation.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock {\em Journal of Machine Learning Research}, 2008.

\bibitem{dai2018sbeed}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and
  Le~Song.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{saleh2019deterministic}
Ehsan Saleh and Nan Jiang.
\newblock Deterministic {B}ellman residual minimization.
\newblock In {\em Proceedings of Optimization Foundations for Reinforcement
  Learning Workshop at NeurIPS}, 2019.

\bibitem{lagoudakis2003least}
Michail~G Lagoudakis and Ronald Parr.
\newblock Least-squares policy iteration.
\newblock {\em Journal of Machine Learning Research}, 2003.

\bibitem{lizotte2011convergent}
Daniel Lizotte.
\newblock Convergent fitted value iteration with linear function approximation.
\newblock {\em Advances in Neural Information Processing Systems}, 2011.

\bibitem{fan2020theoretical}
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang.
\newblock A theoretical analysis of deep {Q}-learning.
\newblock In {\em Learning for Dynamics and Control}, 2020.

\bibitem{munos2007performance}
R{\'e}mi Munos.
\newblock Performance bounds in l\_p-norm for approximate value iteration.
\newblock {\em SIAM journal on control and optimization}, 2007.

\bibitem{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 2008.

\bibitem{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock {\em Advances in Neural Information Processing Systems}, 2010.

\bibitem{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, 2020.

\bibitem{du2019provably}
Simon~S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient {Q}-learning with function approximation via
  distribution shift error checking oracle.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{du2020agnostic}
Simon~S Du, Jason~D Lee, Gaurav Mahajan, and Ruosong Wang.
\newblock Agnostic {Q}-learning with function approximation in deterministic
  systems: Near-optimal bounds on approximation error and sample complexity.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock {\em Advances in neural information processing systems}, 2020.

\bibitem{chen2020zap}
Shuhang Chen, Adithya~M Devraj, Fan Lu, Ana Busic, and Sean Meyn.
\newblock Zap q-learning with nonlinear function approximation.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{sutton2008convergent}
Richard~S Sutton, Hamid Maei, and Csaba Szepesv{\'a}ri.
\newblock A convergent $ o (n) $ temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock {\em Advances in neural information processing systems}, 2008.

\end{thebibliography}
