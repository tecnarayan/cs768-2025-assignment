@article{CM_1940__7__283_0,
     author = {L\'evy, Paul},
     title = {Sur certains processus stochastiques homog\`enes},
     journal = {Compositio Mathematica},
     pages = {283--339},
     publisher = {Johnson Reprint Corporation},
     volume = {7},
     year = {1940},
     zbl = {0022.05903},
     language = {fr},
     url = {http://www.numdam.org/item/CM_1940__7__283_0/}
}
@misc{zhu2021geometric,
      title={A Geometric Analysis of Neural Collapse with Unconstrained Features}, 
      author={Zhihui Zhu and Tianyu Ding and Jinxin Zhou and Xiao Li and Chong You and Jeremias Sulam and Qing Qu},
      year={2021},
      eprint={2105.02375},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

### Logistic loss
@misc{chizat2020implicit,
      title={Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss}, 
      author={Lenaic Chizat and Francis Bach},
      year={2020},
      eprint={2002.04486},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@misc{lyu2020gradient,
      title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks}, 
      author={Kaifeng Lyu and Jian Li},
      year={2020},
      eprint={1906.05890},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{jacot2022saddletosaddle,
      title={Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity}, 
      author={Arthur Jacot and François Ged and Berfin Şimşek and Clément Hongler and Franck Gabriel},
      year={2022},
      eprint={2106.15933},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{bhojanapalli2020lowrank,
      title={Low-Rank Bottleneck in Multi-head Attention Models}, 
      author={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
      year={2020},
      eprint={2002.07028},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{jacot2023implicit,
      title={Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions}, 
      author={Arthur Jacot},
      year={2023},
      eprint={2209.15055},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{galanti2023characterizing,
      title={Characterizing the Implicit Bias of Regularized SGD in Rank Minimization}, 
      author={Tomer Galanti and Zachary S. Siegel and Aparna Gupte and Tomaso Poggio},
      year={2023},
      eprint={2206.05794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{wang2023implicit,
      title={Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank}, 
      author={Zihan Wang and Arthur Jacot},
      year={2023},
      eprint={2305.16038},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{NEURIPS2021_e22cb9d6,
 author = {Dai, Zhen and Karzand, Mina and Srebro, Nathan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26884--26896},
 publisher = {Curran Associates, Inc.},
 title = {Representation Costs of Linear Neural Networks: Analysis and Design},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e22cb9d6bbb4c290a94e4fff4d68a831-Paper.pdf},
 volume = {34},
 year = {2021}
}
@misc{candes2009power,
      title={The Power of Convex Relaxation: Near-Optimal Matrix Completion}, 
      author={Emmanuel J. Candes and Terence Tao},
      year={2009},
      eprint={0903.1476},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}
@article{Sun_2016,
   title={Guaranteed Matrix Completion via Non-Convex Factorization},
   volume={62},
   ISSN={1557-9654},
   url={http://dx.doi.org/10.1109/TIT.2016.2598574},
   DOI={10.1109/tit.2016.2598574},
   number={11},
   journal={IEEE Transactions on Information Theory},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Sun, Ruoyu and Luo, Zhi-Quan},
   year={2016},
   month=nov, pages={6535–6579} }

##### until here related work
@article{tibshirani2021equivalences,
  title={Equivalences between sparse models and neural networks},
  author={Tibshirani, Ryan J},
  journal={Working Notes. URL https://www. stat. cmu. edu/ryantibs/papers/sparsitynn. pdf},
  year={2021}
}
@article{powers1970free,
  title={Free states of the canonical anticommutation relations},
  author={Powers, Robert T and St{\o}rmer, Erling},
  journal={Communications in Mathematical Physics},
  volume={16},
  number={1},
  pages={1--33},
  year={1970},
  publisher={Springer}
}

@article{HU2021218,
title = {Low Rank Regularization: A review},
journal = {Neural Networks},
volume = {136},
pages = {218-232},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S089360802030352X},
author = {Zhanxuan Hu and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Low rank, Regularization, Optimization},
abstract = {Low Rank Regularization (LRR), in essence, involves introducing a low rank or approximately low rank assumption to target we aim to learn, which has achieved great success in many data analysis tasks. Over the last decade, much progress has been made in theories and applications. Nevertheless, the intersection between these two lines is rare. In order to construct a bridge between practical applications and theoretical studies, in this paper we provide a comprehensive survey for LRR. Specifically, we first review the recent advances in two issues that all LRR models are faced with: (1) rank-norm relaxation, which seeks to find a relaxation to replace the rank minimization problem; (2) model optimization, which seeks to use an efficient optimization algorithm to solve the relaxed LRR models. For the first issue, we provide a detailed summarization for various relaxation functions and conclude that the non-convex relaxations can alleviate the punishment bias problem compared with the convex relaxations. For the second issue, we summarize the representative optimization algorithms used in previous studies, and analyze their advantages and disadvantages. As the main goal of this paper is to promote the application of non-convex relaxations, we conduct extensive experiments to compare different relaxation functions. The experimental results demonstrate that the non-convex relaxations generally provide a large advantage over the convex relaxations. Such a result is inspiring for further improving the performance of existing LRR models.}
}

@misc{wu2020visual,
      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, 
      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
      year={2020},
      eprint={2006.03677},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@misc{irandoust2022training,
      title={Training a Vision Transformer from scratch in less than 24 hours with 1 GPU}, 
      author={Saghar Irandoust and Thibaut Durand and Yunduz Rakhmangulova and Wenjie Zi and Hossein Hajimirsadeghi},
      year={2022},
      eprint={2211.05187},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@InProceedings{rank_srebro,
author="Srebro, Nathan
and Shraibman, Adi",
editor="Auer, Peter
and Meir, Ron",
title="Rank, Trace-Norm and Max-Norm",
booktitle="Learning Theory",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="545--560",
abstract="We study the rank, trace-norm and max-norm as complexity measures of matrices, focusing on the problem of fitting a matrix with matrices having low complexity. We present generalization error bounds for predicting unobserved entries that are based on these measures. We also consider the possible relations between these measures. We show gaps between them, and bounds on the extent of such gaps.",
isbn="978-3-540-31892-7"
}

@misc{khodak2022initialization,
      title={Initialization and Regularization of Factorized Neural Layers}, 
      author={Mikhail Khodak and Neil Tenenholtz and Lester Mackey and Nicolò Fusi},
      year={2022},
      eprint={2105.01029},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{sharma2023truth,
      title={The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction}, 
      author={Pratyusha Sharma and Jordan T. Ash and Dipendra Misra},
      year={2023},
      eprint={2312.13558},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@misc{vanlaarhoven2017l2,
      title={L2 Regularization versus Batch and Weight Normalization}, 
      author={Twan van Laarhoven},
      year={2017},
      eprint={1706.05350},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{10.1145/3446776,
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3446776},
doi = {10.1145/3446776},
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.We interpret our experimental findings by comparison with traditional models.We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
journal = {Commun. ACM},
month = {feb},
pages = {107–115},
numpages = {9}
}



@misc{phuong2022formal,
      title={Formal Algorithms for Transformers}, 
      author={Mary Phuong and Marcus Hutter},
      year={2022},
      eprint={2207.09238},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
ziyin2022exact,
title={Exact Solutions of a Deep Linear Network},
author={Liu Ziyin and Botao Li and Xiangming Meng},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=X6bp8ri8dV}
}

@misc{gunasekar2017implicit,
      title={Implicit Regularization in Matrix Factorization}, 
      author={Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
      year={2017},
      eprint={1705.09280},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{razin2020implicit,
      title={Implicit Regularization in Deep Learning May Not Be Explainable by Norms}, 
      author={Noam Razin and Nadav Cohen},
      year={2020},
      eprint={2005.06398},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
li2021towards,
title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
author={Zhiyuan Li and Yuping Luo and Kaifeng Lyu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=AHOs7Sm5H7R}
}
@misc{poli2023hyena,
      title={Hyena Hierarchy: Towards Larger Convolutional Language Models}, 
      author={Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Ré},
      year={2023},
      eprint={2302.10866},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fu2023hungry,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-1905-13655,
  author       = {Sanjeev Arora and
                  Nadav Cohen and
                  Wei Hu and
                  Yuping Luo},
  title        = {Implicit Regularization in Deep Matrix Factorization},
  journal      = {CoRR},
  volume       = {abs/1905.13655},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.13655},
  eprinttype    = {arXiv},
  eprint       = {1905.13655},
  timestamp    = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-13655.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v202-ziyin23a,
  title = 	 {spred: Solving L1 Penalty with {SGD}},
  author =       {Ziyin, Liu and Wang, Zihao},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {43407--43422},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
}


@misc{andriushchenko2023need,
      title={Why Do We Need Weight Decay in Modern Deep Learning?}, 
      author={Maksym Andriushchenko and Francesco D'Angelo and Aditya Varre and Nicolas Flammarion},
      year={2023},
      eprint={2310.04415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Krogh1991ASW,
  title={A Simple Weight Decay Can Improve Generalization},
  author={Anders Krogh and John A. Hertz},
  booktitle={Neural Information Processing Systems},
  year={1991},
  url={https://api.semanticscholar.org/CorpusID:10137788}
}

@article{Mackay,
author = {David J C Mackay},
title = {Probable networks and plausible predictions — a review of practical Bayesian methods for supervised neural networks},
journal = {Network: Computation in Neural Systems},
volume = {6},
number = {3},
pages = {469-505},
year = {1995},
publisher = {Taylor & Francis},
}

@inproceedings{
xie2023on,
title={On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective},
author={Zeke Xie and zhiqiang xu and Jingzhao Zhang and Issei Sato and Masashi Sugiyama},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=vnGcubtzR1}
}

@inproceedings{
zhang2018three,
title={Three Mechanisms of Weight Decay Regularization},
author={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1lz-3Rct7},
}

@article{lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{radford2019language,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  title = {Language models are unsupervised multitask learners},
  year = 2019
}


@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}

@article{nadaraya1964estimating,
  title={On estimating regression},
  author={Nadaraya, Elizbar A},
  journal={Theory of Probability \& its Applications},
  volume={9},
  number={1},
  pages={141--142},
  year={1964}
}

@article{watson1964smooth,
  title={Smooth regression analysis},
  author={Watson, Geoffrey S},
  journal={Sankhy{\=a}: The Indian Journal of Statistics, Series A},
  pages={359--372},
  year={1964}
}

@inproceedings{bertinetto_meta-learning_2019,
	title = {Meta-learning with differentiable closed-form solvers},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Bertinetto, Luca and Henriques, João F. and Torr, Philip H. S. and Vedaldi, Andrea},
	year = {2019}
}


@article{bai_deep_2019,
	title = {Deep equilibrium models},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2019},
}

@inproceedings{amos_optnet_2017,
  title={Optnet: Differentiable optimization as a layer in neural networks},
  author={Amos, Brandon and Kolter, J. Zico},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@inproceedings{hinton_fast_weights,
  author    = {Jimmy Ba and Geoffrey E. Hinton and Volodymyr Mnih and Joel Z. Leibo and Catalin Ionescu},
  title     = {Using Fast Weights to Attend to the Recent Past},
  booktitle = {Advances in Neural Information Processing Systems 29},
  year      = {2016}
}

@article{zucchet_beyond_2022,
	title = {Beyond backpropagation: bilevel optimization through implicit differentiation and equilibrium propagation},
	volume = {34},
	number = {12},
	journal = {Neural Computation},
	author = {Zucchet, Nicolas and Sacramento, João},
	month = dec,
	year = {2022}
}

@inproceedings{madan2021fast,
  title={Fast and slow learning of recurrent independent mechanisms},
  author={Madan, Kanika and Ke, Rosemary Nan and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{
goyal2021recurrent,
title={Recurrent independent mechanisms},
author={Anirudh Goyal and Alex Lamb and Jordan Hoffmann and Shagun Sodhani and Sergey Levine and Yoshua Bengio and Bernhard Sch{\"o}lkopf},
booktitle={International Conference on Learning Representations},
year={2021}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996}
}

@article{aljundi2019online,
  title={Online continual learning with maximally interfered retrieval},
  author={Aljundi, Rahaf and Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Lin, Min and Charlin, Laurent and Tuytelaars, Tinne},
  journal={arXiv preprint arXiv:1908.04742},
  year={2019}
}

@article{kirkpatrick2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	number = {13},
	urldate = {2019-05-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	year = {2017},
	pages = {3521--3526}
}

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  year={2015}
}



@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex},
	year = {2009},
}

@inproceedings{sutton1992adapting,
  title={Adapting bias by gradient descent: An incremental version of delta-bar-delta},
  author={Sutton, Richard S.},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={1992}
}

@INPROCEEDINGS{schraudolph1999,
  author={Schraudolph, Nicol N.},
  booktitle={International Conference on Artificial Neural Networks}, 
  title={Local gain adaptation in stochastic gradient descent}, 
  year={1999}
}

@InProceedings{vivek2017,
author="Veeriah, Vivek
and Zhang, Shangtong
and Sutton, Richard S.",
title="Crossprop: Learning representations by stochastic meta-gradient descent in neural networks",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2017"
}

@inproceedings{jacobsen2019meta,
  title={Meta-descent for online, continual prediction},
  author={Jacobsen, Andrew and Schlegel, Matthew and Linke, Cameron and Degris, Thomas and White, Adam and White, Martha},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019}
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	volume = {405},
	abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded1,2,3,4. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
	number = {6789},
	journal = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	year = {2000},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary},
	pages = {947--951},
}

@article{kearney2019learning,
  title={Learning feature relevance through step size adaptation in temporal-difference learning},
  author={Kearney, Alex and Veeriah, Vivek and Travnik, Jaden and Pilarski, Patrick M and Sutton, Richard S},
  journal={arXiv preprint arXiv:1903.03252},
  year={2019}
}

@article{french_catastrophic_1999,
	title = {Catastrophic forgetting in connectionist networks},
	volume = {3},
	number = {4},
	journal = {Trends in Cognitive Sciences},
	author = {French, Robert M.},
	month = apr,
	year = {1999},
	pages = {128--135}
}

@misc{singh2023transient,
      title={The Transient Nature of Emergent In-Context Learning in Transformers}, 
      author={Aaditya K. Singh and Stephanie C. Y. Chan and Ted Moskovitz and Erin Grant and Andrew M. Saxe and Felix Hill},
      year={2023},
      eprint={2311.08360},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{vonoswald2023uncovering,
      title={Uncovering mesa-optimization algorithms in Transformers}, 
      author={Johannes von Oswald and Eyvind Niklasson and Maximilian Schlegel and Seijin Kobayashi and Nicolas Zucchet and Nino Scherrer and Nolan Miller and Mark Sandler and Blaise Agüera y Arcas and Max Vladymyrov and Razvan Pascanu and João Sacramento},
      year={2023},
      eprint={2309.05858},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{gao_pile_2020,
	title = {The pile: an {800GB} dataset of diverse text for language modeling},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	journal = {arXiv preprint arXiv:2101.00027},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@article{lecun_mnist_1998,
	title = {The {MNIST} database of handwritten digits},
	journal = {Available at http://yann. lecun. com/exdb/mnist},
	author = {LeCun, Yann},
	year = {1998}
}

@inproceedings{lake_one_2011,
	title = {One shot learning of simple visual concepts},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B.},
	year = {2011}
}
@INPROCEEDINGS{meta_opt_net,
  author={Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Meta-Learning With Differentiable Convex Optimization}, 
  year={2019}
 }
  
@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a novel image dataset for benchmarking machine learning algorithms},
	journal = {arXiv preprint arXiv:1708.07747},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	year = {2017}
}

@inproceedings{ren2018meta,
  title={Meta-learning for semi-supervised few-shot classification},
  author={Ren, Mengye and Triantafillou, Eleni and Ravi, Sachin and Snell, Jake and Swersky, Kevin and Tenenbaum, Joshua B. and Larochelle, Hugo and Zemel, Richard S},
  booktitle={{International Conference on Learning Representations}},
  year={2018}
}

@inproceedings{KrauseStarkDengFei-Fei_3DRR2013,
  title = {{3D} Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year = {2013},
  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}

@techreport{welinder2010caltech,
  title={{Caltech-UCSD birds 200}},
  author={Welinder, Peter and Branson, Steve and Mita, Takeshi and Wah, Catherine and Schroff, Florian and Belongie, Serge and Perona, Pietro},
  year={2010},
  institution={CNS-TR-2010-001, California Institute of Technology}
}


%%%%


%%% Code

@misc{lamamlcode,
  title = {Official {La-MAML} repository},
  author = {Gupta, Gunshi and Yadav, Karmesh and Paull, Liam},
  year = 2020,
  howpublished = {Downloaded from: \url{https://github.com/montrealrobotics/La-MAML/blob/main/model/lamaml.py}},
  note = {Accessed: 2020-05-25}
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: an imperative style, high-performance deep learning library},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019}
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: {A} {2D} graphics environment},
	volume = {9},
	abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, J. D.},
	year = {2007},
	pages = {90--95},
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
}
%%%%

@inproceedings{dhillon2019baseline,
  title={A baseline for few-shot image classification},
  author={Dhillon, Guneet S and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano},
  booktitle={{International Conference on Learning Representations}},
  year={2020}
}


@inproceedings{DBLP:conf/eccv/TianWKTI20,
  author    = {Yonglong Tian and
               Yue Wang and
               Dilip Krishnan and
               Joshua B. Tenenbaum and
               Phillip Isola},
  title     = {Rethinking Few-Shot Image Classification: {A} Good Embedding is All
               You Need?},
  booktitle = {16th European Conference on Computer Vision},
  year      = {2020}
}


@article{zucchet2021contrastive,
      title={A contrastive rule for meta-learning}, 
      journal = {arXiv preprint arXiv:2104.01677},
      author={Nicolas Zucchet and Simon Schug and Johannes von Oswald and Dominic Zhao and João Sacramento},
      year={2021}
}

@inproceedings{wu2018understanding,
  title={Understanding short-horizon bias in stochastic meta-optimization},
  author={Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger},
  booktitle={{International Conference on Learning Representations}},
  year={2018}
}


@incollection{Bottou98on-linelearning,
    author = {Léon Bottou},
    title = {On-line learning and stochastic approximations},
    booktitle = {On-line Learning in Neural Networks},
    year = {1998},
    pages = {9--42},
    publisher = {Cambridge University Press}
}

@article{park201965,
  title={A 65-nm neuromorphic image classification processor with energy-efficient training through direct spike-only feedback},
  author={Park, Jeongwoo and Lee, Juyun and Jeon, Dongsuk},
  journal={IEEE Journal of Solid-State Circuits},
  volume={55},
  number={1},
  pages={108--119},
  year={2019}
}


@phdthesis{shalev2007,
	title = {Online learning: theory, algorithms, and applications},
	school = {Hebrew University of Jerusalem},
	author = {Shalev-Shwartz, Shai},
	year = {2007}
}

@inproceedings{ritter2018been,
  title={Been there, done that: Meta-learning with episodic recall},
  author={Ritter, Samuel and Wang, Jane and Kurth-Nelson, Zeb and Jayakumar, Siddhant and Blundell, Charles and Pascanu, Razvan and Botvinick, Matthew},
  booktitle={{International Conference on Machine Learning}},
  year={2018}
}


@article{robins1995catastrophic,
  title={Catastrophic forgetting, rehearsal and pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995}
}


@inproceedings{chen2019closer,
  title={A closer look at few-shot classification},
  author={Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin},
  booktitle={{International Conference on Learning Representations}},
  year={2019}
}

@article{hadsell2020embracing,
  title={Embracing Change: Continual Learning in Deep Neural Networks},
  author={Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A and Pascanu, Razvan},
  journal={{Trends in Cognitive Sciences}},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  booktitle={{Advances in Neural Information Processing Systems}},
  year={2017}
}

@article{li2020energy,
  title={Energy efficient synaptic plasticity},
  author={Li, Ho Ling and van Rossum, Mark C.W.},
  journal={eLife},
  volume={9},
  pages={e50804},
  year={2020}
}

@inproceedings{oh2021boil,
  title={{BOIL}: Towards representation change for few-shot learning},
  author={Oh, Jaehoon and Yoo, Hyungjun and Kim, ChangHwan and Yun, Se-Young},
  booktitle={{International Conference of Learning Representations}},
  year={2021}
}
@inproceedings{DBLP:conf/eccv/GuoCKCSSRF20,
  author    = {Yunhui Guo and
               Noel Codella and
               Leonid Karlinsky and
               James V. Codella and
               John R. Smith and
               Kate Saenko and
               Tajana Rosing and
               Rog{\'{e}}rio Feris},
  editor    = {Andrea Vedaldi and
               Horst Bischof and
               Thomas Brox and
               Jan{-}Michael Frahm},
  title     = {A Broader Study of Cross-Domain Few-Shot Learning},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
               UK, August 23-28, 2020, Proceedings, Part {XXVII}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12372},
  pages     = {124--141},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58583-9\_8},
  doi       = {10.1007/978-3-030-58583-9\_8},
  timestamp = {Mon, 23 Nov 2020 08:37:01 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/GuoCKCSSRF20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/abs-1909-05557,
  author    = {Chen, Yutian and
               Friesen, Abram L. and
               Behbahani, Feryal and
               Budden, David and
               Hoffman, Matthew W. and
               Doucet, Arnaud and
               de Freitas, Nando},
  title     = {Modular Meta-Learning with Shrinkage},
  booktitle   = {Advances in Neural Information Processing Systems},
  year      = {2020},
}

@article{wandell2009plasticity,
  title={Plasticity and stability of visual field maps in adult primary visual cortex},
  author={Wandell, Brian A and Smirnakis, Stelios M},
  journal={Nature Reviews Neuroscience},
  volume={10},
  number={12},
  pages={873--884},
  year={2009}
}

@article{lohmann2014developmental,
  title={The developmental stages of synaptic plasticity},
  author={Lohmann, Christian and Kessels, Helmut W},
  journal={The Journal of Physiology},
  volume={592},
  number={1},
  pages={13--31},
  year={2014}
}


@inproceedings{zhao_meta_learning_hypernetworks,
	title = {Meta-Learning via Hypernetworks},
	urldate = {2021-02-15},
	author = {Zhao, Dominic and Kobayashi, Seijin and Sacramento, João and von Oswald, Johannes},
	booktitle = {{NeurIPS Workshop on Meta-Learning}},
	year = {2020}
}

@inproceedings{raghu_svcca_2017,
	title = {{SVCCA}: {Singular} Vector Canonical Correlation Analysis for {Deep} Learning Dynamics and Interpretability},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	booktitle = {{Advances in Neural Information Processing Systems}},
	year = {2017}
}

@article{brock_freezeout_2017,
	title = {{FreezeOut}: Accelerate Training by Progressively Freezing Layers},
	journal = {arXiv preprint arXiv:1706.04983},
	author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
	year = {2017}
}

@article{he2019task,
  title={Task agnostic continual learning via meta learning},
  author={He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1906.05201},
  year={2019}
}

@inproceedings{lindsey2020learning,
  title={Learning to Learn with Feedback and Local Plasticity},
  author={Lindsey, Jack and Litwin-Kumar, Ashok},
  booktitle={{Advances in Neural Information Processing Systems}},
  year={2020}
}

@inproceedings{javed2019meta,
  title={Meta-learning representations for continual learning},
  author={Javed, Khurram and White, Martha},
  booktitle={{Advances in Neural Information Processing Systems}},
  year={2019}
}

@inproceedings{gupta_-maml_2020,
	title = {La-{MAML}: {Look}-ahead meta learning for continual learning},
	booktitle = {{Advances in Neural Information Processing Systems}},
	author = {Gupta, Gunshi and Yadav, Karmesh and Paull, Liam},
	year = {2020}
}

@inproceedings{
dai2023why,
title={Why Can {GPT} Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers},
author={Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Shuming Ma and Zhifang Sui and Furu Wei},
booktitle={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
year={2023},
url={https://openreview.net/forum?id=fzbHRjAd8U}
}

@inproceedings{riemer2019learning,
  title={Learning to learn without forgetting by maximizing transfer and minimizing interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  booktitle={{International Conference on Learning Representations}},
  year={2019}
}

@article{caccia2020online,
  title={Online Fast Adaptation and Knowledge Accumulation ({OSAKA}): a New Approach to Continual Learning},
  author={Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Page-Caccia, Lucas and Laradji, Issam Hadj and Rish, Irina and Lacoste, Alexandre and V{\'a}zquez, David and Charlin, Laurent},
  journal={{Advances in Neural Information Processing Systems}},
  year={2020}
}

@article{beaulieu2020learning,
  title={Learning to continually learn},
  author={Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff and Cheney, Nick},
  journal={arXiv preprint arXiv:2002.09571},
  year={2020}
}

@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@inproceedings{he_delving_2015,
	title = {Delving deep into rectifiers: surpassing human-level performance on {ImageNet} classification},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	booktitle = {{IEEE International Conference on Computer Vision}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015}
}


@article{lewkowycz_large_2020,
	title = {The large learning rate phase of deep learning: the catapult mechanism},
	shorttitle = {The large learning rate phase of deep learning},
	journal = {arXiv preprint arXiv:2003.02218},
	author = {Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
	year = {2020}
}

@article{nichol_first-order_2018,
	title = {On First-Order Meta-Learning Algorithms},
	journal = {arXiv preprint arXiv:1803.02999},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	year = {2018}
}



@inproceedings{kingma_adam_2015,
	title = {Adam: a method for stochastic optimization},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2015},
}

@inproceedings{ramanujan_whats_2020,
	title = {What's Hidden in a Randomly Weighted Neural Network?},
	booktitle = {{IEEE Conference on Computer Vision and Pattern Recognition}},
	author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
	year = {2020}
}

@inproceedings{bernacchia_meta-learning_2021,
	title = {Meta-learning with negative learning rates},
	booktitle = {International Conference on Learning Representations},
	author = {Bernacchia, Alberto},
	year = {2021}
}

@article{bengio_estimating_2013,
	title = {Estimating or propagating Gradients Through Stochastic Neurons for Conditional Computation},
	journal = {arXiv preprint arXiv:1308.3432},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	year = {2013}
}


@article{shaban_truncated_2019,
	title = {Truncated {Back}-propagation for {Bilevel} {Optimization}},
	url = {http://arxiv.org/abs/1810.10667},
	abstract = {Bilevel optimization has been recently revisited for designing and analyzing algorithms in hyperparameter tuning and meta learning tasks. However, due to its nested structure, evaluating exact gradients for high-dimensional problems is computationally challenging. One heuristic to circumvent this difficulty is to use the approximate gradient given by performing truncated back-propagation through the iterative optimization procedure that solves the lower-level problem. Although promising empirical performance has been reported, its theoretical properties are still unclear. In this paper, we analyze the properties of this family of approximate gradients and establish sufficient conditions for convergence. We validate this on several hyperparameter tuning and meta learning tasks. We find that optimization with the approximate gradient computed using few-step back-propagation often performs comparably to optimization with the exact gradient, while requiring far less memory and half the computation time.},
	urldate = {2021-02-25},
	journal = {arXiv:1810.10667 [cs, stat]},
	author = {Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
	month = apr,
	year = {2019},
	note = {arXiv: 1810.10667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@inproceedings{rajeswaran_meta-learning_2019,
	title = {Meta-{Learning} with {Implicit} {Gradients}},
	booktitle = {{Advances in Neural Information Processing Systems}},
	author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
	year = {2019},
}


@inproceedings{raghu_rapid_2020,
	title = {Rapid Learning or Feature Reuse? {Towards} Understanding the Effectiveness of {MAML}},
	booktitle = {{International Conference on Learning Representations}},
	author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
	year = {2020}
}

@inproceedings{ioffe_batch_2015,
	title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
	booktitle = {{International Conference on Machine Learning}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
}

@misc{
charton2022linear,
title={Linear algebra with transformers},
author={Francois Charton},
year={2022},
url={https://openreview.net/forum?id=L2a_bcarHcF}
}

@inproceedings{tasknorm,
	title = {TaskNorm: Rethinking Batch Normalization for Meta-Learning},
	booktitle = {{International Conference on Machine Learning}},
	author = {John Bronskill and Jonathan Gordon and James Requeima and Sebastian Nowozin and Richard E. Turner},
	year = {2020},
}


@inproceedings{vinyals_matching_2017,
	title = {Matching Networks for One Shot Learning},
	booktitle = {{Advances in Neural Information Processing Systems}},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	year = {2016}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@article{irie2021going,
  publtype={informal},
  author={Kazuki Irie and Imanol Schlag and Róbert Csordás and Jürgen Schmidhuber},
  title={Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
  year={2021},
  cdate={1609459200000},
  journal={CoRR},
  volume={abs/2106.06295},
  url={https://arxiv.org/abs/2106.06295}
}






@inproceedings{lee2018gradient,
  title={Gradient-based meta-learning with learned layerwise metric and subspace},
  author={Lee, Yoonho and Choi, Seungjin},
	booktitle = {International {Conference} on {Machine} {Learning}},  year={2018}
}

@inproceedings{ravi_2016,
	title = {Optimization as a model for few-shot learning},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author={Ravi, Sachin and Larochelle, Hugo},
	year = {2017}
}



@article{schmidhuber_learning_1992,
	title = {Learning to {Control} {Fast}-{Weight} {Memories}: {An} {Alternative} to {Dynamic} {Recurrent} {Networks}},
	volume = {4},
	abstract = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
	number = {1},
	journal = {Neural Computation},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {1992},
	pages = {131--139},
	file = {Schmidhuber - 1992 - Learning to Control Fast-Weight Memories An Alter.pdf:/home/js/Zotero/storage/U5R9MG5P/Schmidhuber - 1992 - Learning to Control Fast-Weight Memories An Alter.pdf:application/pdf;Snapshot:/home/js/Zotero/storage/CC4RTVAS/neco.1992.4.1.html:text/html}
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning p...},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year = {2017}
}

@inproceedings{ha_hypernetworks_2017,
	title = {Hypernetworks},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ha, David and Dai, Andrew M. and Le, Quoc V.},
	year = {2017},
	file = {Ha et al. - 2017 - Hypernetworks.pdf:/home/js/Zotero/storage/CYAJCXJZ/Ha et al. - 2017 - Hypernetworks.pdf:application/pdf}
}

@article{krueger_bayesian_2017,
	title = {Bayesian hypernetworks},
	journal = {arXiv preprint arXiv:1710.04759},
	author = {Krueger, David and Huang, Chin-Wei and Islam, Riashat and Turner, Ryan and Lacoste, Alexandre and Courville, Aaron},
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence}
}

@inproceedings{von_oswald_continual_2020,
	title = {Continual learning with hypernetworks},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {von Oswald, Johannes and Henning, Christian and Grewe, Benjamin F.  and Sacramento, João },
	year = {2020}
}

@inproceedings{dwaracherla_hypermodels_2020,
	title = {Hypermodels for exploration},
	abstract = {We study the use of hypermodels to represent epistemic uncertainty and guide exploration.
  This generalizes and extends the use of ensembles to approximate Thompson sampling. The computational cost...},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dwaracherla, Vikranth and Lu, Xiuyuan and Ibrahimi, Morteza and Osband, Ian and Wen, Zheng and Roy, Benjamin Van},
	year = {2020},
	file = {Full Text PDF:/home/js/Zotero/storage/9S9FTJUB/Dwaracherla et al. - 2019 - Hypermodels for Exploration.pdf:application/pdf;Snapshot:/home/js/Zotero/storage/2KU4VH7F/forum.html:text/html}
}

inproceedings{rusu_meta-learning_2019,
	title = {Meta-learning with latent embedding optimization},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Rusu, Andrei A. and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
	year = {2019}
}

@inproceedings{savarese2019learning,
      title={Learning Implicitly Recurrent {CNNs} Through Parameter Sharing}, 
      author={Pedro Savarese and Michael Maire},
      year={2019},
      booktitle={International Conference on Learning Representations}
}

@InProceedings{pmlr-v97-zintgraf19a, title = {Fast Context Adaptation via Meta-Learning}, author = {Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon}, booktitle = {{International Conference on Machine Learning}}, year = {2019}}

@inproceedings{DBLP:conf/nips/ParkO19,
  author    = {Eunbyung Park and
               Junier B. Oliva},
  title     = {Meta-Curvature},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019}
}

@inproceedings{zhang2019which,
	title={Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model},
	author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger},
	booktitle={Advances in Neural Information Processing Systems 32},
	pages={8194--8205},
	year={2019}
}

@misc{song2020generalized,
      title={Generalized Adaptation for Few-Shot Learning}, 
      author={Liang Song and Jinlu Liu and Yongqiang Qin},
      year={2020},
      eprint={1911.10807},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{DBLP:conf/iclr/DhillonCRS20,
  author    = {Guneet Singh Dhillon and
               Pratik Chaudhari and
               Avinash Ravichandran and
               Stefano Soatto},
  title     = {A Baseline for Few-Shot Image Classification},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@article{DBLP:journals/corr/abs-2007-02933,
  author    = {Allan Zhou and
               Tom Knowles and
               Chelsea Finn},
  title     = {Meta-Learning Symmetries by Reparameterization},
  journal   = {CoRR},
  volume    = {abs/2007.02933},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.02933},
  archivePrefix = {arXiv},
  eprint    = {2007.02933},
  timestamp = {Sat, 18 Jul 2020 18:41:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-02933.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2003-11539,
  author    = {Yonglong Tian and
               Yue Wang and
               Dilip Krishnan and
               Joshua B. Tenenbaum and
               Phillip Isola},
  title     = {Rethinking Few-Shot Image Classification: a Good Embedding Is All
               You Need?},
  journal   = {arXiv preprint arXiv:2003.11539},
  year      = {2020}
}

@inproceedings{DBLP:conf/icml/ZintgrafSKHW19,
  author    = {Luisa M. Zintgraf and
               Kyriacos Shiarlis and
               Vitaly Kurin and
               Katja Hofmann and
               Shimon Whiteson},
  title     = {Fast Context Adaptation via Meta-Learning},
  booktitle = {International Conference on Machine Learning},
  volume    = {97},
  pages     = {7693--7702},
  year      = {2019}
}

@inproceedings{DBLP:conf/iclr/ChenLKWH19,
  author    = {Wei{-}Yu Chen and
               Yen{-}Cheng Liu and
               Zsolt Kira and
               Yu{-}Chiang Frank Wang and
               Jia{-}Bin Huang},
  title     = {A Closer Look at Few-shot Classification},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=HkxLXnAcFQ},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChenLKWH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/JacotHG18,
  author    = {Arthur Jacot and
               Cl{\'{e}}ment Hongler and
               Franck Gabriel},
  title     = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages     = {8580--8589},
  year      = {2018}
}

@inproceedings{DBLP:conf/iclr/GordonBBNT19,
title={Meta-Learning Probabilistic Inference for Prediction},
author={Jonathan Gordon and John Bronskill and Matthias Bauer and Sebastian Nowozin and Richard Turner},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HkxStoC5F7},
}

@article{DBLP:journals/corr/abs-1803-02999,
  author    = {Alex Nichol and
               Joshua Achiam and
               John Schulman},
  title     = {On First-Order Meta-Learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1803.02999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02999},
  archivePrefix = {arXiv},
  eprint    = {1803.02999},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/cvpr/QiaoLSY18,
  author    = {Siyuan Qiao and
               Chenxi Liu and
               Wei Shen and
               Alan L. Yuille},
  title     = {Few-Shot Image Recognition by Predicting Parameters From Activations},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {7229--7238},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Qiao\_Few-Shot\_Image\_Recognition\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00755},
  timestamp = {Fri, 13 Dec 2019 17:13:56 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/QiaoLSY18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LiZCL17,
  author    = {Zhenguo Li and
               Fengwei Zhou and
               Fei Chen and
               Hang Li},
  title     = {{Meta-SGD}: Learning to Learn Quickly for Few Shot Learning},
  journal   = {arXiv preprint arXiv:1707.09835},
  year      = {2017},
  archivePrefix = {arXiv}
}
@inproceedings{DBLP:conf/cvpr/QiaoLSY18,
  author    = {Siyuan Qiao and
               Chenxi Liu and
               Wei Shen and
               Alan L. Yuille},
  title     = {Few-Shot Image Recognition by Predicting Parameters From Activations},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {7229--7238},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Qiao\_Few-Shot\_Image\_Recognition\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00755},
  timestamp = {Fri, 13 Dec 2019 17:13:56 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/QiaoLSY18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/AntoniouES19,
  author    = {Antreas Antoniou and
               Harrison Edwards and
               Amos J. Storkey},
  title     = {How to train your {MAML}},
  booktitle = {International Conference on Learning Representations},
  year      = {2019}
}

@inproceedings{flennerhag2020metalearning,
      title={Meta-Learning with Warped Gradient Descent}, 
      author={Sebastian Flennerhag and Andrei A. Rusu and Razvan Pascanu and Francesco Visin and Hujun Yin and Raia Hadsell},
      year={2020},
      booktitle={{International Conference on Learning Representations}}}
}

@INPROCEEDINGS{Hinton87usingfast,
    author = {Geoffrey E. Hinton and David C. Plaut},
    title = {Using Fast Weights to Deblur Old Memories},
    booktitle = {Proceedings of the 9th Annual Conference of the Cognitive Science Society},
    year = {1987},
    pages = {177--186},
    publisher = {Erlbaum}
}

@inproceedings{von_oswald_learning_2021,
	title = {Learning where to learn: {Gradient} sparsity in meta and continual learning},
	abstract = {Finding neural network weights that generalize well from small datasets is difficult. A promising approach is to learn a weight initialization such that a small number of weight changes results in low generalization error. We show that this form of meta-learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. We find that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. Moreover, we find that sparse learning also emerges in a more expressive model where learning rates are meta-learned. Our results shed light on an ongoing debate on whether meta-learning can discover adaptable features and suggest that learning by sparse gradient descent is a powerful inductive bias for meta-learning systems.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {von Oswald, Johannes and Zhao, Dominic and Kobayashi, Seijin and Schug, Simon and Caccia, Massimo and Zucchet, Nicolas and Sacramento, João},
	year = {2021},
	file = {Full Text PDF:/home/js/Zotero/storage/Z2Z7WMVJ/von Oswald et al. - 2021 - Learning where to learn Gradient sparsity in meta.pdf:application/pdf},
}

@article{Malsburg,
author = {von der Malsburg, Christoph},
year = {1994},
month = {01},
pages = {},
title = {The Correlation Theory of Brain Function},
volume = {2},
journal = {Models Neural Netw.}
}

@inproceedings{
Jayakumar2020Multiplicative,
title={Multiplicative Interactions and Where to Find Them},
author={Siddhant M. Jayakumar and Wojciech M. Czarnecki and Jacob Menick and Jonathan Schwarz and Jack Rae and Simon Osindero and Yee Whye Teh and Tim Harley and Razvan Pascanu},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{perez2017film,
      title={FiLM: Visual Reasoning with a General Conditioning Layer}, 
      author={Ethan Perez and Florian Strub and Harm de Vries and Vincent Dumoulin and Aaron Courville},
      year={2017},
      journal={arXiv preprint arXiv:1709.07871},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{rebuffi2018efficient,
      title={Efficient parametrization of multi-domain deep neural networks}, 
      author={Sylvestre-Alvise Rebuffi and Hakan Bilen and Andrea Vedaldi},
      year={2018},
      journal={arXiv preprint arXiv:1803.10082},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{rusu2019metalearning,
      title={Meta-Learning with Latent Embedding Optimization}, 
      author={Andrei A. Rusu and Dushyant Rao and Jakub Sygnowski and Oriol Vinyals and Razvan Pascanu and Simon Osindero and Raia Hadsell},
      year={2019},
      booktitle = {International Conference on Learning Representations},
}
@inproceedings{DBLP:conf/iclr/RaviL17,
  author    = {Sachin Ravi and
               Hugo Larochelle},
  title     = {Optimization as a Model for Few-Shot Learning},
  booktitle = {International Conference on Learning Representations},
  year      = {2017}
}
@inproceedings{DBLP:conf/nips/VinyalsBLKW16,
  author    = {Oriol Vinyals and
               Charles Blundell and
               Tim Lillicrap and
               Koray Kavukcuoglu and
               Daan Wierstra},
  title     = {Matching Networks for One Shot Learning},
  booktitle = {Advances in Neural Information Processing Systems 29},
  pages     = {3630--3638},
  year      = {2016}
}

@article{reptile,
author = {Nichol, Alex and Schulman, John},
year = {2018},
journal = {arXiv preprint arXiv:1803.02999},
pages = {},
title = {Reptile: a Scalable Metalearning Algorithm}
}


@misc{requeima2020fast,
      title={Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes}, 
      author={James Requeima and Jonathan Gordon and John Bronskill and Sebastian Nowozin and Richard E. Turner},
      year={2020},
      eprint={1906.07697},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{deleu2019torchmeta,
  title={Torchmeta: A Meta-Learning library for {PyTorch}},
  author={Deleu, Tristan and W\"urfl, Tobias and Samiei, Mandana and Cohen, Joseph Paul and Bengio, Yoshua},
  year={2019},
  journal={arXiv preprint arXiv:1909.06576},
}

@article{DBLP:journals/corr/abs-2010-00373,
  author    = {Chen Zeno and
               Itay Golan and
               Elad Hoffer and
               Daniel Soudry},
  title     = {Task Agnostic Continual Learning Using Online Variational Bayes with
               Fixed-Point Updates},
  journal   = {arXiv preprint arXiv:010.00373},
  year      = {2020}
}


@article{caccia2021special,
  title={SPeCiaL: Self-Supervised Pretraining for Continual Learning},
  author={Caccia, Lucas and Pineau, Joelle},
  journal={arXiv preprint arXiv:2106.09065},
  year={2021}
}

@article{Douillard2021ContinuumSM,
  title={Continuum: Simple Management of Complex Continual Learning Scenarios},
  author={Arthur Douillard and Timoth{\'e}e Lesort},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.06253}
}

@misc{lesort2021continual,
      title={Continual Learning in Deep Networks: an Analysis of the Last Layer}, 
      author={Timothée Lesort and Thomas George and Irina Rish},
      year={2021},
      eprint={2106.01834},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{khetarpal2020continual,
      title={Towards Continual Reinforcement Learning: A Review and Perspectives}, 
      author={Khimya Khetarpal and Matthew Riemer and Irina Rish and Doina Precup},
      year={2020},
      eprint={2012.13490},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{lomonaco2021avalanche,
    title={Avalanche: an End-to-End Library for Continual Learning},
    author={Vincenzo Lomonaco and Lorenzo Pellegrini and Andrea Cossu and Antonio Carta and Gabriele Graffieti and Tyler L. Hayes and Matthias De Lange and Marc Masana and Jary Pomponi and Gido van de Ven and Martin Mundt and Qi She and Keiland Cooper and Jeremy Forest and Eden Belouadah and Simone Calderara and German I. Parisi and Fabio Cuzzolin and Andreas Tolias and Simone Scardapane and Luca Antiga and Subutai Amhad and Adrian Popescu and Christopher Kanan and Joost van de Weijer and Tinne Tuytelaars and Davide Bacciu and Davide Maltoni},
    booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},
    series={2nd Continual Learning in Computer Vision Workshop},
    year={2021}
}

@article{Lomonaco2020CVPR2C,
  title={CVPR 2020 Continual Learning in Computer Vision Competition: Approaches, Results, Current Challenges and Future Directions},
  author={Vincenzo Lomonaco and Lorenzo Pellegrini and Pau Rodr{\'i}guez and Massimo Caccia and Qi She and Yu Chen and Quentin Jodelet and Ruiping Wang and Zheda Mai and David V{\'a}zquez and German Ignacio Parisi and Nikhil Churamani and Marc Pickett and Issam H. Laradji and Davide Maltoni},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.09929}
}

@misc{mundt2021clevacompass,
      title={CLEVA-Compass: A Continual Learning EValuation Assessment Compass to Promote Research Transparency and Comparability}, 
      author={Martin Mundt and Steven Lang and Quentin Delfosse and Kristian Kersting},
      year={2021},
      eprint={2110.03331},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{normandin2021sequoia,
      title={Sequoia: A Software Framework to Unify Continual Learning Research}, 
      author={Fabrice Normandin and Florian Golemo and Oleksiy Ostapenko and Pau Rodriguez and Matthew D Riemer and Julio Hurtado and Khimya Khetarpal and Dominic Zhao and Ryan Lindeborg and Timothée Lesort and Laurent Charlin and Irina Rish and Massimo Caccia},
      year={2021},
      eprint={2108.01005},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mundt2020wholistic,
      title={A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning}, 
      author={Martin Mundt and Yong Won Hong and Iuliia Pliushch and Visvanathan Ramesh},
      year={2020},
      eprint={2009.01797},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DeLange2019ContinualLA,
  title={Continual learning: A comparative study on how to defy forgetting in classification tasks},
  author={Matthias De Lange and Rahaf Aljundi and Marc Masana and Sarah Parisot and Xu Jia and Alevs Leonardis and Gregory G. Slabaugh and Tinne Tuytelaars},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08383}
}

@article{caccia2021reducing,
  title={Reducing Representation Drift in Online Continual Learning},
  author={Caccia, Lucas and Aljundi, Rahaf and Tuytelaars, Tinne and Pineau, Joelle and Belilovsky, Eugene},
  journal={arXiv preprint arXiv:2104.05025},
  year={2021}
}

@article{induction_heads,
journal={arXiv preprint arXiv:2209.11895},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},

  title = {In-context Learning and Induction Heads},
  year = {2022},

}


@inproceedings{
simple_case_study,
title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=flNZJ2eOet}
}


@article{data_dis_in_context,
  journal = {Advances in Neural Information Processing Systems},
  
  author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
 
  title = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  year = {2022},
}



@article{https://doi.org/10.48550/arxiv.2209.11208,
  journal = {arXiv preprint arXiv:2209.11208},

  author = {Harrison, James and Metz, Luke and Sohl-Dickstein, Jascha},
  
  title = {A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases},

  year = {2022},
  
}

@article{mikulasch_dendritic_2022,
	title = {Dendritic predictive coding: {A} theory of cortical computation with spiking neurons},
	journal = {arXiv preprint arXiv:2205.05303},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Wibral, Michael and Priesemann, Viola},
	year = {2022},
}

@misc{transformers,
  journal = {arXiv preprint arXiv:1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  title = {Attention Is All You Need},
  year = {2017},
  
}


@article{transformers_few_shot,

  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},

  title = {Language Models are Few-Shot Learners},
 journal={arXiv preprint arXiv:2005.14165},
  publisher = {arXiv},
  
  year = {2020},

}

@inproceedings{
transformers_vision,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}


@inproceedings{NEURIPS2019_9d63484a,
 author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 title = {Graph Transformer Networks},
 year = {2019}
}

@InProceedings{https://doi.org/10.48550/arxiv.2005.12872,
author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
title={End-to-End Object Detection with Transformers},
booktitle={Computer Vision -- ECCV 2020},
year={2020},
publisher={Springer International Publishing},
}


@misc{image_trans,
  doi = {10.48550/ARXIV.1802.05751},
  
  url = {https://arxiv.org/abs/1802.05751},
  
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Łukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Image Transformer},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{convormer,

  journal = {arXiv preprint arXiv:2005.08100},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},

  title = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  
  year = {2020},
  
}


@article{pre_train_prompt,
  journal = {arXiv preprint arXiv:2107.13586},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  
  title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  year = {2021}
}


@software{haiku2020github,
  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
  title = {{H}aiku: {S}onnet for {JAX}},
  url = {http://github.com/deepmind/dm-haiku},
  version = {0.0.3},
  year = {2020},
}

@article{mikulasch_dendritic_2022,
	title = {Dendritic predictive coding: {A} theory of cortical computation with spiking neurons},
	journal = {arXiv preprint arXiv:2205.05303},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Wibral, Michael and Priesemann, Viola},
	year = {2022},
}

@article{Trans_generalize_differently_weights,

  author = {Chan, Stephanie C. Y. and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K. and Hill, Felix},
  title = {Transformers generalize differently from information stored in context vs in weights},
  
  journal = {arXiv preprint arXiv:2210.05675},
  year = {2022},
  
}

@inproceedings{linear_transformers_fast_weight,
  author={Imanol Schlag and Kazuki Irie and Jürgen Schmidhuber},
  title={Linear Transformers Are Secretly Fast Weight Programmers},
  year={2021},
  booktitle={ICML},
}


@ARTICLE{fast_weights,
  author={Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks}, 
  year={1992},
  volume={4},
  number={1},
  pages={131-139},
  doi={10.1162/neco.1992.4.1.131}}
  
  @inproceedings{Hinton1987UsingFW,
  title={Using fast weights to deblur old memories},
  author={Geoffrey E. Hinton and David C. Plaut},
  year={1987}
}

 @inproceedings{widrow:switching,
  added-at = {2008-02-26T11:58:58.000+0100},
  address = {New York},
  author = {Widrow, Bernard and Hoff, Marcian E.},
  biburl = {https://www.bibsonomy.org/bibtex/24c3b6ae932deb6bb1d04ad76c9c94a69/schaul},
  booktitle = {1960 {IRE} {WESCON} Convention Record, Part 4},
  citeulike-article-id = {2379772},
  comment = {Reprinted in \npcite{anderson:neurocomputing}},
  description = {idsia},
  interhash = {2ace34c5debd5abc08e714f8ff1030b3},
  intrahash = {4c3b6ae932deb6bb1d04ad76c9c94a69},
  keywords = {nn},
  pages = {96--104},
  priority = {2},
  publisher = {{IRE}},
  timestamp = {2008-02-26T12:01:40.000+0100},
  title = {Adaptive Switching Circuits},
  year = 1960
}

@inproceedings{
kirsch2021meta,
title={Meta Learning Backpropagation And Improving It},
author={Louis Kirsch and J{\"u}rgen Schmidhuber},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=hhU9TEvB6AF}
}

@inbook{delta_rule,
author = {Widrow, Bernard and Hoff, Marcian E.},
title = {Adaptive Switching Circuits},
year = {1988},
isbn = {0262010976},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Neurocomputing: Foundations of Research},
pages = {123–134},
numpages = {12}
}


@misc{exact_slutions,
  doi = {10.48550/ARXIV.1312.6120},
  
  url = {https://arxiv.org/abs/1312.6120},
  
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Disordered Systems and Neural Networks (cond-mat.dis-nn), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Biological sciences, FOS: Biological sciences},
  
  title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}


@article{gould_deep_2021,
	title = {Deep declarative networks},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gould, Stephen and Hartley, Richard and Campbell, Dylan John},
	year = {2021},
	file = {Gould et al. - 2021 - Deep declarative networks.pdf:/Users/nicolas/Zotero/storage/B58CRS5N/Gould et al. - 2021 - Deep declarative networks.pdf:application/pdf},
}

@misc{kolter_deep_2021,
	title = {Deep implicit layers - neural {ODEs}, deep equilibirum models, and beyond},
	url = {http://implicit-layers-tutorial.org},
	author = {Kolter, Zico and Duvenaud, David and Johnson, Matt},
	year = {2021},
}

@inproceedings{chen_neural_2018,
	title = {Neural ordinary differential equations},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
	year = {2018},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/3M7RAZ68/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:application/pdf},
}


@article{bai_multiscale_2020,
	title = {Multiscale deep equilibrium models},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	year = {2020},
}

@article{bai_deep_2019_v2,
	title = {Deep equilibrium models},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2019},
}


@misc{layer_norm,
  doi = {10.48550/ARXIV.1607.06450},
  
  url = {https://arxiv.org/abs/1607.06450},
  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Layer Normalization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{hopfield,
  journal = {arXiv preprint arXiv:2008.02217},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  title = {Hopfield Networks is All You Need},
  year = {2020},
}

@book{thrun_learning_1998,
	title = {Learning to learn},
	publisher = {Springer US},
	author = {Thrun, Sebastian and Pratt, Lorien},
	year = {1998},
}

@techreport{bengio_learning_1990,
	title = {Learning a synaptic learning rule},
	institution = {Université de Montréal, Département d'Informatique et de Recherche opérationnelle},
	author = {Bengio, Yoshua and Bengio, Samy and Cloutier, Jocelyn},
	year = {1990},
}

@phdthesis{schmidhuber_evolutionary_1987,
	type = {Diploma thesis},
	title = {Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
	school = {Institut für Informatik, Technische Universität München},
	author = {Schmidhuber, Jürgen},
	year = {1987},
}

@incollection{chalmers_evolution_1991,
	title = {The evolution of learning: an experiment in genetic connectionism},
	abstract = {This paper explores how an evolutionary process can produce systems that learn. A general framework for the evolution of learning is outlined, and is applied to the task of evolving mechanisms suitable for supervised learning in single-layer neural networks. Dynamic properties of a network's information-processing capacity are encoded genetically, and these properties are subjected to selective pressure based on their success in producing adaptive behavior in diverse environments. As a result of selection and genetic recombination, various successful learning mechanisms evolve, including the well-known delta rule. The effect of environmental diversity on the evolution of learning is investigated, and the role of different kinds of emergent phenomena in genetic and connectionist systems is discussed.},
	language = {en},
	booktitle = {Connectionist {Models}},
	publisher = {Morgan Kaufmann},
	author = {Chalmers, David J.},
	editor = {Touretzky, David S. and Elman, Jeffrey L. and Sejnowski, Terrence J. and Hinton, Geoffrey E.},
	year = {1991},
	pages = {81--90},
	file = {ScienceDirect Full Text PDF:/home/js/Zotero/storage/FNPQ4LRM/Chalmers - 1991 - The Evolution of Learning An Experiment in Geneti.pdf:application/pdf;ScienceDirect Snapshot:/home/js/Zotero/storage/6LTWIKN8/B9781483214481500147.html:text/html},
}

@InProceedings{meta_hochreiter,
author="Hochreiter, Sepp
and Younger, A. Steven
and Conwell, Peter R.",
editor="Dorffner, Georg
and Bischof, Horst
and Hornik, Kurt",
title="Learning to Learn Using Gradient Descent",
booktitle="Artificial Neural Networks --- ICANN 2001",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="87--94",
isbn="978-3-540-44668-2"
}


@inproceedings{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/js/Zotero/storage/XE9EMJMK/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf:application/pdf;arXiv.org Snapshot:/home/js/Zotero/storage/SAL9JD4R/1606.html:text/html},
}

@inproceedings{
finn2018metalearning_universal,
title={Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm},
author={Chelsea Finn and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HyjC5yWCW},
}

@article{grocking,
  author       = {Alethea Power and
                  Yuri Burda and
                  Harrison Edwards and
                  Igor Babuschkin and
                  Vedant Misra},
  title        = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
 
  volume       = {abs/2201.02177},
  year         = {2022},

  eprinttype    = {arXiv},
}

@inproceedings{
related_work,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0g0X4H8yN4I}
}


@article{JMLR:v22:20-302,
  author  = {Jorge  Pérez and Pablo Barceló and Javier Marinkovic},
  title   = {Attention is Turing-Complete},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {75},
  pages   = {1--35},
  url     = {http://jmlr.org/papers/v22/20-302.html}
}

@article{mikulasch_dendritic_2022,
	title = {Dendritic predictive coding: {A} theory of cortical computation with spiking neurons},
	journal = {arXiv preprint arXiv:2205.05303},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Wibral, Michael and Priesemann, Viola},
	year = {2022},
}

@article{permute1,
  author = {Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},
  
  title = {The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},
  journal = {arXiv preprint arXiv:2110.06296},
  year = {2021},
}



@article{liao_bridging_2016,
	title = {Bridging the gaps between residual learning, recurrent neural networks and visual cortex},
	journal = {arXiv preprint arXiv:1604.03640},
	author = {Liao, Qianli and Poggio, Tomaso},
	year = {2016},
}

@article{permute2,
	title = {Random initialisations performing above chance and how to find them},
	journal = {OPT2022: 14th Annual Workshop on Optimization for Machine Learning},
	author = {Benzing, Frederik and Schug, Simon and Meier, Robert and von Oswald, Johannes and Akram, Yassir and Zucchet, Nicolas and Aitchison, Laurence and Steger, Angelika},
	year = {2022},
}


@article{gelu,
  journal = {arXiv preprint arXiv:1606.08415},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  title = {Gaussian Error Linear Units (GELUs)},
  year = {2016},

}


@misc{adam,

  journal = {arXiv preprint arXiv:1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  title = {Adam: A Method for Stochastic Optimization},
 year = {2014},
}


@inproceedings{
kirsch2022generalpurpose,
title={General-Purpose In-Context Learning by Meta-Learning Transformers},
author={Louis Kirsch and James Harrison and Jascha Sohl-Dickstein and Luke Metz},
booktitle={Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=t6tA-KB4dO}
}


@InProceedings{hypertransformer,
  title = 	 {{H}yper{T}ransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning},
  author =       {Zhmoginov, Andrey and Sandler, Mark and Vladymyrov, Maksym},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27075--27098},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhmoginov22a/zhmoginov22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhmoginov22a.html},
  abstract = 	 {In this work we propose a HyperTransformer, a Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity Transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable.}
}

@inproceedings{
vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}