\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Schuurmans, and
  Norouzi]{agarwal2019striving}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock Striving for simplicity in off-policy deep reinforcement learning.
\newblock \emph{CoRR}, abs/1907.04543, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.04543}.

\bibitem[Ahmed et~al.(2021)Ahmed, Awais, and Kittler]{ahmed2021sit}
Sara Atito~Ali Ahmed, Muhammad Awais, and Josef Kittler.
\newblock Sit: Self-supervised vision transformer.
\newblock \emph{CoRR}, abs/2104.03602, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.03602}.

\bibitem[Atkinson \& Shiffrin(1971)Atkinson and Shiffrin]{atkinson1971control}
Richard~C Atkinson and Richard~M Shiffrin.
\newblock The control of short-term memory.
\newblock \emph{Scientific american}, 225\penalty0 (2):\penalty0 82--91, 1971.

\bibitem[Averbach \& Coriell(1961)Averbach and Coriell]{averbach1961short}
Emanuel Averbach and Abner~S Coriell.
\newblock Short-term memory in vision.
\newblock \emph{The Bell System Technical Journal}, 40\penalty0 (1):\penalty0
  309--328, 1961.

\bibitem[Ba et~al.(2016)Ba, Hinton, Mnih, Leibo, and
  Ionescu]{DBLP:conf/nips/BaHMLI16}
Jimmy Ba, Geoffrey~E. Hinton, Volodymyr Mnih, Joel~Z. Leibo, and Catalin
  Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 29: Annual Conference on Neural Information Processing
  Systems 2016, December 5-10, 2016, Barcelona, Spain}, pp.\  4331--4339, 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html}.

\bibitem[Baddeley et~al.(1984)Baddeley, Lewis, Eldridge, and
  Thomson]{baddeley1984attention}
Alan Baddeley, Vivien Lewis, Margery Eldridge, and Neil Thomson.
\newblock Attention and retrieval from long-term memory.
\newblock \emph{Journal of Experimental Psychology: General}, 113\penalty0
  (4):\penalty0 518, 1984.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{Bahdanau2015NeuralMT}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{CoRR}, abs/1409.0473, 2015.

\bibitem[Bao et~al.(2021)Bao, Dong, and Wei]{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: {BERT} pre-training of image transformers.
\newblock \emph{CoRR}, abs/2106.08254, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.08254}.

\bibitem[Bellemare et~al.(2012)Bellemare, Naddaf, Veness, and
  Bowling]{atari-env}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{CoRR}, abs/1207.4708, 2012.
\newblock URL \url{http://arxiv.org/abs/1207.4708}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{CoRR}, abs/2004.05150, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.05150}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{CoRR}, abs/2005.14165, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'{e}}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'{e}} J{\'{e}}gou, Julien
  Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock \emph{CoRR}, abs/2104.14294, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.14294}.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{CoRR}, abs/2106.01345, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.01345}.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey~E. Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock \emph{CoRR}, abs/2002.05709, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.05709}.

\bibitem[Chevalier{-}Boisvert et~al.(2018{\natexlab{a}})Chevalier{-}Boisvert,
  Bahdanau, Lahlou, Willems, Saharia, Nguyen, and Bengio]{babyai-env}
Maxime Chevalier{-}Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,
  Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio.
\newblock Babyai: First steps towards grounded language learning with a human
  in the loop.
\newblock \emph{CoRR}, abs/1810.08272, 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1810.08272}.

\bibitem[Chevalier{-}Boisvert et~al.(2018{\natexlab{b}})Chevalier{-}Boisvert,
  Bahdanau, Lahlou, Willems, Saharia, Nguyen, and Bengio]{maxime2018babyai}
Maxime Chevalier{-}Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,
  Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio.
\newblock Babyai: First steps towards grounded language learning with a human
  in the loop.
\newblock \emph{CoRR}, abs/1810.08272, 2018{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1810.08272}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{sparse-transformer}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{CoRR}, abs/1904.10509, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.10509}.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, G{\"{u}}l{\c{c}}ehre, Bougares,
  Schwenk, and Bengio]{cho2014gru}
Kyunghyun Cho, Bart van Merrienboer, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Fethi
  Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock \emph{CoRR}, abs/1406.1078, 2014.
\newblock URL \url{http://arxiv.org/abs/1406.1078}.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{Performers}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David Belanger, Lucy~J. Colwell, and Adrian Weller.
\newblock Rethinking attention with performers.
\newblock \emph{CoRR}, abs/2009.14794, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.14794}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Chung et~al.(2016)Chung, Ahn, and Bengio]{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock \emph{CoRR}, abs/1609.01704, 2016.
\newblock URL \url{http://arxiv.org/abs/1609.01704}.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{pmlr-v15-coates11a}
Adam Coates, Andrew Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In Geoffrey Gordon, David Dunson, and Miroslav Dudík (eds.),
  \emph{Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics}, volume~15 of \emph{Proceedings of Machine
  Learning Research}, pp.\  215--223, Fort Lauderdale, FL, USA, 11--13 Apr
  2011. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v15/coates11a.html}.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G. Carbonell, Quoc~V. Le, and
  Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{CoRR}, abs/1901.02860, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.02860}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{CoRR}, abs/1810.04805, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{CoRR}, abs/2010.11929, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.11929}.

\bibitem[Fan et~al.(2021)Fan, Lavril, Grave, Joulin, and
  Sukhbaatar]{fan2021addressing}
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar
  Sukhbaatar.
\newblock Addressing some limitations of transformers with feedback memory,
  2021.
\newblock URL \url{https://openreview.net/forum?id=OCm0rwa1lx1}.

\bibitem[Galashov et~al.(2019)Galashov, Jayakumar, Hasenclever, Tirumala,
  Schwarz, Desjardins, Czarnecki, Teh, Pascanu, and
  Heess]{galashov2019information}
Alexandre Galashov, Siddhant~M Jayakumar, Leonard Hasenclever, Dhruva Tirumala,
  Jonathan Schwarz, Guillaume Desjardins, Wojciech~M Czarnecki, Yee~Whye Teh,
  Razvan Pascanu, and Nicolas Heess.
\newblock Information asymmetry in kl-regularized rl.
\newblock \emph{arXiv preprint arXiv:1905.01240}, 2019.

\bibitem[Goelet et~al.(1986)Goelet, Castellucci, Schacher, and
  Kandel]{goelet1986long}
Philip Goelet, Vincent~F Castellucci, Samuel Schacher, and Eric~R Kandel.
\newblock The long and the short of long--term memory—a molecular framework.
\newblock \emph{Nature}, 322\penalty0 (6078):\penalty0 419--422, 1986.

\bibitem[Goyal et~al.(2019{\natexlab{a}})Goyal, Islam, Strouse, Ahmed,
  Botvinick, Larochelle, Bengio, and Levine]{goyal2019infobot}
Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew
  Botvinick, Hugo Larochelle, Yoshua Bengio, and Sergey Levine.
\newblock Infobot: Transfer and exploration via the information bottleneck.
\newblock \emph{arXiv preprint arXiv:1901.10902}, 2019{\natexlab{a}}.

\bibitem[Goyal et~al.(2019{\natexlab{b}})Goyal, Lamb, Hoffmann, Sodhani,
  Levine, Bengio, and Sch{\"{o}}lkopf]{goyal2019recurrent}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine,
  Yoshua Bengio, and Bernhard Sch{\"{o}}lkopf.
\newblock Recurrent independent mechanisms.
\newblock \emph{CoRR}, abs/1909.10893, 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1909.10893}.

\bibitem[Goyal et~al.(2021)Goyal, Didolkar, Lamb, Badola, Ke, Rahaman, Binas,
  Blundell, Mozer, and Bengio]{goyal2021coordination}
Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan~Rosemary Ke,
  Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, and Yoshua
  Bengio.
\newblock Coordination among neural modules through a shared global workspace.
\newblock \emph{CoRR}, abs/2103.01197, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.01197}.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'{a}}r, and
  Girshick]{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'{a}}r, and
  Ross~B. Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{CoRR}, abs/2111.06377, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.06377}.

\bibitem[Hihi \& Bengio(1995)Hihi and Bengio]{hihi1995hierarchical}
Salah Hihi and Yoshua Bengio.
\newblock Hierarchical recurrent neural networks for long-term dependencies.
\newblock In D.~Touretzky, M.~C. Mozer, and M.~Hasselmo (eds.), \emph{Advances
  in Neural Information Processing Systems}, volume~8. MIT Press, 1995.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/1995/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf}.

\bibitem[Hill et~al.(2020)Hill, Tieleman, Von~Glehn, Wong, Merzic, and
  Clark]{hill2020grounded}
Felix Hill, Olivier Tieleman, Tamara Von~Glehn, Nathaniel Wong, Hamza Merzic,
  and Stephen Clark.
\newblock Grounded language learning fast and slow.
\newblock \emph{arXiv preprint arXiv:2009.01719}, 2020.

\bibitem[Hinton \& Plaut(1987)Hinton and Plaut]{Hinton87usingfast}
Geoffrey~E. Hinton and David~C. Plaut.
\newblock Using fast weights to deblur old memories.
\newblock In \emph{IN PROCEEDINGS OF THE 9TH ANNUAL CONFERENCE OF THE COGNITIVE
  SCIENCE SOCIETY}, pp.\  177--186. Erlbaum, 1987.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochrieter1997long}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9:\penalty0 1735--80, 12 1997.
\newblock \doi{10.1162/neco.1997.9.8.1735}.

\bibitem[Hutchins et~al.(2022)Hutchins, Schlag, Wu, Dyer, and
  Neyshabur]{delesley2022block}
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock Block-recurrent transformers, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.07852}.

\bibitem[III et~al.(2022)III, Abbeel, and Pinto]{iii2022improving}
Donald Joseph~Hejna III, Pieter Abbeel, and Lerrel Pinto.
\newblock Improving long-horizon imitation through language prediction, 2022.
\newblock URL \url{https://openreview.net/forum?id=1Z3h4rCLvo-}.

\bibitem[Jaegle et~al.(2021)Jaegle, Gimeno, Brock, Zisserman, Vinyals, and
  Carreira]{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  Joao Carreira.
\newblock Perceiver: General perception with iterative attention, 2021.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021reinforcement}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Reinforcement learning as one big sequence modeling problem.
\newblock \emph{CoRR}, abs/2106.02039, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.02039}.

\bibitem[Jeneson \& Squire(2012)Jeneson and Squire]{jeneson2012working}
Annette Jeneson and Larry~R Squire.
\newblock Working memory, long-term memory, and medial temporal lobe function.
\newblock \emph{Learning \& memory}, 19\penalty0 (1):\penalty0 15--25, 2012.

\bibitem[Jonides et~al.(2008)Jonides, Lewis, Nee, Lustig, Berman, and
  Moore]{jonides2008mind}
John Jonides, Richard~L Lewis, Derek~Evan Nee, Cindy~A Lustig, Marc~G Berman,
  and Katherine~Sledge Moore.
\newblock The mind and brain of short-term memory.
\newblock \emph{Annu. Rev. Psychol.}, 59:\penalty0 193--224, 2008.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{linear_transformer}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock \emph{CoRR}, abs/2006.16236, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.16236}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{CoRR}, abs/2001.04451, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.04451}.

\bibitem[Kolodner(1983)]{kolodner1983maintaining}
Janet~L Kolodner.
\newblock Maintaining organization in a dynamic long-term memory.
\newblock \emph{Cognitive science}, 7\penalty0 (4):\penalty0 243--280, 1983.

\bibitem[Koutn{\'{\i}}k et~al.(2014)Koutn{\'{\i}}k, Greff, Gomez, and
  Schmidhuber]{jan2014clockwork}
Jan Koutn{\'{\i}}k, Klaus Greff, Faustino~J. Gomez, and J{\"{u}}rgen
  Schmidhuber.
\newblock A clockwork {RNN}.
\newblock \emph{CoRR}, abs/1402.3511, 2014.
\newblock URL \url{http://arxiv.org/abs/1402.3511}.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Yang, Zhang, Gao, Xiao, Dai, Yuan,
  and Gao]{li2021efficient}
Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai,
  Lu~Yuan, and Jianfeng Gao.
\newblock Efficient self-supervised vision transformers for representation
  learning.
\newblock \emph{CoRR}, abs/2106.09785, 2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2106.09785}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Chen, Yang, Li, Zhu, Zhao, Deng, Wu,
  Zhao, Tang, and Wang]{li2021mst}
Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui
  Deng, Liwei Wu, Rui Zhao, Ming Tang, and Jinqiao Wang.
\newblock {MST:} masked self-supervised transformer for visual representation.
\newblock \emph{CoRR}, abs/2106.05656, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2106.05656}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Hu, Lin, Yao, Xie, Wei, Ning, Cao,
  Zhang, Dong, Wei, and Guo]{swinv2}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, Furu Wei, and Baining Guo.
\newblock Swin transformer {V2:} scaling up capacity and resolution.
\newblock \emph{CoRR}, abs/2111.09883, 2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2111.09883}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{lu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock \emph{CoRR}, abs/2103.14030, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2103.14030}.

\bibitem[Mittal et~al.(2020)Mittal, Lamb, Goyal, Voleti, Shanahan, Lajoie,
  Mozer, and Bengio]{mittal2020learning}
Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan,
  Guillaume Lajoie, Michael Mozer, and Yoshua Bengio.
\newblock Learning to combine top-down and bottom-up signals in recurrent
  neural networks with attention over modules.
\newblock \emph{CoRR}, abs/2006.16981, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.16981}.

\bibitem[Mozer(1991)]{mozer1991induction}
Michael~C Mozer.
\newblock Induction of multiscale temporal structure.
\newblock In J.~Moody, S.~Hanson, and R.~P. Lippmann (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~4. Morgan-Kaufmann, 1991.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/1991/file/53fde96fcc4b4ce72d7739202324cd49-Paper.pdf}.

\bibitem[Radford \& Narasimhan(2018)Radford and
  Narasimhan]{Radford2018ImprovingLU}
Alec Radford and Karthik Narasimhan.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang,
  Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, Casas, Guy, Jones, Bradbury,
  Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell,
  Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and
  Irving]{rae2022scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
  Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
  Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson
  d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan
  Clark, Diego de~Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew
  Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac,
  Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem
  Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and
  Geoffrey Irving.
\newblock Scaling language models: Methods, analysis; insights from training
  gopher, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Schmidhuber(1991)]{Schmidhuber91neuralsequence}
Jürgen Schmidhuber.
\newblock Neural sequence chunkers.
\newblock Technical report, 1991.

\bibitem[Schuster \& Paliwal(1997)Schuster and
  Paliwal]{schuster1997bidirection}
M.~Schuster and K.K. Paliwal.
\newblock Bidirectional recurrent neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 45\penalty0
  (11):\penalty0 2673--2681, 1997.
\newblock \doi{10.1109/78.650093}.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{synthesizers}
Yi~Tay, Dara Bahri, Donald Metzler, Da{-}Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock \emph{CoRR}, abs/2005.00743, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2005.00743}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Bahri, Yang, Metzler, and
  Juan]{sinkhorn-transformer}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da{-}Cheng Juan.
\newblock Sparse sinkhorn attention.
\newblock \emph{CoRR}, abs/2002.11296, 2020{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2002.11296}.

\bibitem[Tay et~al.(2020{\natexlab{c}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{yi2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: {A} benchmark for efficient transformers.
\newblock \emph{CoRR}, abs/2011.04006, 2020{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2011.04006}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{CoRR}, abs/1706.03762, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.03762}.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{CoRR}, abs/2006.04768, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.04768}.

\bibitem[Wang et~al.(2021)Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and
  Shao]{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng{-}Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: {A} versatile backbone for dense
  prediction without convolutions.
\newblock \emph{CoRR}, abs/2102.12122, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.12122}.

\bibitem[Wu et~al.(2021)Wu, Xiao, Codella, Liu, Dai, Yuan, and
  Zhang]{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock \emph{CoRR}, abs/2103.15808, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.15808}.

\bibitem[Yang et~al.(2021)Yang, Li, Zhang, Dai, Xiao, Yuan, and
  Gao]{yang2021focal}
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu~Yuan, and
  Jianfeng Gao.
\newblock Focal self-attention for local-global interactions in vision
  transformers.
\newblock \emph{CoRR}, abs/2107.00641, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.00641}.

\bibitem[Yuan et~al.(2021)Yuan, Guo, Liu, Zhou, Yu, and
  Wu]{yuan2021incorporating}
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock \emph{CoRR}, abs/2103.11816, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.11816}.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Onta{\~{n}}{\'{o}}n, Pham, Ravula, Wang, Yang, and Ahmed]{bigbird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Onta{\~{n}}{\'{o}}n, Philip Pham, Anirudh Ravula, Qifan Wang,
  Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{CoRR}, abs/2007.14062, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.14062}.

\bibitem[Zhang et~al.(2021)Zhang, Dai, Yang, Xiao, Yuan, Zhang, and
  Gao]{zhang2021multiscale}
Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu~Yuan, Lei Zhang, and
  Jianfeng Gao.
\newblock Multi-scale vision longformer: {A} new vision transformer for
  high-resolution image encoding.
\newblock \emph{CoRR}, abs/2103.15358, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.15358}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.01068}.

\bibitem[Zhu et~al.(2021)Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and
  Catanzaro]{zhu2021long}
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima
  Anandkumar, and Bryan Catanzaro.
\newblock Long-short transformer: Efficient transformers for language and
  vision.
\newblock \emph{CoRR}, abs/2107.02192, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.02192}.

\end{thebibliography}
