\begin{thebibliography}{122}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2021)Andrychowicz, Raichuk, Stanczyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski, Gelly, and
  Bachem]{andrychowicz2020matters}
Andrychowicz, M., Raichuk, A., Stanczyk, P., Orsini, M., Girgin, S., Marinier,
  R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., Gelly, S., and
  Bachem, O.
\newblock What matters for on-policy deep actor-critic methods? {A} large-scale
  study.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}, 2021.

\bibitem[Arjona{-}Medina et~al.(2019)Arjona{-}Medina, Gillhofer, Widrich,
  Unterthiner, Brandstetter, and Hochreiter]{arjona2019rudder}
Arjona{-}Medina, J.~A., Gillhofer, M., Widrich, M., Unterthiner, T.,
  Brandstetter, J., and Hochreiter, S.
\newblock {RUDDER:} return decomposition for delayed rewards.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[{\AA}str{\"o}m(1965)]{aastrom1965optimal}
{\AA}str{\"o}m, K.~J.
\newblock Optimal control of {M}arkov processes with incomplete state
  information i.
\newblock \emph{Journal of Mathematical Analysis and Applications},
  10:\penalty0 174--205, 1965.

\bibitem[Bagnell et~al.(2001)Bagnell, Ng, and Schneider]{bagnell2001solving}
Bagnell, J.~A., Ng, A.~Y., and Schneider, J.~G.
\newblock Solving uncertain {M}arkov decision processes.
\newblock 2001.

\bibitem[Bakker(2001)]{bakker2001reinforcement}
Bakker, B.
\newblock Reinforcement learning with long short-term memory.
\newblock In \emph{Advances in Neural Information Processing Systems 14 [Neural
  Information Processing Systems: Natural and Synthetic, {NIPS} 2001, December
  3-8, 2001, Vancouver, British Columbia, Canada]}, 2001.

\bibitem[Bellman(1957)]{bellman1957markovian}
Bellman, R.
\newblock A {M}arkovian decision process.
\newblock \emph{Journal of mathematics and mechanics}, 6\penalty0 (5):\penalty0
  679--684, 1957.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Bengio, Y., Simard, P.~Y., and Frasconi, P.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{{IEEE} Trans. Neural Networks}, 1994.

\bibitem[Cassandra et~al.(1994)Cassandra, Kaelbling, and
  Littman]{cassandra1994acting}
Cassandra, A.~R., Kaelbling, L.~P., and Littman, M.~L.
\newblock Acting optimally in partially observable stochastic domains.
\newblock In \emph{Proceedings of the 12th National Conference on Artificial
  Intelligence, Seattle, WA, USA, July 31 - August 4, 1994, Volume 2}, 1994.

\bibitem[Chen et~al.(2018)Chen, Nikolaidis, Soh, Hsu, and
  Srinivasa]{chen2018planning}
Chen, M., Nikolaidis, S., Soh, H., Hsu, D., and Srinivasa, S.~S.
\newblock Planning with trust for human-robot collaboration.
\newblock In \emph{Proceedings of the 2018 {ACM/IEEE} International Conference
  on Human-Robot Interaction, {HRI} 2018, Chicago, IL, USA, March 05-08, 2018},
  2018.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, G{\"{u}}l{\c{c}}ehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Cho, K., van Merrienboer, B., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Bahdanau, D.,
  Bougares, F., Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
  {A} meeting of SIGDAT, a Special Interest Group of the {ACL}}, 2014.

\bibitem[Christodoulou(2019)]{christodoulou2019soft}
Christodoulou, P.
\newblock Soft actor-critic for discrete action settings.
\newblock \emph{arXiv preprint arXiv:1910.07207}, 2019.

\bibitem[Chung et~al.(2014)Chung, G{\"{u}}l{\c{c}}ehre, Cho, and
  Bengio]{chung2014empirical}
Chung, J., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Cobbe et~al.(2019)Cobbe, Klimov, Hesse, Kim, and
  Schulman]{cobbe2019quantifying}
Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J.
\newblock Quantifying generalization in reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, 2019.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, 2020.

\bibitem[Coumans \& Bai(2016)Coumans and Bai]{coumans2016pybullet}
Coumans, E. and Bai, Y.
\newblock Py{B}ullet, a python module for physics simulation for games,
  robotics and machine learning.
\newblock 2016.

\bibitem[Derman et~al.(2018)Derman, Mankowitz, Mann, and
  Mannor]{derman2018soft}
Derman, E., Mankowitz, D.~J., Mann, T.~A., and Mannor, S.
\newblock Soft-robust actor-critic policy-gradient.
\newblock In \emph{Proceedings of the Thirty-Fourth Conference on Uncertainty
  in Artificial Intelligence, {UAI} 2018, Monterey, California, USA, August
  6-10, 2018}, 2018.

\bibitem[Ding(2019)]{ding2019rlalgorithms}
Ding, Z.
\newblock Popular-rl-algorithms.
\newblock \url{https://github.com/quantumiracle/Popular-RL-Algorithms}, 2019.

\bibitem[Dorfman et~al.(2020)Dorfman, Shenfeld, and Tamar]{dorfman2020offline}
Dorfman, R., Shenfeld, I., and Tamar, A.
\newblock Offline meta learning of exploration.
\newblock \emph{arXiv preprint arXiv:2008.02598}, 2020.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Duan, Y., Schulman, J., Chen, X., Bartlett, P.~L., Sutskever, I., and Abbeel,
  P.
\newblock Rl{\textdollar}{\^{}}2{\textdollar}: Fast reinforcement learning via
  slow reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.02779}, 2016.

\bibitem[Elman(1990)]{elman1990finding}
Elman, J.~L.
\newblock Finding structure in time.
\newblock \emph{Cogn. Sci.}, 1990.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{engstrom2019implementation}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L.,
  and Madry, A.
\newblock Implementation matters in deep {RL:} {A} case study on {PPO} and
  {TRPO}.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
\newblock {IMPALA:} scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018.

\bibitem[Fakoor et~al.(2020)Fakoor, Chaudhari, Soatto, and
  Smola]{fakoor2019meta}
Fakoor, R., Chaudhari, P., Soatto, S., and Smola, A.~J.
\newblock Meta-q-learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Farebrother et~al.(2018)Farebrother, Machado, and
  Bowling]{farebrother2018generalization}
Farebrother, J., Machado, M.~C., and Bowling, M.
\newblock Generalization and regularization in {DQN}.
\newblock \emph{arXiv preprint arXiv:1810.00123}, 2018.

\bibitem[Fedus et~al.(2019)Fedus, Gelada, Bengio, Bellemare, and
  Larochelle]{fedus2019hyperbolic}
Fedus, W., Gelada, C., Bengio, Y., Bellemare, M.~G., and Larochelle, H.
\newblock Hyperbolic discounting and learning over multiple horizons.
\newblock \emph{arXiv preprint arXiv:1902.06865}, 2019.

\bibitem[Ferret et~al.(2020)Ferret, Marinier, Geist, and
  Pietquin]{ferret2019self}
Ferret, J., Marinier, R., Geist, M., and Pietquin, O.
\newblock Self-attentional credit assignment for transfer in reinforcement
  learning.
\newblock In \emph{Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence, {IJCAI} 2020}, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, 2017.

\bibitem[Fortunato et~al.(2019)Fortunato, Tan, Faulkner, Hansen, Badia,
  Buttimore, Deck, Leibo, and Blundell]{fortunato2019generalization}
Fortunato, M., Tan, M., Faulkner, R., Hansen, S., Badia, A.~P., Buttimore, G.,
  Deck, C., Leibo, J.~Z., and Blundell, C.
\newblock Generalization of reinforcement learners with working and episodic
  memory.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018.

\bibitem[Gangwani et~al.(2020)Gangwani, Zhou, and Peng]{gangwani2020learning}
Gangwani, T., Zhou, Y., and Peng, J.
\newblock Learning guidance rewards with trajectory-space smoothing.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Gleave et~al.(2020)Gleave, Dennis, Wild, Kant, Levine, and
  Russell]{gleave2019adversarial}
Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S., and Russell, S.
\newblock Adversarial policies: Attacking deep reinforcement learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska{-}Barwinska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia,
  Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and
  Hassabis]{graves2016hybrid}
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I.,
  Grabska{-}Barwinska, A., Colmenarejo, S.~G., Grefenstette, E., Ramalho, T.,
  Agapiou, J.~P., Badia, A.~P., Hermann, K.~M., Zwols, Y., Ostrovski, G., Cain,
  A., King, H., Summerfield, C., Blunsom, P., Kavukcuoglu, K., and Hassabis, D.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nat.}, 2016.

\bibitem[Gregor et~al.(2019)Gregor, Rezende, Besse, Wu, Merzic, and van~den
  Oord]{Gregor2019ShapingBS}
Gregor, K., Rezende, D.~J., Besse, F., Wu, Y., Merzic, H., and van~den Oord, A.
\newblock Shaping belief states with generative environment models for {RL}.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Ha \& Schmidhuber(2018)Ha and Schmidhuber]{ha2018recurrent}
Ha, D. and Schmidhuber, J.
\newblock Recurrent world models facilitate policy evolution.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, 2018.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, and Levine]{haarnoja2018soft2}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{Hafner2019LearningLD}
Hafner, D., Lillicrap, T.~P., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, 2019.

\bibitem[Han et~al.(2020)Han, Doya, and Tani]{han2019variational}
Han, D., Doya, K., and Tani, J.
\newblock Variational recurrent models for solving partially observable control
  tasks.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Harutyunyan et~al.(2019)Harutyunyan, Dabney, Mesnard, Azar, Piot,
  Heess, van Hasselt, Wayne, Singh, Precup, and
  Munos]{harutyunyan2019hindsight}
Harutyunyan, A., Dabney, W., Mesnard, T., Azar, M.~G., Piot, B., Heess, N., van
  Hasselt, H., Wayne, G., Singh, S., Precup, D., and Munos, R.
\newblock Hindsight credit assignment.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Hausknecht \& Stone(2015)Hausknecht and Stone]{hausknecht2015deep}
Hausknecht, M.~J. and Stone, P.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In \emph{2015 {AAAI} Fall Symposia, Arlington, Virginia, USA,
  November 12-14, 2015}, 2015.

\bibitem[Heess et~al.(2015)Heess, Hunt, Lillicrap, and Silver]{heess2015memory}
Heess, N., Hunt, J.~J., Lillicrap, T.~P., and Silver, D.
\newblock Memory-based control with recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1512.04455}, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
Hochreiter, S., Younger, A.~S., and Conwell, P.~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{Artificial Neural Networks - {ICANN} 2001, International
  Conference Vienna, Austria, August 21-25, 2001 Proceedings}, 2001.

\bibitem[Huang et~al.(2017)Huang, Papernot, Goodfellow, Duan, and
  Abbeel]{huang2017adversarial}
Huang, S.~H., Papernot, N., Goodfellow, I.~J., Duan, Y., and Abbeel, P.
\newblock Adversarial attacks on neural network policies.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  2017.

\bibitem[Humplik et~al.(2019)Humplik, Galashov, Hasenclever, Ortega, Teh, and
  Heess]{humplik2019meta}
Humplik, J., Galashov, A., Hasenclever, L., Ortega, P.~A., Teh, Y.~W., and
  Heess, N.
\newblock Meta reinforcement learning as task inference.
\newblock \emph{arXiv preprint arXiv:1905.06424}, 2019.

\bibitem[Hung et~al.(2018)Hung, Lillicrap, Abramson, Wu, Mirza, Carnevale,
  Ahuja, and Wayne]{hung2019optimizing}
Hung, C., Lillicrap, T.~P., Abramson, J., Wu, Y., Mirza, M., Carnevale, F.,
  Ahuja, A., and Wayne, G.
\newblock Optimizing agent behavior over long time scales by transporting
  value.
\newblock \emph{arXiv preprint arXiv:1810.06721}, 2018.

\bibitem[Igl et~al.(2018)Igl, Zintgraf, Le, Wood, and Whiteson]{igl2018deep}
Igl, M., Zintgraf, L.~M., Le, T.~A., Wood, F., and Whiteson, S.
\newblock Deep variational reinforcement learning for pomdps.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018.

\bibitem[Igl et~al.(2019)Igl, Ciosek, Li, Tschiatschek, Zhang, Devlin, and
  Hofmann]{igl2019generalization}
Igl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin, S., and
  Hofmann, K.
\newblock Generalization in reinforcement learning with selective noise
  injection and information bottleneck.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Jiang et~al.(2021)Jiang, Li, Dai, Zou, and Xiong]{jiang2021monotonic}
Jiang, Y., Li, C., Dai, W., Zou, J., and Xiong, H.
\newblock Monotonic robust policy optimization with model discrepancy.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, 2021.

\bibitem[Justesen et~al.(2018)Justesen, Torrado, Bontrager, Khalifa, Togelius,
  and Risi]{justesen2018illuminating}
Justesen, N., Torrado, R.~R., Bontrager, P., Khalifa, A., Togelius, J., and
  Risi, S.
\newblock Illuminating generalization in deep reinforcement learning through
  procedural level generation.
\newblock \emph{arXiv preprint arXiv:1806.10729}, 2018.

\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and
  Cassandra]{kaelbling1998planning}
Kaelbling, L.~P., Littman, M.~L., and Cassandra, A.~R.
\newblock Planning and acting in partially observable stochastic domains.
\newblock \emph{Artif. Intell.}, 1998.

\bibitem[Kapturowski et~al.(2019)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{kapturowski2018recurrent}
Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Katsikopoulos \& Engelbrecht(2003)Katsikopoulos and
  Engelbrecht]{katsikopoulos2003markov}
Katsikopoulos, K.~V. and Engelbrecht, S.~E.
\newblock Markov decision processes with delays and asynchronous cost
  collection.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 2003.

\bibitem[Khalil et~al.(1996)Khalil, Doyle, and Glover]{khalil1996robust}
Khalil, I.~S., Doyle, J., and Glover, K.
\newblock \emph{Robust and optimal control}.
\newblock prentice hall, new jersey, 1996.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem[Kirk et~al.(2021)Kirk, Zhang, Grefenstette, and
  Rockt{\"{a}}schel]{kirk2021survey}
Kirk, R., Zhang, A., Grefenstette, E., and Rockt{\"{a}}schel, T.
\newblock A survey of generalisation in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2111.09794}, 2021.

\bibitem[Kostrikov(2018)]{kostrikov2018pytorch}
Kostrikov, I.
\newblock Pytorch implementations of reinforcement learning algorithms.
\newblock \url{https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail}, 2018.

\bibitem[Lee et~al.(2020{\natexlab{a}})Lee, Nagabandi, Abbeel, and
  Levine]{lee2019stochastic}
Lee, A.~X., Nagabandi, A., Abbeel, P., and Levine, S.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020{\natexlab{a}}.

\bibitem[Lee et~al.(2020{\natexlab{b}})Lee, Lee, Shin, and Lee]{lee2019network}
Lee, K., Lee, K., Shin, J., and Lee, H.
\newblock Network randomization: {A} simple technique for generalization in
  deep reinforcement learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020{\natexlab{b}}.

\bibitem[Lin et~al.(2017)Lin, Hong, Liao, Shih, Liu, and Sun]{lin2017tactics}
Lin, Y., Hong, Z., Liao, Y., Shih, M., Liu, M., and Sun, M.
\newblock Tactics of adversarial attack on deep reinforcement learning agents.
\newblock In \emph{Proceedings of the Twenty-Sixth International Joint
  Conference on Artificial Intelligence, {IJCAI} 2017, Melbourne, Australia,
  August 19-25, 2017}, 2017.

\bibitem[Littman(1996)]{littman1996algorithms}
Littman, M.~L.
\newblock \emph{Algorithms for sequential decision-making}.
\newblock Brown University, 1996.

\bibitem[Liu et~al.(2019)Liu, Luo, Zhong, Chen, Liu, and Peng]{liu2019sequence}
Liu, Y., Luo, Y., Zhong, Y., Chen, X., Liu, Q., and Peng, J.
\newblock Sequence modeling of temporal credit assignment for episodic
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.13420}, 2019.

\bibitem[Mankowitz et~al.(2020)Mankowitz, Levine, Jeong, Abdolmaleki,
  Springenberg, Shi, Kay, Hester, Mann, and Riedmiller]{mankowitz2019robust}
Mankowitz, D.~J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg, J.~T.,
  Shi, Y., Kay, J., Hester, T., Mann, T.~A., and Riedmiller, M.~A.
\newblock Robust reinforcement learning for continuous control with model
  misspecification.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Meng et~al.(2021)Meng, Gorbet, and Kulic]{meng2021memory}
Meng, L., Gorbet, R., and Kulic, D.
\newblock Memory-based deep reinforcement learning for pomdps.
\newblock In \emph{{IEEE/RSJ} International Conference on Intelligent Robots
  and Systems, {IROS} 2021, Prague, Czech Republic, September 27 - Oct. 1,
  2021}, 2021.

\bibitem[Mesnard et~al.(2021)Mesnard, Weber, Viola, Thakoor, Saade,
  Harutyunyan, Dabney, Stepleton, Heess, Guez, Moulines, Hutter, Buesing, and
  Munos]{mesnard2020counterfactual}
Mesnard, T., Weber, T., Viola, F., Thakoor, S., Saade, A., Harutyunyan, A.,
  Dabney, W., Stepleton, T.~S., Heess, N., Guez, A., Moulines, E., Hutter, M.,
  Buesing, L., and Munos, R.
\newblock Counterfactual credit assignment in model-free reinforcement
  learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, 2021.

\bibitem[Mirowski et~al.(2017)Mirowski, Pascanu, Viola, Soyer, Ballard, Banino,
  Denil, Goroshin, Sifre, Kavukcuoglu, Kumaran, and
  Hadsell]{mirowski2016learning}
Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A.,
  Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., Kumaran, D., and
  Hadsell, R.
\newblock Learning to navigate in complex environments.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Morimoto \& Doya(2005)Morimoto and Doya]{morimoto2005robust}
Morimoto, J. and Doya, K.
\newblock Robust reinforcement learning.
\newblock \emph{Neural computation}, 17\penalty0 (2):\penalty0 335--359, 2005.

\bibitem[Nilim \& Ghaoui(2005)Nilim and Ghaoui]{nilim2005robust}
Nilim, A. and Ghaoui, L.~E.
\newblock Robust control of {M}arkov decision processes with uncertain
  transition matrices.
\newblock \emph{Oper. Res.}, 2005.

\bibitem[Oh et~al.(2016)Oh, Chockalingam, Singh, and Lee]{oh2016control}
Oh, J., Chockalingam, V., Singh, S., and Lee, H.
\newblock Control of memory, active perception, and action in minecraft.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, 2016.

\bibitem[Packer et~al.(2018)Packer, Gao, Kos, Kr{\"{a}}henb{\"{u}}hl, Koltun,
  and Song]{packer2018assessing}
Packer, C., Gao, K., Kos, J., Kr{\"{a}}henb{\"{u}}hl, P., Koltun, V., and Song,
  D.
\newblock Assessing generalization in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1810.12282}, 2018.

\bibitem[Papadimitriou \& Tsitsiklis(1987)Papadimitriou and
  Tsitsiklis]{papadimitriou1987complexity}
Papadimitriou, C.~H. and Tsitsiklis, J.~N.
\newblock The complexity of {M}arkov decision processes.
\newblock \emph{Math. Oper. Res.}, 1987.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu,
  G{\"{u}}l{\c{c}}ehre, Jayakumar, Jaderberg, Kaufman, Clark, Noury, Botvinick,
  Heess, and Hadsell]{parisotto2020stabilizing}
Parisotto, E., Song, H.~F., Rae, J.~W., Pascanu, R., G{\"{u}}l{\c{c}}ehre,
  {\c{C}}., Jayakumar, S.~M., Jaderberg, M., Kaufman, R.~L., Clark, A., Noury,
  S., Botvinick, M.~M., Heess, N., and Hadsell, R.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, 2020.

\bibitem[Pattanaik et~al.(2018)Pattanaik, Tang, Liu, Bommannan, and
  Chowdhary]{pattanaik2017robust}
Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., and Chowdhary, G.
\newblock Robust deep reinforcement learning with adversarial attacks.
\newblock In \emph{Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems, {AAMAS} 2018, Stockholm, Sweden,
  July 10-15, 2018}, 2018.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, 2017.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{{M}arkov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley and Sons, 2014.

\bibitem[Raffin(2020)]{rl-zoo3}
Raffin, A.
\newblock Rl baselines3 zoo.
\newblock \url{https://github.com/DLR-RM/rl-baselines3-zoo}, 2020.

\bibitem[Raffin et~al.(2021)Raffin, Kober, and Stulp]{raffin2021smooth}
Raffin, A., Kober, J., and Stulp, F.
\newblock Smooth exploration for robotic reinforcement learning.
\newblock In \emph{Conference on Robot Learning, 8-11 November 2021, London,
  {UK}}, 2021.

\bibitem[Raileanu \& Fergus(2021)Raileanu and Fergus]{raileanu2021decoupling}
Raileanu, R. and Fergus, R.
\newblock Decoupling value and policy for generalization in reinforcement
  learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, 2021.

\bibitem[Rajeswaran et~al.(2017{\natexlab{a}})Rajeswaran, Ghotra, Ravindran,
  and Levine]{rajeswaran2016epopt}
Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S.
\newblock Epopt: Learning robust neural network policies using model ensembles.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017{\natexlab{a}}.

\bibitem[Rajeswaran et~al.(2017{\natexlab{b}})Rajeswaran, Lowrey, Todorov, and
  Kakade]{rajeswaran2017towards}
Rajeswaran, A., Lowrey, K., Todorov, E., and Kakade, S.~M.
\newblock Towards generalization and simplicity in continuous control.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, 2017{\natexlab{b}}.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and
  Quillen]{rakelly2019efficient}
Rakelly, K., Zhou, A., Finn, C., Levine, S., and Quillen, D.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, 2019.

\bibitem[Raposo et~al.(2021)Raposo, Ritter, Santoro, Wayne, Weber, Botvinick,
  van Hasselt, and Song]{raposo2021synthetic}
Raposo, D., Ritter, S., Santoro, A., Wayne, G., Weber, T., Botvinick, M.~M.,
  van Hasselt, H., and Song, H.~F.
\newblock Synthetic returns for long-term credit assignment.
\newblock \emph{arXiv preprint arXiv:2102.12425}, 2021.

\bibitem[Ren et~al.(2021)Ren, Guo, Zhou, and Peng]{ren2021learning}
Ren, Z., Guo, R., Zhou, Y., and Peng, J.
\newblock Learning long-term reward redistribution via randomized return
  decomposition.
\newblock \emph{arXiv preprint arXiv:2111.13485}, 2021.

\bibitem[Ritter et~al.(2018)Ritter, Wang, Kurth{-}Nelson, Jayakumar, Blundell,
  Pascanu, and Botvinick]{ritter2018been}
Ritter, S., Wang, J.~X., Kurth{-}Nelson, Z., Jayakumar, S.~M., Blundell, C.,
  Pascanu, R., and Botvinick, M.~M.
\newblock Been there, done that: Meta-learning with episodic recall.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018.

\bibitem[Russo \& Prouti{\`{e}}re(2021)Russo and
  Prouti{\`{e}}re]{russo2019optimal}
Russo, A. and Prouti{\`{e}}re, A.
\newblock Towards optimal attacks on reinforcement learning policies.
\newblock In \emph{2021 American Control Conference, {ACC} 2021, New Orleans,
  LA, USA, May 25-28, 2021}, 2021.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock In \emph{2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}, 2014.

\bibitem[Sch{\"a}fer \& Zimmermann(2006)Sch{\"a}fer and
  Zimmermann]{schafer2006recurrent}
Sch{\"a}fer, A.~M. and Zimmermann, H.~G.
\newblock Recurrent neural networks are universal approximators.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  632--640. Springer, 2006.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
Schmidhuber, J.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Schmidhuber(1991)]{schmidhuber1991reinforcement}
Schmidhuber, J.
\newblock Reinforcement learning in {M}arkovian and non-{M}arkovian
  environments.
\newblock In \emph{Advances in Neural Information Processing Systems 3,
  NIPS'3}, pp.\  500--506, 1991.

\bibitem[Siegelmann \& Sontag(1995)Siegelmann and
  Sontag]{siegelmann1995computational}
Siegelmann, H.~T. and Sontag, E.~D.
\newblock On the computational power of neural nets.
\newblock \emph{Journal of computer and system sciences}, 50\penalty0
  (1):\penalty0 132--150, 1995.

\bibitem[Song et~al.(2020)Song, Jiang, Tu, Du, and
  Neyshabur]{song2019observational}
Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B.
\newblock Observational overfitting in reinforcement learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Srouji et~al.(2018)Srouji, Zhang, and
  Salakhutdinov]{srouji2018structured}
Srouji, M., Zhang, J., and Salakhutdinov, R.
\newblock Structured control nets for deep reinforcement learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018.

\bibitem[Stulp et~al.(2011)Stulp, Theodorou, Buchli, and
  Schaal]{stulp2011learning}
Stulp, F., Theodorou, E.~A., Buchli, J., and Schaal, S.
\newblock Learning to grasp under uncertainty.
\newblock In \emph{{IEEE} International Conference on Robotics and Automation,
  {ICRA} 2011, Shanghai, China, 9-13 May 2011}, 2011.

\bibitem[Sun et~al.(2021)Sun, Xu, Fang, Peng, Guo, Dai, and Zhou]{sun2021safe}
Sun, H., Xu, Z., Fang, M., Peng, Z., Guo, J., Dai, B., and Zhou, B.
\newblock Safe exploration by solving early terminated {MDP}.
\newblock \emph{arXiv preprint arXiv:2107.04200}, 2021.

\bibitem[Sutton(1984)]{sutton1984temporal}
Sutton, R.~S.
\newblock \emph{Temporal credit assignment in reinforcement learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 1984.

\bibitem[Tessler et~al.(2019)Tessler, Efroni, and Mannor]{tessler2019action}
Tessler, C., Efroni, Y., and Mannor, S.
\newblock Action robust reinforcement learning and applications in continuous
  control.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, 2019.

\bibitem[Thrun \& Pratt(2012)Thrun and Pratt]{thrun2012learning}
Thrun, S. and Pratt, L.
\newblock \emph{Learning to learn}.
\newblock Springer Science and Business Media, 2012.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{tobin2017domain}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{2017 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems, {IROS} 2017, Vancouver, BC, Canada, September 24-28,
  2017}, 2017.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In \emph{2012 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems, {IROS} 2012, Vilamoura, Algarve, Portugal, October 7-12,
  2012}, 2012.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
Trinh, T.~H., Dai, A.~M., Luong, T., and Le, Q.~V.
\newblock Learning longer-term dependencies in rnns with auxiliary losses.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, 2018.

\bibitem[Wang et~al.(2017)Wang, Kurth{-}Nelson, Soyer, Leibo, Tirumala, Munos,
  Blundell, Kumaran, and Botvinick]{wang2016learning}
Wang, J., Kurth{-}Nelson, Z., Soyer, H., Leibo, J.~Z., Tirumala, D., Munos, R.,
  Blundell, C., Kumaran, D., and Botvinick, M.~M.
\newblock Learning to reinforcement learn.
\newblock In \emph{Proceedings of the 39th Annual Meeting of the Cognitive
  Science Society, CogSci 2017, London, UK, 16-29 July 2017}, 2017.

\bibitem[Wang et~al.(2019)Wang, He, and Tan]{wang2019robust}
Wang, Y., He, H., and Tan, X.
\newblock Robust reinforcement learning in pomdps with incomplete and noisy
  observations.
\newblock \emph{arXiv preprint arXiv:1902.05795}, 2019.

\bibitem[Watter et~al.(2015)Watter, Springenberg, Boedecker, and
  Riedmiller]{watter2015embed}
Watter, M., Springenberg, J.~T., Boedecker, J., and Riedmiller, M.~A.
\newblock Embed to control: {A} locally linear latent dynamics model for
  control from raw images.
\newblock In \emph{Advances in Neural Information Processing Systems 28: Annual
  Conference on Neural Information Processing Systems 2015, December 7-12,
  2015, Montreal, Quebec, Canada}, 2015.

\bibitem[Weng et~al.(2021)Weng, Chen, Yan, You, Duburcq, Zhang, Su, and
  Zhu]{weng2021tianshou}
Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang, M., Su, H., and Zhu,
  J.
\newblock Tianshou: a highly modularized deep reinforcement learning library.
\newblock \emph{arXiv preprint arXiv:2107.14171}, 2021.

\bibitem[Whitehead \& Ballard(1990)Whitehead and Ballard]{whitehead1990active}
Whitehead, S.~D. and Ballard, D.~H.
\newblock Active perception and reinforcement learning.
\newblock In \emph{Machine Learning Proceedings 1990}, pp.\  179--188.
  Elsevier, 1990.

\bibitem[Whiteson et~al.(2011)Whiteson, Tanner, Taylor, and
  Stone]{whiteson2011protecting}
Whiteson, S., Tanner, B., Taylor, M.~E., and Stone, P.
\newblock Protecting against evaluation overfitting in empirical reinforcement
  learning.
\newblock In \emph{2011 {IEEE} Symposium on Adaptive Dynamic Programming And
  Reinforcement Learning, {ADPRL} 2011, Paris, France, April 12-14, 2011},
  2011.

\bibitem[Wierstra et~al.(2007)Wierstra, F{\"{o}}rster, Peters, and
  Schmidhuber]{wierstra2007solving}
Wierstra, D., F{\"{o}}rster, A., Peters, J., and Schmidhuber, J.
\newblock Solving deep memory pomdps with recurrent policy gradients.
\newblock In \emph{Artificial Neural Networks - {ICANN} 2007, 17th
  International Conference, Porto, Portugal, September 9-13, 2007, Proceedings,
  Part {I}}, 2007.

\bibitem[Wilson et~al.(2007)Wilson, Fern, Ray, and Tadepalli]{wilson2007multi}
Wilson, A., Fern, A., Ray, S., and Tadepalli, P.
\newblock Multi-task reinforcement learning: a hierarchical bayesian approach.
\newblock In \emph{Machine Learning, Proceedings of the Twenty-Fourth
  International Conference {(ICML} 2007), Corvallis, Oregon, USA, June 20-24,
  2007}, 2007.

\bibitem[Xu et~al.(2020)Xu, van Hasselt, Hessel, Oh, Singh, and
  Silver]{xu2018meta}
Xu, Z., van Hasselt, H.~P., Hessel, M., Oh, J., Singh, S., and Silver, D.
\newblock Meta-gradient reinforcement learning with an objective discovered
  online.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Yang \& Nguyen(2021)Yang and Nguyen]{yang2021recurrent}
Yang, Z. and Nguyen, H.
\newblock Recurrent off-policy baselines for memory-based continuous control.
\newblock \emph{arXiv preprint arXiv:2110.12628}, 2021.

\bibitem[Yu et~al.(2019)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{yu2020meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-{W}orld: {A} benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{3rd Annual Conference on Robot Learning, CoRL 2019, Osaka,
  Japan, October 30 - November 1, 2019, Proceedings}, 2019.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Ballas, and
  Pineau]{zhang2018dissection}
Zhang, A., Ballas, N., and Pineau, J.
\newblock A dissection of overfitting and generalization in continuous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.07937}, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Vinyals, Munos, and
  Bengio]{zhang2018study}
Zhang, C., Vinyals, O., Munos, R., and Bengio, S.
\newblock A study on overfitting in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1804.06893}, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Xiao, Li, Liu, Boning, and
  Hsieh]{zhang2020robust}
Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D.~S., and Hsieh, C.
\newblock Robust deep reinforcement learning against adversarial perturbations
  on state observations.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Chen, Boning, and Hsieh]{zhang2021robust}
Zhang, H., Chen, H., Boning, D.~S., and Hsieh, C.
\newblock Robust reinforcement learning on state observations with learned
  optimal adversary.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}, 2021.

\bibitem[Zhang et~al.(2018{\natexlab{c}})Zhang, Vikram, Smith, Abbeel, Johnson,
  and Levine]{Zhang2018SOLARDS}
Zhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M.~J., and Levine, S.
\newblock {SOLAR:} deep structured latent representations for model-based
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1808.09105}, 2018{\natexlab{c}}.

\bibitem[Zhao et~al.(2019)Zhao, Sigaud, Stulp, and
  Hospedales]{zhao2019investigating}
Zhao, C., Sigaud, O., Stulp, F., and Hospedales, T.~M.
\newblock Investigating generalisation in continuous deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1902.07015}, 2019.

\bibitem[Zhu et~al.(2020)Zhu, Lin, Yang, and Zhang]{zhu2020episodic}
Zhu, G., Lin, Z., Yang, G., and Zhang, C.
\newblock Episodic reinforcement learning with associative memory.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Zhu et~al.(2017)Zhu, Mottaghi, Kolve, Lim, Gupta, Fei{-}Fei, and
  Farhadi]{zhu2017target}
Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.~J., Gupta, A., Fei{-}Fei, L., and
  Farhadi, A.
\newblock Target-driven visual navigation in indoor scenes using deep
  reinforcement learning.
\newblock In \emph{2017 {IEEE} International Conference on Robotics and
  Automation, {ICRA} 2017, Singapore, Singapore, May 29 - June 3, 2017}, 2017.

\bibitem[Ziebart(2010)]{ziebart2010thesis}
Ziebart, B.~D.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2019varibad}
Zintgraf, L.~M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S.
\newblock Varibad: {A} very good method for bayes-adaptive deep {RL} via
  meta-learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Zintgraf et~al.(2021)Zintgraf, Schulze, Lu, Feng, Igl, Shiarlis, Gal,
  Hofmann, and Whiteson]{zintgraf2021varibad}
Zintgraf, L.~M., Schulze, S., Lu, C., Feng, L., Igl, M., Shiarlis, K., Gal, Y.,
  Hofmann, K., and Whiteson, S.
\newblock Varibad: Variational bayes-adaptive deep {RL} via meta-learning.
\newblock \emph{J. Mach. Learn. Res.}, 2021.

\end{thebibliography}
