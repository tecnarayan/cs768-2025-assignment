@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{ICML  = {International Conference on Machine Learning}}
@string{COLT  = {Conference on Learning Theory}}
@string{ICLR  = {International Conference on Learning Representations}}	
@string{TIT = {IEEE Transactions on Information Theory}}


@article{jin2019provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  journal={arXiv preprint arXiv:1907.05388},
  year={2019}
}

@article{dani2008stochastic,
  title={Stochastic linear optimization under bandit feedback},
  author={Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
  journal=COLT,
  year={2008}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle=ICML,
  pages={1889--1897},
  year={2015}
}

@misc{openai2019dota,
  author = {OpenAI},
  title = {Open{AI} {F}ive},
  howpublished = {\url{https://openai.com/five/}},
  year={2019}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle=ICML,
  pages={1329--1338},
  year={2016}
}


@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of {G}o without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{wang2018deep,
  title={Deep reinforcement learning for {NLP}},
  author={Wang, William Yang and Li, Jiwei and He, Xiaodong},
  booktitle={Association for Computational Linguistics},
  pages={19--21},
  year={2018}
}

@article{wang2019neural,
  title={Neural policy gradient methods: Global optimality and rates of convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1909.01150},
  year={2019}
}

@article{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham M and Mesbahi, Mehran},
  journal={arXiv preprint arXiv:1801.05039},
  year={2018}
}

@article{yang2019global,
  title={On the Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost},
  author={Yang, Zhuoran and Chen, Yongxin and Hong, Mingyi and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1907.06246},
  year={2019}
}

@article{bhandari2019global,
  title={Global Optimality Guarantees For Policy Gradient Methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}

@article{agarwal2019optimality,
  title={Optimality and Approximation with Policy Gradient Methods in {M}arkov Decision Processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:1908.00261},
  year={2019}
}

@article{liu2019neural,
  title={Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1906.10306},
  year={2019}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992}
}

@inproceedings{baxter2000direct,
  title={Direct gradient-based reinforcement learning},
  author={Baxter, Jonathan and Bartlett, Peter L},
  booktitle={International Symposium on Circuits and Systems},
  pages={271--274},
  year={2000}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle=NIPS,
  year={2000}
}

@inproceedings{kakade2002approximately,
  title={Approximately Optimal Approximate Reinforcement Learning},
  author={Kakade, Sham and Langford, John},
  booktitle=ICML,
  year={2002}
}

@inproceedings{kakade2002natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle=NIPS,
  year={2002}
}

@phdthesis{konda2002actor,
  title={Actor-Critic Algorithms},
  author={Konda, Vijaymohan},
  year={2002},
  school={Massachusetts Institute of Technology}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle=NIPS,
  year={2000}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{koenig1993complexity,
  title={Complexity analysis of real-time reinforcement learning},
  author={Koenig, Sven and Simmons, Reid G},
  booktitle={Association for the Advancement of Artificial Intelligence},
  pages={99--107},
  year={1993}
}

@inproceedings{azar2011speedy,
  title={Speedy {Q}-learning},
  author={Azar, Mohammad Gheshlaghi and Munos, Remi and Ghavamzadaeh, M and Kappen, Hilbert J},
  year={2011},
  booktitle=NIPS
}

@article{azar2012sample,
  title={On the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Bert},
  journal={arXiv preprint arXiv:1206.6461},
  year={2012}
}

@inproceedings{lattimore2012pac,
  title={{PAC} bounds for discounted {MDP}s},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012}
}

@inproceedings{sidford2018variance,
  title={Variance reduced value iteration and faster algorithms for solving {M}arkov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  booktitle={Symposium on Discrete Algorithms},
  pages={770--787},
  year={2018}
}

@article{wainwright2019variance,
  title={Variance-reduced {Q}-learning is minimax optimal},
  author={Wainwright, Martin J},
  journal={arXiv preprint arXiv:1906.04697},
  year={2019}
}

@inproceedings{sidford2018near,
  title={Near-optimal time and sample complexities for solving {M}arkov decision processes with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin and Ye, Yinyu},
  booktitle=NIPS,
  pages={5186--5196},
  year={2018}
}

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal=JMLR,
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@inproceedings{antos2008fitted,
  title={Fitted {Q}-iteration in continuous action-space MDPs},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle=NIPS,
  pages={9--16},
  year={2008}
}

@article{farahmand2016regularized,
  title={Regularized policy iteration with nonparametric function spaces},
  author={Farahmand, Amir-massoud and Ghavamzadeh, Mohammad and Szepesv{\'a}ri, Csaba and Mannor, Shie},
  journal=JMLR,
  volume={17},
  number={1},
  pages={4809--4874},
  year={2016}
}

@inproceedings{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle=NIPS,
  pages={568--576},
  year={2010}
}

@inproceedings{tosatto2017boosted,
  title={Boosted fitted {Q}-iteration},
  author={Tosatto, Samuele and Pirotta, Matteo and D'Eramo, Carlo and Restelli, Marcello},
  booktitle=ICML,
  pages={3434--3443},
  year={2017}
}

@article{yang2019theoretical,
  title={A Theoretical Analysis of Deep {Q}-Learning},
  author={Yang, Zhuoran and Xie, Yuchen and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1901.00137},
  year={2019}
}

@article{chen2019information,
  title={Information-Theoretic Considerations in Batch Reinforcement Learning},
  author={Chen, Jinglin and Jiang, Nan},
  journal={arXiv preprint arXiv:1905.00360},
  year={2019}
}

@article{mania2018simple,
  title={Simple random search provides a competitive approach to reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={arXiv preprint arXiv:1803.07055},
  year={2018}
}

@phdthesis{kakade2003sample,
  title={On the Sample Complexity of Reinforcement Learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  school={University of London}
}

@inproceedings{leffler2007efficient,
  title={Efficient reinforcement learning with relocatable action models},
  author={Leffler, Bethany R and Littman, Michael L and Edmunds, Timothy},
  booktitle={Association for the Advancement of Artificial Intelligence},
  pages={572--577},
  year={2007}
}

@article{azar2012dynamic,
  title={Dynamic policy programming},
  author={Azar, Mohammad Gheshlaghi and G{\'o}mez, Vicen{\c{c}} and Kappen, Hilbert J},
  journal=JMLR,
  volume={13},
  number={Nov},
  pages={3207--3245},
  year={2012}
}

@article{azar2011reinforcement,
  title={Reinforcement learning with a near optimal rate of convergence},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Ghavamzadeh, Mohammad and Kappen, Hilbert},
  year={2011}
}

@article{yang2019reinforcement,
  title={Reinforcement Leaning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},
  author={Yang, Lin and Wang, Mengdi},
  journal={arXiv preprint arXiv:1905.10389},
  year={2019}
}

@inproceedings{yang2019sample,
  title={Sample-Optimal Parametric {Q}-Learning Using Linearly Additive Features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle=ICML,
  pages={6995--7004},
  year={2019}
}

@article{bubeck2012regret,
  title={Regret analysis of stochastic and nonstochastic multi-armed bandit problems},
  author={Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={5},
  number={1},
  pages={1--122},
  year={2012},
  publisher={Now Publishers}
}

@book{cesa2006prediction,
  title={Prediction, Learning, and Games},
  author={Cesa-Bianchi, Nicolo and Lugosi, Gabor},
  year={2006},
  publisher={Cambridge}
}

@article{neu2017unified,
  title={A unified view of entropy-regularized {M}arkov decision processes},
  author={Neu, Gergely and Jonsson, Anders and G{\'o}mez, Vicen{\c{c}}},
  journal={arXiv preprint arXiv:1705.07798},
  year={2017}
}

@article{geist2019theory,
  title={A Theory of Regularized {M}arkov Decision Processes},
  author={Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  journal={arXiv preprint arXiv:1901.11275},
  year={2019}
}

@article{even2009online,
  title={Online {M}arkov decision processes},
  author={Even-Dar, Eyal and Kakade, Sham M and Mansour, Yishay},
  journal={Mathematics of Operations Research},
  volume={34},
  number={3},
  pages={726--736},
  year={2009}
}

@article{yu2009markov,
  title={{M}arkov decision processes with arbitrary reward processes},
  author={Yu, Jia Yuan and Mannor, Shie and Shimkin, Nahum},
  journal={Mathematics of Operations Research},
  volume={34},
  number={3},
  pages={737--757},
  year={2009}
}

@inproceedings{neu2010online,
  title={The Online Loop-free Stochastic Shortest-Path Problem},
  author={Neu, Gergely and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle=COLT,
  pages={231--243},
  year={2010}
}

@inproceedings{neu2010bandit,
  title={Online {M}arkov decision processes under bandit feedback},
  author={Neu, Gergely and Antos, Andr{\'a}s and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle=NIPS,
  pages={1804--1812},
  year={2010}
}

@inproceedings{zimin2013online,
  title={Online learning in episodic {M}arkovian decision processes by relative entropy policy search},
  author={Zimin, Alexander and Neu, Gergely},
  booktitle=NIPS,
  pages={1583--1591},
  year={2013}
}

@inproceedings{neu2012adversarial,
  title={The adversarial stochastic shortest path problem with unknown transition probabilities},
  author={Neu, Gergely and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle=AISTATS,
  pages={805--813},
  year={2012}
}

@article{rosenberg2019online,
  title={Online Convex Optimization in Adversarial {M}arkov Decision Processes},
  author={Rosenberg, Aviv and Mansour, Yishay},
  journal={arXiv preprint arXiv:1905.07773},
  year={2019}
}

@inproceedings{rosenberg2019bandit,
  title={Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function},
  author={Rosenberg, Aviv and Mansour, Yishay},
  booktitle=NIPS,
  pages={2209--2218},
  year={2019}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal=JMLR,
  volume={11},
  number={4},
  pages={1563--1600},
  year={2010}
}

@article{osband2014generalization,
  title={Generalization and exploration via randomized value functions},
  author={Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
  journal={arXiv preprint arXiv:1402.0635},
  year={2014}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle=ICML,
  pages={263--272},
  year={2017}
}

@inproceedings{dann2017unifying,
  title={Unifying {PAC} and regret: Uniform {PAC} bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle=NIPS,
  pages={5713--5723},
  year={2017}
}

@inproceedings{strehl2006pac,
  title={{PAC} model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={881--888},
  year={2006}
}

@inproceedings{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle=NIPS,
  pages={4863--4873},
  year={2018}
}

@article{osband2016lower,
  title={On lower bounds for regret in reinforcement learning},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1608.02732},
  year={2016}
}

@article{wen2017efficient,
  title={Efficient reinforcement learning in deterministic systems with value function generalization},
  author={Wen, Zheng and Van Roy, Benjamin},
  journal={Mathematics of Operations Research},
  volume={42},
  number={3},
  pages={762--782},
  year={2017}
}

@article{du2019provably,
  title={Provably Efficient {Q}-learning with Function Approximation via Distribution Shift Error Checking Oracle},
  author={Du, Simon S and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
  journal={arXiv preprint arXiv:1906.06321},
  year={2019}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low {B}ellman rank are {PAC}-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle=ICML,
  pages={1704--1713},
  year={2017}
}

@article{dong2019sqrt,
  title={$\sqrt{n}$-Regret for Learning in {M}arkov Decision Processes with Function Approximation and Low {B}ellman Rank},
  author={Dong, Kefan and Peng, Jian and Wang, Yining and Zhou, Yuan},
  journal={arXiv preprint arXiv:1909.02506},
  year={2019}
}

@article{lattimore2019learning,
  title={Learning with Good Feature Representations in Bandits and in {RL} with a Generative Model},
  author={Lattimore, Tor and Szepesvari, Csaba},
  journal={arXiv preprint arXiv:1911.07676},
  year={2019}
}

@article{van2019comments,
  title={Comments on the {D}u-{K}akade-{W}ang-{Y}ang Lower Bounds},
  author={Van Roy, Benjamin and Dong, Shi},
  journal={arXiv preprint arXiv:1911.07910},
  year={2019}
}

@article{du2019good,
  title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine Learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002}
}

@book{nemirovsky1983problem,
  title={Problem Complexity and Method Efficiency in Optimization.},
  author={Nemirovsky, Arkadi Semenovich and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley}
}

@article{xiao2010dual,
  title={Dual averaging methods for regularized stochastic learning and online optimization},
  author={Xiao, Lin},
  journal=JMLR,
  volume={11},
  number={Oct},
  pages={2543--2596},
  year={2010}
}

@article{boyan2002technical,
  title={Least-squares temporal difference learning},
  author={Boyan, Justin A},
  journal={Machine Learning},
  volume={49},
  number={2-3},
  pages={233--246},
  year={2002}
}

@article{bradtke1996linear,
  title={Linear least-squares algorithms for temporal difference learning},
  author={Bradtke, Steven J and Barto, Andrew G},
  journal={Machine Learning},
  volume={22},
  number={1-3},
  pages={33--57},
  year={1996}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT}
}

@inproceedings{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  booktitle=NIPS,
  pages={2312--2320},
  year={2011}
}

@inproceedings{chu2011contextual,
  title={Contextual bandits with linear payoff functions},
  author={Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle=AISTATS,
  pages={208--214},
  year={2011}
}


@article{rusmevichientong2010linearly,
  title={Linearly parameterized bandits},
  author={Rusmevichientong, Paat and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  volume={35},
  number={2},
  pages={395--411},
  year={2010}
}

@inproceedings{abbasi2019politex,
  title={{POLITEX}: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesv{\'a}ri, Csaba and Weisz, Gell{\'e}rt},
  booktitle=ICML,
  pages={3692--3702},
  year={2019}
}

@article{abbasi2019exploration,
  title={Exploration-Enhanced {POLITEX}},
  author={Abbasi-Yadkori, Yasin and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gellert},
  journal={arXiv preprint arXiv:1908.10479},
  year={2019}
}

@article{zhou2020provably,
  title={Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2006.13165},
  year={2020}
}

@article{ayoub2020model,
  title={Model-Based Reinforcement Learning with Value-Targeted Regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  journal={arXiv preprint arXiv:2006.01107},
  year={2020}
}








