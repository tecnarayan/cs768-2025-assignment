\begin{thebibliography}{10}

\bibitem{AQRY2016}
Z~Allen-Zhu, Z~Qu, P~Richt{\'a}rik, and Y~Yuan.
\newblock Even faster accelerated coordinate descent using non-uniform
  sampling.
\newblock In {\em International Conference on Machine Learning}, pages
  1110--1119, 2016.

\bibitem{AS2016}
Y~Arjevani and O~Shamir.
\newblock Dimension-free iteration complexity of finite sum optimization
  problems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3540--3548, 2016.

\bibitem{ACFS2002}
P~Auer, N~Cesa-Bianchi, Y~Freund, and R~Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock {\em SIAM journal on computing}, 32(1):48--77, 2002.

\bibitem{BKL2018}
Z~Borsos, A~Krause, and K~Levy.
\newblock Online variance reduction for stochastic optimization.
\newblock In {\em International Conference on Learning Theory}, 2018.

\bibitem{CL2011}
C~Chang and C~Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM Transactions on Intelligent Systems and Technology},
  2(3):27, 2011.

\bibitem{CQR2015}
D~Csiba, Z~Qu, and P~Richt{\'a}rik.
\newblock Stochastic dual coordinate ascent with adaptive probabilities.
\newblock In {\em International Conference on Machine Learning}, 2015.

\bibitem{DFTJ2016}
C~D\"{u}nner, S~Forte, M~Tak\'{a}\v{c}, and M~Jaggi.
\newblock Primal-dual rates and certificates.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem{DPJ2017}
C~D{\"u}nner, T~Parnell, and M~Jaggi.
\newblock Efficient use of limited-memory accelerators for linear learning on
  heterogeneous systems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4261--4270, 2017.

\bibitem{FR2015}
O~Fercoq and P~Richt{\'a}rik.
\newblock Accelerated, parallel, and proximal coordinate descent.
\newblock {\em SIAM Journal on Optimization}, 25(4):1997--2023, 2015.

\bibitem{GD2013}
T~Glasmachers and U~Dogan.
\newblock Accelerated coordinate descent with adaptive coordinate frequencies.
\newblock In {\em Asian Conference on Machine Learning}, pages 72--86, 2013.

\bibitem{JG2017}
T~Johnson and C~Guestrin.
\newblock Stingycd: Safely avoiding wasteful updates in coordinate descent.
\newblock In {\em International Conference on Machine Learning}, pages
  1752--1760, 2017.

\bibitem{NSYD2017}
H~Namkoong, A~Sinha, S~Yadlowsky, and J~Duchi.
\newblock Adaptive sampling probabilities for non-smooth optimization.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{NSLFK2015}
J~Nutini, M~Schmidt, I~Laradji, M~Friedlander, and H~Koepke.
\newblock Coordinate descent converges faster with the gauss-southwell rule
  than random selection.
\newblock In {\em International Conference on Machine Learning}, pages
  1632--1641, 2015.

\bibitem{OALDL2016}
A~Osokin, J~Alayrac, I~Lukasewitz, P~Dokania, and S~Lacoste-Julien.
\newblock Minding the gaps for block frank-wolfe optimization of structured
  svms.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem{PCJ2017}
D~Perekrestenko, V~Cevher, and M~Jaggi.
\newblock Faster coordinate descent via adaptive importance sampling.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2017.

\bibitem{RKR2017}
A~Rakotomamonjy, S~Ko{\c{c}}o, and L~Ralaivola.
\newblock Greedy methods, randomization approaches, and multiarm bandit
  algorithms for efficient sparsity-constrained optimization.
\newblock {\em IEEE transactions on neural networks and learning systems},
  28(11):2789--2802, 2017.

\bibitem{SCT2017}
F~Salehi, L.E Celis, and P~Thiran.
\newblock Stochastic optimization with bandit sampling.
\newblock {\em arXiv preprint arXiv:1708.02544v2}, 2017.

\bibitem{ST2011}
S~Shalev-Shwartz and A~Tewari.
\newblock Stochastic methods for l1-regularized loss minimization.
\newblock {\em Journal of Machine Learning Research}, 12(Jun):1865--1892, 2011.

\bibitem{SZ2013a}
S~Shalev-Shwartz and T~Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  378--385, 2013a.

\bibitem{SZ2013b}
S~Shalev-Shwartz and T~Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(Feb):567--599, 2013b.

\bibitem{STXY2016}
H~Shi, S~Tu, Y~Xu, and W~Yin.
\newblock A primer on coordinate descent algorithms.
\newblock {\em arXiv preprint arXiv:1610.00040}, 2016.

\bibitem{SRJ2017}
S~Stich, At~Raj, and M~Jaggi.
\newblock Approximate steepest coordinate descent.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{ZG2016}
A~Zhang and Q~Gu.
\newblock Accelerated stochastic block coordinate descent with optimal
  sampling.
\newblock In {\em International Conference on Knowledge Discovery and Data
  Mining}, pages 2035--2044. ACM, 2016.

\bibitem{ZZ2014}
P~Zhao and T~Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em International Conference on Machine Learning}, 2015.

\end{thebibliography}
