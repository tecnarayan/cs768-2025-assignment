\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Hanneke(2014)]{hanneke_theory_2014}
Steve Hanneke.
\newblock Theory of {Disagreement}-{Based} {Active} {Learning}.
\newblock \emph{Foundations and TrendsÂ® in Machine Learning}, 7\penalty0
  (2-3):\penalty0 131--309, 2014.
\newblock ISSN 1935-8237, 1935-8245.
\newblock \doi{10.1561/2200000037}.
\newblock URL
  \url{http://www.nowpublishers.com/articles/foundations-and-trends-in-machine-learning/MAL-037}.

\bibitem[Joshi et~al.(2009)Joshi, Porikli, and Papanikolopoulos]{joshi}
Ajay~J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.
\newblock Multi-class active learning for image classification.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 2372--2379, 2009.
\newblock \doi{10.1109/CVPR.2009.5206627}.

\bibitem[Yang et~al.(2015)Yang, Ma, Nie, Chang, and Hauptmann]{yang2015multi}
Yi~Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander~G Hauptmann.
\newblock Multi-class active learning by uncertainty sampling with diversity
  maximization.
\newblock \emph{International Journal of Computer Vision}, 113\penalty0
  (2):\penalty0 113--127, 2015.

\bibitem[Beluch et~al.(2018)Beluch, Genewein, N{\"u}rnberger, and
  K{\"o}hler]{beluch2018power}
William~H Beluch, Tim Genewein, Andreas N{\"u}rnberger, and Jan~M K{\"o}hler.
\newblock The power of ensembles for active learning in image classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9368--9377, 2018.

\bibitem[Miller et~al.(2014)Miller, Kantchelian, Afroz, Bachwani, Dauber,
  Huang, Tschantz, Joseph, and Tygar]{miller2014adversarial}
Brad Miller, Alex Kantchelian, Sadia Afroz, Rekha Bachwani, Edwin Dauber, Ling
  Huang, Michael~Carl Tschantz, Anthony~D Joseph, and J~Doug Tygar.
\newblock Adversarial active learning.
\newblock In \emph{Proceedings of the 2014 Workshop on Artificial Intelligent
  and Security Workshop}, pages 3--14, 2014.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and
  Gimpel]{hendrycks2018using}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 10477--10486, 2018.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{pmlr-v97-yu19b}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  7164--7173. PMLR, 09--15 Jun 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/yu19b.html}.

\bibitem[Gupta et~al.(2019)Gupta, Koren, and Talwar]{gupta2019better}
Anupam Gupta, Tomer Koren, and Kunal Talwar.
\newblock Better algorithms for stochastic bandits with adversarial
  corruptions.
\newblock In \emph{Conference on Learning Theory}, pages 1562--1578. PMLR,
  2019.

\bibitem[Zimmert and Seldin(2019)]{zimmert2019optimal}
Julian Zimmert and Yevgeny Seldin.
\newblock An optimal algorithm for stochastic and adversarial bandits.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 467--475. PMLR, 2019.

\bibitem[Wei et~al.(2020)Wei, Luo, and Agarwal]{wei2020taking}
Chen-Yu Wei, Haipeng Luo, and Alekh Agarwal.
\newblock Taking a hint: How to leverage loss predictors in contextual
  bandits?, 2020.

\bibitem[Lykouris et~al.(2020)Lykouris, Simchowitz, Slivkins, and
  Sun]{lykouris2020corruption}
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun.
\newblock Corruption robust exploration in episodic reinforcement learning,
  2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Du, and
  Jamieson]{chen2021improved}
Yifang Chen, Simon~S. Du, and Kevin Jamieson.
\newblock Improved corruption robust algorithms for episodic reinforcement
  learning, 2021{\natexlab{a}}.

\bibitem[Balcan et~al.(2009)Balcan, Beygelzimer, and
  Langford]{balcan2009agnostic}
Maria-Florina Balcan, Alina Beygelzimer, and John Langford.
\newblock Agnostic active learning.
\newblock \emph{Journal of Computer and System Sciences}, 75\penalty0
  (1):\penalty0 78--89, 2009.

\bibitem[Dasgupta et~al.(2007)Dasgupta, Hsu, and
  Monteleoni]{dasgupta2007general}
Sanjoy Dasgupta, Daniel~J Hsu, and Claire Monteleoni.
\newblock A general agnostic active learning algorithm.
\newblock \emph{Advances in neural information processing systems},
  20:\penalty0 353--360, 2007.

\bibitem[Settles(2011)]{settles2011theories}
Burr Settles.
\newblock From theories to queries: Active learning in practice.
\newblock In \emph{Active Learning and Experimental Design workshop In
  conjunction with AISTATS 2010}, pages 1--18, 2011.

\bibitem[Zhang and Chaudhuri(2014)]{zhang2014beyond}
Chicheng Zhang and Kamalika Chaudhuri.
\newblock Beyond disagreement-based agnostic active learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  27:\penalty0 442--450, 2014.

\bibitem[Koltchinskii(2010)]{koltchinskii2010rademacher}
Vladimir Koltchinskii.
\newblock Rademacher complexities and bounding the excess risk in active
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2457--2485, 2010.

\bibitem[Balcan et~al.(2007)Balcan, Broder, and Zhang]{balcan2007margin}
Maria-Florina Balcan, Andrei Broder, and Tong Zhang.
\newblock Margin based active learning.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 35--50. Springer, 2007.

\bibitem[Balcan and Long(2013)]{balcan2013active}
Maria-Florina Balcan and Phil Long.
\newblock Active and passive learning of linear separators under log-concave
  distributions.
\newblock In \emph{Conference on Learning Theory}, pages 288--316, 2013.

\bibitem[Huang et~al.(2015)Huang, Agarwal, Hsu, Langford, and
  Schapire]{huang2015efficient}
Tzu-Kuo Huang, Alekh Agarwal, Daniel~J Hsu, John Langford, and Robert~E
  Schapire.
\newblock Efficient and parsimonious agnostic active learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2755--2763, 2015.

\bibitem[Beygelzimer et~al.(2009)Beygelzimer, Dasgupta, and
  Langford]{beygelzimer2009importance}
Alina Beygelzimer, Sanjoy Dasgupta, and John Langford.
\newblock Importance weighted active learning.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 49--56, 2009.

\bibitem[Beygelzimer et~al.(2010)Beygelzimer, Hsu, Langford, and
  Zhang]{beygelzimer2010agnostic}
Alina Beygelzimer, Daniel~J Hsu, John Langford, and Tong Zhang.
\newblock Agnostic active learning without constraints.
\newblock In \emph{Advances in neural information processing systems}, pages
  199--207, 2010.

\bibitem[Hsu(2010)]{hsu2010algorithms}
Daniel~Joseph Hsu.
\newblock \emph{Algorithms for active learning}.
\newblock PhD thesis, UC San Diego, 2010.

\bibitem[Krishnamurthy et~al.(2017)Krishnamurthy, Agarwal, Huang,
  Daum{\'e}~III, and Langford]{krishnamurthy2017active}
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daum{\'e}~III, and John
  Langford.
\newblock Active learning for cost-sensitive classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  1915--1924. PMLR, 2017.

\bibitem[Deng et~al.(2018)Deng, Chen, Shen, and Jin]{deng2018adversarial}
Yue Deng, KaWai Chen, Yilin Shen, and Hongxia Jin.
\newblock Adversarial active learning for sequences labeling and generation.
\newblock In \emph{IJCAI}, pages 4012--4018, 2018.

\bibitem[Pi et~al.(2016)Pi, Lu, Sagduyu, and Chen]{pi2016defending}
Lei Pi, Zhuo Lu, Yalin Sagduyu, and Su~Chen.
\newblock Defending active learning against adversarial inputs in automated
  document classification.
\newblock In \emph{2016 IEEE Global Conference on Signal and Information
  Processing (GlobalSIP)}, pages 257--261. IEEE, 2016.

\bibitem[Khetan and Oh(2016)]{khetan2016achieving}
Ashish Khetan and Sewoong Oh.
\newblock Achieving budget-optimality with adaptive schemes in crowdsourcing.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 4851--4859, 2016.

\bibitem[Yang(2011)]{yang2011active}
Liu Yang.
\newblock Active learning with a drifting distribution.
\newblock In \emph{NIPS}, pages 2079--2087. Citeseer, 2011.

\bibitem[Dekel et~al.(2012)Dekel, Gentile, and Sridharan]{dekel2012selective}
Ofer Dekel, Claudio Gentile, and Karthik Sridharan.
\newblock Selective sampling and active learning from single and multiple
  teachers.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 2655--2697, 2012.

\bibitem[Hanneke and Yang(2021)]{hanneke2021toward}
Steve Hanneke and Liu Yang.
\newblock Toward a general theory of online selective sampling: Trading off
  mistakes and queries.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3997--4005. PMLR, 2021.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Luo, Ma, and
  Zhang]{chen2021active}
Yining Chen, Haipeng Luo, Tengyu Ma, and Chicheng Zhang.
\newblock Active online learning with hidden shifting domains.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2053--2061. PMLR, 2021{\natexlab{b}}.

\bibitem[Rai et~al.(2010)Rai, Saha, Daum{\'e}~III, and
  Venkatasubramanian]{rai2010domain}
Piyush Rai, Avishek Saha, Hal Daum{\'e}~III, and Suresh Venkatasubramanian.
\newblock Domain adaptation meets active learning.
\newblock In \emph{Proceedings of the NAACL HLT 2010 Workshop on Active
  Learning for Natural Language Processing}, pages 27--32, 2010.

\bibitem[Zhao et~al.(2021)Zhao, Liu, Anandkumar, and Yue]{zhao2021active}
Eric Zhao, Anqi Liu, Animashree Anandkumar, and Yisong Yue.
\newblock Active learning under label shift.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3412--3420. PMLR, 2021.

\bibitem[Lee et~al.(2021)Lee, Luo, Wei, Zhang, and Zhang]{lee2021achieving}
Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, Mengxiao Zhang, and Xiaojin Zhang.
\newblock Achieving near instance-optimality and minimax-optimality in
  stochastic and adversarial linear bandits simultaneously, 2021.

\bibitem[Camilleri et~al.(2021)Camilleri, Katz-Samuels, and
  Jamieson]{camilleri2021highdimensional}
Romain Camilleri, Julian Katz-Samuels, and Kevin Jamieson.
\newblock High-dimensional experimental design and kernel bandits, 2021.

\bibitem[Lugosi and Mendelson(2019)]{lugosi2019mean}
G{\'a}bor Lugosi and Shahar Mendelson.
\newblock Mean estimation and regression under heavy-tailed distributions: A
  survey.
\newblock \emph{Foundations of Computational Mathematics}, 19\penalty0
  (5):\penalty0 1145--1190, 2019.

\end{thebibliography}
