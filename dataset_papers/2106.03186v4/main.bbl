\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen:2019-convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora:2019-NTK-gen-bound}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora:2019-cntk}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  8139--8148, 2019{\natexlab{b}}.

\bibitem[Bengio et~al.(2006)Bengio, Delalleau, and Le~Roux]{bengio:2006-curse}
Bengio, Y., Delalleau, O., and Le~Roux, N.
\newblock The curse of highly variable functions for local kernel machines.
\newblock \emph{Advances in neural information processing systems},
  18:\penalty0 107, 2006.

\bibitem[Bengio et~al.(2007)Bengio, LeCun, et~al.]{bengio:2007-scaling}
Bengio, Y., LeCun, Y., et~al.
\newblock Scaling learning algorithms towards ai.
\newblock \emph{Large-scale kernel machines}, 34\penalty0 (5):\penalty0 1--41,
  2007.
\newblock shows there's a small deep net solution to the parity problem.

\bibitem[Bietti \& Bach(2020)Bietti and Bach]{bietti:2020-deep-equals-shallow}
Bietti, A. and Bach, F.
\newblock Deep equals shallow for relu networks in kernel regimes.
\newblock \emph{arXiv preprint arXiv:2009.14397}, 2020.

\bibitem[Bordelon et~al.(2020)Bordelon, Canatar, and
  Pehlevan]{bordelon:2020-learning-curves}
Bordelon, B., Canatar, A., and Pehlevan, C.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1024--1034. PMLR, 2020.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{bradbury:2018-jax}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao:2019-spectral-bias}
Cao, Y., Fang, Z., Wu, Y., Zhou, D.-X., and Gu, Q.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho:2009}
Cho, Y. and Saul, L.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}. Curran Associates, Inc., 2009.

\bibitem[Cortes et~al.(2012)Cortes, Mohri, and Rostamizadeh]{cortes:2012}
Cortes, C., Mohri, M., and Rostamizadeh, A.
\newblock Algorithms for learning kernels based on centered alignment.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 795--828, 2012.

\bibitem[Cristianini et~al.(2006)Cristianini, Kandola, Elisseeff, and
  Shawe-Taylor]{cristianini:2006}
Cristianini, N., Kandola, J., Elisseeff, A., and Shawe-Taylor, J.
\newblock On kernel target alignment.
\newblock In \emph{Innovations in machine learning}, pp.\  205--256. Springer,
  2006.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely:2016}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In Lee, D.~D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2253--2261, 2016.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du:2019-convergence}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Dua \& Graff(2017)Dua and Graff]{dua:2017-UCI-datasets}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Dyer \& Gur-Ari(2019)Dyer and Gur-Ari]{dyer:2019-feynman-diagrams}
Dyer, E. and Gur-Ari, G.
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock \emph{arXiv preprint arXiv:1909.11304}, 2019.

\bibitem[Hohil et~al.(1999)Hohil, Liu, and Smith]{hohil:1999}
Hohil, M.~E., Liu, D., and Smith, S.~H.
\newblock Solving the n-bit parity problem using neural networks.
\newblock \emph{Neural Networks}, 12\penalty0 (9):\penalty0 1321--1323, 1999.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl{-}Dickstein, and
  Novak]{hron:2020-transformer-ntk}
Hron, J., Bahri, Y., Sohl{-}Dickstein, J., and Novak, R.
\newblock Infinite attention: {NNGP} and {NTK} for deep attention networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume
  119 of \emph{Proceedings of Machine Learning Research}, pp.\  4376--4386.
  {PMLR}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot:2018}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  8580--8589, 2018.

\bibitem[Jacot et~al.(2020)Jacot, {\c{S}}im{\c{s}}ek, Spadaro, Hongler, and
  Gabriel]{jacot:2020-KARE}
Jacot, A., {\c{S}}im{\c{s}}ek, B., Spadaro, F., Hongler, C., and Gabriel, F.
\newblock Kernel alignment risk estimator: risk prediction from training data.
\newblock \emph{arXiv preprint arXiv:2006.09796}, 2020.

\bibitem[Kar \& Karnick(2012)Kar and Karnick]{kar:2012-rf_dot_product-kernels}
Kar, P. and Karnick, H.
\newblock Random feature maps for dot product kernels.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  583--591.
  PMLR, 2012.

\bibitem[Krizhevsky(2009)]{krizhevsky:2009}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun:2015-deep-learning}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl{-}Dickstein]{lee:2018-nngp}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl{-}Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}. OpenReview.net, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl{-}Dickstein,
  and Pennington]{lee:2019-ntk}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl{-}Dickstein,
  J., and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  8570--8581, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl{-}Dickstein]{lee:2020}
Lee, J., Schoenholz, S.~S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl{-}Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Lin et~al.(2017)Lin, Tegmark, and Rolnick]{lin:2017}
Lin, H.~W., Tegmark, M., and Rolnick, D.
\newblock Why does deep and cheap learning work so well?
\newblock \emph{Journal of Statistical Physics}, pp.\  1223â€“1247, 2017.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews:2018-nngp}
Matthews, A. G. d.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[Mhaskar et~al.(2017)Mhaskar, Liao, and Poggio]{mhaskar:2017}
Mhaskar, H., Liao, Q., and Poggio, T.~A.
\newblock When and why are deep networks better than shallow ones?
\newblock In Singh, S.~P. and Markovitch, S. (eds.), \emph{AAAI Conference on
  Artificial Intelligence}, pp.\  2343--2349. {AAAI} Press, 2017.

\bibitem[Misiakiewicz \& Mei(2021)Misiakiewicz and
  Mei]{misiakiewicz:2021-convolutional-kernels}
Misiakiewicz, T. and Mei, S.
\newblock Learning with convolution and pooling operations in kernel methods.
\newblock \emph{arXiv preprint arXiv:2111.08308}, 2021.

\bibitem[Neal(1996)]{neal:1996}
Neal, R.~M.
\newblock Priors for infinite networks.
\newblock In \emph{Bayesian Learning for Neural Networks}, pp.\  29--53.
  Springer, 1996.

\bibitem[Novak et~al.(2019{\natexlab{a}})Novak, Xiao, Bahri, Lee, Yang, Hron,
  Abolafia, Pennington, and Sohl{-}Dickstein]{novak_cnngp:2019}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl{-}Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}. OpenReview.net, 2019{\natexlab{a}}.

\bibitem[Novak et~al.(2019{\natexlab{b}})Novak, Xiao, Hron, Lee, Alemi,
  Sohl{-}Dickstein, and Schoenholz]{novak_nt:2019}
Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A.~A., Sohl{-}Dickstein, J., and
  Schoenholz, S.~S.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock \emph{CoRR}, abs/1912.02803, 2019{\natexlab{b}}.

\bibitem[Nye \& Saxe(2018)Nye and Saxe]{nye:2018}
Nye, M. and Saxe, A.
\newblock Are efficient deep representations learnable?
\newblock \emph{arXiv preprint arXiv:1807.06399}, 2018.
\newblock shows the basins for the parity problem are small.

\bibitem[O'Donnell(2014)]{odonnell:2014}
O'Donnell, R.
\newblock \emph{Analysis of boolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Pennington et~al.(2015)Pennington, Felix, and
  Kumar]{pennington:2015-rf_dot_product-kernels}
Pennington, J., Felix, X.~Y., and Kumar, S.
\newblock Spherical random features for polynomial kernels.
\newblock \emph{Advances in neural information processing systems}, 2015.

\bibitem[Poggio et~al.(2017)Poggio, Mhaskar, Rosasco, Miranda, and
  Liao]{poggio:2017}
Poggio, T.~A., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q.
\newblock Why and when can deep-but not shallow-networks avoid the curse of
  dimensionality: {A} review.
\newblock \emph{Int. J. Autom. Comput.}, 14\penalty0 (5):\penalty0 503--519,
  2017.
\newblock \doi{10.1007/s11633-017-1054-2}.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole:2016}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3360--3368. Curran Associates, Inc., 2016.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl{-}Dickstein]{raghu:2017}
Raghu, M., Poole, B., Kleinberg, J.~M., Ganguli, S., and Sohl{-}Dickstein, J.
\newblock On the expressive power of deep neural networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{International Conference
  on Machine Learning (ICML)}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pp.\  2847--2854. {PMLR}, 2017.

\bibitem[Rahimi \& Recht(2008)Rahimi and Recht]{rahimi:2008}
Rahimi, A. and Recht, B.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1313--1320. Curran Associates, Inc., 2008.

\bibitem[Rahimi et~al.(2007)Rahimi, Recht, et~al.]{rahimi:2007}
Rahimi, A., Recht, B., et~al.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~3, pp.\ ~5, 2007.

\bibitem[Ramachandran et~al.(2018)Ramachandran, Zoph, and
  Le]{ramachandran:2018}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}. OpenReview.net, 2018.

\bibitem[Rolnick \& Tegmark(2018)Rolnick and Tegmark]{rolnick:2018}
Rolnick, D. and Tegmark, M.
\newblock The power of deeper networks for expressing natural functions.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}. OpenReview.net, 2018.

\bibitem[Schoenberg(1942)]{schoenberg:1942}
Schoenberg, I.~J.
\newblock {Positive definite functions on spheres}.
\newblock \emph{Duke Mathematical Journal}, 9\penalty0 (1):\penalty0 96 -- 108,
  1942.
\newblock \doi{10.1215/S0012-7094-42-00908-6}.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl{-}Dickstein]{schoenholz:2017}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl{-}Dickstein, J.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}. OpenReview.net, 2017.

\bibitem[Shawe-Taylor et~al.(2004)Shawe-Taylor, Cristianini,
  et~al.]{shawe:2004}
Shawe-Taylor, J., Cristianini, N., et~al.
\newblock \emph{Kernel methods for pattern analysis}.
\newblock Cambridge university press, 2004.

\bibitem[Shi et~al.(2021)Shi, Chen, and Wang]{shi:2021}
Shi, X., Chen, J., and Wang, L.
\newblock Heuristic search for activation functions of neural networks based on
  gaussian processes.
\newblock In \emph{2021 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--8. IEEE, 2021.

\bibitem[Simon et~al.(2021)Simon, Dickens, and
  DeWeese]{simon:2021-eigenlearning}
Simon, J.~B., Dickens, M., and DeWeese, M.~R.
\newblock Neural tangent kernel eigenvalues accurately predict generalization.
\newblock \emph{arXiv preprint arXiv:2110.03922}, 2021.

\bibitem[Telgarsky(2016)]{telgarsky:2016}
Telgarsky, M.
\newblock Benefits of depth in neural networks.
\newblock In \emph{Conference on Learning Theory (COLT)}, volume~49 of
  \emph{{JMLR} Workshop and Conference Proceedings}, pp.\  1517--1539.
  JMLR.org, 2016.

\bibitem[Wilson \& Izmailov(2020)Wilson and Izmailov]{wilson:2020-bayesian-DL}
Wilson, A.~G. and Izmailov, P.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{arXiv preprint arXiv:2002.08791}, 2020.

\bibitem[Xiao(2021)]{xiao:2021-convnet-eigenspectra}
Xiao, L.
\newblock Eigenspace restructuring: a principle of space and frequency in
  neural networks.
\newblock \emph{arXiv preprint arXiv:2112.05611}, 2021.

\bibitem[Yang(2019)]{yang:2019-tensor-programs-I}
Yang, G.
\newblock Tensor programs {I:} wide feedforward or recurrent neural networks of
  any architecture are gaussian processes.
\newblock \emph{CoRR}, abs/1910.12478, 2019.

\bibitem[Yang \& Salman(2019)Yang and
  Salman]{yang:2019-hypercube-spectral-bias}
Yang, G. and Salman, H.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[Zavatone-Veth \& Pehlevan(2021)Zavatone-Veth and
  Pehlevan]{zavatone:2021}
Zavatone-Veth, J. and Pehlevan, C.
\newblock Exact marginal prior distributions of finite bayesian neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou:2018-convergence}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks. arxiv e-prints, art.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
