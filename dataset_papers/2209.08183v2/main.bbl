\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrieu \& Thoms(2008)Andrieu and Thoms]{andrieu2008tutorial}
Andrieu, C. and Thoms, J.
\newblock A tutorial on adaptive mcmc.
\newblock \emph{Statistics and computing}, 18\penalty0 (4):\penalty0 343--373,
  2008.

\bibitem[Beskos et~al.(2013)Beskos, Pillai, Roberts, Sanz-Serna, and
  Stuart]{beskos2013optimal}
Beskos, A., Pillai, N., Roberts, G., Sanz-Serna, J.-M., and Stuart, A.
\newblock Optimal tuning of the hybrid monte carlo algorithm.
\newblock \emph{Bernoulli}, 19\penalty0 (5A):\penalty0 1501--1534, 2013.

\bibitem[Dai et~al.(2020)Dai, Singh, Dai, Sutton, and
  Schuurmans]{dai2020learning}
Dai, H., Singh, R., Dai, B., Sutton, C., and Schuurmans, D.
\newblock Learning discrete energy-based models via auxiliary-variable local
  exploration.
\newblock \emph{arXiv preprint arXiv:2011.05363}, 2020.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Du, Y. and Mordatch, I.
\newblock Implicit generation and generalization in energy-based models.
\newblock \emph{arXiv preprint arXiv:1903.08689}, 2019.

\bibitem[Gelman et~al.(1997)Gelman, Gilks, and Roberts]{gelman1997weak}
Gelman, A., Gilks, W.~R., and Roberts, G.~O.
\newblock Weak convergence and optimal scaling of random walk metropolis
  algorithms.
\newblock \emph{The annals of applied probability}, 7\penalty0 (1):\penalty0
  110--120, 1997.

\bibitem[Gelman et~al.(2013)Gelman, Carlin, Stern, Dunson, Vehtari, and
  Rubin]{gelman2013bayesian}
Gelman, A., Carlin, J.~B., Stern, H.~S., Dunson, D.~B., Vehtari, A., and Rubin,
  D.~B.
\newblock \emph{Bayesian data analysis}.
\newblock CRC press, 2013.

\bibitem[Ghahramani \& Jordan(1995)Ghahramani and
  Jordan]{ghahramani1995factorial}
Ghahramani, Z. and Jordan, M.
\newblock Factorial hidden markov models.
\newblock \emph{Advances in Neural Information Processing Systems}, 8, 1995.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International conference on machine learning}, pp.\
  1263--1272. PMLR, 2017.

\bibitem[Girolami \& Calderhead(2011)Girolami and
  Calderhead]{girolami2011riemann}
Girolami, M. and Calderhead, B.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73\penalty0 (2):\penalty0 123--214, 2011.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Swersky, Hashemi, Duvenaud, and
  Maddison]{grathwohl2021oops}
Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., and Maddison, C.~J.
\newblock Oops i took a gradient: Scalable sampling for discrete distributions.
\newblock \emph{arXiv preprint arXiv:2102.04509}, 2021.

\bibitem[Haeusler(1988)]{haeusler1988rate}
Haeusler, E.
\newblock On the rate of convergence in the central limit theorem for
  martingales with discrete and continuous time.
\newblock \emph{The Annals of Probability}, pp.\  275--299, 1988.

\bibitem[Han et~al.(2020)Han, Ding, Liu, Torresani, Peng, and
  Liu]{han2020stein}
Han, J., Ding, F., Liu, X., Torresani, L., Peng, J., and Liu, Q.
\newblock Stein variational inference for discrete distributions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4563--4572. PMLR, 2020.

\bibitem[Hastings(1970)]{hastings1970monte}
Hastings, W.~K.
\newblock Monte carlo sampling methods using markov chains and their
  applications.
\newblock 1970.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, G.~E.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Hird et~al.(2020)Hird, Livingstone, and Zanella]{hird2020fresh}
Hird, M., Livingstone, S., and Zanella, G.
\newblock A fresh take on'barker dynamics' for mcmc.
\newblock \emph{arXiv preprint arXiv:2012.09731}, 2020.

\bibitem[Hirt et~al.(2021)Hirt, Titsias, and Dellaportas]{hirt2021entropy}
Hirt, M., Titsias, M., and Dellaportas, P.
\newblock Entropy-based adaptive hamiltonian monte carlo.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Hoffman et~al.(2021)Hoffman, Radul, and Sountsov]{hoffman2021adaptive}
Hoffman, M., Radul, A., and Sountsov, P.
\newblock An adaptive-mcmc scheme for setting trajectory lengths in hamiltonian
  monte carlo.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3907--3915. PMLR, 2021.

\bibitem[Hoffman et~al.(2014)Hoffman, Gelman, et~al.]{hoffman2014no}
Hoffman, M.~D., Gelman, A., et~al.
\newblock The no-u-turn sampler: adaptively setting path lengths in hamiltonian
  monte carlo.
\newblock \emph{J. Mach. Learn. Res.}, 15\penalty0 (1):\penalty0 1593--1623,
  2014.

\bibitem[Ising(1924)]{ising1924beitrag}
Ising, E.
\newblock \emph{Beitrag zur theorie des ferro-und paramagnetismus}.
\newblock PhD thesis, Grefe \& Tiedemann, 1924.

\bibitem[Livingstone \& Zanella(2019)Livingstone and
  Zanella]{livingstone2019barker}
Livingstone, S. and Zanella, G.
\newblock The barker proposal: combining robustness and efficiency in
  gradient-based mcmc.
\newblock \emph{arXiv preprint arXiv:1908.11812}, 2019.

\bibitem[Metropolis et~al.(1953)Metropolis, Rosenbluth, Rosenbluth, Teller, and
  Teller]{metropolis1953equation}
Metropolis, N., Rosenbluth, A.~W., Rosenbluth, M.~N., Teller, A.~H., and
  Teller, E.
\newblock Equation of state calculations by fast computing machines.
\newblock \emph{The journal of chemical physics}, 21\penalty0 (6):\penalty0
  1087--1092, 1953.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Nishimura et~al.(2017)Nishimura, Dunson, and
  Lu]{nishimura2017discontinuous}
Nishimura, A., Dunson, D., and Lu, J.
\newblock Discontinuous hamiltonian monte carlo for sampling discrete
  parameters.
\newblock \emph{arXiv preprint arXiv:1705.08510}, 853, 2017.

\bibitem[Pakman \& Paninski(2013)Pakman and Paninski]{pakman2013auxiliary}
Pakman, A. and Paninski, L.
\newblock Auxiliary-variable exact hamiltonian monte carlo samplers for binary
  distributions.
\newblock \emph{arXiv preprint arXiv:1311.2166}, 2013.

\bibitem[Power \& Goldman(2019)Power and Goldman]{power2019accelerated}
Power, S. and Goldman, J.~V.
\newblock Accelerated sampling on discrete spaces with non-reversible markov
  processes.
\newblock \emph{arXiv preprint arXiv:1912.04681}, 2019.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Robert \& Casella(2013)Robert and Casella]{robert2013monte}
Robert, C. and Casella, G.
\newblock \emph{Monte Carlo statistical methods}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Roberts(1998)]{roberts1998cube}
Roberts, G.~O.
\newblock Optimal metropolis algorithms for product measures on the vertices of
  a hypercube.
\newblock \emph{Stochastics and Stochastic Reports}, 62\penalty0
  (3-4):\penalty0 275--283, 1998.

\bibitem[Roberts \& Rosenthal(1998)Roberts and Rosenthal]{roberts1998optimal}
Roberts, G.~O. and Rosenthal, J.~S.
\newblock Optimal scaling of discrete approximations to langevin diffusions.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 60\penalty0 (1):\penalty0 255--268, 1998.

\bibitem[Roberts \& Rosenthal(2001)Roberts and Rosenthal]{roberts2001optimal}
Roberts, G.~O. and Rosenthal, J.~S.
\newblock Optimal scaling for various metropolis-hastings algorithms.
\newblock \emph{Statistical science}, 16\penalty0 (4):\penalty0 351--367, 2001.

\bibitem[Rossky et~al.(1978)Rossky, Doll, and Friedman]{rossky1978brownian}
Rossky, P.~J., Doll, J., and Friedman, H.
\newblock Brownian dynamics as smart monte carlo simulation.
\newblock \emph{The Journal of Chemical Physics}, 69\penalty0 (10):\penalty0
  4628--4633, 1978.

\bibitem[Sansone(2021)]{sansone2021lsb}
Sansone, E.
\newblock Lsb: Local self-balancing mcmc in discrete spaces.
\newblock \emph{arXiv preprint arXiv:2109.03867}, 2021.

\bibitem[Smolensky(1986)]{smolensky1986information}
Smolensky, P.
\newblock Information processing in dynamical systems: Foundations of harmony
  theory.
\newblock Technical report, Colorado Univ at Boulder Dept of Computer Science,
  1986.

\bibitem[Sun et~al.(2021)Sun, Dai, Xia, and Ramamurthy]{sun2021path}
Sun, H., Dai, H., Xia, W., and Ramamurthy, A.
\newblock Path auxiliary proposal for mcmc in discrete space.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Tai et~al.(2015)Tai, Socher, and Manning]{tai2015improved}
Tai, K.~S., Socher, R., and Manning, C.~D.
\newblock Improved semantic representations from tree-structured long
  short-term memory networks.
\newblock \emph{arXiv preprint arXiv:1503.00075}, 2015.

\bibitem[Tieleman \& Hinton(2009)Tieleman and Hinton]{tieleman2009using}
Tieleman, T. and Hinton, G.
\newblock Using fast weights to improve persistent contrastive divergence.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pp.\  1033--1040, 2009.

\bibitem[Titsias \& Dellaportas(2019)Titsias and
  Dellaportas]{titsias2019gradient}
Titsias, M. and Dellaportas, P.
\newblock Gradient-based adaptive markov chain monte carlo.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 15730--15739, 2019.

\bibitem[Titsias \& Yau(2017)Titsias and Yau]{titsias2017hamming}
Titsias, M.~K. and Yau, C.
\newblock The hamming ball sampler.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (520):\penalty0 1598--1611, 2017.

\bibitem[Vogrinc et~al.(2022)Vogrinc, Livingstone, and
  Zanella]{vogrinc2022optimal}
Vogrinc, J., Livingstone, S., and Zanella, G.
\newblock Optimal design of the barker proposal and other locally-balanced
  metropolis-hastings algorithms.
\newblock \emph{arXiv preprint arXiv:2201.01123}, 2022.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pp.\  681--688. Citeseer, 2011.

\bibitem[Zanella(2020)]{zanella2020informed}
Zanella, G.
\newblock Informed proposals for local mcmc in discrete spaces.
\newblock \emph{Journal of the American Statistical Association}, 115\penalty0
  (530):\penalty0 852--865, 2020.

\bibitem[Zhang et~al.(2012)Zhang, Ghahramani, Storkey, and
  Sutton]{zhang2012continuous}
Zhang, Y., Ghahramani, Z., Storkey, A.~J., and Sutton, C.
\newblock Continuous relaxations for discrete hamiltonian monte carlo.
\newblock \emph{Advances in Neural Information Processing Systems},
  25:\penalty0 3194--3202, 2012.

\end{thebibliography}
