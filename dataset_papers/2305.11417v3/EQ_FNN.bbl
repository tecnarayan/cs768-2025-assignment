\begin{thebibliography}{}

\bibitem[Ainsworth et~al., 2022]{ainsworth2022git}
Ainsworth, S.~K., Hayase, J., and Srinivasa, S. (2022).
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock {\em arXiv preprint arXiv:2209.04836}.

\bibitem[Allen-Zhu et~al., 2019]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z. (2019).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252. PMLR.

\bibitem[Anthony et~al., 1999]{anthony1999neural}
Anthony, M., Bartlett, P.~L., Bartlett, P.~L., et~al. (1999).
\newblock {\em Neural network learning: Theoretical foundations}, volume~9.
\newblock cambridge university press Cambridge.

\bibitem[Badrinarayanan et~al., 2015a]{badrinarayanan2015symmetry}
Badrinarayanan, V., Mishra, B., and Cipolla, R. (2015a).
\newblock Symmetry-invariant optimization in deep networks.
\newblock {\em arXiv preprint arXiv:1511.01754}.

\bibitem[Badrinarayanan et~al., 2015b]{badrinarayanan2015understanding}
Badrinarayanan, V., Mishra, B., and Cipolla, R. (2015b).
\newblock Understanding symmetries in deep networks.
\newblock {\em arXiv preprint arXiv:1511.01029}.

\bibitem[Bartlett et~al., 2017]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J. (2017).
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Bartlett et~al., 2019]{bartlett2019nearly}
Bartlett, P.~L., Harvey, N., Liaw, C., and Mehrabian, A. (2019).
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock {\em The Journal of Machine Learning Research}, 20(1):2285--2301.

\bibitem[Baum and Haussler, 1988]{baum1988size}
Baum, E. and Haussler, D. (1988).
\newblock What size net gives valid generalization?
\newblock {\em Advances in neural information processing systems}, 1.

\bibitem[Belkin et~al., 2019]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S. (2019).
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854.

\bibitem[Bona-Pellissier et~al., 2021]{bona2021parameter}
Bona-Pellissier, J., Bachoc, F., and Malgouyres, F. (2021).
\newblock Parameter identifiability of a deep feedforward relu neural network.
\newblock {\em arXiv preprint arXiv:2112.12982}.

\bibitem[Brea et~al., 2019]{brea2019weight}
Brea, J., Simsek, B., Illing, B., and Gerstner, W. (2019).
\newblock Weight-space symmetry in deep networks gives rise to permutation
  saddles, connected by equal-loss valleys across the loss landscape.
\newblock {\em arXiv preprint arXiv:1907.02911}.

\bibitem[Bui Thi~Mai and Lampert, 2020]{bui2020functional}
Bui Thi~Mai, P. and Lampert, C. (2020).
\newblock Functional vs. parametric equivalence of relu networks.
\newblock In {\em 8th International Conference on Learning Representations}.

\bibitem[Chen et~al., 1993]{chen1993geometry}
Chen, A.~M., Lu, H.-m., and Hecht-Nielsen, R. (1993).
\newblock On the geometry of feedforward neural network error surfaces.
\newblock {\em Neural computation}, 5(6):910--927.

\bibitem[Cho and Lee, 2017]{cho2017riemannian}
Cho, M. and Lee, J. (2017).
\newblock Riemannian approach to batch normalization.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Cybenko, 1989]{cybenko1989approximation}
Cybenko, G. (1989).
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314.

\bibitem[Dereich and Kassing, 2022]{dereich2022minimal}
Dereich, S. and Kassing, S. (2022).
\newblock On minimal representations of shallow relu networks.
\newblock {\em Neural Networks}, 148:121--128.

\bibitem[Devlin et~al., 2018]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}.

\bibitem[Du et~al., 2019]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International conference on machine learning}, pages
  1675--1685. PMLR.

\bibitem[Dudley, 1967]{dudley1967sizes}
Dudley, R.~M. (1967).
\newblock The sizes of compact subsets of hilbert space and continuity of
  gaussian processes.
\newblock {\em Journal of Functional Analysis}, 1(3):290--330.

\bibitem[Dudley, 2010]{dudley2010universal}
Dudley, R.~M. (2010).
\newblock Universal donsker classes and metric entropy.
\newblock In {\em Selected Works of RM Dudley}, pages 345--365. Springer.

\bibitem[Elbr{\"a}chter et~al., 2019]{elbrachter2019degenerate}
Elbr{\"a}chter, D.~M., Berner, J., and Grohs, P. (2019).
\newblock How degenerate is the parametrization of neural networks with the
  relu activation function?
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Entezari et~al., 2021]{entezari2021role}
Entezari, R., Sedghi, H., Saukh, O., and Neyshabur, B. (2021).
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock {\em arXiv preprint arXiv:2110.06296}.

\bibitem[Fang et~al., 2020]{fang2020theory}
Fang, Z., Feng, H., Huang, S., and Zhou, D.-X. (2020).
\newblock Theory of deep convolutional neural networks ii: Spherical analysis.
\newblock {\em Neural Networks}, 131:154--162.

\bibitem[Fefferman and Markel, 1993]{fefferman1993recovering}
Fefferman, C. and Markel, S. (1993).
\newblock Recovering a feed-forward net from its output.
\newblock {\em Advances in neural information processing systems}, 6.

\bibitem[Frankle and Carbin, 2019]{frankle2018the}
Frankle, J. and Carbin, M. (2019).
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Glorot and Bengio, 2010]{glorot2010understanding}
Glorot, X. and Bengio, Y. (2010).
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings.

\bibitem[Glorot et~al., 2011]{glorot2011deep}
Glorot, X., Bordes, A., and Bengio, Y. (2011).
\newblock Deep sparse rectifier neural networks.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 315--323. JMLR Workshop and
  Conference Proceedings.

\bibitem[Goldberg and Jerrum, 1993]{goldberg1993bounding}
Goldberg, P. and Jerrum, M. (1993).
\newblock Bounding the vapnik-chervonenkis dimension of concept classes
  parameterized by real numbers.
\newblock In {\em Proceedings of the sixth annual conference on Computational
  learning theory}, pages 361--369.

\bibitem[Golowich et~al., 2018]{golowich2018size}
Golowich, N., Rakhlin, A., and Shamir, O. (2018).
\newblock Size-independent sample complexity of neural networks.
\newblock In {\em Conference On Learning Theory}, pages 297--299. PMLR.

\bibitem[Grigsby et~al., 2022a]{grigsby2022local}
Grigsby, J.~E., Lindsey, K., and Masden, M. (2022a).
\newblock Local and global topological complexity measures of relu neural
  network functions.
\newblock {\em arXiv preprint arXiv:2204.06062}.

\bibitem[Grigsby et~al., 2022b]{grigsby2022functional}
Grigsby, J.~E., Lindsey, K., Meyerhoff, R., and Wu, C. (2022b).
\newblock Functional dimension of feedforward relu neural networks.
\newblock {\em arXiv preprint arXiv:2209.04036}.

\bibitem[Gunasekar et~al., 2018a]{gunasekar2018characterizing}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. (2018a).
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In {\em International Conference on Machine Learning}, pages
  1832--1841. PMLR.

\bibitem[Gunasekar et~al., 2018b]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N. (2018b).
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Hanin and Rolnick, 2019]{hanin2019deep}
Hanin, B. and Rolnick, D. (2019).
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Haussler, 1995]{haussler1995sphere}
Haussler, D. (1995).
\newblock Sphere packing numbers for subsets of the boolean n-cube with bounded
  vapnik-chervonenkis dimension.
\newblock {\em Journal of Combinatorial Theory, Series A}, 69(2):217--232.

\bibitem[He et~al., 2015]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J. (2015).
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[Hecht-Nielsen, 1990]{hecht1990algebraic}
Hecht-Nielsen, R. (1990).
\newblock On the algebraic structure of feedforward network weight spaces.
\newblock In {\em Advanced Neural Computers}, pages 129--135. Elsevier.

\bibitem[Hertrich et~al., 2021]{hertrich2021towards}
Hertrich, C., Basu, A., Di~Summa, M., and Skutella, M. (2021).
\newblock Towards lower bounds on the depth of relu neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:3336--3348.

\bibitem[Hornik, 1991]{hornik1991approximation}
Hornik, K. (1991).
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural networks}, 4(2):251--257.

\bibitem[Jiao et~al., 2023]{jiao2023deep}
Jiao, Y., Shen, G., Lin, Y., and Huang, J. (2023).
\newblock Deep nonparametric regression on approximate manifolds: Nonasymptotic
  error bounds with polynomial prefactors.
\newblock {\em The Annals of Statistics}, 51(2):691--716.

\bibitem[Jordan et~al., 2022]{jordan2022repair}
Jordan, K., Sedghi, H., Saukh, O., Entezari, R., and Neyshabur, B. (2022).
\newblock Repair: Renormalizing permuted activations for interpolation repair.
\newblock {\em arXiv preprint arXiv:2211.08403}.

\bibitem[Kohler and Langer, 2021]{kohler2021rate}
Kohler, M. and Langer, S. (2021).
\newblock On the rate of convergence of fully connected deep neural network
  regression estimates.
\newblock {\em The Annals of Statistics}, 49(4):2231--2249.

\bibitem[K{\r{u}}rkov{'a} and Kainen, 1994]{kuurkova1994functionally}
K{\r{u}}rkov{'a}, V. and Kainen, P.~C. (1994).
\newblock Functionally equivalent feedforward neural networks.
\newblock {\em Neural Computation}, 6(3):543--558.

\bibitem[Li et~al., 2018]{li2018tighter}
Li, X., Lu, J., Wang, Z., Haupt, J., and Zhao, T. (2018).
\newblock On tighter generalization bound for deep neural networks: Cnns,
  resnets, and beyond.
\newblock {\em arXiv preprint arXiv:1806.05159}.

\bibitem[Lin and Zhang, 2019]{lin2019generalization}
Lin, S. and Zhang, J. (2019).
\newblock Generalization bounds for convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1910.01487}.

\bibitem[Lu et~al., 2021a]{lu2020deep}
Lu, J., Shen, Z., Yang, H., and Zhang, S. (2021a).
\newblock Deep network approximation for smooth functions.
\newblock {\em SIAM Journal on Mathematical Analysis}, 53(5):5465--5506.

\bibitem[Lu et~al., 2021b]{lu2021deep}
Lu, J., Shen, Z., Yang, H., and Zhang, S. (2021b).
\newblock Deep network approximation for smooth functions.
\newblock {\em SIAM Journal on Mathematical Analysis}, 53(5):5465--5506.

\bibitem[Lu et~al., 2017]{lu2017expressive}
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. (2017).
\newblock The expressive power of neural networks: A view from the width.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Mao et~al., 2021]{mao2021theory}
Mao, T., Shi, Z., and Zhou, D.-X. (2021).
\newblock Theory of deep convolutional neural networks iii: Approximating
  radial functions.
\newblock {\em Neural Networks}, 144:778--790.

\bibitem[Martinelli et~al., 2023]{martinelli2023expand}
Martinelli, F., Simsek, B., Brea, J., and Gerstner, W. (2023).
\newblock Expand-and-cluster: exact parameter recovery of neural networks.
\newblock {\em arXiv preprint arXiv:2304.12794}.

\bibitem[Meng et~al., 2018]{meng2018g}
Meng, Q., Zheng, S., Zhang, H., Chen, W., Ye, Q., Ma, Z.-M., Yu, N., and Liu,
  T.-Y. (2018).
\newblock G-sgd: Optimizing relu neural networks in its positively
  scale-invariant space.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Mohri et~al., 2018]{mohri2018foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2018).
\newblock {\em Foundations of machine learning}.
\newblock MIT press.

\bibitem[Navon et~al., 2023]{pmlr-v202-navon23a}
Navon, A., Shamsian, A., Achituve, I., Fetaya, E., Chechik, G., and Maron, H.
  (2023).
\newblock Equivariant architectures for learning in deep weight spaces.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J., editors, {\em Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine
  Learning Research}, pages 25790--25816. PMLR.

\bibitem[Neyshabur, 2017]{neyshabur2017implicit}
Neyshabur, B. (2017).
\newblock Implicit regularization in deep learning.
\newblock {\em arXiv preprint arXiv:1709.01953}.

\bibitem[Neyshabur et~al., 2017a]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017a).
\newblock Exploring generalization in deep learning.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Neyshabur et~al., 2017b]{neyshabur2017pac}
Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2017b).
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock {\em arXiv preprint arXiv:1707.09564}.

\bibitem[Neyshabur et~al., 2019]{neyshabur2018the}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. (2019).
\newblock The role of over-parametrization in generalization of neural
  networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Neyshabur et~al., 2015a]{neyshabur2015path}
Neyshabur, B., Salakhutdinov, R.~R., and Srebro, N. (2015a).
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock {\em Advances in neural information processing systems}, 28.

\bibitem[Neyshabur et~al., 2014]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N. (2014).
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock {\em arXiv preprint arXiv:1412.6614}.

\bibitem[Neyshabur et~al., 2015b]{neyshabur2015norm}
Neyshabur, B., Tomioka, R., and Srebro, N. (2015b).
\newblock Norm-based capacity control in neural networks.
\newblock In {\em Conference on learning theory}, pages 1376--1401. PMLR.

\bibitem[Novak et~al., 2018]{novak2018sensitivity}
Novak, R., Bahri, Y., Abolafia, D.~A., Pennington, J., and Sohl-Dickstein, J.
  (2018).
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Ongie et~al., 2019]{ongie2019function}
Ongie, G., Willett, R., Soudry, D., and Srebro, N. (2019).
\newblock A function space view of bounded norm infinite width relu nets: The
  multivariate case.
\newblock {\em arXiv preprint arXiv:1910.01635}.

\bibitem[Petersen et~al., 2021]{petersen2021topological}
Petersen, P., Raslan, M., and Voigtlaender, F. (2021).
\newblock Topological properties of the set of functions generated by neural
  networks of fixed size.
\newblock {\em Foundations of computational mathematics}, 21:375--444.

\bibitem[Petersen and Voigtlaender, 2018]{petersen2018optimal}
Petersen, P. and Voigtlaender, F. (2018).
\newblock Optimal approximation of piecewise smooth functions using deep relu
  neural networks.
\newblock {\em Neural Networks}, 108:296--330.

\bibitem[Petzka et~al., 2020]{petzka2020notes}
Petzka, H., Trimmel, M., and Sminchisescu, C. (2020).
\newblock Notes on the symmetries of 2-layer relu-networks.
\newblock In {\em Proceedings of the northern lights deep learning workshop},
  volume~1, pages 6--6.

\bibitem[Radford et~al., 2018]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al. (2018).
\newblock Improving language understanding by generative pre-training.

\bibitem[Razin and Cohen, 2020]{razin2020implicit}
Razin, N. and Cohen, N. (2020).
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock {\em Advances in neural information processing systems},
  33:21174--21187.

\bibitem[Reddi et~al., 2019]{reddi2019convergence}
Reddi, S.~J., Kale, S., and Kumar, S. (2019).
\newblock On the convergence of adam and beyond.
\newblock {\em arXiv preprint arXiv:1904.09237}.

\bibitem[Rolnick and Kording, 2020]{rolnick2020reverse}
Rolnick, D. and Kording, K. (2020).
\newblock Reverse-engineering deep relu networks.
\newblock In {\em International Conference on Machine Learning}, pages
  8178--8187. PMLR.

\bibitem[Shang et~al., 2016]{shang2016understanding}
Shang, W., Sohn, K., Almeida, D., and Lee, H. (2016).
\newblock Understanding and improving convolutional neural networks via
  concatenated rectified linear units.
\newblock In {\em international conference on machine learning}, pages
  2217--2225. PMLR.

\bibitem[Shen et~al., 2022]{shen2022approximation}
Shen, G., Jiao, Y., Lin, Y., and Huang, J. (2022).
\newblock Approximation with cnns in sobolev space: with applications to
  classification.
\newblock {\em Advances in Neural Information Processing Systems},
  35:2876--2888.

\bibitem[Simsek et~al., 2021]{simsek2021geometry}
Simsek, B., Ged, F., Jacot, A., Spadaro, F., Hongler, C., Gerstner, W., and
  Brea, J. (2021).
\newblock Geometry of the loss landscape in overparameterized neural networks:
  Symmetries and invariances.
\newblock In {\em International Conference on Machine Learning}, pages
  9722--9732. PMLR.

\bibitem[Stock et~al., 2019]{stock2018equinormalization}
Stock, P., Graham, B., Gribonval, R., and Jégou, H. (2019).
\newblock Equi-normalization of neural networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Stock and Gribonval, 2022]{stock2022embedding}
Stock, P. and Gribonval, R. (2022).
\newblock An embedding of relu networks and an analysis of their
  identifiability.
\newblock {\em Constructive Approximation}, pages 1--47.

\bibitem[Sun, 2020]{sun2020optimization}
Sun, R.-Y. (2020).
\newblock Optimization for deep learning: An overview.
\newblock {\em Journal of the Operations Research Society of China},
  8(2):249--294.

\bibitem[Sussmann, 1992]{sussmann1992uniqueness}
Sussmann, H.~J. (1992).
\newblock Uniqueness of the weights for minimal feedforward nets with a given
  input-output map.
\newblock {\em Neural networks}, 5(4):589--593.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Yarotsky, 2017]{yarotsky2017error}
Yarotsky, D. (2017).
\newblock Error bounds for approximations with deep relu networks.
\newblock {\em Neural Networks}, 94:103--114.

\bibitem[Yarotsky, 2018]{yarotsky2018optimal}
Yarotsky, D. (2018).
\newblock Optimal approximation of continuous functions by very deep relu
  networks.
\newblock In {\em Conference on Learning Theory}, pages 639--649. PMLR.

\bibitem[Zhang et~al., 2017]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017).
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zhou, 2020a]{zhou2020theory}
Zhou, D.-X. (2020a).
\newblock Theory of deep convolutional neural networks: Downsampling.
\newblock {\em Neural Networks}, 124:319--327.

\bibitem[Zhou, 2020b]{zhou2020universality}
Zhou, D.-X. (2020b).
\newblock Universality of deep convolutional neural networks.
\newblock {\em Appl. Comput. Harmon. Anal.}, 48(2):787--794.

\end{thebibliography}
