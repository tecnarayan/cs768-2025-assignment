\begin{thebibliography}{86}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmed et~al.(2021)Ahmed, Bengio, van Seijen, and
  Courville]{ahmed2021systematic}
Ahmed, F., Bengio, Y., van Seijen, H., and Courville, A.
\newblock Systematic generalisation with group invariant predictions.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=b9PoimzZFJ}.

\bibitem[Ahuja et~al.(2020)Ahuja, Shanmugam, Varshney, and
  Dhurandhar]{ahuja2020invariant}
Ahuja, K., Shanmugam, K., Varshney, K., and Dhurandhar, A.
\newblock Invariant risk minimization games.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  145--155. PMLR, 2020.

\bibitem[Ahuja et~al.(2021)Ahuja, Wang, Dhurandhar, Shanmugam, and
  Varshney]{ahuja2020empirical}
Ahuja, K., Wang, J., Dhurandhar, A., Shanmugam, K., and Varshney, K.~R.
\newblock Empirical or invariant risk minimization? a sample complexity
  perspective.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=jrA5GAccy_}.

\bibitem[Andreas et~al.(2016)Andreas, Rohrbach, Darrell, and
  Klein]{andreas2016neural}
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D.
\newblock Neural module networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  39--48, 2016.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{Arjovsky2019InvariantRM}
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D.
\newblock Invariant risk minimization.
\newblock \emph{ArXiv}, abs/1907.02893, 2019.

\bibitem[Bahng et~al.(2020)Bahng, Chun, Yun, Choo, and Oh]{bahng2020learning}
Bahng, H., Chun, S., Yun, S., Choo, J., and Oh, S.~J.
\newblock Learning de-biased representations with biased representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  528--539. PMLR, 2020.

\bibitem[Ballard(1987)]{ballard1987modular}
Ballard, D.~H.
\newblock Modular learning in neural networks.
\newblock In \emph{AAAI}, pp.\  279--284, 1987.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Beery, S., Van~Horn, G., and Perona, P.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  pp.\  456--473, 2018.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
  J.~W.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Bengio et~al.(2019)Bengio, Deleu, Rahaman, Ke, Lachapelle, Bilaniuk,
  Goyal, and Pal]{bengio2019meta}
Bengio, Y., Deleu, T., Rahaman, N., Ke, R., Lachapelle, S., Bilaniuk, O.,
  Goyal, A., and Pal, C.
\newblock A meta-transfer objective for learning to disentangle causal
  mechanisms.
\newblock \emph{arXiv preprint arXiv:1901.10912}, 2019.

\bibitem[Brendel \& Bethge(2019)Brendel and Bethge]{brendel2019approximating}
Brendel, W. and Bethge, M.
\newblock Approximating cnns with bag-of-local-features models works
  surprisingly well on imagenet.
\newblock \emph{arXiv preprint arXiv:1904.00760}, 2019.

\bibitem[Chang et~al.(2018)Chang, Gupta, Levine, and
  Griffiths]{chang2018automatically}
Chang, M.~B., Gupta, A., Levine, S., and Griffiths, T.~L.
\newblock Automatically composing representation transformations as a means for
  generalization.
\newblock \emph{arXiv preprint arXiv:1807.04640}, 2018.

\bibitem[Chang et~al.(2020)Chang, Zhang, Yu, and Jaakkola]{chang2020invariant}
Chang, S., Zhang, Y., Yu, M., and Jaakkola, T.
\newblock Invariant rationalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1448--1458. PMLR, 2020.

\bibitem[Clune et~al.(2013)Clune, Mouret, and Lipson]{clune2013evolutionary}
Clune, J., Mouret, J.-B., and Lipson, H.
\newblock The evolutionary origins of modularity.
\newblock \emph{Proceedings of the Royal Society b: Biological sciences},
  280\penalty0 (1755):\penalty0 20122863, 2013.

\bibitem[Crammer et~al.(2008)Crammer, Kearns, and Wortman]{crammer2008learning}
Crammer, K., Kearns, M., and Wortman, J.
\newblock Learning from multiple sources.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (8), 2008.

\bibitem[Creager et~al.(2020)Creager, Jacobsen, and
  Zemel]{creager2020exchanging}
Creager, E., Jacobsen, J.-H., and Zemel, R.
\newblock Exchanging lessons between algorithmic fairness and domain
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.07249}, 2020.

\bibitem[Csord{\'a}s et~al.(2020)Csord{\'a}s, van Steenkiste, and
  Schmidhuber]{csordas2020neural}
Csord{\'a}s, R., van Steenkiste, S., and Schmidhuber, J.
\newblock Are neural nets modular? inspecting functional modularity through
  differentiable weight masks.
\newblock \emph{arXiv preprint arXiv:2010.02066}, 2020.

\bibitem[DeGrave et~al.(2020)DeGrave, Janizek, and Lee]{degrave2020ai}
DeGrave, A.~J., Janizek, J.~D., and Lee, S.-I.
\newblock Ai for radiographic covid-19 detection selects shortcuts over signal.
\newblock \emph{medRxiv}, 2020.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Dong, X., Chen, S., and Pan, S.~J.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock \emph{arXiv preprint arXiv:1705.07565}, 2017.

\bibitem[Filan et~al.(2020)Filan, Hod, Wild, Critch, and
  Russell]{filan2020neural}
Filan, D., Hod, S., Wild, C., Critch, A., and Russell, S.
\newblock Neural networks are surprisingly modular.
\newblock \emph{arXiv preprint arXiv:2003.04881}, 2020.

\bibitem[Fodor et~al.(1988)Fodor, Pylyshyn, et~al.]{fodor1988connectionism}
Fodor, J.~A., Pylyshyn, Z.~W., et~al.
\newblock Connectionism and cognitive architecture: A critical analysis.
\newblock \emph{Cognition}, 28\penalty0 (1-2):\penalty0 3--71, 1988.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020pruning}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenet}
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.~A., and
  Brendel, W.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock \emph{arXiv preprint arXiv:1811.12231}, 2018.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos2020shortcut}
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge,
  M., and Wichmann, F.~A.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (11):\penalty0
  665--673, 2020.

\bibitem[Goyal et~al.(2021)Goyal, Lamb, Hoffmann, Sodhani, Levine, Bengio, and
  Sch{\"o}lkopf]{goyal2021recurrent}
Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., and
  Sch{\"o}lkopf, B.
\newblock Recurrent independent mechanisms.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=mLcmdlEUxy-}.

\bibitem[Gulrajani \& Lopez-Paz(2020)Gulrajani and
  Lopez-Paz]{gulrajani2020search}
Gulrajani, I. and Lopez-Paz, D.
\newblock In search of lost domain generalization.
\newblock \emph{arXiv preprint arXiv:2007.01434}, 2020.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Hassibi, B. and Stork, D.~G.
\newblock \emph{Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem[Hooker et~al.(2019)Hooker, Courville, Clark, Dauphin, and
  Frome]{hooker2019compressed}
Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A.
\newblock What do compressed deep neural networks forget?
\newblock \emph{arXiv preprint arXiv:1911.05248}, 2019.

\bibitem[Hooker et~al.(2020)Hooker, Moorosi, Clark, Bengio, and
  Denton]{hooker2020characterising}
Hooker, S., Moorosi, N., Clark, G., Bengio, S., and Denton, E.
\newblock Characterising bias in compressed models.
\newblock \emph{arXiv preprint arXiv:2010.03058}, 2020.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jin et~al.(2020)Jin, Barzilay, and Jaakkola]{jin2020domain}
Jin, W., Barzilay, R., and Jaakkola, T.
\newblock Domain extrapolation via regret minimization.
\newblock \emph{arXiv preprint arXiv:2006.03908}, 2020.

\bibitem[Kamath et~al.(2021)Kamath, Tangella, Sutherland, and
  Srebro]{kamath2021does}
Kamath, P., Tangella, A., Sutherland, D.~J., and Srebro, N.
\newblock Does invariant risk minimization capture invariance?
\newblock \emph{arXiv preprint arXiv:2101.01134}, 2021.

\bibitem[Khani \& Liang(2021)Khani and Liang]{khani2021removing}
Khani, F. and Liang, P.
\newblock Removing spurious features can hurt accuracy and affect groups
  disproportionately.
\newblock In \emph{ACM Conference on Fairness, Accountability, and Transparency
  (FAccT)}, 2021.

\bibitem[Koh et~al.(2020)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Beery, et~al.]{koh2020wilds}
Koh, P.~W., Sagawa, S., Marklund, H., Xie, S.~M., Zhang, M., Balsubramani, A.,
  Hu, W., Yasunaga, M., Phillips, R.~L., Beery, S., et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock \emph{arXiv preprint arXiv:2012.07421}, 2020.

\bibitem[Koyama \& Yamaguchi(2020)Koyama and Yamaguchi]{koyama2020out}
Koyama, M. and Yamaguchi, S.
\newblock Out-of-distribution generalization with maximal invariant predictor.
\newblock \emph{arXiv preprint arXiv:2008.01883}, 2020.

\bibitem[Krueger et~al.(2020)Krueger, Caballero, Jacobsen, Zhang, Binas, Priol,
  and Courville]{Krueger2020OutofDistributionGV}
Krueger, D., Caballero, E., Jacobsen, J., Zhang, A., Binas, J., Priol, R.~L.,
  and Courville, A.~C.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock \emph{ArXiv}, abs/2003.00688, 2020.

\bibitem[LeCun et~al.(1989)LeCun, Denker, Solla, Howard, and
  Jackel]{lecun1989optimal}
LeCun, Y., Denker, J.~S., Solla, S.~A., Howard, R.~E., and Jackel, L.~D.
\newblock Optimal brain damage.
\newblock In \emph{NIPs}, volume~2, pp.\  598--605. Citeseer, 1989.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.~H.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Li et~al.(2021)Li, Yu, Tan, Mei, Tang, Shen, Yuille, and cihang
  xie]{li2021shapetexture}
Li, Y., Yu, Q., Tan, M., Mei, J., Tang, P., Shen, W., Yuille, A., and cihang
  xie.
\newblock Shape-texture debiased neural network training.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Db4yerZTYkz}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, and
  Zitnick]{Lin2014Microsoft}
Lin, T.~Y., Maire, M., Belongie, S., Hays, J., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{European Conference on Computer Vision}, 2014.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and
  Kingma]{Louizos2018LearningSN}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through l0 regularization.
\newblock \emph{ArXiv}, abs/1712.01312, 2018.

\bibitem[Mallya \& Lazebnik(2018)Mallya and Lazebnik]{mallya2018packnet}
Mallya, A. and Lazebnik, S.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  7765--7773, 2018.

\bibitem[Marcus(1998)]{marcus1998rethinking}
Marcus, G.~F.
\newblock Rethinking eliminative connectionism.
\newblock \emph{Cognitive psychology}, 37\penalty0 (3):\penalty0 243--282,
  1998.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Mostafa, H. and Wang, X.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4646--4655. PMLR, 2019.

\bibitem[Motiian et~al.(2017)Motiian, Piccirilli, Adjeroh, and
  Doretto]{motiian2017unified}
Motiian, S., Piccirilli, M., Adjeroh, D.~A., and Doretto, G.
\newblock Unified deep supervised domain adaptation and generalization.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5715--5725, 2017.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{1989Skeletonization}
Mozer, M.~C. and Smolensky, P.
\newblock \emph{Skeletonization: A Technique for Trimming the Fat from a
  Network via Relevance Assessment}.
\newblock Morgan Kaufmann Publishers Inc., 1989.

\bibitem[Muandet et~al.(2013)Muandet, Balduzzi, and
  Sch{\"o}lkopf]{muandet2013domain}
Muandet, K., Balduzzi, D., and Sch{\"o}lkopf, B.
\newblock Domain generalization via invariant feature representation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10--18.
  PMLR, 2013.

\bibitem[M{\"u}ller et~al.(2020)M{\"u}ller, Schmier, Ardizzone, Rother, and
  K{\"o}the]{muller2020learning}
M{\"u}ller, J., Schmier, R., Ardizzone, L., Rother, C., and K{\"o}the, U.
\newblock Learning robust models using the principle of independent causal
  mechanisms.
\newblock \emph{arXiv preprint arXiv:2010.07167}, 2020.

\bibitem[Nagarajan et~al.(2020)Nagarajan, Andreassen, and
  Neyshabur]{nagarajan2020understanding}
Nagarajan, V., Andreassen, A., and Neyshabur, B.
\newblock Understanding the failure modes of out-of-distribution
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.15775}, 2020.

\bibitem[Nam et~al.(2020)Nam, Cha, Ahn, Lee, and Shin]{nam2020learning}
Nam, J., Cha, H., Ahn, S., Lee, J., and Shin, J.
\newblock Learning from failure: Training debiased classifier from biased
  classifier.
\newblock \emph{arXiv preprint arXiv:2007.02561}, 2020.

\bibitem[Newman(2006)]{newman2006modularity}
Newman, M.~E.
\newblock Modularity and community structure in networks.
\newblock \emph{Proceedings of the national academy of sciences}, 103\penalty0
  (23):\penalty0 8577--8582, 2006.

\bibitem[Parascandolo et~al.(2020)Parascandolo, Neitz, Orvieto, Gresele, and
  Sch{\"o}lkopf]{parascandolo2020learning}
Parascandolo, G., Neitz, A., Orvieto, A., Gresele, L., and Sch{\"o}lkopf, B.
\newblock Learning explanations that are hard to vary.
\newblock \emph{arXiv preprint arXiv:2009.00329}, 2020.

\bibitem[Peters et~al.(2016)Peters, B{\"u}hlmann, and
  Meinshausen]{peters2016causal}
Peters, J., B{\"u}hlmann, P., and Meinshausen, N.
\newblock Causal inference by using invariant prediction: identification and
  confidence intervals.
\newblock \emph{Journal of the Royal Statistical Society. Series B (Statistical
  Methodology)}, pp.\  947--1012, 2016.

\bibitem[Pezeshki et~al.(2020)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2020gradient}
Pezeshki, M., Kaba, S.-O., Bengio, Y., Courville, A., Precup, D., and Lajoie,
  G.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{arXiv preprint arXiv:2011.09468}, 2020.

\bibitem[Priol et~al.(2020)Priol, Harikandeh, Bengio, and
  Lacoste-Julien]{priol2020analysis}
Priol, R.~L., Harikandeh, R.~B., Bengio, Y., and Lacoste-Julien, S.
\newblock An analysis of the adaptation speed of causal models.
\newblock \emph{arXiv preprint arXiv:2005.09136}, 2020.

\bibitem[Ritter et~al.(2017)Ritter, Barrett, Santoro, and
  Botvinick]{ritter2017cognitive}
Ritter, S., Barrett, D.~G., Santoro, A., and Botvinick, M.~M.
\newblock Cognitive psychology for deep neural networks: A shape bias case
  study.
\newblock In \emph{International conference on machine learning}, pp.\
  2940--2949. PMLR, 2017.

\bibitem[Rojas-Carulla et~al.(2018)Rojas-Carulla, Sch{\"o}lkopf, Turner, and
  Peters]{rojas2018invariant}
Rojas-Carulla, M., Sch{\"o}lkopf, B., Turner, R., and Peters, J.
\newblock Invariant models for causal transfer learning.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1309--1342, 2018.

\bibitem[Rosenfeld et~al.(2020)Rosenfeld, Ravikumar, and
  Risteski]{rosenfeld2020risks}
Rosenfeld, E., Ravikumar, P., and Risteski, A.
\newblock The risks of invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:2010.05761}, 2020.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Sagawa et~al.(2020)Sagawa, Raghunathan, Koh, and
  Liang]{sagawa2020investigation}
Sagawa, S., Raghunathan, A., Koh, P.~W., and Liang, P.
\newblock An investigation of why overparameterization exacerbates spurious
  correlations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8346--8356. PMLR, 2020.

\bibitem[Sauer \& Geiger(2021)Sauer and Geiger]{sauer2021counterfactual}
Sauer, A. and Geiger, A.
\newblock Counterfactual generative networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=BXewfAYMmJw}.

\bibitem[Sch{\"o}lkopf et~al.(2012)Sch{\"o}lkopf, Janzing, Peters, Sgouritsa,
  Zhang, and Mooij]{scholkopf2012causal}
Sch{\"o}lkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and
  Mooij, J.
\newblock On causal and anticausal learning.
\newblock \emph{arXiv preprint arXiv:1206.6471}, 2012.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock \emph{arXiv preprint arXiv:2006.07710}, 2020.

\bibitem[Shi et~al.(2020)Shi, Zhang, Dai, Zhu, Mu, and
  Wang]{shi2020informative}
Shi, B., Zhang, D., Dai, Q., Zhu, Z., Mu, Y., and Wang, J.
\newblock Informative dropout for robust representation learning: A shape-bias
  perspective.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8828--8839. PMLR, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Su et~al.(2020)Su, Chen, Cai, Wu, Gao, Wang, and Lee]{su2020sanity}
Su, J., Chen, Y., Cai, T., Wu, T., Gao, R., Wang, L., and Lee, J.~D.
\newblock Sanity-checking pruning methods: Random tickets can win the jackpot.
\newblock \emph{arXiv preprint arXiv:2009.11094}, 2020.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2019robustness}
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A.
\newblock Robustness may be at odds with accuracy, 2019.

\bibitem[Vapnik(1999)]{vapnik1999overview}
Vapnik, V.~N.
\newblock An overview of statistical learning theory.
\newblock \emph{IEEE transactions on neural networks}, 10\penalty0
  (5):\penalty0 988--999, 1999.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{wang2020picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock \emph{arXiv preprint arXiv:2002.07376}, 2020.

\bibitem[Wang et~al.(2019)Wang, He, Lipton, and Xing]{wang2019learning}
Wang, H., He, Z., Lipton, Z.~C., and Xing, E.~P.
\newblock Learning robust representations by projecting superficial statistics
  out.
\newblock \emph{arXiv preprint arXiv:1903.06256}, 2019.

\bibitem[Watanabe et~al.(2019)Watanabe, Hiramatsu, and
  Kashino]{watanabe2019understanding}
Watanabe, C., Hiramatsu, K., and Kashino, K.
\newblock Understanding community structure in layered neural networks.
\newblock \emph{Neurocomputing}, 367:\penalty0 84--102, 2019.

\bibitem[Xie et~al.(2020{\natexlab{a}})Xie, Chen, Liu, and Li]{xie2020risk}
Xie, C., Chen, F., Liu, Y., and Li, Z.
\newblock Risk variance penalization: From distributional robustness to
  causality.
\newblock \emph{arXiv preprint arXiv:2006.07544}, 2020{\natexlab{a}}.

\bibitem[Xie et~al.(2020{\natexlab{b}})Xie, Kumar, Jones, Khani, Ma, and
  Liang]{xie2020n}
Xie, S.~M., Kumar, A., Jones, R., Khani, F., Ma, T., and Liang, P.
\newblock In-n-out: Pre-training and self-training using auxiliary information
  for out-of-distribution robustness.
\newblock \emph{arXiv preprint arXiv:2012.04550}, 2020{\natexlab{b}}.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zeiler \& Fergus(2014)Zeiler and Fergus]{zeiler2014visualizing}
Zeiler, M.~D. and Fergus, R.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European conference on computer vision}, pp.\  818--833.
  Springer, 2014.

\bibitem[Zeng \& Urtasun(2018)Zeng and Urtasun]{zeng2018mlprune}
Zeng, W. and Urtasun, R.
\newblock Mlprune: Multi-layer pruning for automated neural network
  compression.
\newblock 2018.

\bibitem[Zhou et~al.(2018)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{Zhou2018Places}
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE Trans Pattern Anal Mach Intell}, pp.\  1--1, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019deconstructing}
Zhou, H., Lan, J., Liu, R., and Yosinski, J.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3597--3607, 2019.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}, 2017.

\end{thebibliography}
