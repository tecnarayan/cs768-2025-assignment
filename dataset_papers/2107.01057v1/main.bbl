\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bansal et~al.(2019)Bansal, Nushi, Kamar, Weld, Lasecki, and
  Horvitz]{bansal2019updates}
Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel~S Weld, Walter~S Lasecki, and
  Eric Horvitz.
\newblock Updates in human-{AI} teams: Understanding and addressing the
  performance/compatibility tradeoff.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, 2019.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{barbu2019objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
  Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Berend and Kontorovich(2015)]{berend2015}
Daniel Berend and Aryeh Kontorovich.
\newblock A finite sample analysis of the naive bayes classifier.
\newblock \emph{J. Mach. Learn. Res.}, 16\penalty0 (1), 2015.

\bibitem[Beyer et~al.(2020)Beyer, H{\'e}naff, Kolesnikov, Zhai, and
  Oord]{beyer2020we}
Lucas Beyer, Olivier~J H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and
  A{\"a}ron van~den Oord.
\newblock Are we done with imagenet?
\newblock \emph{arXiv preprint arXiv:2006.07159}, 2020.

\bibitem[Boland et~al.(1989)Boland, Proschan, and Tong]{boland1989}
Philip~J Boland, Frank Proschan, and Yung~Liang Tong.
\newblock Modelling dependence in simple and indirect majority systems.
\newblock \emph{Journal of Applied Probability}, 1989.

\bibitem[Breiman(1996)]{bagging}
Leo Breiman.
\newblock Bagging predictors.
\newblock \emph{Machine learning}, 24\penalty0 (2), 1996.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}. Ieee, 2009.

\bibitem[Der~Kiureghian and Ditlevsen(2009)]{der2009aleatory}
Armen Der~Kiureghian and Ove Ditlevsen.
\newblock Aleatory or epistemic? does it matter?
\newblock \emph{Structural safety}, 31\penalty0 (2), 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock 2021.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock 2021.

\bibitem[Freund and Schapire(1997)]{adaboost}
Yoav Freund and Robert~E Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0 (1),
  1997.

\bibitem[Ganin and Lempitsky(2015)]{ganin2015unsupervised}
Yaroslav Ganin and Victor Lempitsky.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International conference on machine learning}. PMLR, 2015.

\bibitem[Good(1953)]{good1953population}
Irving~J Good.
\newblock The population frequencies of species and the estimation of
  population parameters.
\newblock \emph{Biometrika}, 40\penalty0 (3-4), 1953.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2016.

\bibitem[Hoeting et~al.(1999)Hoeting, Madigan, Raftery, and
  Volinsky]{hoeting1999}
Jennifer~A Hoeting, David Madigan, Adrian~E Raftery, and Chris~T Volinsky.
\newblock Bayesian model averaging: a tutorial.
\newblock \emph{Statistical science}, 1999.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2017.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and $<$
  0.5 mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Kendall and Gal(2017)]{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in {B}ayesian deep learning for
  computer vision?
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, 2017.

\bibitem[Kim and Ghahramani(2012)]{kim2012bayesian}
Hyun-Chul Kim and Zoubin Ghahramani.
\newblock {B}ayesian classifier combination.
\newblock In \emph{Artificial Intelligence and Statistics}, 2012.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 2012.

\bibitem[Kuncheva(2014)]{kuncheva2014}
Ludmila~I Kuncheva.
\newblock \emph{Combining pattern classifiers: methods and algorithms}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Lindley(1956)]{lindley1956measure}
Dennis~V Lindley.
\newblock On a measure of the information provided by an experiment.
\newblock \emph{The Annals of Mathematical Statistics}, 1956.

\bibitem[MacKay(1995)]{mackay1995bayesian}
David~JC MacKay.
\newblock {B}ayesian neural networks and density networks.
\newblock \emph{Nuclear Instruments and Methods in Physics Research Section A:
  Accelerators, Spectrometers, Detectors and Associated Equipment},
  354\penalty0 (1), 1995.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2015.

\bibitem[Nitzan and Paroush(1982)]{nitzan1982}
Shmuel Nitzan and Jacob Paroush.
\newblock Optimal decision rules in uncertain dichotomous choice situations.
\newblock \emph{International Economic Review}, 1982.

\bibitem[Pan and Yang(2009)]{pan2009survey}
Sinno~Jialin Pan and Qiang Yang.
\newblock A survey on transfer learning.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  22\penalty0 (10), 2009.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Pham et~al.(2020)Pham, Xie, Dai, and Le]{pham2020meta}
Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc~V Le.
\newblock Meta pseudo labels.
\newblock \emph{arXiv preprint arXiv:2003.10580}, 2020.

\bibitem[Phan(2021)]{huyphan20214431043}
Huy Phan.
\newblock huyvnphan/pytorch\_cifar10, jan 2021.

\bibitem[Pineau et~al.(2018)Pineau, Fried, Ke, and Larochelle]{pineau2018iclr}
Joelle Pineau, Genevieve Fried, Rosemary~Nan Ke, and Hugo Larochelle.
\newblock {ICLR}2018 reproducibility challenge.
\newblock In \emph{ICLR workshop on Reproducibility in Machine Learning}, 2018.

\bibitem[Pineau et~al.(2019)Pineau, Sinha, Fried, Ke, and
  Larochelle]{pineau2019iclr}
Joelle Pineau, Koustuv Sinha, Genevieve Fried, Rosemary~Nan Ke, and Hugo
  Larochelle.
\newblock {ICLR} reproducibility challenge 2019.
\newblock \emph{ReScience C}, 5\penalty0 (2), 2019.

\bibitem[Robbins(1956)]{robbins1956empirical}
Herbert Robbins.
\newblock An empirical {B}ayes approach to statistics.
\newblock In \emph{Proc. 3rd Berkeley Symp. Math. Statist. Probab., 1956},
  volume~1, 1956.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2 inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2018.

\bibitem[Settles(2009)]{settles_survey}
Burr Settles.
\newblock Active learning literature survey.
\newblock \emph{University of Wisconsin, Madison, Department of Computer
  Sciences}, 2009.

\bibitem[Shannon(1948)]{shannon1948mathematical}
Claude~E Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27\penalty0 (3), 1948.

\bibitem[Shen et~al.(2020)Shen, Xiong, Xia, and Soatto]{shen2020towards}
Yantao Shen, Yuanjun Xiong, Wei Xia, and Stefano Soatto.
\newblock Towards backward-compatible representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2020.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2), 2000.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Sinha et~al.(2020)Sinha, Pineau, Forde, Ke, and
  Larochelle]{sinha2020neurips}
Koustuv Sinha, Joelle Pineau, Jessica Forde, Rosemary~Nan Ke, and Hugo
  Larochelle.
\newblock Neurips 2019 reproducibility challenge.
\newblock \emph{ReScience C}, 6\penalty0 (2), 2020.

\bibitem[Srivastava et~al.(2020)Srivastava, Nushi, Kamar, Shah, and
  Horvitz]{srivastava2020empirical}
Megha Srivastava, Besmira Nushi, Ece Kamar, Shital Shah, and Eric Horvitz.
\newblock An empirical analysis of backward compatibility in machine learning
  systems.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, 2020.

\bibitem[Sugiyama and Kawanabe(2012)]{sugiyama2012machine}
Masashi Sugiyama and Motoaki Kawanabe.
\newblock \emph{Machine learning in non-stationary environments: Introduction
  to covariate shift adaptation}.
\newblock MIT press, 2012.

\bibitem[Sutton and Barto(2018)]{sutton_barto_book}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2016.

\bibitem[Touvron et~al.(2019)Touvron, Vedaldi, Douze, and
  J{\'e}gou]{touvron2020fixing}
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Fixing the train-test resolution discrepancy: Fixefficientnet.
\newblock \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2020.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2017.

\bibitem[Yan et~al.(2021)Yan, Xiong, Kundu, Yang, Deng, Wang, Xia, and
  Soatto]{yan2020positive}
Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng Wang, Wei
  Xia, and Stefano Soatto.
\newblock Positive-congruent training: Towards regression-free model updates.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2021.

\bibitem[Yan et~al.(2016)Yan, Chaudhuri, and Javidi]{yan2016}
Songbai Yan, Kamalika Chaudhuri, and Tara Javidi.
\newblock Active learning from imperfect labelers.
\newblock \emph{Advances in Neural Information Processing Systems 29}, 2016.

\end{thebibliography}
