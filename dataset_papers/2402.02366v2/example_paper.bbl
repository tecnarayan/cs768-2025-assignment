\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Bonnet et~al.(2022)Bonnet, Mazari, Cinnella, and patrick gallinari]{bonnet2022airfrans}
Bonnet, F., Mazari, J.~A., Cinnella, P., and patrick gallinari.
\newblock Airf{RANS}: High fidelity computational fluid dynamics dataset for approximating reynolds-averaged navier{\textendash}stokes solutions.
\newblock In \emph{NeurIPS Datasets and Benchmarks Track}, 2022.

\bibitem[Bronstein et~al.(2017)Bronstein, Bruna, LeCun, Szlam, and Vandergheynst]{bronstein2017geometric}
Bronstein, M.~M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P.
\newblock Geometric deep learning: going beyond euclidean data.
\newblock \emph{IEEE Signal Processing Magazine}, 2017.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{NEURIPS2020_1457c0d6}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Bungartz \& Sch{\"a}fer(2006)Bungartz and Sch{\"a}fer]{bungartz2006fluid}
Bungartz, H.-J. and Sch{\"a}fer, M.
\newblock \emph{Fluid-structure interaction: modelling, simulation, optimisation}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Cao(2021)]{Cao2021ChooseAT}
Cao, S.
\newblock Choose a transformer: Fourier or galerkin.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Chang et~al.(2015)Chang, Funkhouser, Guibas, Hanrahan, Huang, Li, Savarese, Savva, Song, Su, et~al.]{chang2015shapenet}
Chang, A.~X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et~al.
\newblock Shapenet: An information-rich 3d model repository.
\newblock \emph{arXiv preprint arXiv:1512.03012}, 2015.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Choromanski et~al.(2021{\natexlab{a}})Choromanski, Likhosherstov, Dohan, Song, Gane, Sarl{\'o}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{performer}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl{\'o}s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Choromanski et~al.(2021{\natexlab{b}})Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{choromanski2021rethinking}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., Belanger, D.~B., Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Deng et~al.(2024)Deng, Li, Xiong, Hu, and Ma]{anonymous2023geometryguided}
Deng, J., Li, X., Xiong, H., Hu, X., and Ma, J.
\newblock Geometry-guided conditional adaption for surrogate models of large-scale 3d {PDE}s on arbitrary geometries.
\newblock In \emph{IJCAI}, 2024.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin2019BERTPO}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Dym et~al.(1973)Dym, Shames, et~al.]{dym1973solid}
Dym, C.~L., Shames, I.~H., et~al.
\newblock \emph{Solid mechanics}.
\newblock Springer, 1973.

\bibitem[Gao \& Ji(2019)Gao and Ji]{gao2019graph}
Gao, H. and Ji, S.
\newblock Graph u-nets.
\newblock In \emph{ICML}, 2019.

\bibitem[Goswami et~al.(2022)Goswami, Kontolati, Shields, and Karniadakis]{goswami2022deep}
Goswami, S., Kontolati, K., Shields, M.~D., and Karniadakis, G.~E.
\newblock Deep transfer operator learning for partial differential equations under conditional shift.
\newblock \emph{Nat. Mach. Intell}, 2022.

\bibitem[Gottlieb \& Orszag(1977)Gottlieb and Orszag]{gottlieb1977numerical}
Gottlieb, D. and Orszag, S.~A.
\newblock \emph{Numerical analysis of spectral methods: theory and applications}.
\newblock SIAM, 1977.

\bibitem[Gupta et~al.(2021)Gupta, Xiao, and Bogdan]{Gupta2021MultiwaveletbasedOL}
Gupta, G., Xiao, X., and Bogdan, P.
\newblock Multiwavelet-based operator learning for differential equations.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{hamilton2017inductive}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Hao et~al.(2023)Hao, Ying, Wang, Su, Dong, Liu, Cheng, Zhu, and Song]{hao2023gnot}
Hao, Z., Ying, C., Wang, Z., Su, H., Dong, Y., Liu, S., Cheng, Z., Zhu, J., and Song, J.
\newblock Gnot: A general neural operator transformer for operator learning.
\newblock \emph{ICML}, 2023.

\bibitem[Hubbert(1956)]{hubbert1956darcy}
Hubbert, M.~K.
\newblock Darcy's law and the field equations of the flow of underground fluids.
\newblock \emph{Transactions of the AIME}, 1956.

\bibitem[Karniadakis et~al.(2021)Karniadakis, Kevrekidis, Lu, Perdikaris, Wang, and Yang]{karniadakis2021physics}
Karniadakis, G.~E., Kevrekidis, I.~G., Lu, L., Perdikaris, P., Wang, S., and Yang, L.
\newblock Physics-informed machine learning.
\newblock \emph{Nat. Rev. Phys.}, 2021.

\bibitem[Katharopoulos et~al.(2020{\natexlab{a}})Katharopoulos, Vyas, Pappas, and Fleuret]{Katharopoulos2020TransformersAR}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Katharopoulos et~al.(2020{\natexlab{b}})Katharopoulos, Vyas, Pappas, and Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{DBLP:journals/corr/KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{ICLR}, 2020.

\bibitem[Kovachki et~al.(2023)Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar]{jmlr_operator}
Kovachki, N., Li, Z., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A., and Anandkumar, A.
\newblock Neural operator: Learning maps between function spaces with applications to pdes.
\newblock \emph{JMLR}, 2023.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{li2020neural}
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A.
\newblock Neural operator: Graph kernel network for partial differential equations.
\newblock \emph{arXiv preprint arXiv:2003.03485}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2021)Li, Kovachki, Azizzadenesheli, liu, Bhattacharya, Stuart, and Anandkumar]{li2021fourier}
Li, Z., Kovachki, N.~B., Azizzadenesheli, K., liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A.
\newblock Fourier neural operator for parametric partial differential equations.
\newblock In \emph{ICLR}, 2021.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Kovachki, Choy, Li, Kossaifi, Otta, Nabian, Stadler, Hundt, Azizzadenesheli, and Anandkumar]{li2023geometryinformed}
Li, Z., Kovachki, N.~B., Choy, C., Li, B., Kossaifi, J., Otta, S.~P., Nabian, M.~A., Stadler, M., Hundt, C., Azizzadenesheli, K., and Anandkumar, A.
\newblock Geometry-informed neural operator for large-scale 3d {PDE}s.
\newblock In \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Kovachki, Choy, Li, Kossaifi, Otta, Nabian, Stadler, Hundt, Azizzadenesheli, et~al.]{li2023geometry}
Li, Z., Kovachki, N.~B., Choy, C., Li, B., Kossaifi, J., Otta, S.~P., Nabian, M.~A., Stadler, M., Hundt, C., Azizzadenesheli, K., et~al.
\newblock Geometry-informed neural operator for large-scale 3d pdes.
\newblock \emph{arXiv preprint arXiv:2309.00583}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Meidani, and Farimani]{li2023transformer}
Li, Z., Meidani, K., and Farimani, A.~B.
\newblock Transformer for partial differential equations{\textquoteright} operator learning.
\newblock \emph{TMLR}, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Shu, and Farimani]{li2023scalable}
Li, Z., Shu, D., and Farimani, A.~B.
\newblock Scalable transformer for pde surrogate modeling.
\newblock \emph{NeurIPS}, 2023{\natexlab{d}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{Li2020NeuralOG}
Li, Z.-Y., Kovachki, N.~B., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A.
\newblock Neural operator: Graph kernel network for partial differential equations.
\newblock \emph{arXiv preprint arXiv:2003.03485}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Huang, Liu, and Anandkumar]{Li2022FourierNO}
Li, Z.-Y., Huang, D.~Z., Liu, B., and Anandkumar, A.
\newblock Fourier neural operator with learned deformations for pdes on general geometries.
\newblock \emph{arXiv preprint arXiv:2207.05209}, 2022.

\bibitem[Liu et~al.(2022)Liu, Xu, and Zhang]{anonymous2023htnet}
Liu, X., Xu, B., and Zhang, L.
\newblock {HT}-net: Hierarchical transformer based operator learning model for multiscale {PDE}s.
\newblock \emph{arXiv preprint arXiv:2210.10890}, 2022.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021Swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S. C.-F., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock \emph{ICCV}, 2021.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Lu et~al.(2021)Lu, Jin, Pang, Zhang, and Karniadakis]{lu2021learning}
Lu, L., Jin, P., Pang, G., Zhang, Z., and Karniadakis, G.~E.
\newblock Learning nonlinear operators via deeponet based on the universal approximation theorem of operators.
\newblock \emph{Nat. Mach. Intell}, 2021.

\bibitem[McCormick(1994)]{mccormick1994aerodynamics}
McCormick, B.~W.
\newblock \emph{Aerodynamics, aeronautics, and flight mechanics}.
\newblock John Wiley \& Sons, 1994.

\bibitem[McLean(2012)]{mclean2012continuum}
McLean, D.
\newblock Continuum fluid mechanics and the navier-stokes equations.
\newblock \emph{Understanding Aerodynamics: Arguing from the Real Physics}, 2012.

\bibitem[Pfaff et~al.(2021)Pfaff, Fortunato, Sanchez-Gonzalez, and Battaglia]{pfaff2021learning}
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and Battaglia, P.
\newblock Learning mesh-based simulation with graph networks.
\newblock In \emph{ICLR}, 2021.

\bibitem[Qi et~al.(2017)Qi, Su, Mo, and Guibas]{qi2017pointnet}
Qi, C.~R., Su, H., Mo, K., and Guibas, L.~J.
\newblock Pointnet: Deep learning on point sets for 3d classification and segmentation.
\newblock In \emph{CVPR}, 2017.

\bibitem[Rahman et~al.(2023)Rahman, Ross, and Azizzadenesheli]{rahman2022u}
Rahman, M.~A., Ross, Z.~E., and Azizzadenesheli, K.
\newblock U-no: U-shaped neural operators.
\newblock \emph{TMLR}, 2023.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and Karniadakis]{Raissi2019PhysicsinformedNN}
Raissi, M., Perdikaris, P., and Karniadakis, G.~E.
\newblock Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.
\newblock \emph{J. Comput. Phys.}, 2019.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and Brox]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{MICCAI}, 2015.

\bibitem[Roub{\'\i}{\v{c}}ek(2013)]{roubivcek2013nonlinear}
Roub{\'\i}{\v{c}}ek, T.
\newblock \emph{Nonlinear partial differential equations with applications}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Sanchez-Gonzalez et~al.(2020)Sanchez-Gonzalez, Godwin, Pfaff, Ying, Leskovec, and Battaglia]{sanchez2020learning}
Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and Battaglia, P.
\newblock Learning to simulate complex physics with graph networks.
\newblock In \emph{International conference on machine learning}, 2020.

\bibitem[{\^S}ol{\'\i}n(2005)]{solin2005partial}
{\^S}ol{\'\i}n, P.
\newblock \emph{Partial differential equations and the finite element method}.
\newblock John Wiley \& Sons, 2005.

\bibitem[Spearman(1961)]{spearman1961proof}
Spearman, C.
\newblock The proof and measurement of association between two things.
\newblock 1961.

\bibitem[Tran et~al.(2023)Tran, Mathews, Xie, and Ong]{anonymous2023factorized}
Tran, A., Mathews, A., Xie, L., and Ong, C.~S.
\newblock Factorized fourier neural operators.
\newblock In \emph{ICLR}, 2023.

\bibitem[Trockman \& Kolter(2022)Trockman and Kolter]{trockman2022patches}
Trockman, A. and Kolter, J.~Z.
\newblock Patches are all you need?
\newblock \emph{arXiv preprint arXiv:2201.09792}, 2022.

\bibitem[Umetani \& Bickel(2018)Umetani and Bickel]{umetani2018learning}
Umetani, N. and Bickel, B.
\newblock Learning three-dimensional flow for interactive aerodynamic design.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{NIPS2017_3f5ee243}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2023)Wang, Fu, Du, Gao, Huang, Liu, Chandak, Liu, Van~Katwyk, Deac, et~al.]{wang2023scientific}
Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van~Katwyk, P., Deac, A., et~al.
\newblock Scientific discovery in the age of artificial intelligence.
\newblock \emph{Nature}, 2023.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Teng, and Perdikaris]{Wang2020UnderstandingAM}
Wang, S., Teng, Y., and Perdikaris, P.
\newblock Understanding and mitigating gradient pathologies in physics-informed neural networks.
\newblock \emph{SIAM J. Sci. Comput.}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Yu, and Perdikaris]{Wang2020WhenAW}
Wang, S., Yu, X., and Perdikaris, P.
\newblock When and why pinns fail to train: A neural tangent kernel perspective.
\newblock \emph{J. Comput. Phys.}, 2020{\natexlab{b}}.

\bibitem[Wazwaz(2002)]{Wazwaz2002PartialDE}
Wazwaz, A.~M.
\newblock Partial differential equations : methods and applications.
\newblock 2002.

\bibitem[Weinan \& Yu(2017)Weinan and Yu]{Weinan2017TheDR}
Weinan, E. and Yu, T.
\newblock The deep ritz method: A deep learning-based numerical algorithm for solving variational problems.
\newblock \emph{Commun. Math. Stat.}, 2017.

\bibitem[Wen et~al.(2022)Wen, Li, Azizzadenesheli, Anandkumar, and Benson]{Wen2021UFNOA}
Wen, G., Li, Z., Azizzadenesheli, K., Anandkumar, A., and Benson, S.~M.
\newblock U-fno--an enhanced fourier neural operator-based deep-learning model for multiphase flow.
\newblock \emph{Advances in Water Resources}, 2022.

\bibitem[Wesseling(1995)]{wesseling1995introduction}
Wesseling, P.
\newblock Introduction to multigrid methods.
\newblock Technical report, 1995.

\bibitem[Wu et~al.(2022)Wu, Wu, Xu, Wang, and Long]{wu2022flowformer}
Wu, H., Wu, J., Xu, J., Wang, J., and Long, M.
\newblock Flowformer: Linearizing transformers with conservation flows.
\newblock In \emph{ICML}, 2022.

\bibitem[Wu et~al.(2023)Wu, Hu, Luo, Wang, and Long]{wu2023LSM}
Wu, H., Hu, T., Luo, H., Wang, J., and Long, M.
\newblock Solving high-dimensional pdes with latent spectral models.
\newblock In \emph{ICML}, 2023.

\bibitem[Xiao et~al.(2024)Xiao, Hao, Lin, Deng, and Su]{anonymous2023improved}
Xiao, Z., Hao, Z., Lin, B., Deng, Z., and Su, H.
\newblock Improved operator learning by orthogonal attention.
\newblock In \emph{ICML}, 2024.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and Singh]{Xiong2021NystrmformerAN}
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G.~M., Li, Y., and Singh, V.
\newblock Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating self-attention.
\newblock \emph{AAAI}, 2021.

\bibitem[Xue et~al.(2023)Xue, Yu, Zhang, Li, Mart{\'\i}n-Mart{\'\i}n, Wu, Xiong, Xu, Niebles, and Savarese]{xue2023ulip}
Xue, L., Yu, N., Zhang, S., Li, J., Mart{\'\i}n-Mart{\'\i}n, R., Wu, J., Xiong, C., Xu, R., Niebles, J.~C., and Savarese, S.
\newblock Ulip-2: Towards scalable multimodal pre-training for 3d understanding.
\newblock \emph{arXiv preprint arXiv:2305.08275}, 2023.

\bibitem[Yu et~al.(2022)Yu, Tang, Rao, Huang, Zhou, and Lu]{yu2022point}
Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., and Lu, J.
\newblock Point-bert: Pre-training 3d point cloud transformers with masked point modeling.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhao et~al.(2021)Zhao, Jiang, Jia, Torr, and Koltun]{zhao2021point}
Zhao, H., Jiang, L., Jia, J., Torr, P.~H., and Koltun, V.
\newblock Point transformer.
\newblock In \emph{ICCV}, 2021.

\end{thebibliography}
