\begin{thebibliography}{10}

\bibitem{copilot}
Github copilot: Your ai pair programmer.
\newblock \url{https://copilot.github.com/}, 2021.

\bibitem{ahmad2021unified}
Wasi~Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
\newblock Unified pre-training for program understanding and generation.
\newblock {\em arXiv preprint arXiv:2103.06333}, 2021.

\bibitem{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock {\em arXiv preprint arXiv:2108.07732}, 2021.

\bibitem{barone2017parallel}
Antonio Valerio~Miceli Barone and Rico Sennrich.
\newblock A parallel corpus of python functions and documentation strings for
  automated code documentation and code generation.
\newblock {\em arXiv preprint arXiv:1707.02275}, 2017.

\bibitem{bilgin2020vulnerability}
Zeki Bilgin, Mehmet~Akif Ersoy, Elif~Ustundag Soykan, Emrah Tomur, Pinar
  {\c{C}}omak, and Leyli Kara{\c{c}}ay.
\newblock Vulnerability prediction from source code using machine learning.
\newblock {\em IEEE Access}, 8:150672--150684, 2020.

\bibitem{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock {\em arXiv preprint arXiv:2204.06745}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cambronero2019deep}
Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra.
\newblock When deep learning met code search.
\newblock In {\em Proceedings of the 2019 27th ACM Joint Meeting on European
  Software Engineering Conference and Symposium on the Foundations of Software
  Engineering}, pages 964--974, 2019.

\bibitem{chakraborty2021deep}
Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray.
\newblock Deep learning based vulnerability detection: Are we there yet.
\newblock {\em IEEE Transactions on Software Engineering}, 2021.

\bibitem{chandel2022training}
Shubham Chandel, Colin~B Clement, Guillermo Serrato, and Neel Sundaresan.
\newblock Training and evaluating a jupyter notebook data science assistant.
\newblock {\em arXiv preprint arXiv:2201.12901}, 2022.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{chen2018execution}
Xinyun Chen, Chang Liu, and Dawn Song.
\newblock Execution-guided neural program synthesis.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{chen2018tree}
Xinyun Chen, Chang Liu, and Dawn Song.
\newblock Tree-to-tree neural networks for program translation.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{chen2021latent}
Xinyun Chen, Dawn Song, and Yuandong Tian.
\newblock Latent execution for neural program synthesis beyond domain-specific
  languages.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{cheng2021deepwukong}
Xiao Cheng, Haoyu Wang, Jiayi Hua, Guoai Xu, and Yulei Sui.
\newblock Deepwukong: Statically detecting software vulnerabilities using deep
  graph neural network.
\newblock {\em ACM Transactions on Software Engineering and Methodology
  (TOSEM)}, 30(3):1--33, 2021.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{clark2020electra}
Kevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.
\newblock Electra: Pre-training text encoders as discriminators rather than
  generators.
\newblock {\em arXiv preprint arXiv:2003.10555}, 2020.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{ellis2019write}
Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando
  Solar-Lezama.
\newblock Write, execute, assess: Program synthesis with a repl.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{feng2020codebert}
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
  Shou, Bing Qin, Ting Liu, Daxin Jiang, et~al.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock {\em arXiv preprint arXiv:2002.08155}, 2020.

\bibitem{fried2022incoder}
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
  Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
\newblock Incoder: A generative model for code infilling and synthesis.
\newblock {\em arXiv preprint arXiv:2204.05999}, 2022.

\bibitem{gu2018deep}
Xiaodong Gu, Hongyu Zhang, and Sunghun Kim.
\newblock Deep code search.
\newblock In {\em 2018 IEEE/ACM 40th International Conference on Software
  Engineering (ICSE)}, pages 933--944. IEEE, 2018.

\bibitem{guo2020graphcodebert}
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou,
  Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et~al.
\newblock Graphcodebert: Pre-training code representations with data flow.
\newblock {\em arXiv preprint arXiv:2009.08366}, 2020.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
  Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et~al.
\newblock Measuring coding challenge competence with apps.
\newblock {\em arXiv preprint arXiv:2105.09938}, 2021.

\bibitem{husain2019codesearchnet}
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
  Brockschmidt.
\newblock Codesearchnet challenge: Evaluating the state of semantic code
  search.
\newblock {\em arXiv preprint arXiv:1909.09436}, 2019.

\bibitem{kulal2019spoc}
Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex
  Aiken, and Percy~S Liang.
\newblock Spoc: Search-based pseudocode to code.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin~Dal Lago,
  et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em arXiv preprint arXiv:2203.07814}, 2022.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{mou2016convolutional}
Lili Mou, Ge~Li, Lu~Zhang, Tao Wang, and Zhi Jin.
\newblock Convolutional neural networks over tree structures for programming
  language processing.
\newblock In {\em Thirtieth AAAI conference on artificial intelligence}, 2016.

\bibitem{mukherjee2021neural}
Rohan Mukherjee, Yeming Wen, Dipak Chaudhari, Thomas Reps, Swarat Chaudhuri,
  and Christopher Jermaine.
\newblock Neural program generation modulo static analysis.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{nguyen2015divide}
Anh~Tuan Nguyen, Tung~Thanh Nguyen, and Tien~N Nguyen.
\newblock Divide-and-conquer approach for multi-phase statistical migration for
  source code (t).
\newblock In {\em 2015 30th IEEE/ACM International Conference on Automated
  Software Engineering (ASE)}, pages 585--596. IEEE, 2015.

\bibitem{nijkamp2022conversational}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio
  Savarese, and Caiming Xiong.
\newblock A conversational paradigm for program synthesis.
\newblock {\em arXiv preprint arXiv:2203.13474}, 2022.

\bibitem{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock {\em arXiv preprint arXiv:2112.00114}, 2021.

\bibitem{schuster2021programming}
Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, and Adam~Tauman Kalai.
\newblock Programming puzzles.
\newblock {\em arXiv preprint arXiv:2106.05784}, 2021.

\bibitem{shen2021generate}
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun
  Liu.
\newblock Generate \& rank: A multi-task framework for math word problems.
\newblock {\em arXiv preprint arXiv:2109.03034}, 2021.

\bibitem{svajlenko2014towards}
Jeffrey Svajlenko, Judith~F Islam, Iman Keivanloo, Chanchal~K Roy, and
  Mohammad~Mamun Mia.
\newblock Towards a big data curated benchmark of inter-project code clones.
\newblock In {\em 2014 IEEE International Conference on Software Maintenance
  and Evolution}, pages 476--480. IEEE, 2014.

\bibitem{wang2021codet5}
Yue Wang, Weishi Wang, Shafiq Joty, and Steven~CH Hoi.
\newblock Codet5: Identifier-aware unified pre-trained encoder-decoder models
  for code understanding and generation.
\newblock {\em arXiv preprint arXiv:2109.00859}, 2021.

\bibitem{xu2022systematic}
Frank~F Xu, Uri Alon, Graham Neubig, and Vincent~J Hellendoorn.
\newblock A systematic evaluation of large language models of code.
\newblock {\em arXiv preprint arXiv:2202.13169}, 2022.

\bibitem{zhou2019devign}
Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu.
\newblock Devign: Effective vulnerability identification by learning
  comprehensive program semantics via graph neural networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
