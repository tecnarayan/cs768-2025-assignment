\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Babes et~al.(2008)Babes, Munoz~de Cote, and Littman]{babes2008social}
Babes, M., Munoz~de Cote, E., and Littman, M.~L.
\newblock Social reward shaping in the prisoner's dilemma.
\newblock \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, pp.\  1389--1392, 2008.

\bibitem[Bard et~al.(2020)Bard, Foerster, Chandar, Burch, Lanctot, Song,
  Parisotto, Dumoulin, Moitra, Hughes, et~al.]{bard2020hanabi}
Bard, N., Foerster, J.~N., Chandar, S., Burch, N., Lanctot, M., Song, H.~F.,
  Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., et~al.
\newblock The hanabi challenge: A new frontier for ai research.
\newblock \emph{Artificial Intelligence}, 280:\penalty0 103216, 2020.

\bibitem[Barrett et~al.(2011)Barrett, Stone, and Kraus]{barrett2011empirical}
Barrett, S., Stone, P., and Kraus, S.
\newblock Empirical evaluation of ad hoc teamwork in the pursuit domain.
\newblock In \emph{AAMAS}, pp.\  567--574, 2011.

\bibitem[Brown \& Sandholm(2018)Brown and Sandholm]{brown2018superhuman}
Brown, N. and Sandholm, T.
\newblock Superhuman ai for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Busoniu et~al.(2006)Busoniu, Babuska, and
  De~Schutter]{busoniu2006multi}
Busoniu, L., Babuska, R., and De~Schutter, B.
\newblock Multi-agent reinforcement learning: A survey.
\newblock In \emph{2006 9th International Conference on Control, Automation,
  Robotics and Vision}, pp.\  1--6. IEEE, 2006.

\bibitem[Camerer(2011)]{camerer2011behavioral}
Camerer, C.~F.
\newblock \emph{Behavioral game theory: Experiments in strategic interaction}.
\newblock Princeton University Press, 2011.

\bibitem[Campbell et~al.(2002)Campbell, Hoane~Jr, and Hsu]{campbell2002deep}
Campbell, M., Hoane~Jr, A.~J., and Hsu, F.-h.
\newblock Deep blue.
\newblock \emph{Artificial intelligence}, 134\penalty0 (1-2):\penalty0 57--83,
  2002.

\bibitem[Carroll et~al.(2019)Carroll, Shah, Ho, Griffiths, Seshia, Abbeel, and
  Dragan]{carroll2019utility}
Carroll, M., Shah, R., Ho, M.~K., Griffiths, T., Seshia, S., Abbeel, P., and
  Dragan, A.
\newblock On the utility of learning about humans for human-ai coordination.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5175--5186, 2019.

\bibitem[Costa-Gomes et~al.(2001)Costa-Gomes, Crawford, and
  Broseta]{costa2001cognition}
Costa-Gomes, M., Crawford, V.~P., and Broseta, B.
\newblock Cognition and behavior in normal-form games: An experimental study.
\newblock \emph{Econometrica}, 69\penalty0 (5):\penalty0 1193--1235, 2001.

\bibitem[Devlin \& Kudenko(2016)Devlin and Kudenko]{devlin2016plan}
Devlin, S. and Kudenko, D.
\newblock Plan-based reward shaping for multi-agent reinforcement learning.
\newblock \emph{The Knowledge Engineering Review}, 31\penalty0 (1):\penalty0
  44--58, 2016.

\bibitem[Devlin et~al.(2011)Devlin, Kudenko, and
  Grze{\'s}]{devlin2011empirical}
Devlin, S., Kudenko, D., and Grze{\'s}, M.
\newblock An empirical study of potential-based reward shaping and advice in
  complex, multi-agent systems.
\newblock \emph{Advances in Complex Systems}, 14\penalty0 (02):\penalty0
  251--278, 2011.

\bibitem[Eger et~al.(2017)Eger, Martens, and C{\'o}rdoba]{eger2017intentional}
Eger, M., Martens, C., and C{\'o}rdoba, M.~A.
\newblock An intentional ai for hanabi.
\newblock In \emph{2017 IEEE Conference on Computational Intelligence and Games
  (CIG)}, pp.\  68--75. IEEE, 2017.

\bibitem[Fitzgerald(2019)]{fitzgerald2019populate}
Fitzgerald, N.
\newblock To populate is to regulate.
\newblock \emph{arXiv preprint arXiv:1911.04362}, 2019.

\bibitem[Foerster et~al.(2016)Foerster, Assael, De~Freitas, and
  Whiteson]{foerster2016learning}
Foerster, J., Assael, I.~A., De~Freitas, N., and Whiteson, S.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2137--2145, 2016.

\bibitem[Foerster et~al.(2017)Foerster, Nardelli, Farquhar, Afouras, Torr,
  Kohli, and Whiteson]{foerster2017stabilising}
Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P.~H., Kohli, P.,
  and Whiteson, S.
\newblock Stabilising experience replay for deep multi-agent reinforcement
  learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1146--1155. JMLR. org, 2017.

\bibitem[Foerster et~al.(2018{\natexlab{a}})Foerster, Farquhar, Afouras,
  Nardelli, and Whiteson]{foerster2018counterfactual}
Foerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018{\natexlab{a}}.

\bibitem[Foerster et~al.(2018{\natexlab{b}})Foerster, Song, Hughes, Burch,
  Dunning, Whiteson, Botvinick, and Bowling]{foerster2018bayesian}
Foerster, J.~N., Song, F., Hughes, E., Burch, N., Dunning, I., Whiteson, S.,
  Botvinick, M., and Bowling, M.
\newblock Bayesian action decoder for deep multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1811.01458}, 2018{\natexlab{b}}.

\bibitem[Fudenberg \& Liang(2019)Fudenberg and Liang]{fudenberg2019predicting}
Fudenberg, D. and Liang, A.
\newblock Predicting and understanding initial play.
\newblock \emph{American Economic Review}, 109\penalty0 (12):\penalty0
  4112--41, 2019.

\bibitem[Gilpin \& Sandholm(2007)Gilpin and Sandholm]{gilpin2007lossless}
Gilpin, A. and Sandholm, T.
\newblock Lossless abstraction of imperfect information games.
\newblock \emph{Journal of the ACM (JACM)}, 54\penalty0 (5):\penalty0 25--es,
  2007.

\bibitem[Hartford et~al.(2016)Hartford, Wright, and
  Leyton-Brown]{hartford2016deep}
Hartford, J.~S., Wright, J.~R., and Leyton-Brown, K.
\newblock Deep learning for predicting human strategic behavior.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2424--2432, 2016.

\bibitem[Hernandez-Leal et~al.(2019)Hernandez-Leal, Kartal, and
  Taylor]{hernandez2019survey}
Hernandez-Leal, P., Kartal, B., and Taylor, M.~E.
\newblock A survey and critique of multiagent deep reinforcement learning.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 33\penalty0
  (6):\penalty0 750--797, 2019.

\bibitem[Hu \& Foerster(2019)Hu and Foerster]{hu2019simplified}
Hu, H. and Foerster, J.~N.
\newblock Simplified action decoder for deep multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1912.02288}, 2019.

\bibitem[Kleiman-Weiner et~al.(2016)Kleiman-Weiner, Ho, Austerweil, Littman,
  and Tenenbaum]{kleiman2016coordinate}
Kleiman-Weiner, M., Ho, M.~K., Austerweil, J.~L., Littman, M.~L., and
  Tenenbaum, J.~B.
\newblock Coordinate to cooperate or compete: abstract goals and joint
  intentions in social interaction.
\newblock In \emph{CogSci}, 2016.

\bibitem[Kleinberg et~al.(2017)Kleinberg, Liang, and
  Mullainathan]{kleinberg2017theory}
Kleinberg, J., Liang, A., and Mullainathan, S.
\newblock The theory is predictive, but is it complete? an application to human
  perception of randomness.
\newblock In \emph{Proceedings of the 2017 ACM Conference on Economics and
  Computation}, pp.\  125--126, 2017.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls,
  P{\'e}rolat, Silver, and Graepel]{lanctot2017unified}
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P{\'e}rolat,
  J., Silver, D., and Graepel, T.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4190--4203, 2017.

\bibitem[Lazaridou et~al.(2016)Lazaridou, Peysakhovich, and
  Baroni]{lazaridou2016multi}
Lazaridou, A., Peysakhovich, A., and Baroni, M.
\newblock Multi-agent cooperation and the emergence of (natural) language.
\newblock \emph{arXiv preprint arXiv:1612.07182}, 2016.

\bibitem[Lerer \& Peysakhovich(2017)Lerer and
  Peysakhovich]{lerer2017maintaining}
Lerer, A. and Peysakhovich, A.
\newblock Maintaining cooperation in complex social dilemmas using deep
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.01068}, 2017.

\bibitem[Lerer \& Peysakhovich(2018)Lerer and Peysakhovich]{lerer2018learning}
Lerer, A. and Peysakhovich, A.
\newblock Learning social conventions in markov games.
\newblock \emph{arXiv preprint arXiv:1806.10071}, 2018.

\bibitem[Lerer et~al.(2019)Lerer, Hu, Foerster, and Brown]{lerer2019improving}
Lerer, A., Hu, H., Foerster, J., and Brown, N.
\newblock Improving policies via search in cooperative partially observable
  games.
\newblock \emph{arXiv preprint arXiv:1912.02318}, 2019.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6379--6390, 2017.

\bibitem[Lowe et~al.(2019)Lowe, Gupta, Foerster, Kiela, and
  Pineau]{lowe2019learning}
Lowe, R., Gupta, A., Foerster, J., Kiela, D., and Pineau, J.
\newblock Learning to learn to communicate, 2019.

\bibitem[Mehta et~al.(1994)Mehta, Starmer, and Sugden]{mehta1994nature}
Mehta, J., Starmer, C., and Sugden, R.
\newblock The nature of salience: An experimental investigation of pure
  coordination games.
\newblock \emph{The American Economic Review}, 84\penalty0 (3):\penalty0
  658--673, 1994.

\bibitem[Nair et~al.(2003)Nair, Tambe, Yokoo, Pynadath, and
  Marsella]{nair2003taming}
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., and Marsella, S.
\newblock Taming decentralized pomdps: Towards efficient policy computation for
  multiagent settings.
\newblock In \emph{IJCAI}, volume~3, pp.\  705--711, 2003.

\bibitem[Nowak(2006)]{nowak2006evolutionary}
Nowak, M.~A.
\newblock \emph{Evolutionary dynamics: exploring the equations of life}.
\newblock Harvard University Press, 2006.

\bibitem[Peysakhovich \& Lerer(2018)Peysakhovich and
  Lerer]{peysakhovich2018prosocial}
Peysakhovich, A. and Lerer, A.
\newblock Prosocial learning agents solve generalized stag hunts better than
  selfish ones.
\newblock In \emph{Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  2043--2044. International
  Foundation for Autonomous Agents and Multiagent Systems, 2018.

\bibitem[Peysakhovich \& Naecker(2017)Peysakhovich and
  Naecker]{peysakhovich2017using}
Peysakhovich, A. and Naecker, J.
\newblock Using methods from machine learning to evaluate behavioral models of
  choice under risk and ambiguity.
\newblock \emph{Journal of Economic Behavior \& Organization}, 133:\penalty0
  373--384, 2017.

\bibitem[Ravindran \& Barto(2004)Ravindran and Barto]{ravindran2004approximate}
Ravindran, B. and Barto, A.~G.
\newblock Approximate homomorphisms: A framework for non-exact minimization in
  markov decision processes.
\newblock 2004.

\bibitem[Resnick et~al.(2018)Resnick, Kulikov, Cho, and
  Weston]{resnick2018vehicle}
Resnick, C., Kulikov, I., Cho, K., and Weston, J.
\newblock Vehicle community strategies.
\newblock \emph{arXiv preprint arXiv:1804.07178}, 2018.

\bibitem[Schelling(1980)]{schelling1980strategy}
Schelling, T.~C.
\newblock \emph{The strategy of conflict}.
\newblock Harvard university press, 1980.

\bibitem[Shum et~al.(2019)Shum, Kleiman-Weiner, Littman, and
  Tenenbaum]{shum2019theory}
Shum, M., Kleiman-Weiner, M., Littman, M.~L., and Tenenbaum, J.~B.
\newblock Theory of minds: Understanding behavior in groups through inverse
  planning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  6163--6170, 2019.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Stahl(1993)]{stahl1993evolution}
Stahl, D.~O.
\newblock Evolution of smartn players.
\newblock \emph{Games and Economic Behavior}, 5\penalty0 (4):\penalty0
  604--617, 1993.

\bibitem[Stone et~al.(2010)Stone, Kaminka, Kraus, and Rosenschein]{stone2010ad}
Stone, P., Kaminka, G.~A., Kraus, S., and Rosenschein, J.~S.
\newblock Ad hoc autonomous agent teams: Collaboration without
  pre-coordination.
\newblock In \emph{Twenty-Fourth AAAI Conference on Artificial Intelligence},
  2010.

\bibitem[Sukhbaatar et~al.(2016)Sukhbaatar, Fergus,
  et~al.]{sukhbaatar2016learning}
Sukhbaatar, S., Fergus, R., et~al.
\newblock Learning multiagent communication with backpropagation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2244--2252, 2016.

\bibitem[Sunehag et~al.(2018)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et~al.]{sunehag2018value}
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,
  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In \emph{Proceedings of the 17th international conference on
  autonomous agents and multiagent systems}, pp.\  2085--2087. International
  Foundation for Autonomous Agents and Multiagent Systems, 2018.

\bibitem[Tesauro(1994)]{tesauro1994td}
Tesauro, G.
\newblock Td-gammon, a self-teaching backgammon program, achieves master-level
  play.
\newblock \emph{Neural computation}, 6\penalty0 (2):\penalty0 215--219, 1994.

\bibitem[Tieleman et~al.(2018)Tieleman, Lazaridou, Mourad, Blundell, and
  Precup]{tieleman2018shaping}
Tieleman, O., Lazaridou, A., Mourad, S., Blundell, C., and Precup, D.
\newblock Shaping representations through communication.
\newblock \emph{OpenReview}, 2018.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{tobin2017domain}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{2017 IEEE/RSJ international conference on intelligent robots
  and systems (IROS)}, pp.\  23--30. IEEE, 2017.

\bibitem[Tucker et~al.(2020)Tucker, Zhou, and Shah]{tucker2020adversarially}
Tucker, M., Zhou, Y., and Shah, J.
\newblock Adversarially guided self-play for adopting social conventions.
\newblock \emph{arXiv preprint arXiv:2001.05994}, 2020.

\bibitem[van~der Pol et~al.(2020)van~der Pol, Kipf, Oliehoek, and
  Welling]{van2020plannable}
van~der Pol, E., Kipf, T., Oliehoek, F.~A., and Welling, M.
\newblock Plannable approximations to mdp homomorphisms: Equivariance under
  actions.
\newblock \emph{arXiv preprint arXiv:2002.11963}, 2020.

\bibitem[Wright \& Leyton-Brown(2010)Wright and Leyton-Brown]{wright2010beyond}
Wright, J.~R. and Leyton-Brown, K.
\newblock Beyond equilibrium: Predicting human behavior in normal-form games.
\newblock In \emph{Twenty-Fourth AAAI Conference on Artificial Intelligence},
  2010.

\end{thebibliography}
