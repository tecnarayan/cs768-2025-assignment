\begin{thebibliography}{10}

\bibitem{ratcliff1990connectionist}
Roger Ratcliff.
\newblock Connectionist models of recognition memory: constraints imposed by
  learning and forgetting functions.
\newblock {\em Psychological review}, 97(2):285, 1990.

\bibitem{robins1995catastrophic}
Anthony Robins.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock {\em Connection Science}, 7(2):123--146, 1995.

\bibitem{riemer2018learning}
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu,
  and Gerald Tesauro.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock {\em arXiv preprint arXiv:1810.11910}, 2018.

\bibitem{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock {\em arXiv preprint arXiv:2004.07211}, 2020.

\bibitem{boschini2022class}
Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone
  Calderara.
\newblock Class-incremental continual learning into the extended der-verse.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2022.

\bibitem{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock {\em Machine learning}, 28(1):41--75, 1997.

\bibitem{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4528--4537. PMLR, 2018.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em International Conference on Machine Learning}, pages
  3987--3995. PMLR, 2017.

\bibitem{mermillod2013stability}
Martial Mermillod, Aur{\'e}lia Bugaiska, and Patrick Bonin.
\newblock The stability-plasticity dilemma: Investigating the continuum from
  catastrophic forgetting to age-limited learning effects.
\newblock {\em Frontiers in psychology}, 4:504, 2013.

\bibitem{vitter1985random}
Jeffrey~S Vitter.
\newblock Random sampling with a reservoir.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)}, 11(1):37--57,
  1985.

\bibitem{aljundi2019online}
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin,
  Laurent Charlin, and Tinne Tuytelaars.
\newblock Online continual learning with maximally interfered retrieval.
\newblock {\em arXiv preprint arXiv:1908.04742}, 2019.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154, 2018.

\bibitem{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 7765--7773, 2018.

\bibitem{serra2018overcoming}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In {\em International Conference on Machine Learning}, pages
  4548--4557. PMLR, 2018.

\bibitem{pham2022continual}
Quang Pham, Chenghao Liu, and Steven HOI.
\newblock Continual normalization: Rethinking batch normalization for online
  continual learning.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{lesort2021continual}
Timoth{\'e}e Lesort, Thomas George, and Irina Rish.
\newblock Continual learning in deep networks: an analysis of the last layer.
\newblock {\em arXiv preprint arXiv:2106.01834}, 2021.

\bibitem{wu2022pretrained}
Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and Gholamreza
  Haffari.
\newblock Pretrained language model in continual learning: A comparative study.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{ramasesh2020anatomy}
Vinay~V Ramasesh, Ethan Dyer, and Maithra Raghu.
\newblock Anatomy of catastrophic forgetting: Hidden representations and task
  semantics.
\newblock {\em arXiv preprint arXiv:2007.07400}, 2020.

\bibitem{zhang2019all}
Chiyuan Zhang, Samy Bengio, and Yoram Singer.
\newblock Are all layers created equal?
\newblock {\em arXiv preprint arXiv:1902.01996}, 2019.

\bibitem{van2019three}
Gido~M Van~de Ven and Andreas~S Tolias.
\newblock Three scenarios for continual learning.
\newblock {\em arXiv preprint arXiv:1904.07734}, 2019.

\bibitem{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock {\em Advances in neural information processing systems},
  30:6467--6476, 2017.

\bibitem{medmnistv2}
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,
  Hanspeter Pfister, and Bingbing Ni.
\newblock Medmnist v2: A large-scale lightweight benchmark for 2d and 3d
  biomedical image classification.
\newblock {\em arXiv preprint arXiv:2110.14795}, 2021.

\bibitem{li2017deeper}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy~M Hospedales.
\newblock Deeper, broader and artier domain generalization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 5542--5550, 2017.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526, 2017.

\bibitem{li2020sequential}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales.
\newblock Sequential learning for domain generalization.
\newblock In {\em European Conference on Computer Vision}, pages 603--619.
  Springer, 2020.

\bibitem{derakhshani2022lifelonger}
Mohammad~Mahdi Derakhshani, Ivona Najdenkoska, Tom van Sonsbeek, Xiantong Zhen,
  Dwarikanath Mahapatra, Marcel Worring, and Cees~GM Snoek.
\newblock Lifelonger: A benchmark for continual disease classification.
\newblock {\em arXiv preprint arXiv:2204.05737}, 2022.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{long2015learning}
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan.
\newblock Learning transferable features with deep adaptation networks.
\newblock In {\em International conference on machine learning}, pages 97--105.
  PMLR, 2015.

\bibitem{chang2019domain}
Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han.
\newblock Domain-specific batch normalization for unsupervised domain
  adaptation.
\newblock In {\em Proceedings of the IEEE/CVF conference on Computer Vision and
  Pattern Recognition}, pages 7354--7362, 2019.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{prabhu2020gdumb}
Ameya Prabhu, Philip~HS Torr, and Puneet~K Dokania.
\newblock Gdumb: A simple approach that questions our progress in continual
  learning.
\newblock In {\em European conference on computer vision}, pages 524--540.
  Springer, 2020.

\bibitem{chaudhry2018efficient}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock {\em arXiv preprint arXiv:1812.00420}, 2018.

\bibitem{benjamin2018measuring}
Ari~S Benjamin, David Rolnick, and Konrad Kording.
\newblock Measuring and regularizing networks in function space.
\newblock {\em arXiv preprint arXiv:1805.08289}, 2018.

\bibitem{hou2019learning}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 831--839, 2019.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 7(7):3, 2015.

\bibitem{volpi2021continual}
Riccardo Volpi, Diane Larlus, and Gr{\'e}gory Rogez.
\newblock Continual adaptation of visual representations via domain
  randomization and meta-learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4443--4453, 2021.

\end{thebibliography}
