\begin{thebibliography}{10}

\bibitem{AbadiCGMMTZ16}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In {\em Proceedings of the 2016 ACM Conference on Computer and
  Communications Security}, CCS '16, pages 308--318, New York, NY, USA, 2016.
  ACM.

\bibitem{AnilGGKM21}
Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi.
\newblock Large-scale differentially private {BERT}.
\newblock {\em arXiv preprint arXiv:2108.01624}, 2021.

\bibitem{BassilyST14}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em Proceedings of the 55th Annual IEEE Symposium on Foundations
  of Computer Science}, FOCS '14, pages 464--473, Washington, DC, USA, 2014.
  IEEE Computer Society.

\bibitem{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{bucilua2006model}
C~Bucilua, R~Caruana, and A~Niculescu-Mizil.
\newblock Model compression, in proceedings of the 12 th acm sigkdd
  international conference on knowledge discovery and data mining.
\newblock {\em New York, NY, USA}, 2006.

\bibitem{cao2019}
Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, and Niranjan
  Balasubramanian.
\newblock {DeFormer}: Decomposing pre-trained transformers for faster question
  answering.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 4487--4497, 2020.

\bibitem{CarliniLEKS19}
Nicholas Carlini, Chang Liu, {\'U}lfar Erlingsson, Jernej Kos, and Dawn Song.
\newblock The secret sharer: Evaluating and testing unintended memorization in
  neural networks.
\newblock In {\em 28th USENIX Security Symposium}, USENIX Security '19, pages
  267--284. USENIX Association, 2019.

\bibitem{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, Alina Oprea, and Colin Raffel.
\newblock Extracting training data from large language models, 2021.

\bibitem{ChaudhuriH11}
Kamalika Chaudhuri and Daniel Hsu.
\newblock Sample complexity bounds for differentially private learning.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, COLT '11, pages 155--186, 2011.

\bibitem{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock {\em Advances in neural information processing systems},
  33:15834--15846, 2020.

\bibitem{de2022unlocking}
Soham De, Leonard Berrada, Jamie Hayes, Samuel~L Smith, and Borja Balle.
\newblock Unlocking high-accuracy differentially private image classification
  through scale.
\newblock {\em arXiv preprint arXiv:2204.13650}, 2022.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{Dietterich00ensemblemethods}
Thomas~G. Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em MULTIPLE CLASSIFIER SYSTEMS, LBCS-1857}, pages 1--15.
  Springer, 2000.

\bibitem{ding2020sdsk2bert}
Lifang Ding and Yujiu Yang.
\newblock {SDSK2BERT}: Explore the specific depth with specific knowledge to
  compress {BERT}.
\newblock In {\em 2020 IEEE International Conference on Knowledge Graph
  (ICKG)}, page 420–425, Aug 2020.

\bibitem{DworkKMMN06}
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
  Naor.
\newblock Our data, ourselves: Privacy via distributed noise generation.
\newblock In {\em Proceedings of the 24th Annual International Conference on
  the Theory and Applications of Cryptographic Techniques}, EUROCRYPT '06,
  pages 486--503, Berlin, Heidelberg, 2006. Springer.

\bibitem{DworkMNS06}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In {\em Proceedings of the 3rd Conference on Theory of Cryptography},
  TCC '06, pages 265--284, Berlin, Heidelberg, 2006. Springer.

\bibitem{DworkR14}
Cynthia Dwork and Aaron Roth.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 9(3--4):211--407, 2014.

\bibitem{elsken2019neural}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock {\em The Journal of Machine Learning Research}, 20(1):1997--2017,
  2019.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{ganesh2021compressing}
Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad~Ali Khan, Yin Yang, Hassan Sajjad,
  Preslav Nakov, Deming Chen, and Marianne Winslett.
\newblock Compressing large-scale transformer-based models: A case study on
  bert.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:1061--1080, 2021.

\bibitem{gopi2021numerical}
Sivakanth Gopi, Yin~Tat Lee, and Lukas Wutschitz.
\newblock Numerical composition of differential privacy.
\newblock {\em arXiv preprint arXiv:2106.02848}, 2021.

\bibitem{gou2021knowledge}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem{guo2019reweighted}
Fu-Ming Guo, Sijia Liu, Finlay~S Mungall, Xue Lin, and Yanzhi Wang.
\newblock Reweighted proximal pruning for large-scale language representation.
\newblock {\em arXiv preprint arXiv:1909.12486}, 2019.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{hassibi1992second}
Babak Hassibi and David Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock {\em Advances in neural information processing systems}, 5, 1992.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{hoefler2021sparsity}
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock {\em Journal of Machine Learning Research}, 22(241):1--124, 2021.

\bibitem{HooryFTCELNSBHM21}
Shlomo Hoory, Amir Feder, Avichai Tendler, Alon Cohen, Sofia Erell, Itay Laish,
  Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, and Yossi
  Matias.
\newblock Learning and evaluating a differentially private pre-trained language
  model.
\newblock In {\em Proceedings of the Third Workshop on Privacy in Natural
  Language Processing}, PrivateNLP '21, pages 21--29, 2021.

\bibitem{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock {TinyBERT}: Distilling {BERT} for natural language understanding.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pages 4163--4174, 2020.

\bibitem{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock {\em Advances in neural information processing systems}, 2, 1989.

\bibitem{li2020efficient}
Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji~Li, Zhengang Li, Hang Liu, and
  Caiwen Ding.
\newblock Efficient transformer-based large scale language representations
  using hardware-friendly block structured pruning.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3187--3199, 2020.

\bibitem{LiTLH21}
Xuechen Li, Florian Tram\`er, Percy Liang, and Tatsunori Hashimoto.
\newblock Large language models can be strong differentially private learners.
\newblock In {\em Proceedings of the 10th International Conference on Learning
  Representations}, ICLR '22, 2022.

\bibitem{Liang21pruningsurvey}
Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang.
\newblock Pruning and quantization for deep neural network acceleration: A
  survey.
\newblock {\em arXiv preprint arXiv:2101.09671}, 2021.

\bibitem{liu2019attentive}
Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, and Caiming Xiong.
\newblock {MKD}: A multi-task knowledge distillation approach for pretrained
  language models.
\newblock {\em arXiv preprint arXiv:1911.03588}, 2019.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Ro{BERT}a: A robustly optimized {BERT} pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{lyu2020differentially}
Lingjuan Lyu and Chi-Hua Chen.
\newblock Differentially private knowledge distillation for mobile analytics.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, pages 1809--1812, 2020.

\bibitem{mao2020ladabert}
Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Quanlu Zhang, Yaming
  Yang, Yunhai Tong, and Jing Bai.
\newblock {LadaBERT}: Lightweight adaptation of {BERT} through hybrid model
  compression.
\newblock In {\em Proceedings of the 28th International Conference on
  Computational Linguistics}, page 3225–3234, 2020.

\bibitem{McMahanRTZ18}
H~Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations}, ICLR '18, 2018.

\bibitem{mukherjee2019distilling}
Subhabrata Mukherjee and Ahmed~H. Awadallah.
\newblock Distilling {BERT} into simple neural networks with unlabeled transfer
  data.
\newblock {\em arXiv preprint arXiv:1910.01769}, 2019.

\bibitem{mukherjee2020xtremedistil}
Subhabrata Mukherjee and Ahmed~H. Awadallah.
\newblock {XtremeDistil}: Multi-stage distillation for massive multilingual
  models.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, page 2221–2234, 2020.

\bibitem{noach2020compressing}
Matan~Ben Noach and Yoav Goldberg.
\newblock Compressing pre-trained language models by matrix decomposition.
\newblock In {\em Proceedings of the 1st Conference of the Asia-Pacific Chapter
  of the Association for Computational Linguistics and the 10th International
  Joint Conference on Natural Language Processing}, pages 884--889, 2020.

\bibitem{PapernotAEGT17}
Nicolas Papernot, Mart{\'\i}n Abadi, Ulfar Erlingsson, Ian Goodfellow, and
  Kunal Talwar.
\newblock Semi-supervised knowledge transfer for deep learning from private
  training data.
\newblock In {\em Proceedings of the 5th International Conference on Learning
  Representations}, ICLR '17, 2017.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9, 2019.

\bibitem{romero2014fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{shokri2017membership}
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
\newblock Membership inference attacks against machine learning models.
\newblock In {\em 2017 IEEE symposium on security and privacy (SP)}, pages
  3--18. IEEE, 2017.

\bibitem{song2020lightpaff}
Kaitao Song, Hao Sun, Xu~Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan
  Liu.
\newblock {LightPAFF}: A two-stage distillation framework for pre-training and
  fine-tuning.
\newblock {\em arXiv preprint arXiv:2004.12817}, 2020.

\bibitem{SongCS13}
Shuang Song, Kamalika Chaudhuri, and Anand~D Sarwate.
\newblock Stochastic gradient descent with differentially private updates.
\newblock In {\em Proceedings of the 2013 IEEE Global Conference on Signal and
  Information Processing}, GlobalSIP '13, pages 245--248, Washington, DC, USA,
  2013. IEEE Computer Society.

\bibitem{stock2020training}
Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herve Jegou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for {BERT} model compression.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 4314--4323, 2019.

\bibitem{edge-tambe}
Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco
  Donato, Victor Sanh, Paul Whatmough, Alexander~M. Rush, David Brooks, and
  Gu-Yeon Wei.
\newblock Edgebert: Sentence-level energy optimizations for latency-aware
  multi-task nlp inference.
\newblock In {\em MICRO-54: 54th Annual IEEE/ACM International Symposium on
  Microarchitecture}, MICRO '21, page 830–844, New York, NY, USA, 2021.
  Association for Computing Machinery.

\bibitem{tambe2020algorithm}
Thierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay~Janapa Reddi,
  Alexander Rush, David Brooks, and Gu-Yeon Wei.
\newblock Algorithm-hardware co-design of adaptive floating-point encodings for
  resilient deep learning inference.
\newblock In {\em 2020 57th ACM/IEEE Design Automation Conference (DAC)}, pages
  1--6. IEEE, 2020.

\bibitem{tan2019mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2820--2828, 2019.

\bibitem{tang2019distilling}
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
\newblock Distilling task-specific knowledge from {BERT} into simple neural
  networks.
\newblock {\em arXiv preprint arXiv:1903.12136}, 2019.

\bibitem{tian2021seqpate}
Zhiliang Tian, Yingxiu Zhao, Ziyue Huang, Yu-Xiang Wang, Nevin Zhang, and
  He~He.
\newblock Seqpate: Differentially private text generation via knowledge
  distillation.
\newblock 2021.

\bibitem{TramerB21}
Florian Tram{\`e}r and Dan Boneh.
\newblock Differentially private learning needs better features (or much more
  data).
\newblock In {\em Proceedings of the 9th International Conference on Learning
  Representations}, ICLR '21, 2021.

\bibitem{turc2019well}
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Well-read students learn better: The impact of student initialization
  on knowledge distillation.
\newblock {\em arXiv preprint arXiv:1908.08962}, 13, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6000--6010, 2017.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wasserblat2020exploring}
Moshe Wasserblat, Oren Pereg, and Peter Izsak.
\newblock Exploring the boundaries of low-resource {BERT} distillation.
\newblock In {\em Proceedings of SustaiNLP: Workshop on Simple and Efficient
  Natural Language Processing}, page 35–40, 2020.

\bibitem{wu2020distilling}
Xing Wu, Yibing Liu, Xiangyang Zhou, and Dianhai Yu.
\newblock Distilling knowledge from pre-trained language models via text
  smoothing.
\newblock {\em arXiv preprint arXiv:2005.03848}, 2020.

\bibitem{nasbert21}
Jin Xu, Xu~Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu.
\newblock Nas-bert: Task-agnostic and adaptive-size bert compression with
  neural architecture search.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, KDD '21, page 1933–1943, New York, NY, USA,
  2021. Association for Computing Machinery.

\bibitem{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R. Salakhutdinov,
  and Quoc~V. Le.
\newblock {XLNet}: Generalized autoregressive pretraining for language
  understanding.
\newblock {\em Advances in Neural Information Processing Systems},
  32:5753--5763, 2019.

\bibitem{yin2021autotinybert}
Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.
\newblock Autotinybert: Automatic hyper-parameter optimization for efficient
  pre-trained language models.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 5146--5157, 2021.

\bibitem{yu2022differentially}
Da~Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin~A Inan, Gautam
  Kamath, Janardhan Kulkarni, Yin~Tat Lee, Andre Manoel, Lukas Wutschitz,
  Sergey Yekhanin, and Huishuai Zhang.
\newblock Differentially private fine-tuning of language models.
\newblock In {\em International Conference on Learning Representations}, ICLR
  '22, 2022.

\bibitem{yu2021not}
Da~Yu, Huishuai Zhang, Wei Chen, and Tie{-}Yan Liu.
\newblock Do not let privacy overbill utility: Gradient embedding perturbation
  for private learning.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem{yu2021large}
Da~Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.
\newblock Large scale private learning via low-rank reparametrization.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, ICML '21. JMLR, Inc., 2021.

\bibitem{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock {Q8BERT}: Quantized 8bit {BERT}.
\newblock In {\em Proceedings of the 5th Workshop on Energy Efficient Machine
  Learning and Cognitive Computing}, 2019.

\end{thebibliography}
