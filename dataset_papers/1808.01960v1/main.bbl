\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein gan.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Asadi et~al.(2018)Asadi, Cater, Misra, and
  Littman]{asadi2018equivalence}
Kavosh Asadi, Evan Cater, Dipendra Misra, and Michael~L Littman.
\newblock Equivalence between wasserstein and value-aware model-based
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.01265}, 2018.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.06887}, 2017.

\bibitem[Bertsekas(1995)]{bertsekas1995dynamic}
Dimitri Bertsekas.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena Scientific, 1995.

\bibitem[Bertsekas(2005)]{bertsekas2005dynamic}
DP~Bertsekas.
\newblock Dynamic programming and optimal control: Volume {II}.
\newblock 2005.

\bibitem[Dabney et~al.(2017)Dabney, Rowland, Bellemare, and
  Munos]{dabney2017distributional}
Will Dabney, Mark Rowland, Marc~G Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock \emph{arXiv preprint arXiv:1710.10044}, 2017.

\bibitem[Doan et~al.(2018)Doan, Mazoure, and Lyle]{doan2018gan}
Thang Doan, Bogdan Mazoure, and Clare Lyle.
\newblock Gan q-learning.
\newblock \emph{arXiv preprint arXiv:1805.04874}, 2018.

\bibitem[Dosovitskiy \& Koltun(2016)Dosovitskiy and
  Koltun]{dosovitskiy2016learning}
Alexey Dosovitskiy and Vladlen Koltun.
\newblock Learning to act by predicting the future.
\newblock \emph{arXiv preprint arXiv:1611.01779}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of wasserstein gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5767--5777, 2017.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2017rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1710.02298}, 2017.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{houthooft2016vime}
Rein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter
  Abbeel.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1109--1117, 2016.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Mannor \& Shimkin(2004)Mannor and Shimkin]{mannor2004geometric}
Shie Mannor and Nahum Shimkin.
\newblock A geometric approach to multi-criterion reinforcement learning.
\newblock \emph{Journal of machine learning research}, 5\penalty0
  (Apr):\penalty0 325--360, 2004.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Rusu, J.~Veness, M.~Bellemare,
  A.~Graves, M.~Riedmiller, A.~Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[M{\"u}ller(1997)]{muller1997integral}
Alfred M{\"u}ller.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock \emph{Advances in Applied Probability}, 29\penalty0 (2):\penalty0
  429--443, 1997.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3003--3011, 2013.

\bibitem[Oudeyer et~al.(2007)Oudeyer, Kaplan, and Hafner]{oudeyer2007intrinsic}
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena~V Hafner.
\newblock Intrinsic motivation systems for autonomous mental development.
\newblock \emph{IEEE transactions on evolutionary computation}, 11\penalty0
  (2):\penalty0 265--286, 2007.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock \emph{arXiv preprint arXiv:1705.05363}, 2017.

\bibitem[Pazis \& Parr(2016)Pazis and Parr]{pazis2016efficient}
Jason Pazis and Ronald Parr.
\newblock Efficient pac-optimal exploration in concurrent, continuous state
  mdps with delayed updates.
\newblock In \emph{AAAI}, pp.\  1977--1985, 2016.

\bibitem[Radford et~al.(2015)Radford, Metz, and
  Chintala]{radford2015unsupervised}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[Schmidhuber(2010)]{schmidhuber2010formal}
J{\"u}rgen Schmidhuber.
\newblock Formal theory of creativity, fun, and intrinsic motivation
  (1990--2010).
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 2\penalty0
  (3):\penalty0 230--247, 2010.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Richard~S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick~M
  Pilarski, Adam White, and Doina Precup.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pp.\  761--768. International Foundation for
  Autonomous Agents and Multiagent Systems, 2011.

\bibitem[Tang \& Agrawal(2018)Tang and Agrawal]{tang2018exploration}
Yunhao Tang and Shipra Agrawal.
\newblock Exploration by distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1805.01907}, 2018.

\bibitem[Tewari \& Bartlett(2008)Tewari and Bartlett]{tewari2008optimistic}
Ambuj Tewari and Peter~L Bartlett.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1505--1512, 2008.

\bibitem[Vamplew et~al.(2011)Vamplew, Dazeley, Berry, Issabekov, and
  Dekker]{vamplew2011empirical}
Peter Vamplew, Richard Dazeley, Adam Berry, Rustam Issabekov, and Evan Dekker.
\newblock Empirical evaluation methods for multiobjective reinforcement
  learning algorithms.
\newblock \emph{Machine learning}, 84\penalty0 (1-2):\penalty0 51--80, 2011.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, volume~2, pp.\ ~5. Phoenix, AZ, 2016.

\bibitem[Villani(2008)]{villani2008optimal}
C{\'e}dric Villani.
\newblock \emph{Optimal transport: old and new}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Zhao et~al.(2009)Zhao, Kosorok, and Zeng]{zhao2009reinforcement}
Yufan Zhao, Michael~R Kosorok, and Donglin Zeng.
\newblock Reinforcement learning design for cancer clinical trials.
\newblock \emph{Statistics in medicine}, 28\penalty0 (26):\penalty0 3294--3315,
  2009.

\end{thebibliography}
