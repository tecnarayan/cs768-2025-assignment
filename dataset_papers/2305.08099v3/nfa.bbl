\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alhanai et~al.(2017)Alhanai, Au, and Glass]{alhanai2017spoken}
Alhanai, T., Au, R., and Glass, J.
\newblock Spoken language biomarkers for detecting cognitive impairment.
\newblock In \emph{Proc. Automatic Speech Recognition and Understanding
  Workshop (ASRU)}, pp.\  409--416, 2017.

\bibitem[Babu et~al.(2022)Babu, Wang, Tjandra, Lakhotia, Xu, Goyal, Singh, von
  Platen, Saraf, Pino, Baevski, Conneau, and Auli]{XLS-R}
Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K.,
  von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A., and Auli, M.
\newblock {XLS-R:} self-supervised cross-lingual speech representation learning
  at scale.
\newblock In \emph{Proc. Interspeech 2022}, pp.\  2278--2282, 2022.

\bibitem[Baevski et~al.(2020{\natexlab{a}})Baevski, Schneider, and
  Auli]{vq-wav2vec}
Baevski, A., Schneider, S., and Auli, M.
\newblock vq-wav2vec: Self-supervised learning of discrete speech
  representations.
\newblock In \emph{Proc. International Conference on Learning Representations,
  {ICLR}}, 2020{\natexlab{a}}.

\bibitem[Baevski et~al.(2020{\natexlab{b}})Baevski, Zhou, Mohamed, and
  Auli]{wav2vec2}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.
\newblock wav2vec 2.0: {A} framework for self-supervised learning of speech
  representations.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Proc. Advances in Neural Information Processing Systems},
  2020{\natexlab{b}}.

\bibitem[Bishop \& Nasrabadi(2006)Bishop and Nasrabadi]{bishop2006pattern}
Bishop, C.~M. and Nasrabadi, N.~M.
\newblock \emph{Pattern Recognition and Machine Learning}, volume~4.
\newblock 2006.

\bibitem[Busso et~al.(2008)Busso, Bulut, Lee, Kazemzadeh, Mower, Kim, Chang,
  Lee, and Narayanan]{IEMOCAP}
Busso, C., Bulut, M., Lee, C., Kazemzadeh, A., Mower, E., Kim, S., Chang,
  J.~N., Lee, S., and Narayanan, S.~S.
\newblock {IEMOCAP:} interactive emotional dyadic motion capture database.
\newblock \emph{Lang. Resour. Evaluation}, 42\penalty0 (4):\penalty0 335--359,
  2008.

\bibitem[Chen et~al.(2022)Chen, Wang, Chen, Wu, Liu, Chen, Li, Kanda, Yoshioka,
  Xiao, Wu, Zhou, Ren, Qian, Qian, Wu, Zeng, Yu, and Wei]{wavlm}
Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N.,
  Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu,
  J., Zeng, M., Yu, X., and Wei, F.
\newblock {WavLM}: Large-scale self-supervised pre-training for full stack
  speech processing.
\newblock \emph{{IEEE} J. Sel. Top. Signal Process.}, 16\penalty0 (6):\penalty0
  1505--1518, 2022.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{simclr}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.~E.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{Proc. International Conference on Machine Learning, {ICML}},
  volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\
  1597--1607, 2020.

\bibitem[Chung \& Glass(2020)Chung and Glass]{apc}
Chung, Y. and Glass, J.~R.
\newblock Generative pre-training for speech with autoregressive predictive
  coding.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, pp.\  3497--3501, 2020.

\bibitem[Dehak et~al.(2010)Dehak, Kenny, Dehak, Dumouchel, and
  Ouellet]{ivector}
Dehak, N., Kenny, P.~J., Dehak, R., Dumouchel, P., and Ouellet, P.
\newblock Front-end factor analysis for speaker verification.
\newblock \emph{IEEE Transactions on Audio, Speech, and Language Processing},
  19\penalty0 (4):\penalty0 788--798, 2010.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In Burstein, J., Doran, C., and Solorio, T. (eds.), \emph{Proceedings
  of the 2019 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019,
  Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}, pp.\
   4171--4186, 2019.

\bibitem[Doersch et~al.(2015)Doersch, Gupta, and Efros]{jagsaw}
Doersch, C., Gupta, A., and Efros, A.~A.
\newblock Unsupervised visual representation learning by context prediction.
\newblock In \emph{Proc. International Conference on Computer Vision, {ICCV}},
  pp.\  1422--1430, 2015.

\bibitem[Hsu et~al.(2021{\natexlab{a}})Hsu, Bolte, Tsai, Lakhotia,
  Salakhutdinov, and Mohamed]{hubert}
Hsu, W., Bolte, B., Tsai, Y.~H., Lakhotia, K., Salakhutdinov, R., and Mohamed,
  A.
\newblock Hubert: Self-supervised speech representation learning by masked
  prediction of hidden units.
\newblock \emph{{IEEE} {ACM} Trans. Audio Speech Lang. Process.}, 29:\penalty0
  3451--3460, 2021{\natexlab{a}}.

\bibitem[Hsu et~al.(2021{\natexlab{b}})Hsu, Sriram, Baevski, Likhomanenko, Xu,
  Pratap, Kahn, Lee, Collobert, Synnaeve, and Auli]{SSL-domain}
Hsu, W., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., Kahn,
  J., Lee, A., Collobert, R., Synnaeve, G., and Auli, M.
\newblock Robust wav2vec 2.0: Analyzing domain shift in self-supervised
  pre-training.
\newblock In \emph{Proc. Interspeech 2021}, pp.\  721--725, 2021{\natexlab{b}}.

\bibitem[Jia et~al.(2018)Jia, Zhang, Weiss, Wang, Shen, Ren, Chen, Nguyen,
  Pang, Lopez{-}Moreno, and Wu]{multispk-tts}
Jia, Y., Zhang, Y., Weiss, R.~J., Wang, Q., Shen, J., Ren, F., Chen, Z.,
  Nguyen, P., Pang, R., Lopez{-}Moreno, I., and Wu, Y.
\newblock Transfer learning from speaker verification to multispeaker
  text-to-speech synthesis.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  4485--4495, 2018.

\bibitem[Kenny et~al.(2007)Kenny, Boulianne, Ouellet, and Dumouchel]{JFA}
Kenny, P., Boulianne, G., Ouellet, P., and Dumouchel, P.
\newblock Joint factor analysis versus eigenchannels in speaker recognition.
\newblock \emph{{IEEE} Trans. Speech Audio Process.}, 15\penalty0 (4):\penalty0
  1435--1447, 2007.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lei et~al.(2014)Lei, Scheffer, Ferrer, and McLaren]{lei2014novel}
Lei, Y., Scheffer, N., Ferrer, L., and McLaren, M.
\newblock A novel scheme for speaker recognition using a phonetically-aware
  deep neural network.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  1695--1699, 2014.

\bibitem[Li et~al.(2013)Li, Ma, and Lee]{langid-overview}
Li, H., Ma, B., and Lee, K.~A.
\newblock Spoken language recognition: from fundamentals to practice.
\newblock \emph{Proceedings of the IEEE}, 101\penalty0 (5):\penalty0
  1136--1159, 2013.

\bibitem[Ling et~al.(2020)Ling, Liu, Salazar, and Kirchhoff]{decoar}
Ling, S., Liu, Y., Salazar, J., and Kirchhoff, K.
\newblock Deep contextualized acoustic representations for semi-supervised
  speech recognition.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, pp.\  6429--6433, 2020.

\bibitem[Liu et~al.(2020)Liu, Yang, Chi, Hsu, and Lee]{Mockingjay}
Liu, A.~T., Yang, S., Chi, P., Hsu, P., and Lee, H.
\newblock Mockingjay: Unsupervised speech representation learning with deep
  bidirectional transformer encoders.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, pp.\  6419--6423, 2020.

\bibitem[Ma et~al.(2016)Ma, Yang, Chen, Huang, and Wang]{ma2016depaudionet}
Ma, X., Yang, H., Chen, Q., Huang, D., and Wang, Y.
\newblock Depaudionet: An efficient deep model for audio based depression
  classification.
\newblock In \emph{Proc. International Workshop on Audio/Visual Emotion
  Challenge}, pp.\  35--42, 2016.

\bibitem[McInnes et~al.(2018)McInnes, Healy, Saul, and Gro{\ss}berger]{umap}
McInnes, L., Healy, J., Saul, N., and Gro{\ss}berger, L.
\newblock {UMAP:} uniform manifold approximation and projection.
\newblock \emph{J. Open Source Softw.}, 3\penalty0 (29):\penalty0 861, 2018.

\bibitem[Murphy(2012)]{murphy2012machine}
Murphy, K.~P.
\newblock \emph{Machine Learning: A Probabilistic Perspective}.
\newblock MIT press, 2012.

\bibitem[Nagrani et~al.(2017)Nagrani, Chung, and Zisserman]{vox1}
Nagrani, A., Chung, J.~S., and Zisserman, A.
\newblock Voxceleb: {A} large-scale speaker identification dataset.
\newblock In Lacerda, F. (ed.), \emph{Proc. Interspeech}, pp.\  2616--2620,
  2017.

\bibitem[Nandwana et~al.(2019)Nandwana, Van~Hout, McLaren, Richey, Lawson, and
  Barrios]{nandwana2019voices}
Nandwana, M.~K., Van~Hout, J., McLaren, M., Richey, C., Lawson, A., and
  Barrios, M.~A.
\newblock The voices from a distance challenge 2019 evaluation plan.
\newblock \emph{arXiv preprint arXiv:1902.10828}, 2019.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{cpc}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and
  Khudanpur]{librispeech}
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.
\newblock Librispeech: An {ASR} corpus based on public domain audio books.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, pp.\  5206--5210, 2015.

\bibitem[Pascual et~al.(2019)Pascual, Ravanelli, Serr{\`{a}}, Bonafonte, and
  Bengio]{paste}
Pascual, S., Ravanelli, M., Serr{\`{a}}, J., Bonafonte, A., and Bengio, Y.
\newblock Learning problem-agnostic speech representations from multiple
  self-supervised tasks.
\newblock In Kubin, G. and Kacic, Z. (eds.), \emph{Proc. Interspeech}, pp.\
  161--165, 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.]{pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{the Journal of machine Learning research}, 12:\penalty0
  2825--2830, 2011.

\bibitem[Prince \& Elder(2007)Prince and Elder]{PLDA}
Prince, S.~J. and Elder, J.~H.
\newblock Probabilistic linear discriminant analysis for inferences about
  identity.
\newblock In \emph{Proc. International Conference on Computer Vision}, pp.\
  1--8, 2007.

\bibitem[Qian et~al.(2019)Qian, Zhang, Chang, Yang, and
  Hasegawa{-}Johnson]{autovc}
Qian, K., Zhang, Y., Chang, S., Yang, X., and Hasegawa{-}Johnson, M.
\newblock Autovc: Zero-shot voice style transfer with only autoencoder loss.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proc.
  International Conference on Machine Learning, {ICML}}, volume~97, pp.\
  5210--5219, 2019.

\bibitem[Qian et~al.(2022)Qian, Zhang, Gao, Ni, Lai, Cox, Hasegawa{-}Johnson,
  and Chang]{contectvec}
Qian, K., Zhang, Y., Gao, H., Ni, J., Lai, C., Cox, D.~D., Hasegawa{-}Johnson,
  M., and Chang, S.
\newblock Contentvec: An improved self-supervised speech representation by
  disentangling speakers.
\newblock In \emph{Proc. International Conference on Machine Learning, {ICML}},
  volume 162, pp.\  18003--18017, 2022.

\bibitem[Radford et~al.(2022)Radford, Kim, Xu, Brockman, McLeavey, and
  Sutskever]{whisper}
Radford, A., Kim, J.~W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock \emph{CoRR}, abs/2212.04356, 2022.

\bibitem[Ravanelli et~al.(2020)Ravanelli, Zhong, Pascual, Swietojanski,
  Monteiro, Trmal, and Bengio]{paste+}
Ravanelli, M., Zhong, J., Pascual, S., Swietojanski, P., Monteiro, J., Trmal,
  J., and Bengio, Y.
\newblock Multi-task self-supervised learning for robust speech recognition.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, pp.\  6989--6993, 2020.

\bibitem[Ravanelli et~al.(2021)Ravanelli, Parcollet, Plantinga, Rouhe, Cornell,
  Lugosch, Subakan, Dawalatabad, Heba, Zhong, Chou, Yeh, Fu, Liao, Rastorgueva,
  Grondin, Aris, Na, Gao, Mori, and Bengio]{speechbrain}
Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch,
  L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh,
  S.-L., Fu, S.-W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na,
  H., Gao, Y., Mori, R.~D., and Bengio, Y.
\newblock {SpeechBrain}: A general-purpose speech toolkit, 2021.
\newblock arXiv:2106.04624.

\bibitem[Schneider et~al.(2019)Schneider, Baevski, Collobert, and Auli]{wa2vec}
Schneider, S., Baevski, A., Collobert, R., and Auli, M.
\newblock wav2vec: Unsupervised pre-training for speech recognition.
\newblock In Kubin, G. and Kacic, Z. (eds.), \emph{Proc. Interspeech 2019},
  pp.\  3465--3469, 2019.

\bibitem[Sculley(2010)]{sculley2010web}
Sculley, D.
\newblock Web-scale k-means clustering.
\newblock In \emph{Proc. International Conference on World Wide Web}, pp.\
  1177--1178, 2010.

\bibitem[Shahnawazuddin et~al.(2021)Shahnawazuddin, Ahmad, Adiga, and
  Kumar]{shahnawazuddin2021children}
Shahnawazuddin, S., Ahmad, W., Adiga, N., and Kumar, A.
\newblock Children's speaker verification in low and zero resource conditions.
\newblock \emph{Digital Signal Processing}, 116:\penalty0 103115, 2021.

\bibitem[Shor et~al.()Shor, Jansen, Han, Park, and Zhang]{shor2022universal}
Shor, J., Jansen, A., Han, W., Park, D., and Zhang, Y.
\newblock Universal paralinguistic speech representations using self-supervised
  conformers.
\newblock In \emph{Proc. ICASSP 2022}, pp.\  3169--3173.

\bibitem[Shor et~al.(2020)Shor, Jansen, Maor, Lang, Tuval, Quitry,
  Tagliasacchi, Shavitt, Emanuel, and Haviv]{shor2020towards}
Shor, J., Jansen, A., Maor, R., Lang, O., Tuval, O., Quitry, F. d.~C.,
  Tagliasacchi, M., Shavitt, I., Emanuel, D., and Haviv, Y.
\newblock Towards learning a universal non-semantic representation of speech.
\newblock \emph{arXiv preprint arXiv:2002.12764}, 2020.

\bibitem[Sinisetty et~al.(2021)Sinisetty, Ruban, Dymov, and
  Ravanelli]{common-lang}
Sinisetty, G., Ruban, P., Dymov, O., and Ravanelli, M.
\newblock Commonlanguage, June 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5036977}.

\bibitem[Snyder et~al.(2018)Snyder, Garcia-Romero, Sell, Povey, and
  Khudanpur]{snyder2018x}
Snyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S.
\newblock X-vectors: Robust {DNN} embeddings for speaker recognition.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  5329--5333, 2018.

\bibitem[Thanh et~al.(2021)Thanh, Viet, and Thu]{thanh2021deep}
Thanh, D.~V., Viet, T.~P., and Thu, T. N.~T.
\newblock Deep speaker verification model for low-resource languages and
  vietnamese dataset.
\newblock In \emph{Proc. Pacific Asia Conference on Language, Information and
  Computation}, pp.\  445--454, 2021.

\bibitem[Tu et~al.(2022)Tu, Lin, and Mak]{svoverview}
Tu, Y., Lin, W., and Mak, M.
\newblock A survey on text-dependent and text-independent speaker verification.
\newblock \emph{{IEEE} Access}, 10:\penalty0 99038--99049, 2022.

\bibitem[Venugopalan et~al.(2021)Venugopalan, Shor, Plakal, Tobin, Tomanek,
  Green, and Brenner]{venugopalan2021comparing}
Venugopalan, S., Shor, J., Plakal, M., Tobin, J., Tomanek, K., Green, J.~R.,
  and Brenner, M.~P.
\newblock Comparing supervised models and learned speech representations for
  classifying intelligibility of disordered speech on selected phrases.
\newblock \emph{arXiv preprint arXiv:2107.03985}, 2021.

\bibitem[Wang et~al.(2022)Wang, Wu, Chen, Liu, Li, Qian, and Yang]{hubert2}
Wang, C., Wu, Y., Chen, S., Liu, S., Li, J., Qian, Y., and Yang, Z.
\newblock Improving self-supervised learning for speech recognition with
  intermediate layer supervision.
\newblock In \emph{Proc. International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP}}, pp.\  7092--7096, 2022.

\bibitem[Wang et~al.(2021)Wang, Boumadane, and Heba]{wang2021fine}
Wang, Y., Boumadane, A., and Heba, A.
\newblock A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion
  recognition, speaker verification and spoken language understanding.
\newblock \emph{arXiv preprint arXiv:2111.02735}, 2021.

\bibitem[Wani et~al.(2021)Wani, Gunawan, Qadri, Kartiwi, and
  Ambikairajah]{emotion-overview}
Wani, T.~M., Gunawan, T.~S., Qadri, S. A.~A., Kartiwi, M., and Ambikairajah, E.
\newblock A comprehensive review of speech emotion recognition systems.
\newblock \emph{IEEE Access}, 9:\penalty0 47795--47814, 2021.

\bibitem[Yang et~al.(2021)Yang, Chi, Chuang, Lai, Lakhotia, Lin, Liu, Shi,
  Chang, Lin, Huang, Tseng, Lee, Liu, Huang, Dong, Li, Watanabe, Mohamed, and
  Lee]{superb}
Yang, S., Chi, P., Chuang, Y., Lai, C.~J., Lakhotia, K., Lin, Y.~Y., Liu,
  A.~T., Shi, J., Chang, X., Lin, G., Huang, T., Tseng, W., Lee, K., Liu, D.,
  Huang, Z., Dong, S., Li, S., Watanabe, S., Mohamed, A., and Lee, H.
\newblock {SUPERB:} speech processing universal performance benchmark.
\newblock In \emph{Proc. Interspeech 2021}, pp.\  1194--1198, 2021.

\end{thebibliography}
