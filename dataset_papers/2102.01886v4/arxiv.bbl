\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi et~al.(2020)Abbasi, Rajabi, Gagn{\'{e}}, and
  Bobba]{AbbasiRGB20}
Abbasi, M., Rajabi, A., Gagn{\'{e}}, C., and Bobba, R.~B.
\newblock Toward adversarial robustness by diversity in an ensemble of
  specialized deep neural networks.
\newblock In \emph{Canadian Conference on Artificial Intelligence}, 2020.

\bibitem[Allen{-}Zhu et~al.(2019)Allen{-}Zhu, Li, and Song]{Allen-ZhuLS19}
Allen{-}Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and Wagner]{AthalyeC018}
Athalye, A., Carlini, N., and Wagner, D.~A.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{{ICML}}, 2018.

\bibitem[Bai et~al.(2021)Bai, Zeng, Jiang, Xia, Ma, and
  Wang]{anonymous2021improving}
Bai, Y., Zeng, Y., Jiang, Y., Xia, S.-T., Ma, X., and Wang, Y.
\newblock Improving adversarial robustness via channel-wise activation
  suppressing.
\newblock In \emph{ICLR}, 2021.

\bibitem[Beck \& Tetruashvili(2013)Beck and Tetruashvili]{BeckT13}
Beck, A. and Tetruashvili, L.
\newblock On the convergence of block coordinate descent type methods.
\newblock \emph{SIAM Journal on Optimization}, 2013.

\bibitem[Bui et~al.(2020)Bui, Le, Zhao, Montague, DeVel, Abraham, and
  Phung]{BuiLZMDAP20}
Bui, A., Le, T., Zhao, H., Montague, P., DeVel, O.~Y., Abraham, T., and Phung,
  D.~Q.
\newblock Improving adversarial robustness by enforcing local and global
  compactness.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Cai et~al.(2018)Cai, Liu, and Song]{CaiLS18}
Cai, Q., Liu, C., and Song, D.
\newblock Curriculum adversarial training.
\newblock In \emph{{IJCAI}}, 2018.

\bibitem[Cao \& Gu(2019)Cao and Gu]{DBLP:journals/corr/abs-1902-01384}
Cao, Y. and Gu, Q.
\newblock A generalization theory of gradient descent for learning
  over-parameterized deep relu networks.
\newblock \emph{CoRR}, abs/1902.01384, 2019.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{Carlini017}
Carlini, N. and Wagner, D.~A.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{{ACM} Workshop on Artificial Intelligence and Security},
  2017.

\bibitem[Cazenavette et~al.(2020)Cazenavette, Murdock, and Lucey]{sparsecoding}
Cazenavette, G., Murdock, C., and Lucey, S.
\newblock Architectural adversarial robustness: The case for deep pursuit.
\newblock \emph{CoRR}, abs/2011.14427, 2020.

\bibitem[Chen et~al.(2020)Chen, Zhang, Xue, Gong, Liu, Ji, and
  Doermann]{ChenZXGLJD20}
Chen, H., Zhang, B., Xue, S., Gong, X., Liu, H., Ji, R., and Doermann, D.~S.
\newblock Anti-bandit neural architecture search for model defense.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Ciss{\'{e}} et~al.(2017)Ciss{\'{e}}, Bojanowski, Grave, Dauphin, and
  Usunier]{CisseBGDU17}
Ciss{\'{e}}, M., Bojanowski, P., Grave, E., Dauphin, Y.~N., and Usunier, N.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{{ICML}}, 2017.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{CohenRK19}
Cohen, J.~M., Rosenfeld, E., and Kolter, J.~Z.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Croce \& Hein(2020)Croce and Hein]{Croce020a}
Croce, F. and Hein, M.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{{ICML}}, 2020.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Schoenholz, and Le]{CubukZSL18}
Cubuk, E.~D., Zoph, B., Schoenholz, S.~S., and Le, Q.~V.
\newblock Intriguing properties of adversarial examples.
\newblock In \emph{{ICLR} Workshop}, 2018.

\bibitem[Danskin(2012)]{danskin}
Danskin, J.~M.
\newblock The theory of max-min and its application to weapons allocation
  problems.
\newblock In \emph{Springer Science \& Business Media}, 2012.

\bibitem[Dong et~al.(2020{\natexlab{a}})Dong, Li, Wang, and Xu]{abs-2009-00902}
Dong, M., Li, Y., Wang, Y., and Xu, C.
\newblock Adversarially robust neural architectures.
\newblock \emph{CoRR}, abs/2009.00902, 2020{\natexlab{a}}.

\bibitem[Dong et~al.(2020{\natexlab{b}})Dong, Deng, Pang, Zhu, and
  Su]{abs-2002-05999}
Dong, Y., Deng, Z., Pang, T., Zhu, J., and Su, H.
\newblock Adversarial distributional training for robust deep learning.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{DuLL0Z19}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Dziugaite et~al.(2016)Dziugaite, Ghahramani, and Roy]{DziugaiteGR16}
Dziugaite, G.~K., Ghahramani, Z., and Roy, D.~M.
\newblock A study of the effect of {JPG} compression on adversarial images.
\newblock \emph{CoRR}, abs/1608.00853, 2016.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{GhadimiL13a}
Ghadimi, S. and Lan, G.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 2013.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{GoodfellowSS14}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Guo et~al.(2020)Guo, Yang, Xu, Liu, and Lin]{GuoYX0L20}
Guo, M., Yang, Y., Xu, R., Liu, Z., and Lin, D.
\newblock When {NAS} meets robustness: In search of robust architectures
  against adversarial attacks.
\newblock In \emph{{CVPR}}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{HeZRS16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Hein \& Andriushchenko(2017)Hein and Andriushchenko]{HeinA17}
Hein, M. and Andriushchenko, M.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Hosseini et~al.(2020)Hosseini, Yang, and Xie]{hosseini2020dsrna}
Hosseini, R., Yang, X., and Xie, P.
\newblock Dsrna: Differentiable search of robust neural architectures.
\newblock \emph{CoRR}, abs/2012.06122, 2020.

\bibitem[Hu et~al.(2018)Hu, Shen, and Sun]{HuSS18}
Hu, J., Shen, L., and Sun, G.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{HuangLMW17}
Huang, G., Liu, Z., van~der Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{{CVPR}}, 2017.

\bibitem[Kariyappa \& Qureshi(2019)Kariyappa and Qureshi]{abs-1901-09981}
Kariyappa, S. and Qureshi, M.~K.
\newblock Improving adversarial robustness of ensembles with diversity
  training.
\newblock \emph{CoRR}, abs/1901.09981, 2019.

\bibitem[Kim et~al.(2020)Kim, Chudomelka, Park, Kang, Hong, and
  Kim]{KimCPKHK20}
Kim, B., Chudomelka, B., Park, J., Kang, J., Hong, Y., and Kim, H.~J.
\newblock Robust neural networks inspired by strong stability preserving
  runge-kutta methods.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2014.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{Li0TSG18}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, He, and Lin]{lmjicml2020}
Li, M., He, L., and Lin, Z.
\newblock Implicit euler skip connections: Enhancing adversarial robustness via
  numerical stability.
\newblock In \emph{{ICML}}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2019)Li, Yi, Zhou, and Zhang]{LiYZZ19}
Li, P., Yi, J., Zhou, B., and Zhang, L.
\newblock Improving the robustness of deep neural networks via adversarial
  training with triplet loss.
\newblock In \emph{{IJCAI}}, 2019.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Dong, Wang, and Xu]{LiDWX20}
Li, Y., Dong, M., Wang, Y., and Xu, C.
\newblock Neural architecture search in {A} proxy validation loss landscape.
\newblock In \emph{{ICML}}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2018)Liu, Cheng, Zhang, and Hsieh]{LiuCZH18}
Liu, X., Cheng, M., Zhang, H., and Hsieh, C.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In \emph{{ECCV}}, 2018.

\bibitem[Ma et~al.(2020)Ma, Faghri, and
  Farahmand]{DBLP:journals/corr/abs-2004-01832}
Ma, A., Faghri, F., and Farahmand, A.
\newblock Adversarial robustness through regularization: {A} second-order
  approach.
\newblock \emph{CoRR}, abs/2004.01832, 2020.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{MadryMSTV18}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{NairH10}
Nair, V. and Hinton, G.~E.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{ICML}, 2010.

\bibitem[Naseer et~al.(2020)Naseer, Khan, Hayat, Khan, and
  Porikli]{NaseerKHKP20}
Naseer, M., Khan, S., Hayat, M., Khan, F.~S., and Porikli, F.
\newblock A self-supervised approach for adversarial robustness.
\newblock In \emph{{CVPR}}, 2020.

\bibitem[Nesterov(2004)]{Nesterov04}
Nesterov, Y.~E.
\newblock \emph{Introductory Lectures on Convex Optimization - {A} Basic
  Course}.
\newblock 2004.

\bibitem[Ning et~al.(2020)Ning, Zhao, Li, Zhao, Yang, and Wang]{abs-2012-11835}
Ning, X., Zhao, J., Li, W., Zhao, T., Yang, H., and Wang, Y.
\newblock Multi-shot {NAS} for discovering adversarially robust convolutional
  neural architectures at targeted capacities.
\newblock \emph{CoRR}, abs/2012.11835, 2020.

\bibitem[Pang et~al.(2019)Pang, Xu, Du, Chen, and Zhu]{PangXDCZ19}
Pang, T., Xu, K., Du, C., Chen, N., and Zhu, J.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Pang et~al.(2021)Pang, Yang, Dong, Su, and Zhu]{pang2021bag}
Pang, T., Yang, X., Dong, Y., Su, H., and Zhu, J.
\newblock Bag of tricks for adversarial training.
\newblock In \emph{ICLR}, 2021.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{PascanuMB13}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{{ICML}}, 2013.

\bibitem[Paulavičius \& Žilinskas(2006)Paulavičius and
  Žilinskas]{Lipschitz}
Paulavičius, R. and Žilinskas, J.
\newblock Analysis of different norms and corresponding lipschitz constants for
  global optimization.
\newblock \emph{Ukio Technologinis ir Ekonominis Vystymas}, 12\penalty0
  (4):\penalty0 301--306, 2006.

\bibitem[Qian \& Wegman(2019)Qian and Wegman]{QianW19}
Qian, H. and Wegman, M.~N.
\newblock L2-nonexpansive neural networks.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Rahnama et~al.(2020)Rahnama, Nguyen, and Raff]{RahnamaNR20}
Rahnama, A., Nguyen, A.~T., and Raff, E.
\newblock Robust design of deep neural networks against adversarial attacks
  based on lyapunov theory.
\newblock In \emph{{CVPR}}, 2020.

\bibitem[Ramachandran et~al.(2018)Ramachandran, Zoph, and Le]{RamachandranZL18}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions.
\newblock In \emph{{ICLR} Workshop}, 2018.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{DBLP:conf/icml/RiceWK20}
Rice, L., Wong, E., and Kolter, J.~Z.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119, pp.\
  8093--8104, 2020.

\bibitem[S. \& Babu(2020)S. and Babu]{SB20}
S., V.~B. and Babu, R.~V.
\newblock Single-step adversarial training with dropout scheduling.
\newblock In \emph{{CVPR}}, 2020.

\bibitem[Shu et~al.(2020)Shu, Wang, and Cai]{Shu0C20}
Shu, Y., Wang, W., and Cai, S.
\newblock Understanding architectures learnt by cell-based neural architecture
  search.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Tan et~al.(2019)Tan, Chen, Pang, Vasudevan, Sandler, Howard, and
  Le]{TanCPVSHL19}
Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., and Le,
  Q.~V.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In \emph{{CVPR}}, 2019.

\bibitem[Tsuzuku et~al.(2018)Tsuzuku, Sato, and
  Sugiyama]{NeurIPS:Tsuzuku+etal:2018}
Tsuzuku, Y., Sato, I., and Sugiyama, M.
\newblock Lipschitz-{M}argin training: {S}calable certification of perturbation
  invariance for deep neural networks.
\newblock In \emph{NeurIPS}, pp.\  6541--6550, 2018.

\bibitem[Vargas \& Kotyan(2019)Vargas and Kotyan]{abs-1906-11667}
Vargas, D.~V. and Kotyan, S.
\newblock Evolving robust neural architectures to defend from adversarial
  attacks.
\newblock \emph{CoRR}, abs/1906.11667, 2019.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Shi, and Osher]{WangSO19}
Wang, B., Shi, Z., and Osher, S.~J.
\newblock Resnets ensemble via the feynman-kac formalism to improve natural and
  robust accuracies.
\newblock In \emph{NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Wang \& Yu(2019)Wang and Yu]{WangY19}
Wang, H. and Yu, C.
\newblock A direct approach to robust deep learning using adversarial networks.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Chen, Gui, Hu, Liu, and
  Wang]{WangCGH0W20}
Wang, H., Chen, T., Gui, S., Hu, T., Liu, J., and Wang, Z.
\newblock Once-for-all adversarial training: In-situ tradeoff between
  robustness and accuracy for free.
\newblock In \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Ma, Bailey, Yi, Zhou, and
  Gu]{WangM0YZG19}
Wang, Y., Ma, X., Bailey, J., Yi, J., Zhou, B., and Gu, Q.
\newblock On the convergence and robustness of adversarial training.
\newblock In \emph{{ICML}}, 2019{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Zou, Yi, Bailey, Ma, and
  Gu]{0001ZY0MG20}
Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{{ICLR}}, 2020{\natexlab{b}}.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Yi, Su, Gao, Hsieh, and
  Daniel]{WengZCYSGHD18}
Weng, T., Zhang, H., Chen, P., Yi, J., Su, D., Gao, Y., Hsieh, C., and Daniel,
  L.
\newblock Evaluating the robustness of neural networks: An extreme value theory
  approach.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Wong \& Kolter(2018)Wong and Kolter]{WongK18}
Wong, E. and Kolter, J.~Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{{ICML}}, 2018.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{WongRK20}
Wong, E., Rice, L., and Kolter, J.~Z.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{WuX020}
Wu, D., Xia, S., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xie et~al.(2018)Xie, Wang, Zhang, Ren, and Yuille]{XieWZRY18}
Xie, C., Wang, J., Zhang, Z., Ren, Z., and Yuille, A.~L.
\newblock Mitigating adversarial effects through randomization.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Xie et~al.(2020{\natexlab{a}})Xie, Tan, Gong, Yuille, and
  Le]{smmothadv}
Xie, C., Tan, M., Gong, B., Yuille, A.~L., and Le, Q.~V.
\newblock Smooth adversarial training.
\newblock \emph{CoRR}, abs/2006.14536, 2020{\natexlab{a}}.

\bibitem[Xie et~al.(2020{\natexlab{b}})Xie, Chen, Bi, Wei, Xu, Chen, Wang,
  Xiao, Chang, Zhang, and Tian]{optgap}
Xie, L., Chen, X., Bi, K., Wei, L., Xu, Y., Chen, Z., Wang, L., Xiao, A.,
  Chang, J., Zhang, X., and Tian, Q.
\newblock Weight-sharing neural architecture search: {A} battle to shrink the
  optimization gap.
\newblock \emph{CoRR}, abs/2008.01475, 2020{\natexlab{b}}.

\bibitem[Yue et~al.(2020)Yue, Lin, Huang, and Zhang]{abs-2011-09820}
Yue, Z., Lin, B., Huang, X., and Zhang, Y.
\newblock Effective, efficient and robust neural architecture search.
\newblock \emph{CoRR}, abs/2011.09820, 2020.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{ZagoruykoK16}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{{BMVC}}, 2016.

\bibitem[Zhai et~al.(2019)Zhai, Cai, He, Dan, He, Hopcroft, and
  Wang]{abs-1906-00555}
Zhai, R., Cai, T., He, D., Dan, C., He, K., Hopcroft, J.~E., and Wang, L.
\newblock Adversarially robust generalization just requires more unlabeled
  data.
\newblock \emph{CoRR}, abs/1906.00555, 2019.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, Lu, Zhu, and
  Dong]{ZhangZLZ019}
Zhang, D., Zhang, T., Lu, Y., Zhu, Z., and Dong, B.
\newblock You only propagate once: Accelerating adversarial training via
  maximal principle.
\newblock In \emph{NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{ZhangYJXGJ19}
Zhang, H., Yu, Y., Jiao, J., Xing, E.~P., Ghaoui, L.~E., and Jordan, M.~I.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{{ICML}}, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Chen, Xiao, Gowal, Stanforth,
  Li, Boning, and Hsieh]{DBLP:conf/iclr/ZhangCXGSLBH20}
Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., Boning, D.~S.,
  and Hsieh, C.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock In \emph{{ICLR}}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Xu, Han, Niu, Cui, Sugiyama,
  and Kankanhalli]{abs-2002-11242}
Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., and Kankanhalli, M.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Zhang \& Liang(2019)Zhang and Liang]{ZhangL19}
Zhang, Y. and Liang, P.
\newblock Defending against whitebox adversarial attacks via randomized
  discretization.
\newblock In \emph{{AISTATS}}, 2019.

\end{thebibliography}
