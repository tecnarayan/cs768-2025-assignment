@incollection{lange2012batch,
 author = {Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
 booktitle = {Reinforcement learning},
 publisher = {Springer},
 title = {Batch reinforcement learning},
 year = {2012}
}

@article{td3+bc,
 author = {Fujimoto, Scott and Gu, Shixiang Shane},
 journal = {NIPS},
 title = {A minimalist approach to offline reinforcement learning},
 year = {2021}
}

@inproceedings{CQL,
 author = {Aviral Kumar and
Aurick Zhou and
George Tucker and
Sergey Levine},
 booktitle = {NIPS},
 title = {Conservative Q-Learning for Offline Reinforcement Learning},
 year = {2020}
}

@inproceedings{
IQL,
title={Offline Reinforcement Learning with Implicit Q-Learning},
author={Ilya Kostrikov and Ashvin Nair and Sergey Levine},
booktitle={NIPS Deep RL Workshop},
year={2021},
}

@inproceedings{chen2020bail,
 author = {Xinyue Chen and
Zijian Zhou and
Zheng Wang and
Che Wang and
Yanqiu Wu and
Keith Ross},
 booktitle = {NIPS},
 title = {{BAIL:} Best-Action Imitation Learning for Batch Deep Reinforcement
Learning},
 year = {2020}
}

@inproceedings{wang2020critic,
 author = {Ziyu Wang and
Alexander Novikov and
Konrad Zolna and
Josh Merel and
Jost Tobias Springenberg and
Scott E. Reed and
Bobak Shahriari and
Noah Y. Siegel and
{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
Nicolas Heess and
Nando de Freitas},
 booktitle = {NIPS},
 title = {Critic Regularized Regression},
 year = {2020}
}

@article{chen2021decision,
 author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
 journal = {NIPS},
 title = {Decision transformer: Reinforcement learning via sequence modeling},
 year = {2021}
}

@inproceedings{oh2018self,
 author = {Junhyuk Oh and
Yijie Guo and
Satinder Singh and
Honglak Lee},
 booktitle = {ICML},
 title = {Self-Imitation Learning},
 year = {2018}
}

@inproceedings{shen2021rank,
 author = {Shen, Yichen and Fang, Zhou and Xu, Yunkun and Cao, Yu and Zhu, Jiangcheng},
 booktitle = {CEI},
 title = {A Rank-Based Sampling Framework For Offline Reinforcement Learning},
 year = {2021}
}

@inproceedings{kang2019decoupling,
 author = {Bingyi Kang and
Saining Xie and
Marcus Rohrbach and
Zhicheng Yan and
Albert Gordo and
Jiashi Feng and
Yannis Kalantidis},
 booktitle = {ICLR},
 title = {Decoupling Representation and Classifier for Long-Tailed Recognition},
 year = {2020}
}

@article{peng2019advantage,
 author = {Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
 journal = {arXiv preprint arXiv:1910.00177},
 title = {Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
 year = {2019}
}

@article{nair2020awac,
 author = {Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
 journal = {arXiv preprint arXiv:2006.09359},
 title = {Awac: Accelerating online reinforcement learning with offline datasets},
 year = {2020}
}

@inproceedings{
yarats2022dont,
title={Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning},
author={Denis Yarats and David Brandfonbrener and Hao Liu and Michael Laskin and Pieter Abbeel and Alessandro Lazaric and Lerrel Pinto},
booktitle={ICLR},
year={2022},
}

@inproceedings{fujimoto2019off,
title={Off-Policy Deep Reinforcement Learning without Exploration},
 author = {Scott Fujimoto and
David Meger and
Doina Precup},
 booktitle = {ICML},
 year = {2019}
}


@article{MuZero,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}


@article{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={NIPS},
  year={2019}
}

@article{jaques2019way,
  publtype={informal},
  author={Natasha Jaques and Asma Ghandeharioun and Judy Hanwen Shen and Craig Ferguson and Ã€gata Lapedriza and Noah Jones and Shixiang Gu and Rosalind W. Picard},
  title={Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog},
  year={2019},
  journal={CoRR},
}

@misc{fu2020d4rl,
    title={D4RL: Datasets for Deep Data-Driven Reinforcement Learning},
    author={Justin Fu and Aviral Kumar and Ofir Nachum and George Tucker and Sergey Levine},
    year={2020},
    eprint={2004.07219},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{traj_transformer,
 author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
 booktitle = {NIPS},
 title = {Offline Reinforcement Learning as One Big Sequence Modeling Problem},
 year = {2021}
}

@article{long-tail-survey,
  title={Deep long-tailed learning: A survey},
  author={Zhang, Yifan and Kang, Bingyi and Hooi, Bryan and Yan, Shuicheng and Feng, Jiashi},
  journal={arXiv preprint arXiv:2110.04596},
  year={2021}
}


@inproceedings{buckman2020pessimism,
  title={The importance of pessimism in fixed-dataset policy optimization},
  author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
  year={2021},
  booktitle={ICLR},
}

@inproceedings{PER,
  author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  title={Prioritized Experience Replay},
  year={2016},
  booktitle={ICLR},
}

@article{wang2018exponentially,
  title={Exponentially weighted imitation learning for batched historical data},
  author={Wang, Qing and Xiong, Jiechao and Han, Lei and Liu, Han and Zhang, Tong and others},
  journal={NIPS},
  year={2018}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@article{gulcehre2020rl,
  title={Rl unplugged: A suite of benchmarks for offline reinforcement learning},
  author={Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Thomas and G{\'o}mez, Sergio and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh S and Mankowitz, Daniel J and Paduraru, Cosmin and others},
  journal={NIPS},
  year={2020}
}

@inproceedings{Fisher,
  title={Offline reinforcement learning with fisher divergence critic regularization},
  author={Kostrikov, Ilya and Fergus, Rob and Tompson, Jonathan and Nachum, Ofir},
  booktitle={ICML},
  year={2021},
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  year={2002},
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={ICML},
  year={2015},
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{tsitsiklis1996deadly_tirad,
  title={An analysis of temporal-difference learning with function approximationtechnical},
  author={Tsitsiklis, JN and Van Roy, B},
  journal={Rep. LIDS-P-2322). Lab. Inf. Decis. Syst. Massachusetts Inst. Technol. Tech. Rep},
  year={1996}
}

@article{van2018deadly_triad,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@inproceedings{td3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={ICML},
  year={2018},
}

@article{Double-Q,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={NIPS},
  year={2010}
}



@article{onestep,
  title={Offline rl without off-policy evaluation},
  author={Brandfonbrener, David and Whitney, Will and Ranganath, Rajesh and Bruna, Joan},
  journal={NIPS},
  year={2021}
}

@article{gae,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={ICLR},
  year={2016}
}

@article{yu2021combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={NIPS},
  year={2021}
}

@article{liu2021curriculum,
  title={Curriculum offline imitating learning},
  author={Liu, Minghuan and Zhao, Hanye and Yang, Zhengyu and Shen, Jian and Zhang, Weinan and Zhao, Li and Liu, Tie-Yan},
  journal={NIPS},
  year={2021}
}



@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={ICML},
  year={2023},
}

@inproceedings{fu2019diagnosing,
  title={Diagnosing bottlenecks in deep q-learning algorithms},
  author={Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
  booktitle={ICML},
  year={2019},
}

@article{fujimoto2020equivalence,
  title={An equivalence between loss functions and non-uniform sampling in experience replay},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  journal={NIPS},
  year={2020}
}

@article{sac-n,
  title={Uncertainty-based offline reinforcement learning with diversified q-ensemble},
  author={An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
  journal={NIPS},
  year={2021}
}

@article{lb-sac,
  title={Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size},
  author={Nikulin, Alexander and Kurenkov, Vladislav and Tarasov, Denis and Akimov, Dmitry and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2211.11092},
  year={2022}
}

@article{ghasemipour2022so,
  title={Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters},
  author={Ghasemipour, Kamyar and Gu, Shixiang Shane and Nachum, Ofir},
  journal={NIPS},
  year={2022}
}

@inproceedings{
wang2023diffusion,
title={Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning},
author={Zhendong Wang and Jonathan J Hunt and Mingyuan Zhou},
booktitle={ICLR},
year={2023},
}

@inproceedings{chen2022offline,
  title={Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling},
  author={Chen, Huayu and Lu, Cheng and Ying, Chengyang and Su, Hang and Zhu, Jun},
 booktitle={ICLR},
year={2023},
}

@inproceedings{
hong2023harnessing,
title={Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting},
author={Zhang-Wei Hong and Pulkit Agrawal and Remi Tachet des Combes and Romain Laroche},
booktitle={ICLR},
year={2023},
}

@article{singh2022offline,
  title={Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints},
  author={Singh, Anikait and Kumar, Aviral and Vuong, Quan and Chebotar, Yevgen and Levine, Sergey},
  journal={arXiv preprint arXiv:2211.01052},
  year={2022}
}

@inproceedings{de2015importance,
  title={The importance of experience replay database composition in deep reinforcement learning},
  author={De Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu{\v{s}}ka, Robert},
  booktitle={Deep reinforcement learning workshop, NIPS},
  year={2015}
}

@article{zhang2017deeper,
  title={A deeper look at experience replay},
  author={Zhang, Shangtong and Sutton, Richard S},
  journal={arXiv preprint arXiv:1712.01275},
  year={2017}
}

@article{levine2020tutorial,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@incollection{baird1995residual,
  title={Residual algorithms: Reinforcement learning with function approximation},
  author={Baird, Leemon},
  booktitle={Machine Learning Proceedings},
  year={1995},
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  year={2015},
}

@article{achiam2019charact,
  title={Towards characterizing divergence in deep q-learning},
  author={Achiam, Joshua and Knight, Ethan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1903.08894},
  year={2019}
}


@inproceedings{zhang2021breaking,
  title={Breaking the deadly triad with a target network},
  author={Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={12621--12631},
  year={2021},
  organization={PMLR}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{yue2022red,
  title={Boosting offline reinforcement learning via data rebalancing},
  author={Yue, Yang and Kang, Bingyi and Ma, Xiao and Xu, Zhongwen and Huang, Gao and Yan, Shuicheng},
  booktitle={Offline RL Workshop},
  year={2022}
}



@article{yue2023offline,
  title={Offline Prioritized Experience Replay},
  author={Yue, Yang and Kang, Bingyi and Ma, Xiao and Huang, Gao and Song, Shiji and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2306.05412},
  year={2023}
}

@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={16--18},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{gulcehre2020rlunplug,
  title={Rl unplugged: A suite of benchmarks for offline reinforcement learning},
  author={Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Thomas and G{\'o}mez, Sergio and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh S and Mankowitz, Daniel J and Paduraru, Cosmin and others},
  journal={NIPS},
  year={2020}
}


@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={NIPS},
  year={2018}
}

@inproceedings{BN,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={ICML},
  year={2015}
}

@misc{kang2023improving,
      title={Improving and Benchmarking Offline Reinforcement Learning Algorithms}, 
      author={Bingyi Kang and Xiao Ma and Yirui Wang and Yang Yue and Shuicheng Yan},
      year={2023},
      eprint={2306.00972},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{SN,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}
@article{WN,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={NIPS},
  year={2016}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={JMLR},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2019}
}


@article{bhatt2019crossnorm,
  title={Crossnorm: Normalization for off-policy td reinforcement learning},
  author={Bhatt, Aditya and Argus, Max and Amiranashvili, Artemij and Brox, Thomas},
  journal={arXiv preprint arXiv:1902.05605},
  year={2019}
}



@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2018},
}

@inproceedings{yue2023vcr,
  title={Value-Consistent Representation Learning for Data-Efficient Reinforcement Learning},
  author={Yue, Yang and Kang, Bingyi and Xu, Zhongwen and Huang, Gao and Yan, Shuicheng},
  booktitle={AAAI},
  year={2023}
}

@inproceedings{ma2022mutual,
  title={Mutual Information Regularized Offline Reinforcement Learning},
  author={Ma, Xiao and Kang, Bingyi and Xu, Zhongwen and Lin, Min and Yan, Shuicheng},
  booktitle={NIPS},
  year={2023}
}

@inproceedings{kang2023efficient,
  title={Efficient Diffusion Policies for Offline Reinforcement Learning},
  author={Kang, Bingyi and Ma, Xiao and Du, Chao and Pang, Tianyu and Yan, Shuicheng},
  booktitle={NIPS},
  year={2023}
}


@inproceedings{
scaled-ql,
title={Offline Q-learning on Diverse Multi-Task Data Both Scales And Generalizes},
author={Aviral Kumar and Rishabh Agarwal and Xinyang Geng and George Tucker and Sergey Levine},
booktitle={ICLR},
year={2023},
}

@inproceedings{
dr3,
title={DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization},
author={Aviral Kumar and Rishabh Agarwal and Tengyu Ma and Aaron Courville and George Tucker and Sergey Levine},
booktitle={ICLR},
year={2022},
}

@misc{lu2021power,
      title={On the Power of Multitask Representation Learning in Linear MDP}, 
      author={Rui Lu and Gao Huang and Simon S. Du},
      year={2021},
      eprint={2106.08053},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lu2022provable,
      title={Provable General Function Class Representation Learning in Multitask Bandits and MDPs}, 
      author={Rui Lu and Andrew Zhao and Simon S. Du and Gao Huang},
      year={2022},
      booktitle={NIPS},

}

@inproceedings{
xu2021how,
title={How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
author={Keyulu Xu and Mozhi Zhang and Jingling Li and Simon Shaolei Du and Ken-Ichi Kawarabayashi and Stefanie Jegelka},
booktitle={ICLR},
year={2021},
}


@InProceedings{zhang21y,
  title = 	 {Breaking the Deadly Triad with a Target Network},
  author =       {Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@inproceedings{ddpg,
  author       = {Timothy P. Lillicrap and
                  Jonathan J. Hunt and
                  Alexander Pritzel and
                  Nicolas Heess and
                  Tom Erez and
                  Yuval Tassa and
                  David Silver and
                  Daan Wierstra},
  title        = {Continuous control with deep reinforcement learning},
  booktitle    = {ICLR},
  year         = {2016},
}

@article{yang2023hundreds,
  title={Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance},
  author={Yang, Qisen and Wang, Shenzhi and Zhang, Qihang and Huang, Gao and Song, Shiji},
  journal={arXiv preprint arXiv:2309.01448},
  year={2023}
}


@inproceedings{yang2023boosting,
  title={Boosting Offline Reinforcement Learning with Action Preference Query},
  author={Yang, Qisen and Wang, Shenzhi and Lin, Matthieu Gaetan and Song, Shiji and Huang, Gao},
  booktitle={ICML},
  year={2023}
}




@InProceedings{pu2023adaptive,
  title     = {Adaptive Rotated Convolution for Rotated Object Detection},
  author    = {Pu, Yifan and Wang, Yiru and Xia, Zhuofan and Han, Yizeng and Wang, Yulin and Gan, Weihao and Wang, Zidong and Song, Shiji and Huang, Gao},
  booktitle = {ICCV},
  year      = {2023}
}

@inproceedings{han2022learning,
  title={Learning to weight samples for dynamic early-exiting networks},
  author={Han, Yizeng and Pu, Yifan and Lai, Zihang and Wang, Chaofei and Song, Shiji and Cao, Junfeng and Huang, Wenhui and Deng, Chao and Huang, Gao},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{wang2021towards,
  title={Towards learning spatially discriminative feature representations},
  author={Wang, Chaofei and Xiao, Jiayu and Han, Yizeng and Yang, Qisen and Song, Shiji and Huang, Gao},
  booktitle={ICCV},
  year={2021}
}

@article{wang2022efficient,
  title={Efficient knowledge distillation from model checkpoints},
  author={Wang, Chaofei and Yang, Qisen and Huang, Rui and Song, Shiji and Huang, Gao},
  journal={NIPS},
  year={2022}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={NIPS},
  year={2012}
}