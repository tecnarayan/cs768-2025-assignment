@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}

@article{yang2023stability,
  title={Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms},
  author={Yang, Ming and Wei, Xiyuan and Yang, Tianbao and Ying, Yiming},
  journal={arXiv preprint arXiv:2307.03357},
  year={2023}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}

@inproceedings{qu2023prevent,
  title={How To Prevent the Poor Performance Clients for Personalized Federated Learning?},
  author={Qu, Zhe and Li, Xingyu and Han, Xiao and Duan, Rui and Shen, Chengchao and Chen, Lixing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12167--12176},
  year={2023}
}

@inproceedings{charles2018stability,
  title={Stability and generalization of learning algorithms that converge to global optima},
  author={Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={International conference on machine learning},
  pages={745--754},
  year={2018},
  organization={PMLR}
}

@inproceedings{sakaue2023improved,
  title={Improved Generalization Bound and Learning of Sparsity Patterns for Data-Driven Low-Rank Approximation},
  author={Sakaue, Shinsaku and Oki, Taihei},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1--10},
  year={2023},
  organization={PMLR}
}

@article{ji2022theoretical,
  title={Theoretical convergence of multi-step model-agnostic meta-learning},
  author={Ji, Kaiyi and Yang, Junjie and Liang, Yingbin},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={1317--1357},
  year={2022},
  publisher={JMLRORG}
}

@article{finn2018probabilistic,
  title={Probabilistic model-agnostic meta-learning},
  author={Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{qi2021stochastic,
  title={Stochastic optimization of areas under precision-recall curves with provable convergence},
  author={Qi, Qi and Luo, Youzhi and Xu, Zhao and Ji, Shuiwang and Yang, Tianbao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1752--1765},
  year={2021}
}

@article{dann2014policy,
  title={Policy evaluation with temporal differences: A survey and comparison},
  author={Dann, Christoph and Neumann, Gerhard and Peters, Jan and others},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={809--883},
  year={2014},
  publisher={Massachusetts Institute of Technology Press (MIT Press)/Microtome Publishing}
}

@article{tran2020hybrid,
  title={Hybrid variance-reduced sgd algorithms for minimax problems with nonconvex-linear function},
  author={Tran Dinh, Quoc and Liu, Deyi and Nguyen, Lam},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11096--11107},
  year={2020}
}

@inproceedings{mao2022improving,
  title={On improving model-free algorithms for decentralized multi-agent reinforcement learning},
  author={Mao, Weichao and Yang, Lin and Zhang, Kaiqing and Basar, Tamer},
  booktitle={International Conference on Machine Learning},
  pages={15007--15049},
  year={2022},
  organization={PMLR}
}

@article{hu2019efficient,
  title={Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent},
  author={Hu, Wenqing and Li, Chris Junchi and Lian, Xiangru and Liu, Ji and Yuan, Huizhuo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{jiang2022optimal,
	title={Optimal algorithms for stochastic multi-level compositional optimization},
	author={Jiang, Wei and Wang, Bokun and Wang, Yibo and Zhang, Lijun and Yang, Tianbao},
	booktitle={International Conference on Machine Learning},
	pages={10195--10216},
	year={2022},
	organization={PMLR}
}

@article{cutkosky2019momentum,
	title={Momentum-based variance reduction in non-convex sgd},
	author={Cutkosky, Ashok and Orabona, Francesco},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@inproceedings{liu2023breaking,
  title={Faster Stochastic Variance Reduction Methods for Compositional MiniMax Optimization},
  author={Liu, Jin and Pan, Xiaokang and Duan, Junwen and Li, Hong-Dong and Li, Youqi and Qu, Zhe},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={12},
  pages={13927--13935},
  year={2024}
}

@article{qu2023convergence,
  title={On the Convergence of Multi-Server Federated Learning With Overlapping Area},
  author={Qu, Zhe and Li, Xingyu and Xu, Jie and Tang, Bo and Lu, Zhuo and Liu, Yao},
  journal={IEEE Transactions on Mobile Computing},
  volume={22},
  number={11},
  pages={6647--6662},
  year={2023},
  publisher={IEEE}
}

@article{xian2021faster,
  title={A faster decentralized algorithm for nonconvex minimax problems},
  author={Xian, Wenhan and Huang, Feihu and Zhang, Yanfu and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25865--25877},
  year={2021}
}

@article{levy2021storm+,
  title={STORM+: Fully Adaptive SGD with Momentum for Nonconvex Optimization},
  author={Levy, Kfir Y and Kavis, Ali and Cevher, Volkan},
  journal={arXiv preprint arXiv:2111.01040},
  year={2021}
}

@article{huang2021super,
  title={Super-adam: faster and universal framework of adaptive gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9074--9085},
  year={2021}
}

@article{hu2023non,
  title={Non-smooth weakly-convex finite-sum coupled compositional optimization},
  author={Hu, Quanqi and Zhu, Dixian and Yang, Tianbao},
  journal={arXiv preprint arXiv:2310.03234},
  year={2023}
}

@article{balasubramanian2022stochastic,
  title={Stochastic multilevel composition optimization algorithms with level-independent convergence rates},
  author={Balasubramanian, Krishnakumar and Ghadimi, Saeed and Nguyen, Anthony},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={519--544},
  year={2022},
  publisher={SIAM}
}

@article{gao2021fast,
  title={Fast training method for stochastic compositional optimization problems},
  author={Gao, Hongchang and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25334--25345},
  year={2021}
}

@article{wang2017stochastic,
  title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
  author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
  journal={Mathematical Programming},
  volume={161},
  pages={419--449},
  year={2017},
  publisher={Springer}
}

@article{chen2021solving,
  title={Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization},
  author={Chen, Tianyi and Sun, Yuejiao and Yin, Wotao},
  journal={IEEE Transactions on Signal Processing},
  volume={69},
  pages={4937--4948},
  year={2021},
  publisher={IEEE}
}
@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{li2023fedlga,
  title={FedLGA: Toward System-Heterogeneity of Federated Learning via Local Gradient Approximation},
  author={Li, Xingyu and Qu, Zhe and Tang, Bo and Lu, Zhuo},
  journal={IEEE Transactions on Cybernetics},
  year={2023},
  publisher={IEEE}
}

@article{zhoustochastic,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={4130--4192},
  year={2020},
  publisher={JMLRORG}
}

@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}
@article{wen2018flipout,
  title={Flipout: Efficient pseudo-independent weight perturbations on mini-batches},
  author={Wen, Yeming and Vicol, Paul and Ba, Jimmy and Tran, Dustin and Grosse, Roger},
  journal={arXiv preprint arXiv:1803.04386},
  year={2018}
}
@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{yuan2021compositional,
  title={Compositional training for end-to-end deep AUC maximization},
  author={Yuan, Zhuoning and Guo, Zhishuai and Chawla, Nitesh and Yang, Tianbao},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{devroye1979distribution,
  title={Distribution-free performance bounds for potential function rules},
  author={Devroye, Luc and Wagner, Terry},
  journal={IEEE Transactions on Information Theory},
  volume={25},
  number={5},
  pages={601--604},
  year={1979},
  publisher={IEEE}
}

@inproceedings{kearns1997algorithmic,
  title={Algorithmic stability and sanity-check bounds for leave-one-out cross-validation},
  author={Kearns, Michael and Ron, Dana},
  booktitle={Proceedings of the tenth annual conference on Computational learning theory},
  pages={152--162},
  year={1997}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@article{rakhlin2005stability,
  title={Stability results in learning theory},
  author={Rakhlin, Alexander and Mukherjee, Sayan and Poggio, Tomaso},
  journal={Analysis and Applications},
  volume={3},
  number={04},
  pages={397--417},
  year={2005},
  publisher={World Scientific}
}
@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}
@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}

@article{vapnik2000bounds,
  title={Bounds on error expectation for support vector machines},
  author={Vapnik, Vladimir and Chapelle, Olivier},
  journal={Neural computation},
  volume={12},
  number={9},
  pages={2013--2036},
  year={2000},
  publisher={MIT Press}
}

@article{cucker2002mathematical,
  title={On the mathematical foundations of learning},
  author={Cucker, Felipe and Smale, Steve},
  journal={Bulletin of the American mathematical society},
  volume={39},
  number={1},
  pages={1--49},
  year={2002}
}
@article{kutin2012almost,
  title={Almost-everywhere algorithmic stability and generalization error},
  author={Kutin, Samuel and Niyogi, Partha},
  journal={arXiv preprint arXiv:1301.0579},
  year={2012}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@article{poggio2004general,
  title={General conditions for predictivity in learning theory},
  author={Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
  journal={Nature},
  volume={428},
  number={6981},
  pages={419--422},
  year={2004},
  publisher={Nature Publishing Group UK London}
}

@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014}
}

@inproceedings{chaudhari2018stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}
@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{davis2019stochastic,
  title={Stochastic model-based minimization of weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={207--239},
  year={2019},
  publisher={SIAM}
}
@inproceedings{zhang2019composite,
  title={A composite randomized incremental gradient method},
  author={Zhang, Junyu and Xiao, Lin},
  booktitle={International Conference on Machine Learning},
  pages={7454--7462},
  year={2019},
  organization={PMLR}
}
@article{yuan2019stochastic,
  title={Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization},
  author={Yuan, Huizhuo and Lian, Xiangru and Liu, Ji},
  journal={arXiv preprint arXiv:1912.13515},
  year={2019}
}

@article{zhang2021multilevel,
  title={Multilevel composite stochastic optimization via nested variance reduction},
  author={Zhang, Junyu and Xiao, Lin},
  journal={SIAM Journal on Optimization},
  volume={31},
  number={2},
  pages={1131--1157},
  year={2021},
  publisher={SIAM}
}
@inproceedings{tarzanagh2022fednest,
  title={Fednest: Federated bilevel, minimax, and compositional optimization},
  author={Tarzanagh, Davoud Ataee and Li, Mingchen and Thrampoulidis, Christos and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={21146--21179},
  year={2022},
  organization={PMLR}
}
@article{yang2019multilevel,
  title={Multilevel stochastic gradient methods for nested composition optimization},
  author={Yang, Shuoguang and Wang, Mengdi and Fang, Ethan X},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={616--659},
  year={2019},
  publisher={SIAM}
}
@article{ghadimi2020single,
  title={A single timescale stochastic approximation method for nested stochastic optimization},
  author={Ghadimi, Saeed and Ruszczynski, Andrzej and Wang, Mengdi},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={1},
  pages={960--979},
  year={2020},
  publisher={SIAM}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{yuan2020stochastic,
  title={Stochastic recursive momentum method for non-convex compositional optimization},
  author={Yuan, Huizhuo and Hu, Wenqing},
  journal={arXiv preprint arXiv:2006.01688},
  year={2020}
}

@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and others},
  volume={112},
  year={2013},
  publisher={Springer}
}

@inproceedings{zhang2021generalization,
  title={Generalization bounds for stochastic saddle point problems},
  author={Zhang, Junyu and Hong, Mingyi and Wang, Mengdi and Zhang, Shuzhong},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={568--576},
  year={2021},
  organization={PMLR}
}

@article{qi2021online,
  title={An online method for a class of distributionally robust optimization with non-convex objectives},
  author={Qi, Qi and Guo, Zhishuai and Xu, Yi and Jin, Rong and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10067--10080},
  year={2021}
}



@article{ruszczynski2021stochastic,
  title={A stochastic subgradient method for nonsmooth nonconvex multilevel composition optimization},
  author={Ruszczynski, Andrzej},
  journal={SIAM Journal on Control and Optimization},
  volume={59},
  number={3},
  pages={2301--2320},
  year={2021},
  publisher={SIAM}
}

@article{li2024convex,
  title={Convex and non-convex optimization under generalized smoothness},
  author={Li, Haochuan and Qian, Jian and Tian, Yi and Rakhlin, Alexander and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}
