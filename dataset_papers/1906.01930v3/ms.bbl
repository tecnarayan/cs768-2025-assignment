\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop(2006)]{bishop2006pattern}
Christopher~M Bishop.
\newblock \emph{Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018Large-Scale}
L.~Bottou, F.~Curtis, and J.~Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.
\newblock \doi{10.1137/16M1080173}.

\bibitem[Bradshaw et~al.(2017)Bradshaw, Matthews, and
  Ghahramani]{bradshaw2017adversarial}
John Bradshaw, Alexander G de~G Matthews, and Zoubin Ghahramani.
\newblock Adversarial examples, uncertainty, and transfer testing robustness in
  {{G}}aussian process hybrid deep networks.
\newblock \emph{arXiv preprint arXiv:1707.02476}, 2017.

\bibitem[Cho and Saul(2009)]{Cho&Saul2009}
Youngmin Cho and Lawrence~K. Saul.
\newblock Kernel methods for deep learning.
\newblock In Y.~Bengio, D.~Schuurmans, J.~D. Lafferty, C.~K.~I. Williams, and
  A.~Culotta, editors, \emph{Advances in Neural Information Processing Systems
  22}, pages 342--350. Curran Associates, Inc., 2009.

\bibitem[de~G.~Matthews et~al.(2018)de~G.~Matthews, Hron, Rowland, Turner, and
  Ghahramani]{Matthews2018}
Alexander~G. de~G.~Matthews, Jiri Hron, Mark Rowland, Richard~E. Turner, and
  Zoubin Ghahramani.
\newblock {{G}}aussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Rasmussen, and
  Aitchison]{garriga-alonso2019deep}
Adri√† Garriga-Alonso, Carl~Edward Rasmussen, and Laurence Aitchison.
\newblock Deep convolutional networks as shallow {{G}}aussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hazan and Jaakkola(2015)]{Hazan&Jaakkola2015steps}
Tamir Hazan and Tommi~S. Jaakkola.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{CoRR}, abs/1508.05133, 2015.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot2018NeuralTK}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 8571--8580. Curran Associates, Inc., 2018.

\bibitem[Khan(2012)]{khan2012variational}
Mohammad Khan.
\newblock \emph{Variational learning for latent Gaussian model of discrete
  data}.
\newblock PhD thesis, University of British Columbia, 2012.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan2018fast}
Mohammad~Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and
  Akash Srivastava.
\newblock Fast and scalable {{B}}ayesian deep learning by weight-perturbation
  in {{A}}dam.
\newblock In \emph{International Conference on Machine Learning}, pages
  2616--2625, 2018.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, Haffner,
  et~al.]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl{-}Dickstein]{LeeBNSPS2018}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S. Schoenholz, Jeffrey
  Pennington, and Jascha Sohl{-}Dickstein.
\newblock Deep neural networks as {{G}}aussian processes.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem[{Lee} et~al.(2019){Lee}, {Xiao}, {Schoenholz}, {Bahri}, {Novak},
  {Sohl-Dickstein}, and {Pennington}]{Lee2019WideNN}
Jaehoon {Lee}, Lechao {Xiao}, Samuel~S. {Schoenholz}, Yasaman {Bahri}, Roman
  {Novak}, Jascha {Sohl-Dickstein}, and Jeffrey {Pennington}.
\newblock {Wide Neural Networks of Any Depth Evolve as Linear Models Under
  Gradient Descent}.
\newblock \emph{arXiv e-prints}, art. arXiv:1902.06720, Feb 2019.

\bibitem[Martens(2014)]{martens2014new}
James Martens.
\newblock New perspectives on the natural gradient method.
\newblock \emph{CoRR}, abs/1412.1193, 2014.

\bibitem[Neal(1996)]{Neal1996}
Radford~M. Neal.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock Springer-Verlag, Berlin, Heidelberg, 1996.
\newblock ISBN 0387947248.

\bibitem[Nocedal and Wright(2006)]{nocedal2006numerical}
Jorge Nocedal and Stephen Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Abolafia, Pennington,
  and Sohl-dickstein]{novak2019bayesian}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are
  {{G}}aussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Jain, Eschenhagen, Turner, Yokota,
  and Khan]{osawa2019}
Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard
  Turner, Rio Yokota, and Mohammad~Emtiyaz Khan.
\newblock Practical deep learning with {B}ayesian principles.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Snelson and Ghahramani(2006)]{Snelson}
Edward Snelson and Zoubin Ghahramani.
\newblock Sparse gaussian processes using pseudo-inputs.
\newblock In Y.~Weiss, B.~Sch\"{o}lkopf, and J.~C. Platt, editors,
  \emph{Advances in Neural Information Processing Systems 18}, pages
  1257--1264. MIT Press, 2006.

\bibitem[Tieleman and Hinton(2012)]{hintonTieleman}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {Lecture 6.5-{R}MSprop: Divide the gradient by a running average of
  its recent magnitude.}
\newblock \emph{COURSERA: Neural Networks for Machine Learning 4}, 2012.

\bibitem[Williams(1997)]{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  295--301, 1997.

\bibitem[Williams and Rasmussen(2006)]{williams2006gaussian}
Christopher~KI Williams and Carl~Edward Rasmussen.
\newblock \emph{Gaussian processes for machine learning}, volume~2.
\newblock MIT Press Cambridge, MA, 2006.

\end{thebibliography}
