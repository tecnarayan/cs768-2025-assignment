\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{DBLP:conf/icml/Allen-Zhu17}
Zeyuan Allen-Zhu.
\newblock Natasha: Faster non-convex stochastic optimization via strongly
  non-convex parameter.
\newblock In \emph{International Conference on Machine Learning}, pages 89--97,
  2017.

\bibitem[An and Nam(2017)]{doi:10.1080/02331934.2016.1253694}
Nguyen~Thai An and Nguyen~Mau Nam.
\newblock Convergence analysis of a proximal point algorithm for minimizing
  differences of functions.
\newblock \emph{Optimization}, 66\penalty0 (1):\penalty0 129--147, 2017.

\bibitem[Attouch et~al.(2013)Attouch, Bolte, and Svaiter]{Attouch2013}
Hedy Attouch, J{\'e}r{\^o}me Bolte, and Benar~Fux Svaiter.
\newblock Convergence of descent methods for semi-algebraic and tame problems:
  proximal algorithms, forward--backward splitting, and regularized
  gauss--seidel methods.
\newblock \emph{Mathematical Programming}, 137\penalty0 (1):\penalty0 91--129,
  Feb 2013.

\bibitem[Bertsekas(2014)]{bertsekas2014constrained}
Dimitri~P Bertsekas.
\newblock \emph{Constrained optimization and Lagrange multiplier methods}.
\newblock Academic press, 2014.

\bibitem[Bolte et~al.(2014)Bolte, Sabach, and
  Teboulle]{Bolte:2014:PAL:2650160.2650169}
J{\'e}r\^{o}me Bolte, Shoham Sabach, and Marc Teboulle.
\newblock Proximal alternating linearized minimization for nonconvex and
  nonsmooth problems.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1-2):\penalty0
  459--494, August 2014.
\newblock ISSN 0025-5610.

\bibitem[Bot et~al.(2016)Bot, Csetnek, and L{\'a}szl{\'o}]{Bot2016}
Radu~Ioan Bot, Ern{\"o}~Robert Csetnek, and Szil{\'a}rd~Csaba L{\'a}szl{\'o}.
\newblock An inertial forward--backward algorithm for the minimization of the
  sum of two nonconvex functions.
\newblock \emph{EURO Journal on Computational Optimization}, 4\penalty0
  (1):\penalty0 3--25, Feb 2016.

\bibitem[Boyd and Vandenberghe(2004)]{boyd-2004-convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Bredies et~al.(2015)Bredies, Lorenz, and
  Reiterer]{bredies2015minimization}
Kristian Bredies, Dirk~A Lorenz, and Stefan Reiterer.
\newblock Minimization of non-smooth, non-convex functionals by iterative
  thresholding.
\newblock \emph{Journal of Optimization Theory and Applications}, 165\penalty0
  (1):\penalty0 78--112, 2015.

\bibitem[Cand{\`e}s et~al.(2008)Cand{\`e}s, Wakin, and Boyd]{Candades2008}
Emmanuel~J. Cand{\`e}s, Michael~B. Wakin, and Stephen~P. Boyd.
\newblock Enhancing sparsity by reweighted l1 minimization.
\newblock \emph{Journal of Fourier Analysis and Applications}, 14\penalty0
  (5):\penalty0 877--905, Dec 2008.

\bibitem[Cao et~al.(2013)Cao, Sun, and Xu]{cao2013fast}
Wenfei Cao, Jian Sun, and Zongben Xu.
\newblock Fast image deconvolution using closed-form thresholding formulas of
  $l_q~(q=1/2,2/3)$ regularization.
\newblock \emph{Journal of Visual Communication and Image Representation},
  24\penalty0 (1):\penalty0 31--41, 2013.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{carmonlowerbound}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points i.
\newblock \emph{arXiv preprint arXiv:abs/1710.11606}, 2017.

\bibitem[Chen and Yang(2018)]{chen2018variance}
Zaiyi Chen and Tianbao Yang.
\newblock A variance reduction method for non-convex optimization with improved
  convergence under large condition number.
\newblock \emph{arXiv preprint arXiv:1809.06754}, 2018.

\bibitem[Clarke(1990)]{clarke1990optimization}
Frank~H Clarke.
\newblock \emph{Optimization and nonsmooth analysis}, volume~5.
\newblock SIAM, 1990.

\bibitem[Davis and Drusvyatskiy(2019)]{davis2019stochastic}
Damek Davis and Dmitriy Drusvyatskiy.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  207--239, 2019.

\bibitem[Davis et~al.(2018)Davis, Drusvyatskiy, Kakade, and
  Lee]{davis2018stochastic}
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason~D Lee.
\newblock Stochastic subgradient method converges on tame functions.
\newblock \emph{Foundations of Computational Mathematics}, pages 1--36, 2018.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste{-}Julien]{DBLP:conf/nips/DefazioBL14}
Aaron Defazio, Francis~R. Bach, and Simon Lacoste{-}Julien.
\newblock {SAGA:} {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Fan and Li(2001)]{CIS-172933}
Jianqing Fan and Runze Li.
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock \emph{Journal of the American Statistical Association}, 96\penalty0
  (456):\penalty0 1348--1360, 2001.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock {\sc{Spider}}: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  687--697, 2018.

\bibitem[Ghadimi et~al.(2016)Ghadimi, Lan, and
  Zhang]{DBLP:journals/mp/GhadimiLZ16}
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1-2):\penalty0
  267--305, 2016.

\bibitem[Goodfellow et~al.()Goodfellow, Bengio, and
  Courville]{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Kruger(2003)]{kruger2003frechet}
A~Ya Kruger.
\newblock On fr{\'e}chet subdifferentials.
\newblock \emph{Journal of Mathematical Sciences}, 116\penalty0 (3):\penalty0
  3325--3358, 2003.

\bibitem[Li and Pong(2015)]{li2015global}
Guoyin Li and Ting~Kei Pong.
\newblock Global convergence of splitting methods for nonconvex composite
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  2434--2460, 2015.

\bibitem[Li and Pong(2016)]{DBLP:journals/mp/LiP16}
Guoyin Li and Ting~Kei Pong.
\newblock Douglas-rachford splitting for nonconvex optimization with
  application to nonconvex feasibility problems.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0
  371--401, 2016.

\bibitem[Li and Lin(2015)]{Li:2015:APG:2969239.2969282}
Huan Li and Zhouchen Lin.
\newblock Accelerated proximal gradient methods for nonconvex programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  379--387, Cambridge, MA, USA, 2015. MIT Press.

\bibitem[Li and Li(2018)]{li2018simple}
Zhize Li and Jian Li.
\newblock A simple proximal stochastic gradient method for nonsmooth nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5569--5579, 2018.

\bibitem[Liu et~al.(2017)Liu, Pong, and Takeda]{liu2017successive}
Tianxiang Liu, Ting~Kei Pong, and Akiko Takeda.
\newblock A successive difference-of-convex approximation method for a class of
  nonconvex nonsmooth optimization problems.
\newblock \emph{Mathematical Programming}, pages 1--29, 2017.

\bibitem[Luenberger and Ye(2015)]{luenberger2015linear}
David~G Luenberger and Yinyu Ye.
\newblock \emph{Linear and Nonlinear Programming}, volume 228.
\newblock Springer, 2015.

\bibitem[Metel and Takeda(2019)]{metel2019stochastic}
Michael~R Metel and Akiko Takeda.
\newblock Stochastic gradient methods for non-smooth non-convex regularized
  optimization.
\newblock \emph{arXiv preprint arXiv:1901.08369}, 2019.

\bibitem[Nesterov(2013)]{Composite}
Yu. Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nguyen et~al.(2017{\natexlab{a}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}c]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}c.
\newblock $\text{SARAH}$: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{International Conference on Machine Learning}, pages
  2613--2621, 2017{\natexlab{a}}.

\bibitem[Nguyen et~al.(2017{\natexlab{b}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}c]{nguyen2017stochastic}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}c.
\newblock Stochastic recursive gradient algorithm for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1705.07261}, 2017{\natexlab{b}}.

\bibitem[Nitanda and Suzuki(2017)]{pmlr-v54-nitanda17a}
Atsushi Nitanda and Taiji Suzuki.
\newblock {Stochastic Difference of Convex Algorithm and its Application to
  Training Deep Boltzmann Machines}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 470--478, 2017.

\bibitem[Paquette et~al.(2018)Paquette, Lin, Drusvyatskiy, Mairal, and
  Harchaoui]{paquette2018catalyst}
Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal, and Zaid
  Harchaoui.
\newblock Catalyst for gradient-based nonconvex optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1--10, 2018.

\bibitem[Pham et~al.(2019)Pham, Nguyen, Phan, and Tran-Dinh]{pham2019proxsarah}
Nhan~H Pham, Lam~M Nguyen, Dzung~T Phan, and Quoc Tran-Dinh.
\newblock Prox$\text{SARAH}$: An efficient algorithmic framework for stochastic
  composite nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1902.05679}, 2019.

\bibitem[Polino et~al.(2018)Polino, Pascanu, and Alistarh]{polino2018model}
Antonio Polino, Razvan Pascanu, and Dan Alistarh.
\newblock Model compression via distillation and quantization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Poliquin et~al.(2000)Poliquin, T., and L.]{polrock2000}
R.A. Poliquin, Rockafellar~R. T., and Thibault L.
\newblock Local differentiability of distance functions.
\newblock \emph{Transactions of the American Mathematical Society},
  352:\penalty0 5231--5249, 01 2000.

\bibitem[Reddi et~al.(2016)Reddi, Sra, P{\'o}czos, and
  Smola]{reddi2016proximal}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alexander~J Smola.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1145--1153, 2016.

\bibitem[Rockafellar and Wets(1998)]{RockWets98}
{R. Tyrrell} Rockafellar and Roger J.-B. Wets.
\newblock \emph{Variational Analysis}.
\newblock Springer Verlag, Heidelberg, Berlin, New York, 1998.

\bibitem[Thi et~al.(2017)Thi, Le, Phan, and Tran]{pmlr-v70-thi17a}
Hoai An~Le Thi, Hoai~Minh Le, Duy~Nhat Phan, and Bach Tran.
\newblock Stochastic {DCA} for the large-sum of non-convex functions problem
  and its application to group variable selection in classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  3394--3403, 2017.

\bibitem[Wang et~al.(2018)Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2018spiderboost}
Zhe Wang, Kaiyi Ji, Yi~Zhou, Yingbin Liang, and Vahid Tarokh.
\newblock Spider$\text{B}$oost: A class of faster variance-reduced algorithms
  for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1810.10690}, 2018.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{wu2016quantized}
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
\newblock Quantized convolutional neural networks for mobile devices.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4820--4828, 2016.

\bibitem[Xu et~al.(2018{\natexlab{a}})Xu, Qi, Lin, Jin, and
  Yang]{xu2018stochastic}
Yi~Xu, Qi~Qi, Qihang Lin, Rong Jin, and Tianbao Yang.
\newblock Stochastic optimization for dc functions and non-smooth non-convex
  regularizers with non-asymptotic convergence.
\newblock \emph{arXiv preprint arXiv:1811.11829}, 2018{\natexlab{a}}.

\bibitem[Xu et~al.(2018{\natexlab{b}})Xu, Zhu, Yang, Zhang, Jin, and
  Yang]{DBLP:journals/corr/abs-1805-07880}
Yi~Xu, Shenghuo Zhu, Sen Yang, Chi Zhang, Rong Jin, and Tianbao Yang.
\newblock Learning with non-convex truncated losses by {SGD}.
\newblock \emph{arXiv preprint arXiv:1805.07880}, 2018{\natexlab{b}}.

\bibitem[Xu et~al.(2012)Xu, Chang, Xu, and Zhang]{xu2012l12}
Zongben Xu, Xiangyu Chang, Fengmin Xu, and Hai Zhang.
\newblock $l_{1/2}$ regularization: A thresholding representation theory and a
  fast solver.
\newblock \emph{IEEE Transactions on neural networks and learning systems},
  23\penalty0 (7):\penalty0 1013--1027, 2012.

\bibitem[Yang(2018)]{leiyangpg18}
Lei Yang.
\newblock Proximal gradient method with extrapolation and line search for a
  class of nonconvex and nonsmooth problems.
\newblock \emph{arXiv preprint arXiv:1711.06831}, 2018.

\bibitem[Yu et~al.(2015)Yu, Zheng, Marchetti-Bowick, and Xing]{YuZMX15}
Yaoliang Yu, Xun Zheng, Micol Marchetti-Bowick, and Eric~P. Xing.
\newblock Minimizing nonconvex non-separable functions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1107--1115, 2015.

\bibitem[Zhang(2010)]{cunzhang10}
Cun-Hui Zhang.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of Statistics}, 38:\penalty0 894 -- 942, 2010.

\bibitem[Zhong and Kwok(2014)]{DBLP:conf/aaai/ZhongK14}
Wenliang Zhong and James~T. Kwok.
\newblock Gradient descent with proximal average for nonconvex and composite
  regularization.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
  2206--2212, 2014.

\bibitem[Zhou and Gu(2019)]{zhou2019lower}
Dongruo Zhou and Quanquan Gu.
\newblock Lower bounds for smooth nonconvex finite-sum optimization.
\newblock \emph{arXiv preprint arXiv:1901.11224}, 2019.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{zhou2018stochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduced gradient descent for nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3925--3936, 2018.

\end{thebibliography}
