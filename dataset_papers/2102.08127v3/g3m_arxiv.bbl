\begin{thebibliography}{10}

\bibitem{seung1992statistical}
Hyunjune~Sebastian Seung, Haim Sompolinsky, and Naftali Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock {\em Physical review A}, 45(8):6056, 1992.

\bibitem{watkin1993statistical}
Timothy~LH Watkin, Albrecht Rau, and Michael Biehl.
\newblock The statistical mechanics of learning a rule.
\newblock {\em Reviews of Modern Physics}, 65(2):499, 1993.

\bibitem{engel2001statistical}
Andreas Engel and Christian Van~den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem{donoho2009message}
David~L Donoho, Arian Maleki, and Andrea Montanari.
\newblock Message-passing algorithms for compressed sensing.
\newblock {\em Proceedings of the National Academy of Sciences},
  106(45):18914--18919, 2009.

\bibitem{el2013robust}
Noureddine El~Karoui, Derek Bean, Peter~J Bickel, Chinghway Lim, and Bin Yu.
\newblock On robust regression with high-dimensional predictors.
\newblock {\em Proceedings of the National Academy of Sciences},
  110(36):14557--14562, 2013.

\bibitem{zdeborova2016statistical}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Statistical physics of inference: Thresholds and algorithms.
\newblock {\em Advances in Physics}, 65(5):453--552, 2016.

\bibitem{donoho2016high}
David Donoho and Andrea Montanari.
\newblock High dimensional robust m-estimation: Asymptotic variance via
  approximate message passing.
\newblock {\em Probability Theory and Related Fields}, 166(3-4):935--969, 2016.

\bibitem{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock {Understanding deep learning requires rethinking generalization}.
\newblock In {\em ICLR}, 2017.

\bibitem{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(4):1167--1180,
  2020.

\bibitem{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}, 2019.

\bibitem{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem{candes2020phase}
Emmanuel~J Cand{\`e}s, Pragya Sur, et~al.
\newblock The phase transition for the existence of the maximum likelihood
  estimate in high-dimensional logistic regression.
\newblock {\em The Annals of Statistics}, 48(1):27--42, 2020.

\bibitem{aubin2020generalization}
Benjamin Aubin, Florent Krzakala, Yue~M Lu, and Lenka Zdeborov{\'a}.
\newblock Generalization error in high-dimensional perceptrons: Approaching
  bayes error with convex optimization.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{salehi2020performance}
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi.
\newblock The performance analysis of generalized margin maximizers on
  separable data.
\newblock In {\em International Conference on Machine Learning}, pages
  8417--8426. PMLR, 2020.

\bibitem{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{andreux2020kymatio}
Mathieu Andreux, Tom{\'a}s Angles, Georgios Exarchakis, Roberto Leonarduzzi,
  Gaspar Rochette, Louis Thiry, John Zarka, St{\'e}phane Mallat, Joakim
  And{\'e}n, Eugene Belilovsky, et~al.
\newblock Kymatio: Scattering transforms in python.
\newblock {\em Journal of Machine Learning Research}, 21(60):1--6, 2020.

\bibitem{goodfellow2014generative}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks, 2014.

\bibitem{gordon1985some}
Yehoram Gordon.
\newblock Some inequalities for gaussian processes and applications.
\newblock {\em Israel Journal of Mathematics}, 50(4):265--289, 1985.

\bibitem{mezard1987spin}
Marc M{\'e}zard, Giorgio Parisi, and Miguel Virasoro.
\newblock {\em Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem{mezard2009information}
Marc M\'ezard and Andrea Montanari.
\newblock {\em Information, physics, and computation}.
\newblock Oxford University Press, 2009.

\bibitem{williams96}
Christopher K.~I. Williams.
\newblock Computing with infinite networks.
\newblock In {\em Proceedings of the 9th International Conference on Neural
  Information Processing Systems}, NIPS'96, page 295â€“301, Cambridge, MA, USA,
  1996. MIT Press.

\bibitem{gardner1989three}
Elizabeth Gardner and Bernard Derrida.
\newblock Three unfinished works on the optimal storage capacity of networks.
\newblock {\em Journal of Physics A: Mathematical and General}, 22(12):1983,
  1989.

\bibitem{opper1996statistical}
Manfred Opper and Wolfgang Kinzel.
\newblock Statistical mechanics of generalization.
\newblock In {\em Models of neural networks III}, pages 151--209. Springer,
  1996.

\bibitem{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{thrampoulidis2018precise}
Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi.
\newblock Precise error analysis of regularized $ m $-estimators in high
  dimensions.
\newblock {\em IEEE Transactions on Information Theory}, 64(8):5592--5628,
  2018.

\bibitem{montanari2019generalization}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock {\em arXiv preprint arXiv:1911.01544}, 2019.

\bibitem{celentano2020lasso}
Michael Celentano, Andrea Montanari, and Yuting Wei.
\newblock The lasso with general gaussian designs with applications to
  hypothesis testing.
\newblock {\em arXiv preprint arXiv:2007.13716}, 2020.

\bibitem{stojnic2013framework}
Mihailo Stojnic.
\newblock A framework to characterize performance of lasso algorithms.
\newblock {\em arXiv preprint arXiv:1303.7291}, 2013.

\bibitem{oymak2013squared}
Samet Oymak, Christos Thrampoulidis, and Babak Hassibi.
\newblock The squared-error of generalized lasso: A precise analysis.
\newblock In {\em 2013 51st Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 1002--1009. IEEE, 2013.

\bibitem{bordelon2020}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1024--1034. PMLR, 2020.

\bibitem{huang2020large}
Hanwen Huang and Qinglong Yang.
\newblock Large scale analysis of generalization error in learning using margin
  based classification methods.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(10):103407, 2020.

\bibitem{mitra2019understanding}
Partha~P Mitra.
\newblock Understanding overfitting peaks in generalization error: Analytical
  risk curves for $ l\_2 $ and $ l\_1 $ penalized interpolation.
\newblock {\em arXiv preprint arXiv:1906.03667}, 2019.

\bibitem{dhifallah2020precise}
Oussama Dhifallah and Yue~M Lu.
\newblock A precise performance analysis of learning with random features.
\newblock {\em arXiv preprint arXiv:2008.11904}, 2020.

\bibitem{dobriban2018high}
Edgar Dobriban, Stefan Wager, et~al.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock {\em The Annals of Statistics}, 46(1):247--279, 2018.

\bibitem{wu2020optimal}
Denny Wu and Ji~Xu.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{liao2020random}
Zhenyu Liao, Romain Couillet, and Michael~W Mahoney.
\newblock A random matrix analysis of random fourier features: beyond the
  gaussian kernel, a precise phase transition, and the corresponding double
  descent.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{liu2020kernel}
Fanghui Liu, Zhenyu Liao, and Johan~AK Suykens.
\newblock Kernel regression in high dimension: Refined analysis beyond double
  descent.
\newblock {\em arXiv preprint arXiv:2010.02681}, 2020.

\bibitem{Bartlett30063}
Peter~L. Bartlett, Philip~M. Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30063--30070, 2020.

\bibitem{jacot2020kernel}
Arthur Jacot, Berfin {\c{S}}im{\c{s}}ek, Francesco Spadaro, Cl{\'e}ment
  Hongler, and Franck Gabriel.
\newblock Kernel alignment risk estimator: Risk prediction from training data.
\newblock {\em arXiv preprint arXiv:2006.09796}, 2020.

\bibitem{gerbelot2020colt}
C{\'e}dric Gerbelot, Alia Abbara, and Florent Krzakala.
\newblock Asymptotic errors for high-dimensional convex penalized linear
  regression beyond gaussian matrices.
\newblock In {\em Conference on Learning Theory}, pages 1682--1713. PMLR, 2020.

\bibitem{gerace2020generalisation}
F.~Gerace, B.~Loureiro, F.~Krzakala, M.~M{\'e}zard, and L.~Zdeborov{\'a}.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock In {\em 37th International Conference on Machine Learning}, 2020.

\bibitem{goldt2020modelling}
S.~Goldt, M.~M{\'e}zard, F.~Krzakala, and L.~Zdeborov{\'a}.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock {\em Phys. Rev. X}, 10(4):041044, 2020.

\bibitem{goldt2020gaussian}
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Marc M{\'e}zard, Florent
  Krzakala, and Lenka Zdeborov{\'a}.
\newblock The gaussian equivalence of generative models for learning with
  two-layer neural networks.
\newblock In {\em Mathematical and Scientific Machine Learning}, 2021.

\bibitem{hu2020universality}
Hong Hu and Yue~M Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock {\em arXiv preprint arXiv:2009.07669}, 2020.

\bibitem{el2010spectrum}
Noureddine El~Karoui et~al.
\newblock The spectrum of kernel random matrices.
\newblock {\em Annals of statistics}, 38(1):1--50, 2010.

\bibitem{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~30, pages 2637--2646, 2017.

\bibitem{louart2018concentration}
Cosme Louart and Romain Couillet.
\newblock Concentration of measure and large random matrices with an
  application to sample covariance matrices.
\newblock {\em arXiv preprint arXiv:1805.08295}, 2018.

\bibitem{seddik2020random}
Mohamed El~Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain
  Couillet.
\newblock Random matrix theory proves that deep learning representations of
  gan-data behave as gaussian mixtures.
\newblock In {\em International Conference on Machine Learning}, pages
  8573--8582. PMLR, 2020.

\bibitem{miolane2018distribution}
L{\'e}o Miolane and Andrea Montanari.
\newblock The distribution of the lasso: Uniform control over sparse balls and
  adaptive parameter tuning.
\newblock {\em arXiv preprint arXiv:1811.01212}, 2018.

\bibitem{rosset2003margin}
Saharon Rosset, Ji~Zhu, and Trevor Hastie.
\newblock Margin maximizing loss functions.
\newblock In {\em NIPS}, pages 1237--1244, 2003.

\bibitem{scholkopf2018learning}
B.~Scholkopf and A.J. Smola.
\newblock {\em Learning with Kernels: Support Vector Machines, Regularization,
  Optimization, and Beyond}.
\newblock Adaptive Computation and Machine Learning. MIT Press, 2018.

\bibitem{steinwart2009optimal}
Ingo Steinwart, Don~R Hush, Clint Scovel, et~al.
\newblock Optimal rates for regularized least squares regression.
\newblock In {\em COLT}, pages 79--93, 2009.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{pillaud2018statistical}
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach.
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, pages 8114--8124, 2018.

\bibitem{opper99}
Rainer Dietrich, Manfred Opper, and Haim Sompolinsky.
\newblock Statistical mechanics of support vector networks.
\newblock {\em Phys. Rev. Lett.}, 82:2975--2978, Apr 1999.

\bibitem{opper01}
M.~Opper and R.~Urbanczik.
\newblock Universal learning curves of support vector machines.
\newblock {\em Phys. Rev. Lett.}, 86:4410--4413, May 2001.

\bibitem{radford2015unsupervised}
A.~Radford, L.~Metz, and S.~Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In {\em ICLR}, 2016.

\bibitem{xiao2017}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem{bruna2012}
J.~{Bruna} and S.~{Mallat}.
\newblock Invariant scattering convolution networks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  35(8):1872--1886, 2013.

\bibitem{spigler2019jamming}
Stefano Spigler, Mario Geiger, St{\'e}phane dâ€™Ascoli, Levent Sagun, Giulio
  Biroli, and Matthieu Wyart.
\newblock A jamming transition from under-to over-parametrization affects
  generalization in deep learning.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  52(47):474001, 2019.

\bibitem{bai2008large}
Zhidong Bai and Wang Zhou.
\newblock Large sample covariance matrices without independence structures in
  columns.
\newblock {\em Statistica Sinica}, pages 425--442, 2008.

\bibitem{ledoit2011eigenvectors}
Olivier Ledoit and Sandrine P{\'e}ch{\'e}.
\newblock Eigenvectors of some large sample covariance matrix ensembles.
\newblock {\em Probability Theory and Related Fields}, 151(1):233--264, 2011.

\bibitem{el2009concentration}
Noureddine El~Karoui et~al.
\newblock Concentration of measure and spectra of random matrices: Applications
  to correlation matrices, elliptical distributions and beyond.
\newblock {\em Annals of Applied Probability}, 19(6):2362--2405, 2009.

\bibitem{cheng2013spectrum}
Xiuyuan Cheng and Amit Singer.
\newblock The spectrum of random inner-product kernel matrices.
\newblock {\em Random Matrices: Theory and Applications}, 2(04):1350010, 2013.

\bibitem{fan2019spectral}
Zhou Fan and Andrea Montanari.
\newblock The spectral norm of random inner-product kernel matrices.
\newblock {\em Probability Theory and Related Fields}, 173(1):27--85, 2019.

\bibitem{bauschke2011convex}
Heinz~H Bauschke, Patrick~L Combettes, et~al.
\newblock {\em Convex analysis and monotone operator theory in Hilbert spaces},
  volume 408.
\newblock Springer, 2011.

\bibitem{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem{ma2017analysis}
Yanting Ma, Cynthia Rush, and Dror Baron.
\newblock Analysis of approximate message passing with a class of non-separable
  denoisers.
\newblock In {\em 2017 IEEE International Symposium on Information Theory
  (ISIT)}, pages 231--235. IEEE, 2017.

\bibitem{vershynin2010introduction}
Roman Vershynin.
\newblock {Introduction to the non-asymptotic analysis of random matrices}.
\newblock In Y.~Eldar and G.~Kutyniok., editors, {\em Compressed Sensing,
  Theory and Applications}. Cambridge University Press, 2012.

\bibitem{durrett2019probability}
Rick Durrett.
\newblock {\em Probability: theory and examples}, volume~49.
\newblock Cambridge university press, 2019.

\bibitem{andersen1982cox}
Per~Kragh Andersen and Richard~D Gill.
\newblock Cox's regression model for counting processes: a large sample study.
\newblock {\em The annals of statistics}, pages 1100--1120, 1982.

\bibitem{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem{kaiminginit}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock {\em 2015 IEEE International Conference on Computer Vision (ICCV)},
  pages 1026--1034, 2015.

\bibitem{Adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em Proceedings of the 3rd International Conference for Learning
  Representations}, volume~3, 2015.

\end{thebibliography}
