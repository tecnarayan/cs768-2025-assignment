% Encoding: UTF-8
%% Optimization

@Article{Nemirovskii1983,
  author    = {Nemirovskii, Arkadii and Yudin, David Borisovich and Dawson, Edgar Ronald},
  title     = {Problem complexity and method efficiency in optimization},
  year      = {1983},
  publisher = {Wiley},
}

@InProceedings{Nesterov1983,
  author    = {Nesterov, Yurii},
  booktitle = {Soviet Mathematics Doklady},
  title     = {A method of solving a convex programming problem with convergence rate O (1/k2)},
  year      = {1983},
  number    = {2},
  pages     = {372--376},
  volume    = {27},
}

@Article{Bubeck2015,
  author  = {Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  journal = {arXiv preprint arXiv:1506.08187},
  title   = {A geometric alternative to Nesterov's accelerated gradient descent},
  year    = {2015},
}

@Article{Lessard2016,
  author    = {Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  journal   = {SIAM Journal on Optimization},
  title     = {Analysis and design of optimization algorithms via integral quadratic constraints},
  year      = {2016},
  number    = {1},
  pages     = {57--95},
  volume    = {26},
  publisher = {SIAM},
}

@Article{Hu2017,
  author  = {Hu, Bin and Lessard, Laurent},
  journal = {arXiv preprint arXiv:1706.04381},
  title   = {Dissipativity Theory for Nesterov's Accelerated Method},
  year    = {2017},
}

@Article{Fazlyab2017,
  author  = {Fazlyab, Mahyar and Ribeiro, Alejandro and Morari, Manfred and Preciado, Victor M},
  journal = {arXiv preprint arXiv:1705.03615},
  title   = {Analysis of optimization algorithms via integral quadratic constraints: Nonstrongly convex problems},
  year    = {2017},
}

@InProceedings{Scieur2016,
  author    = {Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
  booktitle = {Advances In Neural Information Processing Systems},
  title     = {Regularized nonlinear acceleration},
  year      = {2016},
  pages     = {712--720},
}

@InProceedings{Su2014,
  author    = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  year      = {2014},
  pages     = {2510--2518},
}

@Article{Wibisono2016,
  author    = {Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {A variational perspective on accelerated methods in optimization},
  year      = {2016},
  number    = {47},
  pages     = {E7351--E7358},
  volume    = {113},
  publisher = {National Acad Sciences},
}

@Article{Wilson2016,
  author  = {Wilson, Ashia C and Recht, Benjamin and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1611.02635},
  title   = {A lyapunov analysis of momentum methods in optimization},
  year    = {2016},
}

@Article{Diakonikolas2017,
  author  = {Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal = {arXiv preprint arXiv:1712.02485},
  title   = {The Approximate Duality Gap Technique: A Unified Theory of First-Order Methods},
  year    = {2017},
}

@InProceedings{Krichene2015,
  author    = {Krichene, Walid and Bayen, Alexandre and Bartlett, Peter L},
  booktitle = {Advances in neural information processing systems},
  title     = {Accelerated mirror descent in continuous and discrete time},
  year      = {2015},
  pages     = {2845--2853},
}

@Article{Attouch1996,
  author    = {Attouch, Hedy and Cominetti, Roberto},
  journal   = {Journal of Differential Equations},
  title     = {A dynamical approach to convex minimization coupling approximation with the steepest descent method},
  year      = {1996},
  number    = {2},
  pages     = {519--540},
  volume    = {128},
  publisher = {Elsevier},
}

@PhdThesis{West2004,
  author = {West, Matthew},
  school = {California Institute of Technology},
  title  = {Variational integrators},
  year   = {2004},
}

@Article{BruckJr1975,
  author    = {Bruck Jr, Ronald E},
  journal   = {Journal of Functional Analysis},
  title     = {Asymptotic convergence of nonlinear contraction semigroups in Hilbert space},
  year      = {1975},
  number    = {1},
  pages     = {15--26},
  volume    = {18},
  publisher = {Elsevier},
}

@Article{Alvarez2000,
  author    = {Alvarez, Felipe},
  journal   = {SIAM Journal on Control and Optimization},
  title     = {On the minimizing property of a second order dissipative system in Hilbert spaces},
  year      = {2000},
  number    = {4},
  pages     = {1102--1119},
  volume    = {38},
  publisher = {SIAM},
}

@Article{Scieur2017,
  author  = {Scieur, Damien and Roulet, Vincent and Bach, Francis and d'Aspremont, Alexandre},
  journal = {arXiv preprint arXiv:1702.06751},
  title   = {Integration methods and accelerated optimization algorithms},
  year    = {2017},
}

@Article{Betancourt2018,
  author  = {Betancourt, Michael and Jordan, Michael I and Wilson, Ashia C},
  journal = {arXiv preprint arXiv:1802.03653},
  title   = {On Symplectic Optimization},
  year    = {2018},
}

@Article{AllenZhu2014,
  author  = {Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal = {arXiv preprint arXiv:1407.1537},
  title   = {Linear coupling: An ultimate unification of gradient and mirror descent},
  year    = {2014},
}

@Article{Attouch2016,
  author    = {Attouch, Hedy and Peypouquet, Juan},
  journal   = {SIAM Journal on Optimization},
  title     = {The Rate of Convergence of Nesterov's Accelerated Forward-Backward Method is Actually Faster Than 1/k\^{}2},
  year      = {2016},
  number    = {3},
  pages     = {1824--1834},
  volume    = {26},
  publisher = {SIAM},
}

%% Paper specific

@Article{Verner1996,
  author    = {Verner, JH},
  journal   = {Applied numerical mathematics},
  title     = {High-order explicit Runge-Kutta pairs with low stage order},
  year      = {1996},
  number    = {1-3},
  pages     = {345--357},
  volume    = {22},
  publisher = {Elsevier},
}

@Article{Zhang2012,
  author    = {Zhang, Xinzhen and Ling, Chen and Qi, Liqun},
  journal   = {SIAM Journal on Matrix Analysis and Applications},
  title     = {The best rank-1 approximation of a symmetric tensor and related spherical optimization problems},
  year      = {2012},
  number    = {3},
  pages     = {806--821},
  volume    = {33},
  publisher = {SIAM},
}

@Article{Kofidis2002,
  author    = {Kofidis, Eleftherios and Regalia, Phillip A},
  journal   = {SIAM Journal on Matrix Analysis and Applications},
  title     = {On the best rank-1 approximation of higher-order supersymmetric tensors},
  year      = {2002},
  number    = {3},
  pages     = {863--884},
  volume    = {23},
  publisher = {SIAM},
}

@Article{Hairer1997,
  author    = {Hairer, Ernst and Lubich, Christian},
  journal   = {Numerische Mathematik},
  title     = {The life-span of backward error analysis for numerical integrators},
  year      = {1997},
  number    = {4},
  pages     = {441--462},
  volume    = {76},
  publisher = {Springer},
}

@Book{Hairer2006,
  author    = {Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  publisher = {Springer Science \& Business Media},
  title     = {Geometric numerical integration: structure-preserving algorithms for ordinary differential equations},
  year      = {2006},
  volume    = {31},
}

@Article{Wang2017,
  author    = {Wang, Miaoyan and Duc, Khanh Dao and Fischer, Jonathan and Song, Yun S},
  journal   = {Linear algebra and its applications},
  title     = {Operator norm inequalities between tensor unfoldings on the partition lattice},
  year      = {2017},
  pages     = {44--66},
  volume    = {520},
  publisher = {Elsevier},
}

@Misc{Rudin1991,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@Book{Mujica2010,
  author    = {Mujica, Jorge},
  publisher = {Courier Corporation},
  title     = {Complex analysis in Banach spaces},
  year      = {2010},
}

@Article{Mujica2006,
  author  = {Mujica, Jorge},
  journal = {Note di Matematica},
  title   = {Holomorphic functions on Banach spaces},
  year    = {2006},
  number  = {2},
  pages   = {113--138},
  volume  = {25},
}

@Article{Barletta2009,
  author    = {Barletta, Elisabetta and Dragomir, Sorin},
  journal   = {Bulletin math{\'e}matique de la Soci{\'e}t{\'e} des Sciences Math{\'e}matiques de Roumanie},
  title     = {Vector valued holomorphic functions},
  year      = {2009},
  pages     = {211--226},
  publisher = {JSTOR},
}

@Article{Attouch2010,
  author    = {Attouch, H{\'e}dy and Bolte, J{\'e}r{\^o}me and Redont, Patrick and Soubeyran, Antoine},
  journal   = {Mathematics of Operations Research},
  title     = {Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-{\L}ojasiewicz inequality},
  year      = {2010},
  number    = {2},
  pages     = {438--457},
  volume    = {35},
  publisher = {INFORMS},
}

@Article{Lojasiewicz1965,
  author  = {Lojasiewicz, Stanislaw},
  journal = {Lectures Notes IHES (Bures-sur-Yvette)},
  title   = {Ensembles semi-analytiques},
  year    = {1965},
}

@Book{Haraux2006,
  author    = {Haraux, Alain},
  publisher = {Springer},
  title     = {Nonlinear evolution equations-Global behavior of solutions},
  year      = {2006},
  volume    = {841},
}

@InProceedings{Raginsky2012,
  author       = {Raginsky, Maxim and Bouvrie, Jake},
  booktitle    = {Decision and Control (CDC), 2012 IEEE 51st Annual Conference on},
  title        = {Continuous-time stochastic mirror descent on a network: Variance reduction, consensus, convergence},
  year         = {2012},
  organization = {IEEE},
  pages        = {6793--6800},
}

@Article{Polyak1964,
  author    = {Polyak, Boris T},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  title     = {Some methods of speeding up the convergence of iteration methods},
  year      = {1964},
  number    = {5},
  pages     = {1--17},
  volume    = {4},
  publisher = {No longer published by Elsevier},
}

@Article{Uribe2018,
  author  = {Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  journal = {arXiv preprint arXiv:1809.00710},
  title   = {A Dual Approach for Optimal Algorithms in Distributed Optimization over Networks},
  year    = {2018},
}

@Article{Zhang2018,
  author  = {Zhang, Jingzhao and Mokhtari, Aryan and Sra, Suvrit and Jadbabaie, Ali},
  journal = {arXiv preprint arXiv:1805.00521},
  title   = {Direct {Runge}-{Kutta} Discretization Achieves Acceleration},
  year    = {2018},
}

@Article{Yuan2016,
  author    = {Yuan, Kun and Ling, Qing and Yin, Wotao},
  journal   = {SIAM Journal on Optimization},
  title     = {On the convergence of decentralized gradient descent},
  year      = {2016},
  number    = {3},
  pages     = {1835--1854},
  volume    = {26},
  publisher = {SIAM},
}

@Article{Mokhtari2017,
  author    = {Mokhtari, Aryan and Ling, Qing and Ribeiro, Alejandro},
  journal   = {IEEE Transactions on Signal Processing},
  title     = {Network {Newton} distributed optimization methods},
  year      = {2017},
  number    = {1},
  pages     = {146--161},
  volume    = {65},
  publisher = {IEEE},
}

@Article{Tutunov2016,
  author  = {Tutunov, Rasul and Ammar, Haitham Bou and Jadbabaie, Ali},
  journal = {arXiv preprint arXiv:1606.06593},
  title   = {A distributed newton method for large scale consensus optimization},
  year    = {2016},
  note    = {To appear in {\it IEEE Transactions on Automatic Control}},
}

@Article{Scaman2018,
  author  = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  journal = {arXiv preprint arXiv:1806.00291},
  title   = {Optimal Algorithms for Non-Smooth Distributed Optimization in Networks},
  year    = {2018},
}

@Article{Fazlyab2018,
  author    = {Fazlyab, Mahyar and Ribeiro, Alejandro and Morari, Manfred and Preciado, Victor M},
  journal   = {SIAM Journal on Optimization},
  title     = {Analysis of optimization algorithms via integral quadratic constraints: Nonstrongly convex problems},
  year      = {2018},
  number    = {3},
  pages     = {2654--2689},
  volume    = {28},
  publisher = {SIAM},
}
%%NLP

@Article{Chen2018,
  author  = {Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  journal = {arXiv preprint arXiv:1808.02941},
  title   = {On the convergence of a class of adam-type algorithms for non-convex optimization},
  year    = {2018},
}

@Article{Zou2018,
  author  = {Zou, Fangyu and Shen, Li},
  journal = {arXiv preprint arXiv:1808.03408},
  title   = {On the convergence of weighted {AdaGrad} with momentum for training deep neural networks},
  year    = {2018},
}

@Article{Zhou2018,
  author  = {Zhou, Zhiming and Zhang, Qingru and Lu, Guansong and Wang, Hongwei and Zhang, Weinan and Yu, Yong},
  journal = {arXiv preprint arXiv:1810.00143},
  title   = {AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods},
  year    = {2018},
}

@Article{Agarwal2018,
  author  = {Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
  journal = {arXiv preprint arXiv:1806.02958},
  title   = {The case for full-matrix adaptive regularization},
  year    = {2018},
}

@InProceedings{Zou2019,
  author    = {Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {A sufficient condition for convergences of adam and rmsprop},
  year      = {2019},
  pages     = {11127--11135},
}

@InProceedings{Wilson2017,
  author    = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {The marginal value of adaptive gradient methods in machine learning},
  year      = {2017},
  pages     = {4148--4158},
}

@InProceedings{Zhou2018a,
  author       = {Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  booktitle    = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  title        = {Stochastic nested variance reduction for nonconvex optimization},
  year         = {2018},
  organization = {Curran Associates Inc.},
  pages        = {3925--3936},
}

@Article{Armijo1966,
  author    = {Armijo, Larry},
  journal   = {Pacific Journal of mathematics},
  title     = {Minimization of functions having {L}ipschitz continuous first partial derivatives},
  year      = {1966},
  number    = {1},
  pages     = {1--3},
  volume    = {16},
  publisher = {Mathematical Sciences Publishers},
}

@Article{Polyak1987,
  author  = {Polyak, Boris T},
  journal = {Inc., Publications Division, New York},
  title   = {Introduction to Optimization. Optimization Software},
  year    = {1987},
  volume  = {1},
}

@InProceedings{Kawaguchi2016,
  author    = {Kawaguchi, Kenji},
  booktitle = {Advances in neural information processing systems},
  title     = {Deep learning without poor local minima},
  year      = {2016},
  pages     = {586--594},
}

@Article{Luo2019,
  author  = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal = {arXiv preprint arXiv:1902.09843},
  title   = {Adaptive gradient methods with dynamic bound of learning rate},
  year    = {2019},
}

@InProceedings{Santurkar2018,
  author    = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {How does batch normalization help optimization?},
  year      = {2018},
  pages     = {2483--2493},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Deep residual learning for image recognition},
  year      = {2016},
  pages     = {770--778},
}

@TechReport{Krizhevsky2009,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  title  = {Learning multiple layers of features from tiny images},
  year   = {2009},
  school = {Citeseer},
}

@Article{Defazio2018,
  author  = {Defazio, Aaron and Bottou, L{\'e}on},
  journal = {arXiv preprint arXiv:1812.04529},
  title   = {On the Ineffectiveness of Variance Reduced Optimization for Deep Learning},
  year    = {2018},
}

@Article{Simsekli2019,
  author  = {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  journal = {arXiv preprint arXiv:1901.06053},
  title   = {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  year    = {2019},
}

@Article{Hochreiter1997,
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  title     = {Long short-term memory},
  year      = {1997},
  number    = {8},
  pages     = {1735--1780},
  volume    = {9},
  added-at  = {2016-11-15T08:49:43.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  keywords  = {lstm rnn},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
}

@Article{Srivastava2014,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  journal = {Journal of Machine Learning Research},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  year    = {2014},
  pages   = {1929-1958},
  volume  = {15},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html},
}

@InProceedings{Wan2013,
  author    = {Li Wan and Matthew Zeiler and Sixin Zhang and Yann Le Cun and Rob Fergus},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  title     = {Regularization of Neural Networks using {DropConnect}},
  year      = {2013},
  address   = {Atlanta, Georgia, USA},
  editor    = {Sanjoy Dasgupta and David McAllester},
  month     = {17--19 Jun},
  number    = {3},
  pages     = {1058--1066},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {28},
  abstract  = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
  pdf       = {http://proceedings.mlr.press/v28/wan13.pdf},
  url       = {http://proceedings.mlr.press/v28/wan13.html},
}

@Article{Dai2019,
  author        = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  journal       = {CoRR},
  title         = {{Transformer-XL}: Attentive Language Models Beyond a Fixed-Length Context},
  year          = {2019},
  volume        = {abs/1901.02860},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1901-02860},
  eprint        = {1901.02860},
  timestamp     = {Fri, 01 Feb 2019 13:39:59 +0100},
  url           = {http://arxiv.org/abs/1901.02860},
}

@InProceedings{Merity2018,
  author    = {Stephen Merity and Nitish Shirish Keskar and Richard Socher},
  booktitle = {International Conference on Learning Representations},
  title     = {Regularizing and Optimizing {LSTM} Language Models},
  year      = {2018},
  url       = {https://openreview.net/forum?id=SyyGPP0TZ},
}

@InProceedings{Sundermeyer2012,
  author    = {Martin Sundermeyer and Ralf Schl{\"{u}}ter and Hermann Ney},
  booktitle = {{INTERSPEECH} 2012, 13th Annual Conference of the International Speech Communication Association, Portland, Oregon, USA, September 9-13, 2012},
  title     = {{LSTM} Neural Networks for Language Modeling},
  year      = {2012},
  pages     = {194--197},
  timestamp = {Mon, 16 Mar 2015 00:00:00 +0100},
  url       = {http://www.isca-speech.org/archive/interspeech\_2012/i12\_0194.html},
}
%crossref  = {DBLP:conf/interspeech/2012},
%biburl    = {https://dblp.org/rec/bib/conf/interspeech/SundermeyerSN12},
%bibsource = {dblp computer science bibliography, https://dblp.org}

@Article{Young2017,
  author        = {Tom Young and Devamanyu Hazarika and Soujanya Poria and Erik Cambria},
  journal       = {CoRR},
  title         = {Recent Trends in Deep Learning Based Natural Language Processing},
  year          = {2017},
  volume        = {abs/1708.02709},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1708-02709},
  eprint        = {1708.02709},
  timestamp     = {Mon, 13 Aug 2018 16:46:22 +0200},
  url           = {http://arxiv.org/abs/1708.02709},
}

@InProceedings{Mikolov2010,
  author    = {Tomas Mikolov and Martin Karafi{\'{a}}t and Luk{\'{a}}s Burget and Jan Cernock{\'{y}} and Sanjeev Khudanpur},
  booktitle = {{INTERSPEECH} 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010},
  title     = {Recurrent neural network based language model},
  year      = {2010},
  pages     = {1045--1048},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/interspeech/MikolovKBCK10},
  timestamp = {Thu, 24 Aug 2017 11:26:53 +0200},
  url       = {http://www.isca-speech.org/archive/interspeech\_2010/i10\_1045.html},
}
%  crossref  = {DBLP:conf/interspeech/2010},

@InProceedings{Sutskever2014,
  author    = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada},
  title     = {Sequence to Sequence Learning with Neural Networks},
  year      = {2014},
  pages     = {3104--3112},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/nips/SutskeverVL14},
  timestamp = {Wed, 10 Dec 2014 21:34:12 +0100},
  url       = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks},
}
 % crossref  = {DBLP:conf/nips/2014},

@InProceedings{Cho2014,
  author    = {Cho, Kyunghyun and van Merri{\"{e}}nboer, Bart and G{\"{u}}l{\c c}ehre, {\c C}ağlar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  year      = {2014},
  address   = {Doha, Qatar},
  month     = oct,
  pages     = {1724--1734},
  publisher = {Association for Computational Linguistics},
  url       = {http://www.aclweb.org/anthology/D14-1179},
}


%% Optimization

@Article{Ghadimi2016,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  journal   = {Mathematical Programming},
  title     = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  year      = {2016},
  number    = {1-2},
  pages     = {59--99},
  volume    = {156},
  publisher = {Springer},
}

@Article{Ghadimi2012,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  journal   = {SIAM Journal on Optimization},
  title     = {Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  year      = {2012},
  number    = {4},
  pages     = {1469--1492},
  volume    = {22},
  publisher = {SIAM},
}

@InProceedings{Jin2018,
  author    = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
  booktitle = {Conference On Learning Theory},
  title     = {Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent},
  year      = {2018},
  pages     = {1042--1085},
}

@Article{Beck2009,
  author    = {Beck, Amir and Teboulle, Marc},
  journal   = {SIAM journal on imaging sciences},
  title     = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  year      = {2009},
  number    = {1},
  pages     = {183--202},
  volume    = {2},
  publisher = {SIAM},
}

@Article{Nesterov2012,
  author    = {Nesterov, Yu},
  journal   = {SIAM Journal on Optimization},
  title     = {Efficiency of coordinate descent methods on huge-scale optimization problems},
  year      = {2012},
  number    = {2},
  pages     = {341--362},
  volume    = {22},
  publisher = {SIAM},
}

@Article{Fang2018,
  author  = {Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal = {arXiv preprint arXiv:1807.01695},
  title   = {SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator},
  year    = {2018},
}

@InProceedings{Lin2015,
  author    = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {A universal catalyst for first-order optimization},
  year      = {2015},
  pages     = {3384--3392},
}

@InProceedings{ShalevShwartz2014,
  author    = {Shalev-Shwartz, Shai and Zhang, Tong},
  booktitle = {International Conference on Machine Learning},
  title     = {Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
  year      = {2014},
  pages     = {64--72},
}

@Article{Carmon2018,
  author    = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal   = {SIAM Journal on Optimization},
  title     = {Accelerated methods for nonconvex optimization},
  year      = {2018},
  number    = {2},
  pages     = {1751--1772},
  volume    = {28},
  publisher = {SIAM},
}

@Article{AllenZhu2017,
  author    = {Allen-Zhu, Zeyuan},
  journal   = {The Journal of Machine Learning Research},
  title     = {Katyusha: The first direct acceleration of stochastic gradient methods},
  year      = {2017},
  number    = {1},
  pages     = {8194--8244},
  volume    = {18},
  publisher = {JMLR. org},
}

@Misc{Rudin1991a,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@Article{Yuan2017,
  author  = {Xinru Yuan and Wen Huang and P.-A. Absil and K. A. Gallivan},
  journal = {Florida State University},
  title   = {A {R}iemannian quasi-{N}ewton method for computing the {K}archer mean of symmetric positive definite matrices},
  year    = {2017},
  number  = {FSU17-02},
}

@Book{Bhatia2007,
  author    = {R. Bhatia},
  publisher = {Princeton University Press},
  title     = {{Positive Definite Matrices}},
  year      = {2007},
}

@Article{Jeuris2012,
  author  = {B. Jeuris and R. Vandebril and B. Vandereycken},
  journal = {Electronic Transactions on Numerical Analysis},
  title   = {A survey and comparison of contemporary algorithms for computing the matrix geometric mean},
  year    = {2012},
  pages   = {379--402},
  volume  = {39},
}

@Article{Oja1992,
  author    = {Oja, Erkki},
  journal   = {Neural Networks},
  title     = {Principal components, minor components, and linear neural networks},
  year      = {1992},
  number    = {6},
  pages     = {927--935},
  volume    = {5},
  publisher = {Elsevier},
}

@Article{Gong2014,
  author  = {Gong, Pinghua and Ye, Jieping},
  journal = {arXiv preprint arXiv:1406.1102},
  title   = {Linear Convergence of Variance-Reduced Stochastic Gradient without Strong Convexity},
  year    = {2014},
}

@Article{Liu2004,
  author  = {Liu, Xiuwen and Srivastava, Anuj and Gallivan, Kyle},
  journal = {IEEE TPAMI},
  title   = {Optimal linear representations of images for object recognition},
  year    = {2004},
  number  = {5},
  pages   = {662--666},
  volume  = {26},
}

@Article{Nemirovski2009,
  author    = {A. Nemirovski and A. Juditsky and G. Lan and A. Shapiro},
  journal   = {SIAM Journal on Optimization},
  title     = {Robust Stochastic Approximation Approach to Stochastic Programming},
  year      = {2009},
  number    = {4},
  pages     = {1574-1609},
  volume    = {19},
  publisher = {SIAM},
}

@Article{Mania2015,
  author  = {Mania, Horia and Pan, Xinghao and Papailiopoulos, Dimitris and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I},
  journal = {arXiv:1507.06970},
  title   = {Perturbed Iterate Analysis for Asynchronous Stochastic Optimization},
  year    = {2015},
}

@Book{Kushner2003,
  author    = {Kushner, Harold and Yin, G George},
  publisher = {Springer Science \& Business Media},
  title     = {Stochastic approximation and recursive algorithms and applications},
  year      = {2003},
  volume    = {35},
}

@Article{Gardner1984,
  author    = {Gardner, William A},
  journal   = {Signal Processing},
  title     = {Learning characteristics of stochastic-gradient-descent algorithms: A general study, analysis, and critique},
  year      = {1984},
  number    = {2},
  pages     = {113--133},
  volume    = {6},
  publisher = {Elsevier},
}

@InProceedings{Agarwal2015,
  author    = {Agarwal, Alekh and Bottou, Leon},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  title     = {A Lower Bound for the Optimization of Finite Sums},
  year      = {2015},
  pages     = {78--86},
}

@Article{Kasai2016,
  author  = {Kasai, Hiroyuki and Sato, Hiroyuki and Mishra, Bamdev},
  journal = {arXiv preprint arXiv:1605.07367},
  title   = {{R}iemannian stochastic variance reduced gradient on Grassmann manifold},
  year    = {2016},
}

@Article{Bini2013,
  author    = {Bini, Dario A and Iannazzo, Bruno},
  journal   = {Linear Algebra and its Applications},
  title     = {Computing the Karcher mean of symmetric positive definite matrices},
  year      = {2013},
  number    = {4},
  pages     = {1700--1710},
  volume    = {438},
  publisher = {Elsevier},
}

@Article{Congedo2015,
  author    = {Congedo, Marco and Afsari, Bijan and Barachant, Alexandre and Moakher, Maher},
  journal   = {PloS one},
  title     = {Approximate joint diagonalization and geometric mean of symmetric positive definite matrices},
  year      = {2015},
  number    = {4},
  pages     = {e0121423},
  volume    = {10},
  publisher = {Public Library of Science},
}

@Article{Yuan2016a,
  author    = {Yuan, Xinru and Huang, Wen and Absil, Pierre-Antoine and Gallivan, Kyle},
  journal   = {Procedia Computer Science},
  title     = {A {R}iemannian Limited-memory BFGS Algorithm for Computing the Matrix Geometric Mean},
  year      = {2016},
  pages     = {2147--2157},
  volume    = {80},
  publisher = {Elsevier},
}

@Article{Vandereycken2013,
  author    = {Vandereycken, Bart},
  journal   = {SIAM Journal on Optimization},
  title     = {Low-rank matrix completion by {R}iemannian optimization},
  year      = {2013},
  number    = {2},
  pages     = {1214--1236},
  volume    = {23},
  publisher = {SIAM},
}

@Article{Wiesel2012,
  author    = {Wiesel, Ami},
  journal   = {IEEE Transactions on Signal Processing},
  title     = {Geodesic convexity and covariance estimation},
  year      = {2012},
  number    = {12},
  pages     = {6182--6189},
  volume    = {60},
  publisher = {IEEE},
}

@Article{Edelman1998,
  author    = {Edelman, Alan and Arias, Tom{\'a}s A and Smith, Steven T},
  journal   = {SIAM journal on Matrix Analysis and Applications},
  title     = {The geometry of algorithms with orthogonality constraints},
  year      = {1998},
  number    = {2},
  pages     = {303--353},
  volume    = {20},
  publisher = {SIAM},
}

@Article{Robbins1951,
  author  = {Robbins, H. and Monro, S.},
  journal = {Annals of Mathematical Statistics},
  title   = {A stochastic approximation method},
  year    = {1951},
  pages   = {400-407},
  volume  = {22},
}

@InProceedings{Zhang2016,
  author    = {Zhang, Hongyi and Sra, Suvrit},
  booktitle = {Conference on Learning Theory},
  title     = {First-order Methods for Geodesically Convex Optimization},
  year      = {2016},
  pages     = {1617--1638},
}

@Article{ShalevShwartz2015,
  author  = {Shalev-Shwartz, Shai},
  journal = {arXiv preprint arXiv:1502.06177},
  title   = {SDCA without duality},
  year    = {2015},
}

@Article{Reddi2016,
  author  = {Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
  journal = {arXiv:1603.06160},
  title   = {Stochastic Variance Reduction for Nonconvex Optimization},
  year    = {2016},
}

@Article{AllenZhu2016,
  author  = {Allen-Zhu, Zeyuan and Hazan, Elad},
  journal = {arXiv:1603.05643},
  title   = {Variance Reduction for Faster Non-Convex Optimization},
  year    = {2016},
}

@Article{Ghadimi2013,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  journal   = {SIAM Journal on Optimization},
  title     = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  year      = {2013},
  number    = {4},
  pages     = {2341--2368},
  volume    = {23},
  publisher = {SIAM},
}

@InProceedings{Johnson2013,
  author    = {Johnson, Rie and Zhang, Tong},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Accelerating stochastic gradient descent using predictive variance reduction},
  year      = {2013},
  pages     = {315--323},
}

@Book{Rubinstein2011,
  author    = {Rubinstein, Reuven Y and Kroese, Dirk P},
  publisher = {John Wiley \& Sons},
  title     = {Simulation and the Monte Carlo method},
  year      = {2011},
  volume    = {707},
}

@Article{Schmidt2017,
  author  = {Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal = {Mathematical Programming},
  title   = {Minimizing finite sums with the stochastic average gradient.},
  year    = {2017},
  volume  = {162},
}

@Article{Xiao2014,
  author    = {Xiao, Lin and Zhang, Tong},
  journal   = {SIAM Journal on Optimization},
  title     = {A proximal stochastic gradient method with progressive variance reduction},
  year      = {2014},
  number    = {4},
  pages     = {2057--2075},
  volume    = {24},
  publisher = {SIAM},
}

@InProceedings{Defazio2014,
  author    = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle = {NIPS},
  title     = {{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  year      = {2014},
  pages     = {1646--1654},
}

@Article{Konecnỳ2013,
  author  = {Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter},
  journal = {arXiv:1312.1666},
  title   = {Semi-stochastic gradient descent methods},
  year    = {2013},
}

@InProceedings{Bach2013,
  author    = {Bach, Francis and Moulines, Eric},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Non-strongly-convex smooth stochastic approximation with convergence rate $o(1/n)$},
  year      = {2013},
  pages     = {773--781},
}

@Book{Udriste1994,
  author    = {Udriste, Constantin},
  publisher = {Springer Science \& Business Media},
  title     = {Convex functions and optimization methods on {R}iemannian manifolds},
  year      = {1994},
  volume    = {297},
}

@Book{Absil2009,
  author    = {Absil, P-A and Mahony, Robert and Sepulchre, Rodolphe},
  publisher = {Princeton University Press},
  title     = {Optimization algorithms on matrix manifolds},
  year      = {2009},
}

@Article{Vandereycken2013a,
  author    = {Vandereycken, Bart},
  journal   = {SIAM Journal on Optimization},
  title     = {Low-rank matrix completion by {R}iemannian optimization},
  year      = {2013},
  number    = {2},
  pages     = {1214--1236},
  volume    = {23},
  publisher = {SIAM},
}

@InProceedings{Tan2014,
  author    = {Tan, Mingkui and Tsang, Ivor W and Wang, Li and Vandereycken, Bart and Pan, Sinno J},
  booktitle = {International Conference on Machine Learning (ICML-14)},
  title     = {{R}iemannian pursuit for big matrix recovery},
  year      = {2014},
  pages     = {1539--1547},
}

@Article{Cherian2015,
  author  = {Cherian, Anoop and Sra, Suvrit},
  journal = {arXiv:1507.02772},
  title   = {{R}iemannian Dictionary Learning and Sparse Coding for Positive Definite Matrices},
  year    = {2015},
}

@Article{Sun2015,
  author  = {Sun, Ju and Qu, Qing and Wright, John},
  journal = {arXiv:1511.04777},
  title   = {Complete Dictionary Recovery over the Sphere II: Recovery by {R}iemannian Trust-region Method},
  year    = {2015},
}

@Article{Edelman1998a,
  author    = {Edelman, Alan and Arias, Tom{\'a}s A and Smith, Steven T},
  journal   = {SIAM journal on Matrix Analysis and Applications},
  title     = {The geometry of algorithms with orthogonality constraints},
  year      = {1998},
  number    = {2},
  pages     = {303--353},
  volume    = {20},
  publisher = {SIAM},
}

@Article{Moakher2002,
  author    = {Moakher, Maher},
  journal   = {SIAM journal on matrix analysis and applications},
  title     = {Means and averaging in the group of rotations},
  year      = {2002},
  number    = {1},
  pages     = {1--16},
  volume    = {24},
  publisher = {SIAM},
}

@Article{Zhang2013,
  author    = {Zhang, Teng and Wiesel, Ami and Greco, Maria S},
  journal   = {Signal Processing, IEEE Transactions on},
  title     = {Multivariate generalized {G}aussian distribution: {C}onvexity and graphical models},
  year      = {2013},
  number    = {16},
  pages     = {4141--4148},
  volume    = {61},
  publisher = {IEEE},
}

@InProceedings{Sra2013,
  author    = {Sra, Suvrit and Hosseini, Reshad},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Geometric optimisation on positive definite matrices for elliptically contoured distributions},
  year      = {2013},
  pages     = {2562--2570},
}

@InProceedings{Hosseini2015,
  author    = {Reshad Hosseini and Suvrit Sra},
  booktitle = {NIPS},
  title     = {Matrix manifold optimization for {G}aussian mixtures},
  year      = {2015},
}

@Article{Karcher1977,
  author    = {Karcher, Hermann},
  journal   = {Communications on pure and applied mathematics},
  title     = {{R}iemannian center of mass and mollifier smoothing},
  year      = {1977},
  number    = {5},
  pages     = {509--541},
  volume    = {30},
  publisher = {Wiley Online Library},
}

@InProceedings{Shamir2015,
  author    = {Shamir, Ohad},
  booktitle = {International Conference on Machine Learning (ICML-15)},
  title     = {{A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate}},
  year      = {2015},
  pages     = {144--152},
}

@Article{Jin2015,
  author  = {Jin, Chi and Kakade, Sham M and Musco, Cameron and Netrapalli, Praneeth and Sidford, Aaron},
  journal = {arXiv:1510.08896},
  title   = {Robust Shift-and-Invert Preconditioning: Faster and More Sample Efficient Algorithms for Eigenvector Computation},
  year    = {2015},
}

@Book{Petersen2006,
  author    = {Petersen, Peter},
  publisher = {Springer Science \& Business Media},
  title     = {{R}iemannian geometry},
  year      = {2006},
  volume    = {171},
}

@InProceedings{Zhang2016a,
  author    = {Zhang, Hongyi and Reddi, Sashank J and Sra, Suvrit},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{R}iemannian {SVRG}: Fast stochastic optimization on {R}iemannian manifolds},
  year      = {2016},
  pages     = {4592--4600},
}

@Article{Boumal2016,
  author  = {Boumal, Nicolas and Absil, Pierre-Antoine and Cartis, Coralia},
  journal = {IMA Journal of Numerical Analysis},
  title   = {Global rates of convergence for nonconvex optimization on manifolds},
  year    = {2016},
}

@InProceedings{Nickel2017,
  author    = {Nickel, Maximillian and Kiela, Douwe},
  booktitle = {Advances in neural information processing systems},
  title     = {Poincar{\'e} embeddings for learning hierarchical representations},
  year      = {2017},
  pages     = {6338--6347},
}

@Misc{Kasai2016a,
  author        = {Hiroyuki Kasai and Hiroyuki Sato and Bamdev Mishra},
  title         = {{R}iemannian stochastic variance reduced gradient on Grassmann manifold},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1605.07367},
  primaryclass  = {cs.LG},
}

@Article{Sato2017,
  author  = {Sato, Hiroyuki and Kasai, Hiroyuki and Mishra, Bamdev},
  journal = {arXiv preprint arXiv:1702.05594},
  title   = {{R}iemannian stochastic variance reduced gradient},
  year    = {2017},
}

@Article{AllenZhu2017a,
  author  = {Allen-Zhu, Zeyuan},
  journal = {arXiv preprint arXiv:1708.08694},
  title   = {Natasha 2: Faster non-convex optimization than sgd},
  year    = {2017},
}

@InProceedings{Lei2017,
  author    = {Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Non-convex finite-sum optimization via scsg methods},
  year      = {2017},
  pages     = {2348--2358},
}

@Article{Carmon2017,
  author  = {Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal = {arXiv preprint arXiv:1705.02766},
  title   = {" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  year    = {2017},
}

@Article{Carmon2017a,
  author  = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal = {arXiv preprint arXiv:1710.11606},
  title   = {Lower bounds for finding stationary points I},
  year    = {2017},
}

@InProceedings{AllenZhu2016a,
  author    = {Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle = {International Conference on Machine Learning},
  title     = {Variance reduction for faster non-convex optimization},
  year      = {2016},
  pages     = {699--707},
}

% second order stationary

@Article{Reddi2017,
  author  = {Reddi, Sashank J and Zaheer, Manzil and Sra, Suvrit and Poczos, Barnabas and Bach, Francis and Salakhutdinov, Ruslan and Smola, Alexander J},
  journal = {arXiv preprint arXiv:1709.01434},
  title   = {A generic approach for escaping saddle points},
  year    = {2017},
}

@Article{Nesterov2006,
  author    = {Nesterov, Yurii and Polyak, Boris T},
  journal   = {Mathematical Programming},
  title     = {Cubic regularization of Newton method and its global performance},
  year      = {2006},
  number    = {1},
  pages     = {177--205},
  volume    = {108},
  publisher = {Springer},
}

@Article{Levy2016,
  author  = {Levy, Kfir Y},
  journal = {arXiv preprint arXiv:1611.04831},
  title   = {The power of normalization: Faster evasion of saddle points},
  year    = {2016},
}

@Article{Jin2017,
  author  = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1703.00887},
  title   = {How to escape saddle points efficiently},
  year    = {2017},
}

@InProceedings{Ge2015,
  author    = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle = {Conference on Learning Theory},
  title     = {Escaping from saddle points—online stochastic gradient for tensor decomposition},
  year      = {2015},
  pages     = {797--842},
}

@Article{Duchi2011,
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of Machine Learning Research},
  title   = {Adaptive subgradient methods for online learning and stochastic optimization},
  year    = {2011},
  number  = {Jul},
  pages   = {2121--2159},
  volume  = {12},
}

@Article{AllenZhu2017b,
  author  = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal = {arXiv preprint arXiv:1711.06673},
  title   = {Neon2: Finding Local Minima via First-Order Oracles},
  year    = {2017},
}

@Book{Nesterov2013,
  author    = {Nesterov, Yurii},
  publisher = {Springer Science \& Business Media},
  title     = {Introductory lectures on convex optimization: A basic course},
  year      = {2013},
  volume    = {87},
}

@Article{Tripuraneni2018,
  author  = {Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1802.09128},
  title   = {Averaging Stochastic Gradient Descent on {R}iemannian Manifolds},
  year    = {2018},
}

@Article{Zhang2018a,
  author  = {Zhang, Hongyi and Sra, Suvrit},
  journal = {arXiv preprint arXiv:1806.02812},
  title   = {Towards {R}iemannian Accelerated Gradient Methods},
  year    = {2018},
}

@Article{Wang2018,
  author  = {Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal = {arXiv preprint arXiv:1810.10690},
  title   = {SpiderBoost: A Class of Faster Variance-reduced Algorithms for Nonconvex Optimization},
  year    = {2018},
}

@Article{Bento2017,
  author    = {Bento, Glaydston C and Ferreira, Orizon P and Melo, Jefferson G},
  journal   = {Journal of Optimization Theory and Applications},
  title     = {Iteration-complexity of gradient, subgradient and proximal point methods on Riemannian manifolds},
  year      = {2017},
  number    = {2},
  pages     = {548--562},
  volume    = {173},
  publisher = {Springer},
}

@Article{Zhou2018b,
  author  = {Zhou, Pan and Yuan, Xiao-Tong and Feng, Jiashi},
  journal = {arXiv preprint arXiv:1811.08109},
  title   = {Faster First-Order Methods for Stochastic Non-Convex Optimization on Riemannian Manifolds},
  year    = {2018},
}

@Article{McMahan2010,
  author  = {McMahan, H Brendan and Streeter, Matthew},
  journal = {arXiv preprint arXiv:1002.4908},
  title   = {Adaptive bound optimization for online convex optimization},
  year    = {2010},
}

@Article{Kingma2014,
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  title   = {{ADAM}: A method for stochastic optimization},
  year    = {2014},
}

@Article{Tieleman2012,
  author  = {Tieleman, Tijmen and Hinton, Geoffrey},
  journal = {COURSERA: Neural networks for machine learning},
  title   = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  year    = {2012},
  number  = {2},
  pages   = {26--31},
  volume  = {4},
}

@InProceedings{Hazan2015,
  author    = {Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Beyond convexity: Stochastic quasi-convex optimization},
  year      = {2015},
  pages     = {1594--1602},
}

@Article{Gehring2017,
  author        = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  journal       = {ArXiv e-prints},
  title         = {Convolutional Sequence to Sequence Learning},
  year          = {2017},
  month         = May,
  archiveprefix = {arXiv},
  eprint        = {1705.03122},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Computation and Language},
  primaryclass  = {cs.CL},
}

@Article{Peters2018,
  author  = {Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1802.05365},
  title   = {Deep contextualized word representations},
  year    = {2018},
}

@InProceedings{Pascanu2013,
  author    = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = {International conference on machine learning},
  title     = {On the difficulty of training recurrent neural networks},
  year      = {2013},
  pages     = {1310--1318},
}

@Article{Pascanu2012,
  author  = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal = {CoRR, abs/1211.5063},
  title   = {Understanding the exploding gradient problem},
  year    = {2012},
  volume  = {2},
}

@Book{Goodfellow2016,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher = {MIT press},
  title     = {Deep learning},
  year      = {2016},
}

@Article{Reddi2019,
  author  = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal = {arXiv preprint arXiv:1904.09237},
  title   = {On the convergence of {ADAM} and beyond},
  year    = {2019},
}

@InProceedings{Zaheer2018,
  author    = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Adaptive methods for nonconvex optimization},
  year      = {2018},
  pages     = {9815--9825},
}

@Article{Zhou2018c,
  author  = {Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
  journal = {arXiv preprint arXiv:1808.05671},
  title   = {On the convergence of adaptive gradient methods for nonconvex optimization},
  year    = {2018},
}

@InProceedings{Mukkamala2017,
  author       = {Mukkamala, Mahesh Chandra and Hein, Matthias},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  title        = {Variants of rmsprop and {AdaGrad} with logarithmic regret bounds},
  year         = {2017},
  organization = {JMLR. org},
  pages        = {2545--2553},
}

@Article{Wu2019,
  author  = {Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal = {arXiv preprint arXiv:1902.07111},
  title   = {Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network},
  year    = {2019},
}

@Article{Li2018,
  author  = {Li, Xiaoyu and Orabona, Francesco},
  journal = {arXiv preprint arXiv:1805.08114},
  title   = {On the convergence of stochastic gradient descent with adaptive stepsizes},
  year    = {2018},
}

@Article{Staib2019,
  author  = {Staib, Matthew and Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  journal = {arXiv preprint arXiv:1901.09149},
  title   = {Escaping Saddle Points with Adaptive Gradient Methods},
  year    = {2019},
}

@Article{Ward2018,
  author  = {Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal = {arXiv preprint arXiv:1806.01811},
  title   = {{Adagrad} stepsizes: Sharp convergence over nonconvex landscapes, from any initialization},
  year    = {2018},
}

@Article{Zou2018a,
  author  = {Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  journal = {arXiv preprint arXiv:1811.09358},
  title   = {A Sufficient Condition for Convergences of {ADAM} and {RMSProp}},
  year    = {2018},
}

@Article{Liu2019,
  author  = {Liu, Mingrui and Mroueh, Youssef and Ross, Jerret and Zhang, Wei and Cui, Xiaodong and Das, Payel and Yang, Tianbao},
  journal = {arXiv preprint arXiv:1912.11940},
  title   = {Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},
  year    = {2019},
}

@Article{Cutkosky2020,
  author  = {Cutkosky, Ashok and Mehta, Harsh},
  journal = {arXiv preprint arXiv:2002.03305},
  title   = {Momentum Improves Normalized SGD},
  year    = {2020},
}

@Article{Arjevani2019,
  author  = {Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal = {arXiv preprint arXiv:1912.02365},
  title   = {Lower bounds for non-convex stochastic optimization},
  year    = {2019},
}

@Article{Rakhlin2011,
  author  = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal = {arXiv preprint arXiv:1109.5647},
  title   = {Making gradient descent optimal for strongly convex stochastic optimization},
  year    = {2011},
}

@Article{Gorbunov2020,
  author  = {Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal = {arXiv preprint arXiv:2005.10785},
  title   = {Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping},
  year    = {2020},
}

@Article{Nguyen2019,
  author  = {Nguyen, Thanh Huy and {\c{S}}im{\c{s}}ekli, Umut and G{\"u}rb{\"u}zbalaban, Mert and Richard, Ga{\"e}l},
  journal = {arXiv preprint arXiv:1906.09069},
  title   = {First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise},
  year    = {2019},
}

@Article{Huang2018,
  author  = {Huang, Haiwen and Wang, Chang and Dong, Bin},
  journal = {arXiv preprint arXiv:1805.07557},
  title   = {Nostalgic {Adam}: Weighting more of the past gradients when designing the adaptive learning rate},
  year    = {2018},
}

@Article{Ma2019,
  author  = {Ma, Jerry and Yarats, Denis},
  journal = {arXiv preprint arXiv:1910.04209},
  title   = {On the adequacy of untuned warmup for adaptive optimization},
  year    = {2019},
}

@Article{Liu2019a,
  author  = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal = {arXiv preprint arXiv:1908.03265},
  title   = {On the variance of the adaptive learning rate and beyond},
  year    = {2019},
}

@InProceedings{Zhang2020,
  author    = {Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
  booktitle = {International Conference on Learning Representations},
  title     = {Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
  year      = {2020},
}

@Article{Panigrahi2019,
  author  = {Panigrahi, Abhishek and Somani, Raghav and Goyal, Navin and Netrapalli, Praneeth},
  journal = {arXiv preprint arXiv:1910.09626},
  title   = {Non-Gaussianity of Stochastic Gradient Noise},
  year    = {2019},
}

@Article{Dai2019a,
  author  = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal = {arXiv preprint arXiv:1901.02860},
  title   = {Transformer-xl: Attentive language models beyond a fixed-length context},
  year    = {2019},
}

@Article{Simsekli2020,
  author  = {{\c{S}}im{\c{s}}ekli, Umut and Zhu, Lingjiong and Teh, Yee Whye and G{\"u}rb{\"u}zbalaban, Mert},
  journal = {arXiv preprint arXiv:2002.05685},
  title   = {Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise},
  year    = {2020},
}

@Article{Zhang2019,
  author  = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  journal = {arXiv preprint arXiv:1905.11881},
  title   = {Analysis of Gradient Clipping and Adaptive Scaling with a Relaxed Smoothness Condition},
  year    = {2019},
}

@Article{Juditsky2019,
  author  = {Juditsky, Anatoli and Nazin, Alexander and Nemirovsky, Arkadi and Tsybakov, Alexandre},
  journal = {arXiv preprint arXiv:1907.02707},
  title   = {Algorithms of robust stochastic optimization based on mirror descent method},
  year    = {2019},
}

@Article{Devlin2018,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  year    = {2018},
}

@InProceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  title     = {Attention is all you need},
  year      = {2017},
  pages     = {5998--6008},
}

@Misc{Rudin1991b,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@Article{LacosteJulien2012,
  author  = {Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal = {arXiv preprint arXiv:1212.2002},
  title   = {A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method},
  year    = {2012},
}

@Article{Bubeck2013,
  author    = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  journal   = {IEEE Transactions on Information Theory},
  title     = {Bandits with heavy tail},
  year      = {2013},
  number    = {11},
  pages     = {7711--7717},
  volume    = {59},
  publisher = {IEEE},
}

@Article{Flammarion2017,
  author  = {Flammarion, Nicolas and Bach, Francis},
  journal = {arXiv preprint arXiv:1702.06429},
  title   = {Stochastic composite least-squares regression with convergence rate O (1/n)},
  year    = {2017},
}

@Article{Nesterov2009,
  author    = {Nesterov, Yurii},
  journal   = {Mathematical programming},
  title     = {Primal-dual subgradient methods for convex problems},
  year      = {2009},
  number    = {1},
  pages     = {221--259},
  volume    = {120},
  publisher = {Springer},
}

@Article{Neumann1928,
  author    = {Neumann, J. v.},
  journal   = {Mathematische annalen},
  title     = {Zur theorie der gesellschaftsspiele},
  year      = {1928},
  number    = {1},
  pages     = {295--320},
  volume    = {100},
  publisher = {Springer},
}

@Article{Ostrovskii2020,
  author  = {Ostrovskii, Dmitrii M and Lowy, Andrew and Razaviyayn, Meisam},
  journal = {arXiv preprint arXiv:2002.07919},
  title   = {Efficient search of first-order nash equilibria in nonconvex-concave smooth min-max problems},
  year    = {2020},
}

@Article{Alkousa2019,
  author  = {Alkousa, Mohammad and Dvinskikh, Darina and Stonyakin, Fedor and Gasnikov, Alexander and Kovalev, Dmitry},
  journal = {arXiv preprint arXiv:1906.03620},
  title   = {Accelerated methods for composite non-bilinear saddle point problem},
  year    = {2019},
}

@Article{Kong2019,
  author  = {Kong, Weiwei and Monteiro, Renato DC},
  journal = {arXiv preprint arXiv:1905.13433},
  title   = {An accelerated inexact proximal point method for solving nonconvex-concave min-max problems},
  year    = {2019},
}

@Article{Nouiehed2019,
  author  = {Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
  journal = {arXiv preprint arXiv:1902.08297},
  title   = {Solving a class of non-convex min-max games using iterative first order methods},
  year    = {2019},
}

@Article{Lu2020,
  author    = {Lu, Songtao and Tsaknakis, Ioannis and Hong, Mingyi and Chen, Yongxin},
  journal   = {IEEE Transactions on Signal Processing},
  title     = {Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications},
  year      = {2020},
  pages     = {3676--3691},
  volume    = {68},
  publisher = {IEEE},
}

@Article{Thekumparampil2019,
  author  = {Thekumparampil, Kiran Koshy and Jain, Prateek and Netrapalli, Praneeth and Oh, Sewoong},
  journal = {arXiv preprint arXiv:1907.01543},
  title   = {Efficient algorithms for smooth minimax optimization},
  year    = {2019},
}

@Article{Rafique2018,
  author  = {Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
  journal = {arXiv preprint arXiv:1810.02060},
  title   = {Non-convex min-max optimization: Provable algorithms and applications in machine learning},
  year    = {2018},
}

@Article{Gidel2018,
  author  = {Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  journal = {arXiv preprint arXiv:1802.10551},
  title   = {A variational inequality perspective on generative adversarial networks},
  year    = {2018},
}

@Article{Chambolle2011,
  author    = {Chambolle, Antonin and Pock, Thomas},
  journal   = {Journal of mathematical imaging and vision},
  title     = {A first-order primal-dual algorithm for convex problems with applications to imaging},
  year      = {2011},
  number    = {1},
  pages     = {120--145},
  volume    = {40},
  publisher = {Springer},
}

@Article{Yadav2017,
  author  = {Yadav, Abhay and Shah, Sohil and Xu, Zheng and Jacobs, David and Goldstein, Tom},
  journal = {arXiv preprint arXiv:1705.07364},
  title   = {Stabilizing adversarial nets with prediction methods},
  year    = {2017},
}

@Article{Tseng1995,
  author    = {Tseng, Paul},
  journal   = {Journal of Computational and Applied Mathematics},
  title     = {On linear convergence of iterative methods for the variational inequality problem},
  year      = {1995},
  number    = {1-2},
  pages     = {237--252},
  volume    = {60},
  publisher = {Elsevier},
}

@Article{Korpelevich1976,
  author  = {Korpelevich, Galina M},
  journal = {Matecon},
  title   = {The extragradient method for finding saddle points and other problems},
  year    = {1976},
  pages   = {747--756},
  volume  = {12},
}

@Misc{Defazio2014a,
  author        = {Aaron Defazio and Francis Bach and Simon Lacoste-Julien},
  title         = {SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1407.0202},
  primaryclass  = {cs.LG},
}

@Article{Goodfellow2014,
  author  = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal = {arXiv preprint arXiv:1412.6572},
  title   = {Explaining and harnessing adversarial examples},
  year    = {2014},
}

@Article{Nesterov2005,
  author    = {Nesterov, Yu},
  journal   = {Mathematical programming},
  title     = {Smooth minimization of non-smooth functions},
  year      = {2005},
  number    = {1},
  pages     = {127--152},
  volume    = {103},
  publisher = {Springer},
}

@Article{Nemirovski2004,
  author    = {Nemirovski, Arkadi},
  journal   = {SIAM Journal on Optimization},
  title     = {Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  year      = {2004},
  number    = {1},
  pages     = {229--251},
  volume    = {15},
  publisher = {SIAM},
}

@Article{Nemirovski,
  author = {Nemirovski, A},
  title  = {INFORMATION-BASED COMPLEXITY},
}

@Article{Nemirovsky1991,
  author  = {A. S. Nemirovsky},
  journal = {J. Complex.},
  title   = {On optimality of Krylov's information when solving linear operator equations},
  year    = {1991},
  pages   = {121-130},
  volume  = {7},
}

@Article{Nemirovsky1992,
  author  = {A. S. Nemirovsky},
  journal = {J. Complex.},
  title   = {Information-based complexity of linear operator equations},
  year    = {1992},
  pages   = {153-175},
  volume  = {8},
}

@InProceedings{Ibrahim2020,
  author       = {Ibrahim, Adam and Azizian, Wa{\i}ss and Gidel, Gauthier and Mitliagkas, Ioannis},
  booktitle    = {International Conference on Machine Learning},
  title        = {Linear lower bounds and conditioning of differentiable games},
  year         = {2020},
  organization = {PMLR},
  pages        = {4583--4593},
}

@Article{Boob2019,
  author  = {Boob, Digvijay and Deng, Qi and Lan, Guanghui},
  journal = {arXiv preprint arXiv:1908.02734},
  title   = {Stochastic first-order methods for convex and nonconvex functional constrained optimization},
  year    = {2019},
}

@Misc{Mangoubi2020,
  author        = {Oren Mangoubi and Nisheeth K. Vishnoi},
  title         = {Greedy Adversarial Equilibrium: An Efficient Alternative to Nonconvex-Nonconcave Min-Max Optimization},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.12363},
  primaryclass  = {cs.DS},
}

@InProceedings{Diakonikolas2021,
  author       = {Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  title        = {Efficient methods for structured nonconvex-nonconcave min-max optimization},
  year         = {2021},
  organization = {PMLR},
  pages        = {2746--2754},
}

@Article{Rakhlin2013,
  author  = {Rakhlin, Alexander and Sridharan, Karthik},
  journal = {arXiv preprint arXiv:1311.1869},
  title   = {Optimization, learning, and games with predictable sequences},
  year    = {2013},
}

@Article{Mertikopoulos2018,
  author  = {Mertikopoulos, Panayotis and Lecouat, Bruno and Zenati, Houssam and Foo, Chuan-Sheng and Chandrasekhar, Vijay and Piliouras, Georgios},
  journal = {arXiv preprint arXiv:1807.02629},
  title   = {Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile},
  year    = {2018},
}

@Article{Hsieh2019,
  author  = {Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal = {arXiv preprint arXiv:1908.08465},
  title   = {On the convergence of single-call stochastic extra-gradient methods},
  year    = {2019},
}

@InProceedings{Arjevani2016,
  author       = {Arjevani, Yossi and Shamir, Ohad},
  booktitle    = {International Conference on Machine Learning},
  title        = {On the iteration complexity of oblivious first-order optimization algorithms},
  year         = {2016},
  organization = {PMLR},
  pages        = {908--916},
}

@InProceedings{Azizian2020,
  author       = {Azizian, Wa{\"\i}ss and Scieur, Damien and Mitliagkas, Ioannis and Lacoste-Julien, Simon and Gidel, Gauthier},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  title        = {Accelerating smooth games by manipulating spectral shapes},
  year         = {2020},
  organization = {PMLR},
  pages        = {1705--1715},
}

@Article{Zhang2019a,
  author  = {Zhang, Junyu and Hong, Mingyi and Zhang, Shuzhong},
  journal = {arXiv preprint arXiv:1912.07481},
  title   = {On lower iteration complexity bounds for the saddle point problems},
  year    = {2019},
}

@Article{Zhang2021,
  author  = {Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
  journal = {arXiv preprint arXiv:2103.15888},
  title   = {The Complexity of Nonconvex-Strongly-Concave Minimax Optimization},
  year    = {2021},
}

@Article{Madry2017,
  author  = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal = {arXiv preprint arXiv:1706.06083},
  title   = {Towards deep learning models resistant to adversarial attacks},
  year    = {2017},
}

@Article{Sion1958,
  author    = {Sion, Maurice},
  journal   = {Pacific Journal of mathematics},
  title     = {On general minimax theorems.},
  year      = {1958},
  number    = {1},
  pages     = {171--176},
  volume    = {8},
  publisher = {Pacific Journal of Mathematics},
}

@Misc{Fang2018a,
  author        = {Cong Fang and Chris Junchi Li and Zhouchen Lin and Tong Zhang},
  title         = {SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1807.01695},
  primaryclass  = {math.OC},
}

@Book{BenTal2009,
  author    = {Ben-Tal, Aharon and El Ghaoui, Laurent and Nemirovski, Arkadi},
  publisher = {Princeton university press},
  title     = {Robust optimization},
  year      = {2009},
}

@Article{Goodfellow2014a,
  author  = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1406.2661},
  title   = {Generative adversarial networks},
  year    = {2014},
}

@Article{Lin2019,
  author        = {Tianyi Lin and Chi Jin and Michael I. Jordan},
  journal       = {CoRR},
  title         = {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
  year          = {2019},
  volume        = {abs/1906.00331},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1906-00331.bib},
  eprint        = {1906.00331},
  timestamp     = {Thu, 13 Jun 2019 13:36:00 +0200},
  url           = {http://arxiv.org/abs/1906.00331},
}

@Article{Daskalakis2017,
  author  = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal = {arXiv preprint arXiv:1711.00141},
  title   = {Training gans with optimism},
  year    = {2017},
}

@InProceedings{Golowich2020,
  author       = {Golowich, Noah and Pattathil, Sarath and Daskalakis, Constantinos and Ozdaglar, Asuman},
  booktitle    = {Conference on Learning Theory},
  title        = {Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems},
  year         = {2020},
  organization = {PMLR},
  pages        = {1758--1784},
}

@Article{Daskalakis2020,
  author  = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  journal = {arXiv preprint arXiv:2009.09623},
  title   = {The complexity of constrained min-max optimization},
  year    = {2020},
}

@Article{Yang2020,
  author  = {Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal = {arXiv preprint arXiv:2002.09621},
  title   = {Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems},
  year    = {2020},
}

@InProceedings{Lin2020,
  author       = {Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle    = {International Conference on Machine Learning},
  title        = {On gradient descent ascent for nonconvex-concave minimax problems},
  year         = {2020},
  organization = {PMLR},
  pages        = {6083--6093},
}

@Misc{Lin2020a,
  author        = {Tianyi Lin and Chi Jin and Michael. I. Jordan},
  title         = {Near-Optimal Algorithms for Minimax Optimization},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2002.02417},
  primaryclass  = {math.OC},
}

@Article{Lee2020,
  author  = {Lee, Chung-Wei and Luo, Haipeng and Wei, Chen-Yu and Zhang, Mengxiao},
  journal = {arXiv preprint arXiv:2006.09517},
  title   = {Linear last-iterate convergence for matrix games and stochastic games},
  year    = {2020},
}

@Article{Daskalakis2021,
  author  = {Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
  journal = {arXiv preprint arXiv:2101.04233},
  title   = {Independent policy gradient methods for competitive reinforcement learning},
  year    = {2021},
}

@Article{Wei2021,
  author  = {Wei, Chen-Yu and Lee, Chung-Wei and Zhang, Mengxiao and Luo, Haipeng},
  journal = {arXiv preprint arXiv:2102.04540},
  title   = {Last-iterate Convergence of Decentralized Optimistic Gradient Descent/Ascent in Infinite-horizon Competitive Markov Games},
  year    = {2021},
}

@Article{Wang2017a,
  author  = {Wang, Mengdi},
  journal = {arXiv preprint arXiv:1710.06100},
  title   = {Primal-Dual $pi$ Learning: Sample Complexity and Sublinear Run Time for Ergodic Markov Decision Problems},
  year    = {2017},
}

@InProceedings{Serrano2020,
  author       = {Serrano, Joan Bas and Neu, Gergely},
  booktitle    = {Learning for Dynamics and Control},
  title        = {Faster saddle-point optimization for solving large-scale Markov decision processes},
  year         = {2020},
  organization = {PMLR},
  pages        = {413--423},
}

@Misc{Zhang2020a,
  author        = {Junyu Zhang and Mingyi Hong and Shuzhong Zhang},
  title         = {On Lower Iteration Complexity Bounds for the Saddle Point Problems},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1912.07481},
  primaryclass  = {math.OC},
}

@Article{Ouyang2019,
  author    = {Ouyang, Yuyuan and Xu, Yangyang},
  journal   = {Mathematical Programming},
  title     = {Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  year      = {2019},
  pages     = {1--35},
  publisher = {Springer},
}

@Article{Mokhtari2020,
  author    = {Mokhtari, Aryan and Ozdaglar, Asuman E and Pattathil, Sarath},
  journal   = {SIAM Journal on Optimization},
  title     = {Convergence Rate of O(1/k) for Optimistic Gradient and Extragradient Methods in Smooth Convex-Concave Saddle Point Problems},
  year      = {2020},
  number    = {4},
  pages     = {3230--3251},
  volume    = {30},
  publisher = {SIAM},
}

@InProceedings{Jin2020,
  author       = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael},
  booktitle    = {International Conference on Machine Learning},
  title        = {What is local optimality in nonconvex-nonconcave minimax optimization?},
  year         = {2020},
  organization = {PMLR},
  pages        = {4880--4889},
}

@Article{Wang2019,
  author  = {Wang, Yuanhao and Zhang, Guodong and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1910.07512},
  title   = {On solving minimax optimization locally: A follow-the-ridge approach},
  year    = {2019},
}

@Article{Daskalakis2018,
  author  = {Daskalakis, Constantinos and Panageas, Ioannis},
  journal = {arXiv preprint arXiv:1807.03907},
  title   = {The limit points of (optimistic) gradient descent in min-max optimization},
  year    = {2018},
}

@Book{Nesterov2018,
  author    = {Nesterov, Yurii},
  publisher = {Springer},
  title     = {Lectures on convex optimization},
  year      = {2018},
  volume    = {137},
}

@Article{ElMikkawy2004,
  author  = {M. El-Mikkawy},
  journal = {Appl. Math. Comput.},
  title   = {On the inverse of a general tridiagonal matrix},
  year    = {2004},
  pages   = {669-679},
  volume  = {150},
}

@Book{Nemirovski1983,
  author    = {Nemirovski, Arkadi. S. and Yudin, David. B.},
  publisher = {Wiley},
  title     = {Problem Complexity and Method Efficiency in Optimization},
  year      = {1983},
}

@Misc{Ghadimi2013a,
  author        = {Saeed Ghadimi and Guanghui Lan and Hongchao Zhang},
  title         = {Mini-batch Stochastic Approximation Methods for Nonconvex Stochastic Composite Optimization},
  year          = {2013},
  archiveprefix = {arXiv},
  eprint        = {1308.6594},
  primaryclass  = {math.OC},
}

@Book{Lee2018,
  author    = {Lee, John M},
  publisher = {Springer},
  title     = {Introduction to Riemannian manifolds},
  year      = {2018},
}

@Article{Eberle2016,
  author    = {Eberle, Andreas},
  journal   = {Probability theory and related fields},
  title     = {Reflection couplings and contraction rates for diffusions},
  year      = {2016},
  number    = {3},
  pages     = {851--886},
  volume    = {166},
  publisher = {Springer},
}

@Article{Manton2013,
  author    = {Manton, Jonathan H},
  journal   = {IEEE Journal of Selected Topics in Signal Processing},
  title     = {A primer on stochastic differential geometry for signal processing},
  year      = {2013},
  number    = {4},
  pages     = {681--699},
  volume    = {7},
  publisher = {IEEE},
}

@InProceedings{Lee2017,
  author    = {Lee, Yin Tat and Vempala, Santosh S},
  booktitle = {Proceedings of the 49th Annual ACM SIGACT Symposium on theory of Computing},
  title     = {Geodesic walks in polytopes},
  year      = {2017},
  pages     = {927--940},
}

@InProceedings{Cheng2020,
  author       = {Cheng, Xiang and Yin, Dong and Bartlett, Peter and Jordan, Michael},
  booktitle    = {International Conference on Machine Learning},
  title        = {Stochastic gradient and langevin processes},
  year         = {2020},
  organization = {PMLR},
  pages        = {1810--1819},
}

@Article{Drori2019,
  author  = {Drori, Yoel and Shamir, Ohad},
  journal = {arXiv preprint arXiv:1910.01845},
  title   = {The complexity of finding stationary points with stochastic gradient descent},
  year    = {2019},
}

@Article{Bolte2019,
  author  = {Bolte, J{\'e}r{\^o}me and Pauwels, Edouard},
  journal = {arXiv preprint arXiv:1909.10300},
  title   = {Conservative set valued fields, automatic differentiation, stochastic gradient method and deep learning},
  year    = {2019},
}

@Article{Kiwiel2007,
  author    = {Kiwiel, Krzysztof C},
  journal   = {SIAM Journal on Optimization},
  title     = {Convergence of the gradient sampling algorithm for nonsmooth nonconvex optimization},
  year      = {2007},
  number    = {2},
  pages     = {379--388},
  volume    = {18},
  publisher = {SIAM},
}

@Article{Beck2020,
  author    = {Beck, Amir and Hallak, Nadav},
  journal   = {SIAM Journal on Optimization},
  title     = {On the convergence to stationary points of deterministic and randomized feasible descent directions methods},
  year      = {2020},
  number    = {1},
  pages     = {56--79},
  volume    = {30},
  publisher = {SIAM},
}

@Misc{Delfour2019,
  author    = {Delfour, Michel C},
  title     = {Introduction to Optimization and {H}adamard Semidifferential Calculus},
  year      = {2019},
  publisher = {SIAM},
}

@Article{Foster2019,
  author  = {Foster, Dylan and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  journal = {arXiv preprint arXiv:1902.04686},
  title   = {The Complexity of Making the Gradient Small in Stochastic Convex Optimization},
  year    = {2019},
}

@InProceedings{Reddi2016a,
  author    = {Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle = {International conference on machine learning},
  title     = {Stochastic variance reduction for nonconvex optimization},
  year      = {2016},
  pages     = {314--323},
}

@InProceedings{AllenZhu2018,
  author    = {Allen-Zhu, Zeyuan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {How to make the gradients small stochastically: Even faster convex and nonconvex {SGD}},
  year      = {2018},
  pages     = {1157--1167},
}

@Article{Duchi2018,
  author    = {Duchi, John C and Ruan, Feng},
  journal   = {SIAM Journal on Optimization},
  title     = {Stochastic methods for composite and weakly convex optimization problems},
  year      = {2018},
  number    = {4},
  pages     = {3229--3259},
  volume    = {28},
  publisher = {SIAM},
}

@Article{Davis2019,
  author    = {Davis, Damek and Drusvyatskiy, Dmitriy},
  journal   = {SIAM Journal on Optimization},
  title     = {Stochastic model-based minimization of weakly convex functions},
  year      = {2019},
  number    = {1},
  pages     = {207--239},
  volume    = {29},
  publisher = {SIAM},
}

@Book{Coste2000,
  author    = {Coste, Michel},
  publisher = {Istituti editoriali e poligrafici internazionali Pisa},
  title     = {An introduction to o-minimal geometry},
  year      = {2000},
}

@Article{Mifflin1977,
  author    = {Mifflin, Robert},
  journal   = {Mathematics of Operations Research},
  title     = {An algorithm for constrained optimization with semismooth functions},
  year      = {1977},
  number    = {2},
  pages     = {191--207},
  volume    = {2},
  publisher = {INFORMS},
}

@Article{Burke2018,
  author  = {Burke, James V and Curtis, Frank E and Lewis, Adrian S and Overton, Michael L and Sim{\~o}es, Lucas EA},
  journal = {arXiv preprint arXiv:1804.11003},
  title   = {Gradient sampling methods for nonsmooth optimization},
  year    = {2018},
}

@Article{Benaim2005,
  author    = {Bena{\"\i}m, Michel and Hofbauer, Josef and Sorin, Sylvain},
  journal   = {SIAM Journal on Control and Optimization},
  title     = {Stochastic approximations and differential inclusions},
  year      = {2005},
  number    = {1},
  pages     = {328--348},
  volume    = {44},
  publisher = {SIAM},
}

@Article{Bolte2018,
  author    = {Bolte, J{\'e}r{\^o}me and Sabach, Shoham and Teboulle, Marc and Vaisbourd, Yakov},
  journal   = {SIAM Journal on Optimization},
  title     = {First order methods beyond convexity and {L}ipschitz gradient continuity with applications to quadratic inverse problems},
  year      = {2018},
  number    = {3},
  pages     = {2131--2151},
  volume    = {28},
  publisher = {SIAM},
}

@Article{Attouch2013,
  author    = {Attouch, Hedy and Bolte, J{\'e}r{\^o}me and Svaiter, Benar Fux},
  journal   = {Mathematical Programming},
  title     = {Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward--backward splitting, and regularized Gauss--Seidel methods},
  year      = {2013},
  number    = {1-2},
  pages     = {91--129},
  volume    = {137},
  publisher = {Springer},
}

@Article{Carmon2017b,
  author    = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal   = {Mathematical Programming},
  title     = {Lower bounds for finding stationary points {I}},
  year      = {2017},
  pages     = {1--50},
  publisher = {Springer},
}

@Article{Bertsekas1997,
  author    = {Bertsekas, Dimitri P},
  journal   = {Journal of the Operational Research Society},
  title     = {Nonlinear programming},
  year      = {1997},
  number    = {3},
  pages     = {334--334},
  volume    = {48},
  publisher = {Taylor \& Francis},
}

@Article{Shapiro1990,
  author    = {Shapiro, Alexander},
  journal   = {Journal of optimization theory and applications},
  title     = {On concepts of directional differentiability},
  year      = {1990},
  number    = {3},
  pages     = {477--487},
  volume    = {66},
  publisher = {Springer},
}

@Article{Mifflin1977a,
  author    = {Mifflin, Robert},
  journal   = {SIAM Journal on Control and Optimization},
  title     = {Semismooth and semiconvex functions in constrained optimization},
  year      = {1977},
  number    = {6},
  pages     = {959--972},
  volume    = {15},
  publisher = {SIAM},
}

@Book{Clarke1990,
  author    = {Clarke, Frank H},
  publisher = {Siam},
  title     = {Optimization and nonsmooth analysis},
  year      = {1990},
  volume    = {5},
}

@Article{Majewski2018,
  author  = {Majewski, Szymon and Miasojedow, B{\l}a{\.z}ej and Moulines, Eric},
  journal = {arXiv preprint arXiv:1805.01916},
  title   = {Analysis of nonsmooth stochastic approximation: the differential inclusion approach},
  year    = {2018},
}

@Article{Davis2018,
  author    = {Davis, Damek and Drusvyatskiy, Dmitriy and Kakade, Sham and Lee, Jason D},
  journal   = {Foundations of Computational Mathematics},
  title     = {Stochastic subgradient method converges on tame functions},
  year      = {2018},
  pages     = {1--36},
  publisher = {Springer},
}

@Article{Wijsman1966,
  author    = {Wijsman, Robert A},
  journal   = {Transactions of the American Mathematical Society},
  title     = {Convergence of sequences of convex sets, cones and functions. {II}},
  year      = {1966},
  number    = {1},
  pages     = {32--45},
  volume    = {123},
  publisher = {JSTOR},
}

@InProceedings{Agarwal2017,
  author       = {Agarwal, Naman and Allen-Zhu, Zeyuan and Bullins, Brian and Hazan, Elad and Ma, Tengyu},
  booktitle    = {Proceedings of the 49th Annual ACM Symposium on Theory of Computing},
  title        = {Finding approximate local minima faster than gradient descent},
  year         = {2017},
  organization = {ACM},
  pages        = {1195--1199},
}

@InProceedings{Fang2018b,
  author    = {Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  year      = {2018},
  pages     = {689--699},
}

@InProceedings{Nguyen2017,
  author       = {Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  title        = {{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  year         = {2017},
  organization = {JMLR. org},
  pages        = {2613--2621},
}

@Article{Drusvyatskiy2019,
  author    = {Drusvyatskiy, Dmitriy and Paquette, Courtney},
  journal   = {Mathematical Programming},
  title     = {Efficiency of minimizing compositions of convex functions and smooth maps},
  year      = {2019},
  number    = {1-2},
  pages     = {503--558},
  volume    = {178},
  publisher = {Springer},
}

@Article{Nguyen2019a,
  author  = {Nguyen, Lam M and van Dijk, Marten and Phan, Dzung T and Nguyen, Phuong Ha and Weng, Tsui-Wei and Kalagnanam, Jayant R},
  journal = {arXiv preprint arXiv:1901.07648},
  title   = {Optimal finite-sum smooth non-convex optimization with {SARAH}},
  year    = {2019},
}

@InProceedings{Jin2017a,
  author       = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  title        = {How to escape saddle points efficiently},
  year         = {2017},
  organization = {JMLR. org},
  pages        = {1724--1732},
}

@Article{Zhang2018b,
  author  = {Zhang, Siqi and He, Niao},
  journal = {arXiv preprint arXiv:1806.04781},
  title   = {On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization},
  year    = {2018},
}

@InProceedings{Fang2019,
  author    = {Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle = {Conference on Learning Theory},
  title     = {Sharp analysis for nonconvex sgd escaping from saddle points},
  year      = {2019},
}

@InProceedings{Daneshmand2018,
  author    = {Daneshmand, Hadi and Kohler, Jonas and Lucchi, Aurelien and Hofmann, Thomas},
  booktitle = {International Conference on Machine Learning},
  title     = {Escaping Saddles with Stochastic Gradients},
  year      = {2018},
  pages     = {1163--1172},
}

@Article{Burke2005,
  author    = {Burke, James V and Lewis, Adrian S and Overton, Michael L},
  journal   = {SIAM Journal on Optimization},
  title     = {A robust gradient sampling algorithm for nonsmooth, nonconvex optimization},
  year      = {2005},
  number    = {3},
  pages     = {751--779},
  volume    = {15},
  publisher = {SIAM},
}

@Article{Goldstein1977,
  author    = {Goldstein, AA},
  journal   = {Mathematical Programming},
  title     = {Optimization of Lipschitz continuous functions},
  year      = {1977},
  number    = {1},
  pages     = {14--22},
  volume    = {13},
  publisher = {Springer},
}

@Article{Sova1964,
  author  = {Sova, M.},
  journal = {Czechoslovak Mathematical Journal},
  title   = {General Theory of Differentiation in Linear Topological Spaces},
  year    = {1964},
  pages   = {485-508},
  volume  = {14},
}

@Article{Burke2002,
  author    = {Burke, James V and Lewis, Adrian S and Overton, Michael L},
  journal   = {Mathematics of Operations Research},
  title     = {Approximating subdifferentials by random sampling of gradients},
  year      = {2002},
  number    = {3},
  pages     = {567--584},
  volume    = {27},
  publisher = {INFORMS},
}

@InProceedings{Kingma2015,
  author    = {Kingma, Diederik P and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations},
  title     = {{Adam}: A method for stochastic optimization},
  year      = {2015},
}

@Article{Paszke2017,
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  title  = {Automatic differentiation in pytorch},
  year   = {2017},
}

@Article{Shamir2020,
  author  = {Shamir, Ohad},
  journal = {arXiv preprint arXiv:2002.11962},
  title   = {Can We Find Near-Approximately-Stationary Points of Nonsmooth Nonconvex Functions?},
  year    = {2020},
}

@Article{Borkar2000,
  author    = {Borkar, Vivek S and Meyn, Sean P},
  journal   = {SIAM Journal on Control and Optimization},
  title     = {The ODE method for convergence of stochastic approximation and reinforcement learning},
  year      = {2000},
  number    = {2},
  pages     = {447--469},
  volume    = {38},
  publisher = {SIAM},
}

@TechReport{Ruppert1988,
  author = {Ruppert, David},
  title  = {Efficient estimations from a slowly convergent Robbins-Monro process},
  year   = {1988},
  school = {Cornell University Operations Research and Industrial Engineering},
}

@Article{DulacArnold2019,
  author  = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal = {arXiv preprint arXiv:1904.12901},
  title   = {Challenges of real-world reinforcement learning},
  year    = {2019},
}

@InProceedings{ShalevShwartz2009,
  author    = {Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  booktitle = {COLT},
  title     = {Stochastic Convex Optimization.},
  year      = {2009},
}

@Article{DulacArnold2020,
  author  = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
  journal = {arXiv preprint arXiv:2003.11881},
  title   = {An empirical investigation of the challenges of real-world reinforcement learning},
  year    = {2020},
}

@Article{Fabian1968,
  author    = {Fabian, Vaclav and others},
  journal   = {The Annals of Mathematical Statistics},
  title     = {On asymptotic normality in stochastic approximation},
  year      = {1968},
  number    = {4},
  pages     = {1327--1332},
  volume    = {39},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Schulman2017,
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  title   = {Proximal policy optimization algorithms},
  year    = {2017},
}

@InProceedings{Devraj2017,
  author    = {Devraj, Adithya M and Meyn, Sean},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Zap Q-learning},
  year      = {2017},
  pages     = {2235--2244},
}

@InProceedings{Jadbabaie2015,
  author    = {Jadbabaie, Ali and Rakhlin, Alexander and Shahrampour, Shahin and Sridharan, Karthik},
  booktitle = {Artificial Intelligence and Statistics},
  title     = {Online optimization: Competing with dynamic comparators},
  year      = {2015},
  pages     = {398--406},
}

@Article{Anava2015,
  author  = {Anava, Oren and Hazan, Elad and Mannor, Shie},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Online learning for adversaries with memory: price of past mistakes},
  year    = {2015},
  pages   = {784--792},
  volume  = {28},
}

@InProceedings{Yuan2018,
  author    = {Yuan, Jianjun and Lamperski, Andrew},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Online convex optimization for cumulative constraints},
  year      = {2018},
  pages     = {6137--6146},
}

@InProceedings{Jenatton2016,
  author       = {Jenatton, Rodolphe and Huang, Jim and Archambeau, C{\'e}dric},
  booktitle    = {International Conference on Machine Learning},
  title        = {Adaptive algorithms for online convex optimization with long-term constraints},
  year         = {2016},
  organization = {PMLR},
  pages        = {402--411},
}

@Article{Rakhlin2013a,
  author = {Rakhlin, Alexander and Sridharan, Karthik},
  title  = {Online learning with predictable sequences},
  year   = {2013},
}

@Article{Byrd2012,
  author    = {Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen},
  journal   = {Mathematical programming},
  title     = {Sample size selection in optimization methods for machine learning},
  year      = {2012},
  number    = {1},
  pages     = {127--155},
  volume    = {134},
  publisher = {Springer},
}

@Article{Bubeck2014,
  author  = {Bubeck, S{\'e}bastien},
  journal = {arXiv preprint arXiv:1405.4980},
  title   = {Convex optimization: Algorithms and complexity},
  year    = {2014},
}

@Article{Jofre2019,
  author    = {Jofr{\'e}, Alejandro and Thompson, Philip},
  journal   = {Mathematical Programming},
  title     = {On variance reduction for stochastic smooth convex optimization with multiplicative noise},
  year      = {2019},
  number    = {1-2},
  pages     = {253--292},
  volume    = {174},
  publisher = {Springer},
}

@InProceedings{Rakhlin2012,
  author    = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle = icml,
  title     = {Making gradient descent optimal for strongly convex stochastic optimization},
  year      = {2012},
}

@Article{Besbes2015,
  author    = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  journal   = {Operations research},
  title     = {Non-stationary stochastic optimization},
  year      = {2015},
  number    = {5},
  pages     = {1227--1244},
  volume    = {63},
  publisher = {INFORMS},
}

@InProceedings{Liu2020,
  author    = {Liu, Mingrui and Mroueh, Youssef and Ross, Jerret and Zhang, Wei and Cui, Xiaodong and Das, Payel and Yang, Tianbao},
  booktitle = iclr,
  title     = {Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},
  year      = {2020},
}

@Article{Zhang2019b,
  author  = {Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  journal = {arXiv preprint arXiv:1912.03194},
  title   = {Why {ADAM} Beats SGD for Attention Models},
  year    = {2019},
}

@InProceedings{Mokhtari2016,
  author       = {Mokhtari, Aryan and Shahrampour, Shahin and Jadbabaie, Ali and Ribeiro, Alejandro},
  booktitle    = {2016 IEEE 55th Conference on Decision and Control (CDC)},
  title        = {Online optimization in dynamic environments: Improved regret rates for strongly convex problems},
  year         = {2016},
  organization = {IEEE},
  pages        = {7195--7201},
}

@InProceedings{Besbes2014,
  author    = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
  booktitle = {Advances in neural information processing systems},
  title     = {Stochastic multi-armed-bandit problem with non-stationary rewards},
  year      = {2014},
  pages     = {199--207},
}

@Article{Gadat2017,
  author  = {Gadat, S{\'e}bastien and Panloup, Fabien},
  journal = {arXiv preprint arXiv:1709.03342},
  title   = {Optimal non-asymptotic bound of the {Ruppert-Polyak} averaging without strong convexity},
  year    = {2017},
}

@Article{Friedlander2012,
  author    = {Friedlander, Michael P and Schmidt, Mark},
  journal   = {SIAM Journal on Scientific Computing},
  title     = {Hybrid deterministic-stochastic methods for data fitting},
  year      = {2012},
  number    = {3},
  pages     = {A1380--A1405},
  volume    = {34},
  publisher = {SIAM},
}

@Article{Rosasco2019,
  author    = {Rosasco, Lorenzo and Villa, Silvia and V{\~u}, Bang C{\^o}ng},
  journal   = {Applied Mathematics \& Optimization},
  title     = {Convergence of stochastic proximal gradient algorithm},
  year      = {2019},
  pages     = {1--27},
  publisher = {Springer},
}

@InProceedings{Moulines2011,
  author    = {Moulines, Eric and Bach, Francis R},
  booktitle = nips,
  title     = {Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  year      = {2011},
}

@Article{Simsekli2019a,
  author  = {{\c{S}}im{\c{s}}ekli, Umut and G{\"u}rb{\"u}zbalaban, Mert and Nguyen, Thanh Huy and Richard, Ga{\"e}l and Sagun, Levent},
  journal = {arXiv preprint arXiv:1912.00018},
  title   = {On the Heavy-Tailed Theory of Stochastic Gradient Descent for Deep Neural Networks},
  year    = {2019},
}

@Article{Radford2015,
  author  = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal = {arXiv preprint arXiv:1511.06434},
  title   = {Unsupervised representation learning with deep convolutional generative adversarial networks},
  year    = {2015},
}

@InProceedings{Huang2019,
  author    = {Huang, Haiwen and Wang, Chang and Dong, Bin},
  booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  title     = {Nostalgic {A}dam: weighting more of the past gradients when designing the adaptive learning rate},
  year      = {2019},
}

@InProceedings{Chen2019,
  author    = {Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle = iclr,
  title     = {On the convergence of a class of {ADAM}-type algorithms for non-convex optimization},
  year      = {2019},
}

@InProceedings{Zhou2019,
  author    = {Zhou, Zhiming and Zhang, Qingru and Lu, Guansong and Wang, Hongwei and Zhang, Weinan and Yu, Yong},
  booktitle = iclr,
  title     = {AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods},
  year      = {2019},
}

@Article{Simsekli2019b,
  author  = {{\c{S}}im{\c{s}}ekli, Umut and Sagun, Levent and G{\"u}rb{\"u}zbalaba, Mert},
  journal = {arXiv preprint arXiv:1901.06053},
  title   = {A tail-index analysis of stochastic gradient noise in deep neural networks},
  year    = {2019},
}

@InProceedings{Fang2018c,
  author    = {Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle = nips,
  title     = {SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator},
  year      = {2018},
}

@Misc{Rudin1991c,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@InProceedings{Agarwal2009,
  author    = {Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter L and Ravikumar, Pradeep K},
  booktitle = nips,
  title     = {Information-theoretic lower bounds on the oracle complexity of convex optimization},
  year      = {2009},
}

@InProceedings{Cheng2018,
  author       = {Cheng, Xiang and Chatterji, Niladri S and Bartlett, Peter L and Jordan, Michael I},
  booktitle    = {Conference on Learning Theory},
  title        = {Underdamped Langevin MCMC: A non-asymptotic analysis},
  year         = {2018},
  organization = {PMLR},
  pages        = {300--323},
}

@Article{Xu2017,
  author  = {Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  journal = {arXiv preprint arXiv:1707.06618},
  title   = {Global convergence of Langevin dynamics based algorithms for nonconvex optimization},
  year    = {2017},
}

@InProceedings{Mou2018,
  author       = {Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  booktitle    = {Conference on Learning Theory},
  title        = {Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  year         = {2018},
  organization = {PMLR},
  pages        = {605--638},
}

@Article{Ma2019a,
  author    = {Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Sampling can be faster than optimization},
  year      = {2019},
  number    = {42},
  pages     = {20881--20885},
  volume    = {116},
  publisher = {National Acad Sciences},
}

@InProceedings{Raginsky2017,
  author       = {Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle    = {Conference on Learning Theory},
  title        = {Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  year         = {2017},
  organization = {PMLR},
  pages        = {1674--1703},
}

@InProceedings{Harmeling2009,
  author       = {Harmeling, Stefan and Hirsch, Michael and Sra, Suvrit and Sch{\"o}lkopf, Berhard},
  booktitle    = {2009 IEEE International Conference on Computational Photography (ICCP)},
  title        = {Online blind deconvolution for astronomical imaging},
  year         = {2009},
  organization = {IEEE},
  pages        = {1--7},
}

@InProceedings{Li2019,
  author    = {Li, Xiaoyu and Orabona, Francesco},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
  title     = {On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes},
  year      = {2019},
}

@InProceedings{Staib2019a,
  author    = {Staib, Matthew and Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle = icml,
  title     = {Escaping Saddle Points with Adaptive Gradient Methods},
  year      = {2019},
}

@InProceedings{Ward2019,
  author    = {Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle = icml,
  title     = {{AdaGrad} stepsizes: Sharp convergence over nonconvex landscapes},
  year      = {2019},
}

@Article{Bottou2018,
  author    = {Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal   = {Siam Review},
  title     = {Optimization methods for large-scale machine learning},
  year      = {2018},
  number    = {2},
  pages     = {223--311},
  volume    = {60},
  publisher = {SIAM},
}

@Article{Strehl2008,
  author    = {Strehl, Alexander L and Littman, Michael L},
  journal   = {Journal of Computer and System Sciences},
  title     = {An analysis of model-based interval estimation for Markov decision processes},
  year      = {2008},
  number    = {8},
  pages     = {1309--1331},
  volume    = {74},
  publisher = {Academic Press},
}

@InProceedings{Jin2018a,
  author    = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle = nips,
  title     = {Is q-learning provably efficient?},
  year      = {2018},
}

@InProceedings{Azar2017,
  author    = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle = icml,
  title     = {Minimax regret bounds for reinforcement learning},
  year      = {2017},
}

@InProceedings{Levy2017,
  author    = {Levy, Kfir},
  booktitle = nips,
  title     = {Online to offline conversions, universality and adaptive minibatch sizes},
  year      = {2017},
}

@InProceedings{Levy2018,
  author    = {Levy, Kfir Y and Yurtsever, Alp and Cevher, Volkan},
  booktitle = nips,
  title     = {Online adaptive methods, universality and acceleration},
  year      = {2018},
}

@Article{Silva2018,
  author  = {da Silva, Andr{\'e} Belotto and Gazeau, Maxime},
  journal = {arXiv preprint arXiv:1810.13108},
  title   = {A general system of differential equations to model first order adaptive algorithms},
  year    = {2018},
}

@Article{Ljung1977,
  author    = {Ljung, Lennart},
  journal   = {IEEE transactions on automatic control},
  title     = {Analysis of recursive stochastic algorithms},
  year      = {1977},
  number    = {4},
  pages     = {551--575},
  volume    = {22},
  publisher = {IEEE},
}

@Book{Borkar2009,
  author    = {Borkar, Vivek S},
  publisher = {Springer},
  title     = {Stochastic approximation: a dynamical systems viewpoint},
  year      = {2009},
  volume    = {48},
}

@Article{Tsitsiklis1994,
  author    = {Tsitsiklis, John N},
  journal   = {Machine learning},
  title     = {Asynchronous stochastic approximation and Q-learning},
  year      = {1994},
  number    = {3},
  pages     = {185--202},
  volume    = {16},
  publisher = {Springer},
}

@Article{Szepesvari2010,
  author    = {Szepesv{\'a}ri, Csaba},
  journal   = {Synthesis lectures on artificial intelligence and machine learning},
  title     = {Algorithms for reinforcement learning},
  year      = {2010},
  number    = {1},
  pages     = {1--103},
  volume    = {4},
  publisher = {Morgan \& Claypool Publishers},
}

@Article{Borkar1997,
  author    = {Borkar, Vivek S},
  journal   = {Systems \& Control Letters},
  title     = {Stochastic approximation with two time scales},
  year      = {1997},
  number    = {5},
  pages     = {291--294},
  volume    = {29},
  publisher = {Elsevier},
}

@Article{Polyak1992,
  author    = {Polyak, Boris T and Juditsky, Anatoli B},
  journal   = {SIAM journal on control and optimization},
  title     = {Acceleration of stochastic approximation by averaging},
  year      = {1992},
  number    = {4},
  pages     = {838--855},
  volume    = {30},
  publisher = {SIAM},
}

@Book{Ljung1983,
  author    = {Ljung, Lennart and S{\"o}derstr{\"o}m, Torsten},
  publisher = {MIT press},
  title     = {Theory and practice of recursive identification},
  year      = {1983},
}

@Article{,
}
@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}
@InProceedings{Martens,
  author = {Martens, James and Dahl, George and Hinton, Geoffrey},
}

@Article{Lan2017,
  author  = {Lan, Guanghui and Lee, Soomin and Zhou, Yi},
  journal = {arXiv preprint arXiv:1701.03961},
  title   = {Communication-Efficient Algorithms for Decentralized and Stochastic Optimization},
  year    = {2017},
}

@Article{Zhang2019c,
  author  = {Zhang, Jingzhao and Sra, Suvrit and Jadbabaie, Ali},
  journal = {submitted to CDC 2019},
  title   = {Acceleration in First Order Quasi-strongly Convex Optimization by {ODE} Discretization},
  year    = {2019},
}

@Article{Necoara2018,
  author    = {Necoara, Ion and Nesterov, Yu and Glineur, Francois},
  journal   = {Mathematical Programming},
  title     = {Linear convergence of first order methods for non-strongly convex optimization},
  year      = {2018},
  pages     = {1--39},
  publisher = {Springer},
}

@Article{Zhang2018c,
  author  = {Zhang, Jingzhao and Mokhtari, Aryan and Sra, Suvrit and Jadbabaie, Ali},
  journal = {arXiv preprint arXiv:1805.00521},
  title   = {Direct {R}unge-{K}utta Discretization Achieves Acceleration},
  year    = {2018},
}

@Article{Shi2018,
  author  = {Shi, Bin and Du, Simon S and Jordan, Michael I and Su, Weijie J},
  journal = {arXiv preprint arXiv:1810.08907},
  title   = {Understanding the acceleration phenomenon via high-resolution differential equations},
  year    = {2018},
}

@Article{Barakat2018,
  author  = {Barakat, Anas and Bianchi, Pascal},
  journal = {arXiv preprint arXiv:1810.02263},
  title   = {Convergence of the {ADAM} algorithm from a Dynamical System Viewpoint},
  year    = {2018},
}

@Article{Franca2018,
  author  = {Fran{\c{c}}a, Guilherme and Robinson, Daniel P and Vidal, Ren{\'e}},
  journal = {arXiv preprint arXiv:1805.06579},
  title   = {ADMM and Accelerated {ADMM} as Continuous Dynamical Systems},
  year    = {2018},
}

@Misc{Shi2019,
  author  = {Bin Shi and Simon S. Du and Weijie J. Su and Michael I. Jordan},
  title   = {Acceleration via Symplectic Discretization of High-Resolution Differential Equations},
  year    = {2019},
  journal = {arXiv:1902.03694},
}

@InProceedings{Xu2018,
  author    = {Xu, Pan and Wang, Tianhao and Gu, Quanquan},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  title     = {Accelerated stochastic mirror descent: From continuous-time dynamics to discrete-time algorithms},
  year      = {2018},
  pages     = {1087--1096},
}

@Misc{Rudin1991d,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@InProceedings{Scaman2018a,
  author    = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Optimal algorithms for non-smooth distributed optimization in networks},
  year      = {2018},
}

@Article{Donoghue2015,
  author    = {Donoghue, Brendan and Candes, Emmanuel},
  journal   = {Foundations of computational mathematics},
  title     = {Adaptive restart for accelerated gradient schemes},
  year      = {2015},
  number    = {3},
  pages     = {715--732},
  volume    = {15},
  publisher = {Springer},
}

@Book{Rockafellar2015,
  author    = {Rockafellar, Ralph Tyrell},
  publisher = {Princeton university press},
  title     = {Convex analysis},
  year      = {2015},
}

@Book{Rockafellar2009,
  author    = {Rockafellar, R Tyrrell and Wets, Roger J-B},
  publisher = {Springer Science \& Business Media},
  title     = {Variational analysis},
  year      = {2009},
  volume    = {317},
}

@Article{Jin2017b,
  author  = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1711.10456},
  title   = {Accelerated gradient descent escapes saddle points faster than gradient descent},
  year    = {2017},
}

@Misc{Rudin1991e,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@Article{Zhang2016b,
  author  = {Zhang, Hongyi and Sra, Suvrit},
  journal = {arXiv:1602.06053},
  title   = {First-order Methods for Geodesically Convex Optimization},
  year    = {2016},
}

@Article{Schmidt2013,
  author  = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
  journal = {arXiv:1309.2388},
  title   = {Minimizing finite sums with the stochastic average gradient},
  year    = {2013},
}

@Article{Merity2017,
  author  = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal = {arXiv preprint arXiv:1708.02182},
  title   = {Regularizing and optimizing LSTM language models},
  year    = {2017},
}

@Article{,
}

@Article{Carmon2016,
  author        = {Yair Carmon and John C. Duchi},
  title         = {Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex Newton Step},
  year          = {2016},
  month         = dec,
  abstract      = {We consider the minimization of non-convex quadratic forms regularized by a cubic term, which exhibit multiple saddle points and poor local minima. Nonetheless, we prove that, under mild assumptions, gradient descent approximates the $\textit{global minimum}$ to within $\varepsilon$ accuracy in $O(\varepsilon^{-1}\log(1/\varepsilon))$ steps for large $\varepsilon$ and $O(\log(1/\varepsilon))$ steps for small $\varepsilon$ (compared to a condition number we define), with at most logarithmic dependence on the problem dimension. When we use gradient descent to approximate the Nesterov-Polyak cubic-regularized Newton step, our result implies a rate of convergence to second-order stationary points of general smooth non-convex functions.},
  archiveprefix = {arXiv},
  eprint        = {1612.00547},
  file          = {:Carmon2016 - Gradient Descent Efficiently Finds the Cubic Regularized Non Convex Newton Step.pdf:PDF},
  keywords      = {math.OC, cs.DS},
  primaryclass  = {math.OC},
}

@Article{,
}
@article{nesterov2006cubic,
  title={Cubic regularization of Newton method and its global performance},
  author={Nesterov, Yurii and Polyak, Boris T},
  journal={Mathematical Programming},
  volume={108},
  number={1},
  pages={177--205},
  year={2006},
  publisher={Springer}
}@article{nesterov2006cubic,
  title={Cubic regularization of Newton method and its global performance},
  author={Nesterov, Yurii and Polyak, Boris T},
  journal={Mathematical Programming},
  volume={108},
  number={1},
  pages={177--205},
  year={2006},
  publisher={Springer}
}

@Article{Woodworth2016,
  author        = {Blake Woodworth and Nathan Srebro},
  title         = {Tight Complexity Bounds for Optimizing Composite Objectives},
  year          = {2016},
  month         = may,
  abstract      = {We provide tight upper and lower bounds on the complexity of minimizing the average of $m$ convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.},
  archiveprefix = {arXiv},
  eprint        = {1605.08003},
  file          = {:Woodworth2016 - Tight Complexity Bounds for Optimizing Composite Objectives.pdf:PDF},
  keywords      = {math.OC, cs.LG, stat.ML},
  primaryclass  = {math.OC},
}

@Article{AllenZhu2018a,
  author        = {Zeyuan Allen-Zhu},
  title         = {Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization},
  year          = {2018},
  month         = feb,
  abstract      = {The problem of minimizing sum-of-nonconvex functions (i.e., convex functions that are average of non-convex ones) is becoming increasingly important in machine learning, and is the core machinery for PCA, SVD, regularized Newton's method, accelerated non-convex optimization, and more. We show how to provably obtain an accelerated stochastic algorithm for minimizing sum-of-nonconvex functions, by $\textit{adding one additional line}$ to the well-known SVRG method. This line corresponds to momentum, and shows how to directly apply momentum to the finite-sum stochastic minimization of sum-of-nonconvex functions. As a side result, our method enjoys linear parallel speed-up using mini-batch.},
  archiveprefix = {arXiv},
  eprint        = {1802.03866},
  file          = {:- Katyusha X_ Practical Momentum Method for Stochastic Sum of Nonconvex Optimization.pdf:PDF},
  keywords      = {cs.LG, cs.DS, math.OC, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{AllenZhu2016b,
  author        = {Zeyuan Allen-Zhu},
  title         = {Katyusha: The First Direct Acceleration of Stochastic Gradient Methods},
  year          = {2016},
  month         = mar,
  abstract      = {Nesterov's momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov's momentum from providing similar acceleration, even if the underlying problem is convex and finite-sum. We introduce $\mathtt{Katyusha}$, a direct, primal-only stochastic gradient method to fix this issue. In convex finite-sum stochastic optimization, $\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an optimal parallel linear speedup in the mini-batch setting. The main ingredient is $\textit{Katyusha momentum}$, a novel "negative momentum" on top of Nesterov's momentum. It can be incorporated into a variance-reduction based algorithm and speed it up, both in terms of $\textit{sequential and parallel}$ performance. Since variance reduction has been successfully applied to a growing list of practical problems, our paper suggests that in each of such cases, one could potentially try to give Katyusha a hug.},
  archiveprefix = {arXiv},
  eprint        = {1603.05953},
  file          = {:AllenZhu2016b - Katyusha_ the First Direct Acceleration of Stochastic Gradient Methods.pdf:PDF},
  keywords      = {math.OC, cs.DS, cs.LG, stat.ML},
  primaryclass  = {math.OC},
}

@Article{Ghadimi2014,
  author        = {Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  title         = {Global convergence of the Heavy-ball method for convex optimization},
  year          = {2014},
  month         = dec,
  abstract      = {This paper establishes global convergence and provides global bounds of the convergence rate of the Heavy-ball method for convex optimization problems. When the objective function has Lipschitz-continuous gradient, we show that the Cesaro average of the iterates converges to the optimum at a rate of $O(1/k)$ where k is the number of iterations. When the objective function is also strongly convex, we prove that the Heavy-ball iterates converge linearly to the unique optimum.},
  archiveprefix = {arXiv},
  eprint        = {1412.7457},
  file          = {:Ghadimi2014 - Global Convergence of the Heavy Ball Method for Convex Optimization.pdf:PDF},
  keywords      = {math.OC},
  primaryclass  = {math.OC},
}

@Article{Yang2016,
  author        = {Tianbao Yang and Qihang Lin and Zhe Li},
  title         = {Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  year          = {2016},
  month         = apr,
  abstract      = {Recently, {\it stochastic momentum} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods.},
  archiveprefix = {arXiv},
  eprint        = {1604.03257},
  file          = {:Yang2016 - Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non Convex Optimization.pdf:PDF},
  keywords      = {math.OC, stat.ML},
  primaryclass  = {math.OC},
}

@Article{Choi2019,
  author        = {Dami Choi and Christopher J. Shallue and Zachary Nado and Jaehoon Lee and Chris J. Maddison and George E. Dahl},
  title         = {On Empirical Comparisons of Optimizers for Deep Learning},
  year          = {2019},
  month         = oct,
  abstract      = {Selecting an optimizer is a central step in the contemporary deep learning pipeline. In this paper, we demonstrate the sensitivity of optimizer comparisons to the hyperparameter tuning protocol. Our findings suggest that the hyperparameter search space may be the single most important factor explaining the rankings obtained by recent empirical comparisons in the literature. In fact, we show that these results can be contradicted when hyperparameter search spaces are changed. As tuning effort grows without bound, more general optimizers should never underperform the ones they can approximate (i.e., Adam should never perform worse than momentum), but recent attempts to compare optimizers either assume these inclusion relationships are not practically relevant or restrict the hyperparameters in ways that break the inclusions. In our experiments, we find that inclusion relationships between optimizers matter in practice and always predict optimizer comparisons. In particular, we find that the popular adaptive gradient methods never underperform momentum or gradient descent. We also report practical tips around tuning often ignored hyperparameters of adaptive gradient methods and raise concerns about fairly benchmarking optimizers for neural network training.},
  archiveprefix = {arXiv},
  eprint        = {1910.05446},
  file          = {:Choi2019 - On Empirical Comparisons of Optimizers for Deep Learning.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Ioffe2015,
  author        = {Sergey Ioffe and Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year          = {2015},
  month         = feb,
  abstract      = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  eprint        = {1502.03167},
  file          = {:Ioffe2015 - Batch Normalization_ Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Santurkar2018a,
  author        = {Shibani Santurkar and Dimitris Tsipras and Andrew Ilyas and Aleksander Madry},
  title         = {How Does Batch Normalization Help Optimization?},
  year          = {2018},
  month         = may,
  abstract      = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  archiveprefix = {arXiv},
  eprint        = {1805.11604},
  file          = {:Santurkar2018a - How Does Batch Normalization Help Optimization_.pdf:PDF},
  keywords      = {stat.ML, cs.LG, cs.NE},
  primaryclass  = {stat.ML},
}

@Article{Zhang2020b,
  author        = {Jingzhao Zhang and Hongzhou Lin and Stefanie Jegelka and Ali Jadbabaie and Suvrit Sra},
  title         = {Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions},
  year          = {2020},
  month         = feb,
  abstract      = {We provide the first non-asymptotic analysis for finding stationary points of nonsmooth, nonconvex functions. In particular, we study the class of Hadamard semi-differentiable functions, perhaps the largest class of nonsmooth functions for which the chain rule of calculus holds. This class contains examples such as ReLU neural networks and others with non-differentiable activation functions. We first show that finding an $\epsilon$-stationary point with first-order methods is impossible in finite time. We then introduce the notion of $(\delta, \epsilon)$-stationarity, which allows for an $\epsilon$-approximate gradient to be the convex combination of generalized gradients evaluated at points within distance $\delta$ to the solution. We propose a series of randomized first-order methods and analyze their complexity of finding a $(\delta, \epsilon)$-stationary point. Furthermore, we provide a lower bound and show that our stochastic algorithm has min-max optimal dependence on $\delta$. Empirically, our methods perform well for training ReLU neural networks.},
  archiveprefix = {arXiv},
  eprint        = {2002.04130},
  file          = {:Zhang2020b - Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions.pdf:PDF},
  keywords      = {math.OC, cs.LG},
  primaryclass  = {math.OC},
}

@Article{Zhang2018d,
  author        = {Jingzhao Zhang and Hongyi Zhang and Suvrit Sra},
  title         = {R-SPIDER: A Fast Riemannian Stochastic Optimization Algorithm with Curvature Independent Rate},
  year          = {2018},
  month         = nov,
  abstract      = {We study smooth stochastic optimization problems on Riemannian manifolds. Via adapting the recently proposed SPIDER algorithm \citep{fang2018spider} (a variance reduced stochastic method) to Riemannian manifold, we can achieve faster rate than known algorithms in both the finite sum and stochastic settings. Unlike previous works, by \emph{not} resorting to bounding iterate distances, our analysis yields curvature independent convergence rates for both the nonconvex and strongly convex cases.},
  archiveprefix = {arXiv},
  eprint        = {1811.04194},
  file          = {:- R SPIDER_ a Fast Riemannian Stochastic Optimization Algorithm with Curvature Independent Rate.pdf:PDF},
  keywords      = {math.OC, cs.LG},
  primaryclass  = {math.OC},
}

@Article{Zhang2018e,
  author        = {Jingzhao Zhang and César A. Uribe and Aryan Mokhtari and Ali Jadbabaie},
  title         = {Achieving Acceleration in Distributed Optimization via Direct Discretization of the Heavy-Ball ODE},
  year          = {2018},
  month         = nov,
  abstract      = {We develop a distributed algorithm for convex Empirical Risk Minimization, the problem of minimizing large but finite sum of convex functions over networks. The proposed algorithm is derived from directly discretizing the second-order heavy-ball differential equation and results in an accelerated convergence rate, i.e, faster than distributed gradient descent-based methods for strongly convex objectives that may not be smooth. Notably, we achieve acceleration without resorting to the well-known Nesterov's momentum approach. We provide numerical experiments and contrast the proposed method with recently proposed optimal distributed optimization algorithms.},
  archiveprefix = {arXiv},
  eprint        = {1811.02521},
  file          = {:- Achieving Acceleration in Distributed Optimization Via Direct Discretization of the Heavy Ball ODE.pdf:PDF},
  keywords      = {math.OC},
  primaryclass  = {math.OC},
}

@Article{Zhang2019d,
  author        = {Jingzhao Zhang and Suvrit Sra and Ali Jadbabaie},
  title         = {Acceleration in First Order Quasi-strongly Convex Optimization by ODE Discretization},
  year          = {2019},
  month         = may,
  abstract      = {We study gradient-based optimization methods obtained by direct Runge-Kutta discretization of the ordinary differential equation (ODE) describing the movement of a heavy-ball under constant friction coefficient. When the function is high order smooth and strongly convex, we show that directly simulating the ODE with known numerical integrators achieve acceleration in a nontrivial neighborhood of the optimal solution. In particular, the neighborhood can grow larger as the condition number of the function increases. Furthermore, our results also hold for nonconvex but quasi-strongly convex objectives. We provide numerical experiments that verify the theoretical rates predicted by our results.},
  archiveprefix = {arXiv},
  eprint        = {1905.12436},
  file          = {:- Acceleration in First Order Quasi Strongly Convex Optimization by ODE Discretization.pdf:PDF},
  keywords      = {math.OC},
  primaryclass  = {math.OC},
}

@Article{Zhang2018f,
  author        = {Yiou Zhang and Jingzhao Zhang and Junyi Zhu},
  journal       = {Phys. Rev. Materials 2, 073401 (2018)},
  title         = {Stability of wurtzite semi-polar surfaces: algorithms and practices},
  year          = {2018},
  month         = jun,
  abstract      = {A complete knowledge of absolute surface energies with any arbitrary crystal orientation is important for the improvements of semiconductor devices because it determines the equilibrium and nonequilibrium crystal shapes of thin films and nanostructures. It is also crucial in the control of thin film crystal growth and surface effect studies in broad research fields. However, obtaining accurate absolute formation energies is still a huge challenge for the semi-polar surfaces of compound semiconductors. It mainly results from the asymmetry nature of crystal structures and the complicated step morphologies and related reconstructions of these surface configurations. Here we propose a general approach to calculate the absolute formation energies of wurtzite semi-polar surfaces by first-principles calculations, taking GaN as an example. We mainly focused on two commonly seen sets of semi-polar surfaces: a-family (11-2X) and m-family (10-1X). For all the semi-polar surfaces that we have calculated in this paper, the self-consistent accuracy is within 1.5 meV/{\AA}^2. Our work fills the last technical gap to fully investigate and understand the shape and morphology of compound semiconductors.},
  archiveprefix = {arXiv},
  doi           = {10.1103/PhysRevMaterials.2.073401},
  eprint        = {1806.00268},
  file          = {:- Stability of Wurtzite Semi Polar Surfaces_ Algorithms and Practices.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci, physics.comp-ph},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Zhang2018g,
  author        = {Jingzhao Zhang and Aryan Mokhtari and Suvrit Sra and Ali Jadbabaie},
  title         = {Direct Runge-Kutta Discretization Achieves Acceleration},
  year          = {2018},
  month         = may,
  abstract      = {We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-$(s+2)$ differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of $\mathcal{O}({N^{-2\frac{s}{s+1}}})$, where $s$ is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than $\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results.},
  archiveprefix = {arXiv},
  eprint        = {1805.00521},
  file          = {:- Direct Runge Kutta Discretization Achieves Acceleration.pdf:PDF},
  keywords      = {math.OC, cs.LG, stat.ML},
  primaryclass  = {math.OC},
}

@Article{Zhang2019e,
  author        = {Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
  title         = {Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  year          = {2019},
  month         = may,
  abstract      = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, \emph{gradient clipping} and \emph{normalized gradient}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
  archiveprefix = {arXiv},
  eprint        = {1905.11881},
  file          = {:- Why Gradient Clipping Accelerates Training_ a Theoretical Justification for Adaptivity.pdf:PDF},
  keywords      = {math.OC, cs.LG},
  primaryclass  = {math.OC},
}

@Article{Zhang2019f,
  author        = {Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J Reddi and Sanjiv Kumar and Suvrit Sra},
  title         = {Why are Adaptive Methods Good for Attention Models?},
  year          = {2019},
  month         = dec,
  abstract      = {While stochastic gradient descent (SGD) is still the \emph{de facto} algorithm in deep learning, adaptive methods like Clipped SGD/Adam have been observed to outperform SGD across important tasks, such as attention models. The settings under which SGD performs poorly in comparison to adaptive methods are not well understood yet. In this paper, we provide empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients is one cause of SGD's poor performance. We provide the first tight upper and lower convergence bounds for adaptive gradient methods under heavy-tailed noise. Further, we demonstrate how gradient clipping plays a key role in addressing heavy-tailed gradient noise. Subsequently, we show how clipping can be applied in practice by developing an \emph{adaptive} coordinate-wise clipping algorithm (ACClip) and demonstrate its superior performance on BERT pretraining and finetuning tasks.},
  archiveprefix = {arXiv},
  eprint        = {1912.03194},
  file          = {:- Why Are Adaptive Methods Good for Attention Models_.pdf:PDF},
  keywords      = {math.OC, cs.LG},
  primaryclass  = {math.OC},
}

@Article{Zhang2020c,
  author        = {Jingzhao Zhang and Hongzhou Lin and Subhro Das and Suvrit Sra and Ali Jadbabaie},
  title         = {Stochastic Optimization with Non-stationary Noise},
  year          = {2020},
  month         = jun,
  abstract      = {We investigate stochastic optimization problems under relaxed assumptions on the distribution of noise that are motivated by empirical observations in neural network training. Standard results on optimal convergence rates for stochastic optimization assume either there exists a uniform bound on the moments of the gradient noise, or that the noise decays as the algorithm progresses. These assumptions do not match the empirical behavior of optimization algorithms used in neural network training where the noise level in stochastic gradients could even increase with time. We address this behavior by studying convergence rates of stochastic gradient methods subject to changing second moment (or variance) of the stochastic oracle as the iterations progress. When the variation in the noise is known, we show that it is always beneficial to adapt the step-size and exploit the noise variability. When the noise statistics are unknown, we obtain similar improvements by developing an online estimator of the noise level, thereby recovering close variants of RMSProp. Consequently, our results reveal an important scenario where adaptive stepsize methods outperform SGD.},
  archiveprefix = {arXiv},
  eprint        = {2006.04429},
  file          = {:- Stochastic Optimization with Non Stationary Noise.pdf:PDF},
  keywords      = {math.OC, cs.LG},
  primaryclass  = {math.OC},
}

@Article{Zhang2018h,
  author        = {Xiaodong Zhang and Jingzhao Zhang and Shengbai Zhang and Junyi Zhu},
  journal       = {Phys. Rev. B 99, 134435 (2019)},
  title         = {Long Range Magnetic order stabilized by acceptors},
  year          = {2018},
  month         = mar,
  abstract      = {Tuning magnetic order in magnetic semiconductors is a long sought goal. A proper concentration of acceptors can dramatically suppress local magnetic order in favor of the long one. Using Mn and an acceptor codoped LiZnAs as an example, we demonstrate, by first-principles calculation, the emergence of a long-range magnetic order. This intriguing phenomenon can be understood from an interplay between an acceptor-free magnetism and a band coupling magnetism. Our observation thus lays the ground for a precise control of the magnetic order in future spintronic devices.},
  archiveprefix = {arXiv},
  doi           = {10.1103/PhysRevB.99.134435},
  eprint        = {1803.01179},
  file          = {:- Long Range Magnetic Order Stabilized by Acceptors.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Zhang2018i,
  author        = {Yi Zhang and Xuewen Liu and Jingzhao Qi and Hongsheng Zhang},
  journal       = {JCAP08(2018)027},
  title         = {Cosmological Model Independent Time Delay Method},
  year          = {2018},
  month         = may,
  abstract      = {We propose a Cosmological Model Independent Time Delay (CMITD) method where the Lorentz invariance violation (LIV) variable $K(z)$ is constructed by observational data instead of cosmological model. The simulated time delay data show the CMITD method could present the validity of LIV test. And, the errors in the propagating process is critical for the existence and magnitude of LIV.},
  archiveprefix = {arXiv},
  doi           = {10.1088/1475-7516/2018/08/027},
  eprint        = {1805.02586},
  file          = {:- Cosmological Model Independent Time Delay Method.pdf:PDF},
  keywords      = {astro-ph.CO, gr-qc},
  primaryclass  = {astro-ph.CO},
}

@Article{Hu2021,
  author        = {Jingzhao Hu and Xiaoqi Zhang and Qiaomei Jia and Chen Wang and Qirong Bu and Jun Feng},
  title         = {Lookup subnet based Spatial Graph Convolutional neural Network},
  year          = {2021},
  month         = feb,
  abstract      = {Convolutional Neural Networks(CNNs) has achieved remarkable performance breakthrough in Euclidean structure data. Recently, aggregation-transformation based Graph Neural networks(GNNs) gradually produce a powerful performance on non-Euclidean data. In this paper, we propose a cross-correlation based graph convolution method allowing to naturally generalize CNNs to non-Euclidean domains and inherit the excellent natures of CNNs, such as local filters, parameter sharing, flexible receptive field, etc. Meanwhile, it leverages dynamically generated convolution kernel and cross-correlation operators to address the shortcomings of prior methods based on aggregation-transformation or their approximations. Our method has achieved or matched popular state-of-the-art results across three established graph benchmarks: the Cora, Citeseer, and Pubmed citation network datasets.},
  archiveprefix = {arXiv},
  eprint        = {2102.02588},
  file          = {:- Lookup Subnet Based Spatial Graph Convolutional Neural Network.pdf:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Zhang2015,
  author        = {Jingzhao Zhang and Yiou Zhang and Kinfai Tse and Bei Deng and Hu Xu and Junyi Zhu},
  journal       = {J. Appl. Phys. 119, 205302 (2016)},
  title         = {Surface energy calculations from Zinc blende (111)/(-1-1-1) to Wurtzite (0001)/(000-1):a study of ZnO and GaN},
  year          = {2015},
  month         = oct,
  abstract      = {The accurate absolute surface energies of (0001)/(000-1) surfaces of wurtzite structures are crucial in determining the thin film growth mode of important energy materials. However, the surface energies still remain to be solved due to the intrinsic difficulty of calculating dangling bond energy of asymmetrically bonded surface atoms. In this study, we used a pseudo-hydrogen passivation method to estimate the dangling bond energy and calculate the polar surfaces of ZnO and GaN. The calculations were based on the pseudo chemical potentials obtained from a set of tetrahedral clusters or simple pseudo-molecules, using density functional theory approaches. And the surface energies of (0001)/(000-1) surfaces of wurtzite ZnO and GaN we obtained showed relatively high self-consistencies. A wedge structure calculation with a new bottom surface passivation scheme of group I and group VII elements was also proposed and performed to show converged absolute surface energy of wurtzite ZnO polar surfaces, and the result were also compared with the above method. These calculations and comparisons may provide important insights to crystal growths of the above materials, thereby leading to significant performance enhancements of semiconductor devices.},
  archiveprefix = {arXiv},
  doi           = {10.1063/1.4952395},
  eprint        = {1510.08961},
  file          = {:- Surface Energy Calculations from Zinc Blende (111)_( 1 1 1) to Wurtzite (0001)_(000 1)_a Study of ZnO and GaN.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci, physics.comp-ph},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Zhang2017,
  author        = {Jingzhao Zhang and Yiou Zhang and Kinfai Tse and Junyi Zhu},
  journal       = {Phys. Rev. Materials 2, 013403 (2018)},
  title         = {Hydrogen Surfactant Assisted Coherent Growth of GaN on ZnO Substrate},
  year          = {2017},
  month         = oct,
  abstract      = {Heterostructures of wurtzite based devices have attracted great research interests since the tremendous success of GaN in light emitting diodes (LED) industry. Among the possible heterostructure material candidates, high quality GaN thin films on inexpensive and lattice matched ZnO substrate are both commercially and technologically desirable. However, the energy of ZnO polar surfaces is much lower than that of GaN polar surfaces. Therefore, the intrinsic wetting condition forbids such heterostructures. As a result, poor crystal quality and 3D growth mode were obtained. To dramatically change the growth mode of the heterostructure, we propose to use hydrogen as a surfactant, confirmed by our first principles calculations. Stable H involved surface configurations and interfaces are investigated, with the help of newly developed algorithms. By applying the experimental Gibbs free energy of H$_2$, we also predict the temperature and chemical potential of H, which is critical in experimental realizations of our strategy. This novel approach will for the first time make the growth of high quality GaN thin films on ZnO substrates possible. We believe that our new strategy may reduce the manufactory cost and improve the crystal quality and the efficiency of GaN based devices.},
  archiveprefix = {arXiv},
  doi           = {10.1103/PhysRevMaterials.2.013403},
  eprint        = {1710.03472},
  file          = {:- Hydrogen Surfactant Assisted Coherent Growth of GaN on ZnO Substrate.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci, physics.app-ph, physics.comp-ph},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Zhang2018j,
  author        = {Jingzhao Zhang and Wenjing Zhao and Junyi Zhu},
  journal       = {Nanoscale (2018)},
  title         = {Missing links towards understanding equilibrium shapes of hexagonal boron nitride: algorithm, hydrogen passivation, and temperature effects},
  year          = {2018},
  month         = jun,
  abstract      = {There is a large discrepancy between the experimental observations and the theoretical predictions in the morphology of hexagonal boron nitride (h-BN) nanosheets. Theoretically-predicted hexagons terminated by armchair edges are not observed in experiments; and experimentally-observed triangles terminated by zigzag edges are found theoretically unstable. There are two key issues in theoretical investigations, namely, an efficient and accurate algorithm of absolute formation energy of h-BN edges, and a good understanding of the role of hydrogen passivation during h-BN growth. Here, we first proposed an efficient algorithm to calculate asymmetric edges with a self-consistent accuracy of about 0.0014 eV/{\AA}. This method can also potentially serve as a standard approach for other two-dimensional (2D) compound materials. Then, by using this method, we discovered that only when edges are passivated by hydrogen atoms and temperature effects are taken into account can experimental morphology be explained. We further employed Wulff construction to obtain the equilibrium shapes of H-passivated h-BN nanosheets under its typical growth conditions at T = 1300 K and p = 1 bar, and found out that the equilibrium shapes are sensitive to hydrogen passivation and the growth conditions. Our results resolved long-standing discrepancies between experimental observations and theoretical analysis, explaining the thermodynamic driving force of the triangular, truncated triangular, and hexagonal shapes, and revealing the key role of hydrogen in h-BN growth. These discoveries and the advancement in algorithm may open the gateway towards the realization of 2D electronic and spintronic devices based on h-BN.},
  archiveprefix = {arXiv},
  doi           = {10.1039/C8NR04732D},
  eprint        = {1806.03799},
  file          = {:- Missing Links Towards Understanding Equilibrium Shapes of Hexagonal Boron Nitride_ Algorithm, Hydrogen Passivation, and Temperature Effects.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci, physics.comp-ph},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Zhang2020d,
  author        = {Jingzhao Zhang and Aditya Menon and Andreas Veit and Srinadh Bhojanapalli and Sanjiv Kumar and Suvrit Sra},
  title         = {Coping with Label Shift via Distributionally Robust Optimisation},
  year          = {2020},
  month         = oct,
  abstract      = {The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an \emph{unlabelled} test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in \emph{multiple} test environments. Can one instead learn a \emph{single} classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective. %, and establish its convergence. Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present.},
  archiveprefix = {arXiv},
  eprint        = {2010.12230},
  file          = {:- Coping with Label Shift Via Distributionally Robust Optimisation.pdf:PDF},
  keywords      = {cs.LG, cs.CV, math.OC},
  primaryclass  = {cs.LG},
}

@Article{Zhang2015a,
  author        = {Yiou Zhang and Jingzhao Zhang and Kinfai Tse and Chunkai Chan and Bei Deng and Junyi Zhu},
  journal       = {Scientific Reports 6, Article number: 20055 (2016)},
  title         = {Pseudo-Hydrogen Passivation_A Novel Way to Calculate Absolute Surface Energy of Zinc Blende (111) Surface},
  year          = {2015},
  month         = jun,
  abstract      = {Determining accurate absolute surface energies for polar surfaces of semiconductors has been a great challenge in decades. Here, we propose pseudo-hydrogen passivation to calculate them, using density functional theory approaches. By calculating the energy contribution from pseudo-hydrogen using either a pseudo molecule method or a tetrahedral cluster method, we obtained (111) surfaces energies of Si, GaP, and ZnS with high self-consistency. This method quantitatively confirms that surface energy is determined by the number and the energy of dangling bonds of surface atoms. Our findings may greatly enhance the basic understandings of different surfaces and lead to novel strategies in the crystal growth.},
  archiveprefix = {arXiv},
  doi           = {10.1038/srep20055},
  eprint        = {1506.05570},
  file          = {:- Pseudo Hydrogen Passivation_A Novel Way to Calculate Absolute Surface Energy of Zinc Blende (111) Surface.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Tse2017,
  author        = {Kinfai Tse and Manhoi Wong and Yiou Zhang and Jingzhao Zhang and Michael Scarpulla and Junyi Zhu},
  title         = {Defect Properties of Na and K in Cu2ZnSnS4 from Hybrid Functional Calculation},
  year          = {2017},
  month         = jul,
  abstract      = {In-growth or post-deposition treatment of $Cu_{2}ZnSnS_{4}$ (CZTS) absorber layer had led to improved photovoltaic efficiency, however, the underlying physical mechanism of such improvements are less studied. In this study, the thermodynamics of Na and K related defects in CZTS are investigated from first principle approach using hybrid functional, with chemical potential of Na and K established from various phases of their polysulphides. Both Na and K predominantly substitute on Cu sites similar to their behavior in $Cu(In,Ga)Se_{2}$, in contrast to previous results using the generalized gradient approximation (GGA). All substitutional and interstitial defects are shown to be either shallow levels or highly energetically unfavorable. Defect complexing between Na and abundant intrinsic defects did not show possibility of significant incorporation enhancement or introducing deep n-type levels. The possible benefit of Na incorporation on enhancing photovoltaic efficiency is discussed. The negligible defect solubility of K in CZTS also suggests possible surfactant candidate.},
  archiveprefix = {arXiv},
  doi           = {10.1063/1.5046734},
  eprint        = {1707.01121},
  file          = {:- Defect Properties of Na and K in Cu2ZnSnS4 from Hybrid Functional Calculation.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Gong2019,
  author        = {Weiyi Gong and Ching-Him Leung and Chuen-Keung Sin and Jingzhao Zhang and Xiaodong Zhang and Bin Xi and Junyi Zhu},
  title         = {Intrinsic long range antiferromagnetic coupling in dilutely V doped CuInTe$_2$},
  year          = {2019},
  month         = feb,
  abstract      = {Despite the various magnetic orders mediated by superexchange mechanism, the existence of a long range antiferromagnetic (AFM) coupling is unknown. Based on DFT calculations, we discovered an intrinsic long range AFM coupling in V doped CuInTe$_2$. The AFM coupling is mainly due to the $p-d$ coupling and electron redistribution along the interacting chains. The relatively small energy difference between $p$ and $d$ orbitals and the small energy difference between d orbitals of the dopants and that of stepping stone sites can enhance the stability of this AFM configuration. A multi-bands Hubbard model was proposed to provide fundamental understanding to the long range AFM coupling in chalcopyrite diluted magnetic semiconductors(DMS).},
  archiveprefix = {arXiv},
  doi           = {10.1088/0256-307X/37/2/027501},
  eprint        = {1902.05281},
  file          = {:- Intrinsic Long Range Antiferromagnetic Coupling in Dilutely V Doped CuInTe$_2$.pdf:PDF},
  keywords      = {cond-mat.mtrl-sci, cond-mat.str-el},
  primaryclass  = {cond-mat.mtrl-sci},
}

@Article{Mi2018,
  author        = {Lu Mi and Macheng Shen and Jingzhao Zhang},
  title         = {A Probe Towards Understanding GAN and VAE Models},
  year          = {2018},
  month         = dec,
  abstract      = {This project report compares some known GAN and VAE models proposed prior to 2017. There has been significant progress after we finished this report. We upload this report as an introduction to generative models and provide some personal interpretations supported by empirical evidence. Both generative adversarial network models and variational autoencoders have been widely used to approximate probability distributions of data sets. Although they both use parametrized distributions to approximate the underlying data distribution, whose exact inference is intractable, their behaviors are very different. We summarize our experiment results that compare these two categories of models in terms of fidelity and mode collapse. We provide a hypothesis to explain their different behaviors and propose a new model based on this hypothesis. We further tested our proposed model on MNIST dataset and CelebA dataset.},
  archiveprefix = {arXiv},
  eprint        = {1812.05676},
  file          = {:Mi2018 - A Probe Towards Understanding GAN and VAE Models.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Misc{Yoccoz,
  author       = {Jean-Christophe Yoccoz},
  howpublished = {\url{https://www.college-de-france.fr/media/jean-christophe-yoccoz/UPL54030_birkhoff.pdf}},
  title        = {An example of non convergence of Birkhoff sums},
}

@Book{Katok1997,
  author    = {Katok, Anatole and Hasselblatt, Boris},
  publisher = {Cambridge university press},
  title     = {Introduction to the modern theory of dynamical systems},
  year      = {1997},
  number    = {54},
}

@Misc{Rudin1991f,
  author    = {Rudin, Walter},
  title     = {Functional analysis. International series in pure and applied mathematics},
  year      = {1991},
  publisher = {McGraw-Hill, Inc., New York},
}

@Article{Cohen2021,
  author        = {Jeremy M. Cohen and Simran Kaur and Yuanzhi Li and J. Zico Kolter and Ameet Talwalkar},
  title         = {Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  year          = {2021},
  month         = feb,
  abstract      = {We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the numerical value $2 / \text{(step size)}$, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability. Code is available at https://github.com/locuslab/edge-of-stability.},
  archiveprefix = {arXiv},
  eprint        = {2103.00065},
  file          = {:Cohen2021 - Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Henderson2017,
  author        = {Peter Henderson and Riashat Islam and Philip Bachman and Joelle Pineau and Doina Precup and David Meger},
  title         = {Deep Reinforcement Learning that Matters},
  year          = {2017},
  month         = sep,
  abstract      = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  archiveprefix = {arXiv},
  eprint        = {1709.06560},
  file          = {:Henderson2017 - Deep Reinforcement Learning That Matters.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Madhyastha2019,
  author        = {Pranava Madhyastha and Rishabh Jain},
  title         = {On Model Stability as a Function of Random Seed},
  year          = {2019},
  month         = sep,
  abstract      = {In this paper, we focus on quantifying model stability as a function of random seed by investigating the effects of the induced randomness on model performance and the robustness of the model in general. We specifically perform a controlled study on the effect of random seeds on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations. Our analysis suggests that random seeds can adversely affect the consistency of models resulting in counterfactual interpretations. We propose a technique called Aggressive Stochastic Weight Averaging (ASWA)and an extension called Norm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the robustness of the original model, on average reducing the standard deviation of the model's performance by 72%.},
  archiveprefix = {arXiv},
  eprint        = {1909.10447},
  file          = {:Madhyastha2019 - On Model Stability As a Function of Random Seed.pdf:PDF},
  keywords      = {cs.LG, cs.CL, cs.NE, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Bhojanapalli2021,
  author        = {Srinadh Bhojanapalli and Kimberly Wilber and Andreas Veit and Ankit Singh Rawat and Seungyeon Kim and Aditya Menon and Sanjiv Kumar},
  title         = {On the Reproducibility of Neural Network Predictions},
  year          = {2021},
  month         = feb,
  abstract      = {Standard training techniques for neural networks involve multiple sources of randomness, e.g., initialization, mini-batch ordering and in some cases data augmentation. Given that neural networks are heavily over-parameterized in practice, such randomness can cause {\em churn} -- for the same input, disagreements between predictions of the two models independently trained by the same algorithm, contributing to the `reproducibility challenges' in modern machine learning. In this paper, we study this problem of churn, identify factors that cause it, and propose two simple means of mitigating it. We first demonstrate that churn is indeed an issue, even for standard image classification tasks (CIFAR and ImageNet), and study the role of the different sources of training randomness that cause churn. By analyzing the relationship between churn and prediction confidences, we pursue an approach with two components for churn reduction. First, we propose using \emph{minimum entropy regularizers} to increase prediction confidences. Second, \changes{we present a novel variant of co-distillation approach~\citep{anil2018large} to increase model agreement and reduce churn}. We present empirical results showing the effectiveness of both techniques in reducing churn while improving the accuracy of the underlying model.},
  archiveprefix = {arXiv},
  eprint        = {2102.03349},
  file          = {:Bhojanapalli2021 - On the Reproducibility of Neural Network Predictions.pdf:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}
@inproceedings{cheung2019vortices,
  title={Vortices instead of equilibria in minmax optimization: Chaos and butterfly effects of online learning in zero-sum games},
  author={Cheung, Yun Kuen and Piliouras, Georgios},
  booktitle={Conference on Learning Theory},
  pages={807--834},
  year={2019},
  organization={PMLR}
}
@InBook{2019,
  pages = {807--834},
  title = {title={Vortices instead of equilibria in minmax optimization: Chaos and butterfly effects of online learning in zero-sum games}, author={Cheung, Yun Kuen and Piliouras}, booktitle = {Georgios}, booktitle={Conference on Learning Theory}},
  year  = {2019},
  date  = {2019},
}
@article{papadimitriou2019game,
  title={Game dynamics as the meaning of a game},
  author={Papadimitriou, Christos and Piliouras, Georgios},
  journal={ACM SIGecom Exchanges},
  volume={16},
  number={2},
  pages={53--63},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@InBook{,
  booktitle = {@article{papadimitriou2019game, title={Game dynamics as the meaning of a game}, author={Papadimitriou, Christos and Piliouras, Georgios}, journal={ACM SIGecom Exchanges}, volume={16}, number={2}}, date = {2019}, year = {2019}, address = {New York, NY, USA}},
}
@article{letcher2020impossibility,
  title={On the impossibility of global convergence in multi-loss optimization},
  author={Letcher, Alistair},
  journal={arXiv preprint arXiv:2005.12649},
  year={2020}
}@article{hsieh2020limits,
  title={The limits of min-max optimization algorithms: Convergence to spurious non-critical sets},
  author={Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
  journal={arXiv preprint arXiv:2006.09065},
  year={2020}
}@article{hsieh2020limits,
  title={The limits of min-max optimization algorithms: Convergence to spurious non-critical sets},
  author={Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
  journal={arXiv preprint arXiv:2006.09065},
  year={2020}
}@article{flokas2020no,
  title={No-regret learning and mixed Nash equilibria: They do not mix},
  author={Flokas, Lampros and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Lianeas, Thanasis and Mertikopoulos, Panayotis and Piliouras, Georgios},
  journal={arXiv preprint arXiv:2010.09514},
  year={2020}
}@inproceedings{cheng2020stochastic,
  title={Stochastic gradient and langevin processes},
  author={Cheng, Xiang and Yin, Dong and Bartlett, Peter and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={1810--1819},
  year={2020},
  organization={PMLR}
}@article{li2020reconciling,
  title={Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in sgd},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021},
  organization={PMLR}
}@article{zhang2020stochastic,
  title={Stochastic Optimization with Non-stationary Noise},
  author={Zhang, Jingzhao and Lin, Hongzhou and Das, Subhro and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:2006.04429},
  year={2020}
}@article{lobacheva2021periodic,
  title={On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay},
  author={Lobacheva, Ekaterina and Kodryan, Maxim and Chirkova, Nadezhda and Malinin, Andrey and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:2106.15739},
  year={2021}
}@article{kunin2021rethinking,
  title={Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion},
  author={Kunin, Daniel and Sagastuy-Brena, Javier and Gillespie, Lauren and Margalit, Eshed and Tanaka, Hidenori and Ganguli, Surya and Yamins, Daniel LK},
  journal={arXiv preprint arXiv:2107.09133},
  year={2021}
}@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}@misc{wightman2021resnet,
      title={ResNet strikes back: An improved training procedure in timm}, 
      author={Ross Wightman and Hugo Touvron and Hervé Jégou},
      year={2021},
      eprint={2110.00476},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}@inproceedings{spielman2005smoothed,
  title={The smoothed analysis of algorithms},
  author={Spielman, Daniel A},
  booktitle={International Symposium on Fundamentals of Computation Theory},
  pages={17--18},
  year={2005},
  organization={Springer}
}@article{spielman2009smoothed,
  title={Smoothed analysis: an attempt to explain the behavior of algorithms in practice},
  author={Spielman, Daniel A and Teng, Shang-Hua},
  journal={Communications of the ACM},
  volume={52},
  number={10},
  pages={76--84},
  year={2009},
  publisher={ACM New York, NY, USA}
}@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={8279--8288},
  year={2018}
}
@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}
@inproceedings{karmarkar1984new,
  title={A new polynomial-time algorithm for linear programming},
  author={Karmarkar, Narendra},
  booktitle={Proceedings of the sixteenth annual ACM symposium on Theory of computing},
  pages={302--311},
  year={1984}
}@article{khachiyan1980polynomial,
  title={Polynomial algorithms in linear programming},
  author={Khachiyan, Leonid G},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={20},
  number={1},
  pages={53--72},
  year={1980},
  publisher={Elsevier}
}@incollection{dantzig1990origins,
  title={Origins of the simplex method},
  author={Dantzig, George B},
  booktitle={A history of scientific computing},
  pages={141--151},
  year={1990}
}@article{klee1972good,
  title={How good is the simplex algorithm},
  author={Klee, Victor and Minty, George J},
  journal={Inequalities},
  volume={3},
  number={3},
  pages={159--175},
  year={1972},
  publisher={New York}
}@article{spielman2004smoothed,
  title={Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time},
  author={Spielman, Daniel A and Teng, Shang-Hua},
  journal={Journal of the ACM (JACM)},
  volume={51},
  number={3},
  pages={385--463},
  year={2004},
  publisher={ACM New York, NY, USA}
}
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}
@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}
@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1805.09545},
  year={2018}
}


@Comment{jabref-meta: databaseType:bibtex;}



