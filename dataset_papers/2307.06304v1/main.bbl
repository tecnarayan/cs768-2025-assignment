\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.
\newblock Exploring the limits of large scale pre-training.
\newblock \emph{arXiv preprint arXiv:2110.02095}, 2021.

\bibitem[Adebayo et~al.(2023)Adebayo, Hall, Yu, and Chern]{adebayoquantifying}
Julius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern.
\newblock Quantifying and mitigating the impact of label errors on model
  disparity metrics.
\newblock In \emph{{ICLR}}, 2023.

\bibitem[Akbari et~al.(2021)Akbari, Yuan, Qian, Chuang, Chang, Cui, and
  Gong]{akbari2021vatt}
Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin
  Cui, and Boqing Gong.
\newblock Vatt: Transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 24206--24221, 2021.

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lu{\v{c}}i{\'c}, and
  Schmid]{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock {ViViT}: A video vision transformer.
\newblock In \emph{{CVPR}}, 2021.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{barbu2019objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
  Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock {ObjectNet}: {A} large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock In \emph{{NeurIPS}}, pages 9448--9458, 2019.

\bibitem[Beyer et~al.(2023)Beyer, Izmailov, Kolesnikov, Caron, Kornblith, Zhai,
  Minderer, Tschannen, Alabdulmohsin, and Pavetic]{beyer2022flexivit}
Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon
  Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim
  Alabdulmohsin, and Filip Pavetic.
\newblock Flexivit: One model for all patch sizes.
\newblock In \emph{{CVPR}}, 2023.

\bibitem[Bolya et~al.(2022)Bolya, Fu, Dai, Zhang, Feichtenhofer, and
  Hoffman]{bolya2022tokenmerge}
Daniel Bolya, Cheng{-}Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph
  Feichtenhofer, and Judy Hoffman.
\newblock Token merging: Your vit but faster.
\newblock \emph{CoRR}, abs/2210.09461, 2022.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Caron et~al.(2022)Caron, Houlsby, and Schmid]{caron2022location}
Mathilde Caron, Neil Houlsby, and Cordelia Schmid.
\newblock Location-aware self-supervised transformers for semantic
  segmentation.
\newblock \emph{arXiv preprint arXiv:2212.02400}, 2022.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Du, Yang, Beyer, Zhai, Lin, Chen,
  Li, Song, Wang, and Zhou]{chen2021uvit}
Wuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung{-}Yi Lin,
  Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, and Denny Zhou.
\newblock A simple single-scale vision transformer for object detection and
  instance segmentation.
\newblock In Shai Avidan, Gabriel~J. Brostow, Moustapha Ciss{\'{e}},
  Giovanni~Maria Farinella, and Tal Hassner, editors, \emph{Computer Vision -
  {ECCV} 2022}, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Huang, Du, Song, Wang, and
  Zhou]{chen2022autoscaling}
Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang, and Denny
  Zhou.
\newblock Auto-scaling vision transformers without training.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR}}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2022{\natexlab{c}})Chen, Wang, Changpinyo, Piergiovanni,
  Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, Kolesnikov, Puigcerver,
  Ding, Rong, Akbari, Mishra, Xue, Thapliyal, Bradbury, Kuo, Seyedhosseini,
  Jia, Ayan, Riquelme, Steiner, Angelova, Zhai, Houlsby, and
  Soricut]{chen2022pali}
Xi~Chen, Xiao Wang, Soravit Changpinyo, A.~J. Piergiovanni, Piotr Padlewski,
  Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
  Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,
  Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo,
  Mojtaba Seyedhosseini, Chao Jia, Burcu~Karagol Ayan, Carlos Riquelme, Andreas
  Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.
\newblock {PaLI}: {A} jointly-scaled multilingual language-image model.
\newblock \emph{arXiv preprint arXiv:2209.06794}, 2022{\natexlab{c}}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'{e}}]{dao2022flash}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'{e}}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL
  \url{http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html}.

\bibitem[Dehghani et~al.(2021)Dehghani, Arnab, Beyer, Vaswani, and
  Tay]{dehghani2021efficiency}
Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi~Tay.
\newblock The efficiency misnomer.
\newblock \emph{arXiv preprint arXiv:2110.12894}, 2021.

\bibitem[Dehghani et~al.(2022)Dehghani, Gritsenko, Arnab, Minderer, and
  Tay]{dehghani2021scenic}
Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and
  Yi~Tay.
\newblock Scenic: A jax library for computer vision research and beyond.
\newblock In \emph{{CVPR}}, 2022.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, Jenatton, Beyer, Tschannen,
  Arnab, Wang, Riquelme, Minderer, Puigcerver, Evci, Kumar, van Steenkiste,
  Elsayed, Mahendran, Yu, Oliver, Huot, Bastings, Collier, Gritsenko, Birodkar,
  Vasconcelos, Tay, Mensink, Kolesnikov, Pavetić, Tran, Kipf, Lučić, Zhai,
  Keysers, Harmsen, and Houlsby]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag
  Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku
  Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin~F. Elsayed, Aravindh
  Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings,
  Mark~Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina
  Vasconcelos, Yi~Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić,
  Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers,
  Jeremiah Harmsen, and Neil Houlsby.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{{CVPR}}, pages 248--255, 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Fan et~al.(2021)Fan, Xiong, Mangalam, Li, Yan, Malik, and
  Feichtenhofer]{fan2021mvit}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In \emph{2021 {IEEE/CVF} International Conference on Computer Vision,
  {ICCV}}, 2021.

\bibitem[Geirhos et~al.(2021)Geirhos, Narayanappa, Mitzkus, Thieringer, Bethge,
  Wichmann, and Brendel]{geirhos2021partial}
Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer,
  Matthias Bethge, Felix~A Wichmann, and Wieland Brendel.
\newblock Partial success in closing the gap between human and machine vision.
\newblock In \emph{{NeurIPS}}, pages 23885--23899, 2021.

\bibitem[Gupta et~al.(2019)Gupta, Doll{\'{a}}r, and Girshick]{gupta2019lvis}
Agrim Gupta, Piotr Doll{\'{a}}r, and Ross~B. Girshick.
\newblock {LVIS:} {A} dataset for large vocabulary instance segmentation.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR}}, 2019.

\bibitem[Hamidi et~al.(2018)Hamidi, Scheuerman, and Branham]{hamidi2018gender}
Foad Hamidi, Morgan~Klaus Scheuerman, and Stacy~M Branham.
\newblock Gender recognition or gender reductionism? the social implications of
  embedded gender recognition systems.
\newblock In \emph{Proceedings of the 2018 chi conference on human factors in
  computing systems}, pages 1--13, 2018.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'{a}}r, and
  Girshick]{he2022mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'{a}}r, and
  Ross~B. Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{{CVPR}}, pages 15979--15988. {IEEE}, 2022.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand
  Rondepierre, Andreas Steiner, and Marc van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Zhao, Basart, Steinhardt, and
  Song]{hendrycks2021imagenet_a}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{{CVPR}}, pages 15262--15271, 2021.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{kay2017kinetics}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
  Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[Keyes(2018)]{keyes2018misgendering}
Os~Keyes.
\newblock The misgendering machines: Trans/hci implications of automatic gender
  recognition.
\newblock \emph{Proceedings of the ACM on human-computer interaction},
  2\penalty0 (CSCW):\penalty0 1--22, 2018.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock {Big Transfer ({BiT}): General visual representation learning}.
\newblock In \emph{{ECCV}}, pages 491--507, 2020.

\bibitem[Krell et~al.(2021)Krell, Kosec, Perez, and
  Fitzgibbon]{krell2021efficient}
Mario~Michael Krell, Matej Kosec, Sergio~P Perez, and Andrew~William
  Fitzgibbon.
\newblock Efficient sequence packing without cross-contamination: Accelerating
  large language models without impacting performance, 2021.

\bibitem[Kudo and Richardson(2018)]{kudo-richardson-2018-sentencepiece}
Taku Kudo and John Richardson.
\newblock {S}entence{P}iece: A simple and language independent subword
  tokenizer and detokenizer for neural text processing.
\newblock In \emph{EMNLP}, pages 66--71, November 2018.

\bibitem[Kärkkäinen and Joo(2019)]{fairface}
Kimmo Kärkkäinen and Jungseock Joo.
\newblock Fairface: Face attribute dataset for balanced race, gender, and age,
  2019.

\bibitem[Lee et~al.(2022)Lee, Joshi, Turc, Hu, Liu, Eisenschlos, Khandelwal,
  Shaw, Chang, and Toutanova]{lee2022pix2struct}
Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian
  Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina
  Toutanova.
\newblock Pix2struct: Screenshot parsing as pretraining for visual language
  understanding, 2022.

\bibitem[Li et~al.(2022)Li, Wu, Fan, Mangalam, Xiong, Malik, and
  Feichtenhofer]{li2022mvitv2}
Yanghao Li, Chao{-}Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo~Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Mvitv2: Improved multiscale vision transformers for classification
  and detection.
\newblock In \emph{{CVPR}}, 2022.

\bibitem[Li et~al.(2023)Li, Fan, Hu, Feichtenhofer, and He]{li2023scaling}
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
\newblock Scaling language-image pre-training via masking, 2023.

\bibitem[Liu et~al.(2022)Liu, Hu, Lin, Yao, Xie, Wei, Ning, Cao, Zhang, Dong,
  et~al.]{liu2022swinv2}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, et~al.
\newblock {Swin Transformer V2: Scaling Up Capacity and Resolution}.
\newblock \emph{{CVPR}}, 2022.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{{ICCV}}, 2015.

\bibitem[Minderer et~al.(2022)Minderer, Gritsenko, Stone, Neumann, Weissenborn,
  Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, et~al.]{minderer2022simple}
Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk
  Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa
  Dehghani, Zhuoran Shen, et~al.
\newblock Simple open-vocabulary object detection with vision transformers.
\newblock \emph{arXiv preprint arXiv:2205.06230}, 2022.

\bibitem[Mustafa et~al.(2023)Mustafa, Djolonga, and
  Dehghani]{mustafa2023chunked}
Basil Mustafa, Josip Djolonga, and Mostafa Dehghani.
\newblock On efficient losses for distributed contrastive learning.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Nixon et~al.(2019)Nixon, Dusenberry, Zhang, Jerfel, and
  Tran]{nixon2019measuring}
Jeremy Nixon, Michael~W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin
  Tran.
\newblock Measuring calibration in deep learning.
\newblock In \emph{CVPR Workshops}, 2019.

\bibitem[Piergiovanni et~al.(2022)Piergiovanni, Kuo, and Angelova]{tubevit}
AJ~Piergiovanni, Weicheng Kuo, and Anelia Angelova.
\newblock Rethinking video vits: Sparse video tubes for joint image and video
  learning, 2022.

\bibitem[Rabe and Staats(2021)]{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need {$O(n^2)$} memory.
\newblock \emph{arXiv preprint arXiv:2112.05682}, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{{ICML}}, pages 8748--8763, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5paper}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Raji and Buolamwini(2019)]{raji2019actionable}
Inioluwa~Deborah Raji and Joy Buolamwini.
\newblock Actionable auditing: Investigating the impact of publicly naming
  biased performance results of commercial ai products.
\newblock In \emph{Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 429--435, 2019.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In Nassir Navab, Joachim Hornegger, William M.~Wells III, and
  Alejandro~F. Frangi, editors, \emph{Medical Image Computing and
  Computer-Assisted Intervention - {MICCAI}}, 2015.

\bibitem[Strudel et~al.(2021)Strudel, Garcia, Laptev, and
  Schmid]{strudel2021segmenter}
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In \emph{{ICCV}}, 2021.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich{-}Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Matthew Tancik, Pratul~P. Srinivasan, Ben Mildenhall, Sara Fridovich{-}Keil,
  Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan~T. Barron, and
  Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (6):\penalty0 1--28, 2022.

\bibitem[Touvron et~al.(2019)Touvron, Vedaldi, Douze, and
  J{\'{e}}gou]{touvron2019fixres}
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv{\'{e}} J{\'{e}}gou.
\newblock Fixing the train-test resolution discrepancy.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, 2019.

\bibitem[Touvron et~al.(2022)Touvron, Cord, and J{\'e}gou]{touvron2022deit3}
Hugo Touvron, Matthieu Cord, and Herv{\'e} J{\'e}gou.
\newblock {DeiT III}: Revenge of the {ViT}.
\newblock In \emph{{ECCV}}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wilcoxon(1992)]{wilcoxon1992individual}
Frank Wilcoxon.
\newblock \emph{Individual comparisons by ranking methods}.
\newblock Springer, 1992.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs,
  Lopes, Hajishirzi, Farhadi, Namkoong, and Schmidt]{wortsmann2022robustft}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith,
  Rebecca Roelofs, Raphael~Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi,
  Hongseok Namkoong, and Ludwig Schmidt.
\newblock Robust fine-tuning of zero-shot models.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pages
  7949--7961. {IEEE}, 2022.
\newblock \doi{10.1109/CVPR52688.2022.00780}.
\newblock URL \url{https://doi.org/10.1109/CVPR52688.2022.00780}.

\bibitem[Wu et~al.(2020)Wu, Girshick, He, Feichtenhofer, and
  Kr{\"{a}}henb{\"{u}}hl]{yu2020multigrid}
Chao{-}Yuan Wu, Ross~B. Girshick, Kaiming He, Christoph Feichtenhofer, and
  Philipp Kr{\"{a}}henb{\"{u}}hl.
\newblock A multigrid method for efficiently training video models.
\newblock In \emph{{CVPR}}, pages 150--159. Computer Vision Foundation /
  {IEEE}, 2020.

\bibitem[Yin et~al.(2022)Yin, Vahdat, Alvarez, Mallya, Kautz, and
  Molchanov]{yin2022avit}
Hongxu Yin, Arash Vahdat, Jose~M. Alvarez, Arun Mallya, Jan Kautz, and Pavlo
  Molchanov.
\newblock A-vit: Adaptive tokens for efficient vision transformer.
\newblock In \emph{{CVPR}}, 2022.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{{CVPR}}, pages 12104--12113, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Jiang, Miura, Manning, and
  Langlotz]{zhang2022convirt}
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher~D. Manning, and Curtis~P.
  Langlotz.
\newblock Contrastive learning of medical visual representations from paired
  images and text.
\newblock In \emph{Proceedings of the Machine Learning for Healthcare
  Conference, {MLHC}}, 2022.

\bibitem[Zhou et~al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
  Torralba]{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In \emph{{CVPR}}, 2017.

\end{thebibliography}
