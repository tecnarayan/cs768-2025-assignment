\newcommand{\colt}[1]{Proceedings of the #1 Annual Conference On Learning
  Theory (COLT)}\newcommand{\nips}{Advances in Neural Information Processing
  Systems (NIPS)}\newcommand{\icml}[1]{Proceedings of the #1 International
  Conference on Machine Learning (ICML)}\newcommand{\aistats}[1]{Proceedings of
  the #1 International Conference on Artificial Intelligence and Statistics
  Learning (AISTATS)}\newcommand{\alt}[1]{Proceedings of the #1 Annual
  Conference on Algorithmic Learning Theory
  (ALT)}\newcommand{\icdar}[1]{Proceedings of the #1 International Conference
  on Document Analysis and Recognition
  (ICDAR)}\newcommand{\iclr}[1]{Proceedings of the #1 International Conference
  on Learning Representations (ICLR)}\newcommand{\uai}[1]{Proceedings of the #1
  Conference on Uncertainty in Artificial Intelligence (UAI)}
\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal and Goyal(2012)]{agrawal_analysis_2012}
Shipra Agrawal and Navin Goyal.
\newblock Analysis of {T}hompson sampling for the multi-armed bandit problem.
\newblock In \emph{\colt{25th}}, volume~23, pages 39.1--39.26, 2012.

\bibitem[Agrawal and Goyal(2013)]{agrawal_further_2013}
Shipra Agrawal and Navin Goyal.
\newblock Further optimal regret bounds for {T}hompson sampling.
\newblock In \emph{\aistats{16th}}, pages 99--107, 2013.

\bibitem[Bache and Lichman(2013)]{bache_uci_2013}
Kevin Bache and Moshe Lichman.
\newblock \emph{{UCI} Machine Learning Repository}.
\newblock University of California, Irvine, School of Information and Computer
  Sciences, 2013.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Bishop(2006)]{bishop_section_2006}
Christopher~M Bishop.
\newblock Section 10.1: variational inference.
\newblock In \emph{Pattern Recognition and Machine Learning}. Springer, 2006.
\newblock ISBN 9780387310732.

\bibitem[Buntine and Weigend(1991)]{buntine_bayesian_1991}
Wray~L Buntine and Andreas~S Weigend.
\newblock {B}ayesian back-propagation.
\newblock \emph{Complex systems}, 5\penalty0 (6):\penalty0 603--643, 1991.

\bibitem[Chapelle and Li(2011)]{chapelle_empirical_2011}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of {T}hompson sampling.
\newblock In \emph{\nips{}}, pages 2249--2257, 2011.

\bibitem[Chipman(1996)]{chipman1996bayesian}
Hugh Chipman.
\newblock {B}ayesian variable selection with related predictors.
\newblock \emph{Canadian Journal of Statistics}, 24\penalty0 (1):\penalty0
  17--36, 1996.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean_large_2012}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{\nips{}}, pages 1223--1231, 2012.

\bibitem[Filippi et~al.(2010)Filippi, Cappe, Garivier, and
  Szepesvári]{filippi_parametric_2010}
Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári.
\newblock Parametric bandits: The generalized linear case.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2010.

\bibitem[Friston et~al.(2007)Friston, Mattout, Trujillo-Barreto, Ashburner, and
  Penny]{friston_variational_2007}
Karl Friston, J\'er\'emie Mattout, Nelson Trujillo-Barreto, John Ashburner, and
  Will Penny.
\newblock Variational free energy and the {L}aplace approximation.
\newblock \emph{Neuroimage}, 34\penalty0 (1):\penalty0 220--234, 2007.

\bibitem[Gelman(2008)]{gelman_objections_2008}
Andrew Gelman.
\newblock Objections to {B}ayesian statistics.
\newblock \emph{Bayesian Analysis}, 3:\penalty0 445--450, 2008.
\newblock ISSN 1931-6690.
\newblock \doi{11.1214/08-BA318}.

\bibitem[George and McCulloch(1993)]{george1993variable}
Edward~I George and Robert~E McCulloch.
\newblock Variable selection via gibbs sampling.
\newblock \emph{Journal of the American Statistical Association}, 88\penalty0
  (423):\penalty0 881--889, 1993.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot_deep_2011}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Deep sparse rectifier networks.
\newblock In \emph{\aistats{14th}}, volume~15, pages 315--323, 2011.

\bibitem[Graves(2011)]{graves_practical_2011}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{\nips{}}, pages 2348--2356, 2011.

\bibitem[Gregor et~al.(2014)Gregor, Danihelka, Mnih, Blundell, and
  Wierstra]{gregor_deep_2014}
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra.
\newblock Deep {AutoRegressive} networks.
\newblock In \emph{\icml{31st}}, pages 1242--1250, 2014.

\bibitem[Guez(2015)]{guez_sample_2015}
Arthur Guez.
\newblock \emph{{Sample-Based Search Methods For {B}ayes-Adaptive Planning}}.
\newblock PhD thesis, University College London, 2015.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{hinton_distilling_2014}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{NIPS 2014 Deep Learning and Representation Learning
  Workshop}, 2014.

\bibitem[Hinton and Van~Camp(1993)]{hinton_keeping_1993}
Geoffrey~E Hinton and Drew Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{\colt{16th}}, pages 5--13. {ACM}, 1993.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton_dropout_2012}
Geoffrey~E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R. Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{{arXiv}:1207.0580}, July 2012.

\bibitem[Jaakkola and Jordan(2000)]{jaakkola_bayesian_2000}
Tommi~S. Jaakkola and Michael~I. Jordan.
\newblock {B}ayesian parameter estimation via variational methods.
\newblock \emph{Statistics and Computing}, 10\penalty0 (1):\penalty0 25--37,
  2000.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and
  Munos]{kaufmann_thompson_2012}
Emilie Kaufmann, Nathaniel Korda, and R\'emi Munos.
\newblock {T}hompson sampling: An asymptotically optimal finite-time analysis.
\newblock In \emph{\alt{23rd}}, pages 199--213. Springer, 2012.

\bibitem[Kingma and Welling(2014)]{kingma_autoencoding_2014}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{\iclr{2nd}}, 2014.
\newblock {arXiv}: 1312.6114.

\bibitem[LeCun(1985)]{lecun_procedure_1985}
Yann LeCun.
\newblock Une proc\'edure d'apprentissage pour r\'eseau \`a seuil asymmetrique
  (a learning scheme for asymmetric threshold networks).
\newblock In \emph{Proceedings of Cognitiva 85, Paris, France}, pages 599--604,
  1985.

\bibitem[LeCun and Cortes(1998)]{lecun_mnist_1998}
Yann LeCun and Corinna Cortes.
\newblock \emph{The {MNIST} database of handwritten digits}.
\newblock 1998.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and
  Schapire]{li_contextual_bandit_2010}
Lihong Li, Wei Chu, John Langford, and Robert~E. Schapire.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \emph{Proceedings of the 19th International Conference on World
  Wide Web}, {WWW} '10, pages 661--670, New York, {NY}, {USA}, 2010. {ACM}.
\newblock ISBN 978-1-60558-799-8.
\newblock \doi{10.1145/1772690.1772758}.

\bibitem[MacKay(1992)]{mackay_practical_1992}
David~JC MacKay.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[MacKay(1995)]{mackay_probable_1995}
David~JC MacKay.
\newblock Probable networks and plausible predictions-a review of practical
  {B}ayesian methods for supervised neural networks.
\newblock \emph{Network: Computation in Neural Systems}, 6\penalty0
  (3):\penalty0 469--505, 1995.

\bibitem[May et~al.(2012)May, Korda, Lee, and Leslie]{may_optimistic_2012}
Benedict~C May, Nathan Korda, Anthony Lee, and David~S. Leslie.
\newblock Optimistic {B}ayesian sampling in contextual-bandit problems.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 2069--2106, 2012.

\bibitem[Minka(2001)]{minka_family_2001}
Thomas~P Minka.
\newblock \emph{A family of algorithms for approximate {B}ayesian inference}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2001.

\bibitem[Minka(2005)]{minka_divergence_2005}
Thomas~P Minka.
\newblock Divergence measures and message passing.
\newblock Technical report, Microsoft Research, 2005.

\bibitem[Mitchell and Beauchamp(1988)]{mitchell1988bayesian}
Toby~J Mitchell and John~J Beauchamp.
\newblock {B}ayesian variable selection in linear regression.
\newblock \emph{Journal of the American Statistical Association}, 83\penalty0
  (404):\penalty0 1023--1032, 1988.

\bibitem[Nair and Hinton(2010)]{nair_rectified_2010}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted {B}oltzmann machines.
\newblock In \emph{\icml{27th}}, pages 807--814, 2010.

\bibitem[Neal and Hinton(1998)]{neal_view_1998}
Radford~M Neal and Geoffrey~E Hinton.
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock In \emph{Learning in graphical models}, pages 355--368. Springer,
  1998.

\bibitem[Opper and Archambeau(2009)]{opper_variational_2009}
Manfred Opper and C\'edric Archambeau.
\newblock The variational {G}aussian approximation revisited.
\newblock \emph{Neural computation}, 21\penalty0 (3):\penalty0 786--792, 2009.

\bibitem[Owen(2013)]{mcbook}
Art~B. Owen.
\newblock \emph{Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende_stochastic_2014}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{\icml{31st}}, pages 1278--1286, 2014.

\bibitem[Rumelhart et~al.(1988)Rumelhart, Hinton, and
  Williams]{rumelhart1988learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Cognitive modeling}, 5, 1988.

\bibitem[Saul et~al.(1996)Saul, Jaakkola, and Jordan]{saul_mean_1996}
Lawrence~K Saul, Tommi Jaakkola, and Michael~I Jordan.
\newblock Mean field theory for sigmoid belief networks.
\newblock \emph{Journal of artificial intelligence research}, 4\penalty0
  (1):\penalty0 61--76, 1996.

\bibitem[Simard et~al.(2003)Simard, Steinkraus, and Platt]{simard_best_2003}
Patrice~Y Simard, Dave Steinkraus, and John~C Platt.
\newblock Best practices for convolutional neural networks applied to visual
  document analysis.
\newblock In \emph{\icdar{12th}}, volume~2, pages 958--958. {IEEE} Computer
  Society, 2003.

\bibitem[Thompson(1933)]{thompson_likelihood_1933}
William~R Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, pages 285--294, 1933.

\bibitem[Titsias and L{\'a}zaro-Gredilla(2014)]{titsias2014doubly}
Michalis Titsias and Miguel L{\'a}zaro-Gredilla.
\newblock Doubly stochastic variational bayes for non-conjugate inference.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 1971--1979, 2014.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Cun, and
  Fergus]{wan2013regularization}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann~L Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning (ICML-13)}, pages 1058--1066, 2013.

\bibitem[Yedidia et~al.(2000)Yedidia, Freeman, and
  Weiss]{yedidia_generalized_2000}
Jonathan~S Yedidia, William~T Freeman, and Yair Weiss.
\newblock Generalized belief propagation.
\newblock In \emph{\nips{}}, volume~13, pages 689--695, 2000.

\end{thebibliography}
