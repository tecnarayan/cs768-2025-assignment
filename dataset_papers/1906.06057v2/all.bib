@misc{cohen2006making,
  title={Making headway under hellacious circumstances},
  author={Cohen, Jon},
  year={2006},
  publisher={American Association for the Advancement of Science}
}
@article{hoffmann2019,
	title={Learning Graphs from Noisy Epidemic Cascades},
	author={Hoffmann, Jessica and Caramanis, Constantine},
	journal={arXiv preprint arXiv:1903.02650},
	year={2019}
}

@article{Kruskal56,
	ISSN = {00029939, 10886826},
	URL = {http://www.jstor.org/stable/2033241},
	author = {Joseph B. Kruskal},
	journal = {Proceedings of the American Mathematical Society},
	number = {1},
	pages = {48--50},
	publisher = {American Mathematical Society},
	title = {On the Shortest Spanning Subtree of a Graph and the Traveling Salesman Problem},
	volume = {7},
	year = {1956}
}


@incollection{Cayley1897,
author = {Cayley, A.},
booktitle = {Collected Mathematical Papers Vol. 13},
pages = {26--28},
publisher = {Cambridge University Press},
title = {{A theorem on trees}},
year = {1897}
}
@article{Goldenberg2001,
author = {Goldenberg, Jacob and Libai, Barak and Muller, Eitan},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Goldenberg2001{\_}Article{\_}TalkOfTheNetworkAComplexSystem.pdf:pdf},
journal = {Marketing Letters},
keywords = {and intriguing phenomenon,cellular automata,communications is a pervasive,complex systems,it,social networks,w-o-m,word-of-mouth},
pages = {211--223},
title = {{Talk of the Network : A Complex Systems Look at the Underlying Process of Word-of-Mouth}},
year = {2001}
}
@article{Dempster1977,
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Dempster77.pdf:pdf},
journal = {Journal ofthe Royal Statistical Society},
number = {1},
pages = {1--38},
title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
volume = {39},
year = {1977}
}
@article{Kwon2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.05752v3},
author = {Kwon, Jeongyeol and Qian, Wei and Caramanis, Constantine and Chen, Yudong and Davis, Damek},
eprint = {arXiv:1810.05752v3},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/1810.05752.pdf:pdf},
pages = {1--57},
title = {{Global Convergence of the EM Algorithm for Mixtures of Two Component Linear Regression}},
volume = {XX},
year = {2019}
}
@inproceedings{Iwata2013,
author = {Iwata, Tomoharu and Shah, Amar and Ghahramani, Zoubin},
booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD' 13)},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/kdd2013.pdf:pdf},
isbn = {9781450321747},
keywords = {latent variable models,poisson processes,social community},
title = {{Discovering Latent Influence in Online Social Activities via Shared Cascade Poisson Processes}},
year = {2013}
}
@inproceedings{Myers2012,
author = {Myers, Seth and Zhu, Chenguang and Leskovec, Jure},
booktitle = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD' 12)},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/ext-kdd12.pdf:pdf},
isbn = {9781450314626},
keywords = {a node,by a big circle,denoted,diffusion of innovations,external influence,figure 1,influence,infor-,information cascades,is exposed to information,mation diffusion,our model of external,social networks,through an external,twitter},
pages = {33--41},
title = {{Information Diffusion and External Influence in Networks}},
year = {2012}
}
@inproceedings{Zarezade2017,
author = {Zarezade, Ali and Khodadadi, Ali and Farajtabar, Mehrdad and Rabiee, Hamid R and Zha, Hongyuan},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/14360-66249-1-PB.pdf:pdf},
keywords = {Artificial Intelligence and the Web},
pages = {238--244},
title = {{Correlated Cascades : Compete or Cooperate}},
year = {2017}
}
@inproceedings{Gomez-Rodriguez2013,
author = {Gomez-Rodriguez, Manuel and Leskovec, Jure and Sch{\"{o}}lkopf, Bernhard},
booktitle = {6th International Conference on Web Search and Data Mining (WSDM 2013)},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/infopath-wsdm13.pdf:pdf},
isbn = {9781450318693},
keywords = {16,18,and,blogs,diffu-,for example,in case of information,information cascades,meme-tracking,networks of diffusion,news media,piece of information,sion,social networks,the contagion represents a,the underlying network},
title = {{Structure and Dynamics of Information Pathways in Online Media}},
year = {2013}
}
@misc{McLachlan,
author = {McLachlan and Peel},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Mixtures/finite{\_}mixture{\_}models{\_}mclachlan.pdf:pdf},
title = {{Finite Mixture Models}}
}
@article{Newyork2008,
author = {Newyork, Berlin Heidelberg and London, Hong Kong and Tokyo, Milan Paris},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Mixtures/book{\_}matlab{\_}version{\_}2.0.pdf:pdf},
title = {{Finite Mixture and Markov Switching Models}},
year = {2008}
}
@article{Kannan2004,
author = {Kannan, Ravindran and Salmasian, Hadi},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Mixtures/ Moment{\_}methods{\_}mixtures.pdf:pdf},
journal = {Electronic Colloquium on Computational Complexity},
number = {67},
title = {{The Spectral Method for Mixture models}},
volume = {67},
year = {2004}
}
@article{Maetschke,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.1083v1},
author = {Maetschke, Stefan R and Madhamshettiwar, Piyush B and Davis, Melissa J and Ragan, Mark A},
eprint = {arXiv:1301.1083v1},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/GRN/survey{\_}GRN{\_}2014.pdf:pdf},
title = {{Supervised, semi-supervised and unsupervised inference of gene regulatory networks}}
}
@article{spencer2015impossibility,
author = {Spencer, Sam and Srikant, R},
journal = {ACM SIGMETRICS Performance Evaluation Review},
number = {2},
pages = {66--68},
publisher = {ACM},
title = {{On the impossibility of localizing multiple rumor sources in a line graph}},
volume = {43},
year = {2015}
}
@article{Mansour2011,
author = {Mansour, Yishay},
title = {{Lecture 5 : Lower Bounds using Information Theory Tools Distance between Distributions KL-Divergence}},
year = {2011}
}
@misc{Whetten,
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Whetten, Joshua L and Williamson, Philip C and Heo, Giseon and Varnhagen, Connie and Major, Paul W},
booktitle = {American Journal of Orthodontics and Dentofacial Orthopedics},
doi = {10.1016/j.ajodo.2005.02.022},
eprint = {1708.02002},
isbn = {9788476662106},
issn = {0034-7612},
pages = {485--491},
pmid = {23766329},
title = {{Study Models}}
}
@article{Goldberg2001,
abstract = {Eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real-valued user ratings on a common set of items and applies principal component analysis (PCA) to the resulting dense subset of the ratings matrix. PCA facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations. For a database of n users, standard nearest-neighbor techniques require O(n) processing time to compute recommendations, whereas Eigentaste requires O(1) (constant) time. We compare Eigentaste to alternative algorithms using data from Jester, an online joke recommending system. Jester has collected approximately 2,500,000 ratings from 57,000 users. We use the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms. In the Appendix we use Uniform and Normal distribution models to derive analytic estimates of NMAE when predictions are random. On the Jester dataset, Eigentaste computes recommendations two orders of magnitude faster with no loss of accuracy. Jester is online at: http://eigentaste.berkeley.edu},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Goldberg, Ken and Roeder, Theresa and Gupta, Dhruv and Perkins, Chris},
doi = {10.1023/A:1011419012209},
eprint = {0005074v1},
isbn = {1386-4564},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Collaborative filtering,Dimensionality reduction,Google2017,Jokes,NotRead,Recommender,Recommender systems},
mendeley-tags = {Google2017,NotRead,Recommender},
number = {2},
pages = {133--151},
pmid = {160},
primaryClass = {arXiv:astro-ph},
title = {{Eigentaste: A Constant Time Collaborative Filtering Algorithm}},
volume = {4},
year = {2001}
}
@article{Easley2010,
author = {Easley, David and Kleinberg, Jon},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Networks, Crowds, and Markets: Reasoning about a Highly Connected World.}},
year = {2010}
}
@inproceedings{wang2014rumor,
author = {Wang, Zhaoxu and Dong, Wenxiang and Zhang, Wenyi and Tan, Chee Wei},
booktitle = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
organization = {ACM},
pages = {1--13},
title = {{Rumor source detection with multiple observations: Fundamental limits and algorithms}},
volume = {42},
year = {2014}
}
@inproceedings{bose2008,
address = {New York, NY, USA},
author = {Bose, Abhijit and Hu, Xin and Shin, Kang G and Park, Taejoon},
booktitle = {Proceedings of the 6th International Conference on Mobile Systems, Applications, and Services},
doi = {10.1145/1378600.1378626},
isbn = {978-1-60558-139-2},
keywords = {machine learning,mobile handsets,security,worm detection},
pages = {225--238},
publisher = {ACM},
series = {MobiSys '08},
title = {{Behavioral Detection of Malware on Mobile Handsets}},
url = {http://doi.acm.org/10.1145/1378600.1378626},
year = {2008}
}
@inproceedings{fanti2016rumor,
author = {Fanti, Giulia and Kairouz, Peter and Oh, Sewoong and Ramchandran, Kannan and Viswanath, Pramod},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science (SIGMETRICS' 16 )},
organization = {ACM},
pages = {153--164},
title = {{Rumor source obfuscation on irregular trees}},
year = {2016}
}
@article{gregoire2008,
author = {Jacob, Gr{\'{e}}goire and Debar, Herv{\'{e}} and Filiol, Eric},
doi = {10.1007/s11416-008-0086-0},
issn = {1772-9890},
journal = {Journal in Computer Virology},
number = {3},
pages = {251--266},
publisher = {Springer-Verlag},
title = {{Behavioral detection of malware: from a survey towards an established taxonomy}},
url = {http://dx.doi.org/10.1007/s11416-008-0086-0},
volume = {4},
year = {2008}
}
@article{Daniely2014,
abstract = {We present a PTAS for agnostically learning halfspaces w.r.t. the uniform distribution on the {\$}d{\$} dimensional sphere. Namely, we show that for every {\$}\backslashmu{\textgreater}0{\$} there is an algorithm that runs in time {\$}\backslashmathrm{\{}poly{\}}(d,\backslashfrac{\{}1{\}}{\{}\backslashepsilon{\}}){\$}, and is guaranteed to return a classifier with error at most {\$}(1+\backslashmu)\backslashmathrm{\{}opt{\}}+\backslashepsilon{\$}, where {\$}\backslashmathrm{\{}opt{\}}{\$} is the error of the best halfspace classifier. This improves on Awasthi, Balcan and Long [ABL14] who showed an algorithm with an (unspecified) constant approximation ratio. Our algorithm combines the classical technique of polynomial regression (e.g. [LMN89, KKMS05]), together with the new localization technique of [ABL14].},
archivePrefix = {arXiv},
arxivId = {1410.7050},
author = {Daniely, Amit},
eprint = {1410.7050},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/LearningTheory/PTAS{\_}halfspaces.pdf:pdf},
issn = {15337928},
title = {{A PTAS for Agnostically Learning Halfspaces}},
url = {http://arxiv.org/abs/1410.7050},
year = {2014}
}
@article{Brown,
author = {Brown, Daniel G},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Brown - Unknown - How I wasted too long finding a concentration inequality for sums of geometric variables.pdf:pdf},
journal = {Found at https://cs. uwaterloo. ca/{\~{}} browndg/negbin. pdf},
title = {{How I wasted too long finding a concentration inequality for sums of geometric variables}},
volume = {6}
}
@article{Kulldorff2005,
abstract = {BACKGROUND: The ability to detect disease outbreaks early is important in order to minimize morbidity and mortality through timely implementation of disease prevention and control measures. Many national, state, and local health departments are launching disease surveillance systems with daily analyses of hospital emergency department visits, ambulance dispatch calls, or pharmacy sales for which population-at-risk information is unavailable or irrelevant. METHODS AND FINDINGS: We propose a prospective space-time permutation scan statistic for the early detection of disease outbreaks that uses only case numbers, with no need for population-at-risk data. It makes minimal assumptions about the time, geographical location, or size of the outbreak, and it adjusts for natural purely spatial and purely temporal variation. The new method was evaluated using daily analyses of hospital emergency department visits in New York City. Four of the five strongest signals were likely local precursors to citywide outbreaks due to rotavirus, norovirus, and influenza. The number of false signals was at most modest. CONCLUSION: If such results hold up over longer study times and in other locations, the space-time permutation scan statistic will be an important tool for local and national health departments that are setting up early disease detection surveillance systems.},
author = {Kulldorff, Martin and Heffernan, Richard and Hartman, Jessica and Assun????o, Renato and Mostashari, Farzad},
doi = {10.1371/journal.pmed.0020059},
isbn = {1549-1676 (Electronic){\$}\backslash{\$}n1549-1277 (Linking)},
issn = {15491277},
journal = {PLoS Medicine},
keywords = {Epidemics,NotRead},
mendeley-tags = {Epidemics,NotRead},
number = {3},
pages = {216--224},
pmid = {15719066},
title = {{A space-time permutation scan statistic for disease outbreak detection}},
volume = {2},
year = {2005}
}
@article{Elidan2005,
author = {Elidan, Gal},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Elidan - 2005 - Learning Hidden Variable Networks The Information Bottleneck Approach.pdf:pdf},
keywords = {Constantine,GraphicalModels,HiddenVariables,bayesian networks,continuation,hidden variables,information bottleneck,variational},
mendeley-tags = {Constantine,GraphicalModels,HiddenVariables},
pages = {81--127},
title = {{Learning Hidden Variable Networks : The Information Bottleneck Approach}},
volume = {6},
year = {2005}
}
@article{Niu2011,
abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently pro-posed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and im-plementation that SGD can be implemented without any locking. We present an update scheme called Hogwild! which allows processors access to shared memory with the possibility of over-writing each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild! achieves a nearly optimal rate of convergence. We demonstrate experimentally that Hogwild! outperforms alternative schemes that use locking by an order of magnitude.},
author = {Niu, Feng and Recht, Benjamin and R{\'{e}}, Christopher and Wright, Stephen J},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Niu et al. - 2011 - Hogwild! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.pdf:pdf},
keywords = {LargeScale,NotRead},
mendeley-tags = {LargeScale,NotRead},
title = {{Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}},
year = {2011}
}
@inproceedings{Milling2012,
author = {Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay},
booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE joint international conference on Measurement and Modeling of Computer Systems (SIGMETRICS' 12)},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Milling - 2012 - Network Forensics Random Infection vs Spreading Epidemic.pdf:pdf},
isbn = {9781450310970},
keywords = {Constantine,Epidemics,NotRead,epidemic process,network inference},
mendeley-tags = {Constantine,Epidemics,NotRead},
title = {{Network Forensics : Random Infection vs Spreading Epidemic}},
year = {2012}
}
@article{Dinur2013,
abstract = {We propose an analytical framework for studying parallel repetition, a basic product operation for one-round two-player games. In this framework, we consider a relaxation of the value of a game, {\$}\backslashbackslashmathrm{\{}\backslash{\{}{\}}val{\{}\backslash{\}}{\}}{\_}+{\$}, and prove that for projection games, it is both multiplicative (under parallel repetition) and a good approximation for the true value. These two properties imply a parallel repetition bound as {\$}{\$} {\$}\backslash{\$}mathrm{\{}val{\}}(G{\^{}}{\{}{\$}\backslash{\$}otimes k{\}}) {\$}\backslash{\$}approx {\$}\backslash{\$}mathrm{\{}val{\}}{\_}+(G{\^{}}{\{}{\$}\backslash{\$}otimes k{\}}) = {\$}\backslash{\$}mathrm{\{}val{\}}{\_}+(G){\^{}}{\{}k{\}} {\$}\backslash{\$}approx {\$}\backslash{\$}mathrm{\{}val{\}}(G){\^{}}{\{}k{\}}. {\$}{\$} Using this framework, we can also give a short proof for the NP-hardness of Label-Cover{\$}(1,\backslashbackslashdelta){\$} for all {\$}\backslashbackslashdelta{\{}\backslashtextgreater{\}}0{\$}, starting from the basic PCP theorem. We prove the following new results: - A parallel repetition bound for projection games with small soundness. Previously, it was not known whether parallel repetition decreases the value of such games. This result implies stronger inapproximability bounds for Set-Cover and Label-Cover. - An improved bound for few parallel repetitions of projection games, showing that Raz's counterexample is tight even for a small number of repetitions. Our techniques also allow us to bound the value of the direct product of multiple games, namely, a bound on {\$}\backslashbackslashmathrm{\{}\backslash{\{}{\}}val{\{}\backslash{\}}{\}}(G{\_}1\backslashbackslashotimes ...\backslashbackslashotimes G{\_}k){\$} for different projection games {\$}G{\_}1,...,G{\_}k{\$}.},
archivePrefix = {arXiv},
arxivId = {1305.1979v2},
author = {Dinur, Irit and Steurer, David},
doi = {10.1145/2591796.2591884},
eprint = {1305.1979v2},
isbn = {9781450327107},
issn = {07378017},
journal = {CoRR},
keywords = {advanced study,and applied mathematics,approximation,copositive programming,department of computer science,done at microsoft research,ff e institute for,hardness of,label cover,new england and radcli,one-round two-player games,operator norms,parallel repetition,part of this work,set cover,was,weizmann institute,work supported in},
number = {1179},
pages = {624--633},
title = {{Analytical Approach to Parallel Repetition}},
url = {http://arxiv.org/abs/1305.1979},
volume = {abs/1305.1},
year = {2013}
}
@article{Kordalewski2013,
author = {Kordalewski, David},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kordalewski - 2013 - New Greedy Heuristics for Approximating Set Cover and Set Packing.pdf:pdf},
keywords = {SetCover},
mendeley-tags = {SetCover},
title = {{New Greedy Heuristics for Approximating Set Cover and Set Packing}},
year = {2013}
}
@article{Milling2015,
abstract = {In many networks the operator is faced with nodes that report a potentially important phenomenon such as failures, illnesses, and viruses. The operator is faced with the question: Is it spreading over the network, or simply occurring at random? We seek to answer this question from highly noisy and incomplete data, where at a single point in time we are given a possibly very noisy subset of the infected population (including false positives and negatives). While previous work has focused on uniform spreading rates for the infection, heterogeneous graphs with unequal edge weights are more faithful models of reality. Critically, the network structure may not be fully known and modeling epidemic spread on unknown graphs relies on non-homogeneous edge (spreading) weights. Such heterogeneous graphs pose considerable challenges, requiring both algorithmic and analytical development. We develop an algorithm that can distinguish between a spreading phenomenon and a randomly occurring phenomenon while using only local information and not knowing the complete network topology and the weights. Further, we show that this algorithm can succeed even in the presence of noise, false positives and unknown graph edges.},
author = {Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay},
doi = {10.1109/INFOCOM.2015.7218530},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Milling et al. - 2015 - Local detection of infections in heterogeneous networks.pdf:pdf},
isbn = {9781479983810},
issn = {0743166X},
journal = {Proceedings - IEEE INFOCOM},
keywords = {Constantine,Epidemics,No,NotRead},
mendeley-tags = {Constantine,Epidemics,No,NotRead},
pages = {1517--1525},
title = {{Local detection of infections in heterogeneous networks}},
volume = {26},
year = {2015}
}
@book{Newman2014a,
abstract = {NIH's Fogarty International Center has provided grants for the development of training programs in international research ethics for low- and middle-income (LMIC) professionals since 2000. Drawing on 12 years of research ethics training experience, a group of Fogarty grantees, trainees, and other ethics experts sought to map the current capacity and need for research ethics in LMICs, analyze the lessons learned about teaching bioethics, and chart a way forward for research ethics training in a rapidly changing health research landscape. This collection of papers is the result.},
author = {Newman, M. E. J.},
booktitle = {Cambridge Quarterly of Healthcare Ethics},
doi = {10.1017/S0963180113000479},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Newman - 2014 - Networks An Introduction.pdf:pdf},
isbn = {9780199206650},
issn = {0963-1801},
keywords = {Book,Books},
mendeley-tags = {Book,Books},
number = {01},
pages = {73--75},
pmid = {24256602},
title = {{Networks: An Introduction}},
url = {http://www.journals.cambridge.org/abstract{\_}S0963180113000479},
volume = {23},
year = {2014}
}
@article{Austin2013,
author = {Austin, U T and Caramanis, Constantine and Austin, U T},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Austin, Caramanis, Austin - 2013 - Detecting Epidemics Using Highly Noisy Data.pdf:pdf},
isbn = {9781450321938},
keywords = {Constantine,Epidemics,NotRead,epidemic process,network inference},
mendeley-tags = {Constantine,Epidemics,NotRead},
pages = {177--186},
title = {{Detecting Epidemics Using Highly Noisy Data}},
year = {2013}
}
@article{Fanti2017,
abstract = {Anonymous social media platforms like Secret, Yik Yak, and Whisper have emerged as important tools for sharing ideas without the fear of judgment. Such anonymous platforms are also important in nations under authoritarian rule, where freedom of expression and the personal safety of message authors may depend on anonymity. Whether for fear of judgment or retribution, it is sometimes crucial to hide the identities of users who post sensitive messages. In this paper, we consider a global adversary who wishes to identify the author of a message; it observes either a snapshot of the spread of a message at a certain time, sampled timestamp metadata, or both. Recent advances in rumor source detection show that existing messaging protocols are vulnerable against such an adversary. We introduce a novel messaging protocol, which we call adaptive diffusion, and show that under the snapshot adversarial model, adaptive diffusion spreads content fast and achieves perfect obfuscation of the source when the underlying contact network is an infinite regular tree. That is, all users with the message are nearly equally likely to have been the origin of the message. When the contact network is an irregular tree, we characterize the probability of maximum likelihood detection by proving a concentration result over Galton-Watson trees. Experiments on a sampled Facebook network demonstrate that adaptive diffusion effectively hides the location of the source even when the graph is finite, irregular and has cycles.},
archivePrefix = {arXiv},
arxivId = {1509.02849},
author = {Fanti, Giulia and Kairouz, Peter and Oh, Sewoong and Ramchandran, Kannan and Viswanath, Pramod},
doi = {10.1109/TIT.2017.2696960},
eprint = {1509.02849},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/Hiding{\_}rumor{\_}source.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Privacy,anonymous social media,diffusion},
number = {10},
pages = {6679--6713},
title = {{Hiding the Rumor Source}},
volume = {63},
year = {2017}
}
@article{Leskovec2007,
abstract = {Given a water distribution network, where should we place$\backslash$r$\backslash$nsensors to quickly detect contaminants? Or, which blogs$\backslash$r$\backslash$nshould we read to avoid missing important stories?$\backslash$r$\backslash$nThese seemingly different problems share common structure:$\backslash$r$\backslash$nOutbreak detection can be modeled as selecting nodes$\backslash$r$\backslash$n(sensor locations, blogs) in a network, in order to detect the$\backslash$r$\backslash$nspreading of a virus or information as quickly as possible.$\backslash$r$\backslash$nWe present a general methodology for near optimal sensor$\backslash$r$\backslash$nplacement in these and related problems. We demonstrate$\backslash$r$\backslash$nthat many realistic outbreak detection objectives (e.g., detection$\backslash$r$\backslash$nlikelihood, population affected) exhibit the property$\backslash$r$\backslash$nof “submodularity”. We exploit submodularity to develop$\backslash$r$\backslash$nan efficient algorithm that scales to large problems,$\backslash$r$\backslash$nachieving near optimal placements, while being 700 times$\backslash$r$\backslash$nfaster than a simple greedy algorithm. We also derive online$\backslash$r$\backslash$nbounds on the quality of the placements obtained by$\backslash$r$\backslash$nany algorithm. Our algorithms and bounds also handle cases$\backslash$r$\backslash$nwhere nodes (sensor locations, blogs) have different costs.$\backslash$r$\backslash$nWe evaluate our approach on several large real-world problems,$\backslash$r$\backslash$nincluding a model of a water distribution network from$\backslash$r$\backslash$nthe EPA, and real blog data. The obtained sensor placements$\backslash$r$\backslash$nare provably near optimal, providing a constant fraction$\backslash$r$\backslash$nof the optimal solution. We show that the approach$\backslash$r$\backslash$nscales, achieving speedups and savings in storage of several$\backslash$r$\backslash$norders of magnitude. We also show how the approach leads$\backslash$r$\backslash$nto deeper insights in both applications, answering multicriteria$\backslash$r$\backslash$ntrade-off, cost-sensitivity and generalization questions.},
author = {Leskovec, Jure and Krause, Andreas and Guestrin, Carlos and Faloutsos, Christos and VanBriesen, Jeanne and Glance, Natalie},
doi = {10.1145/1281192.1281239},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Leskovec et al. - 2007 - Cost-effective Outbreak Detection in Networks.pdf:pdf;:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Leskovec, Krause, Guestrin - 2007 - Cost-effective outbreak detection in networks.pdf:pdf},
isbn = {978-1-59593-609-7},
journal = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '07)},
keywords = {Epidemics,Learning/Statistics {\&} Optimisation,NotRead,Theory {\&} Algorithms},
mendeley-tags = {Epidemics,NotRead},
pages = {420},
title = {{Cost-effective Outbreak Detection in Networks}},
url = {http://eprints.pascal-network.org/archive/00005342/ http://dl.acm.org/citation.cfm?id=1281239},
year = {2007}
}
@article{Hethcote2000,
author = {Hethcote, Herbert W},
doi = {10.1137/S0036144500371907},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Hethcote - 2000 - Diseases ∗.pdf:pdf},
isbn = {10.1137/S0036144500371907},
issn = {0036-1445},
keywords = {1,34c23,34c60,35b32,35f25,92d30,Epidemics,ams subject classifications,and vac-,antibiotics,basic reproduction number,contact number,epidemiology,infectious diseases,introduction,pii,primary,s0036144500371907,sanitation,secondary,the effectiveness of improved,thresholds},
mendeley-tags = {Epidemics},
number = {4},
pages = {599--653},
pmid = {2653135},
title = {{Diseases ∗}},
volume = {42},
year = {2000}
}
@article{Milling2015a,
abstract = {The history of infections and epidemics holds famous examples where understanding, containing, and ultimately treating an outbreak began with understanding its mode of spread. Influenza, HIV, and most computer viruses spread person to person, device to device, and through contact networks; Cholera, Cancer, and seasonal allergies, on the other hand, do not. In this paper, we study two fundamental questions of detection. First, given a snapshot view of a (perhaps vanishingly small) fraction of those infected, under what conditions is an epidemic spreading via contact (e.g., Influenza), distinguishable from a random illness operating independently of any contact network (e.g., seasonal allergies)? Second, if we do have an epidemic, under what conditions is it possible to determine which network of interactions is the main cause of the spread-the causative network-without any knowledge of the epidemic, other than the identity of a minuscule subsample of infected nodes? The core, therefore, of this paper, is to obtain an understanding of the diagnostic power of network information. We derive sufficient conditions that networks must satisfy for these problems to be identifiable, and produce efficient, highly scalable algorithms that solve these problems. We show that the identifiability condition we give is fairly mild, and in particular, is satisfied by two common graph topologies: the d-dimensional grid, and the Erdos-Renyi graphs.},
archivePrefix = {arXiv},
arxivId = {1309.6545},
author = {Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay},
doi = {10.1109/TIT.2015.2424875},
eprint = {1309.6545},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Milling et al. - 2015 - Distinguishing Infections on Different Graph Topologies.pdf:pdf},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {Constantine,Epidemics,NotRead,Read},
mendeley-tags = {Constantine,Epidemics,NotRead,Read},
number = {6},
pages = {3100--3120},
title = {{Distinguishing Infections on Different Graph Topologies}},
volume = {61},
year = {2015}
}
@book{Meyers2005,
abstract = {Abstract Objective: In this work, a model of bioheat distribution is discussed for ex vivo human tissue samples, and the thermal penetration depth measurements performed on several tissues are presented. Background data: Optical radiation is widely applied in the treatment and diagnosis of different pathologies. A power density of incident light at 100 mW/cm(2) is sufficiently high enough to induce a temperature increase of {\textgreater}5C in irradiated human tissue. In this case, knowledge of the thermal properties of the tissue is needed to achieve a better understanding of the therapeutic effects. Method: The application of the diffusion approximation of the radiative transfer equation for the distribution of optical radiation, the experimental setup, and the results thereof are presented and discussed. Results: The effective thermal penetration depth in the studied tissues has been determined to be in the range of 4.3-7.0mm. Conclusions: The effective thermal penetration depth has been defined, and this could be useful for developing models to describe the thermal effects with a separate analysis of the tissue itself and the blood that irrigates it.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Meyers, Scott},
booktitle = {Analysis},
doi = {10.2307/3327886},
eprint = {arXiv:1011.1669v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Meyers - 2005 - Effective C 55 Specific Ways to Improve Your Programs and Designs.pdf:pdf},
isbn = {9780321515827},
issn = {00032638},
pages = {85},
pmid = {21612514},
title = {{Effective C++: 55 Specific Ways to Improve Your Programs and Designs}},
url = {http://www.jstor.org/stable/3327886?origin=crossref},
volume = {43},
year = {2005}
}
@book{Cover2005,
abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
archivePrefix = {arXiv},
arxivId = {ISBN 0-471-06259-6},
author = {Cover, Thomas M. and Thomas, Joy A.},
booktitle = {Elements of Information Theory},
doi = {10.1002/047174882X},
eprint = {ISBN 0-471-06259-6},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Cover, Thomas - 2005 - Elements of Information Theory.pdf:pdf},
isbn = {9780471241959},
issn = {15579654},
keywords = {Books},
mendeley-tags = {Books},
pages = {1--748},
pmid = {20660925},
publisher = {John Wiley {\&} Sons},
title = {{Elements of Information Theory}},
year = {2005}
}
@inproceedings{Netrapalli2012,
abstract = {We consider the problem of finding the graph on which an epidemic cascade spreads, given only the times when each node gets infected. While this is a problem of importance in several contexts -- offline and online social networks, e-commerce, epidemiology, vulnerabilities in infrastructure networks -- there has been very little work, analytical or empirical, on finding the graph. Clearly, it is impossible to do so from just one cascade; our interest is in learning the graph from a small number of cascades. For the classic and popular "independent cascade" SIR epidemics, we analytically establish the number of cascades required by both the global maximum-likelihood (ML) estimator, and a natural greedy algorithm. Both results are based on a key observation: the global graph learning problem decouples into {\$}n{\$} local problems -- one for each node. For a node of degree {\$}d{\$}, we show that its neighborhood can be reliably found once it has been infected {\$}O(d{\^{}}2 \backslashlog n){\$} times (for ML on general graphs) or {\$}O(d\backslashlog n){\$} times (for greedy on trees). We also provide a corresponding information-theoretic lower bound of {\$}\backslashOmega(d\backslashlog n){\$}; thus our bounds are essentially tight. Furthermore, if we are given side-information in the form of a super-graph of the actual graph (as is often the case), then the number of cascade samples required -- in all cases -- becomes independent of the network size {\$}n{\$}. Finally, we show that for a very general SIR epidemic cascade model, the Markov graph of infection times is obtained via the moralization of the network graph.},
archivePrefix = {arXiv},
arxivId = {1202.1779},
author = {Netrapalli, Praneeth and Sanghavi, Sujay},
booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE joint international conference on Measurement and Modeling of Computer Systems (SIGMETRICS' 12)},
doi = {10.1145/2318857.2254783},
eprint = {1202.1779},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Netrapalli, Sanghavi - Unknown - Finding the Graph of Epidemic Cascades.pdf:pdf;:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Netrapalli, Sanghavi - 2012 - Finding the Graph of Epidemic Cascades.pdf:pdf},
isbn = {9781450310970},
issn = {01635999},
keywords = {Constantine,Epidemics,Markov random fields,NotRead,cascades,network inverse problems,sample complexity,structure learning},
mendeley-tags = {Constantine,Epidemics,NotRead},
pages = {211--222},
title = {{Learning the Graph of Epidemic Cascades}},
url = {http://arxiv.org/abs/1202.1779},
year = {2012}
}
@article{Shao2016a,
abstract = {Massive amounts of misinformation have been observed to spread in uncontrolled fashion across social media. Examples include rumors, hoaxes, fake news, and conspiracy theories. At the same time, several journalistic organizations devote significant efforts to high-quality fact checking of online claims. The resulting information cascades contain instances of both accurate and inaccurate information, unfold over multiple time scales, and often reach audiences of considerable size. All these factors pose challenges for the study of the social dynamics of online news sharing. Here we introduce Hoaxy, a platform for the collection, detection, and analysis of online misinformation and its related fact-checking efforts. We discuss the design of the platform and present a preliminary analysis of a sample of public tweets containing both fake news and fact checking. We find that, in the aggregate, the sharing of fact-checking content typically lags that of misinformation by 10--20 hours. Moreover, fake news are dominated by very active users, while fact checking is a more grass-roots activity. With the increasing risks connected to massive online misinformation, social news observatories have the potential to help researchers, journalists, and the general public understand the dynamics of real and fake news sharing.},
archivePrefix = {arXiv},
arxivId = {1603.01511},
author = {Shao, Chengcheng and Ciampaglia, Giovanni Luca and Flammini, Alessandro and Menczer, Filippo},
doi = {10.1145/2872518.2890098},
eprint = {1603.01511},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shao et al. - 2016 - Hoaxy A Platform for Tracking Online Misinformation.pdf:pdf},
isbn = {9781450341448},
keywords = {fact,fake news,hoaxes,misinformation,rumor tracking},
title = {{Hoaxy: A Platform for Tracking Online Misinformation}},
url = {http://arxiv.org/abs/1603.01511 http://dx.doi.org/10.1145/2872518.2890098},
year = {2016}
}
@inproceedings{shah2010detecting,
author = {Shah, Devavrat and Zaman, Tauhid},
booktitle = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
organization = {ACM},
pages = {203--214},
title = {{Detecting sources of computer viruses in networks: theory and experiment}},
volume = {38},
year = {2010}
}
@article{Evans,
abstract = {a graduate course},
author = {Evans, Lawrence C},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Evans - Unknown - An Introduction to Stochastic Differential Equations.pdf:pdf},
pages = {126},
title = {{An Introduction to Stochastic Differential Equations}},
url = {http://math.berkeley.edu/{\%}7B{~}{\%}7Devans/{\$}{\%}5C{\$}nhttp://math.berkeley.edu/{\%}7B{~}{\%}7Devans/control.course.pdf http://math.berkeley.edu/{~}evans/{\%}5Cnhttp://math.berkeley.edu/{~}evans/control.course.pdf}
}
@article{Sharpnack2012,
abstract = {We consider the change-point detection problem of deciding, based on noisymeasurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi$2 testing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.0773v1},
author = {Sharpnack, James and Rinaldo, Alessandro and Singh, Aarti},
eprint = {arXiv:1206.0773v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Sharpnack, Rinaldo, Singh - 2012 - Changepoint detection over graphs with the spectral scan statistic.pdf:pdf;:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Sharpnack, Rinaldo, Singh - 2012 - Changepoint detection over graphs with the spectral scan statistic(2).pdf:pdf},
issn = {15337928},
journal = {arXiv preprint},
keywords = {Constantine,Epidemics,NotRead,Read},
mendeley-tags = {Constantine,Epidemics,NotRead,Read},
pages = {1--14},
title = {{Changepoint detection over graphs with the spectral scan statistic}},
url = {http://arxiv.org/abs/1206.0773},
volume = {31},
year = {2012}
}
@article{Chvatal1979,
abstract = {Let A be a binary matrix of size m X n, let c "{\^{}} be a positive row vector of length n and let e be the column vector, all of whose m components are ones. The set-covering problem is to minimize c {\^{}}x subject to {\^{}}x {\textgreater}e and x binary. We compare the value of the objective function at a feasible solution found by a simple greedy heuristic to the true optimum. It turns out that the ratio between the two grows at most logarithmically in the largest column sum of A. When all the components of c{\^{}} are the same, our result reduces to a theorem established previously by Johnson and Lovasz. In the set-covering problem [2], the data consist of finite sets /*,, P2, • • • ,? " and positive numbers c,, Cj,. . ., c{\^{}}. We denote U({\^{}}, • I {\textless}j {\textless}n) by / and write / = • (1, 2, . . . , m], /= {\{}1, 2, ...,«{\}}. A subset /* of J is called a cover if {\$}\backslash{\$}J(,Pj ' • J E /*) = /; the cost of this cover is 2(c : j E J*). The problem is to find a cover of tninimum cost. The set-covering problem is notoriously hard; in fact, it is known to be NP-complete [4], [1]. In view of this fact, the relative importance of hetiristics for solving the set-covering problem increases. The purpose of this note is to establish a tight bound on the worst-case behaviour of a rather straightforward heuristic. In case c, = 1 for all /, our theorem reduces to one obtained previously by Johnson [3] and Lovasz [5]. Intuitively, it seems that the desirability of including{\^{}} in an optimal cover increases with the ratio {\$}\backslash{\$}Pj{\$}\backslash{\$}/Cj which counts the number of points covered by Pj per unit cost. This sentiment suggests a recursive procedure for finding near-optimal covers. Step 0. Set J* = 0. Step 1. Ii Pj=0 for ally then stop: J* is a cover. Otherwise find a subscript k maximizing the ratio {\$}\backslash{\$}Pj{\$}\backslash{\$}/cj and proceed to Step 2. Step 2. Add k to /*, replace each Pj by P, -f{\^{}} atid return to Step 1. Heuristic procedures of a similar character are called greedy. For illustration, consider sets P,, Pj, . . . , P{\^{}}+j and numbers c,, Cj, • • . , c{\^{}}{\^{}}{\^{}} such that PJ = {\{};'{\}} and c, = l/j for J = 1, 2, . . ., m whereas P{\^{}}+j = / and c{\^{}}+, {\textgreater}1. Our greedy heuristic returns J* = {\{}{\$}\backslash{\$},2, . . . , m{\}}, the winning ratio in iteration r being {\$}\backslash{\$}P{\^{}}+i-r{\$}\backslash{\$}/Cm-{\textgreater}-i{\~{}}r = f{\^{}}+ 1 -'' • The cost of 7* is m ,},
archivePrefix = {arXiv},
arxivId = {arXiv:0910.4284v1},
author = {Chvatal, V},
doi = {10.1287/moor.4.3.233},
eprint = {arXiv:0910.4284v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Chvatal - 1979 - a Greedy Heuristic for the Set-Covering Problem.pdf:pdf},
isbn = {0364765X},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
number = {3},
pages = {1973--1976},
title = {{a Greedy Heuristic for the Set-Covering Problem*}},
volume = {4},
year = {1979}
}
@inproceedings{shah2012rumor,
author = {Shah, Devavrat and Zaman, Tauhid},
booktitle = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
organization = {ACM},
pages = {199--210},
title = {{Rumor centrality: a universal source detector}},
volume = {40},
year = {2012}
}
@inproceedings{meirom2015,
author = {Meirom, Eli A and Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay and Orda, Ariel},
booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS' 15)},
number = {1},
organization = {ACM},
pages = {441--442},
title = {{Localized epidemic detection in networks with overwhelming noise}},
volume = {43},
year = {2015}
}
@article{Abrahao2013,
abstract = {The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infection times as epidemics spread through the network. This problem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed patterns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or computational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot. We deﬁne the trace complexity as the number of distinct traces required to achieve high ﬁdelity in reconstructing the topology of the unobserved network or, more generally, some of its properties. We give algorithms that are competitive with, while being simpler and more efﬁcient than, existing network inference approaches. Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the number of traces that an optimal inference algorithm requires for performing this task in the general case. Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution without inferring the network. We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.2954v1},
author = {Abrahao, Bruno and Chierichetti, Flavio and Kleinberg, Robert and Panconesi, Alessandro},
doi = {10.1145/2487575.2487664},
eprint = {arXiv:1308.2954v1},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/trace complexity of network inference.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
pages = {491},
title = {{Trace complexity of network inference}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2487664},
year = {2013}
}
@article{Khim2017,
abstract = {We formulate and analyze a hypothesis testing problem for inferring the edge structure of an infection graph. Our model is as follows: A disease spreads over a network via contagion and random infection, where uninfected nodes contract the disease at a time corresponding to an independent exponential random variable and infected nodes transmit the disease to uninfected neighbors according to independent exponential random variables with an unknown rate parameter. A subset of nodes is also censored, meaning the infection statuses of the nodes are unobserved. Given the statuses of all nodes in the network, the goal is to determine the underlying graph. Our procedure consists of a permutation test, and we derive a condition in terms of automorphism groups of the graphs corresponding to the null and alternative hypotheses that ensures the validity of our test. Notably, the permutation test does not involve estimating unknown parameters governing the infection process; instead, it leverages differences in the topologies of the null and alternative graphs. We derive risk bounds for our testing procedure in settings of interest; provide extensions to situations involving relaxed versions of the algebraic condition; and discuss multiple observations of infection spreads. We conclude with experiments validating our results.},
archivePrefix = {arXiv},
arxivId = {1705.07997},
author = {Khim, Justin and Loh, Po-Ling},
eprint = {1705.07997},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/ permutation test for infection graph.pdf:pdf},
pages = {1--28},
title = {{Permutation Tests for Infection Graphs}},
url = {http://arxiv.org/abs/1705.07997},
year = {2017}
}
@article{Jin2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.10325v1},
author = {Jin, Yan and Mossel, Elchanan and Ramnarayan, Govind},
eprint = {arXiv:1809.10325v1},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/corrupt-clever.pdf:pdf},
pages = {1--21},
title = {{Being Corrupt Requires Being Clever , But Detecting Corruption Doesn ' t}},
year = {2018}
}
@article{Guo2008,
author = {Guo, Yuhong and Schuurmans, Dale},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/convex relaxation latent variable.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems 20},
pages = {1--8},
title = {{Convex Relaxations of Latent Variable Training}},
year = {2008}
}
@article{Springer2012,
author = {Springer, Theresa and Urban, Karsten},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/comparison EM and alternatives.pdf:pdf},
keywords = {47j25,62f99,62p30,65c60,and phrases,convergence,em algorithm,gradient method,method,msc,newton,s,tracking},
number = {Ml},
title = {{Comparison of the EM Algorithm and Alternatives.pdf}},
year = {2012}
}
@article{Kilbertus2017,
abstract = {Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about the causal data generating process?" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.},
archivePrefix = {arXiv},
arxivId = {1706.02744},
author = {Kilbertus, Niki and Rojas-Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
eprint = {1706.02744},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/avoiding discrimination through causal reasoning.pdf:pdf},
issn = {10495258},
number = {Nips},
pages = {1--15},
title = {{Avoiding Discrimination through Causal Reasoning}},
url = {http://arxiv.org/abs/1706.02744},
year = {2017}
}
@article{Maayan2010,
abstract = {This Teaching Resource provides lecture notes, slides, and a problem set for a set of three lectures from a course entitled “Systems Biology: Biomedical Modeling.” The materials are from three separate lectures introducing applications of graph theory and network analysis in systems biology. The first lecture describes different types of intracellular networks, methods for constructing biological networks, and different types of graphs used to represent regulatory intracellular networks. The second lecture surveys milestones and key concepts in network analysis by introducing topological measures, random networks, growing network models, and topological observations from molecular biological systems abstracted to networks. The third lecture discusses methods for analyzing lists of genes and experimental data in the context of prior knowledge networks to make predictions. Introduction},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ma'ayan, Avi},
doi = {10.1126/scisignal.2001965.Introduction},
eprint = {NIHMS150003},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/GRN/network analysis in biology.pdf:pdf},
isbn = {2040-3437 (Electronic)$\backslash$r1367-6733 (Linking)},
issn = {2040-3437},
journal = {Current opinion in drug discovery {\&} development},
keywords = {cell signaling,computational biology,gene regulatory networks,graph theory,interactome,networks,pathway analysis},
number = {3},
pages = {297--309},
pmid = {20443163},
title = {{Introduction to Network Analysis in Systems Biology}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20443163{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3068535},
volume = {13},
year = {2010}
}
@article{Khim2018,
abstract = {We study the problem of parameter estimation based on infection data from an epidemic out-break on a graph. We assume that successive infections occur via contagion; i.e., transmissions can only spread across existing directed edges in the graph. Our stochastic spreading model allows individual nodes to be infected more than once, and the probability of the transmission spreading across a particular edge is proportional to both the cumulative number of times the source nodes has been infected in previous stages of the epidemic and the weight parameter of the edge. We propose a maximum likelihood estimator for inferring the unknown edge weights when full information is available concerning the order and identity of successive edge transmis-sions. When the weights take a particular form as exponential functions of a linear combination of known edge covariates, we show that maximum likelihood estimation amounts to optimizing a convex function, and produces a solution that is both consistent and asymptotically normal. Our proofs are based on martingale convergence theorems and the theory of weighted P{\'{o}}lya urns. We also show how our theory may be generalized to settings where the weights are not exponential. Finally, we analyze the case where the available infection data comes in the form of an unordered set of edge transmissions. We propose two algorithms for weight parameter estimation in this setting and derive corresponding theoretical guarantees. Our methods are validated using both synthetic data and real-world data from the Ebola spread in West Africa.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.05273v1},
author = {Khim, Justin and Loh, Po-Ling},
eprint = {arXiv:1806.05273v1},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/Epidemics/max-likelihood for weighted infection graphs.pdf:pdf},
pages = {1--47},
title = {{A theory of maximum likelihood for weighted infection graphs}},
url = {https://arxiv.org/pdf/1806.05273.pdf},
year = {2018}
}
@article{Venkatakrishnan2017,
abstract = {Bitcoin and other cryptocurrencies have surged in popular- ity over the last decade. Although Bitcoin does not claim to provide anonymity for its users, it enjoys a public percep- tion of being a ‘privacy-preserving' financial system. In real- ity, cryptocurrencies publish users' entire transaction histo- ries in plaintext, albeit under a pseudonym; this is required for transaction validation. Therefore, if a user's pseudonym can be linked to their human identity, the privacy fallout can be significant. Recently, researchers have demonstrated deanonymization attacks that exploit weaknesses in the Bit- coin network's peer-to-peer (P2P) networking protocols. In particular, the P2P network currently forwards content in a structured way that allows observers to deanonymize users. In this work, we redesign the P2P network from first princi- ples with the goal of providing strong, provable anonymity guarantees. We propose a simple networking policy called Dandelion, which achieves nearly-optimal anonymity guar- antees at minimal cost to the network's utility. We also provide a practical implementation of Dandelion for de- ployment. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.07016v1},
author = {Venkatakrishnan, Shaileshh Bojja and Fanti, Giulia and Viswanath, Pramod},
doi = {10.1145/1235},
eprint = {arXiv:1603.07016v1},
file = {:Users/jessicahoffmann/Documents/UT/Research/Articles/dandelion.pdf:pdf},
isbn = {9781450321389},
issn = {16130073},
keywords = {Multimodal learning analytics,Multimodal teaching analytics,STEM education,Sensors,Smart classroom,Smart school},
pages = {53--59},
pmid = {214160309},
title = {{Dandelion: Redesigning the Bitcoin Network for Anonymity}},
volume = {1828},
year = {2017}
}
@article{dragomir2000some,
	title={Some Inequalities For The Kullback-Leibler And x$^2$- Distances In Information Theory And Applications},
	author={Dragomir, Sever S and Gluscevic, V},
	journal={RGMIA research report collection},
	volume={3},
	number={2},
	pages={199--210},
	year={2000},
	publisher={School of Communications and Informatics, Faculty of Engineering and Science~…}
}
@article{newman1960double,
	title={The double dixie cup problem},
	author={Newman, Donald J},
	journal={The American Mathematical Monthly},
	volume={67},
	number={1},
	pages={58--61},
	year={1960},
	publisher={JSTOR}
}
@article{erdHos1961classical,
	title={On a classical problem of probability theory},
	author={Erd{\H{o}}s, Paul},
	year={1961},
	publisher={Citeseer}
}

@article{sridhar2019sequential,
	title={Sequential Estimation of Network Cascades},
	author={Sridhar, Anirudh and Poor, H Vincent},
	journal={arXiv preprint arXiv:1912.03800},
	year={2019}
}

@inproceedings{dong2019multiple,
	title={Multiple Rumor Source Detection with Graph Convolutional Networks},
	author={Dong, Ming and Zheng, Bolong and Quoc Viet Hung, Nguyen and Su, Han and Li, Guohui},
	booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	pages={569--578},
	year={2019}
}

@article{liu2019ct,
	title={CT LIS: Learning Influences and Susceptibilities through Temporal Behaviors},
	author={Liu, Shenghua and Shen, Huawei and Zheng, Houdong and Cheng, Xueqi and Liao, Xiangwen},
	journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
	volume={13},
	number={6},
	pages={1--21},
	year={2019},
	publisher={ACM New York, NY, USA}
}

@article{kolli2019influence,
	title={Influence Maximization From Cascade Information Traces in Complex Networks in the Absence of Network Structure},
	author={Kolli, Naimisha and Narayanaswamy, Balakrishnan},
	journal={IEEE Transactions on Computational Social Systems},
	volume={6},
	number={6},
	pages={1147--1155},
	year={2019},
	publisher={IEEE}
}

@inproceedings{wang2019analysis,
	title={Analysis of Antagonistic Dynamics for Rumor Propagation},
	author={Wang, Shengling and Chen, Shasha and Cheng, Xiuzhen and Lv, Weifeng and Yu, Jiguo},
	booktitle={2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)},
	pages={1253--1263},
	year={2019},
	organization={IEEE}
}

@article{yan2019conquering,
	title={Conquering the Worst Case of Infections in Networks},
	author={Yan, Wen and Loh, Po-Ling and Li, Chunguo and Huang, Yongming and Yang, Luxi},
	journal={IEEE Access},
	year={2019},
	publisher={IEEE}
}

@misc{ou2019screen,
	title={Who and When to Screen: Multi-Round Active Screening for Recurrent Infectious Diseases Under Uncertainty},
	author={Han-Ching Ou and Arunesh Sinha and Sze-Chuan Suen and Andrew Perrault and Milind Tambe},
	year={2019},
	eprint={1903.06113},
	archivePrefix={arXiv},
	primaryClass={q-bio.QM}
}

@inproceedings{xie2019meta,
	title={Meta Learning with Relational Information for Short Sequences},
	author={Xie, Yujia and Jiang, Haoming and Liu, Feng and Zhao, Tuo and Zha, Hongyuan},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9901--9912},
	year={2019}
}

@inproceedings{prokhorenkova2019learning,
	title={Learning clusters through information diffusion},
	author={Prokhorenkova, Liudmila and Tikhonov, Alexey and Litvak, Nelly},
	booktitle={The World Wide Web Conference},
	pages={3151--3157},
	year={2019}
}

@article{pasdeloup2017characterization,
	title={Characterization and inference of graph diffusion processes from observations of stationary signals},
	author={Pasdeloup, Bastien and Gripon, Vincent and Mercier, Gr{\'e}goire and Pastor, Dominique and Rabbat, Michael G},
	journal={IEEE transactions on Signal and Information Processing over Networks},
	volume={4},
	number={3},
	pages={481--496},
	year={2017},
	publisher={IEEE}
}

@article{Li2018,
abstract = {Mixtures of Linear Regressions (MLR) is an important mixture model with many applications. In this model, each observation is generated from one of the several unknown linear regression components, where the identity of the generated component is also unknown. Previous works either assume strong assumptions on the data distribution or have high complexity. This paper proposes a fixed parameter tractable algorithm for the problem under general conditions, which achieves global convergence and the sample complexity scales nearly linearly in the dimension. In particular, different from previous works that require the data to be from the standard Gaussian, the algorithm allows the data from Gaussians with different covariances. When the conditional number of the covariances and the number of components are fixed, the algorithm has nearly optimal sample complexity {\$}N = \backslashtilde{\{}O{\}}(d){\$} as well as nearly optimal computational complexity {\$}\backslashtilde{\{}O{\}}(Nd){\$}, where {\$}d{\$} is the dimension of the data space. To the best of our knowledge, this approach provides the first such recovery guarantee for this general setting.},
archivePrefix = {arXiv},
arxivId = {1802.07895},
author = {Li, Yuanzhi and Liang, Yingyu},
eprint = {1802.07895},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Li, Liang - 2018 - Learning Mixtures of Linear Regressions with Nearly Optimal Complexity.pdf:pdf},
title = {{Learning Mixtures of Linear Regressions with Nearly Optimal Complexity}},
url = {http://arxiv.org/abs/1802.07895},
year = {2018}
}
@article{Fanti2014,
abstract = {Anonymous messaging platforms, such as Secret and Whisper, have emerged as important social media for sharing one's thoughts without the fear of being judged by friends, family, or the public. Further, such anonymous platforms are crucial in nations with authoritarian governments; the right to free expression and sometimes the personal safety of the author of the message depend on anonymity. Whether for fear of judgment or personal endangerment, it is crucial to keep anonymous the identity of the user who initially posted a sensitive message. In this paper, we consider an adversary who observes a snapshot of the spread of a message at a certain time. Recent advances in rumor source detection shows that the existing messaging protocols are vulnerable against such an adversary. We introduce a novel messaging protocol, which we call adaptive diffusion, and show that it spreads the messages fast and achieves a perfect obfuscation of the source when the underlying contact network is an infinite regular tree: all users with the message are nearly equally likely to have been the origin of the message. Experiments on a sampled Facebook network show that it effectively hides the location of the source even when the graph is finite, irregular and has cycles. We further consider a stronger adversarial model where a subset of colluding users track the reception of messages. We show that the adaptive diffusion provides a strong protection of the anonymity of the source even under this scenario.},
archivePrefix = {arXiv},
arxivId = {1412.8439},
author = {Fanti, Giulia and Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
doi = {10.1145/2745844.2745866},
eprint = {1412.8439},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Fanti et al. - 2014 - Spy vs. Spy Rumor Source Obfuscation.pdf:pdf},
isbn = {9781450334860},
issn = {01635999},
journal = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS' 14)},
keywords = {Reviewed,anonymous social media,privacy,rumor spreading},
mendeley-tags = {Reviewed},
pages = {271--284},
title = {{Spy vs. Spy: Rumor Source Obfuscation}},
url = {http://arxiv.org/abs/1412.8439},
year = {2015}
}
@article{Parikh2014,
abstract = {Thismonograph is about a class of optimization algorithms called prox- imal algorithms.Much like Newton's method is a standard tool for solv- ing unconstrained smooth optimization problems of modest size, proxi- mal algorithms can be viewed as an analogous tool for nonsmooth, con- strained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical al- gorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed- form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpreta- tions of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
archivePrefix = {arXiv},
arxivId = {1502.03175},
author = {Parikh, Neal},
doi = {10.1561/2400000003},
eprint = {1502.03175},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Parikh - 2014 - Proximal Algorithms.pdf:pdf},
isbn = {9781601987167},
issn = {2167-3888},
journal = {Foundations and Trends{\textregistered} in Optimization},
number = {3},
pages = {127--239},
title = {{Proximal Algorithms}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-optimization/OPT-003},
volume = {1},
year = {2014}
}
@article{Hoffmann2018,
author = {Hoffmann, Jessica and Caramanis, Constantine},
doi = {10.1145/3219617.3219622},
file = {:Users/jessicahoffmann/Downloads/pomacs31-hoffmann.pdf:pdf},
isbn = {978-1-4503-5846-0},
journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems (SIGMETRICS' 18)},
keywords = {contact process on graph,contagion,controlled SI model,partial information,time to extinction},
number = {2},
pages = {11--13},
title = {{The Cost of Uncertainty in Curing Epidemics}},
url = {http://doi.acm.org/10.1145/3219617.3219622},
volume = {2},
year = {2018}
}
@inproceedings{Daneshmand2014,
abstract = {Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees? Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an {\$}l{\_}1{\$}-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe {\$}O(d{\^{}}3 \backslashlog N){\$} cascades, where {\$}d{\$} is the maximum number of parents of a node and {\$}N{\$} is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice.},
archivePrefix = {arXiv},
arxivId = {1405.2936},
author = {Daneshmand, Hadi and Gomez-Rodriguez, Manuel and Song, Le and Schoelkopf, Bernhard},
booktitle = {Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014 - ICML 14},
doi = {10.1002/aur.1474.Replication},
eprint = {1405.2936},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Daneshmand et al. - 2014 - Estimating Diffusion Network Structures Recovery Conditions, Sample Complexity {\&} Soft-thresholding Algorithm.pdf:pdf},
isbn = {9781634393973},
issn = {15337928},
pmid = {25932466},
title = {{Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity {\&} Soft-thresholding Algorithm}},
url = {http://arxiv.org/abs/1405.2936},
year = {2014}
}
@article{Davenport2016,
abstract = {Low-rank matrices play a fundamental role in modeling and computational methods for signal processing and machine learning. In many applications where low-rank matrices arise, these matrices cannot be fully sampled or directly observed, and one encounters the problem of recovering the matrix given only incomplete and indirect observations. This paper provides an overview of modern techniques for exploiting low-rank structure to perform matrix recovery in these settings, providing a survey of recent advances in this rapidly-developing field. Specific attention is paid to the algorithms most commonly used in practice, the existing theoretical guarantees for these algorithms, and representative practical applications of these techniques.},
archivePrefix = {arXiv},
arxivId = {1601.06422},
author = {Davenport, Mark A. and Romberg, Justin},
doi = {10.1109/JSTSP.2016.2539100},
eprint = {1601.06422},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Davenport, Romberg - 2016 - An Overview of Low-Rank Matrix Recovery from Incomplete Observations.pdf:pdf},
issn = {19324553},
journal = {IEEE Journal on Selected Topics in Signal Processing},
number = {4},
pages = {608--622},
title = {{An Overview of Low-Rank Matrix Recovery from Incomplete Observations}},
volume = {10},
year = {2016}
}
@article{Lee2001,
author = {Lee, Sang-gu and Seol, Han-guk},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Seol - 2001 - A survey on the matrix completion problem.pdf:pdf},
journal = {Trends in Mathematics},
keywords = {2000,and phrases,c 2001 information center,for mathematical sciences,matrix completion,partial matrix,positive definite,received august 30},
number = {1},
pages = {38--43},
title = {{A survey on the matrix completion problem}},
volume = {4},
year = {2001}
}
@article{Lovasz1993,
abstract = {This paper we'll formulate the results in terms of random walks, and mostly restrict our attention to the undirected case. 2 L. Lov'asz},
author = {Lov{\'{a}}sz, L},
doi = {10.1.1.39.2847},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lov{\'{a}}sz - 1993 - Random walks on graphs A survey.pdf:pdf},
isbn = {9789638022738},
issn = {03044149},
journal = {Combinatorics Paul Erdos is Eighty},
number = {Volume 2},
pages = {1--46},
title = {{Random walks on graphs: A survey}},
url = {http://www.cs.yale.edu/publications/techreports/tr1029.pdf},
volume = {2},
year = {1993}
}
@article{Bertsimas2010,
abstract = {In this paper we survey the primary research, both theoretical and applied, in the area of Robust Optimization (RO). Our focus is on the computational attractiveness of RO approaches, as well as the modeling power and broad applicability of the methodology. In addition to surveying prominent theoretical results of RO, we also present some recent results linking RO to adaptable models for multi-stage decision-making problems. Finally, we highlight applications of RO across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering.},
archivePrefix = {arXiv},
arxivId = {1010.5445},
author = {Bertsimas, Dimitris and Brown, David B. and Caramanis, Constantine},
doi = {10.1137/080734510},
eprint = {1010.5445},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bertsimas, Brown, Caramanis - 2010 - Theory and Applications of Robust Optimization.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
keywords = {adaptable optimization,applications of robust op-,robust optimization,robustness},
pmid = {20879563},
title = {{Theory and Applications of Robust Optimization}},
url = {http://arxiv.org/abs/1010.5445},
year = {2010}
}
@article{Settles,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
author = {Settles, Burr},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Settles - Unknown - Active Learning Literature Survey.pdf:pdf},
title = {{Active Learning Literature Survey}}
}
@article{Balcan2009,
abstract = {We state and analyze the first active learning algorithm that finds an $\epsilon${\{}lunate{\}}-optimal hypothesis in any hypothesis class, when the underlying distribution has arbitrary forms of noise. The algorithm, A2 (for Agnostic Active), relies only upon the assumption that it has access to a stream of unlabeled examples drawn i.i.d. from a fixed distribution. We show that A2 achieves an exponential improvement (i.e., requires only O (ln frac(1, $\epsilon${\{}lunate{\}})) samples to find an $\epsilon${\{}lunate{\}}-optimal classifier) over the usual sample complexity of supervised learning, for several settings considered before in the realizable case. These include learning threshold classifiers and learning homogeneous linear separators with respect to an input distribution which is uniform over the unit sphere. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Balcan, Maria Florina and Beygelzimer, Alina and Langford, John},
doi = {10.1016/j.jcss.2008.07.003},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Balcan, Beygelzimer, Langford - 2009 - Agnostic active learning.pdf:pdf},
isbn = {1595933832},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {Active learning,Agnostic setting,Linear separators,Sample complexity},
number = {1},
pages = {78--89},
title = {{Agnostic active learning}},
volume = {75},
year = {2009}
}
@article{,
title = {{Imported from Combinatorial Pure Exploration with Continuous and Separable Reward Functions and Its Applications (Extended Version) http://arxiv.org/abs/1805.01685v1}}
}
@article{Phillips,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.11284v1},
author = {Phillips, Jeff M and Tang, Pingfan},
eprint = {arXiv:1804.11284v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Phillips, Tang - Unknown - A Data-Dependent Distance for Regression.pdf:pdf},
keywords = {4230,and phrases data-dependent distance,digital object identifier 10,leverage score,lipics,sensitivity,trajectories},
pages = {1--24},
title = {{A Data-Dependent Distance for Regression}}
}
@article{Prasad2018,
abstract = {We provide a new computationally-efficient class of estimators for risk minimization. We show that these estimators are robust for general statistical models: in the classical Huber epsilon-contamination model and in heavy-tailed settings. Our workhorse is a novel robust variant of gradient descent, and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem. We provide specific consequences of our theory for linear regression, logistic regression and for estimation of the canonical parameters in an exponential family. These results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models. Finally, we study the empirical performance of our proposed methods on synthetic and real datasets, and find that our methods convincingly outperform a variety of baselines.},
archivePrefix = {arXiv},
arxivId = {1802.06485},
author = {Prasad, Adarsh and Suggala, Arun Sai and Balakrishnan, Sivaraman and Ravikumar, Pradeep},
eprint = {1802.06485},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Prasad et al. - 2018 - Robust Estimation via Robust Gradient Estimation(2).pdf:pdf},
number = {1},
pages = {1--48},
title = {{Robust Estimation via Robust Gradient Estimation}},
url = {http://arxiv.org/abs/1802.06485},
year = {2018}
}
@article{Diakonikolas2018,
abstract = {In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, possesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires computing the top singular vector of a certain {\$}n \backslashtimes d{\$} matrix. We apply Sever on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the spam dataset, with {\$}1\backslash{\%}{\$} corruptions, we achieved {\$}7.4\backslash{\%}{\$} test error, compared to {\$}13.4\backslash{\%}-20.5\backslash{\%}{\$} for the baselines, and {\$}3\backslash{\%}{\$} error on the uncorrupted dataset. Similarly, on the drug design dataset, with {\$}10\backslash{\%}{\$} corruptions, we achieved {\$}1.42{\$} mean-squared error test error, compared to {\$}1.51{\$}-{\$}2.33{\$} for the baselines, and {\$}1.23{\$} error on the uncorrupted dataset.},
archivePrefix = {arXiv},
arxivId = {1803.02815},
author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M. and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
eprint = {1803.02815},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas et al. - 2018 - Sever A Robust Meta-Algorithm for Stochastic Optimization.pdf:pdf},
pages = {1--32},
title = {{Sever: A Robust Meta-Algorithm for Stochastic Optimization}},
url = {http://arxiv.org/abs/1803.02815},
year = {2018}
}
@inproceedings{Bhatia2015,
abstract = {We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X $\backslash$in R{\^{}}{\{}p x n{\}} and an underlying model w*, the response vector is generated as y = X'w* + b where b $\backslash$in R{\^{}}n is the corruption vector supported over at most C.n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X. In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* $\backslash$in R{\^{}}p. Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40{\%} corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20x faster than the best L1 solver.},
archivePrefix = {arXiv},
arxivId = {1506.02428},
author = {Bhatia, Kush and Jain, Prateek and Kar, Purushottam},
booktitle = {29th Annual Conf. on Neural Information Processing Systems (NIPS) 2015},
eprint = {1506.02428},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bhatia, Jain, Kar - 2015 - Robust Regression via Hard Thresholding.pdf:pdf},
issn = {10495258},
number = {1},
pages = {1--24},
title = {{Robust Regression via Hard Thresholding}},
url = {http://arxiv.org/abs/1506.02428},
year = {2015}
}
@article{Studer2012,
abstract = {We investigate the recovery of signals exhibiting a sparse representation in a general (i.e., possibly redundant or incomplete) dictionary that are corrupted by additive noise admitting a sparse representation in another general dictionary. This setup covers a wide range of applications, such as image inpainting, super-resolution, signal separation, and recovery of signals that are impaired by, e.g., clipping, impulse noise, or narrowband interference. We present deterministic recovery guarantees based on a novel uncertainty relation for pairs of general dictionaries and we provide corresponding practicable recovery algorithms. The recovery guarantees we find depend on the signal and noise sparsity levels, on the coherence parameters of the involved dictionaries, and on the amount of prior knowledge about the signal and noise support sets.},
archivePrefix = {arXiv},
arxivId = {1102.1621},
author = {Studer, Christoph and Kuppinger, Patrick and Pope, Graeme and B{\"{o}}lcskei, Helmut},
doi = {10.1109/TIT.2011.2179701},
eprint = {1102.1621},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Studer et al. - 2012 - Recovery of sparsely corrupted signals.pdf:pdf},
isbn = {9781457705946},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Coherence-based recovery guarantees,greedy algorithms,signal restoration,signal separation,uncertainty relations,ℓ1-norm minimization},
number = {5},
pages = {3115--3130},
title = {{Recovery of sparsely corrupted signals}},
volume = {58},
year = {2012}
}
@article{Garivier2016,
abstract = {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the 'Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.},
author = {Garivier, Aur{\'{e}}lien and {Kaufmann EMILIEKAUFMANN}, Emilie},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Garivier, Kaufmann EMILIEKAUFMANN - 2016 - Optimal Best Arm Identification with Fixed Confidence.pdf:pdf},
pages = {1--30},
title = {{Optimal Best Arm Identification with Fixed Confidence}},
volume = {49},
year = {2016}
}
@article{Chen,
abstract = {We study the combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a decision class, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.},
author = {Chen, Shouyuan and Lin, Tian and King, Irwin and Lyu, Michael R and Chen, Wei},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Combinatorial Pure Exploration of Multi-Armed Bandits.pdf:pdf},
title = {{Combinatorial Pure Exploration of Multi-Armed Bandits}}
}
@article{Diakonikolas2017,
abstract = {We study the problem of list-decodable (robust) Gaussian mean estimation and the related problem of learning mixtures of separated spherical Gaussians. In the former problem, we are given a set T of points in R n with the promise that an $\alpha$-fraction of points in T , where 0 {\textless} $\alpha$ {\textless} 1/2, are drawn from an unknown mean identity covariance Gaussian G, and no assumptions are made about the remaining points. The goal is to output a small list of candidate vectors with the guarantee that at least one of the candidates is close to the mean of G. In the latter problem, we are given samples from a k-mixture of spherical Gaussians on R n and the goal is to estimate the unknown model parameters up to small accuracy. We develop a set of techniques that yield new efficient algorithms with significantly improved guarantees for these problems. Specifically, our main contributions are as follows: List-Decodable Mean Estimation. Fix any d ∈ Z + and 0 {\textless} $\alpha$ {\textless} 1/2. We design an algorithm with sample complexity O d (poly(n d /$\alpha$)) and runtime O d (poly(n/$\alpha$) d) that outputs a list of O(1/$\alpha$) many candidate vectors such that with high probability one of the candidates is within ℓ 2 -distance O d ($\alpha$ −1/(2d)) from the mean of G. The only previous algorithm for this problem [CSV17] achieved erro O($\alpha$ −1/2) under second moment conditions. For d = O(1/$\epsilon$), where $\epsilon$ {\textgreater} 0 is a constant, our algorithm runs in polynomial time and achieves error O($\alpha$ $\epsilon$). For d = $\Theta$(log(1/$\alpha$)), our algorithm runs in time (n/$\alpha$) O(log(1/$\alpha$)) and achieves error O(log 3/2 (1/$\alpha$)), almost matching the information-theoretically optimal bound of $\Theta$(log 1/2 (1/$\alpha$)) that we establish. We also give a Statistical Query (SQ) lower bound suggesting that the complexity of our algorithm is qualitatively close to best possible. Learning Mixtures of Spherical Gaussians. We give a learning algorithm for mixtures of spherical Gaussians, with unknown spherical covariances, that succeeds under significantly weaker separation assumptions compared to prior work. For the prototypical case of a uniform k-mixture of identity co-variance Gaussians we obtain the following: For any $\epsilon$ {\textgreater} 0, if the pairwise separation between the means is at least Ω(k $\epsilon$ + log(1/$\delta$)), our algorithm learns the unknown parameters within accuracy $\delta$ with sample complexity and running time poly(n, 1/$\delta$, (k/$\epsilon$) 1/$\epsilon$). Moreover, our algorithm is robust to a small dimension-independent fraction of corrupted data. The previously best known polynomial time algorithm [VW02] required separation at least k 1/4 polylog(k/$\delta$). Finally, our algorithm works under separation o O(log 3/2 (k) + log(1/$\delta$)) with sample complexity and running time poly(n, 1/$\delta$, k log k). This bound is close to the information-theoretically minimum separation of Ω(√ log k) [RV17]. Our main technical contribution is a new technique, using degree-d multivariate polynomials, to remove outliers from high-dimensional datasets where the majority of the points are corrupted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.07211v1},
author = {Diakonikolas, Ilias and Kane, Daniel M and Stewart, Alistair},
eprint = {arXiv:1711.07211v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas, Kane, Stewart - 2017 - List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical Gaussians.pdf:pdf},
title = {{List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical Gaussians}},
year = {2017}
}
@article{Domingo2002,
abstract = {Scalability is a key requirement for any KDD and data mining algorithm, and one of the biggest research challenges is to develop methods that allow to use large amounts of data. One possible approach for dealing with huge amounts of data is to take a random sample and do data mining on it, since for many data mining applications approximate answers are acceptable. However, as argued by several researchers, random sampling is difficult to use due to the difficulty of determining an appropriate sample size. In this paper, we take a sequential sampling approach for solving this difficulty, and propose an adaptive sampling method that solves a general problem covering many actual problems arising in applications of discovery science. An algorithm following this method obtains examples sequentially in an on-line fashion, and it determines from the obtained examples whether it has already seen a large enough number of examples. Thus, sample size is not fixed a priori; instead, it adaptively depends on the situation. Due to this adaptiveness, if we are not in a worst case situation as fortunately happens in many practical applications, then we can solve the problem with a number of examples much smaller than required in the worst case. We prove the correctness of our method and estimates its efficiency theoretically. For illustrating its usefulness, we consider one concrete task requiring sampling, provide an algorithm based on our method, and show its efficiency experimentally.},
author = {Domingo, Carlos and Gaval, Ricard},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Domingo, Gaval - 2002 - Adaptive Sampling Methods for Scaling Up Knowledge Discovery Algorithms.pdf:pdf},
journal = {Data Mining and Knowledge Discovery},
pages = {131--152},
title = {{Adaptive Sampling Methods for Scaling Up Knowledge Discovery Algorithms}},
volume = {6},
year = {2002}
}
@article{MannorSHIE2006,
abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O (n/$\epsilon$ 2) log(1/$\delta$) times to find an $\epsilon$-optimal arm with probability of at least 1 − $\delta$. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over $\epsilon$-greedy Q-learning.},
author = {{Mannor SHIE}, Shie and {Mansour MANSOUR}, Yishay},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Mannor SHIE, Mansour MANSOUR - 2006 - Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning P.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1079--1105},
title = {{Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems * Eyal Even-Dar}},
volume = {7},
year = {2006}
}
@article{,
abstract = {A wide range of learning tasks require human input in labeling massive data. The collected data though are usually low quality and contain inaccuracies and errors. As a result, modern science and business face the problem of learning from unreliable data sets. In this work, we provide a generic approach that is based on verification of only few records of the data set to guarantee high quality learning outcomes for various optimization objectives. Our method, identifies small sets of critical records and verifies their validity. We show that many problems only need poly(1/$\epsilon$) verifications, to ensure that the output of the computation is at most a factor of (1 ± $\epsilon$) away from the truth. For any given instance, we provide an instance optimal solution that verifies the minimum possible number of records to approximately certify correctness. Then using this instance optimal formulation of the problem we prove our main result: " every function that satisfies some Lipschitz continuity condition can be certified with a small number of verifications " . We show that the required Lipschitz continuity condition is satisfied even by some NP-complete problems, which illustrates the generality and importance of this theorem. In case this certification step fails, an invalid record will be identified. Removing these records and repeating until success, guarantees that the result will be accurate and will depend only on the verified records. Surprisingly, as we show, for several computation tasks more efficient methods are possible. These methods always guarantee that the produced result is not affected by the invalid records, since any invalid record that affects the output will be detected and verified.},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2018 - Learning from Unreliable Datasets.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
keywords = {Lipschitz continuity,unreliable data set,verification},
pages = {1--23},
title = {{Learning from Unreliable Datasets}},
volume = {75},
year = {2018}
}
@article{Prasad2018a,
abstract = {We provide a new computationally-efficient class of estimators for risk minimization. We show that these estimators are robust for general statistical models: in the classical Huber epsilon-contamination model and in heavy-tailed settings. Our workhorse is a novel robust variant of gradient descent, and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem. We provide specific consequences of our theory for linear regression, logistic regression and for estimation of the canonical parameters in an exponential family. These results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models. Finally, we study the empirical performance of our proposed methods on synthetic and real datasets, and find that our methods convincingly outperform a variety of baselines.},
archivePrefix = {arXiv},
arxivId = {1802.06485},
author = {Prasad, Adarsh and Suggala, Arun Sai and Balakrishnan, Sivaraman and Ravikumar, Pradeep},
eprint = {1802.06485},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Prasad et al. - 2018 - Robust Estimation via Robust Gradient Estimation.pdf:pdf},
number = {1},
title = {{Robust Estimation via Robust Gradient Estimation}},
url = {http://arxiv.org/abs/1802.06485},
year = {2018}
}
@article{Charikar2016,
abstract = {The vast majority of theoretical results in machine learning and statistics assume that the available training data is a reasonably reliable reflection of the phenomena to be learned or estimated. Similarly, the majority of machine learning and statistical techniques used in practice are brittle to the presence of large amounts of biased or malicious data. In this work we consider two frameworks in which to study estimation, learning, and optimization in the presence of significant fractions of arbitrary data. The first framework, list-decodable learning, asks whether it is possible to return a list of answers, with the guarantee that at least one of them is accurate. For example, given a dataset of {\$}n{\$} points for which an unknown subset of {\$}\backslashalpha n{\$} points are drawn from a distribution of interest, and no assumptions are made about the remaining {\$}(1-\backslashalpha)n{\$} points, is it possible to return a list of {\$}\backslashoperatorname{\{}poly{\}}(1/\backslashalpha){\$} answers, one of which is correct? The second framework, which we term the semi-verified learning model, considers the extent to which a small dataset of trusted data (drawn from the distribution in question) can be leveraged to enable the accurate extraction of information from a much larger but untrusted dataset (of which only an {\$}\backslashalpha{\$}-fraction is drawn from the distribution). We show strong positive results in both settings, and provide an algorithm for robust learning in a very general stochastic optimization setting. This general result has immediate implications for robust estimation in a number of settings, including for robustly estimating the mean of distributions with bounded second moments, robustly learning mixtures of such distributions, and robustly finding planted partitions in random graphs in which significant portions of the graph have been perturbed by an adversary.},
archivePrefix = {arXiv},
arxivId = {1611.02315},
author = {Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory},
doi = {10.1145/3055399.3055491},
eprint = {1611.02315},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Charikar, Steinhardt, Valiant - 2016 - Learning from Untrusted Data.pdf:pdf},
isbn = {9781450345286},
issn = {07378017},
title = {{Learning from Untrusted Data}},
url = {http://arxiv.org/abs/1611.02315},
year = {2016}
}
@article{If2011,
author = {If, Bernoulli},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/If - 2011 - Useful Inequalities.pdf:pdf},
pages = {1--3},
title = {{Useful Inequalities}},
volume = {1},
year = {2011}
}
@article{Bertsimas2018,
abstract = {The last decade witnessed an explosion in the availability of data for operations research applications. Motivated by this growing availability, we propose a novel schema for utilizing data to design uncertainty sets for robust optimization using statistical hypothesis tests. The approach is flexible and widely applicable, and robust optimization problems built from our new sets are computationally tractable, both theoretically and practically. Furthermore, optimal solutions to these problems enjoy a strong, finite-sample probabilistic guarantee. $\backslash$edit{\{}We describe concrete procedures for choosing an appropriate set for a given application and applying our approach to multiple uncertain constraints. Computational evidence in portfolio management and queuing confirm that our data-driven sets significantly outperform traditional robust optimization techniques whenever data is available.}},
archivePrefix = {arXiv},
arxivId = {1401.0212},
author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
doi = {10.1007/s10107-017-1125-8},
eprint = {1401.0212},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bertsimas, Gupta, Kallus - 2018 - Data-driven robust optimization.pdf:pdf},
isbn = {1010701711258},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Chance-constraints,Data-driven optimization,Hypothesis testing,Robust optimization},
number = {2},
pages = {235--292},
title = {{Data-driven robust optimization}},
volume = {167},
year = {2018}
}
@article{Bertsimas2017,
abstract = {Sample average approximation (SAA) is a widely popular approach to data-driven decision-making under uncertainty. Under mild assumptions, SAA is both tractable and enjoys strong asymptotic performance guarantees. Similar guarantees, however, do not typically hold in finite samples. In this paper, we propose a modification of SAA, which we term Robust SAA, which retains SAA's tractability and asymptotic properties and, additionally, enjoys strong finite-sample performance guarantees. The key to our method is linking SAA, distributionally robust optimization, and hypothesis testing of goodness-of-fit. Beyond Robust SAA, this connection provides a unified perspective enabling us to characterize the finite sample and asymptotic guarantees of various other data-driven procedures that are based upon distributionally robust optimization. This analysis provides insight into the practical performance of these various methods in real applications. We present examples from inventory management and portfolio allocation, and demonstrate numerically that our approach outperforms other data-driven approaches in these applications.},
archivePrefix = {arXiv},
arxivId = {1408.4445},
author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
doi = {10.1007/s10107-017-1174-z},
eprint = {1408.4445},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bertsimas, Gupta, Kallus - 2017 - Robust sample average approximation.pdf:pdf},
isbn = {1010701711},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Conic programming,Data-driven optimization,Distributionally robust optimization,Goodness-of-fit testing,Inventory management,Portfolio allocation,Sample average approximation of stochastic optimiz},
pages = {1--66},
title = {{Robust sample average approximation}},
year = {2017}
}
@article{Duchi2016,
abstract = {We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.},
archivePrefix = {arXiv},
arxivId = {1610.02581},
author = {Duchi, John and Namkoong, Hongseok},
eprint = {1610.02581},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Duchi, Namkoong - 2016 - Variance-based regularization with convex objectives.pdf:pdf},
title = {{Variance-based regularization with convex objectives}},
url = {http://arxiv.org/abs/1610.02581},
year = {2016}
}
@article{Goel2018,
abstract = {We give the first provably efficient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially overlapping) patches. Additionally, our algorithm requires only mild conditions on the underlying distribution. We prove that our framework captures commonly used schemes from computer vision, including one-dimensional and two-dimensional " patch and stride " convolutions. Our algorithm– Convotron– is inspired by recent work applying isotonic regression to learning neural networks. Convotron uses a simple, iterative update rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as opposed to the realizable setting). In contrast to gradient descent, Convotron requires no special initialization or learning-rate tuning to converge to the global optimum. We also point out that learning one hidden convolutional layer with respect to a Gaussian distribution and just one disjoint patch P (the other patches may be arbitrary) is easy in the following sense: Convotron can efficiently recover the hidden weight vector by updating only in the direction of P .},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.02547v1},
author = {Goel, Surbhi and Klivans, Adam and Meka, Raghu},
eprint = {arXiv:1802.02547v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Goel, Klivans, Meka - 2018 - Learning One Convolutional Layer with Overlapping Patches.pdf:pdf},
number = {Icml},
title = {{Learning One Convolutional Layer with Overlapping Patches}},
url = {https://arxiv.org/pdf/1802.02547.pdf},
year = {2018}
}
@article{News2018,
author = {News, Fake and News, Fake and News, Fake and New, Fake and News, Fake and News, Fake and News, Fake and News, Fake and News, Fake and News, Fake},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/News et al. - 2018 - An Optimal Algorithm for Halting Fake News.pdf:pdf},
pages = {1--7},
title = {{An Optimal Algorithm for Halting Fake News}},
year = {2018}
}
@article{Minimization1979,
author = {Minimization, Alternating and Regression, Mixed Linear},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Minimization, Regression - 1979 - A . Appendix.pdf:pdf},
number = {0},
title = {{A . Appendix}},
year = {1979}
}
@article{Maurer2009,
abstract = {We give improved constants for data dependent and variance sensitive confidence bounds, called empirical Bernstein bounds, and extend these inequalities to hold uniformly over classes of functionswhose growth function is polynomial in the sample size n. The bounds lead us to consider sample variance penalization, a novel learning method which takes into account the empirical variance of the loss function. We give conditions under which sample variance penalization is effective. In particular, we present a bound on the excess risk incurred by the method. Using this, we argue that there are situations in which the excess risk of our method is of order 1/n, while the excess risk of empirical risk minimization is of order 1/sqrt/{\{}n{\}}. We show some experimental results, which confirm the theory. Finally, we discuss the potential application of our results to sample compression schemes.},
archivePrefix = {arXiv},
arxivId = {0907.3740},
author = {Maurer, Andreas and Pontil, Massimiliano},
eprint = {0907.3740},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Maurer, Pontil - 2009 - Empirical Bernstein Bounds and Sample Variance Penalization.pdf:pdf},
title = {{Empirical Bernstein Bounds and Sample Variance Penalization}},
url = {http://arxiv.org/abs/0907.3740},
year = {2009}
}
@article{Guo2015,
author = {Guo, Philip J},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Guo - 2015 - THE PH.D. GRIND A Ph.D. Student Memoir.pdf:pdf},
title = {{THE PH.D. GRIND A Ph.D. Student Memoir}},
year = {2015}
}
@article{Chen2014,
abstract = {We consider the mixed regression problem with two components, under adversarial and stochastic noise. We give a convex optimization formulation that provably recovers the true solution, and pro-vide upper bounds on the recovery errors for both arbitrary noise and stochastic noise settings. We also give matching minimax lower bounds (up to log factors), showing that under certain assump-tions, our algorithm is information-theoretically optimal. Our results represent the first tractable algorithm guaranteeing successful recovery with tight bounds on recovery errors and sample com-plexity.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.7006v1},
author = {Chen, Yudong and Yi, X and Caramanis, Constantine},
eprint = {arXiv:1312.7006v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Yi, Caramanis - 2014 - A Convex Formulation for Mixed Regression with Two Components Minimax Optimal Rates.pdf:pdf},
issn = {15337928},
journal = {Colt},
keywords = {()},
pages = {1--45},
title = {{A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates}},
url = {http://arxiv.org/abs/1312.7006},
volume = {35},
year = {2014}
}
@article{Kwak2010a,
abstract = {Twitter, a microblogging service less than three years old, com-mands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing. We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4, 262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-law follower distribution, a short effec-tive diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be sim-ilar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behav-ior and user participation. We have classified the trending topics based on the active period and the tweets and show that the ma-jority (over 85{\%}) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1, 000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet. To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it.},
author = {Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kwak et al. - 2010 - What is Twitter , a Social Network or a News Media.pdf:pdf},
keywords = {Degree of separation,Homophily,Influential,Information diffusion,Measurement Keywords Twitter,Online social network,PageRank,Reciprocity,Retweet},
title = {{What is Twitter, a Social Network or a News Media?}},
year = {2010}
}
@inproceedings{Volkova2017,
author = {Volkova, Svitlana and Shaffer, Kyle and Jang, Jin Yea and Hodas, Nathan Oken},
booktitle = {ACL},
title = {{Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter}},
year = {2017}
}
@article{Wu2016,
author = {Wu, Shu and Liu, Qiang and Liu, Yong and Wang, Liang and Tan, Tieniu},
journal = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4403--4404},
publisher = {AAAI Press},
series = {AAAI'16},
title = {{Information Credibility Evaluation on Social Media}},
url = {http://dl.acm.org/citation.cfm?id=3016387.3016615},
year = {2016}
}
@article{Wei2017,
archivePrefix = {arXiv},
arxivId = {1705.06031},
author = {Wei, Wei and Wan, Xiaojun},
eprint = {1705.06031},
journal = {CoRR},
title = {{Learning to Identify Ambiguous and Misleading News Headlines}},
url = {http://arxiv.org/abs/1705.06031},
volume = {abs/1705.0},
year = {2017}
}
@article{facebookpressrelease2,
author = {Weedon, Jen and Nuland, William and Stamos, Alex},
title = {{Information operations and facebook}},
url = {https://fbnewsroomus.files.wordpress.com/2017/04/facebook-and-information-operations-v1.pdf},
year = {2017}
}
@article{facebookpressrelease1,
author = {Mosseri, Adam},
journal = {Facebook press release},
title = {{News feed fyi: Showing more informative links in news feed}},
url = {https://newsroom.fb.com/news/2017/06/news-feed-fyi-showing-more-informative-links-in-news-feed/},
year = {2017}
}
@article{Ciampaglia2015,
abstract = {Traditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this approach is feasible with efficient computational techniques. We evaluate this approach by examining tens of thousands of claims related to history, entertainment, geography, and biographical information using a public knowledge graph extracted from Wikipedia. Statements independently known to be true consistently receive higher support via our method than do false ones. These findings represent a significant step toward scalable computational fact-checking methods that may one day mitigate the spread of harmful misinformation.},
archivePrefix = {arXiv},
arxivId = {1501.03471v1},
author = {Ciampaglia, Giovanni Luca and Shiralkar, Prashant and Rocha, Luis M. and Bollen, Johan and Menczer, Filippo and Flammini, Alessandro},
doi = {10.1371/journal.pone.0128193},
eprint = {1501.03471v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Ciampaglia et al. - 2015 - Computational fact checking from knowledge networks.pdf:pdf},
isbn = {19326203 (Electronic)},
issn = {19326203},
journal = {PLoS ONE},
number = {6},
pages = {1--13},
pmid = {26083336},
title = {{Computational fact checking from knowledge networks}},
volume = {10},
year = {2015}
}
@article{Balmas2012,
abstract = {This research assesses possible associations between viewing fake news (i.e., political satire) and attitudes of inefficacy, alienation, and cynicism toward political candidates. Using survey data collected during the 2006 Israeli election campaign, the study provides evidence for an indirect positive effect of fake news viewing in fostering the feelings of inefficacy, alienation, and cynicism, through the mediator variable of perceived realism of fake news. Within this process, hard news viewing serves as a moderator of the association between viewing fake news and their perceived realism. It was also demonstrated that perceived realism of fake news is stronger among individuals with high exposure to fake news and low exposure to hard news than among those with high exposure to both fake and hard news. Overall, this study contributes to the scientific knowledge regarding the influence of the interaction between various types of media use on political effects.},
annote = {doi: 10.1177/0093650212453600},
author = {Balmas, Meital},
doi = {10.1177/0093650212453600},
issn = {0093-6502},
journal = {Communication Research},
month = {jul},
number = {3},
pages = {430--454},
publisher = {SAGE Publications Inc},
title = {{When Fake News Becomes Real: Combined Exposure to Multiple News Sources and Political Attitudes of Inefficacy, Alienation, and Cynicism}},
url = {https://doi.org/10.1177/0093650212453600},
volume = {41},
year = {2012}
}
@inproceedings{Maasberg2018,
author = {Maasberg, Michele and Ayaburi, Emanuel and Liu, Charles Z},
booktitle = {HICSS '18 Proceedings of the 51st Hawaii International Conference on System Sciences},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Maasberg, Ayaburi, Liu - 2018 - Exploring the Propagation of Fake Cyber News An Experimental Approach.pdf:pdf},
isbn = {9780998133119},
title = {{Exploring the Propagation of Fake Cyber News : An Experimental Approach}},
volume = {9},
year = {2018}
}
@article{hotez2016texas,
author = {Hotez, Peter J},
journal = {PLoS medicine},
number = {10},
pages = {e1002153},
publisher = {Public Library of Science},
title = {{Texas and its measles epidemics}},
volume = {13},
year = {2016}
}
@article{ferrara2016rise,
author = {Ferrara, Emilio and Varol, Onur and Davis, Clayton and Menczer, Filippo and Flammini, Alessandro},
journal = {Communications of the ACM},
number = {7},
pages = {96--104},
publisher = {ACM},
title = {{The rise of social bots}},
volume = {59},
year = {2016}
}
@article{Silverman2016teenbalkans,
author = {Silverman, C and Alexander, L},
journal = {BuzzFeed},
title = {{How Teens In The Balkans Are Duping Trump Supporters With Fake News}},
url = {https://www.buzzfeed.com/craigsilverman/how-macedonia-became-a-global-hub-for-pro-trump- misinfo?utm{\_}term=.jxX7xRvNr0{\#}.pnj8qZRxWP},
year = {2016}
}
@misc{Lipton2017ICMLsoldout,
author = {Lipton, Zachary C},
booktitle = {Approximately Correct blog},
title = {{ICML 2018 Registrations Sell Out Before Submission Deadline}},
url = {http://approximatelycorrect.com/2017/11/09/icml-registrations-sell-out-before-submission-deadline/},
year = {2017}
}
@article{Silverman2016top20fakereal,
author = {Silverman, Craig},
journal = {BuzzFeed},
title = {{This analysis shows how viral fake election news stories outperformed real news on facebook}},
url = {https://www.buzzfeed.com/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook},
year = {2016}
}
@article{gu2017fake,
author = {Gu, L and Kropotov, V and Yarochkin, F},
journal = {pdf] Trend Micro},
pages = {1073547711--1497355570},
title = {{The fake news machine, how propagandists abuse the internet and manipulate the public}},
volume = {81},
year = {2017}
}
@inproceedings{markines2009social,
author = {Markines, Benjamin and Cattuto, Ciro and Menczer, Filippo},
booktitle = {Proceedings of the 5th International Workshop on Adversarial Information Retrieval on the Web},
organization = {ACM},
pages = {41--48},
title = {{Social spam detection}},
year = {2009}
}
@article{skyview2016threatdemocracy,
journal = {Sky Views},
title = {{Facebook's fake news threatens democracy}},
url = {https://news.sky.com/story/sky-views-democracy-burns-as-facebook-lets-fake-news-thrive-10652711},
year = {2016}
}
@article{Gottfried2016newsuse,
author = {Gottfried, Jeffrey and Shearer, Elisa},
journal = {Pew Research Center},
title = {{News Use Across Social Media Platforms 2016}},
url = {http://www.journalism.org/2016/05/26/news-use-across-social-media-platforms-2016/},
year = {2016}
}
@article{Anderson2017savvyteen,
author = {Anderson, J},
journal = {Quartz},
title = {{Even social media-savvy teens can't spot a fake news story.}},
url = {https://qz.com/927543/even-social-media-savvy-teens-cant-spot-a-fake-news-story/},
year = {2017}
}
@article{Ratkiewicz2010,
abstract = {Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information. In particular, microblogs have become crucial grounds on which public relations, marketing, and political battles are fought. We introduce an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining, visualizing, mapping, classifying, and modeling massive streams of public microblogging events. We describe a Web service that leverages this framework to track political memes in Twitter and help detect astroturfing, smear campaigns, and other misinformation in the context of U.S. political elections. We present some cases of abusive behaviors uncovered by our service. Finally, we discuss promising preliminary results on the detection of suspicious memes via supervised learning based on features extracted from the topology of the diffusion networks, sentiment analysis, and crowdsourced annotations.},
archivePrefix = {arXiv},
arxivId = {1011.3768},
author = {Ratkiewicz, Jacob and Conover, Michael and Meiss, Mark and Gon{\c{c}}alves, Bruno and Patil, Snehal and Flammini, Alessandro and Menczer, Filippo},
doi = {10.1145/1963192.1963301},
eprint = {1011.3768},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Ratkiewicz et al. - 2010 - Detecting and Tracking the Spread of Astroturf Memes in Microblog Streams.pdf:pdf},
isbn = {9781450306379},
issn = {978-1-4503-0637-9},
pages = {297--304},
title = {{Detecting and Tracking the Spread of Astroturf Memes in Microblog Streams}},
url = {http://arxiv.org/abs/1011.3768 http://dx.doi.org/10.1145/1963192.1963301},
year = {2010}
}
@article{Mustafaraj2010,
abstract = {Recently, all major search engines introduced a new fea- ture: real-time search results, embedded in the first page of organic search results. The content appearing in these results is pulled within minutes of its generation from the so-called real-time Web such as Twitter, blogs, and news websites. In this paper, we argue that in the context of political speech, this feature provides disproportionate ex- posure to personal opinions, fabricated content, unverified events, lies and misrepresentations that otherwise would not find their way in the first page, giving them the opportunity to spread virally. To support our argument we provide con- crete evidence from the recent Massachusetts (MA) senate race between Martha Coakley and Scott Brown, analyzing political community behavior on Twitter. In the process, we analyze the Twitter activity of those involved in exchanging messages, and we find that it is possible to predict their po- litical orientation and detect attacks launched on Twitter, based on behavioral patterns of activity.},
author = {Mustafaraj, E and Metaxas, Pt},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Mustafaraj, Metaxas - 2010 - From obscurity to prominence in minutes Political speech and real-time search.pdf:pdf},
journal = {WebSci10: Extending the Frontiers of Society On-Line},
keywords = {real-time web,social web,twitter,twitter-,us elections},
pages = {317},
title = {{From obscurity to prominence in minutes: Political speech and real-time search}},
url = {http://repository.wellesley.edu/computersciencefaculty/9/},
year = {2010}
}
@article{howell2013digital,
author = {Howell, Lee and Others},
journal = {WEF Report},
pages = {15--94},
title = {{Digital wildfires in a hyperconnected world}},
volume = {3},
year = {2013}
}
@article{Wang2017,
abstract = {Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.},
archivePrefix = {arXiv},
arxivId = {1705.00648},
author = {Wang, William Yang},
doi = {10.18653/v1/P17-2067},
eprint = {1705.00648},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Wang - 2017 - Liar, Liar Pants on Fire A New Benchmark Dataset for Fake News Detection.pdf:pdf},
isbn = {978-1-945626-76-0},
keywords = {FakeNews,READ},
mendeley-tags = {FakeNews,READ},
title = {{"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection}},
url = {http://arxiv.org/abs/1705.00648},
year = {2017}
}
@article{Torres2018,
author = {Torres, Russell R and Gerhart, Natalie and Negahban, Arash},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Torres, Gerhart, Negahban - 2018 - Combating fake news an investigation of information verification behaviors on social networking sites.pdf:pdf},
isbn = {9780998133119},
journal = {51st Hawaii International Conference on System Sciences (HICSS) 2018, Honolulu},
pages = {3976--3985},
title = {{Combating fake news: an investigation of information verification behaviors on social networking sites}},
url = {http://hdl.handle.net/10125/50387},
year = {2018}
}
@article{Reimche2018,
author = {Reimche, Roman},
doi = {10.13140/RG.2.2.35221.22243},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Reimche - 2018 - Comparison of the diffusion of real and fake news in social networks.pdf:pdf},
keywords = {Reviewed},
mendeley-tags = {Reviewed},
title = {{Comparison of the diffusion of real and fake news in social networks}},
year = {2018}
}
@inproceedings{Osatuyi2018,
author = {Osatuyi, Babajide and Hughes, Jerald},
booktitle = {Proceedings of the 51st Hawaii International Conference on System Sciences},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Osatuyi, Hughes - 2018 - A Tale of Two Internet News Platforms-Real vs . Fake An Elaboration Likelihood Model Perspective.pdf:pdf},
isbn = {9780998133119},
keywords = {diffusion,elaboration likelihood,fake news,information presentation,model,persuasion},
pages = {1--9},
title = {{A Tale of Two Internet News Platforms-Real vs . Fake : An Elaboration Likelihood Model Perspective}},
volume = {9},
year = {2018}
}
@inproceedings{Wu2016,
abstract = {The evolution of social media popularity exhibits rich tem-porality, i.e., popularities change over time at various levels of temporal granularity. This is influenced by temporal vari-ations of public attentions or user activities. For example, popularity patterns of street snap on Flickr are observed to depict distinctive fashion styles at specific time scales, such as season-based periodic fluctuations for Trench Coat or one-off peak in days for Evening Dress. However, this fact is often overlooked by existing research of popularity modeling. We present the first study to incorporate mul-tiple time-scale dynamics into predicting online popularity. We propose a novel computational framework in the paper, named Multi-scale Temporalization, for estimating popular-ity based on multi-scale decomposition and structural re-construction in a tensor space of user, post, and time by joint low-rank constraints. By considering the noise caused by context inconsistency, we design a data rearrangement step based on context aggregation as preprocessing to en-hance contextual relevance of neighboring data in the tensor space. As a result, our approach can leverage multiple lev-els of temporal characteristics and reduce the noise of data decomposition to improve modeling effectiveness. We eval-uate our approach on two large-scale Flickr image datasets with over 1.8 million photos in total, for the task of popu-larity prediction. The results show that our approach sig-nificantly outperforms state-of-the-art popularity prediction techniques, with a relative improvement of 10.9{\%}–47.5{\%} in terms of prediction accuracy.},
archivePrefix = {arXiv},
arxivId = {1801.05853},
author = {Wu, Bo and Cheng, Wen-Huang and Zhang, Yongdong and Mei, Tao},
booktitle = {MM '16 Proceedings of the 2016 ACM on Multimedia Conference},
doi = {10.1145/2964284.2964335},
eprint = {1801.05853},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Time Matters Multi-scale Temporalization of Social Media Popularity.pdf:pdf},
isbn = {978-1-4503-3603-1},
keywords = {NewsSpread,multi-scale temporal modeling,popularity prediction,social media popularity,tensor decomposition and reconstruction},
mendeley-tags = {NewsSpread},
pages = {1336--1344},
title = {{Time Matters: Multi-scale Temporalization of Social Media Popularity}},
url = {http://doi.acm.org/10.1145/2964284.2964335},
year = {2016}
}
@inproceedings{Mishra2016,
abstract = {Predicting popularity, or the total volume of information outbreaks, is an important subproblem for understanding collective behavior in networks. Each of the two main types of recent approaches to the problem, feature-driven and generative models, have desired qualities and clear limitations. This paper bridges the gap between these solutions with a new hybrid approach and a new performance benchmark. We model each social cascade with a marked Hawkes self-exciting point process, and estimate the content virality, memory decay, and user influence. We then learn a predictive layer for popularity prediction using a collection of cascade history. To our surprise, Hawkes process with a predictive overlay outperform recent feature-driven and generative approaches on existing tweet data [43] and a new public benchmark on news tweets. We also found that a basic set of user features and event time summary statistics performs competitively in both classification and regression tasks, and that adding point process information to the feature set further improves predictions. From these observations, we argue that future work on popularity prediction should compare across feature-driven and generative modeling approaches in both classification and regression tasks.},
archivePrefix = {arXiv},
arxivId = {1608.04862},
author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
booktitle = {CIKM '16 Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
doi = {10.1145/2983323.2983812},
eprint = {1608.04862},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Mishra, Rizoiu, Xie - 2016 - Feature Driven and Point Process Approaches for Popularity Prediction.pdf:pdf},
isbn = {9781450340731},
keywords = {cascade prediction,infor-,mation diffusion,self-exciting point process,social media},
title = {{Feature Driven and Point Process Approaches for Popularity Prediction}},
url = {http://arxiv.org/abs/1608.04862 http://dx.doi.org/10.1145/2983323.2983812},
year = {2016}
}
@inproceedings{Kwak2010,
abstract = {→ Beispiel f{\"{u}}r: Verwendung der Netzwerkanalyse zur Bestimmung des Anteils reziproker Beziehungen (hier Twitter); Implikation: viele einseitige Verbindungen + (nicht nachgewiesen:) Verbundenheit {\"{u}}ber kurze Pfaddistanzen die gruppen{\"{u}}bergreifende Verbreitung von Informationen},
archivePrefix = {arXiv},
arxivId = {0809.1869v1},
author = {Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
booktitle = {The International World Wide Web Conference Committee (IW3C2)},
doi = {10.1145/1772690.1772751},
eprint = {0809.1869v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kwak et al. - 2010 - What is Twitter , a Social Network or a News Media.pdf:pdf},
isbn = {9781605587998},
issn = {1932-8036},
keywords = {copyright is held by,degree of,homophily,influential,information diffusion,online social network,pagerank,reciprocity,retweet,separation,the international world wide,twitter,web conference com-},
pages = {1--10},
pmid = {70583450},
title = {{What is Twitter , a Social Network or a News Media?}},
year = {2010}
}
@article{Goel2015,
abstract = {Viral products and ideas are intuitively understood to grow through a person-to-persondiusion process analogous to the spread of an infectious disease; however, until recently it hasbeen prohibitively dicult to directly observe purportedly viral events, and thus to rigorouslyquantify or characterize their structural properties. Here we propose a formal measure of whatwe label $\backslash$structural virality" that interpolates between two extremes: content that gains itspopularity through a single, large broadcast, and that which grows through multiple generationswith any one individual directly responsible for only a fraction of the total adoption. We use thisnotion of structural virality to analyze a unique dataset of a billion diusion events on Twitter,including the propagation of news stories, videos, images, and petitions. We nd that the verylargest observed events nearly always exhibit high structural virality, providing some of the rstdirect evidence that many of the most popular products and ideas grow through person-to-persondiusion. However, medium-sized events|having thousands of adopters|exhibit surprisingstructural diversity, and regularly grow via both broadcast and viral mechanisms. We ndthat these empirical results are largely consistent with a simple contagion model characterizedby a low infection rate spreading on a scale-free network, reminiscent of previous work on thelong-term persistence of computer viruses},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07526v1},
author = {Goel, Sharad and Anderson, Ashton and Hofman, Jake and Watts, Duncan J.},
doi = {10.1287/mnsc.2015.2158},
eprint = {arXiv:1502.07526v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Goel et al. - 2015 - The Structural Virality of Online Diffusion.pdf:pdf},
isbn = {9781450314152},
issn = {0025-1909},
journal = {Management Science},
keywords = {2013,2014,2015,accepted november 26,by lorin hitt,diffusion,history,in advance july 22,information systems,published online in articles,received august 14,twitter,viral media},
number = {January 2018},
pages = {150722112809007},
pmid = {1000285845},
title = {{The Structural Virality of Online Diffusion}},
url = {http://pubsonline.informs.org/doi/10.1287/mnsc.2015.2158},
year = {2015}
}
@article{Cha2010,
abstract = {Directed links in social media could represent anything from intimate friendships to common interests, or even a passion for breaking news or celebrity gossip. Such directed links determine the flow of information and hence indicate a user's influence on others—a concept that is crucial in sociology and viral marketing. In this paper, using a large amount of data collected from Twit- ter, we present an in-depth comparison of three mea- sures of influence: indegree, retweets, and mentions. Based on these measures, we investigate the dynam- ics of user influence across topics and time. We make several interesting observations. First, popular users who have high indegree are not necessarily influential in terms of spawning retweets or mentions. Second, most influential users can hold significant influence over a variety of topics. Third, influence is not gained spon- taneously or accidentally, but through concerted effort such as limiting tweets to a single topic. We believe that these findings provide new insights for viral marketing and suggest that topological measures such as indegree alone reveals very little about the influence of a user.},
author = {Cha, Meeyoung and Haddadi, Hamed and Benevenuto, Fabricio and Gummadi, P Krishna},
doi = {10.1.1.167.192},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Cha et al. - 2010 - Measuring user influence in twitter The million follower fallacy.pdf:pdf},
isbn = {9781450304931},
issn = {1556-4029},
journal = {Icwsm},
number = {10-17},
pages = {30},
pmid = {20015166},
title = {{Measuring user influence in twitter: The million follower fallacy.}},
volume = {10},
year = {2010}
}
@inproceedings{Kumar,
author = {Kumar, Ravi},
booktitle = {KDD '10 Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kumar - Unknown - Dynamics of Conversations.pdf:pdf},
title = {{Dynamics of Conversations}}
}
@article{Bild2015,
abstract = {Most previous analysis of Twitter user behavior is focused on individual information cascades and the social followers graph. We instead study aggregate user behavior and the retweet graph with a focus on quantitative descriptions. We find that the lifetime tweet distribution is a type-II discrete Weibull stemming from a power law hazard function, the tweet rate distribution, although asymptotically power law, exhibits a lognormal cutoff over finite sample intervals, and the inter-tweet interval distribution is power law with exponential cutoff. The retweet graph is small-world and scale-free, like the social graph, but is less disassortative and has much stronger clustering. These differences are consistent with it better capturing the real-world social relationships of and trust between users. Beyond just understanding and modeling human communication patterns and social networks, applications for alternative, decentralized microblogging systems-both predicting real-word performance and detecting spam-are discussed.},
archivePrefix = {arXiv},
arxivId = {1402.2671},
author = {Bild, David R. and Liu, Yue and Dick, Robert P. and Mao, Z. Morley and Wallach, Dan S.},
doi = {10.1145/2700060},
eprint = {1402.2671},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bild et al. - 2015 - Aggregate Characterization of User Behavior in Twitter and Analysis of the Retweet Graph.pdf:pdf},
issn = {15335399},
journal = {ACM Transactions on Internet Technology},
number = {1},
pages = {1--24},
title = {{Aggregate Characterization of User Behavior in Twitter and Analysis of the Retweet Graph}},
url = {http://dl.acm.org/citation.cfm?doid=2745838.2700060},
volume = {15},
year = {2015}
}
@article{Brown2017,
abstract = {In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.},
archivePrefix = {arXiv},
arxivId = {1705.02955},
author = {Brown, Noam and Sandholm, Tuomas},
eprint = {1705.02955},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Sandholm - 2017 - Safe and Nested Subgame Solving for Imperfect-Information Games.pdf:pdf},
title = {{Safe and Nested Subgame Solving for Imperfect-Information Games}},
year = {2017}
}
@article{Baddeley2007,
abstract = {spatial point patterns},
author = {Baddeley, Adrian},
doi = {10.1007/3-540-38174-0},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Baddeley - 2007 - Spatial Point Processes and their Applications.pdf:pdf},
isbn = {9783540381747},
issn = {00018708},
journal = {Lecture Notes in Mathematics 1892},
pages = {1--75},
title = {{Spatial Point Processes and their Applications}},
volume = {3},
year = {2007}
}
@article{Reinhart2017,
abstract = {Self-exciting spatio-temporal point process models predict the rate of events as a function of space, time, and the previous history of events. These models naturally capture triggering and clustering behavior, and have been widely used in fields where spatio-temporal clustering of events is observed, such as earthquake modeling, infectious disease, and crime. In the past several decades, advances have been made in estimation, inference, simulation, and diagnostic tools for self-exciting point process models. In this review, I describe the basic theory, survey related estimation and inference techniques from each field, highlight several key applications, and suggest directions for future research.},
archivePrefix = {arXiv},
arxivId = {1708.02647},
author = {Reinhart, Alex},
eprint = {1708.02647},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Reinhart - 2017 - A Review of Self-Exciting Spatio-Temporal Point Processes and Their Applications.pdf:pdf},
keywords = {and phrases,conditional,epidemic-type aftershock sequence,hawkes process,intensity,stochastic declustering},
pages = {1--30},
title = {{A Review of Self-Exciting Spatio-Temporal Point Processes and Their Applications}},
url = {http://arxiv.org/abs/1708.02647},
year = {2017}
}
@inproceedings{Rizoiu2016,
abstract = {Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor - the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors - sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6{\%} from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.},
archivePrefix = {arXiv},
arxivId = {1602.06033},
author = {Rizoiu, Marian-Andrei and Xie, Lexing and Sanner, Scott and Cebrian, Manuel and Yu, Honglin and {Van Hentenryck}, Pascal},
booktitle = {WWW '16 Proceedings of the 26th International Conference on World Wide Web},
doi = {10.1145/3038912.3052650},
eprint = {1602.06033},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Rizoiu et al. - 2016 - Expecting to be HIP Hawkes Intensity Processes for Social Media Popularity.pdf:pdf},
isbn = {9781450349130},
issn = {16130073},
keywords = {Hawkes intensity process,item virality,popularity forecasting,popularity modeling,self-exciting processes},
pmid = {18056803},
title = {{Expecting to be HIP: Hawkes Intensity Processes for Social Media Popularity}},
url = {http://arxiv.org/abs/1602.06033 http://dx.doi.org/10.1145/3038912.3052650},
year = {2016}
}
@inproceedings{Kobayashi2016,
abstract = {Online social networking services allow their users to post content in the form of text, images or videos. The main mechanism driving content diffusion is the possibility for users to re-share the content posted by their social connections, which may then cascade across the system. A fundamental problem when studying information cascades is the possibility to develop sound mathematical models, whose parameters can be calibrated on empirical data, in order to predict the future course of a cascade after a window of observation. In this paper, we focus on Twitter and, in particular, on the temporal patterns of retweet activity for an original tweet. We model the system by Time-Dependent Hawkes process (TiDeH), which properly takes into account the circadian nature of the users and the aging of information. The input of the prediction model are observed retweet times and structural information about the underlying social network. We develop a procedure for parameter optimization and for predicting the future profiles of retweet activity at different time resolutions. We validate our methodology on a large corpus of Twitter data and demonstrate its systematic improvement over existing approaches in all the time regimes.},
archivePrefix = {arXiv},
arxivId = {1603.09449},
author = {Kobayashi, Ryota and Lambiotte, Renaud},
booktitle = {ICWSM 2016 The 10th International AAAI Conference on Web and Social Media},
eprint = {1603.09449},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kobayashi, Lambiotte - 2016 - TiDeH Time-Dependent Hawkes Process for Predicting Retweet Dynamics.pdf:pdf},
isbn = {9781577357582},
keywords = {Full Papers,NewsSpread,READ,Reviewed},
mendeley-tags = {NewsSpread,READ,Reviewed},
number = {ICWSM},
pages = {191--200},
title = {{TiDeH: Time-Dependent Hawkes Process for Predicting Retweet Dynamics}},
url = {http://arxiv.org/abs/1603.09449},
year = {2016}
}
@article{Zhao2015,
abstract = {Social networking websites allow users to create and share content. Big information cascades of post resharing can form as users of these sites reshare others' posts with their friends and followers. One of the central challenges in understanding such cascading behaviors is in forecasting information outbreaks, where a single post becomes widely popular by being reshared by many users. In this paper, we focus on predicting the final number of reshares of a given post. We build on the theory of self-exciting point processes to develop a statistical model that allows us to make accurate predictions. Our model requires no training or expensive feature engineering. It results in a simple and efficiently computable formula that allows us to answer questions, in real-time, such as: Given a post's resharing history so far, what is our current estimate of its final number of reshares? Is the post resharing cascade past the initial stage of explosive growth? And, which posts will be the most reshared in the future? We validate our model using one month of complete Twitter data and demonstrate a strong improvement in predictive accuracy over existing approaches. Our model gives only 15{\%} relative error in predicting final size of an average information cascade after observing it for just one hour.},
archivePrefix = {arXiv},
arxivId = {1506.02594},
author = {Zhao, Qingyuan and Erdogdu, Murat A. and He, Hera Y. and Rajaraman, Anand and Leskovec, Jure},
doi = {10.1145/2783258.2783401},
eprint = {1506.02594},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Zhao et al. - 2015 - SEISMIC A Self-Exciting Point Process Model for Predicting Tweet Popularity.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '15 )},
keywords = {READ,Reviewed,cascade prediction,contagion,information diffusion,point process,self-exciting,social media},
mendeley-tags = {READ,Reviewed},
title = {{SEISMIC: A Self-Exciting Point Process Model for Predicting Tweet Popularity}},
url = {http://arxiv.org/abs/1506.02594 http://dx.doi.org/10.1145/2783258.2783401},
year = {2015}
}
@article{Conroy2015,
author = {Conroy, Niall J and Conroy, Niall J and Rubin, Victoria L and Chen, Yimin},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Conroy et al. - 2015 - Automatic Deception Detection Methods for Finding Fake News.pdf:pdf},
keywords = {but critical challenge,deception detection research has,detect lies in text,four decades of,helped us learn about,how,online is a daunting,the findings show,well humans are able},
number = {October},
title = {{Automatic Deception Detection : Methods for Finding Fake News}},
year = {2015}
}
@book{Bounegru,
author = {Bounegru, Liliana and Gray, Jonathan and Venturini, Tommaso and Mauri, Michele},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bounegru et al. - Unknown - A Field Guide to Fake News and Other Information Disorders.pdf:pdf},
isbn = {9781107415324},
keywords = {FakeNews},
mendeley-tags = {FakeNews},
title = {{A Field Guide to "Fake News" and Other Information Disorders}}
}
@book{Zafarani2014,
abstract = {Social media mining is a rapidly growing new field. It is an interdis- ciplinary field at the crossroad of disparate disciplines deeply rooted in computer science and social sciences. There are an active community and a large body of literature about social media. The fast-growing interests and intensifying need to harness social media data require research and the development of tools for finding insights from big social media data. This book is one of the intellectual efforts to answer the novel challenges of social media. It is designed to enable students, researchers, and practi- tioners to acquire fundamental concepts and algorithms for social media mining.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zafarani, Reza and Abbasi, Mohammad Ali and Liu, Huan},
booktitle = {Social Media Mining: An Introduction},
doi = {10.1017/CBO9781139088510},
eprint = {arXiv:1011.1669v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Zafarani, Abbasi, Liu - 2014 - Social media mining An introduction.pdf:pdf},
isbn = {9781139088510},
issn = {1882-0875},
keywords = {Book},
mendeley-tags = {Book},
pages = {1--320},
pmid = {25246403},
title = {{Social media mining: An introduction}},
volume = {9781107018},
year = {2014}
}
@inproceedings{Wu2018,
abstract = {When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classiication of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Speciically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classiication accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.},
author = {Wu, Liang and Liu, Huan},
booktitle = {(WSDM 2018) The 11th ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3159652.3159677},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Liu - 2018 - Tracing Fake-News Footprints Characterizing Social Media Messages by How They Propagate.pdf:pdf},
isbn = {9781450355810},
keywords = {@BULLET Networks → Online social networks,Classiication,Dimensionality reduction,Fake News Detection,Graph Mining,KEYWORDS Misinformation,Social Media Mining,Social Net-work Analysis,classiication},
title = {{Tracing Fake-News Footprints: Characterizing Social Media Messages by How They Propagate}},
url = {http://www.public.asu.edu/{~}liangwu1/WSDM18{\_}TraceMiner.pdf},
year = {2018}
}
@article{Kwon2017,
abstract = {This study determines the major difference between rumors and non-rumors and explores rumor classification performance levels over varying time windows-from the first three days to nearly two months. A comprehensive set of user, structural, linguistic, and temporal features was examined and their relative strength was compared from near-complete date of Twitter. Our contribution is at providing deep insight into the cumulative spreading patterns of rumors over time as well as at tracking the precise changes in predictive powers across rumor features. Statistical analysis finds that structural and temporal features distinguish rumors from non-rumors over a long-term window, yet they are not available during the initial propagation phase. In contrast, user and linguistic features are readily available and act as a good indicator during the initial propagation phase. Based on these findings, we suggest a new rumor classification algorithm that achieves competitive accuracy over both short and long time windows. These findings provide new insights for explaining rumor mechanism theories and for identifying features of early rumor detection.},
author = {Kwon, Sejeong and Cha, Meeyoung and Jung, Kyomin},
doi = {10.1371/journal.pone.0168344},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kwon, Cha, Jung - 2017 - Rumor detection over varying time windows.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pages = {1--19},
pmid = {28081135},
title = {{Rumor detection over varying time windows}},
volume = {12},
year = {2017}
}
@article{Allcott2017,
abstract = {We present new evidence on the role of false stories circulated on social media prior to the 2016 US presidential election. Drawing on audience data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of news in the run-up to the election, with 14 percent of Americans calling social media their " most important " source of election news; (ii) of the known false news stories that appeared in the three months before the election, those favoring Trump were shared a total of 30 million times on Facebook, while those favoring Clinton were shared eight million times; (iii) the average American saw and remembered 0.92 pro-Trump fake news stories and 0.23 pro-Clinton fake news stories, with just over half of those who recalled seeing fake news stories believing them; (iv) for fake news to have changed the outcome of the election, a single fake article would need to have had the same persuasive effect as 36 television campaign ads.},
archivePrefix = {arXiv},
arxivId = {1704.07506},
author = {Allcott, Hunt and Gentzkow, Matthew},
doi = {10.1257/jep.31.2.211},
eprint = {1704.07506},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Allcott, Gentzkow - 2017 - Social Media and Fake News in the 2016 Election.pdf:pdf},
issn = {0895-3309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {211--236},
title = {{Social Media and Fake News in the 2016 Election}},
url = {http://pubs.aeaweb.org/doi/10.1257/jep.31.2.211},
volume = {31},
year = {2017}
}
@article{Chen2015,
abstract = {Widespread adoption of internet technologies has changed the way that news is created and consumed. The current online news environment is one that incentivizes speed and spectacle in reporting, at the cost of fact-checking and verification. The line between user generated content and traditional news has also become increasingly blurred. This poster reviews some of the professional and cultural issues surrounding online news and argues for a two-pronged approach inspired by Hemingway's “automatic crap detector” (Manning, 1965) in order to address these problems: a) proactive public engagement by educators, librarians, and information specialists to promote digital literacy practices; b) the development of automated tools and technologies to assist journalists in vetting, verifying, and fact-checking, and to assist news readers by filtering and flagging dubious information.},
author = {Chen, Yimin and Conroy, Niall J. and Rubin, Victoria L.},
doi = {10.1002/pra2.2015.145052010081},
isbn = {087715547X},
issn = {23739231},
journal = {Proceedings of the Association for Information Science and Technology},
title = {{News in an online world: The need for an “automatic crap detector”}},
year = {2015}
}
@article{Qiu2017,
abstract = {Social media are massive marketplaces in which memes compete for our attention. We investigate the conditions in which the best ideas prevail in a stylized model of online social network, where agents have behavioral limitations in managing a heavy flow of information. We measure the relationship between the quality of an idea and its likelihood to become prevalent at the system level. We find that both information overload and limited attention contribute to a degradation in the market's discriminative power. A good tradeoff between discriminative power and diversity of information is possible according to the model. However, calibration with empirical data characterizing information load and finite attention in real social media reveals a weak correlation between quality and popularity of information. In these realistic conditions, the model predicts that low-quality information is just as likely to go viral, providing an interpretation for the high volume of misinformation we observe online.},
archivePrefix = {arXiv},
arxivId = {1701.02694},
author = {Qiu, Xiaoyan and Oliveira, Diego F.M. and {Sahami Shirazi}, Alireza and Flammini, Alessandro and Menczer, Filippo},
doi = {10.1038/s41562-017-0132},
eprint = {1701.02694},
issn = {23973374},
journal = {Nature Human Behaviour},
title = {{Limited individual attention and online virality of low-quality information}},
year = {2017}
}
@inproceedings{Fourney2017,
abstract = {We present an analysis of traffic to websites known for publish-ing fake news in the months preceding the 2016 US presidential election. The study is based on the combined instrumentation data from two popular desktop web browsers: Internet Explorer 11 and Edge. We find that social media was the primary outlet for the circulation of fake news stories and that aggregate voting patterns were strongly correlated with the average daily fraction of users visiting websites serving fake news. This correlation was observed both at the state level and at the county level, and remained stable throughout the main election season. We propose a simple model based on homophily in social networks to explain the linear associ-ation. Finally, we highlight examples of different types of fake news stories: while certain stories continue to circulate in the population, others are short-lived and die out in a few days.},
author = {Fourney, Adam and Racz, Miklos Z. and Ranade, Gireeja and Mobius, Markus and Horvitz, Eric},
booktitle = {CIKM '17 Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
doi = {10.1145/3132847.3133147},
isbn = {9781450349185},
title = {{Geographic and Temporal Trends in Fake News Consumption During the 2016 US Presidential Election}},
year = {2017}
}
@article{Rubin2015,
abstract = {A fake news detection system aims to assist users in detecting and filtering out varieties of potentially deceptive news. The prediction of the chances that a particular news item is intentionally deceptive is based on the analysis of previously seen truthful and deceptive news. A scarcity of deceptive news, available as corpora for predictive modeling, is a major stumbling block in this field of natural language processing (NLP) and deception detection. This paper discusses three types of fake news, each in contrast to genuine serious reporting, and weighs their pros and cons as a corpus for text analytics and predictive modeling. Filtering, vetting, and verifying online information continues to be essential in library and information science (LIS), as the lines between traditional news and online information are blurring.},
author = {Rubin, Victoria L. and Chen, Yimin and Conroy, Niall J.},
doi = {10.1002/pra2.2015.145052010083},
isbn = {0-87715-547-X},
issn = {23739231},
journal = {Proceedings of the Association for Information Science and Technology},
title = {{Deception detection for news: Three types of fakes}},
year = {2015}
}
@article{Figueira2017,
author = {Figueira, {\'{A}}lvaro and Oliveira, Luciana},
doi = {10.1016/j.procs.2017.11.106},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Figueira, Oliveira - 2017 - The current state of fake news challenges and opportunities.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Epidemics,FakeNews,FastRead,READ,Reviewed},
mendeley-tags = {Epidemics,FakeNews,FastRead,READ,Reviewed},
title = {{The current state of fake news: challenges and opportunities}},
year = {2017}
}
@inproceedings{Farajtabar2017,
abstract = {We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms. By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework. We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal total reward under budget constraints. Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.},
archivePrefix = {arXiv},
arxivId = {1703.07823},
author = {Farajtabar, Mehrdad and Yang, Jiachen and Ye, Xiaojing and Xu, Huan and Trivedi, Rakshit and Khalil, Elias and Li, Shuang and Song, Le and Zha, Hongyuan},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML' 17)},
eprint = {1703.07823},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Farajtabar et al. - 2017 - Fake News Mitigation via Point Process Based Intervention.pdf:pdf},
title = {{Fake News Mitigation via Point Process Based Intervention}},
url = {http://arxiv.org/abs/1703.07823},
year = {2017}
}
@article{Tacchini2017,
abstract = {In recent years, the reliability of information on the Internet has emerged as a crucial issue of modern society. Social network sites (SNSs) have revolutionized the way in which information is spread by allowing users to freely share content. As a consequence, SNSs are also increasingly used as vectors for the diffusion of misinformation and hoaxes. The amount of disseminated information and the rapidity of its diffusion make it practically impossible to assess reliability in a timely manner, highlighting the need for automatic hoax detection systems. As a contribution towards this objective, we show that Facebook posts can be classified with high accuracy as hoaxes or non-hoaxes on the basis of the users who "liked" them. We present two classification techniques, one based on logistic regression, the other on a novel adaptation of boolean crowdsourcing algorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users, we obtain classification accuracies exceeding 99{\%} even when the training set contains less than 1{\%} of the posts. We further show that our techniques are robust: they work even when we restrict our attention to the users who like both hoax and non-hoax posts. These results suggest that mapping the diffusion pattern of information can be a useful component of automatic hoax detection systems.},
archivePrefix = {arXiv},
arxivId = {1704.07506},
author = {Tacchini, Eugenio and Ballarin, Gabriele and {Della Vedova}, Marco L. and Moret, Stefano and de Alfaro, Luca},
doi = {10.1257/jep.31.2.211},
eprint = {1704.07506},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Tacchini et al. - 2017 - Some Like it Hoax Automated Fake News Detection in Social Networks.pdf:pdf},
issn = {0895-3309},
keywords = {FakeNews,KindaBad,Reviewed},
mendeley-tags = {FakeNews,KindaBad,Reviewed},
title = {{Some Like it Hoax: Automated Fake News Detection in Social Networks}},
year = {2017}
}
@article{Buntain,
abstract = {Evaluating information accuracy in social media is an increasingly important and well-studied area, but limited research has compared journalist-sourced accuracy assessments to their crowdsourced counterparts. .is paper demonstrates the diierences between these two populations by comparing the features used to predict accuracy assessments in two Twiier data sets: CREDBANK and PHEME. While our rndings are consistent with existing results on feature importance, we develop models that outperform past research. We also show limited overlap exists between the features used by journalists and crowdsourced assessors, and the resulting models poorly predict each other but produce statistically corre-lated results. .is correlation suggests crowdsourced workers are assessing a diierent aspect of these stories than their journalist counterparts, but these two aspects are linked in a signiicant way. diierences may be explained by contrasting factual with per-ceived accuracy as assessed by expert journalists and non-experts respectively. Following this outcome, we also show preliminary results that models trained from crowdsourced workers outperform journalist-trained models in identifying highly shared " fake news " stories.},
author = {Buntain, Cody and Golbeck, Jennifer},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Buntain, Golbeck - Unknown - I Want to Believe Journalists and Crowdsourced Accuracy Assessments in Twiier.pdf:pdf},
keywords = {KEYWORDS misinformation,data quality,fake news,twiier},
title = {{I Want to Believe: Journalists and Crowdsourced Accuracy Assessments in Twiier}}
}
@article{Riedel2017,
abstract = {Identifying public misinformation is a complicated and challenging task. Stance detection, i.e. determining the relative perspective a news source takes towards a specific claim, is an important part of evaluating the veracity of the assertion. Automating the process of stance detection would arguably benefit human fact checkers. In this paper, we present our stance detection model which claimed third place in the first stage of the Fake News Challenge. Despite our straightforward approach, our model performs at a competitive level with the complex ensembles of the top two winning teams. We therefore propose our model as the 'simple but tough-to-beat baseline' for the Fake News Challenge stance detection task.},
archivePrefix = {arXiv},
arxivId = {1707.03264},
author = {Riedel, Benjamin and Augenstein, Isabelle and Spithourakis, Georgios P. and Riedel, Sebastian},
eprint = {1707.03264},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Riedel et al. - 2017 - A simple but tough-to-beat baseline for the Fake News Challenge stance detection task.pdf:pdf},
title = {{A simple but tough-to-beat baseline for the Fake News Challenge stance detection task}},
year = {2017}
}
@article{Shao2017,
abstract = {The massive spread of fake news has been identified as a major global risk and has been alleged to influence elections and threaten democracies. Communication, cognitive, social, and computer scientists are engaged in efforts to study the complex causes for the viral diffusion of digital misinformation and to develop solutions, while search and social media platforms are beginning to deploy countermeasures. However, to date, these efforts have been mainly informed by anecdotal evidence rather than systematic data. Here we analyze 14 million messages spreading 400 thousand claims on Twitter during and following the 2016 U.S. presidential campaign and election. We find evidence that social bots play a key role in the spread of fake news. Accounts that actively spread misinformation are significantly more likely to be bots. Automated accounts are particularly active in the early spreading phases of viral claims, and tend to target influential users. Humans are vulnerable to this manipulation, retweeting bots who post false news. Successful sources of false and biased claims are heavily supported by social bots. These results suggest that curbing social bots may be an effective strategy for mitigating the spread of online misinformation.},
archivePrefix = {arXiv},
arxivId = {1707.07592},
author = {Shao, Chengcheng and Ciampaglia, Giovanni Luca and Varol, Onur and Flammini, Alessandro and Menczer, Filippo},
doi = {1707.07592},
eprint = {1707.07592},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shao et al. - 2017 - The spread of fake news by social bots.pdf:pdf},
keywords = {FakeNews,READ,Reviewed},
mendeley-tags = {FakeNews,READ,Reviewed},
title = {{The spread of fake news by social bots}},
year = {2017}
}
@article{Mukherjee2017,
abstract = {One of the major hurdles preventing the full exploitation of information from online communities is the widespread concern regarding the quality and credibility of user-contributed content. Prior works in this domain operate on a static snapshot of the community, making strong assumptions about the structure of the data (e.g., relational tables), or consider only shallow features for text classification. To address the above limitations, we propose probabilistic graphical models that can leverage the joint interplay between multiple factors in online communities --- like user interactions, community dynamics, and textual content --- to automatically assess the credibility of user-contributed online content, and the expertise of users and their evolution with user-interpretable explanation. To this end, we devise new models based on Conditional Random Fields for different settings like incorporating partial expert knowledge for semi-supervised learning, and handling discrete labels as well as numeric ratings for fine-grained analysis. This enables applications such as extracting reliable side-effects of drugs from user-contributed posts in healthforums, and identifying credible content in news communities. Online communities are dynamic, as users join and leave, adapt to evolving trends, and mature over time. To capture this dynamics, we propose generative models based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian Motion to trace the continuous evolution of user expertise and their language model over time. This allows us to identify expert users and credible content jointly over time, improving state-of-the-art recommender systems by explicitly considering the maturity of users. This also enables applications such as identifying helpful product reviews, and detecting fake and anomalous reviews with limited information.},
archivePrefix = {arXiv},
arxivId = {1707.08309},
author = {Mukherjee, Subhabrata},
eprint = {1707.08309},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Mukherjee - 2017 - Probabilistic Graphical Models for Credibility Analysis in Evolving Online Communities.pdf:pdf},
title = {{Probabilistic Graphical Models for Credibility Analysis in Evolving Online Communities}},
year = {2017}
}
@article{Aker2017,
abstract = {Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction.},
archivePrefix = {arXiv},
arxivId = {1708.05286},
author = {Aker, Ahmet and Derczynski, Leon and Bontcheva, Kalina},
eprint = {1708.05286},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Aker, Derczynski, Bontcheva - 2017 - Simple Open Stance Classification for Rumour Analysis.pdf:pdf},
title = {{Simple Open Stance Classification for Rumour Analysis}},
year = {2017}
}
@article{Perez-Rosas2017,
abstract = {The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news.},
archivePrefix = {arXiv},
arxivId = {1708.07104},
author = {P{\'{e}}rez-Rosas, Ver{\'{o}}nica and Kleinberg, Bennett and Lefevre, Alexandra and Mihalcea, Rada},
eprint = {1708.07104},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/P{\'{e}}rez-Rosas et al. - 2017 - Automatic Detection of Fake News.pdf:pdf},
title = {{Automatic Detection of Fake News}},
year = {2017}
}
@article{Guha,
abstract = {The emergence of " Fake News " and misinformation via on-line news and social media has spurred an interest in compu-tational tools to combat this phenomenon. In this paper we present a new " Related Fact Checks " service, which can help a reader critically evaluate an article and make a judgment on its veracity by bringing up fact checks that are relevant to the article. We describe the core technical problems that need to be solved in building a " Related Fact Checks " service, and present results from an evaluation of an implementation.},
author = {Guha, Sreya},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Guha - Unknown - Related Fact Checks a tool for combating fake news.pdf:pdf},
title = {{Related Fact Checks: a tool for combating fake news}}
}
@article{Nithyanand2017,
abstract = {We identify general trends in the (in)civility and complexity of political discussions occurring on Reddit between January 2007 and May 2017 -- a period spanning both terms of Barack Obama's presidency and the first 100 days of Donald Trump's presidency. We then investigate four factors that are frequently hypothesized as having contributed to the declining quality of American political discourse -- (1) the rising popularity of Donald Trump, (2) increasing polarization and negative partisanship, (3) the democratization of news media and the rise of fake news, and (4) merging of fringe groups into mainstream political discussions.},
archivePrefix = {arXiv},
arxivId = {1711.05303},
author = {Nithyanand, Rishab and Schaffner, Brian and Gill, Phillipa},
eprint = {1711.05303},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Nithyanand, Schaffner, Gill - 2017 - Online Political Discourse in the Trump Era.pdf:pdf},
title = {{Online Political Discourse in the Trump Era}},
year = {2017}
}
@article{Wilder2017,
abstract = {Election control considers the problem of an adversary who attempts to tamper with a voting process, in order to either ensure that their favored candidate wins (constructive control) or another candidate loses (destructive control). As online social networks have become significant sources of information for potential voters, a new tool in an attacker's arsenal is to effect control by harnessing social influence, for example, by spreading fake news and other forms of misinformation through online social media. We consider the computational problem of election control via social influence, studying the conditions under which finding good adversarial strategies is computationally feasible. We consider two objectives for the adversary in both the constructive and destructive control settings: probability and margin of victory (POV and MOV, respectively). We present several strong negative results, showing, for example, that the problem of maximizing POV is inapproximable for any constant factor. On the other hand, we present approximation algorithms which provide somewhat weaker approximation guarantees, such as bicriteria approximations for the POV objective and constant-factor approximations for MOV. Finally, we present mixed integer programming formulations for these problems. Experimental results show that our approximation algorithms often find near-optimal control strategies, indicating that election control through social influence is a salient threat to election integrity.},
archivePrefix = {arXiv},
arxivId = {1711.08615},
author = {Wilder, Bryan and Vorobeychik, Yevgeniy},
eprint = {1711.08615},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Wilder, Vorobeychik - 2017 - Controlling Elections through Social Influence.pdf:pdf},
title = {{Controlling Elections through Social Influence}},
year = {2017}
}
@article{Tschiatschek,
abstract = {Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregat-ing users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propa-gation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, DE-TECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that directly maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effective-ness of our approach via extensive experiments and show the power of leveraging community signals.},
author = {Tschiatschek, Sebastian and Singla, Adish and {Gomez Rodriguez}, Manuel and Merchant, Arpit and Krause, Andreas},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Tschiatschek et al. - Unknown - Detecting Fake News in Social Networks via Crowdsourcing.pdf:pdf},
title = {{Detecting Fake News in Social Networks via Crowdsourcing}}
}
@article{Kim2017,
abstract = {Online social networking sites are experimenting with the following crowd-powered procedure to reduce the spread of fake news and misinformation: whenever a user is exposed to a story through her feed, she can flag the story as misinformation and, if the story receives enough flags, it is sent to a trusted third party for fact checking. If this party identifies the story as misinformation, it is marked as disputed. However, given the uncertain number of exposures, the high cost of fact checking, and the trade-off between flags and exposures, the above mentioned procedure requires careful reasoning and smart algorithms which, to the best of our knowledge, do not exist to date. In this paper, we first introduce a flexible representation of the above procedure using the framework of marked temporal point processes. Then, we develop a scalable online algorithm, Curb, to select which stories to send for fact checking and when to do so to efficiently reduce the spread of misinformation with provable guarantees. In doing so, we need to solve a novel stochastic optimal control problem for stochastic differential equations with jumps, which is of independent interest. Experiments on two real-world datasets gathered from Twitter and Weibo show that our algorithm may be able to effectively reduce the spread of fake news and misinformation.},
archivePrefix = {arXiv},
arxivId = {1711.09918},
author = {Kim, Jooyeon and Tabibian, Behzad and Oh, Alice and Schoelkopf, Bernhard and Gomez-Rodriguez, Manuel},
eprint = {1711.09918},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2017 - Leveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation.pdf:pdf},
isbn = {9781450355810},
title = {{Leveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation}},
year = {2017}
}
@article{Bhatt,
abstract = {Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detec-tion of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incor-porate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to deter-mine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and hand crafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. We compare our proposed technique with the current state-of-the-art models on the fake news chal-lenge dataset. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the sub-missions to the fake news challenge.},
author = {Bhatt, Gaurav and Sharma, Aman and Sharma, Shivam and Nagpal, Ankush and Raman, Balasubramanian and Mittal, Ankush},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bhatt et al. - Unknown - On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification.pdf:pdf},
keywords = {Deep learning,External features,Fake news,Statistical features,Word embeddings},
title = {{On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification}}
}
@article{Alon2015,
abstract = {We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced {\$}T{\$}-round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with {\$}\backslashwidetilde\backslashTheta(\backslashalpha{\^{}}{\{}1/2{\}} T{\^{}}{\{}1/2{\}}){\$} minimax regret, where {\$}\backslashalpha{\$} is the independence number of the underlying graph; the second class induces problems with {\$}\backslashwidetilde\backslashTheta(\backslashdelta{\^{}}{\{}1/3{\}}T{\^{}}{\{}2/3{\}}){\$} minimax regret, where {\$}\backslashdelta{\$} is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time.},
archivePrefix = {arXiv},
arxivId = {1502.07617},
author = {Alon, Noga and Cesa-Bianchi, Nicol{\`{o}} and Dekel, Ofer and Koren, Tomer},
eprint = {1502.07617},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Alon et al. - 2015 - Online Learning with Feedback Graphs Beyond Bandits.pdf:pdf},
issn = {15337928},
title = {{Online Learning with Feedback Graphs: Beyond Bandits}},
year = {2015}
}
@article{Shakarian2015,
author = {Shakarian, Paulo and Bhatnagar, Abhinav and Aleali, Ashkan and Shaabani, Elham and Guo, Ruocheng},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shakarian et al. - 2015 - Diffusion in Social Networks.pdf:pdf},
keywords = {Book,Books,Epidemics,FakeNews},
mendeley-tags = {Book,Books,Epidemics,FakeNews},
title = {{Diffusion in Social Networks}},
year = {2015}
}
@inproceedings{Kempe2003,
abstract = {Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of “word of mouth” in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target? We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63{\%} of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks. We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform nodeselection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.},
archivePrefix = {arXiv},
arxivId = {0806.2034v2},
author = {Kempe, David and Kleinberg, Jon and Tardos, {\'{E}}va},
booktitle = {Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '03},
doi = {10.1145/956755.956769},
eprint = {0806.2034v2},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kempe, Kleinberg, Tardos - 2003 - Maximizing the spread of influence through a social network.pdf:pdf},
isbn = {1581137370},
issn = {1557-2862},
keywords = {BestOf,Epidemics,NotRead,TestOfTime,ViralityPrediction},
mendeley-tags = {BestOf,Epidemics,NotRead,TestOfTime,ViralityPrediction},
pmid = {17255001},
title = {{Maximizing the spread of influence through a social network}},
year = {2003}
}
@article{Yi2013,
abstract = {Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels). In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EM's performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem.},
archivePrefix = {arXiv},
arxivId = {1310.3745},
author = {Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},
eprint = {1310.3745},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Yi, Caramanis, Sanghavi - 2013 - Alternating Minimization for Mixed Linear Regression.pdf:pdf},
isbn = {9781634393973},
title = {{Alternating Minimization for Mixed Linear Regression}},
url = {http://arxiv.org/abs/1310.3745},
volume = {32},
year = {2013}
}
@inproceedings{Cheng2014,
abstract = {On many social networking web sites such as Facebook and Twitter, resharing or reposting functionality allows users to share others' content with their own friends or followers. As content is reshared from user to user, large cascades of reshares can form. While a growing body of research has focused on analyzing and characterizing such cascades, a recent, parallel line of work has argued that the future trajectory of a cascade may be inherently unpredictable. In this work, we develop a framework for addressing cascade prediction problems. On a large sample of photo reshare cascades on Facebook, we find strong performance in predicting whether a cascade will continue to grow in the future. We find that the relative growth of a cascade becomes more predictable as we observe more of its reshares, that temporal and structural features are key predictors of cascade size, and that initially, breadth, rather than depth in a cascade is a better indicator of larger cascades. This prediction performance is robust in the sense that multiple distinct classes of features all achieve similar performance. We also discover that temporal features are predictive of a cascade's eventual shape. Observing independent cascades of the same content, we find that while these cascades differ greatly in size, we are still able to predict which ends up the largest.},
archivePrefix = {arXiv},
arxivId = {1403.4608},
author = {Cheng, Justin and Adamic, Lada A. and Dow, P. Alex and Kleinberg, Jon and Leskovec, Jure},
booktitle = {Proceedings of the 23rd international conference on World wide web (WWW' 14)},
doi = {10.1145/2566486.2567997},
eprint = {1403.4608},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Cheng et al. - 2014 - Can Cascades be Predicted.pdf:pdf},
isbn = {9781450327442},
keywords = {Epidemics,FakeNews,Great,READ,ViralityPrediction},
mendeley-tags = {Epidemics,FakeNews,Great,READ,ViralityPrediction},
title = {{Can Cascades be Predicted?}},
year = {2014}
}
@article{,
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Conference course.pdf:pdf},
pages = {15},
title = {{Conference course}}
}
@article{Hazan2017,
abstract = {We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.},
archivePrefix = {arXiv},
arxivId = {1711.00946},
author = {Hazan, Elad and Singh, Karan and Zhang, Cyril},
eprint = {1711.00946},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Hazan, Singh, Zhang - 2017 - Learning Linear Dynamical Systems via Spectral Filtering.pdf:pdf},
title = {{Learning Linear Dynamical Systems via Spectral Filtering}},
year = {2017}
}
@article{Tatar2014,
abstract = {Social media platforms have democratized the process of web content creation allowing mere consumers to become creators and distributors of content. But this has also contributed to an explosive growth of information and has intensified the online competition for users attention, since only a small number of items become popular while the rest remain unknown. Understanding what makes one item more popular than another, observing its popularity dynamics, and being able to predict its popularity has thus attracted a lot of interest in the past few years. Predicting the popularity of web content is useful in many areas such as network dimensioning (e.g., caching and replication), online marketing (e.g., recommendation systems and media advertising), or real-world outcome prediction (e.g., economical trends). In this survey, we review the current findings on web content popularity prediction. We describe the different popularity prediction models, present the features that have shown good predictive capabilities, and reveal factors known to influence web content popularity.},
archivePrefix = {arXiv},
arxivId = {1203.1647},
author = {Tatar, Alexandru and de Amorim, Marcelo Dias and Fdida, Serge and Antoniadis, Panayotis},
doi = {10.1186/s13174-014-0008-y},
eprint = {1203.1647},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Tatar et al. - 2014 - A survey on predicting the popularity of web content.pdf:pdf},
issn = {1867-4828},
journal = {Journal of Internet Services and Applications},
title = {{A survey on predicting the popularity of web content}},
year = {2014}
}
@article{Garcia-Herranz2014,
abstract = {Recent research has focused on the monitoring of global-scale online data for improved detection of epidemics, mood patterns, movements in the stock market political revolutions, box-office revenues, consumer behaviour and many other important phenomena. However, privacy considerations and the sheer scale of data available online are quickly making global monitoring infeasible, and existing methods do not take full advantage of local network structure to identify key nodes for monitoring. Here, we develop a model of the contagious spread of information in a global-scale, publicly-articulated social network and show that a simple method can yield not just early detection, but advance warning of contagious outbreaks. In this method, we randomly choose a small fraction of nodes in the network and then we randomly choose a friend of each node to include in a group for local monitoring. Using six months of data from most of the full Twittersphere, we show that this friend group is more central in the network and it helps us to detect viral outbreaks of the use of novel hashtags about 7 days earlier than we could with an equal-sized randomly chosen group. Moreover, the method actually works better than expected due to network structure alone because highly central actors are both more active and exhibit increased diversity in the information they transmit to others. These results suggest that local monitoring is not just more efficient, but also more effective, and it may be applied to monitor contagious processes in global-scale networks.},
archivePrefix = {arXiv},
arxivId = {1211.6512},
author = {Garcia-Herranz, Manuel and Moro, Esteban and Cebrian, Manuel and Christakis, Nicholas A. and Fowler, James H.},
doi = {10.1371/journal.pone.0092413},
eprint = {1211.6512},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Garcia-Herranz et al. - 2014 - Using friends as sensors to detect global-scale contagious outbreaks.pdf:pdf},
isbn = {9781450317153},
issn = {19326203},
journal = {PLoS ONE},
pmid = {24718030},
title = {{Using friends as sensors to detect global-scale contagious outbreaks}},
year = {2014}
}
@article{Weng2013,
abstract = {How does network structure affect diffusion? Recent studies suggest that the answer depends on the type of contagion. Complex contagions, unlike infectious diseases (simple contagions), are affected by social reinforcement and homophily. Hence, the spread within highly clustered communities is enhanced, while diffusion across communities is hampered. A common hypothesis is that memes and behaviors are complex contagions. We show that, while most memes indeed spread like complex contagions, a few viral memes spread across many communities, like diseases. We demonstrate that the future popularity of a meme can be predicted by quantifying its early spreading pattern in terms of community concentration. The more communities a meme permeates, the more viral it is. We present a practical method to translate data about community structure into predictive knowledge about what information will spread widely. This connection contributes to our understanding in computational social science, social media analytics, and marketing applications.},
archivePrefix = {arXiv},
arxivId = {1306.0158v1},
author = {Weng, Lilian and Menczer, Filippo and Ahn, Yong-Yeol},
doi = {10.1038/srep02522},
eprint = {1306.0158v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Weng, Menczer, Ahn - 2013 - Virality Prediction and Community Structure in Social Networks.pdf:pdf},
isbn = {2045-2322 (Electronic)$\backslash$r2045-2322 (Linking)},
issn = {2045-2322},
journal = {Scientific Reports},
keywords = {FakeNews},
mendeley-tags = {FakeNews},
pmid = {23982106},
title = {{Virality Prediction and Community Structure in Social Networks}},
year = {2013}
}
@inproceedings{Ma2015,
abstract = {ABSTRACT Automatically identifying rumors from online social media especially microblogging websites is an important research issue. Most of existing work for rumor detection focuses on modeling features related to microblog contents, users and propagation patterns, but ignore the importance of the variation of these social context features during the message propagation over time. In this study, we propose a novel approach to capture the temporal characteristics of these features based on the time series of rumor鈥檚 lifecycle, for which time series modeling technique is applied to incorporate various social context information. Our experiments using the events in two microblog datasets confirm that the method outperforms state-of-the-art rumor detection approaches by large margins. Moreover, our model demonstrates strong performance on detecting rumors at early stage after their initial broadcast.},
author = {Ma, Jing and Gao, Wei and Wei, Zhongyu and Lu, Yueming and Wong, Kam-Fai},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management - CIKM '15},
doi = {10.1145/2806416.2806607},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Ma et al. - 2015 - Detect Rumors Using Time Series of Social Context Information on Microblogging Websites.pdf:pdf},
isbn = {9781450337946},
keywords = {FakeNews},
mendeley-tags = {FakeNews},
title = {{Detect Rumors Using Time Series of Social Context Information on Microblogging Websites}},
year = {2015}
}
@article{Kwon,
abstract = {—The problem of identifying rumors is of practical importance especially in online social networks, since infor-mation can diffuse more rapidly and widely than the offline counterpart. In this paper, we identify characteristics of rumors by examining the following three aspects of diffusion: temporal, structural, and linguistic. For the temporal characteristics, we propose a new periodic time series model that considers daily and external shock cycles, where the model demonstrates that rumor likely have fluctuations over time. We also identify key structural and linguistic differences in the spread of rumors and non-rumors. Our selected features classify rumors with high precision and recall in the range of 87{\%} to 92{\%}, that is higher than other states of the arts on rumor classification.},
author = {Kwon, Sejeong and Cha, Meeyoung and {Jung Wei Chen Yajun Wang}, Kyomin},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kwon, Cha, Jung Wei Chen Yajun Wang - Unknown - Prominent Features of Rumor Propagation in Online Social Media.pdf:pdf},
keywords = {FakeNews,NotRead},
mendeley-tags = {FakeNews,NotRead},
title = {{Prominent Features of Rumor Propagation in Online Social Media}}
}
@article{Shu2017,
abstract = {Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of "fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ineffective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.},
archivePrefix = {arXiv},
arxivId = {1708.01967},
author = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
doi = {10.1145/3137597.3137600},
eprint = {1708.01967},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shu et al. - 2017 - Fake News Detection on Social Media A Data Mining Perspective.pdf:pdf},
issn = {19310145},
title = {{Fake News Detection on Social Media: A Data Mining Perspective}},
year = {2017}
}
@article{Aymanns2017,
abstract = {We model the spread of news as a social learning game on a network. Agents can either endorse or oppose a claim made in a piece of news, which itself may be either true or false. Agents base their decision on a private signal and their neighbors' past actions. Given these inputs, agents follow strategies derived via multi-agent deep reinforcement learning and receive utility from acting in accordance with the veracity of claims. Our framework yields strategies with agent utility close to a theoretical, Bayes optimal benchmark, while remaining flexible to model re-specification. Optimized strategies allow agents to correctly identify most false claims, when all agents receive unbiased private signals. However, an adversary's attempt to spread fake news by targeting a subset of agents with a biased private signal can be successful. Even more so when the adversary has information about agents' network position or private signal. When agents are aware of the presence of an adversary they re-optimize their strategies in the training stage and the adversary's attack is less effective. Hence, exposing agents to the possibility of fake news can be an effective way to curtail the spread of fake news in social networks. Our results also highlight that information about the users' private beliefs and their social network structure can be extremely valuable to adversaries and should be well protected.},
archivePrefix = {arXiv},
arxivId = {1708.06233},
author = {Aymanns, Christoph and Foerster, Jakob and Georg, Co-Pierre},
eprint = {1708.06233},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Aymanns, Foerster, Georg - 2017 - Fake News in Social Networks.pdf:pdf},
title = {{Fake News in Social Networks}},
year = {2017}
}
@article{Xu,
abstract = {— Principal Component Analysis plays a central role in statistics, engineering and science. Because of the prevalence of corrupted data in real-world applications, much research has focused on developing robust algorithms. Perhaps surprisingly, these algorithms are unequipped – indeed, unable – to deal with outliers in the high dimensional setting where the number of observations is of the same magnitude as the number of variables of each observation, and the data set contains some (arbitrarily) corrupted observations. We propose a High-dimensional Robust Principal Component Analysis (HR-PCA) algorithm that is efficient, robust to contaminated points, and easily kernelizable. In particular, our algorithm achieves maximal robustness – it has a breakdown point of 50{\%} (the best possible) while all existing algorithms have a breakdown point of zero. Moreover, our algorithm recovers the optimal solution exactly in the case where the number of corrupted points grows sub linearly in the dimension.},
author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Xu, Caramanis, Mannor - Unknown - Outlier-Robust PCA The High Dimensional Case.pdf:pdf},
keywords = {Dimension Reduction,Index Terms— Statistical Learning,Outlier,Principal Component Analysis,Robustness},
title = {{Outlier-Robust PCA: The High Dimensional Case}}
}
@article{Yan2011,
abstract = {Many algorithms have been proposed for predicting missing edges in networks, but they do not usually take account of which edges are missing. We focus on networks which have missing edges of the form that is likely to occur in real networks, and compare algorithms that find these missing edges. We also investigate the effect of this kind of missing data on community detection algorithms.},
archivePrefix = {arXiv},
arxivId = {1109.2215},
author = {Yan, Bowen and Gregory, Steve},
doi = {10.1088/1751-8113/44/49/495102},
eprint = {1109.2215},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Yan, Gregory - 2011 - Finding missing edges and communities in incomplete networks.pdf:pdf},
isbn = {1751-8121},
issn = {1751-8113},
journal = {Journal of Physics A: Mathematical and Theoretical},
keywords = {MissingEdges,Read},
mendeley-tags = {MissingEdges,Read},
title = {{Finding missing edges and communities in incomplete networks}},
year = {2011}
}
@article{Drakopoulos2013,
abstract = {We consider an infinite collection of agents who make decisions, sequentially, about an unknown underlying binary state of the world. Each agent, prior to making a decision, receives an independent private signal whose distribution depends on the state of the world. Moreover, each agent also observes the decisions of its last K immediate predecessors. We study conditions under which the agent decisions converge to the correct value of the underlying state. We focus on the case where the private signals have bounded information content and investigate whether learning is possible, that is, whether there exist decision rules for the different agents that result in the convergence of their sequence of individual decisions to the correct state of the world. We first consider learning in the almost sure sense and show that it is impossible, for any value of K. We then explore the possibility of convergence in probability of the decisions to the correct state. Here, a distinction arises: if K = 1, learning in probability is impossible under any decision rule, while for K {\textgreater}= 2, we design a decision rule that achieves it. We finally consider a new model, involving forward looking strategic agents, each of which maximizes the discounted sum (over all agents) of the probabilities of a correct decision. (The case, studied in the previous literature, of myopic agents who maximize the probability of their own decision being correct is an extreme special case.) We show that for any value of K, for any equilibrium of the associated Bayesian game, and under the assumption that each private signal has bounded information content, learning in probability fails to obtain.},
archivePrefix = {arXiv},
arxivId = {1209.1122},
author = {Drakopoulos, Kimon and Ozdaglar, Asuman and Tsitsiklis, John N.},
doi = {10.1109/TIT.2013.2262037},
eprint = {1209.1122},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Drakopoulos, Ozdaglar, Tsitsiklis - 2013 - On learning with finite memory.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Decentralized,Drakopoulos,No,decision making,distributed estimation,distributed learning,estimation,estimation error,forward looking learning,learning,social learning},
mendeley-tags = {Drakopoulos,No},
title = {{On learning with finite memory}},
year = {2013}
}
@article{Rosenfeld2017,
abstract = {The goal of semi-supervised learning meth-ods is to effectively combine labeled and unla-beled data to arrive at a better model. Many methods rely on graph-based approaches, where labels are propagated through a graph over the input examples. In most current methods, the propagation mechanism un-derlying the learning objective is based on random walks. While theoretically elegant, random walks suffer from several drawbacks which can hurt predictive performance. In this work, we explore dynamic infec-tion processes as an alternative propagation mechanism. In these, unlabeled nodes can be " infected " with the label of their already infected neighbors. We provide an efficient, scalable, and parallelizable algorithm for esti-mating the expected infection outcomes. We also describe an optimization view of the method, relating it to Laplacian approaches. Finally, experiments demonstrate that the method is highly competitive across multiple benchmarks and for various learning settings.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06426v3},
author = {Rosenfeld, Nir and Globerson, Amir},
eprint = {arXiv:1703.06426v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Rosenfeld, Globerson - 2017 - Semi-Supervised Learning with Competitive Infection Models.pdf:pdf},
keywords = {Epidemics,NotRead},
mendeley-tags = {Epidemics,NotRead},
title = {{Semi-Supervised Learning with Competitive Infection Models}},
year = {2017}
}
@article{Fortunato2016,
abstract = {Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.},
archivePrefix = {arXiv},
arxivId = {1608.00163},
author = {Fortunato, Santo and Hric, Darko},
doi = {10.1016/j.physrep.2016.09.002},
eprint = {1608.00163},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Fortunato, Hric - 2016 - Community detection in networks A user guide.pdf:pdf},
issn = {0370-1573},
keywords = {CommunityDetection,Read,Survey},
mendeley-tags = {CommunityDetection,Read,Survey},
title = {{Community detection in networks: A user guide}},
year = {2016}
}
@article{Somanchi2017,
abstract = {Processes such as disease propagation and information diffusion often spread over some latent network structure which must be learned from observation. Given a set of unlabeled training examples representing occurrences of an event type of interest (e.g., a disease outbreak), our goal is to learn a graph structure that can be used to accurately detect future events of that type. Motivated by new theoretical results on the consistency of constrained and unconstrained subset scans, we propose a novel framework for learning graph structure from unlabeled data by comparing the most anomalous subsets detected with and without the graph constraints. Our framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure, and efficiently searches for the highest-scoring graph structure. Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County, we show that our method learns a structure similar to the true underlying graph, but enables faster and more accurate detection.},
archivePrefix = {arXiv},
arxivId = {1701.01470},
author = {Somanchi, Sriram and Neill, Daniel B.},
doi = {10.1109/MIS.2017.25},
eprint = {1701.01470},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Somanchi, Neill - 2017 - Graph Structure Learning from Unlabeled Data for Event Detection.pdf:pdf},
issn = {15411672},
keywords = {MissingEdges,NotRead},
mendeley-tags = {MissingEdges,NotRead},
title = {{Graph Structure Learning from Unlabeled Data for Event Detection}},
year = {2017}
}
@article{Klivans2017,
abstract = {We give a simple, multiplicative-weight update algorithm for learning undirected graphical models or Markov random fields (MRFs). The approach is new, and for the well-studied case of Ising models or Boltzmann machines, we obtain an algorithm that uses a nearly optimal number of samples and has quadratic running time (up to logarithmic factors), subsuming and improving on all prior work. Additionally, we give the first efficient algorithm for learning Ising models over general alphabets. Our main application is an algorithm for learning the structure of t-wise MRFs with nearly-optimal sample complexity (up to polynomial losses in necessary terms that depend on the weights) and running time that is {\$}n{\^{}}{\{}O(t){\}}{\$}. In addition, given {\$}n{\^{}}{\{}O(t){\}}{\$} samples, we can also learn the parameters of the model and generate a hypothesis that is close in statistical distance to the true MRF. All prior work runs in time {\$}n{\^{}}{\{}\backslashOmega(d){\}}{\$} for graphs of bounded degree d and does not generate a hypothesis close in statistical distance even for t=3. We observe that our runtime has the correct dependence on n and t assuming the hardness of learning sparse parities with noise. Our algorithm--the Sparsitron-- is easy to implement (has only one parameter) and holds in the on-line setting. Its analysis applies a regret bound from Freund and Schapire's classic Hedge algorithm. It also gives the first solution to the problem of learning sparse Generalized Linear Models (GLMs).},
archivePrefix = {arXiv},
arxivId = {1706.06274},
author = {Klivans, Adam and Meka, Raghu},
eprint = {1706.06274},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Klivans, Meka - 2017 - Learning Graphical Models Using Multiplicative Weights.pdf:pdf},
keywords = {Constantine,NotRead},
mendeley-tags = {Constantine,NotRead},
title = {{Learning Graphical Models Using Multiplicative Weights}},
url = {http://arxiv.org/abs/1706.06274},
year = {2017}
}
@article{Bernoulli2004,
abstract = {Should the general population be vaccinated against smallpox (Variola Major)? Would the benefits of mass vaccination outweigh the risks? How many deaths would occur as the result of a mass vaccination campaign against smallpox? Can mathematical models of smallpox vaccination be used to determine health policy? Although smallpox was declared eradicated by the World Health Organization in 1979, these questions have all been recently debated based upon the premise that smallpox may be used as a weapon of bioterrorism. Hence, a series of analyses has recently been published that use mathematical models to try to determine the most effective public health response in the event of such an attack [1–4]. However, these same controversial public health questions were debated in the 18th century when smallpox was endemic and Reviews in Medical Virology has published two classic papers describing the natural history of smallpox in 1902 and 1913 to help inform these discussions [5,6]. We now publish an even earlier paper. In 1760 Daniel Bernoulli (1700–1782), one of the greatest scientists of the 18th century, wrote a mathematical analysis of the problem in order to try to influence public health policy by encouraging the universal inoculation against smallpox; his analysis was first presented at the Royal Academy of Sciences in Paris in 1760 and later published in 1766 [7]. Here, we republish and discuss both the historical and the current significance of Bernoulli's classic paper. A detailed discussion of the mathematics of Bernoulli's analysis has previously been presented by Dietz and Heesterbeck [8].},
author = {Bernoulli, Daniel and Blower, Sally},
doi = {10.1002/rmv.443},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bernoulli, Blower - 2004 - An attempt at a new analysis of the mortality caused by smallpox and of the advantages of inoculation to prev.pdf:pdf},
isbn = {1052-9276 (Print)$\backslash$n1052-9276 (Linking)},
issn = {1052-9276},
journal = {Reviews in medical virology},
keywords = {Classic,Epidemics},
mendeley-tags = {Classic,Epidemics},
pages = {275--288},
pmid = {15334536},
title = {{An attempt at a new analysis of the mortality caused by smallpox and of the advantages of inoculation to prevent it}},
volume = {14},
year = {2004}
}
@misc{Whetten,
abstract = {Introduction: Study models provide invaluable information in treatment planning. Digital models have proved to be an effective measurement tool, but their use in treatment planning has not been studied. Methods: Ten sets of records of Class II malocclusion subjects (dental study models, lateral cephalograms/tracings, panoramic radiographs, intraoral and extraoral photographs) were used for treatment planning by 20 orthodontists on 2 separate occasions. Digital models were used to evaluate the patients at 1 session and plaster models were used at the other session. Treatment recommendations were scored and compared for agreement. Eleven orthodontists served as the control group, looking at the records on 2 occasions with plaster models for agreement. Results: Good agreement was noted for surgery (P = 1.00, ?? = 0.549), extractions (P = .360, ?? = 0.570), and auxiliary appliances (P = 1.00, ?? = 0.539) for the digital/plaster group. Agreement in the plaster/plaster group for surgery (P = 1.00, ?? = 0.671), extractions (P = 1.00, ?? = 0.626), and auxiliary appliances (P = .791, ?? = 0.672) was also good. Overall proportions of agreement ranged between 0.777 and 0.870 for digital/plaster and 0.818 and 0.873 for plaster/plaster. Conclusions: There was no statistical difference in intrarater treatment-planning agreement for Class II malocclusions based on the use of digital models in place of traditional plaster models. Digital orthodontic study models (e-models) are a valid alternative to traditional plaster study models in treatment planning for Class II malocclusion patients. ?? 2006 American Association of Orthodontists.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Whetten, Joshua L. and Williamson, Philip C. and Heo, Giseon and Varnhagen, Connie and Major, Paul W.},
booktitle = {American Journal of Orthodontics and Dentofacial Orthopedics},
doi = {10.1016/j.ajodo.2005.02.022},
eprint = {1708.02002},
isbn = {1097-6752 (Electronic)$\backslash$r0889-5406 (Linking)},
issn = {08895406},
number = {4},
pages = {485--491},
pmid = {17045148},
title = {{Variations in orthodontic treatment planning decisions of Class II patients between virtual 3-dimensional models and traditional plaster study models}},
volume = {130},
year = {2006}
}
@article{Kazdagli,
archivePrefix = {arXiv},
arxivId = {arXiv:1708.01864v1},
author = {Kazdagli, Mikhail and Caramanis, Constantine and Shakkottai, Sanjay and Tiwari, Mohit},
eprint = {arXiv:1708.01864v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kazdagli et al. - Unknown - Exploiting Latent Attack Semantics for Intelligent Malware Detection.pdf:pdf},
keywords = {Constntine,Malware},
mendeley-tags = {Constntine,Malware},
title = {{Exploiting Latent Attack Semantics for Intelligent Malware Detection}}
}
@article{Newman2014,
abstract = {Random graph models have played a dominant role in the theoretical study of networked systems. The Poisson random graph of Erdos and Renyi, in particular, as well as the so-called configuration model, have served as the starting point for numerous calculations. In this paper we describe another large class of random graph models, which we call equitable random graphs and which are flexible enough to represent networks with diverse degree distributions and many nontrivial types of structure, including community structure, bipartite structure, degree correlations, stratification, and others, yet are exactly solvable for a wide range of properties in the limit of large graph size, including percolation properties, complete spectral density, and the behavior of homogeneous dynamical systems, such as coupled oscillators or epidemic models.},
archivePrefix = {arXiv},
arxivId = {1405.1440},
author = {Newman, M. E J and Martin, Travis},
doi = {10.1103/PhysRevE.90.052824},
eprint = {1405.1440},
isbn = {1539-3755},
issn = {15502376},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {Epidemics,RandomGraphs,Read},
mendeley-tags = {Epidemics,RandomGraphs,Read},
pmid = {25493850},
title = {{Equitable random graphs}},
year = {2014}
}
@article{,
abstract = {The theory of random graphs began in the late 1950s in several papers by Erd{\"{o}}s and R{\'{e}}nyi. In the late twentieth century, the notion of six degrees of separation, meaning that any two people on the planet can be connected by a short chain of people who know each other, inspired Strogatz and Watts to define the small world random graph in which each site is con-nected to k close neighbors, but also has long-range connections. At about the same time, it was observed in human social and sexual networks and on the Internet that the number of neighbors of an individual or computer has a power law distribution. This inspired Barab{\'{a}}si and Albert to define the preferential attachment model, which has these properties. These two papers have led to an explosion of research. While this literature is ex-tensive, many of the papers are based on simulations and nonrigorous arguments. The purpose of this book is to use a wide variety of mathe-matical argument to obtain insights into the properties of these graphs. A unique feature of this book is the interest in the dynamics of process taking place on the graph in addition to their geometric properties, such as connectedness and diameter.},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2006 - Random Graph Dynamics.pdf:pdf},
keywords = {Books,Epidemics,NotRead},
mendeley-tags = {Books,Epidemics,NotRead},
title = {{Random Graph Dynamics}},
year = {2006}
}
@article{,
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
title = {{No Title}}
}
@article{Christakis2010,
abstract = {Current methods for the detection of contagious outbreaks give contemporaneous information about the course of an epidemic at best. It is known that individuals near the center of a social network are likely to be infected sooner during the course of an outbreak, on average, than those at the periphery. Unfortunately, mapping a whole network to identify central individuals who might be monitored for infection is typically very difficult. We propose an alternative strategy that does not require ascertainment of global network structure, namely, simply monitoring the friends of randomly selected individuals. Such individuals are known to be more central. To evaluate whether such a friend group could indeed provide early detection, we studied a flu outbreak at Harvard College in late 2009. We followed 744 students who were either members of a group of randomly chosen individuals or a group of their friends. Based on clinical diagnoses, the progression of the epidemic in the friend group occurred 13.9 days (95{\%} C.I. 9.9-16.6) in advance of the randomly chosen group (i.e., the population as a whole). The friend group also showed a significant lead time (p{\textless}0.05) on day 16 of the epidemic, a full 46 days before the peak in daily incidence in the population as a whole. This sensor method could provide significant additional time to react to epidemics in small or large populations under surveillance. The amount of lead time will depend on features of the outbreak and the network at hand. The method could in principle be generalized to other biological, psychological, informational, or behavioral contagions that spread in networks.},
archivePrefix = {arXiv},
arxivId = {1004.4792},
author = {Christakis, Nicholas A. and Fowler, James H.},
doi = {10.1371/journal.pone.0012948},
eprint = {1004.4792},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Christakis, Fowler - 2010 - Social network sensors for early detection of contagious outbreaks.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Epidemics,NotRead},
mendeley-tags = {Epidemics,NotRead},
number = {9},
pages = {1--8},
pmid = {20856792},
title = {{Social network sensors for early detection of contagious outbreaks}},
volume = {5},
year = {2010}
}
@article{Kulldorff2001,
abstract = {Most disease registries are updated at least yearly. If a geographically localized health hazard suddenly occurs, we would like to have a surveillance system in place that can pick up a new geographical disease cluster as quickly as possible, irrespective of its location and size. At the same time, we want to minimize the number of false alarms. By using a space-time scan statistic, we propose and illustrate a system for regular time periodic disease surveillance to detect any currently 'active' geographical clusters of disease and which tests the statistical significance of such clusters adjusting for the multitude of possible geographical locations and sizes, time intervals and time periodic analyses. The method is illustrated on thyroid cancer among men in New Mexico 1973-1992.},
author = {Kulldorff, M},
doi = {Doi 10.1111/1467-985x.00186},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kulldorff - 2001 - Prospective time periodic geographical disease surveillance using a scan statistic.pdf:pdf},
isbn = {0964-1998},
issn = {1467-985X},
journal = {Journal of the Royal Statistical Society Series a-Statistics in Society},
keywords = {Epidemics,NotRead,childhood,chronic disease surveillance,clusters,congenital-malformations,empirical bayes,geographical clusters,infectious disease surveillance,inhomogeneous populations,leukemia,new mexico,rare diseases,rates,risks,space-time clustering,spatial statistics,thyroid cancer,thyroid-cancer},
mendeley-tags = {Epidemics,NotRead},
pages = {61--72},
title = {{Prospective time periodic geographical disease surveillance using a scan statistic}},
volume = {164},
year = {2001}
}
@article{Arias-Castro2008,
abstract = {Consider a graph with a set of vertices and oriented edges connecting pairs of vertices. Each vertex is associated with a random variable and these are assumed to be independent. In this setting, suppose we wish to solve the following hypothesis testing problem: under the null, the random variables have common distribution N(0,1) while under the alternative, there is an unknown path along which random variables have distribution {\$}N(\backslashmu,1){\$}, {\$}\backslashmu{\textgreater} 0{\$}, and distribution N(0,1) away from it. For which values of the mean shift {\$}\backslashmu{\$} can one reliably detect and for which values is this impossible? Consider, for example, the usual regular lattice with vertices of the form $\backslash$[$\backslash${\{}(i,j):0$\backslash$le i,-i$\backslash$le j$\backslash$le i and j has the parity of i$\backslash${\}}$\backslash$] and oriented edges {\$}(i,j)\backslashto (i+1,j+s){\$}, where {\$}s=\backslashpm1{\$}. We show that for paths of length {\$}m{\$} starting at the origin, the hypotheses become distinguishable (in a minimax sense) if {\$}\backslashmu{\_}m\backslashgg1/\backslashsqrt{\{}\backslashlog m{\}}{\$}, while they are not if {\$}\backslashmu{\_}m\backslashll1/\backslashlog m{\$}. We derive equivalent results in a Bayesian setting where one assumes that all paths are equally likely; there, the asymptotic threshold is {\$}\backslashmu{\_}m\backslashapprox m{\^{}}{\{}-1/4{\}}{\$}. We obtain corresponding results for trees (where the threshold is of order 1 and independent of the size of the tree), for distributions other than the Gaussian and for other graphs. The concept of the predictability profile, first introduced by Benjamini, Pemantle and Peres, plays a crucial role in our analysis.},
archivePrefix = {arXiv},
arxivId = {math/0701668},
author = {Arias-Castro, Ery and Cand{\`{e}}s, Emmanuel J. and Helgason, Hannes and Zeitouni, Ofer},
doi = {10.1214/07-AOS526},
eprint = {0701668},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Arias-Castro et al. - 2008 - Searching for a trail of evidence in a maze.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian detection,Detecting a chain of nodes in a network,Epidemics,Exponential families of random variables,Martingales,Minimax detection,NotRead,Predictability profile of a stochastic process},
mendeley-tags = {Epidemics,NotRead},
number = {4},
pages = {1726--1757},
primaryClass = {math},
title = {{Searching for a trail of evidence in a maze}},
volume = {36},
year = {2008}
}
@article{Kulldorff2005,
abstract = {BACKGROUND: The ability to detect disease outbreaks early is important in order to minimize morbidity and mortality through timely implementation of disease prevention and control measures. Many national, state, and local health departments are launching disease surveillance systems with daily analyses of hospital emergency department visits, ambulance dispatch calls, or pharmacy sales for which population-at-risk information is unavailable or irrelevant. METHODS AND FINDINGS: We propose a prospective space-time permutation scan statistic for the early detection of disease outbreaks that uses only case numbers, with no need for population-at-risk data. It makes minimal assumptions about the time, geographical location, or size of the outbreak, and it adjusts for natural purely spatial and purely temporal variation. The new method was evaluated using daily analyses of hospital emergency department visits in New York City. Four of the five strongest signals were likely local precursors to citywide outbreaks due to rotavirus, norovirus, and influenza. The number of false signals was at most modest. CONCLUSION: If such results hold up over longer study times and in other locations, the space-time permutation scan statistic will be an important tool for local and national health departments that are setting up early disease detection surveillance systems.},
author = {Kulldorff, Martin and Heffernan, Richard and Hartman, Jessica and Assun????o, Renato and Mostashari, Farzad},
doi = {10.1371/journal.pmed.0020059},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kulldorff et al. - 2005 - A space-time permutation scan statistic for disease outbreak detection.pdf:pdf},
isbn = {1549-1676 (Electronic)$\backslash$n1549-1277 (Linking)},
issn = {15491277},
journal = {PLoS Medicine},
keywords = {Epidemics,NotRead},
mendeley-tags = {Epidemics,NotRead},
number = {3},
pages = {0216--0224},
pmid = {15719066},
title = {{A space-time permutation scan statistic for disease outbreak detection}},
volume = {2},
year = {2005}
}
@article{Meirom2014,
abstract = {We consider the problem of detecting an epidemic in a population where individual diagnoses are extremely noisy. The motivation for this problem is the plethora of examples (influenza strains in humans, or computer viruses in smartphones, etc.) where reliable diagnoses are scarce, but noisy data plentiful. In flu/phone-viruses, exceedingly few infected people/phones are professionally diagnosed (only a small fraction go to a doctor) but less reliable secondary signatures (e.g., people staying home, or greater-than-typical upload activity) are more readily available. These secondary data are often plagued by unreliability: many people with the flu do not stay home, and many people that stay home do not have the flu. This paper identifies the precise regime where knowledge of the contact network enables finding the needle in the haystack: we provide a distributed, efficient and robust algorithm that can correctly identify the existence of a spreading epidemic from highly unreliable local data. Our algorithm requires only local-neighbor knowledge of this graph, and in a broad array of settings that we describe, succeeds even when false negatives and false positives make up an overwhelming fraction of the data available. Our results show it succeeds in the presence of partial information about the contact network, and also when there is not a single "patient zero", but rather many (hundreds, in our examples) of initial patient-zeroes, spread across the graph.},
archivePrefix = {arXiv},
arxivId = {1402.1263},
author = {Meirom, Eli A. and Milling, Chris and Caramanis, Constantine and Mannor, Shie and Orda, Ariel and Shakkottai, Sanjay},
eprint = {1402.1263},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Meirom et al. - 2014 - Localized epidemic detection in networks with overwhelming noise.pdf:pdf},
keywords = {Constantine,Epidemics,NotRead},
mendeley-tags = {Constantine,Epidemics,NotRead},
pages = {1--27},
title = {{Localized epidemic detection in networks with overwhelming noise}},
url = {http://arxiv.org/abs/1402.1263},
year = {2014}
}
@incollection{Young2008,
address = {Boston, MA},
author = {Young, Neal E},
booktitle = {Encyclopedia of Algorithms},
doi = {10.1007/978-0-387-30162-4_175},
editor = {Kao, Ming-Yang},
isbn = {978-0-387-30162-4},
pages = {1--99},
publisher = {Springer US},
title = {{Greedy Set-Cover Algorithms}},
url = {http://dx.doi.org/10.1007/978-0-387-30162-4{\_}175},
year = {2008}
}
@article{Lund:1994:HAM:185675.306789,
address = {New York, NY, USA},
author = {Lund, Carsten and Yannakakis, Mihalis},
doi = {10.1145/185675.306789},
issn = {0004-5411},
journal = {J. ACM},
keywords = {NP-hard,approximation algorithms,chromatic number,clique cover,dominating set,graph coloring,hitting set,independent set,probabilistically checkable proofs,set cover},
number = {5},
pages = {960--981},
publisher = {ACM},
title = {{On the Hardness of Approximating Minimization Problems}},
url = {http://doi.acm.org/10.1145/185675.306789},
volume = {41},
year = {1994}
}
@misc{Karp2010,
abstract = {A large class of computational problems involve the determination of properties of graphs, digraphs, integers, arrays of integers, finite families of finite sets, boolean formulas and elements of other countable domains. Through simple encodings from such domains into the set of words over a finite alphabet these problems can be converted into language recognition problems, and we can inquire into their computational complexity. It is reasonable to consider such a problem satisfactorily solved when an algorithm for its solution is found which terminates within a number of steps bounded by a polynomial in the length of the input. We show that a large number of classic unsolved problems of covering, matching, packing, routing, assignment and sequencing are equivalent, in the sense that either each of them possesses a polynomial-bounded algorithm or none of them does.},
author = {Karp, Richard M.},
booktitle = {50 Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art},
doi = {10.1007/978-3-540-68279-0_8},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Karp - 2010 - Reducibility among combinatorial problems.pdf:pdf},
isbn = {9783540682745},
issn = {00224812},
pages = {219--241},
pmid = {15890271},
title = {{Reducibility among combinatorial problems}},
year = {2010}
}
@incollection{Vazirani2003,
address = {Berlin, Heidelberg},
author = {Vazirani, Vijay V},
booktitle = {Approximation Algorithms},
doi = {10.1007/978-3-662-04565-7_2},
isbn = {978-3-662-04565-7},
keywords = {SetCover},
mendeley-tags = {SetCover},
pages = {15--26},
publisher = {Springer Berlin Heidelberg},
title = {{Set Cover}},
url = {http://dx.doi.org/10.1007/978-3-662-04565-7{\_}2},
year = {2003}
}
@incollection{Vemuganti1999,
address = {Boston, MA},
author = {Vemuganti, R R},
booktitle = {Handbook of Combinatorial Optimization: Volume1--3},
doi = {10.1007/978-1-4613-0303-9_9},
editor = {Du, Ding-Zhu and Pardalos, Panos M},
isbn = {978-1-4613-0303-9},
keywords = {SetCover},
mendeley-tags = {SetCover},
pages = {573--746},
publisher = {Springer US},
title = {{Applications of Set Covering, Set Packing and Set Partitioning Models: A Survey}},
url = {http://dx.doi.org/10.1007/978-1-4613-0303-9{\_}9},
year = {1999}
}
@article{Shekhar2009,
author = {Shekhar, Himanshu},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shekhar - 2009 - Survey of Approximation Algorithms for Set Cover Problem.pdf:pdf},
journal = {Network},
keywords = {SetCover},
mendeley-tags = {SetCover},
title = {{Survey of Approximation Algorithms for Set Cover Problem}},
year = {2009}
}
@misc{Sanov1961,
author = {Sanov, Ivan N.},
booktitle = {Selected Translations in Mathematical Statistics and Probability},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Sanov - 1961 - On the Probability of Large Deviations of Random Variables.pdf:pdf},
pages = {213--224},
title = {{On the Probability of Large Deviations of Random Variables}},
volume = {1},
year = {1961}
}
@article{Beel2016,
abstract = {In the last 16 years, more than 200 research arti-cles were published about research-paper recommender sys-tems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 {\%}). Collaborative filtering was applied by only 18 {\%} of the reviewed approaches, and graph-based recommendations by 16 {\%}. Other recommenda-tion concepts included stereotyping, item-centric recommen-dations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addi-tion to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and B Joeran Beel sometimes it performed worse. We identified three poten-tial reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommenda-tions approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong vari-ations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a sec-ond limitation, we noted that many authors neglected to take into account factors other than accuracy, for exam-ple overall user satisfaction. In addition, most approaches (81 {\%}) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 {\%} of the approaches. Finally, few research papers had an impact on research-paper rec-ommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 {\%} of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user model-ing, a platform for researchers to exchange information, and an open-source framework that bundles the available recom-mendation approaches. 123 306 J. Beel et al.},
author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
doi = {10.1007/s00799-015-0156-0},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Beel et al. - 2016 - Research-paper recommender systems a literature survey.pdf:pdf},
issn = {14321300},
journal = {International Journal on Digital Libraries},
keywords = {Content based filtering,Google2017,NotRead,Recommender,Recommender system,Research paper recommender systems,Review,Survey,User modeling},
mendeley-tags = {Google2017,NotRead,Recommender},
number = {4},
pages = {305--338},
title = {{Research-paper recommender systems: a literature survey}},
volume = {17},
year = {2016}
}
@article{Kirkpatrick,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scal-able and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially. synaptic consolidation | artificial intelligence | stability plasticity | continual learning | deep learning},
annote = {NULL},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
doi = {10.1073/pnas.1611835114},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick et al. - Unknown - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
title = {{Overcoming catastrophic forgetting in neural networks}}
}
@article{Platt1999,
abstract = {Preface -- Introduction to support vector learning -- Roadmap -- Three remarks on the support vector method of function estimation / Valdimir Vapnik -- Generalization performance of support vector machines and other pattern classifiers / Peter Bartlett and John Shawe-Taylor -- Bayesian voting schemes and large margin classifiers / Nello Cristianini and John Shawe-Taylor -- Support vector machines, reproducing kernal Hilbert spaces, and randomized GACV / Grace Wahba -- Geometry and invariance in kernel based methods / Christopher J.C. Burgess -- On the annealed VC entropy for margin classifiers : a statistical mechanics study / Manfred Opper -- Entropy numbers, operators and support vector kernels / Robert C. Williamson, Alex J. Smola and Berhard Schölkopf -- Solving the quadratic programming problem arising in support vector classification / Linda Kaufman -- Making large-scale support vector machine learning practical / Thorsten Joachims -- Fast training of support vector machines using sequential minimal optimization / John C. Platt -- Support vector machines for dynamic reconstruction of a chaotic system / David Mattera and Simon Haykin -- Using support vector machines for time series prediction / Klaus-Robert Müller . [and others] -- Pairwise classification and support vector machines / Ulrich Kressel -- Reducing the run-time complexity in support vector machines / Edgar E. Osuna and Federico Girosi -- Support vector regression with ANOVA decomposition kernels / Mark O. Stitson . [and others] -- Support vector density estimation / Jason Weston . [and others] -- Combining support vector and mathematical programming methods for classification / Kristin P. Bennett -- Kernel principal component analysis / Berhard Schölkopf, Alex J. Smola and Klaus-Robert Müller.},
author = {Platt, John C.},
doi = {10.1109/ISKE.2008.4731075},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Platt - 1999 - Fast training of support vector machines using sequential minimal optimization.pdf:pdf},
isbn = {0262194163},
journal = {Advances in kernel methods},
keywords = {BestOf},
mendeley-tags = {BestOf},
pages = {376},
pmid = {6136161575780916930},
title = {{Fast training of support vector machines using sequential minimal optimization}},
url = {http://dl.acm.org/citation.cfm?id=299105},
year = {1999}
}
@article{Lafferty2001,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.4088v1},
author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando C N},
doi = {10.1038/nprot.2006.61},
eprint = {arXiv:1011.4088v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lafferty, McCallum, Pereira - 2001 - Conditional random fields Probabilistic models for segmenting and labeling sequence data.pdf:pdf},
isbn = {1558607781},
issn = {1750-2799},
journal = {ICML '01 Proceedings of the Eighteenth International Conference on Machine Learning},
keywords = {Animals,Antigen,BestOf,Gene Transfer Techniques,Genetic Engineering,Genetic Engineering: methods,Genetic Vectors,Genetic Vectors: metabolism,Mice,Receptors,Retroviridae,Retroviridae: genetics,Stem Cells,Stem Cells: metabolism,T-Cell,Transgenic,Transgenic: genetics,Transgenic: metabolism,alpha-beta,alpha-beta: genetics,alpha-beta: metabolism},
mendeley-tags = {BestOf},
number = {June},
pages = {282--289},
pmid = {17406263},
title = {{Conditional random fields: Probabilistic models for segmenting and labeling sequence data}},
url = {http://repository.upenn.edu/cis{\_}papers/159/{\%}5Cnhttp://dl.acm.org/citation.cfm?id=655813},
volume = {8},
year = {2001}
}
@article{Yann1998,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read superficially. Incredibly well written. Sumarry of conclusions in case it was too complicated. 

Model of writting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Yann, Lecun},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Yann - 1998 - Efficient backprop.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Neural networks: tricks of the trade},
keywords = {BestOf,READ,icle},
mendeley-tags = {BestOf,READ},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Efficient backprop}},
volume = {53},
year = {1998}
}
@article{Quinlan1986,
abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
author = {Quinlan, J. R.},
doi = {10.1023/A:1022643204877},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Quinlan - 1986 - Induction of Decision Trees.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {BestOf,classification,decision trees,expert systems,induction,information theory,knowledge acquisition},
mendeley-tags = {BestOf},
number = {1},
pages = {81--106},
pmid = {17050186},
title = {{Induction of Decision Trees}},
volume = {1},
year = {1986}
}
@article{Freund1995,
abstract = {In the first part of the paper we consider the problem of dynamically$\backslash$n$\backslash$napportioning resources among a set of options in a worst-case on-line$\backslash$n$\backslash$nframework. The model we study can be interpreted as a broad, abstract$\backslash$n$\backslash$nextension of the well-studied on-line prediction model to a general$\backslash$n$\backslash$ndecision-theoretic setting. We show that the multiplicative weight-$\backslash$n$\backslash$nupdate LittlestoneWarmuth rule can be adapted to this model, yielding$\backslash$n$\backslash$nbounds that are slightly weaker in some cases, but applicable to a$\backslash$ncon-$\backslash$n$\backslash$nsiderably more general class of learning problems. We show how the$\backslash$n$\backslash$nresulting learning algorithm can be applied to a variety of problems,$\backslash$n$\backslash$nincluding gambling, multiple-outcome prediction, repeated games, and$\backslash$n$\backslash$nprediction of points in R{\^{}}{\{}n{\}}. In the second part of the paper we$\backslash$napply the$\backslash$n$\backslash$nmultiplicative weight-update technique to derive a new boosting algo-$\backslash$n$\backslash$nrithm. This boosting algorithm does not require any prior knowledge$\backslash$n$\backslash$nabout the performance of the weak learning algorithm. We also study$\backslash$n$\backslash$ngeneralizations of the new boosting algorithm to the problem of$\backslash$n$\backslash$nlearning functions whose range, rather than being binary, is an arbitrary$\backslash$n$\backslash$nfinite set or a bounded segment of the real line.},
author = {Freund, Y and Schapire, Re},
doi = {10.1006/jcss.1997.1504},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Freund, Schapire - 1995 - A desicion-theoretic generalization of on-line learning and an application to boosting.pdf:pdf},
isbn = {3540591192},
issn = {00220000},
journal = {Computational learning theory},
keywords = {Adaboost,BestOf,boosting,multi-class classification},
mendeley-tags = {BestOf},
pages = {119--139},
pmid = {10394},
title = {{A desicion-theoretic generalization of on-line learning and an application to boosting}},
url = {http://link.springer.com/chapter/10.1007/3-540-59119-2{\_}166},
volume = {55},
year = {1995}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
keywords = {BestOf},
mendeley-tags = {BestOf},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@book{Cover2006,
abstract = {The latest edition of this classic is updated with new problem sets and material The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory. All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points. The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications. An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
archivePrefix = {arXiv},
arxivId = {ISBN 0-471-06259-6},
author = {Cover, Thomas. M and Thomas, Joy. a},
doi = {10.1002/047174882X},
eprint = {ISBN 0-471-06259-6},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Cover, Thomas - 2006 - Elements of Information Theory. Second Edition.pdf:pdf},
isbn = {9780471241959},
issn = {15579654},
keywords = {Books},
mendeley-tags = {Books},
pmid = {20660925},
title = {{Elements of Information Theory. Second Edition}},
url = {http://www.elementsofinformationtheory.com/{\%}5Cnhttp://onlinelibrary.wiley.com/doi/10.1002/047174882X.fmatter/summary},
year = {2006}
}
@article{Cortes1995,
abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
eprint = {arXiv:1011.1669v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Vapnik - 1995 - Support-Vector Networks.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {BestOf,efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
mendeley-tags = {BestOf},
number = {3},
pages = {273--297},
pmid = {19549084},
title = {{Support-Vector Networks}},
volume = {20},
year = {1995}
}
@article{Williamson2011,
abstract = {Discrete optimization problems are everywhere, from traditional operations research planning problems, such as scheduling, facility location, and network design; to computer science problems in databases; to advertising issues in viral marketing. Yet most such problems are NP-hard. Thus unless P = NP, there are no efficient algorithms to find optimal solutions to such problems. This book shows how to design approximation algorithms: efficient algorithms that find provably near-optimal solutions. The book is organized around central algorithmic techniques for designing approximation algorithms, including greedy and local search algorithms, dynamic programming, linear and semidefinite programming, and randomization. Each chapter in the first part of the book is devoted to a single algorithmic technique, which is then applied to several different problems. The second part revisits the techniques but offers more sophisticated treatments of them. The book also covers methods for proving that optimization problems are hard to approximate. Designed as a textbook for graduate-level algorithms courses, the book will also serve as a reference for researchers interested in the heuristic solution of discrete optimization problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Williamson, David P. and Shmoys, David B.},
doi = {10.1017/CBO9780511921735},
eprint = {arXiv:1011.1669v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Williamson, Shmoys - 2011 - The Design of Approximation Algorithms.pdf:pdf},
isbn = {0521195276},
issn = {1098-6596},
keywords = {Books},
mendeley-tags = {Books},
pages = {504},
pmid = {25246403},
title = {{The Design of Approximation Algorithms}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=Cc{\_}Fdqf3bBgC{\&}pgis=1},
year = {2011}
}
@article{Newman2001,
abstract = {Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0007235},
author = {Newman, M E and Strogatz, S H and Watts, D J},
doi = {10.1103/PhysRevE.64.026118},
eprint = {0007235},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Newman, Strogatz, Watts - 2001 - Random graphs with arbitrary degree distributions and their applications.pdf:pdf},
isbn = {1539-3755 (Print)$\backslash$r1539-3755 (Linking)},
issn = {1063-651X},
journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},
keywords = {Epidemics},
mendeley-tags = {Epidemics},
number = {2 Pt 2},
pages = {026118},
pmid = {11497662},
primaryClass = {cond-mat},
title = {{Random graphs with arbitrary degree distributions and their applications.}},
volume = {64},
year = {2001}
}
@article{DelVicario2016,
abstract = {SignificanceThe wide availability of user-provided content in online social media facilitates the aggregation of people around common interests, worldviews, and narratives. However, the World Wide Web is a fruitful environment for the massive diffusion of unverified rumors. In this work, using a massive quantitative analysis of Facebook, we show that information related to distinct narratives--conspiracy theories and scientific news--generates homogeneous and polarized communities (i.e., echo chambers) having similar information consumption patterns. Then, we derive a data-driven percolation model of rumor spreading that demonstrates that homogeneity and polarization are the main determinants for predicting cascades size. The wide availability of user-provided content in online social media facilitates the aggregation of people around common interests, worldviews, and narratives. However, the World Wide Web (WWW) also allows for the rapid dissemination of unsubstantiated rumors and conspiracy theories that often elicit rapid, large, but naive social responses such as the recent case of Jade Helm 15--where a simple military exercise turned out to be perceived as the beginning of a new civil war in the United States. In this work, we address the determinants governing misinformation spreading through a thorough quantitative analysis. In particular, we focus on how Facebook users consume information related to two distinct narratives: scientific and conspiracy news. We find that, although consumers of scientific and conspiracy stories present similar consumption patterns with respect to content, cascade dynamics differ. Selective exposure to content is the primary driver of content diffusion and generates the formation of homogeneous clusters, i.e., "echo chambers." Indeed, homogeneity appears to be the primary driver for the diffusion of contents and each echo chamber has its own cascade dynamics. Finally, we introduce a data-driven percolation model mimicking rumor spreading and we show that homogeneity and polarization are the main determinants for predicting cascades size.},
author = {{Del Vicario}, Michela and Bessi, Alessandro and Zollo, Fabiana and Petroni, Fabio and Scala, Antonio and Caldarelli, Guido and Stanley, H. Eugene and Quattrociocchi, Walter},
doi = {10.1073/pnas.1517441113},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Del Vicario et al. - 2016 - The spreading of misinformation online.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
keywords = {Epidemics,FakeNews,KindaBad},
mendeley-tags = {Epidemics,FakeNews,KindaBad},
pages = {201517441},
pmid = {24889601},
title = {{The spreading of misinformation online}},
url = {http://www.pnas.org/content/early/2016/01/02/1517441113.abstract},
year = {2016}
}
@article{Grimmett2012,
abstract = {This introduction to some of the principal models in the theory of disordered systems leads the reader through the basics, to the very edge of contemporary research, with the minimum of technical fuss. Topics covered include random walk, percolation, self-avoiding walk, interacting particle systems, uniform spanning tree, random graphs, as well as the Ising, Potts, and random-cluster models for ferromagnetism, and the Lorentz model for motion in a random medium. SchrammL{\"{o}}wner evolutions (SLE) arise in various contexts. The choice of topics is strongly motivated by modern applications and focuses on areas that merit further research. Special features include a simple account of Smirnov's proof of Cardy's formula for critical percolation, and a fairly full account of the theory of influence and sharp-thresholds. Accessible to a wide audience of mathematicians and physicists, this book can be used as a graduate course text. Each chapter ends with a range of exercises.},
author = {Grimmett, Geoffrey},
doi = {10.1080/00107514.2012.665392},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Grimmett - 2012 - Probability on Graphs Random Processes on Graphs and Lattices.pdf:pdf},
isbn = {978-0-521-19798-4},
issn = {0010-7514},
journal = {Contemporary Physics},
number = {3},
pages = {291--292},
title = {{Probability on Graphs: Random Processes on Graphs and Lattices}},
volume = {53},
year = {2012}
}
@article{Newman2002,
abstract = {The study of social networks, and in particular the spread of disease on networks, has attracted considerable recent attention in the physics community. In this paper, we show that a large class of standard epidemiological models, the so-called susceptible/infective/removed (SIR) models can be solved exactly on a wide variety of networks. In addition to the standard but unrealistic case of fixed infectiveness time and fixed and uncorrelated probability of transmission between all pairs of individuals, we solve cases in which times and probabilities are non-uniform and correlated. We also consider one simple case of an epidemic in a structured population, that of a sexually transmitted disease in a population divided into men and women. We confirm the correctness of our exact solutions with numerical simulations of SIR epidemics on networks.},
annote = {Hypothesis: 
SIR epidemics can be modeled as bond percolation.


Methodology: 
- reduced the problem to an easier one with like Tij being iid 
- derived equations for the generating functions that were solved numerically
- derived size of epidemic cluster, and Tc through the equations
- experimentally verified the model through simulation

Result(s): 
- In this paper, we have shown that a large class of the so-called SIRmodels of epidemic disease can be solved exactly on networks of various kinds using a combination of mapping to percolation models and generating function methods
- solutions for simple unipartite graphs with arbitrary degree distributions and hetero- geneous and possibly correlated infectiveness times and transmission probabilities

Summary of key points: 

Context (how this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself): 
I think it's one of the fundationnal papers on epidemics on networks (2002). It showed the difference btween epidemics on a wide variety of networks and on the fully mixed case.

Significance (to the field; in relation to your own work): 
It was interesting to see the epidemics as a one time percolation, and not as a process. However, I think this is only possible due to the R, and wouldn't apply to the SIS.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0205009},
author = {Newman, Mark E. J.},
doi = {10.1103/PhysRevE.66.016128},
eprint = {0205009},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Newman - 2002 - Spread of epidemic disease on networks.pdf:pdf},
isbn = {1539-3755 (Print)},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {Classic,Epidemics},
mendeley-tags = {Classic,Epidemics},
number = {1},
pmid = {12241447},
primaryClass = {cond-mat},
title = {{Spread of epidemic disease on networks}},
volume = {66},
year = {2002}
}
@article{Wan,
abstract = {Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain " correctness " guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.},
author = {Wan, Yali and Meil˘, Marina},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Wan, Meil˘ - Unknown - Graph Clustering Block-models and model free results.pdf:pdf},
title = {{Graph Clustering: Block-models and model free results}}
}
@article{Tamar,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural net-work with a 'planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a con-volutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Tamar et al. - Unknown - Value Iteration Networks.pdf:pdf},
keywords = {BestOf,NIPS 2016},
mendeley-tags = {BestOf,NIPS 2016},
title = {{Value Iteration Networks}}
}
@article{Lokhov,
abstract = {Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabili-ties. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs.},
author = {Lokhov, Andrey Y},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lokhov - Unknown - Reconstructing Parameters of Spreading Models from Partial Observations.pdf:pdf},
title = {{Reconstructing Parameters of Spreading Models from Partial Observations}}
}
@article{Zhu,
author = {Zhu, Kai and Ying, Lei},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Ying - Unknown - Information Source Detection in Networks Possibility and Impossibility Results.pdf:pdf},
keywords = {Ep},
mendeley-tags = {Ep},
title = {{Information Source Detection in Networks : Possibility and Impossibility Results}}
}
@article{Mansour2011,
author = {Mansour, Yishay},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Mansour - 2011 - Lecture 5 Lower Bounds using Information Theory Tools Distance between Distributions KL-Divergence.pdf:pdf},
title = {{Lecture 5 : Lower Bounds using Information Theory Tools Distance between Distributions KL-Divergence}},
year = {2011}
}
@book{Lyons2010,
abstract = {This book began as lecture notes for an advanced graduate course called Probability on Trees that Lyons gave in Spring 1993. We are grateful to Rabi Bhattacharya for having suggested that he teach such a course. We have attempted to preserve the informal flavor of lectures. Many exercises are also included, so that real courses can be given based on the book. Indeed, previous versions have already been used for courses or seminars in several countries. The most current version of the book can be found on the web. A few of the authors results and proofs appear here for the first time. At this point, almost all of the actual writing was done by Lyons. We hope to have a more balanced co-authorship eventually. This book is concerned with certain aspects of discrete probability on infinite graphs that are currently in vigorous development. We feel that there are three main classes of graphs on which discrete probability is most interesting, namely, trees, Cayley graphs of groups (or more generally, transitive, or even quasi-transitive, graphs), and planar graphs. Thus, this book develops the general theory of certain probabilistic processes and then specializes to these particular classes. In addition, there are several reasons for a special study of trees. Since in most cases, analysis is easier on trees, analysis can be carried further. Then one can often either apply the results from trees to other situations or can transfer to other situations the techniques developed by working on trees. Trees also occur naturally in many situations, either combinatorially or as descriptions of compact sets in Euclidean space Rd. (More classical probability, of course, has tended to focus on the special and important case of the Euclidean lattices Zd.) It is well known that there are many connections among probability, trees, and groups. We survey only some of them, concentrating on recent developments of the past twenty years. We discuss some of those results that are most interesting, most beautiful, or easiest to prove without much background. Of course, we are biased by our own research interests and knowledge. We include the best proofs available of recent as well as classic results. Much more is known about almost every subject we present. The only prerequisite is knowledge of conditional expectation with respect to a $\sigma$-algebra, and even that is rarely used. For part of Chapter 13 and all of Chapter 16, basic knowledge of ergodic theory is also required. Most exercises that appear in the text, as opposed to those at the end of the chapters, are ones that will be particularly helpful to do when they are reached. They either facilitate ones understanding or will be used later in the text. These in-text exercises are also collected at the end of each chapter for easy reference, just before additional exercises are presented.},
author = {Lyons, Russell and Peres, Yuval},
booktitle = {New York},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lyons, Peres - 2010 - Probability on trees and networks.pdf:pdf},
isbn = {9781107160156},
keywords = {Book},
mendeley-tags = {Book},
number = {September},
pages = {1--577},
title = {{Probability on trees and networks}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Probability+on+Trees+and+Networks{\#}0},
year = {2010}
}
@article{,
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The An.pdf:pdf},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. {\textregistered} www.jstor.org}}
}
@article{Drakopoulos2015,
abstract = {We consider an SIS-type epidemic process that evolves on a known graph. We assume that a fixed curing budget can be allocated at each instant to the nodes of the graph, towards the objective of minimizing the expected extinction time of the epidemic. We provide a lower bound on the optimal expected extinction time as a function of the available budget, the epidemic parameters, the maximum degree, and the CutWidth of the graph. For graphs with large CutWidth (close to the largest possible), and under a budget which is sublinear in the number of nodes, our lower bound scales exponentially with the size of the graph.},
archivePrefix = {arXiv},
arxivId = {1510.06055},
author = {Drakopoulos, Kimon and Ozdaglar, Asuman and Tsitsiklis, John N.},
doi = {10.1109/CDC.2015.7402770},
eprint = {1510.06055},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Drakopoulos, Ozdaglar, Tsitsiklis - 2015 - A lower bound on the performance of dynamic curing policies for epidemics on graphs.pdf:pdf},
isbn = {9781479978854},
issn = {07431546},
keywords = {Epidemics,NotRead},
mendeley-tags = {Epidemics,NotRead},
number = {978},
pages = {3560--3567},
title = {{A lower bound on the performance of dynamic curing policies for epidemics on graphs}},
year = {2015}
}
@book{Kreyszig1978,
abstract = {Provides avenues for applying functional analysis to the practical study of natural sciences as well as mathematics. Contains worked problems on Hilbert space theory and on Banach spaces and emphasizes concepts, principles, methods and major applications of functional analysis.},
author = {Kreyszig, Erwin},
booktitle = {John Wiley},
doi = {10.1002/zamm.19660460126},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Kreyszig - 1978 - Introductory Functional Analysis with Applications.pdf:pdf},
isbn = {0471504599},
issn = {00442267},
pages = {704},
title = {{Introductory Functional Analysis with Applications}},
url = {http://books.google.com/books?id=nZmpQgAACAAJ{\&}pgis=1},
year = {1978}
}
@article{,
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - The Shape of Alerts Robust Community-level Malware Detection.pdf:pdf},
title = {{The Shape of Alerts : Robust Community-level Malware Detection}}
}
@article{Miller2004,
abstract = {Miller and Childers have focused on creating a clear presentation of foundational concepts with specific applications to signal processing and communications, clearly the two areas of most interest to students and instructors in this course. It is aimed at graduate students as well as practicing engineers, and includes unique chapters on narrowband random processes and simulation techniques. The appendices provide a refresher in such areas as linear algebra, set theory, random variables, and more. Probability and Random Processes also includes applications in digital communications, information theory, coding theory, image processing, speech analysis, synthesis and recognition, and other fields. * Exceptional exposition and numerous worked out problems make the book extremely readable and accessible * The authors connect the applications discussed in class to the textbook * The new edition contains more real world signal processing and communications applications * Includes an entire chapter devoted to simulation techniques},
author = {Miller, S L and Childers, D},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Miller, Childers - 2004 - Probability and Random Processes.pdf:pdf},
isbn = {978-0-12-172651-5},
pages = {529},
title = {{Probability and Random Processes}},
year = {2004}
}
@article{,
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Detecting Cascades from Weak Signatures.pdf:pdf},
title = {{Detecting Cascades from Weak Signatures}}
}
@article{Ganesh,
author = {Ganesh, A and Massouli{\'{e}}, L and Towsley, D},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Ganesh, Massouli{\'{e}}, Towsley - Unknown - The Effect of Network Topology on the Spread of Epidemics.pdf:pdf},
keywords = {Epidemics,NotRead},
mendeley-tags = {Epidemics,NotRead},
pages = {1--12},
title = {{The Effect of Network Topology on the Spread of Epidemics}}
}
@article{Gallager2013,
author = {Gallager, Robert},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Gallager - 2013 - Stochastic Processes 9 - Random Walks, Large Deviations, and Martingales.pdf:pdf},
journal = {Stochastic Processes},
title = {{Stochastic Processes: 9 - Random Walks, Large Deviations, and Martingales}},
year = {2013}
}
@article{Xu2012,
abstract = {Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efficient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.},
archivePrefix = {arXiv},
arxivId = {1010.4237},
author = {Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay},
doi = {10.1109/TIT.2011.2173156},
eprint = {1010.4237},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Xu, Caramanis, Sanghavi - 2012 - Robust PCA via outlier pursuit.pdf:pdf},
isbn = {9781617823800},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Constantine,FastRead},
mendeley-tags = {Constantine,FastRead},
number = {5},
pages = {3047--3064},
title = {{Robust PCA via outlier pursuit}},
volume = {58},
year = {2012}
}
@article{Addario-Berry2010,
abstract = {We study a class of hypothesis testing problems in which, upon observ- ing the realization of an n-dimensional Gaussian vector, one has to decide whether the vector was drawn from a standard normal distribution or, alter- natively, whether there is a subset of the components belonging to a certain given class of sets whose elements have been “contaminated,” that is, have a mean different from zero. We establish some general conditions under which testing is possible and others under which testing is hopeless with a small risk. The combinatorial and geometric structure of the class of sets is shown to play a crucial role. The bounds are illustrated on various examples.},
archivePrefix = {arXiv},
arxivId = {0908.3437},
author = {Addario-Berry, Louigi and Broutin, Nicolas and Devroye, Luc and Lugosi, G??bor},
doi = {10.1214/10-AOS817},
eprint = {0908.3437},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Addario-Berry et al. - 2010 - On combinatorial testing problems.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Gaussian processes.,Hypothesis testing,Multiple hypotheses},
number = {5},
pages = {3063--3092},
title = {{On combinatorial testing problems}},
volume = {38},
year = {2010}
}
@article{Blei2006,
abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural param-eters of the multinomial distributions that repre-sent the topics. Variational approximations based on Kalman filters and nonparametric wavelet re-gression are developed to carry out approximate posterior inference over the latent topics. In addi-tion to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demon-strated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Blei, David M and Lafferty, John D},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Blei, Lafferty - 2006 - Dynamic Topic Models.pdf:pdf},
isbn = {1595933832},
issn = {1932-6157},
journal = {International Conference on Machine Learning},
keywords = {BestOf,ICML2016,TestOfTime},
mendeley-tags = {BestOf,ICML2016,TestOfTime},
pages = {113--120},
pmid = {9013932},
title = {{Dynamic Topic Models}},
year = {2006}
}
@article{Bach2010,
abstract = {Set-functions appear in many areas of computer science and applied mathematics, such as machine learning, computer vision, operations research or electrical networks. Among these set-functions, submodular functions play an important role, similar to convex functions on vector spaces. In this tutorial, the theory of submodular functions is presented, in a self-contained way, with all results shown from first principles. A good knowledge of convex analysis is assumed.},
archivePrefix = {arXiv},
arxivId = {1010.4207},
author = {Bach, Francis},
eprint = {1010.4207},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bach - 2010 - Convex Analysis and Optimization with Submodular Functions a Tutorial.pdf:pdf},
journal = {October},
keywords = {NotRead,Submodular,Tutorial},
mendeley-tags = {NotRead,Submodular,Tutorial},
pages = {1--36},
title = {{Convex Analysis and Optimization with Submodular Functions: a Tutorial}},
url = {http://arxiv.org/abs/1010.4207},
year = {2010}
}
@article{Sharpnack2012a,
abstract = {We consider the detection of activations over graphs under Gaussian noise, where signals are piece-wise constant over the graph. Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies. We cast this as a hypothesis testing problem, and first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses. We then introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph, and prove that for any spanning tree, this approach can distinguish null from alternative in a low signal-to-noise regime. Lastly, we improve on this result and show that using the uniform spanning tree in the basis construction yields a randomized test with stronger theoretical guarantees that in many cases matches our necessary conditions. Specifically, we obtain near-optimal performance in edge transitive graphs, {\$}k{\$}-nearest neighbor graphs, and {\$}\backslashepsilon{\$}-graphs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.0937v3},
author = {Sharpnack, James and Krishnamurthy, Akshay Krishnamurthy and Singh, Aarti},
eprint = {arXiv:1206.0937v3},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Sharpnack, Krishnamurthy, Singh - 2012 - Detecting activations over graphs using spanning tree wavelet bases.pdf:pdf},
issn = {15337928},
journal = {arXiv preprint arXiv:1206.0937},
number = {Section 3},
pages = {536--544},
title = {{Detecting activations over graphs using spanning tree wavelet bases}},
url = {http://arxiv.org/abs/1206.0937},
year = {2012}
}
@article{Arias-castro2010,
author = {Arias-Castro, Ery and Cand{\`{e}}s, Emmanuel J and Durand, Arnaud},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Arias-castro, Cand{\`{e}}s, Durand - 2010 - APPENDIX A GENERAL TOOLS Let K.pdf:pdf},
keywords = {Constantine,Weak Signals},
mendeley-tags = {Constantine,Weak Signals},
pages = {1--22},
title = {{APPENDIX A: GENERAL TOOLS Let |K}},
year = {2010}
}
@article{Valiant1984,
abstract = {Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learned using it in a reasonable (polynomial) number of steps. Although inherent algorithmic complexity appears to set serious limits to the range of concepts that can be learned, we show that there are some important nontrivial classes of propositional concepts that can be learned in a realistic sense.},
annote = {Goal: give an infrstructure to describle learnable concepts

Hypothesis: We have acces to 2 functions, EXAMPLES and ORACLES, and we want to see which boolean functions can be learnt in polynomial amount of steps.

Methodology: 

Result(s): CNF, DNF on total vectors, and mu-functions are learnable.

Summary of key points: They use L(h,S), the minmum number of binomial trials with proba h you have to do until the proba of having fewer than S successes is lower than h.

Context (how this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself): First theory of learnable ever, introduce theory in ML.


Significance (to the field; in relation to your own work): Major


Important Figures and/or Tables (brief description; page number): 

Other Comments: I like the way they explain why they thought this was a good infrastructure and why the model they introduce is a good idea.},
author = {Valiant, L. G.},
doi = {10.1145/1968.1972},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Valiant - 1984 - A theory of the learnable.pdf:pdf},
isbn = {0897911334},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {Classic,READ},
mendeley-tags = {Classic,READ},
number = {11},
pages = {1134--1142},
pmid = {12929239},
title = {{A theory of the learnable}},
volume = {27},
year = {1984}
}
@article{Feldman2013,
abstract = {We introduce a framework for proving lower bounds on computational problems over distributions, based on a class of algorithms called statistical algorithms. For such algorithms, access to the input distribution is limited to obtaining an estimate of the expectation of any given function on a sample drawn randomly from the input distribution, rather than directly accessing samples. Most natural algorithms of interest in theory and in practice, e.g., moments-based methods, local search, standard iterative methods for convex optimization, MCMC and simulated annealing, are statistical algorithms or have statistical counterparts. Our framework is inspired by and generalizes the statistical query model in learning theory. Our main application is a nearly optimal lower bound on the complexity of any statistical algorithm for detecting planted bipartite clique distributions (or planted dense subgraph distributions) when the planted clique has size O(n{\^{}}(1/2-$\backslash$delta)) for any constant $\backslash$delta {\textgreater} 0. Variants of these problems have been assumed to be hard to prove hardness for other problems and for cryptographic applications. Our lower bounds provide concrete evidence of hardness, thus supporting these assumptions.},
archivePrefix = {arXiv},
arxivId = {1201.1214},
author = {Feldman, Vitaly and Grigorescu, Elena and Reyzin, Lev and Vempala, Santosh and Xiao, Ying},
doi = {10.1145/2488608.2488692},
eprint = {1201.1214},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Feldman et al. - 2013 - Statistical algorithms and a lower bound for detecting planted cliques.pdf:pdf},
isbn = {9781450320290},
issn = {07378017},
journal = {Proceedings of the 45th annual ACM symposium on Symposium on theory of computing - STOC '13},
keywords = {Constantine,Group meeting,Theory,lower bounds,planted clique,statistical algorithms,statistical query},
mendeley-tags = {Constantine,Group meeting,Theory},
number = {64},
pages = {655},
title = {{Statistical algorithms and a lower bound for detecting planted cliques}},
url = {http://dl.acm.org/citation.cfm?id=2488608.2488692},
volume = {64},
year = {2013}
}
@article{Singh2010,
author = {Singh, Aarti and Nowak, Robert and Calderbank, Robert},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Nowak, Calderbank - 2010 - Detecting Weak but Hierarchically-Structured Patterns in Networks.pdf:pdf},
keywords = {Constantine,Weak signals},
mendeley-tags = {Constantine,Weak signals},
pages = {749--756},
title = {{Detecting Weak but Hierarchically-Structured Patterns in Networks}},
year = {2010}
}
@article{Arias-castro,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.01009v1},
author = {Arias-Castro, Ery and Nov, S T},
eprint = {arXiv:1511.01009v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Arias-castro, Nov - Unknown - Detecting a Path of Correlations in a Network.pdf:pdf},
keywords = {Constantine,NotRead,Weak Signals},
mendeley-tags = {Constantine,NotRead,Weak Signals},
pages = {1--12},
title = {{Detecting a Path of Correlations in a Network}}
}
@article{Arias-castro2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1001.3209v2},
author = {Arias-castro, Ery and Cand{\`{e}}s, Emmanuel J and Durand, Arnaud},
doi = {10.1214/10-AOS839},
eprint = {arXiv:1001.3209v2},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Arias-castro, Cand{\`{e}}s, Durand - 2011 - Detection of an anomalous cluster in a network.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {Constantine,Weak signals},
mendeley-tags = {Constantine,Weak signals},
number = {1},
pages = {278--304},
title = {{Detection of an anomalous cluster in a network}},
volume = {39},
year = {2011}
}
@article{Sharpnack,
author = {Sharpnack, James and Krishnamurthy, Akshay and Singh, Aarti},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Sharpnack, Krishnamurthy, Singh - Unknown - Near-optimal Anomaly Detection in Graphs using Lov ´ asz Extended Scan Statistic.pdf:pdf},
keywords = {Constantine,Epidemics},
mendeley-tags = {Constantine,Epidemics},
pages = {1--13},
title = {{Near-optimal Anomaly Detection in Graphs using Lov ´ asz Extended Scan Statistic}}
}
@article{Li2011,
author = {Li, Yu-Feng and Zhou, Zhi-Hua},
doi = {10.1109/TPAMI.2014.2299812},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Li, Zhou - 2011 - Towards Making U nlabeled Data Never Hurt.pdf:pdf},
issn = {1939-3539},
journal = {Icml},
keywords = {Charles Martin,NotRead,Unlabeled},
mendeley-tags = {Charles Martin,NotRead,Unlabeled},
pmid = {26353217},
title = {{Towards Making U nlabeled Data Never Hurt}},
year = {2011}
}
@article{Fernandez-Delgado2014,
abstract = {We evaluate 179 classifers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classi ers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classi ers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve signi cant conclusions about the classi er behavior, not dependent on the data set collection. The classi ers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1{\%} of the maximum accuracy overcoming 90{\%} in the 84.3{\%} of the data sets. However, the difference is not statistically signi cant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3{\%} of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classi ers (3 out of 5 bests classi ers are RF), followed by SVM (4 classi ers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
author = {Fern{\'{a}}ndez-Delgado, Manuel and Cernadas, Eva and Barro, Sen{\'{e}}n and Amorim, Dinani},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Fern{\'{a}}ndez-Delgado et al. - 2014 - Do we Need Hundreds of Classifiers to Solve Real World Classification Problems.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Browsing,Charles Martin},
mendeley-tags = {Browsing,Charles Martin},
pages = {3133--3181},
title = {{Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?}},
url = {http://jmlr.org/papers/v15/delgado14a.html},
volume = {15},
year = {2014}
}
@article{Mitchell1998,
abstract = {We consider the problem of using a large unla- beled sample to boost performance of a learn- ing algorit,hrn when only a small set of labeled examples is available. In particular, we con- sider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the de- scription of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled ex- amples. Specifically, the presence of two dis- tinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algo- rithm's predictions on new unlabeled exam- ples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and un- labeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This},
author = {Mitchell, Tom and Blum, Avrim},
doi = {10.1145/279943.279962},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Mitchell, Blum - 1998 - Combining labeled and unlabeled data with co-training.pdf:pdf},
isbn = {1581130570},
journal = {Proceedings of the eleventh annual conference on Computational learning theory},
keywords = {Browsing,Charles Martin,Classic,NotRead,Unlabeled},
mendeley-tags = {Browsing,Charles Martin,Classic,NotRead,Unlabeled},
pages = {92--100},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279943.279962},
year = {1998}
}
@article{Socher2013,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the {\ldots}},
keywords = {Browsing,Charles Martin,NotRead,SVM,StructuralSVM},
mendeley-tags = {Browsing,Charles Martin,NotRead,SVM,StructuralSVM},
pages = {1631--1642},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
year = {2013}
}
@article{Pinto2012,
abstract = {How can we localize the source of diffusion in a complex network? Because of the tremendous size of many real networks-such as the internet or the human social graph-it is usually unfeasible to observe the state of all nodes in a network. We show that it is fundamentally possible to estimate the location of the source from measurements collected by sparsely placed observers. We present a strategy that is optimal for arbitrary trees, achieving maximum probability of correct localization. We describe efficient implementations with complexity O(N($\alpha$)), where $\alpha$=1 for arbitrary trees and $\alpha$=3 for arbitrary graphs. In the context of several case studies, we determine how localization accuracy is affected by various system parameters, including the structure of the network, the density of observers, and the number of observed cascades.},
archivePrefix = {arXiv},
arxivId = {1208.2534},
author = {Pinto, Pedro C. and Thiran, Patrick and Vetterli, Martin},
doi = {10.1103/PhysRevLett.109.068702},
eprint = {1208.2534},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Pinto, Thiran, Vetterli - 2012 - Locating the source of diffusion in large-scale networks.pdf:pdf},
isbn = {9781605585727},
issn = {00319007},
journal = {Physical Review Letters},
keywords = {Epidemics},
mendeley-tags = {Epidemics},
number = {6},
pages = {1--4},
pmid = {23006310},
title = {{Locating the source of diffusion in large-scale networks}},
volume = {109},
year = {2012}
}
@article{Tradeoffs,
author = {Tradeoffs, Near-optimal Sample-robustness-rank and Chen, Yudong and Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Tradeoffs et al. - Unknown - Matrix Completion with Column Manipulation.pdf:pdf},
keywords = {Constantine,Matrix completion,Prof},
mendeley-tags = {Constantine,Matrix completion,Prof},
number = {Xx},
title = {{Matrix Completion with Column Manipulation :}},
volume = {XX}
}
@article{Jain2012,
abstract = {Alternating minimization represents a widely applicable and empirically successful approach for finding low-rank matrices that best fit the given data. For example, for the problem of low-rank matrix completion, this method is believed to be one of the most accurate and efficient, and formed a major component of the winning entry in the Netflix Challenge. In the alternating minimization approach, the low-rank target matrix is written in a bi-linear form, i.e. {\$}X = UV{\^{}}\backslashdag{\$}; the algorithm then alternates between finding the best {\$}U{\$} and the best {\$}V{\$}. Typically, each alternating step in isolation is convex and tractable. However the overall problem becomes non-convex and there has been almost no theoretical understanding of when this approach yields a good result. In this paper we present first theoretical analysis of the performance of alternating minimization for matrix completion, and the related problem of matrix sensing. For both these problems, celebrated recent results have shown that they become well-posed and tractable once certain (now standard) conditions are imposed on the problem. We show that alternating minimization also succeeds under similar conditions. Moreover, compared to existing results, our paper shows that alternating minimization guarantees faster (in particular, geometric) convergence to the true matrix, while allowing a simpler analysis.},
archivePrefix = {arXiv},
arxivId = {1212.0467},
author = {Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
eprint = {1212.0467},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Jain, Netrapalli, Sanghavi - 2012 - Low-rank Matrix Completion using Alternating Minimization.pdf:pdf},
keywords = {Matrix completion,N,Prof,Sujay},
mendeley-tags = {Matrix completion,N,Prof,Sujay},
pages = {1--40},
title = {{Low-rank Matrix Completion using Alternating Minimization}},
url = {http://arxiv.org/abs/1212.0467},
year = {2012}
}
@article{Bhojanapalli,
annote = {Hypothesis: 
Methodology: 
Result(s): 
Summary of key points: 
Context (how this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself): 
Significance (to the field; in relation to your own work): 
Important Figures and/or Tables (brief description; page number): 
Other Comments: },
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03917v2},
author = {Bhojanapalli, Srinadh and Kyrillidis, Anastasios and Sanghavi, Sujay},
eprint = {arXiv:1509.03917v2},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Bhojanapalli, Kyrillidis, Sanghavi - Unknown - Dropping Convexity for Faster Semi-definite Optimization.pdf:pdf},
keywords = {NotRead,Prof,Sujay},
mendeley-tags = {NotRead,Prof,Sujay},
pages = {1--43},
title = {{Dropping Convexity for Faster Semi-definite Optimization}}
}
@article{Rosenfeld,
author = {Rosenfeld, Nir},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Rosenfeld - Unknown - Discriminative Learning of Infection Models.pdf:pdf},
keywords = {Constantine,Epidemics,Prof},
mendeley-tags = {Constantine,Epidemics,Prof},
title = {{Discriminative Learning of Infection Models}}
}
@article{Drakopoulos2014,
annote = {Hypothesis: 
- continuous time markov chain for epidemics
- contagion rate proportionnal to cut of the set
- graph categorized by cutwidth
- total information
- limited budget by time unit

Methodology: 
Result(s): 
- if cutwidth sublinear in the number of nodes, there exist a sublinear budget (4*cutwidth) that makes the epidemics disappear in sublinear time
- if cutwidth and crusade are already computed

Summary of key points: 
Context (how this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself): 
Significance (to the field; in relation to your own work): 
Important Figures and/or Tables (brief description; page number): 
Other Comments: },
archivePrefix = {arXiv},
arxivId = {arXiv:1407.2241v1},
author = {Drakopoulos, Kimon and Ozdaglar, Asuman and Tsitsiklis, John N.},
doi = {10.1109/TNSE.2015.2393291},
eprint = {arXiv:1407.2241v1},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Drakopoulos, Ozdaglar, Tsitsiklis - 2014 - An efficient curing policy for epidemics on graphs.pdf:pdf},
isbn = {9781467360883},
journal = {arXiv preprint arXiv:1407.2241},
keywords = {Constantine,Epidemics,Prof},
mendeley-tags = {Constantine,Epidemics,Prof},
number = {December},
pages = {1--10},
title = {{An efficient curing policy for epidemics on graphs}},
url = {http://arxiv.org/abs/1407.2241},
year = {2014}
}
@inproceedings{Gomez-rodriguez2012,
author = {Gomez-rodriguez, Manuel and Leskovec, Jure and Krause, Andreas},
booktitle = {ACM Transactions on Knowledge Discovery from Data (TKDD' 12)},
doi = {10.1145/2086737.2086741},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Gomez-rodriguez, Leskovec, Krause - 2012 - Inferring Networks of Diffusion and Influence.pdf:pdf},
keywords = {Epidemics},
mendeley-tags = {Epidemics},
number = {4},
title = {{Inferring Networks of Diffusion and Influence}},
volume = {5},
year = {2012}
}
@article{Shamir,
abstract = {We describe and analyze a simple algorithm for principal component analysis and singular value de-composition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponen-tially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly con-vex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.2848v5},
author = {Shamir, Ohad},
eprint = {arXiv:1409.2848v5},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shamir - Unknown - A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate.pdf:pdf},
keywords = {Gradient descent,NotRead,Prof,Rachel},
mendeley-tags = {Gradient descent,NotRead,Prof,Rachel},
pages = {1--18},
title = {{A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate}}
}
@article{Khalvati,
annote = {Hypothesis: 


Methodology: 


Result(s): 


Summary of key points: 
Context (how this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself): 
Significance (to the field; in relation to your own work): 
Important Figures and/or Tables (brief description; page number): 
Other Comments: },
author = {Khalvati, Koosha and Rao, Rajesh P N},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Khalvati, Rao - Unknown - A Bayesian Framework for Modeling Confidence in Perceptual Decision Making.pdf:pdf},
keywords = {Browsing,NIPS 2015,POMDP},
mendeley-tags = {Browsing,NIPS 2015,POMDP},
pages = {1--9},
title = {{A Bayesian Framework for Modeling Confidence in Perceptual Decision Making}}
}
@article{Drakopoulos2015a,
abstract = {We consider the propagation of a contagion process (epidemic) on a network and study the problem of dynamically allocating a fixed curing budget to the nodes of the graph, at each time instant. For bounded degree graphs, we provide a lower bound on the expected time to extinction under any such dynamic allocation policy, in terms of a combinatorial quantity that we call the resistance of the set of initially infected nodes, the available budget, and the number of nodes n. Specifically, we consider the case of bounded degree graphs, with the resistance growing linearly in n. We show that if the curing budget is less than a certain multiple of the resistance, then the expected time to extinction grows exponentially with n. As a corollary, if all nodes are initially infected and the CutWidth of the graph grows linearly, while the curing budget is less than a certain multiple of the CutWidth, then the expected time to extinction grows exponentially in n. The combination of the latter with our prior work establishes a fairly sharp phase transition on the expected time to extinction (sub-linear versus exponential) based on the relation between the CutWidth and the curing budget.},
annote = {Hypothesis: 
- continuous time markov chain for epidemics
- contagion rate proportionnal to cut of the set
- graph categorized by cutwidth
- total information
- limited budget by time unit

Methodology: 
Result(s): 
- if cutwidth is linear in the number of nodes, if the budget is cst*cutwidth, exponential extinction time
Summary of key points: 
Context (how this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself): 
Significance (to the field; in relation to your own work): 
Important Figures and/or Tables (brief description; page number): 
Other Comments: },
archivePrefix = {arXiv},
arxivId = {1510.06054},
author = {Drakopoulos, Kimon and Ozdaglar, Asuman and Tsitsiklis, John N.},
eprint = {1510.06054},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Drakopoulos, Ozdaglar, Tsitsiklis - 2015 - When is a network epidemic hard to eliminate.pdf:pdf},
keywords = {1,Constantine,Epidemics,Prof,and,and on the infection,contact process,contagion,control of contagion processes,cutwidth,epidemics,from now on called,introduction,policies that,sis model,specifically,state of individuals,time to extinction,under limited curing resources,underlying structure of contacts,use information on the,we study dynamic allocation,we study the dynamic},
mendeley-tags = {Constantine,Epidemics,Prof},
pages = {1--17},
title = {{When is a network epidemic hard to eliminate?}},
url = {http://arxiv.org/abs/1510.06054},
year = {2015}
}
@article{Lind2007,
abstract = {We study a simple model of information propagation in social networks, where two quantities are introduced: the spread factor, which measures the average maximal reachability of the neighbors of a given node that interchange information among each other, and the spreading time needed for the information to reach such a fraction of nodes. When the information refers to a particular node at which both quantities are measured, the model can be taken as a model for gossip propagation. In this context, we apply the model to real empirical networks of social acquaintances and compare the underlying spreading dynamics with different types of scale-free and small-world networks. We find that the number of friendship connections strongly influences the probability of being gossiped. Finally, we discuss how the spread factor is able to be applied to other situations.},
archivePrefix = {arXiv},
arxivId = {0705.3224},
author = {Lind, Pedro G. and {Da Silva}, Luciano R. and Andrade, Jos{\'{e}} S. and Herrmann, Hans J.},
doi = {10.1103/PhysRevE.76.036117},
eprint = {0705.3224},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lind et al. - 2007 - Spreading gossip in social networks.pdf:pdf},
isbn = {1539-3755},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {Epidemics,information spreading,network dynamics,social networks},
mendeley-tags = {Epidemics},
number = {3},
pages = {1--10},
pmid = {17930316},
title = {{Spreading gossip in social networks}},
volume = {76},
year = {2007}
}
@article{Shah2010,
abstract = {We provide a systematic study of the problem of finding the source of a rumor in a network. We model rumor spreading in a network with a variant of the popular SIR model and then construct an estimator for the rumor source. This estimator is based upon a novel topological quantity which we term textbfrumor centrality. We establish that this is an ML estimator for a class of graphs. We find the following surprising threshold phenomenon: on trees which grow faster than a line, the estimator always has non-trivial detection probability, whereas on trees that grow like a line, the detection probability will go to 0 as the network grows. Simulations performed on synthetic networks such as the popular small-world and scale-free networks, and on real networks such as an internet AS network and the U.S. electric power grid network, show that the estimator either finds the source exactly or within a few hops of the true source across different network topologies. We compare rumor centrality to another common network centrality notion known as distance centrality. We prove that on trees, the rumor center and distance center are equivalent, but on general networks, they may differ. Indeed, simulations show that rumor centrality outperforms distance centrality in finding rumor sources in networks which are not tree-like.},
archivePrefix = {arXiv},
arxivId = {0909.4370},
author = {Shah, Devavrat and Zaman, Tauhid},
doi = {10.1109/TIT.2011.2158885},
eprint = {0909.4370},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Shah, Zaman - 2010 - Rumors in a Network Who ' s the Culprit.pdf:pdf},
journal = {IEEE Transactions on information theory},
keywords = {Epidemics},
mendeley-tags = {Epidemics},
number = {8},
pages = {1--43},
title = {{Rumors in a Network : Who ' s the Culprit ?}},
url = {http://arxiv.org/abs/0909.4370},
volume = {57},
year = {2010}
}
@article{Lloyd,
author = {Lloyd, James Robert},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Lloyd - Unknown - Statistical Model Criticism using Kernel Two Sample Tests.pdf:pdf},
keywords = {Browsing,NIPS 2015,NotRead},
mendeley-tags = {Browsing,NIPS 2015,NotRead},
pages = {1--9},
title = {{Statistical Model Criticism using Kernel Two Sample Tests}},
url = {http://mlg.eng.cam.ac.uk/Lloyd/papers/kernel-model-checking.pdf}
}
@article{Adomavicius2005,
abstract = {This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
author = {Adomavicius, Gediminas and Tuzhilin, Alexander},
file = {:Users/jessicahoffmann/Library/Application Support/Mendeley Desktop/Downloaded/Adomavicius, Tuzhilin - 2005 - Toward the next generation of recommender systems A survey of the state-of-the-art and possible extension.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Browsing,Collaborative filtering,Extensions to recommander systems,Google2017,Rating estimation methods,Recommander systems,Recommender},
mendeley-tags = {Browsing,Google2017,Recommender},
number = {6},
pages = {734--749},
title = {{Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions}},
volume = {17},
year = {2005}
}
@article{Alon2009,
abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure. ?? 2009 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {z0024},
author = {Alon, Uri},
doi = {10.1016/j.molcel.2009.09.013},
eprint = {z0024},
isbn = {0769520561},
issn = {10972765},
journal = {Molecular Cell},
keywords = {HowToReseach,NotRead},
mendeley-tags = {HowToReseach,NotRead},
number = {6},
pages = {726--728},
pmid = {19782018},
publisher = {Elsevier Inc.},
title = {{How To Choose a Good Scientific Problem}},
url = {http://dx.doi.org/10.1016/j.molcel.2009.09.013},
volume = {35},
year = {2009}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Browsing,NotRead,pattern recognition,statistical learning theory,support vector machines,vc dimension},
mendeley-tags = {Browsing,NotRead},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf{\%}5Cnhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
author = {Lowe, David G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
keywords = {Browsing,NotRead},
mendeley-tags = {Browsing,NotRead},
number = {[8},
pages = {1150--1157},
publisher = {IEEE},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}
@article{Blei2012,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Browsing,NotRead,lda,topic model},
mendeley-tags = {Browsing,NotRead},
number = {4-5},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/{~}blei/lda-c/{\%}5Cnpapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993{\%}5Cnpapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A{\%}5Cnpapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6{\%}5Cnhttp://www.crossref.org/jmlr},
volume = {3},
year = {2012}
}
@article{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M Cm Christopher M.},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {978-0387310732},
issn = {10179909},
journal = {Pattern Recognition},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern recognition and machine learning}},
url = {http://soic.iupui.edu/syllabi/semesters/4142/INFO{\_}B529{\_}Liu{\_}s.pdf{\%}5Cnhttp://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
