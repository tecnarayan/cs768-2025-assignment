@inproceedings{Zhang2017,
  author    = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  title     = {Understanding Deep Learning Requires Re-Thinking Generalization},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2017}
}
@inproceedings{Auer96,
  title	    = {Exponentially many local minima for single neurons},
  author    = {Peter Auer and Mark Herbster and Manfred K. K. Warmuth},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {1996}
}
@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}
@preprint{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
  note={\texttt{arXiv:1903.08560}},
  year={2019}
}
@preprint{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Song Mei and Andrea Montanari},
  note={\texttt{arXiv:1908.05355}},
  year={2019}
}
@inproceedings{liao2020random,
  title={{A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent}},
  author={Zhenyu Liao and Romain Couillet and Michael W. Mahoney},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@book{bai2010spectral,
  title={Spectral analysis of large dimensional random matrices},
  author={Zhidong Bai and Jack W. Silverstein},
  year={2010},
  publisher={Springer}
}
@preprint{adlam2019random,
  title={A random matrix perspective on mixtures of nonlinearities for deep learning},
  author={Ben Adlam and Jake Levinson and Jeffrey Pennington},
  note={\texttt{arXiv:1912.00827}},
  year={2019}
}
@inproceedings{hanin2019deep,
  title={Deep relu networks have surprisingly few activation patterns},
  author={Boris Hanin and David Rolnick},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Ben Poole and Subhaneil Lahiri and Maithra Raghu and Jascha Sohl-Dickstein and Surya Ganguli},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2016}
}
@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Amit Daniely and Roy Frostig and Yoram Singer},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2016}
}
@inproceedings{han2017deep,
  title={Deep pyramidal residual networks},
  author={Dongyoon Han and Jiwhan Kim and Junmo Kim},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}
@article{lecun2015deep,
  title={Deep learning},
  author={Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}
@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Sanjeev Arora and Simon Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}
@inproceedings{bubeck2020network,
  title={Network size and weights size for memorization with two-layers neural networks},
  author={S{\'e}bastien Bubeck and Ronen Eldan and Yin Tat Lee and Dan Mikulincer},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@article{vershynin2020memory,
  title={Memory Capacity of Neural Networks with Threshold and Rectified Linear Unit Activations},
  author={Roman Vershynin},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1004--1033},
  year={2020},
  publisher={SIAM}
}
@inproceedings{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Jeffrey Pennington and Samuel Schoenholz and Surya Ganguli},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2018}
}
@preprint{SongYang2020,
  author    = {Zhao Song and Xin Yang},
  title     = {Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound},
  note      = {\texttt{arXiv:1906.03593}},
  year 	    = {2020},
}
@preprint{Boris15,
  author    = {Boris Mityagin},
  title     = {The Zero Set of a Real Analytic Function},
  note      = {\texttt{arXiv:1512.07276}},
  year      = {2015}
}
@inproceedings{pennington2017nonlinear,
  title={Nonlinear random matrix theory for deep learning},
  author={Jeffrey Pennington and Pratik Worah},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}
@preprint{benigni2019eigenvalue,
  title={Eigenvalue distribution of nonlinear models of random matrices},
  author={Lucas Benigni and Sandrine P{\'e}ch{\'e}},
  note={\texttt{arXiv:1904.03090}},
  year={2019}
}
@article{louart2018random,
  title={A random matrix approach to neural networks},
  author={Cosme Louart and Zhenyu Liao and Romain Couillet},
  journal={The Annals of Applied Probability},
  volume={28},
  number={2},
  pages={1190--1248},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}
@inproceedings{pennington2017geometry,
  title={Geometry of neural network loss surfaces via random matrix theory},
  author={Jeffrey Pennington and Yasaman Bahri},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}
@preprint{pastur2020random,
  title={On Random Matrices Arising in Deep Neural Networks. Gaussian Case},
  author={Leonid Pastur},
  note={\texttt{arXiv:2001.06188}},
  year={2020}
}
@inproceedings{pennington2018spectrum,
  title={The spectrum of the fisher information matrix of a single-hidden-layer neural network},
  author={Jeffrey Pennington and Pratik Worah},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2018}
}
@inproceedings{liao2018spectrum,
  title={On the Spectrum of Random Features Maps of High Dimensional Data},
  author={Zhenyu Liao and Romain Couillet},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}
@inproceedings{SafSha2016,
  author    = {Itay Safran and Ohad Shamir},
  title     = {On the quality of the initial basin in overspecified networks},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2016},	
}
@inproceedings{QuynhICML2017,
  title     = {The loss surface of deep and wide neural networks},
  author    = {Quynh Nguyen and Matthias Hein},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2017}
}
@inproceedings{Hardt2017,
  author    = {Moris Hardt and Tengyu Ma},
  title     = {Identity matters in deep learning},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2017},
}
@article{Cybenko1989,
  author    = {G. Cybenko},
  title     = {Approximation by superpositions of a sigmoidal function},
  journal   = {Mathematics of Control, Signals, and Systems},
  volume    = {2},
  pages     = {303-314},
  year      = {1989}
}
@book{Apostol1974,
  author    = {T. M. Apostol},
  title     = {Mathematical analysis},
  publisher = {Addison Wesley},
  year      = {1974}
}
@article{Blum1989,
  title     = {Training a 3-node neural network is {NP}-complete},
  author    = {Avrim Blum and Ronald L. Rivest},
  journal   = {Neural Networks},
  volume    = {5},
  number    = {1},
  pages     = {117--127},
  year      = {1992},
  publisher = {Elsevier}
}
@inproceedings{Yun2019,
  author    = {Chulhee Yun and Suvrit Sra and Ali Jadbabaie},
  title     = {Small nonlinearities in activation functions create bad local minima in neural networks},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2019},
}
@article{bartlett2019nearly,
  title={Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks.},
  author={Peter Bartlett and Nick Harvey and Christopher Liaw and Abbas Mehrabian},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={20},
  number={63},
  pages={1--17},
  year={2019}
}
@inproceedings{baum1989size,
  title={What size net gives valid generalization?},
  author={Eric Baum and David Haussler},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={1989}
}
@preprint{ge2019mildly,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Rong Ge and Runzhe Wang and Haoyu Zhao},
  note={\texttt{arXiv:1909.11837}},
  year={2019}
}
@inproceedings{yun2019small,
  title={Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity},
  author={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
@inproceedings{QuynhICML2018,
  title     = {Optimization Landscape and Expressivity of Deep {CNN}s},
  author    = {Quynh Nguyen and Matthias Hein},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018}
}
@inproceedings{SafranShamir2018,
  title	    = {Spurious Local Minima are Common in Two-Layer ReLU Neural Networks},
  author    = {Itay Safran and Ohad Shamir},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018}
}
@inproceedings{XavierBengio2010,
  author    = {Xavier Glorot and Yoshua Bengio},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle   = {International Conference on Machine Learning (ICML)},
  year      = {2010},	
}
@inproceedings{DuEtal2018_ICLR,
  author    = {Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
  title     = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year 	    = {2019},
}
@inproceedings{DuEtal2019,
  title     = {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author    = {Simon S. Du and Jason D. Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}
@preprint{ZouEtal2018,
  author    = {Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  title     = {Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks},
  note      = {\texttt{arXiv:1811.08888}},
  year 	    = {2018},
}
@inproceedings{AllenZhuEtal2018,
  title     = {A Convergence Theory for Deep Learning via Over-Parameterization},
  author    = {Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}
@inproceedings{QuynhICLR2019,
  author    = {Quynh Nguyen and Mahesh Chandra Mukkamala and Matthias Hein},
  title     = {On the loss landscape of a class of deep neural networks with no bad local valleys},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2019},
}
@inproceedings{ChizatBach2018,
  author    = {Lenaic Chizat and Francis Bach},
  title     = {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
}
@article{Stewart1990,
  author    = {G. W. Stewart},
  title     = {Perturbation theory for the singular value decomposition},
  journal   = {Technical Report},
  year 	    = {1990},
}
@preprint{Vershynin2010,
  author    = {Roman Vershynin},
  title     = {Introduction to the non-asymptotic analysis of random matrices},
  note      = {\texttt{arXiv:1011.3027}},
  year      = {2010},
}
@inproceedings{QuynhICML2019,
  title     = {On Connected Sublevel Sets in Deep Learning},
  author    = {Quynh Nguyen},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}
@inproceedings{JiMatus2020,
  author    = {Ziwei Ji and Matus Telgarsky},
  title     = {Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020}
}
@preprint{Vaswani2016,
  author    = {Namrata Vaswani and Seyedehsara Nayer},
  title     = {A Simple Generalization of a Result for Random Matrices with Independent Sub-Gaussian Rows},
  note      = {\texttt{arXiv:1612.00127}},
  year      = {2016}
}

@inproceedings{fan2020spectra,
  title={Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks},
  author={Zhou Fan and Zhichao Wang},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{seddik2020random,
  title={Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures},
  author={Mohamed El Amine Seddik and Cosme Louart and Mohamed Tamaazousti and Romain Couillet},
  booktitle = {International Conference on Machine Learning (ICML)},
  year={2020}
}

@preprint{ChenCaoZouGu2019,
  author    = {Zixiang Chen and Yuan Cao and Difan Zou and Quanquan Gu},
  title     = {How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  note      = {\texttt{arXiv:1911.12360}},
  year      = {2019}
}
@inproceedings{JacotEtc2018,
  title     = {Neural tangent kernel: Convergence and generalization in neural networks},
  author    = {Arthur Jacot and Franck Gabriel and Clément Hongler},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2018}
}
@inproceedings{LeeEtc2019,
  author    = {Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and Roman Novak and Jascha Sohl-Dickstein and Jeffrey Pennington},
  title     = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
}
@inproceedings{ChizatEtc2019,
  title     = {On lazy training in differentiable programming},
  author    = {Lenaic Chizat and Edouard Oyallon and Francis Bach},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}
@article{cover1965geometrical,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Thomas M. Cover},
  journal={IEEE transactions on electronic computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}
@article{OymakMahdi2019,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  year={2020},
  publisher={IEEE}
}
@inproceedings{ZouGu2019,
  title     = {An improved analysis of training over-parameterized deep neural networks},
  author    = {Difan Zou and Quanquan Gu},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}
@inproceedings{Sohl2019,
  title     = {The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study},
  author    = {Daniel S. Park and Jascha Sohl-Dickstein and Quoc V. Le and Samuel L. Smith},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}
@article{soltanolkotabi2018theoretical,
  title     = {Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author    = {Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee},
  journal   = {IEEE Transactions on Information Theory},
  volume    = {65},
  number    = {2},
  pages     = {742--769},
  year      = {2018},
  publisher = {IEEE}
}
@article{adamczak2015note,
  title     = {A note on the Hanson-Wright inequality for random vectors with dependencies},
  author    = {Radoslaw Adamczak},
  journal   = {Electronic Communications in Probability},
  volume    = {20},
  year      = {2015},
  publisher = {The Institute of Mathematical Statistics and the Bernoulli Society}
}
@book{vershynin2018high,
  title     = {High-dimensional probability: An introduction with applications in data science},
  author    = {Roman Vershynin},
  year      = {2018},
  publisher = {Cambridge university press}
}
@article{adamczak2011restricted,
  title     = {Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling},
  author    = {Radosław Adamczak and Alexander E. Litvak and Alain Pajor and Nicole Tomczak-Jaegermann},
  journal   = {Constructive Approximation},
  volume    = {34},
  number    = {1},
  pages     = {61--88},
  year      = {2011},
  publisher = {Springer}
}
@book{boucheron2013concentration,
  title     = {Concentration inequalities: A nonasymptotic theory of independence},
  author    = {Stephane Boucheron and Gabor Lugosi and Pascal Massart},
  year      = {2013},
  publisher = {Oxford university press}
}
@inproceedings{he2015delving,
  title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015}
}
@book{Abramowitz1974,
  author    = {Milton Abramowitz},
  title     = {Handbook of Mathematical Functions, With Formulas, Graphs, and Mathematical Tables},
  publisher = {Dover Publications},
  year      = {1974}
}
@incollection{dolbeault2014sharp,
  title     = {Sharp interpolation inequalities on the sphere: new methods and consequences},
  author    = {Jean Dolbeault and Maria J. Esteban and Michal Kowalczyk and Michael Loss},
  booktitle = {Partial Differential Equations: Theory, Control and Approximation},
  pages     = {225--242},
  year      = {2014},
  publisher = {Springer}
}
@article{adamczak2015concentration,
  title     = {Concentration inequalities for non-Lipschitz functions with bounded derivatives of higher order},
  author    = {Radosław Adamczak and Paweł Wolff},
  journal   = {Probability Theory and Related Fields},
  volume    = {162},
  number    = {3-4},
  pages     = {531--586},
  year      = {2015},
  publisher = {Springer}
}
@article{aida1994moment,
  title     = {Moment estimates derived from Poincar{\'e} and logarithmic Sobolev inequalities},
  author    = {S. Aida and D. Stroock},
  journal   = {Mathematical Research Letters},
  volume    = {1},
  number    = {1},
  pages     = {75--86},
  year      = {1994},
  publisher = {International Press of Boston}
}
@preprint{wu2019global,
  title     = {Global convergence of adaptive gradient methods for an over-parameterized neural network},
  author    = {Xiaoxia Wu and Simon S. Du and Rachel Ward},
  note      = {\texttt{arXiv:1902.07111}},
  year      = {2019}
}
@article{bobkov2019higher,
  title     = {Higher order concentration of measure},
  author    = {Sergey G. Bobkov and Friedrich Götze and Holger Sambale},
  journal   = {Communications in Contemporary Mathematics},
  volume    = {21},
  number    = {03},
  year      = {2019},
  publisher = {World Scientific}
}
@inproceedings{Amit2019,
  title     = {Neural Networks Learning and Memorization with (almost) no Over-Parameterization},
  author    = {Amit Daniely},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2020}
}
@inproceedings{BrutzkusEtal2018,
  title     = {SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
  author    = {Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2018}
}
@inproceedings{LiLiang2018,
  title     = {Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
  author    = {Yuanzhi Li and Yingyu Liang},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2018}
}
@preprint{NitandaEtal2019,
  title     = {Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems},
  author    = {Atsushi Nitanda and Geoffrey Chinot and Taiji Suzuki},
  note      = {\texttt{arXiv:1905.09870}},
  year      = {2019}
}
@preprint{Venturi2018,
  author    = {Luca Venturi and Afonso S. Bandeira and Joan Bruna},
  title     = {Spurious Valleys in Two-layer Neural Network Optimization Landscapes},
  note      = {\texttt{arXiv:1802.06384}},
  year 	    = {2018},
}
@article{Polyak1963,
  author    = {B. T. Polyak},
  title     = {Gradient methods for minimizing functionals},
  journal   = {Zh. Vychisl. Mat. Mat. Fiz.},
  year 	    = {1963},
  volume    = {3},
  number    = {4},
}
@article{SoudryEtal2018,
  author    = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  title     = {The implicit bias of gradient descent on separable data},
  journal   = {Journal of Machine Learning Research (JMLR)},
  pages     = {2822-2878},
  year 	    = {2018},
  volume    = {19},
}
@inproceedings{kowalczyk1994counting,
  title={Counting function theorem for multi-layer networks},
  author={Adam Kowalczyk},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={1994}
}
@inproceedings{sakurai1992nh,
  title={nh-1 networks store no less n* h+ 1 examples, but sometimes no more},
  author={Akito Sakurai},
  booktitle={IJCNN International Joint Conference on Neural Networks},
  year={1992},
}
@inproceedings{lee2018deep,
  title={Deep Neural Networks as Gaussian Processes},
  author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}
@inproceedings{schoenholz2017deep,
  title={Deep information propagation},
  author={Samuel S. Schoenholz and Justin Gilmer and Surya Ganguli and Jascha Sohl-Dickstein},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
@article{Baum1988,
  author    = {Eric B. Baum},
  title     = {On the capabilities of multilayer perceptrons},
  journal   = {Journal of Complexity},
  year 	    = {1988},
  volume    = {4},
}
@inproceedings{VGG,
  author    = {Karen Simonyan and Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2015},
}
@preprint{JaschaEtal2020,
  author    = {Jascha Sohl-Dickstein and Roman Novak and Samuel S. Schoenholz and Jaehoon Lee},
  title     = {On the infinite width limit of neural networks with a standard parameterization},
  note      = {\texttt{arXiv:2001.07301}},
  year 	    = {2020},
}
@inproceedings{QuynhMarco2020,
  author    = {Quynh Nguyen and Marco Mondelli},
  title     = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year 	    = {2020},
}
@preprint{Andrea2020,
  author    = {Andrea Montanari and Yiqiao Zhong},
  title     = {The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training},
  note      = {\texttt{arXiv:2007.12826}},
  year 	    = {2020},
}
@article{schur1911bemerkungen,
  title={Bemerkungen zur Theorie der beschr{\"a}nkten Bilinearformen mit unendlich vielen Ver{\"a}nderlichen.},
  author={Jssai Schur},
  journal={Journal f{\"u}r die reine und angewandte Mathematik (Crelles Journal)},
  volume={1911},
  number={140},
  pages={1--28},
  year={1911},
  publisher={De Gruyter}
}
@preprint{Gorokhovik2011,
  author    = {Valentin V. Gorokhovik},
  title     = {Geometrical and analytical characteristic properties of piecewise affine mappings},
  note      = {\texttt{arXiv:1111.1389}},
  year 	    = {2011},
}
@inproceedings{HaninRolnick2019,
  title	    = {Deep ReLU Networks Have Surprisingly Few Activation Patterns},
  author    = {Boris Hanin and David Rolnick},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}
@inproceedings{SerraEtal2018,
  title	    = {Bounding and Counting Linear Regions of Deep Neural Networks},
  author    = {Thiago Serra and Christian Tjandraatmadja and Srikumar Ramalingam},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2018}
}
@inproceedings{AroraEtal2019,
  title	    = {On Exact Computation with an Infinitely Wide Neural Net},
  author    = {Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Rousong Wang},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}
@inproceedings{HuangYau2020,
  title     = {Dynamics of Deep Neural Networks and Neural Tangent Hierarchy},
  author    = {Jiaoyang Huang and Horng-Tzer Yau},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2020}
}
@preprint{GhorbaniEtal2020,
  author    = {Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  title     = {Linearized two-layers neural networks in high dimension},
  note      = {\texttt{arXiv:1904.12191}},
  year 	    = {2020},
}
@inproceedings{Xie2017,
  author    = {Bo Xie and Yingyu Liang and Le Song},
  title     = {Diverse Neural Network Learns True Target Functions},
  booktitle = {AISTATS},
  year 	    = {2017},
}
@inproceedings{ChenEtal2020,
  author    = {Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
  title     = {A Generalized Neural Tangent Kernel Analysis forTwo-layer Neural Networks},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year 	    = {2020},
}

@inproceedings{MontufarEtal2014,
  author    = {Guido F. Montufar and Razvan Pascanu and Kyunghyun Cho and Yoshua Bengio},
  title     = {On the number of linear regions of deep neural networks},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year 	    = {2014},
}
@article{Davidson2001,
  title     = {Local operator theory, random matrices and Banach spaces},
  author    = {Kenneth R. Davidson and Stanislaw J. Szarek},
  journal   = {Handbook of the geometry of Banach spaces},
  volume    = {1},
  number    = {140},
  pages     = {317--366},
  year      = {2001},
  publisher = {Amsterdam: North-Holland}
}
@article{Tropp2011,
  author    = {Joel Tropp},
  title     = {User-friendly tail bounds for sums of random matrices},
  journal   = {Foundations of Computational Mathematics },
  pages     = {389–434},
  year 	    = {2012},
}
@inproceedings{Kawaguchi2019,
  title     = {Gradient Descent Finds Global Minima for Generalizable Deep Neural Networks of Practical Sizes},
  author    = {Kenji Kawaguchi and Jiaoyang Huang},
  booktitle = {Allerton Conference on Communication, Control, and Computing (Allerton)},
  year      = {2019}
}
@article{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  journal={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}
@inproceedings{HaninNTK,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Boris Hanin and Mihai Nica},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}
@inproceedings{QuynhNTK2021,
  author    = {Quynh Nguyen and Marco Mondelli and Guido Montufar},
  title     = {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks},
  booktitle = {International Conference on Machine Learning (ICML)},
  year 	    = {2021},
}
@inproceedings{ChizatBach2020,
  title     = {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
  author    = {Lenaic Chizat and Francis Bach},
  booktitle = {Conference on Learning Theory (COLT)},
  year      = {2020}
}
@inproceedings{MeiEtal2018,
  author    = {Song Mei and Andrea Montanari and Phan M. Nguyen},
  title     = {A mean field view of the landscape of two-layers neural networks},
  booktitle = {Proceedings of the National Academy of Sciences},
  year 	    = {2018},
}
@inproceedings{Amit2017,
  title     = {SGD Learns the Conjugate Kernel Class of the Network},
  author    = {Amit Daniely},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2017}
}
@inproceedings{CaoGu2020,
  author    = {Yuan Cao and Quanquan Gu},
  title     = {Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year 	    = {2020},
}
