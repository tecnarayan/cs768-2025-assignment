\begin{thebibliography}{10}

\bibitem{alaoui2015fast}
Ahmed Alaoui and Michael~W. Mahoney.
\newblock Fast randomized kernel ridge regression with statistical guarantees.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  775--783, 2015.

\bibitem{amini2021tkrr}
A.~A. Amini.
\newblock Spectrally-truncated kernel ridge regression and its free lunch.
\newblock {\em Electronic Journal of Statistics}, 15:1935--7524, 2021.

\bibitem{Amini_Target_alignment_in_2022}
Arash~A. Amini, Richard Baumgartner, and Dai Feng.
\newblock {Target alignment in truncated kernel ridge regression}.
\newblock \url{https://github.com/aaamini/krr-target-align}, 2022.

\bibitem{bach2013sharp}
Francis Bach.
\newblock Sharp analysis of low-rank kernel matrix approximations.
\newblock In {\em Conference on Learning Theory}, pages 185--209, 2013.

\bibitem{bartlett2020benign}
P.~Bartlett, P.~Long, G.~Lugosi, and A.~Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30063–30070, 2020.

\bibitem{belkin2019reconciling}
D.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias–variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849–15854, 2019.

\bibitem{bordelon2020spectrum}
B.~Bordelon, A.~Canatar, and C.~Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1024--1034, 2020.

\bibitem{canatar2021spectral}
A.~Canatar, B.~Bordelon, and C.~Pehlevan.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock {\em Nature Communications}, 12(2914), 2021.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{cortes2010impact}
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
\newblock On the impact of kernel approximation on learning accuracy.
\newblock In {\em Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 113--120, 2010.

\bibitem{cristianini2001onkernel}
Nello Cristianini, John Shawe-Taylor, Andr\'{e} Elisseeff, and Jaz Kandola.
\newblock On kernel-target alignment.
\newblock In T.~Dietterich, S.~Becker, and Z.~Ghahramani, editors, {\em
  Advances in Neural Information Processing Systems}, volume~14. MIT Press,
  2001.

\bibitem{cui2021generalization}
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Generalization error rates in kernel regression: The crossover from
  the noiseless to noisy regime.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{hastie2022surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em The Annals of Statistics}, 50(2):949--986, 2022.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{jin2013improved}
Rong Jin, Tianbao Yang, Mehrdad Mahdavi, Yu-Feng Li, and Zhi-Hua Zhou.
\newblock Improved bounds for the {Nystr{\"o}m} method with application to
  kernel classification.
\newblock {\em IEEE Transactions on Information Theory}, 59(10):6939--6949,
  2013.

\bibitem{kimeldorf1971some}
George Kimeldorf and Grace Wahba.
\newblock Some results on {Tchebycheffian} spline functions.
\newblock {\em Journal of Mathematical Analysis and Applications},
  33(1):82--95, 1971.

\bibitem{kobak2020theoptimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanches.
\newblock The optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock {\em The Journal of Machine Learning Research}, 21(1):1--16, 2020.

\bibitem{kumar2009ensemble}
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
\newblock Ensemble {Nystr{\"o}m} method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1060--1068, 2009.

\bibitem{li2010making}
Mu~Li, James~T. Kwok, and Bao-Liang Lu.
\newblock Making large-scale {Nystr{\"o}m} approximation possible.
\newblock In {\em Proceedings of the 27th International Conference on Machine
  Learning}, pages 631--638, 2010.

\bibitem{loog2020abrief}
Marco Loog, Tom Viering, Alexander Mey, and David Tax.
\newblock A brief prehistory of double descent.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(20):10625--10626, 2020.

\bibitem{rudi2015less}
Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Less is more: {Nystr{\"o}m} computational regularization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1657--1665, 2015.

\bibitem{scholkopf2001learning}
B.~Schoelkopf and A.~Smola.
\newblock {\em Learning with Kernels: Support Vector Machines, Regularization,
  Optimization, and Beyond.}
\newblock MIT Press, 2001.

\bibitem{steinwart2008support}
Ingo Steinwart and Andreas Christmann.
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{talwalkar2014matrix}
Ameet Talwalkar and Afshin Rostamizadeh.
\newblock Matrix coherence and the {Nystr{\"o}m} method.
\newblock {\em arXiv preprint arXiv:1408.2044}, 2014.

\bibitem{tsybakov2009intro}
Alexandre~B. Tsybakov.
\newblock {\em Introduction to Nonparametric Estimation}.
\newblock Springer, 2009.

\bibitem{wahba1990spline}
Grace Wahba.
\newblock {\em Spline Models for Observational Data}.
\newblock SIAM, 1990.

\bibitem{wainwright2019high}
Martin~J. Wainwright.
\newblock {\em High-dimensional Statistics: A Non-asymptotic Viewpoint}.
\newblock Cambridge University Press, 2019.

\bibitem{wasserman2006all}
Larry Wasserman.
\newblock {\em All of Nonparametric Statistics}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{williams2001using}
Christopher K.~I. Williams and Matthias Seeger.
\newblock Using the {Nystr{\"o}m} method to speed up kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  682--688, 2001.

\bibitem{yang2012nystrom}
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou.
\newblock {Nystr{\"o}m} method vs random {Fourier features}: A theoretical and
  empirical comparison.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  476--484, 2012.

\bibitem{yang2017randomized}
Yun Yang, Mert Pilanci, and Martin~J. Wainwright.
\newblock Randomized sketches for kernels: Fast and optimal nonparametric
  regression.
\newblock {\em The Annals of Statistics}, 45(3):991--1023, 2017.

\bibitem{zhang2008improved}
Kai Zhang, Ivor~W. Tsang, and James~T. Kwok.
\newblock Improved {Nystr{\"o}m} low-rank approximation and error analysis.
\newblock In {\em Proceedings of the 25th International Conference on Machine
  Learning}, pages 1232--1239. ACM, 2008.

\end{thebibliography}
