\begin{thebibliography}{97}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[bat(2021)]{batch-prediction}
{Online versus batch prediction}.
\newblock
  \url{https://cloud.google.com/ai-platform/prediction/docs/online-vs-batch-prediction},
  2021.

\bibitem[Athalye et~al.(2017)Athalye, Engstrom, Ilyas, and
  Kwok]{Athalye2017SynthesizingRA}
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K.
\newblock Synthesizing robust adversarial examples.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Bartler et~al.(2022)Bartler, B\"uhler, Wiewel, D\"obler, and
  Yang]{bartler22a}
Bartler, A., B\"uhler, A., Wiewel, F., D\"obler, M., and Yang, B.
\newblock Mt3: Meta test-time training for self-supervised test-time adaption.
\newblock In Camps-Valls, G., Ruiz, F. J.~R., and Valera, I. (eds.),
  \emph{Proceedings of The 25th International Conference on Artificial
  Intelligence and Statistics}, volume 151 of \emph{Proceedings of Machine
  Learning Research}, pp.\  3080--3090. PMLR, 28--30 Mar 2022.
\newblock URL \url{https://proceedings.mlr.press/v151/bartler22a.html}.

\bibitem[Biggio et~al.(2012)Biggio, Nelson, and Laskov]{biggio2012poisoning}
Biggio, B., Nelson, B., and Laskov, P.
\newblock Poisoning attacks against support vector machines.
\newblock \emph{arXiv preprint arXiv:1206.6389}, 2012.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, Srndic, Laskov,
  Giacinto, and Roli]{Biggio2013EvasionAA}
Biggio, B., Corona, I., Maiorca, D., Nelson, B., Srndic, N., Laskov, P.,
  Giacinto, G., and Roli, F.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{ECML/PKDD}, 2013.

\bibitem[Carlini(2021)]{Carlini2021PoisoningTU}
Carlini, N.
\newblock Poisoning the unlabeled dataset of semi-supervised learning.
\newblock \emph{ArXiv}, abs/2105.01622, 2021.

\bibitem[Carlini \& Terzis(2021)Carlini and Terzis]{Carlini2021PoisoningAB}
Carlini, N. and Terzis, A.
\newblock Poisoning and backdooring contrastive learning.
\newblock \emph{ArXiv}, abs/2106.09667, 2021.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{Carlini2017TowardsET}
Carlini, N. and Wagner, D.~A.
\newblock Towards evaluating the robustness of neural networks.
\newblock \emph{2017 IEEE Symposium on Security and Privacy (SP)}, pp.\
  39--57, 2017.

\bibitem[Chakraborty et~al.(2018)Chakraborty, Alam, Dey, Chattopadhyay, and
  Mukhopadhyay]{Chakraborty2018AdversarialAA}
Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., and Mukhopadhyay, D.
\newblock Adversarial attacks and defences: A survey.
\newblock \emph{ArXiv}, abs/1810.00069, 2018.

\bibitem[Chen et~al.(2021)Chen, Wu, Guo, Liang, and Jha]{chen2021towards}
Chen, J., Wu, X., Guo, Y., Liang, Y., and Jha, S.
\newblock Towards evaluating the robustness of neural networks learned by
  transduction.
\newblock \emph{CoRR}, abs/2110.14735, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14735}.

\bibitem[Chen et~al.(2022)Chen, Cheng, Gan, Gu, and Liu]{Chen2022EfficientRT}
Chen, J., Cheng, Y., Gan, Z., Gu, Q., and Liu, J.
\newblock Efficient robust training via backward smoothing.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2022.

\bibitem[Croce et~al.(2020)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{croce2020robustbench}
Croce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N.,
  Chiang, M., Mittal, P., and Hein, M.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock \emph{arXiv preprint arXiv:2010.09670}, 2020.

\bibitem[Croce et~al.(2022)Croce, Gowal, Brunner, Shelhamer, Hein, and
  Cemgil]{Croce2022EvaluatingTA}
Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, A.~T.
\newblock Evaluating the adversarial robustness of adaptive test-time defenses.
\newblock \emph{ArXiv}, abs/2202.13711, 2022.

\bibitem[Dai et~al.(2022)Dai, Mahloujifar, and Mittal]{Dai2022ParameterizingAF}
Dai, S., Mahloujifar, S., and Mittal, P.
\newblock Parameterizing activation functions for adversarial robustness.
\newblock \emph{2022 IEEE Security and Privacy Workshops (SPW)}, pp.\  80--87,
  2022.

\bibitem[Damodaran et~al.(2018)Damodaran, Kellenberger, Flamary, Tuia, and
  Courty]{damodaran2018deepjdot}
Damodaran, B.~B., Kellenberger, B., Flamary, R., Tuia, D., and Courty, N.
\newblock Deepjdot: Deep joint distribution optimal transport for unsupervised
  domain adaptation.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  447--463, 2018.

\bibitem[Deng et~al.(2021)Deng, Garg, Jha, Mahloujifar, Mahmoody, and
  Guha~Thakurta]{deng2021separation}
Deng, S., Garg, S., Jha, S., Mahloujifar, S., Mahmoody, M., and Guha~Thakurta,
  A.
\newblock A separation result between data-oblivious and data-aware poisoning
  attacks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10862--10875, 2021.

\bibitem[Di et~al.(2022)Di, Douglas, Acharya, Kamath, and
  Sekhari]{Di2022HiddenPM}
Di, J.~Z., Douglas, J., Acharya, J., Kamath, G., and Sekhari, A.
\newblock Hidden poison: Machine unlearning enables camouflaged poisoning
  attacks.
\newblock \emph{ArXiv}, abs/2212.10717, 2022.

\bibitem[D{\"o}bler et~al.(2022)D{\"o}bler, Marsden, and
  Yang]{Dbler2022RobustMT}
D{\"o}bler, M., Marsden, R.~A., and Yang, B.
\newblock Robust mean teacher for continual and gradual test-time adaptation.
\newblock \emph{ArXiv}, abs/2211.13081, 2022.

\bibitem[Ebrahimi et~al.(2022)Ebrahimi, Arik, and Pfister]{TTAVDU}
Ebrahimi, S., Arik, S.~O., and Pfister, T.
\newblock Test-time adaptation for visual document understanding, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.07240}.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Salman, Santurkar, and
  Tsipras]{robustness}
Engstrom, L., Ilyas, A., Salman, H., Santurkar, S., and Tsipras, D.
\newblock Robustness (python library), 2019.
\newblock URL \url{https://github.com/MadryLab/robustness}.

\bibitem[Fournier et~al.(1982)Fournier, Fussell, and
  Carpenter]{Fournier1982ComputerRO}
Fournier, A., Fussell, D.~S., and Carpenter, L.~C.
\newblock Computer rendering of stochastic models.
\newblock \emph{Commun. ACM}, 25:\penalty0 371--384, 1982.

\bibitem[Galstyan \& Cohen(2007)Galstyan and Cohen]{Galstyan2007EmpiricalCO}
Galstyan, A.~G. and Cohen, P.~R.
\newblock Empirical comparison of "hard" and "soft" label propagation for
  relational classification.
\newblock In \emph{ILP}, 2007.

\bibitem[Gandelsman et~al.(2022)Gandelsman, Sun, Chen, and
  Efros]{Gandelsman2022TestTimeTW}
Gandelsman, Y., Sun, Y., Chen, X., and Efros, A.~A.
\newblock Test-time training with masked autoencoders.
\newblock \emph{ArXiv}, abs/2209.07522, 2022.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,
  F., Marchand, M., and Lempitsky, V.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The journal of machine learning research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Gao et~al.(2022{\natexlab{a}})Gao, Zhang, Liu, Darrell, Shelhamer, and
  Wang]{gao2022back}
Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D.
\newblock Back to the source: Diffusion-driven test-time adaptation.
\newblock \emph{arXiv preprint arXiv:2207.03442}, 2022{\natexlab{a}}.

\bibitem[Gao et~al.(2022{\natexlab{b}})Gao, Shi, Zhu, Wang, Tang, Zhou, Li, and
  Metaxas]{Gao2022VisualPT}
Gao, Y., Shi, X., Zhu, Y., Wang, H., Tang, Z., Zhou, X., Li, M., and Metaxas,
  D.~N.
\newblock Visual prompt tuning for test-time domain adaptation.
\newblock \emph{ArXiv}, abs/2210.04831, 2022{\natexlab{b}}.

\bibitem[Geiping et~al.(2020)Geiping, Fowl, Huang, Czaja, Taylor, Moeller, and
  Goldstein]{Geiping2020WitchesBI}
Geiping, J., Fowl, L., Huang, W.~R., Czaja, W., Taylor, G., Moeller, M., and
  Goldstein, T.
\newblock Witches' brew: Industrial scale data poisoning via gradient matching.
\newblock \emph{ArXiv}, abs/2009.02276, 2020.

\bibitem[Geiping et~al.(2021)Geiping, Fowl, Somepalli, Goldblum, Moeller, and
  Goldstein]{geiping2021doesn}
Geiping, J., Fowl, L., Somepalli, G., Goldblum, M., Moeller, M., and Goldstein,
  T.
\newblock What doesn't kill you makes you robust (er): Adversarial training
  against poisons and backdoors.
\newblock \emph{arXiv preprint arXiv:2102.13624}, 2021.

\bibitem[Goldwasser et~al.(2020)Goldwasser, Kalai, Kalai, and
  Montasser]{goldwasser2020beyond}
Goldwasser, S., Kalai, A.~T., Kalai, Y., and Montasser, O.
\newblock Beyond perturbations: Learning guarantees with arbitrary adversarial
  test examples.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15859--15870, 2020.

\bibitem[Gong et~al.(2022)Gong, Jeong, Kim, Kim, Shin, and Lee]{gong2022note}
Gong, T., Jeong, J., Kim, T., Kim, Y., Shin, J., and Lee, S.-J.
\newblock {NOTE}: Robust continual test-time adaptation against temporal
  correlation.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=E9HNxrCFZPV}.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{Goodfellow2015ExplainingAH}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{CoRR}, abs/1412.6572, 2015.

\bibitem[Gowal et~al.(2021)Gowal, Rebuffi, Wiles, Stimberg, Calian, and
  Mann]{Gowal2021ImprovingRU}
Gowal, S., Rebuffi, S.-A., Wiles, O., Stimberg, F., Calian, D.~A., and Mann, T.
\newblock Improving robustness using generated data.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Goyal et~al.(2022)Goyal, Sun, Raghunathan, and
  Kolter]{Goyal2022TestTimeAV}
Goyal, S., Sun, M., Raghunathan, A., and Kolter, Z.
\newblock Test-time adaptation via conjugate pseudo-labels.
\newblock \emph{ArXiv}, abs/2207.09640, 2022.

\bibitem[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Gu, T., Dolan-Gavitt, B., and Garg, S.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{Arxiv}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016DeepRL}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{Hendrycks2019BenchmarkingNN}
Hendrycks, D. and Dietterich, T.~G.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{ArXiv}, abs/1903.12261, 2019.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{Hendrycks2020AugMixAS}
Hendrycks, D., Mu, N., Cubuk, E.~D., Zoph, B., Gilmer, J., and
  Lakshminarayanan, B.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{ArXiv}, abs/1912.02781, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and Gilmer]{Hendrycks2021TheMF}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,
  R., Zhu, T.~L., Parajuli, S., Guo, M., Song, D.~X., Steinhardt, J., and
  Gilmer, J.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pp.\  8320--8329, 2021.

\bibitem[Hu et~al.(2021)Hu, Uzunbas, Chen, Wang, Shah, Nevatia, and
  Lim]{Hu2021MixNormTA}
Hu, X., Uzunbas, M.~G., Chen, S., Wang, R., Shah, A., Nevatia, R., and Lim,
  S.-N.
\newblock Mixnorm: Test-time adaptation through online normalization
  estimation.
\newblock \emph{ArXiv}, abs/2110.11478, 2021.

\bibitem[Huang et~al.(2022)Huang, Gu, Wang, Xiao, Liu, and
  Wang]{huang2022extrapolative}
Huang, H., Gu, X., Wang, H., Xiao, C., Liu, H., and Wang, Y.
\newblock Extrapolative continuous-time bayesian neural network for fast
  training-free test-time adaptation.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=wiHzQWwg3l}.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456. PMLR, 2015.

\bibitem[Iwasawa \& Matsuo(2021)Iwasawa and Matsuo]{Iwasawa2021TestTimeCA}
Iwasawa, Y. and Matsuo, Y.
\newblock Test-time classifier adjustment module for model-agnostic domain
  generalization.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Jia et~al.(2022)Jia, Zhang, Wu, Ma, Wang, and Cao]{Jia2022LASATAT}
Jia, X., Zhang, Y., Wu, B., Ma, K., Wang, J., and Cao, X.
\newblock Las-at: Adversarial training with learnable attack strategy.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  13388--13398, 2022.

\bibitem[Kang et~al.(2019)Kang, Sun, Hendrycks, Brown, and
  Steinhardt]{Kang2019TestingRA}
Kang, D., Sun, Y., Hendrycks, D., Brown, T.~B., and Steinhardt, J.
\newblock Testing robustness against unforeseen adversaries.
\newblock \emph{ArXiv}, abs/1908.08016, 2019.

\bibitem[Kantorovich(1942)]{kantorovich1942translocation}
Kantorovich, L.~V.
\newblock On the translocation of masses.
\newblock In \emph{Dokl. Akad. Nauk. USSR (NS)}, volume~37, pp.\  199--201,
  1942.

\bibitem[Koh \& Liang(2017)Koh and Liang]{Koh2017UnderstandingBP}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock \emph{ArXiv}, abs/1703.04730, 2017.

\bibitem[Kojima et~al.(2022)Kojima, Matsuo, and
  Iwasawa]{Kojima2022RobustifyingVT}
Kojima, T., Matsuo, Y., and Iwasawa, Y.
\newblock Robustifying vision transformer without retraining from scratch by
  test-time class-conditional feature alignment.
\newblock \emph{ArXiv}, abs/2206.13951, 2022.

\bibitem[Kundu et~al.(2020)Kundu, Venkat, RahulM., and
  Babu]{Kundu2020UniversalSD}
Kundu, J.~N., Venkat, N., RahulM., V., and Babu, R.~V.
\newblock Universal source-free domain adaptation.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  4543--4552, 2020.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
Lee, D.-H. et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML},
  volume~3, pp.\  896, 2013.

\bibitem[Li et~al.(2020)Li, Jiao, Cao, Wong, and Wu]{Li2020ModelAU}
Li, R., Jiao, Q., Cao, W., Wong, H.-S., and Wu, S.
\newblock Model adaptation: Unsupervised domain adaptation without source data.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  9638--9647, 2020.

\bibitem[Li et~al.(2016)Li, Wang, Shi, Liu, and Hou]{li2016revisiting}
Li, Y., Wang, N., Shi, J., Liu, J., and Hou, X.
\newblock Revisiting batch normalization for practical domain adaptation.
\newblock \emph{arXiv preprint arXiv:1603.04779}, 2016.

\bibitem[Liang et~al.(2020)Liang, Hu, and Feng]{liang20a}
Liang, J., Hu, D., and Feng, J.
\newblock Do we really need to access the source data? {S}ource hypothesis
  transfer for unsupervised domain adaptation.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  6028--6039. PMLR,
  13--18 Jul 2020.

\bibitem[Liu et~al.(2021)Liu, Kothari, van Delft, Bellot-Gurlet, Mordan, and
  Alahi]{Liu2021TTTWD}
Liu, Y., Kothari, P., van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi,
  A.
\newblock Ttt++: When does self-supervised test-time training fail or thrive?
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Long et~al.(2018)Long, Cao, Wang, and Jordan]{long2018conditional}
Long, M., Cao, Z., Wang, J., and Jordan, M.~I.
\newblock Conditional adversarial domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{Loshchilov2016SGDRSG}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv: Learning}, 2016.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{Madry2018TowardsDL}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{ArXiv}, abs/1706.06083, 2018.

\bibitem[Marchant et~al.(2021)Marchant, Rubinstein, and
  Alfeld]{Marchant2021HardTF}
Marchant, N.~G., Rubinstein, B. I.~P., and Alfeld, S.
\newblock Hard to forget: Poisoning attacks on certified machine unlearning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2021.

\bibitem[Mehra et~al.(2021)Mehra, Kailkhura, Chen, and
  Hamm]{NEURIPS2021_90cc440b}
Mehra, A., Kailkhura, B., Chen, P.-Y., and Hamm, J.
\newblock Understanding the limits of unsupervised domain adaptation via data
  poisoning.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  17347--17359. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/90cc440b1b8caa520c562ac4e4bbcb51-Paper.pdf}.

\bibitem[Mirza et~al.(2021)Mirza, Micorek, Possegger, and
  Bischof]{Mirza2021TheNM}
Mirza, M.~J., Micorek, J., Possegger, H., and Bischof, H.
\newblock The norm must go on: Dynamic unsupervised domain adaptation by
  normalization.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  14745--14755, 2021.

\bibitem[Nado et~al.(2020)Nado, Padhy, Sculley, D'Amour, Lakshminarayanan, and
  Snoek]{Nado2020EvaluatingPB}
Nado, Z., Padhy, S., Sculley, D., D'Amour, A., Lakshminarayanan, B., and Snoek,
  J.
\newblock Evaluating prediction-time batch normalization for robustness under
  covariate shift.
\newblock \emph{ArXiv}, abs/2006.10963, 2020.

\bibitem[Nelson et~al.(2008)Nelson, Barreno, Chi, Joseph, Rubinstein, Saini,
  Sutton, Tygar, and Xia]{Nelson2008ExploitingML}
Nelson, B., Barreno, M., Chi, F.~J., Joseph, A.~D., Rubinstein, B. I.~P.,
  Saini, U., Sutton, C., Tygar, J.~D., and Xia, K.
\newblock Exploiting machine learning to subvert your spam filter.
\newblock In \emph{USENIX Workshop on Large-Scale Exploits and Emergent
  Threats}, 2008.

\bibitem[Niu et~al.(2022)Niu, Wu, Zhang, Chen, Zheng, Zhao, and
  Tan]{niu2022efficient}
Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., and Tan, M.
\newblock Efficient test-time model adaptation without forgetting.
\newblock In \emph{The Internetional Conference on Machine Learning}, 2022.

\bibitem[Pang et~al.(2022)Pang, Lin, Yang, Zhu, and Yan]{Pang2022RobustnessAA}
Pang, T., Lin, M., Yang, X., Zhu, J., and Yan, S.
\newblock Robustness and accuracy could be reconcilable by (proper) definition.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and
  Mann]{Rebuffi2021FixingDA}
Rebuffi, S.-A., Gowal, S., Calian, D.~A., Stimberg, F., Wiles, O., and Mann,
  T.~A.
\newblock Fixing data augmentation to improve adversarial robustness.
\newblock \emph{ArXiv}, abs/2103.01946, 2021.

\bibitem[Rusak et~al.(2022)Rusak, Schneider, Pachitariu, Eck, Gehler,
  Bringmann, Brendel, and Bethge]{rusak2022if}
Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P.~V., Bringmann,
  O., Brendel, W., and Bethge, M.
\newblock If your data distribution shifts, use self-learning, 2022.
\newblock URL \url{https://openreview.net/forum?id=1oEvY1a67c1}.

\bibitem[Saito et~al.(2018)Saito, Watanabe, Ushiku, and
  Harada]{saito2018maximum}
Saito, K., Watanabe, K., Ushiku, Y., and Harada, T.
\newblock Maximum classifier discrepancy for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3723--3732, 2018.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{Salman2020DoAR}
Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., and Madry, A.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{ArXiv}, abs/2007.08489, 2020.

\bibitem[Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and
  Bethge]{Schneider2020ImprovingRA}
Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M.
\newblock Improving robustness against common corruptions by covariate shift
  adaptation.
\newblock \emph{ArXiv}, abs/2006.16971, 2020.

\bibitem[Sehwag et~al.(2022)Sehwag, Mahloujifar, Handina, Dai, Xiang, Chiang,
  and Mittal]{Sehwag2022RobustLM}
Sehwag, V., Mahloujifar, S., Handina, T., Dai, S., Xiang, C., Chiang, M., and
  Mittal, P.
\newblock Robust learning meets generative models: Can proxy distributions
  improve adversarial robustness?
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Shafahi et~al.(2019)Shafahi, Najibi, Ghiasi, Xu, Dickerson, Studer,
  Davis, Taylor, and Goldstein]{shafahi2019free}
Shafahi, A., Najibi, M., Ghiasi, M.~A., Xu, Z., Dickerson, J., Studer, C.,
  Davis, L.~S., Taylor, G., and Goldstein, T.
\newblock Adversarial training for free!
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{Simonyan2015VeryDC}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{CoRR}, abs/1409.1556, 2015.

\bibitem[Singh \& Shrivastava(2019)Singh and Shrivastava]{Singh2019EvalNormEB}
Singh, S. and Shrivastava, A.
\newblock Evalnorm: Estimating batch normalization statistics for evaluation.
\newblock \emph{2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pp.\  3632--3640, 2019.

\bibitem[Sinha et~al.(2023)Sinha, Gehler, Locatello, and
  Schiele]{Sinha_2023_WACV}
Sinha, S., Gehler, P., Locatello, F., and Schiele, B.
\newblock Test: Test-time self-training under distribution shift.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV)}, pp.\  2759--2769, January 2023.

\bibitem[Sun et~al.(2020)Sun, Wang, Liu, Miller, Efros, and
  Hardt]{Sun2020TestTimeTW}
Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A.~A., and Hardt, M.
\newblock Test-time training with self-supervision for generalization under
  distribution shifts.
\newblock In \emph{ICML}, 2020.

\bibitem[Tram{\`e}r et~al.(2022)Tram{\`e}r, Shokri, Joaquin, Le, Jagielski,
  Hong, and Carlini]{Tramr2022TruthSP}
Tram{\`e}r, F., Shokri, R., Joaquin, A.~S., Le, H.~M., Jagielski, M., Hong, S.,
  and Carlini, N.
\newblock Truth serum: Poisoning machine learning models to reveal their
  secrets.
\newblock \emph{Proceedings of the 2022 ACM SIGSAC Conference on Computer and
  Communications Security}, 2022.

\bibitem[Vapnik(1998)]{vapnik1998statistical}
Vapnik, V.
\newblock \emph{Statistical learning theory}.
\newblock John Wiley \& Sons, 1998.

\bibitem[Vorobeychik \& Kantarcioglu(2018)Vorobeychik and
  Kantarcioglu]{Vorobeychik18book}
Vorobeychik, Y. and Kantarcioglu, M.
\newblock \emph{Adversarial Machine Learning}.
\newblock Morgan Claypool, 2018.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Ju, Shelhamer, Wagner, and
  Darrell]{Wang2021FightingGW}
Wang, D., Ju, A., Shelhamer, E., Wagner, D.~A., and Darrell, T.
\newblock Fighting gradients with gradients: Dynamic defenses against
  adversarial attacks.
\newblock \emph{ArXiv}, abs/2105.08714, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Shelhamer, Liu, Olshausen, and
  Darrell]{Wang2021TentFT}
Wang, D., Shelhamer, E., Liu, S., Olshausen, B.~A., and Darrell, T.
\newblock Tent: Fully test-time adaptation by entropy minimization.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Zhang, Zheng, Shi, Li, and
  Wang]{Wang2022RemovingBN}
Wang, H., Zhang, A., Zheng, S., Shi, X., Li, M., and Wang, Z.
\newblock Removing batch normalization boosts adversarial training.
\newblock In \emph{ICML}, 2022{\natexlab{a}}.

\bibitem[Wang \& Wibisono(2022)Wang and Wibisono]{Wang2022TowardsUG}
Wang, J.-K. and Wibisono, A.
\newblock Towards understanding gd with hard and conjugate pseudo-labels for
  test-time adaptation.
\newblock \emph{ArXiv}, abs/2210.10019, 2022.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Fink, Van~Gool, and
  Dai]{wang2022continual}
Wang, Q., Fink, O., Van~Gool, L., and Dai, D.
\newblock Continual test-time domain adaptation.
\newblock In \emph{Proceedings of Conference on Computer Vision and Pattern
  Recognition}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Jin, Long, Wang, and
  Jordan]{wang2019transferable}
Wang, X., Jin, Y., Long, M., Wang, J., and Jordan, M.~I.
\newblock Transferable normalization: Towards improving transferability of deep
  neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Wong et~al.(2019)Wong, Rice, and Kolter]{wong2019fast}
Wong, E., Rice, L., and Kolter, J.~Z.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wu et~al.(2020{\natexlab{a}})Wu, Xia, and Wang]{Wu2020AdversarialWP}
Wu, D., Xia, S., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{arXiv: Learning}, 2020{\natexlab{a}}.

\bibitem[Wu \& He(2021)Wu and He]{10.1145/3447548.3467214}
Wu, J. and He, J.
\newblock Indirect invisible poisoning attacks on domain adaptation.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, KDD '21, pp.\  1852â€“1862, New York, NY, USA,
  2021. Association for Computing Machinery.
\newblock ISBN 9781450383325.
\newblock \doi{10.1145/3447548.3467214}.
\newblock URL \url{https://doi.org/10.1145/3447548.3467214}.

\bibitem[Wu et~al.(2020{\natexlab{b}})Wu, Yuan, and Wu]{pmlr-v119-wu20f}
Wu, Y.-H., Yuan, C.-H., and Wu, S.-H.
\newblock Adversarial robustness via runtime masking and cleansing.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10399--10409. PMLR,
  13--18 Jul 2020{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v119/wu20f.html}.

\bibitem[Xie et~al.(2018)Xie, Zhang, Wang, Zhou, Ren, and
  Yuille]{Xie2018ImprovingTO}
Xie, C., Zhang, Z., Wang, J., Zhou, Y., Ren, Z., and Yuille, A.~L.
\newblock Improving transferability of adversarial examples with input
  diversity.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  2725--2734, 2018.

\bibitem[Xie et~al.(2019)Xie, Wu, van~der Maaten, Yuille, and
  He]{Xie2019FeatureDF}
Xie, C., Wu, Y., van~der Maaten, L., Yuille, A.~L., and He, K.
\newblock Feature denoising for improving adversarial robustness.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  501--509, 2019.

\bibitem[Xie et~al.(2020)Xie, Tan, Gong, Wang, Yuille, and
  Le]{Xie2020AdversarialEI}
Xie, C., Tan, M., Gong, B., Wang, J., Yuille, A.~L., and Le, Q.~V.
\newblock Adversarial examples improve image recognition.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  816--825, 2020.

\bibitem[You et~al.(2021)You, Li, and Zhao]{You2021TesttimeBS}
You, F., Li, J., and Zhao, Z.
\newblock Test-time batch statistics calibration for covariate shift.
\newblock \emph{ArXiv}, abs/2110.04065, 2021.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{Zagoruyko2016WideRN}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{ArXiv}, abs/1605.07146, 2016.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Levine, and
  Finn]{Zhang2021MEMOTT}
Zhang, M., Levine, S., and Finn, C.
\newblock Memo: Test time robustness via adaptation and augmentation.
\newblock \emph{ArXiv}, abs/2110.09506, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Marklund, Dhawan, Gupta,
  Levine, and Finn]{Zhang2021AdaptiveRM}
Zhang, M., Marklund, H., Dhawan, N., Gupta, A., Levine, S., and Finn, C.
\newblock Adaptive risk minimization: Learning to adapt to domain shift.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Gu, Matsuo, and
  Iwasawa]{Zhang2021DomainPL}
Zhang, X., Gu, S.~S., Matsuo, Y., and Iwasawa, Y.
\newblock Domain prompt learning for efficiently adapting clip to unseen
  domains, 2021{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2111.12853}.

\bibitem[Zhou \& Levine(2021)Zhou and Levine]{zhou2021bayesian}
Zhou, A. and Levine, S.
\newblock Bayesian adaptation for covariate shift.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 914--927, 2021.

\end{thebibliography}
