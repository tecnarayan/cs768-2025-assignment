\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Badrinath \& Kalathil(2021)Badrinath and
  Kalathil]{badrinath2021robust}
Badrinath, K.~P. and Kalathil, D.
\newblock Robust reinforcement learning using least squares policy iteration
  with provable performance guarantees.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  511--520. PMLR, 2021.

\bibitem[Bagnell et~al.(2001)Bagnell, Ng, and Schneider]{bagnell2001solving}
Bagnell, J.~A., Ng, A.~Y., and Schneider, J.~G.
\newblock Solving uncertain {M}arkov decision processes.
\newblock 2001.

\bibitem[Bertsekas(2009)]{bertsekas2009convex}
Bertsekas, D.~P.
\newblock \emph{Convex optimization theory}.
\newblock Athena Scientific Belmont, 2009.

\bibitem[Borwein \& Lewis(2010)Borwein and Lewis]{borwein2010convex}
Borwein, J. and Lewis, A.~S.
\newblock \emph{Convex analysis and nonlinear optimization: theory and
  examples}.
\newblock Springer Science \& Business Media, 2010.

\bibitem[Cen et~al.(2020)Cen, Cheng, Chen, Wei, and Chi]{cen2020fast}
Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{arXiv preprint arXiv:2007.06558}, 2020.

\bibitem[Derman \& Mannor(2020)Derman and Mannor]{derman2020distributional}
Derman, E. and Mannor, S.
\newblock Distributional robustness and regularization in reinforcement
  learning.
\newblock \emph{ICML Workshop}, 2020.

\bibitem[Derman et~al.(2018)Derman, Mankowitz, Mann, and
  Mannor]{derman2018soft}
Derman, E., Mankowitz, D., Mann, T., and Mannor, S.
\newblock Soft-robust actor-critic policy-gradient.
\newblock \emph{AUAI press for Association for Uncertainty in Artificial
  Intelligence}, pp.\  208--218, 2018.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and
  Chandra]{duchi2008efficient}
Duchi, J., Shalev-Shwartz, S., Singer, Y., and Chandra, T.
\newblock Efficient projections onto the l 1-ball for learning in high
  dimensions.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  272--279, 2008.

\bibitem[Eysenbach \& Levine(2021)Eysenbach and Levine]{eysenbach2021maximum}
Eysenbach, B. and Levine, S.
\newblock Maximum entropy {RL} (provably) solves some robust {RL} problems.
\newblock \emph{arXiv preprint arXiv:2103.06257}, 2021.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169. PMLR, 2019.

\bibitem[Grand-Cl{\'e}ment \& Kroer(2021)Grand-Cl{\'e}ment and
  Kroer]{grand2021scalable}
Grand-Cl{\'e}ment, J. and Kroer, C.
\newblock Scalable first-order methods for robust mdps.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  12086--12094, 2021.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1352--1361. PMLR, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hiriart-Urruty \& Lemar{\'e}chal(2004)Hiriart-Urruty and
  Lemar{\'e}chal]{hiriart2004fundamentals}
Hiriart-Urruty, J.-B. and Lemar{\'e}chal, C.
\newblock \emph{Fundamentals of convex analysis}.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Ho et~al.(2018)Ho, Petrik, and Wiesemann]{ho2018fast}
Ho, C.~P., Petrik, M., and Wiesemann, W.
\newblock Fast {B}ellman updates for robust {MDP}s.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1979--1988. PMLR, 2018.

\bibitem[Husain et~al.(2021)Husain, Ciosek, and Tomioka]{husain2021regularized}
Husain, H., Ciosek, K., and Tomioka, R.
\newblock Regularized policies are reward robust.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  64--72. PMLR, 2021.

\bibitem[Iyengar(2005)]{iyengar2005robust}
Iyengar, G.~N.
\newblock Robust dynamic programming.
\newblock \emph{Mathematics of Operations Research}, 30\penalty0 (2):\penalty0
  257--280, 2005.

\bibitem[Kaufman \& Schaefer(2013)Kaufman and Schaefer]{kaufman2013robust}
Kaufman, D.~L. and Schaefer, A.~J.
\newblock Robust modified policy iteration.
\newblock \emph{INFORMS Journal on Computing}, 25\penalty0 (3):\penalty0
  396--410, 2013.

\bibitem[Kuhn et~al.(2019)Kuhn, Esfahani, Nguyen, and
  Shafieezadeh-Abadeh]{kuhn2019wasserstein}
Kuhn, D., Esfahani, P.~M., Nguyen, V.~A., and Shafieezadeh-Abadeh, S.
\newblock Wasserstein distributionally robust optimization: Theory and
  applications in machine learning.
\newblock In \emph{Operations Research \& Management Science in the Age of
  Analytics}, pp.\  130--166. INFORMS, 2019.

\bibitem[Lee et~al.(2018)Lee, Choi, and Oh]{lee2018sparse}
Lee, K., Choi, S., and Oh, S.
\newblock Sparse {M}arkov decision processes with causal sparse {T}sallis
  entropy regularization for reinforcement learning.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (3):\penalty0
  1466--1473, 2018.

\bibitem[Mankowitz et~al.(2018)Mankowitz, Mann, Bacon, Precup, and
  Mannor]{mankowitz2018learning}
Mankowitz, D., Mann, T., Bacon, P.-L., Precup, D., and Mannor, S.
\newblock Learning robust options.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Mannor et~al.(2007)Mannor, Simester, Sun, and
  Tsitsiklis]{mannor2007bias}
Mannor, S., Simester, D., Sun, P., and Tsitsiklis, J.~N.
\newblock Bias and variance approximation in value function estimates.
\newblock \emph{Management Science}, 53\penalty0 (2):\penalty0 308--322, 2007.

\bibitem[Mannor et~al.(2012)Mannor, Mebel, and Xu]{mannor2012lightning}
Mannor, S., Mebel, O., and Xu, H.
\newblock Lightning does not strike twice: Robust {MDP}s with coupled
  uncertainty.
\newblock \emph{ICML}, 2012.

\bibitem[Mannor et~al.(2016)Mannor, Mebel, and Xu]{mannor2016robust}
Mannor, S., Mebel, O., and Xu, H.
\newblock Robust {MDP}s with k-rectangular uncertainty.
\newblock \emph{Mathematics of Operations Research}, 41\penalty0 (4):\penalty0
  1484--1509, 2016.

\bibitem[Mensch \& Blondel(2018)Mensch and Blondel]{mensch2018differentiable}
Mensch, A. and Blondel, M.
\newblock Differentiable dynamic programming for structured prediction and
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3462--3471. PMLR, 2018.

\bibitem[Nachum \& Dai(2020)Nachum and Dai]{nachum2020reinforcement}
Nachum, O. and Dai, B.
\newblock Reinforcement learning via {F}enchel-{R}ockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[Nilim \& El~Ghaoui(2005)Nilim and El~Ghaoui]{nilim2005robust}
Nilim, A. and El~Ghaoui, L.
\newblock Robust control of {M}arkov decision processes with uncertain
  transition matrices.
\newblock \emph{Operations Research}, 53\penalty0 (5):\penalty0 780--798, 2005.

\bibitem[Parlett(1974)]{parlett1974rayleigh}
Parlett, B.~N.
\newblock The {R}ayleigh quotient iteration and some generalizations for
  nonnormal matrices.
\newblock \emph{Mathematics of Computation}, 28\penalty0 (127):\penalty0
  679--693, 1974.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2817--2826. PMLR, 2017.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
Rockafellar, R.~T.
\newblock \emph{Convex analysis}, volume~36.
\newblock Princeton university press, 1970.

\bibitem[Roy et~al.(2017)Roy, Xu, and Pokutta]{roy2017reinforcement}
Roy, A., Xu, H., and Pokutta, S.
\newblock Reinforcement learning under model mismatch.
\newblock \emph{NIPS}, 2017.

\bibitem[Scherrer et~al.(2015)Scherrer, Ghavamzadeh, Gabillon, Lesner, and
  Geist]{scherrer2015approximate}
Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M.
\newblock Approximate modified policy iteration and its application to the game
  of {T}etris.
\newblock \emph{J. Mach. Learn. Res.}, 16:\penalty0 1629--1676, 2015.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Shafieezadeh-Abadeh et~al.(2015)Shafieezadeh-Abadeh, Esfahani, and
  Kuhn]{shafieezadeh2015distributionally}
Shafieezadeh-Abadeh, S., Esfahani, P.~M., and Kuhn, D.
\newblock Distributionally robust logistic regression.
\newblock \emph{NIPS}, 2015.

\bibitem[Shani et~al.(2020)Shani, Efroni, and Mannor]{shani2020adaptive}
Shani, L., Efroni, Y., and Mannor, S.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized {MDP}s.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5668--5675, 2020.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, Mansour,
  et~al.]{sutton1999policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., Mansour, Y., et~al.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{NIPs}, volume~99, pp.\  1057--1063. Citeseer, 1999.

\bibitem[Tamar et~al.(2014)Tamar, Mannor, and Xu]{tamar2014scaling}
Tamar, A., Mannor, S., and Xu, H.
\newblock Scaling up robust {MDP}s using function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  181--189. PMLR, 2014.

\bibitem[Tessler et~al.(2019)Tessler, Efroni, and Mannor]{tessler2019action}
Tessler, C., Efroni, Y., and Mannor, S.
\newblock Action robust reinforcement learning and applications in continuous
  control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6215--6224. PMLR, 2019.

\bibitem[Wiesemann et~al.(2013)Wiesemann, Kuhn, and
  Rustem]{wiesemann2013robust}
Wiesemann, W., Kuhn, D., and Rustem, B.
\newblock Robust {M}arkov decision processes.
\newblock \emph{Mathematics of Operations Research}, 38\penalty0 (1):\penalty0
  153--183, 2013.

\bibitem[Xu et~al.(2009)Xu, Caramanis, and Mannor]{xu2009robustness}
Xu, H., Caramanis, C., and Mannor, S.
\newblock Robustness and regularization of support vector machines.
\newblock \emph{Journal of machine learning research}, 10\penalty0 (7), 2009.

\bibitem[Yang \& Zhang(2021)Yang and Zhang]{yang2021non}
Yang, W. and Zhang, Z.
\newblock Non-asymptotic performances of {R}obust {M}arkov {D}ecision
  {P}rocesses.
\newblock \emph{arXiv preprint arXiv:2105.03863}, 2021.

\end{thebibliography}
