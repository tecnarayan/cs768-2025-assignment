\begin{thebibliography}{10}

\bibitem{atashgahi2020quick}
Z.~Atashgahi, G.~Sokar, T.~van~der Lee, E.~Mocanu, D.~C. Mocanu, R.~Veldhuis,
  and M.~Pechenizkiy.
\newblock Quick and robust feature selection: the strength of energy-efficient
  sparse training for autoencoders.
\newblock {\em arXiv:2012.00560}, 2020.

\bibitem{bartoldson2019generalization}
B.~Bartoldson, A.~Morcos, A.~Barbu, and G.~Erlebacher.
\newblock The generalization-stability tradeoff in neural network pruning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 20852--20864. Curran Associates, Inc., 2020.

\bibitem{bellec2018deep}
G.~Bellec, D.~Kappel, W.~Maass, and R.~Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.
  arXiv:1711.05136 (2017).

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~Ziegler, J.~Wu, C.~Winter,
  C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark,
  C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{chen2020lottery2}
T.~Chen, J.~Frankle, S.~Chang, S.~Liu, Y.~Zhang, M.~Carbin, and Z.~Wang.
\newblock The lottery tickets hypothesis for supervised and self-supervised
  pre-training in computer vision models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 16306--16316, June 2021.

\bibitem{de2020progressive}
P.~de~Jorge, A.~Sanyal, H.~Behl, P.~Torr, G.~Rogez, and P.~K. Dokania.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock In {\em International Conference on Learning Representations}, 2021.
  arXiv:cs.CV/2006.09081.

\bibitem{denton2014exploiting}
E.~Denton, W.~Zaremba, J.~Bruna, Y.~LeCun, and R.~Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock {\em Twenty-eighth Conference on Neural Information Processing
  Systems. arXiv:1404.0736}, 2014.

\bibitem{dettmers2019sparse}
T.~Dettmers and L.~Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{evci2020rigging}
U.~Evci, T.~Gale, J.~Menick, P.~S. Castro, and E.~Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning}, pages
  2943--2952. PMLR, 2020.

\bibitem{frankle2018lottery}
J.~Frankle and M.~Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.
  arXiv:1803.03635.

\bibitem{frankle2020pruning}
J.~Frankle, G.~K. Dziugaite, D.~Roy, and M.~Carbin.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock In {\em International Conference on Learning Representations}, 2021.
  arXiv:2009.08576.

\bibitem{frankle2019lottery}
J.~Frankle, G.~K. Dziugaite, D.~M. Roy, and M.~Carbin.
\newblock The lottery ticket hypothesis at scale.
\newblock {\em arXiv preprint arXiv:1903.01611}, 2019.

\bibitem{gale2019state}
T.~Gale, E.~Elsen, and S.~Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{gale2020sparse}
T.~Gale, M.~Zaharia, C.~Young, and E.~Elsen.
\newblock Sparse gpu kernels for deep learning.
\newblock {\em arXiv preprint arXiv:2006.10901}, 2020.

\bibitem{garcia2019estimation}
E.~Garc{\'\i}a-Mart{\'\i}n, C.~F. Rodrigues, G.~Riley, and H.~Grahn.
\newblock Estimation of energy consumption in machine learning.
\newblock {\em Journal of Parallel and Distributed Computing}, 134:75--88,
  2019.

\bibitem{gebhart2021unified}
T.~Gebhart, U.~Saxena, and P.~Schrater.
\newblock A unified paths perspective for pruning at initialization.
\newblock {\em arXiv preprint arXiv:2101.10552}, 2021.

\bibitem{guo2018network}
J.~Guo, Y.~Li, W.~Lin, Y.~Chen, and J.~Li.
\newblock Network decoupling: From regular to depthwise separable convolutions.
\newblock {\em arXiv preprint arXiv:1808.05517}, 2018.

\bibitem{han2015learning}
S.~Han, J.~Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in neural information processing systems}, pages
  1135--1143, 2015.

\bibitem{hassibi1993second}
B.~Hassibi and D.~G. Stork.
\newblock {\em Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hoefler2021sparsity}
T.~Hoefler, D.~Alistarh, T.~Ben-Nun, N.~Dryden, and A.~Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock {\em Journal of Machine Learning Research}, 22(241):1--124, 2021.

\bibitem{hubara2021accelerated}
I.~Hubara, B.~Chmiel, M.~Island, R.~Banner, S.~Naor, and D.~Soudry.
\newblock Accelerated sparse neural training: A provable and efficient method
  to find n: M transposable masks.
\newblock {\em arXiv preprint arXiv:2102.08124}, 2021.

\bibitem{jaderberg2014speeding}
M.~Jaderberg, A.~Vedaldi, and A.~Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock {\em arXiv preprint arXiv:1405.3866}, 2014.

\bibitem{janowsky1989pruning}
S.~A. Janowsky.
\newblock Pruning versus clipping in neural networks.
\newblock {\em Physical Review A}, 39(12):6600, 1989.

\bibitem{jayakumar2020top}
S.~Jayakumar, R.~Pascanu, J.~Rae, S.~Osindero, and E.~Elsen.
\newblock Top-kast: Top-k always sparse training.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20744--20754, 2020.

\bibitem{kandel2000principles}
E.~R. Kandel, J.~H. Schwartz, T.~M. Jessell, S.~Siegelbaum, A.~J. Hudspeth, and
  S.~Mack.
\newblock {\em Principles of neural science}, volume~4.
\newblock McGraw-hill New York, 2000.

\bibitem{Krizhevsky09}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem{kusupati2020soft}
A.~Kusupati, V.~Ramanujan, R.~Somani, M.~Wortsman, P.~Jain, S.~Kakade, and
  A.~Farhadi.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In {\em International Conference on Machine Learning}, pages
  5544--5555. PMLR, 2020.

\bibitem{lecun1990optimal}
Y.~LeCun, J.~S. Denker, and S.~A. Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem{lee2019signal}
N.~Lee, T.~Ajanthan, S.~Gould, and P.~H.~S. Torr.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In {\em International Conference on Learning Representations}, 2020.
  arXiv:1906.06307.

\bibitem{lee2018snip}
N.~Lee, T.~Ajanthan, and P.~H. Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{li2016pruning}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em International Conference on Learning Representations}, 2016.

\bibitem{Lin2020Dynamic}
T.~Lin, S.~U. Stich, L.~Barba, D.~Dmitriev, and M.~Jaggi.
\newblock Dynamic model pruning with feedback.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{LIU2020Dynamic}
J.~Liu, Z.~Xu, R.~Shi, R.~C.~C. Cheung, and H.~K. So.
\newblock Dynamic sparse training: Find efficient sparse network from scratch
  with trainable masked layers.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{liu2021sparse}
S.~Liu, D.~C. Mocanu, A.~R.~R. Matavalam, Y.~Pei, and M.~Pechenizkiy.
\newblock Sparse evolutionary deep learning with over one million artificial
  neurons on commodity hardware.
\newblock {\em Neural Computing and Applications}, 33(7):2589--2604, 2021.

\bibitem{liu2021selfish}
S.~Liu, D.~C. Mocanu, Y.~Pei, and M.~Pechenizkiy.
\newblock Selfish sparse rnn training.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, pages 6893--6904. PMLR, 2021.

\bibitem{liu2021we}
S.~Liu, L.~Yin, D.~C. Mocanu, and M.~Pechenizkiy.
\newblock Do we actually need dense over-parameterization? in-time
  over-parameterization in sparse training.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, pages 6989--7000. PMLR, 2021.

\bibitem{liu2020finding}
T.~Liu and F.~Zenke.
\newblock Finding trainable sparse networks through neural tangent transfer.
\newblock In {\em International Conference on Machine Learning}, pages
  6336--6347. PMLR, 2020.

\bibitem{liu2018rethinking}
Z.~Liu, M.~Sun, T.~Zhou, G.~Huang, and T.~Darrell.
\newblock Rethinking the value of network pruning.
\newblock {\em International Conference on Learning Representations}, 2019.

\bibitem{louizos2017learning}
C.~Louizos, M.~Welling, and D.~P. Kingma.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{mahar2018intrinsic}
M.~Mahar and V.~Cavalli.
\newblock Intrinsic mechanisms of neuronal axon regeneration.
\newblock {\em Nature Reviews Neuroscience}, 19(6):323--337, 2018.

\bibitem{mittal2018recovering}
D.~Mittal, S.~Bhardwaj, M.~M. Khapra, and B.~Ravindran.
\newblock Recovering from random pruning: On the plasticity of deep
  convolutional neural networks.
\newblock In {\em 2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 848--857. IEEE, 2018.

\bibitem{mocanu2021sparse}
D.~C. Mocanu, E.~Mocanu, T.~Pinto, S.~Curci, P.~H. Nguyen, M.~Gibescu,
  D.~Ernst, and Z.~A. Vale.
\newblock Sparse training theory for scalable and efficient agents.
\newblock {\em International Conference on Autonomous Agents and Multiagent
  Systems (AAMAS). arXiv:2103.01636}, 2021.

\bibitem{mocanu2018scalable}
D.~C. Mocanu, E.~Mocanu, P.~Stone, P.~H. Nguyen, M.~Gibescu, and A.~Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature communications}, 9(1):2383, 2018.

\bibitem{molchanov2017variational}
D.~Molchanov, A.~Ashukha, and D.~Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2498--2507. PMLR, 2017.

\bibitem{molchanov2019importance}
P.~Molchanov, A.~Mallya, S.~Tyree, I.~Frosio, and J.~Kautz.
\newblock Importance estimation for neural network pruning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11264--11272, 2019.

\bibitem{molchanov2016pruning}
P.~Molchanov, S.~Tyree, T.~Karras, T.~Aila, and J.~Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em International Conference on Learning Representations}, 2016.

\bibitem{mostafa2019parameter}
H.~Mostafa and X.~Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock {\em International Conference on Machine Learning}, 2019.

\bibitem{mozer1989skeletonization}
M.~C. Mozer and P.~Smolensky.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In {\em Advances in neural information processing systems}, pages
  107--115, 1989.

\bibitem{mozer1989using}
M.~C. Mozer and P.~Smolensky.
\newblock Using relevance to reduce network size automatically.
\newblock {\em Connection Science}, 1(1):3--16, 1989.

\bibitem{nagappan2020neuroregeneration}
P.~G. Nagappan, H.~Chen, and D.-Y. Wang.
\newblock Neuroregeneration and plasticity: a review of the physiological
  mechanisms for achieving functional recovery postinjury.
\newblock {\em Military Medical Research}, 7(1):1--16, 2020.

\bibitem{nvidia2020}
Nvidia.
\newblock Nvidia a100 tensor core gpu architecture.
\newblock {\em
  https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  2020.

\bibitem{patterson2021carbon}
D.~Patterson, J.~Gonzalez, Q.~Le, C.~Liang, L.-M. Munguia, D.~Rothchild, D.~So,
  M.~Texier, and J.~Dean.
\newblock Carbon emissions and large neural network training.
\newblock {\em arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{renda2020comparing}
A.~Renda, J.~Frankle, and M.~Carbin.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In {\em International Conference on Learning Representations}, 2020.
  arXiv:2003.02389.

\bibitem{savarese2019winning}
P.~Savarese, H.~Silva, and M.~Maire.
\newblock Winning the lottery with continuous sparsification.
\newblock {\em arXiv preprint arXiv:1912.04427}, 2019.

\bibitem{schwartz2019green}
R.~Schwartz, J.~Dodge, N.~A. Smith, and O.~Etzioni.
\newblock Green ai.
\newblock {\em arXiv preprint arXiv:1907.10597}, 2019.

\bibitem{shen2020cpot}
Y.~Shen, L.~Shen, H.-Z. Huang, X.~Wang, and W.~Liu.
\newblock Cpot: Channel pruning via optimal transport.
\newblock {\em arXiv preprint arXiv:2005.10451}, 2020.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em International Conference on Learning Representations}, 2014.

\bibitem{singh2020woodfisher}
S.~P. Singh and D.~Alistarh.
\newblock Woodfisher: Efficient second-order approximation for neural network
  compression.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{srinivas2017training}
S.~Srinivas, A.~Subramanya, and R.~Venkatesh~Babu.
\newblock Training sparse neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 138--145, 2017.

\bibitem{strom2015scalable}
N.~Strom.
\newblock Scalable distributed dnn training using commodity gpu cloud
  computing.
\newblock In {\em Sixteenth Annual Conference of the International Speech
  Communication Association}, 2015.

\bibitem{strubell2019energy}
E.~Strubell, A.~Ganesh, and A.~McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock {\em arXiv preprint arXiv:1906.02243}, 2019.

\bibitem{tanaka2020pruning}
H.~Tanaka, D.~Kunin, D.~L. Yamins, and S.~Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock {\em Advances in Neural Information Processing Systems.
  arXiv:2006.05467}, 2020.

\bibitem{touvron2020fixing}
H.~Touvron, A.~Vedaldi, M.~Douze, and H.~Jegou.
\newblock Fixing the train-test resolution discrepancy.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{verdenius2020pruning}
S.~Verdenius, M.~Stol, and P.~Forr{\'e}.
\newblock Pruning via iterative ranking of sensitivity statistics.
\newblock {\em arXiv preprint arXiv:2006.00896}, 2020.

\bibitem{verma2021sparsifying}
S.~Verma and J.-C. Pesquet.
\newblock Sparsifying networks via subdifferential inclusion.
\newblock In M.~Meila and T.~Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 10542--10552. PMLR, 18--24 Jul 2021.

\bibitem{Wang2020Picking}
C.~Wang, G.~Zhang, and R.~Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{NIPS2016_41bfd20a}
W.~Wen, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.

\bibitem{wortsman2019discovering}
M.~Wortsman, A.~Farhadi, and M.~Rastegari.
\newblock Discovering neural wirings.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{xiao2019autoprune}
X.~Xiao, Z.~Wang, and S.~Rajasekaran.
\newblock Autoprune: Automatic network pruning by regularizing auxiliary
  parameters.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{xu2018trained}
Y.~Xu, Y.~Li, S.~Zhang, W.~Wen, B.~Wang, Y.~Qi, Y.~Chen, W.~Lin, and H.~Xiong.
\newblock Trained rank pruning for efficient deep neural networks.
\newblock {\em arXiv preprint arXiv:1812.02402}, 2018.

\bibitem{yang2020procrustes}
D.~Yang, A.~Ghasemazar, X.~Ren, M.~Golub, G.~Lemieux, and M.~Lis.
\newblock Procrustes: a dataflow and accelerator for sparse deep neural network
  training.
\newblock In {\em 2020 53rd Annual IEEE/ACM International Symposium on
  Microarchitecture (MICRO)}, pages 711--724. IEEE, 2020.

\bibitem{yiu2006glial}
G.~Yiu and Z.~He.
\newblock Glial inhibition of cns axon regeneration.
\newblock {\em Nature Reviews Neuroscience}, 7(8):617--627, 2006.

\bibitem{you2019drawing}
H.~You, C.~Li, P.~Xu, Y.~Fu, Y.~Wang, X.~Chen, R.~G. Baraniuk, Z.~Wang, and
  Y.~Lin.
\newblock Drawing early-bird tickets: Towards more efficient training of deep
  networks.
\newblock {\em arXiv preprint arXiv:1909.11957}, 2019.

\bibitem{yuan2020block}
G.~Yuan, L.~Shen, and W.-S. Zheng.
\newblock A block decomposition algorithm for sparse optimization.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 275--285, 2020.

\bibitem{zhou2021learning}
A.~Zhou, Y.~Ma, J.~Zhu, J.~Liu, Z.~Zhang, K.~Yuan, W.~Sun, and H.~Li.
\newblock Learning n:m fine-grained structured sparse neural networks from
  scratch.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhu2017prune}
M.~H. Zhu and S.~Gupta.
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression, 2018.

\end{thebibliography}
