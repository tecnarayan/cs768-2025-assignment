@article{wang2019neural,
  title={Neural policy gradient methods: Global optimality and rates of convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1909.01150},
  year={2019}
}

@inproceedings{braverman2019multi,
  title={Multi-armed bandit problems with strategic arms},
  author={Braverman, Mark and Mao, Jieming and Schneider, Jon and Weinberg, S Matthew},
  booktitle={Conference on Learning Theory},
  pages={383--416},
  year={2019},
  organization={PMLR}
}
@inproceedings{letchford2009learning,
  title={Learning and approximating the optimal strategy to commit to},
  author={Letchford, Joshua and Conitzer, Vincent and Munagala, Kamesh},
  booktitle={Algorithmic Game Theory: Second International Symposium, SAGT 2009, Paphos, Cyprus, October 18-20, 2009. Proceedings 2},
  pages={250--262},
  year={2009},
  organization={Springer}
}

@inproceedings{zhang2021robust,
  title={Robust policy gradient against strong data corruption},
  author={Zhang, Xuezhou and Chen, Yiding and Zhu, Xiaojin and Sun, Wen},
  booktitle={International Conference on Machine Learning},
  pages={12391--12401},
  year={2021},
  organization={PMLR}
}

@article{dann2017unifying,
  title={Unifying {PAC} and regret: Uniform {PAC} bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{yang2020overview,
  title={An overview of multi-agent reinforcement learning from game theoretical perspective},
  author={Yang, Yaodong and Wang, Jun},
  journal={arXiv preprint arXiv:2011.00583},
  year={2020}
}
@article{moravvcik2017deepstack,
  title={Deepstack: Expert-level artificial intelligence in heads-up no-limit poker},
  author={Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\`y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  journal={Science},
  volume={356},
  number={6337},
  pages={508--513},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{brown2018superhuman,
  title={Superhuman {AI} for heads-up no-limit poker: Libratus beats top professionals},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={359},
  number={6374},
  pages={418--424},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{jaderberg2019human,
  title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},
  journal={Science},
  volume={364},
  number={6443},
  pages={859--865},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{baker2019emergent,
  title={Emergent tool use from multi-agent autocurricula},
  author={Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  journal={arXiv preprint arXiv:1909.07528},
  year={2019}
}

@article{bai2020near,
  title={Near-optimal reinforcement learning with self-play},
  author={Bai, Yu and Jin, Chi and Yu, Tiancheng},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={2159--2170},
  year={2020}
}





@inproceedings{liu2021sharp,
  title={A sharp analysis of model-based reinforcement learning with self-play},
  author={Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={7001--7010},
  year={2021},
  organization={PMLR}
}

@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move {M}arkov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on learning theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR}
}

@inproceedings{bai2020provable,
  title={Provable self-play algorithms for competitive reinforcement learning},
  author={Bai, Yu and Jin, Chi},
  booktitle={International conference on machine learning},
  pages={551--560},
  year={2020},
  organization={PMLR}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{shalev2016safe,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{ahmadi2022classification,
  title={On classification of strategic agents who can both game and improve},
  author={Ahmadi, Saba and Beyhaghi, Hedyeh and Blum, Avrim and Naggita, Keziah},
  journal={arXiv preprint arXiv:2203.00124},
  year={2022}
}



@article{vinyals2019grandmaster,
  title={Grandmaster level in {StarCraft II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}


@article{blum2014learning,
  title={Learning optimal commitment to overcome insecurity},
  author={Blum, Avrim and Haghtalab, Nika and Procaccia, Ariel D},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}


@inproceedings{
do2023multiagent,
title={Multi-Agent Learning with Heterogeneous Linear Contextual Bandits},
author={Anh Do and Thanh Nguyen-Tang and Raman Arora},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=7f6vH3mmhr}
}

@article{cheng2022provable,
  title={Provable benefit of multitask representation learning in reinforcement learning},
  author={Cheng, Yuan and Feng, Songtao and Yang, Jing and Zhang, Hong and Liang, Yingbin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31741--31754},
  year={2022}
}

@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}
@inproceedings{cui2023breaking,
  title={Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation},
  author={Cui, Qiwen and Zhang, Kaiqing and Du, Simon},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2651--2652},
  year={2023},
  organization={PMLR}
}

@phdthesis{yu2023near,
  title={Near-Optimal Learning in Sequential Games},
  author={Yu, Tiancheng},
  year={2023},
  school={Massachusetts Institute of Technology}
}

@article{nash1950equilibrium,
  title={Equilibrium points in n-person games},
  author={Nash Jr, John F},
  journal={Proceedings of the national academy of sciences},
  volume={36},
  number={1},
  pages={48--49},
  year={1950},
  publisher={National Acad Sciences}
}

@inproceedings{nguyen2023adversarial,
  title={Adversarial Online Multi-Task Reinforcement Learning},
  author={Nguyen, Quan and Mehta, Nishant},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1124--1165},
  year={2023},
  organization={PMLR}
}

@article{fudenberg2016whither,
  title={Whither game theory? Towards a theory of learning in games},
  author={Fudenberg, Drew and Levine, David K},
  journal={Journal of Economic Perspectives},
  volume={30},
  number={4},
  pages={151--170},
  year={2016},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2418}
}

@inproceedings{liu2022learning,
  title={Learning {M}arkov games with adversarial opponents: Efficient algorithms and fundamental limits},
  author={Liu, Qinghua and Wang, Yuanhao and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={14036--14053},
  year={2022},
  organization={PMLR}
}

@book{fudenberg1998theory,
  title={The theory of learning in games},
  author={Fudenberg, Drew and Levine, David K},
  volume={2},
  year={1998},
  publisher={MIT press}
}

@inproceedings{daniely2014optimal,
  title={Optimal learners for multiclass problems},
  author={Daniely, Amit and Shalev-Shwartz, Shai},
  booktitle={Conference on Learning Theory},
  pages={287--316},
  year={2014},
  organization={PMLR}
}
@article{shapley1953stochastic,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953},
  publisher={National Acad Sciences}
}
@inproceedings{foster2023complexity,
  title={On the complexity of multi-agent decision making: From learning in games to partial monitoring},
  author={Foster, Dean and Foster, Dylan J and Golowich, Noah and Rakhlin, Alexander},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2678--2792},
  year={2023},
  organization={PMLR}
}

@article{jin2021v,
  title={V-Learning--A Simple, Efficient, Decentralized Algorithm for Multiagent {RL}},
  author={Jin, Chi and Liu, Qinghua and Wang, Yuanhao and Yu, Tiancheng},
  journal={arXiv preprint arXiv:2110.14555},
  year={2021}
}

@inproceedings{tian2021online,
  title={Online learning in unknown {M}arkov games},
  author={Tian, Yi and Wang, Yuanhao and Yu, Tiancheng and Sra, Suvrit},
  booktitle={International conference on machine learning},
  pages={10279--10288},
  year={2021},
  organization={PMLR}
}

@book{filar2012competitive,
  title={Competitive Markov decision processes},
  author={Filar, Jerzy and Vrieze, Koos},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{arora2012online,
  title={Online bandit learning against an adaptive adversary: from regret to policy regret},
  author={Arora, Raman and Dekel, Ofer and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1206.6400},
  year={2012}
}

@inproceedings{jin2022power,
  title={The power of exploiter: Provable multi-agent {RL} in large state spaces},
  author={Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={10251--10279},
  year={2022},
  organization={PMLR}
}

@article{wei2017online,
  title={Online reinforcement learning in stochastic games},
  author={Wei, Chen-Yu and Hong, Yi-Te and Lu, Chi-Jen},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{wang2023breaking,
  title={Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation},
  author={Wang, Yuanhao and Liu, Qinghua and Bai, Yu and Jin, Chi},
  journal={arXiv preprint arXiv:2302.06606},
  year={2023}
}

@article{dinh2023online,
  title={Online {M}arkov decision processes with non-oblivious strategic adversary},
  author={Dinh, Le Cong and Mguni, David Henry and Tran-Thanh, Long and Wang, Jun and Yang, Yaodong},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={37},
  number={1},
  pages={15},
  year={2023},
  publisher={Springer}
}

@article{russo2013eluder,
  title={Eluder dimension and the sample complexity of optimistic exploration},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@inproceedings{daskalakis2023complexity,
  title={The complexity of markov equilibrium in stochastic games},
  author={Daskalakis, Constantinos and Golowich, Noah and Zhang, Kaiqing},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={4180--4234},
  year={2023},
  organization={PMLR}
}


@book{stackelberg1934marktform,
  title={Market structure and equilibrium},
  author={Von Stackelberg, Heinrich},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@inproceedings{domingues2021episodic,
  title={Episodic reinforcement learning in finite {MDP}s: Minimax lower bounds revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={578--598},
  year={2021},
  organization={PMLR}
}

@article{jin2023v,
  title={V-learningâ€”a simple, efficient, decentralized algorithm for multiagent reinforcement learning},
  author={Jin, Chi and Liu, Qinghua and Wang, Yuanhao and Yu, Tiancheng},
  journal={Mathematics of Operations Research},
  year={2023},
  publisher={INFORMS}
}

@article{jin2022policy,
  title={Policy learning" without''overlap: Pessimism and generalized empirical Bernstein's inequality},
  author={Jin, Ying and Ren, Zhimei and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2212.09900},
  year={2022}
}

@article{DBLP:journals/ml/Natarajan89,
  author       = {B. K. Natarajan},
  title        = {On Learning Sets and Functions},
  journal      = {Mach. Learn.},
  volume       = {4},
  pages        = {67--97},
  year         = {1989},
  url          = {https://doi.org/10.1007/BF00114804},
  doi          = {10.1007/BF00114804},
  timestamp    = {Mon, 02 Mar 2020 16:29:44 +0100},
  biburl       = {https://dblp.org/rec/journals/ml/Natarajan89.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{brukhim2022characterization,
  title={A characterization of multiclass learnability},
  author={Brukhim, Nataly and Carmon, Daniel and Dinur, Irit and Moran, Shay and Yehudayoff, Amir},
  booktitle={2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={943--955},
  year={2022},
  organization={IEEE}
}
@article{zhan2021policy,
  title={Policy learning with adaptively collected data},
  author={Zhan, Ruohan and Ren, Zhimei and Athey, Susan and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2105.02344},
  year={2021}
}

@inproceedings{heidari2016tight,
  title={Tight Policy Regret Bounds for Improving and Decaying Bandits.},
  author={Heidari, Hoda and Kearns, Michael J and Roth, Aaron},
  booktitle={IJCAI},
  pages={1562--1570},
  year={2016}
}

@article{koren2017multi,
  title={Multi-armed bandits with metric movement costs},
  author={Koren, Tomer and Livni, Roi and Mansour, Yishay},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{koren2017bandits,
  title={Bandits with movement costs and adaptive pricing},
  author={Koren, Tomer and Livni, Roi and Mansour, Yishay},
  booktitle={Conference on Learning Theory},
  pages={1242--1268},
  year={2017},
  organization={PMLR}
}

@inproceedings{dekel2014bandits,
  title={Bandits with switching costs: ${T}^{2/3}$ regret},
  author={Dekel, Ofer and Ding, Jian and Koren, Tomer and Peres, Yuval},
  booktitle={Proceedings of the forty-sixth annual ACM symposium on Theory of computing},
  pages={459--467},
  year={2014}
}

@article{merhav2002sequential,
  title={On sequential strategies for loss functions with memory},
  author={Merhav, Neri and Ordentlich, Erik and Seroussi, Gadiel and Weinberger, Marcelo J},
  journal={IEEE Transactions on Information Theory},
  volume={48},
  number={7},
  pages={1947--1958},
  year={2002},
  publisher={IEEE}
}

@article{arora2018policy,
  title={Policy regret in repeated games},
  author={Arora, Raman and Dinitz, Michael and Marinov, Teodor Vanislavov and Mohri, Mehryar},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{zhou2018offline,
  title={Offline multi-action policy learning: Generalization and optimization},
  author={Zhou, Zhengyuan and Athey, Susan and Wager, Stefan},
  journal={arXiv preprint arXiv:1810.04778},
  year={2018}
}

@inproceedings{audibert2009minimax,
  title={Minimax policies for adversarial and stochastic bandits},
  author={Audibert, Jean-Yves and Bubeck, S{\'e}bastien},
  booktitle={COLT},
  pages={217--226},
  year={2009}
}

@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}

@article{conitzer2002complexity,
  title={Complexity of mechanism design},
  author={Conitzer, Vincent and Sandholm, Tuomas},
  journal={arXiv preprint cs/0205075},
  year={2002}
}

@inproceedings{cole2014sample,
  title={The sample complexity of revenue maximization},
  author={Cole, Richard and Roughgarden, Tim},
  booktitle={Proceedings of the forty-sixth annual ACM symposium on Theory of computing},
  pages={243--252},
  year={2014}
}

@article{wei2020linear,
  title={Linear last-iterate convergence in constrained saddle-point optimization},
  author={Wei, Chen-Yu and Lee, Chung-Wei and Zhang, Mengxiao and Luo, Haipeng},
  journal={arXiv preprint arXiv:2006.09517},
  year={2020}
}

@article{hansen2013strategy,
  title={Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor},
  author={Hansen, Thomas Dueholm and Miltersen, Peter Bro and Zwick, Uri},
  journal={Journal of the ACM (JACM)},
  volume={60},
  number={1},
  pages={1--16},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{hu2003nash,
  title={Nash Q-learning for general-sum stochastic games},
  author={Hu, Junling and Wellman, Michael P},
  journal={Journal of machine learning research},
  volume={4},
  number={Nov},
  pages={1039--1069},
  year={2003}
}

@incollection{littman1994markov,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine learning proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@inproceedings{dutting2019optimal,
  title={Optimal auctions through deep learning},
  author={D{\"u}tting, Paul and Feng, Zhe and Narasimhan, Harikrishna and Parkes, David and Ravindranath, Sai Srivatsa},
  booktitle={International Conference on Machine Learning},
  pages={1706--1715},
  year={2019},
  organization={PMLR}
}

@inproceedings{balcan2005mechanism,
  title={Mechanism design via machine learning},
  author={Balcan, M-F and Blum, Avrim and Hartline, Jason D and Mansour, Yishay},
  booktitle={46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)},
  pages={605--614},
  year={2005},
  organization={IEEE}
}

@article{zheng2020ai,
  title={The {AI} economist: Improving equality and productivity with {AI}-driven tax policies},
  author={Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Naik, Nikhil and Gruesbeck, Melvin and Parkes, David C and Socher, Richard},
  journal={arXiv preprint arXiv:2004.13332},
  year={2020}
}


@inproceedings{tirinzoni2023optimistic,
  title={Optimistic {PAC} reinforcement learning: the instance-dependent view},
  author={Tirinzoni, Andrea and Al-Marjani, Aymen and Kaufmann, Emilie},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1460--1480},
  year={2023},
  organization={PMLR}
}

@inproceedings{qiao2022sample,
  title={Sample-efficient reinforcement learning with loglog (t) switching cost},
  author={Qiao, Dan and Yin, Ming and Min, Ming and Wang, Yu-Xiang},
  booktitle={International Conference on Machine Learning},
  pages={18031--18061},
  year={2022},
  organization={PMLR}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank {MDP}s},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20095--20107},
  year={2020}
}

@article{zhang2006,
  title={From $\epsilon$-entropy to {KL}-entropy: Analysis of minimum information complexity density estimation},
  author={Zhang, Tong},
  year={2006}
}

@inproceedings{bousquet2021theory,
  title={A theory of universal learning},
  author={Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Van Handel, Ramon and Yehudayoff, Amir},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={532--541},
  year={2021}
}

@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={ACM New York, NY, USA}
}

@article{wu2021crop,
  title={Crop: Certifying robust policies for reinforcement learning through functional smoothing},
  author={Wu, Fan and Li, Linyi and Huang, Zijian and Vorobeychik, Yevgeniy and Zhao, Ding and Li, Bo},
  journal={arXiv preprint arXiv:2106.09292},
  year={2021}
}

@article{yang2022rorl,
  title={RORL: Robust Offline Reinforcement Learning via Conservative Smoothing},
  author={Yang, Rui and Bai, Chenjia and Ma, Xiaoteng and Wang, Zhaoran and Zhang, Chongjie and Han, Lei},
  journal={arXiv preprint arXiv:2206.02829},
  year={2022}
}


@INPROCEEDINGS {Littlestone87,
author = {N. Littlestone},
booktitle = {2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
title = {Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm},
year = {1987},
volume = {},
issn = {0272-5428},
pages = {68-77},
keywords = {},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@inproceedings{daskalakis2022fast,
  title={Fast rates for nonparametric online learning: from realizability to learning in games},
  author={Daskalakis, Constantinos and Golowich, Noah},
  booktitle={Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={846--859},
  year={2022}
}

@article{attias2023optimal,
  title={Optimal Learners for Realizable Regression: PAC Learning and Online Learning},
  author={Attias, Idan and Hanneke, Steve and Kalavasis, Alkis and Karbasi, Amin and Velegkas, Grigoris},
  journal={arXiv preprint arXiv:2307.03848},
  year={2023}
}

@article{balcan2023reliable,
  title={Reliable Learning for Test-time Attacks and Distribution Shift},
  author={Balcan, Maria-Florina and Hanneke, Steve and Pukdee, Rattana and Sharma, Dravyansh},
  journal={arXiv preprint arXiv:2304.03370},
  year={2023}
}


@InProceedings{pmlr-v195-hanneke23d,
  title = 	 {Bandit Learnability can be Undecidable},
  author =       {Hanneke, Steve and Yang, Liu},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {5813--5849},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/hanneke23d/hanneke23d.pdf},
  abstract = 	 {We initiate a general investigation into structured bandits. Specifically, for an abstract space $X$, we suppose a true reward function $f$ resides in a known, but arbitrary, function class $F$. The algorithm may then pull a number of arms $x$ (i.e., query for the value $f(x)$), and thereby attempts to identify an arm $\hat{x}$ of near-maximum reward: $f(\hat{x}) \geq \sup_x f(x) - \epsilon$. While special cases of this problem are well understood in the literature, our interest is in the possibility of a fully-general theory of bandit learnability, analogous to the PAC model for classification: that is, a theory which precisely characterizes which function classes $F$ admit a learning algorithm guaranteed to identify a near-optimal arm within a bounded number of pulls.Our main result in this regard is an illuminating impossibility result. Namely, there exist well-defined function classes $F$ such that bandit learnability is \emph{undecidable} within ZFC set theory. While such undecidability results have previously been shown for a certain abstractly-defined learning problem known as EMX, this is the first example of a natural or commonly-encountered learning problem (i.e., bandits) for which learnability can be provably undecidable. Our proof is based on establishing a (rather-sophisticated) equivalence between certain subfamilies of EMX learning problems and corresponding constructed bandit problems.Despite this general undecidability result, we also establish new general results in special cases. Specifically, we characterize the optimal query complexity in the special case of binary-valued reward functions in terms of a combinatorial complexity measure related to the teaching dimension.  We also present an extension to general bounded real-valued rewards, though in this case the upper bound is not always optimal.  We instantiate the new complexity measures for several important families of function classes $F$.}
}
@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{hanneke2019value,
  title={On the value of target data in transfer learning},
  author={Hanneke, Steve and Kpotufe, Samory},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{wang2023oracle,
  title={Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits},
  author={Wang, Lequn and Krishnamurthy, Akshay and Slivkins, Aleksandrs},
  journal={arXiv preprint arXiv:2306.07923},
  year={2023}
}

@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@inproceedings{kong2020sublinear,
  title={Sublinear optimal policy value estimation in contextual bandits},
  author={Kong, Weihao and Brunskill, Emma and Valiant, Gregory},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4377--4387},
  year={2020},
  organization={PMLR}
}

@inproceedings{balcan2022robustly,
  title={Robustly-reliable learners under poisoning attacks},
  author={Balcan, Maria-Florina and Blum, Avrim and Hanneke, Steve and Sharma, Dravyansh},
  booktitle={Conference on Learning Theory},
  pages={4498--4534},
  year={2022},
  organization={PMLR}
}

@article{lee2023estimating,
  title={Estimating Optimal Policy Value in General Linear Contextual Bandits},
  author={Lee, Jonathan N and Kong, Weihao and Pacchiano, Aldo and Muthukumar, Vidya and Brunskill, Emma},
  journal={arXiv preprint arXiv:2302.09451},
  year={2023}
}

@inproceedings{alon2022theory,
  title={A theory of PAC learnability of partial concept classes},
  author={Alon, Noga and Hanneke, Steve and Holzman, Ron and Moran, Shay},
  booktitle={2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={658--671},
  year={2022},
  organization={IEEE}
}

@inproceedings{NEURIPS2020_a822554e,
 author = {Montasser, Omar and Hanneke, Steve and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14626--14637},
 publisher = {Curran Associates, Inc.},
 title = {Reducing Adversarially Robust Learning to Non-Robust PAC Learning},
 volume = {33},
 year = {2020}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@inproceedings{lee2022model,
  title={Model selection in batch policy optimization},
  author={Lee, Jonathan and Tucker, George and Nachum, Ofir and Dai, Bo},
  booktitle={International Conference on Machine Learning},
  pages={12542--12569},
  year={2022},
  organization={PMLR}
}

@article{hanneke2023limits,
  title={Limits of Model Selection under Transfer Learning},
  author={Hanneke, Steve and Kpotufe, Samory and Mahdaviyeh, Yasaman},
  journal={arXiv preprint arXiv:2305.00152},
  year={2023}
}

@article{li2023reward,
  title={Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning},
  author={Li, Gen and Zhan, Wenhao and Lee, Jason D and Chi, Yuejie and Chen, Yuxin},
  journal={arXiv preprint arXiv:2305.10282},
  year={2023}
}

@article{song2022hybrid,
  title={Hybrid rl: Using both offline and online data can make rl efficient},
  author={Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Bagnell, J Andrew and Krishnamurthy, Akshay and Sun, Wen},
  journal={arXiv preprint arXiv:2210.06718},
  year={2022}
}

@article{zhang2023policy,
  title={Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data},
  author={Zhang, Ruiqi and Zanette, Andrea},
  journal={arXiv preprint arXiv:2307.04354},
  year={2023}
}

@article{tang2023efficient,
  title={Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach},
  author={Tang, Dengwang and Jain, Rahul and Hao, Botao and Wen, Zheng},
  journal={arXiv preprint arXiv:2310.11531},
  year={2023}
}

@inproceedings{wagenmaker2023leveraging,
  title={Leveraging offline data in online reinforcement learning},
  author={Wagenmaker, Andrew and Pacchiano, Aldo},
  booktitle={International Conference on Machine Learning},
  pages={35300--35338},
  year={2023},
  organization={PMLR}
}

@inproceedings{agarwal2023provable,
  title={Provable benefits of representational transfer in reinforcement learning},
  author={Agarwal, Alekh and Song, Yuda and Sun, Wen and Wang, Kaiwen and Wang, Mengdi and Zhang, Xuezhou},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2114--2187},
  year={2023},
  organization={PMLR}
}

@article{doi:10.1137/18M1183480,
author = {Udell, Madeleine and Townsend, Alex},
title = {Why Are Big Data Matrices Approximately Low Rank?},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {1},
number = {1},
pages = {144-160},
year = {2019},
doi = {10.1137/18M1183480},
URL = { 
        https://doi.org/10.1137/18M1183480
},
eprint = { 
        https://doi.org/10.1137/18M1183480
}
,
    abstract = { Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention paid to explaining why the low rank structure appears in the first place. Here, we explain the effectiveness of low rank models in data science by considering a simple generative model for these matrices: we suppose that each row or column is associated to a (possibly high dimensional) bounded latent variable, and entries of the matrix are generated by applying a piecewise analytic function to these latent variables. These matrices are in general full rank. However, we show that we can approximate every entry of an \$m\times n\$ matrix drawn from this model to within a fixed absolute error by a low rank matrix whose rank grows as \$\mathcal{O}(\log(m+n))\$. Hence any sufficiently large matrix from such a latent variable model can be approximated, up to a small entrywise error, by a low rank matrix. }
}

@article{kwon2021rl,
  title={{RL} for latent {MDP}s: Regret guarantees and a lower bound},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24523--24534},
  year={2021}
}

@inproceedings{li2023when,
  title={When is Agnostic Reinforcement Learning Statistically Tractable?},
  author={Jia, Zeyu and Li, Gene and Rakhlin, Alexander and Sekhari, Ayush and Srebro, Nathan},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{blum2017collaborative,
  title={Collaborative PAC learning},
  author={Blum, Avrim and Haghtalab, Nika and Procaccia, Ariel D and Qiao, Mingda},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{zhang2023settling,
  title={Settling the sample complexity of online reinforcement learning},
  author={Zhang, Zihan and Chen, Yuxin and Lee, Jason D and Du, Simon S},
  journal={arXiv preprint arXiv:2307.13586},
  year={2023}
}

@article{kaufmann2016complexity,
  title={On the complexity of best-arm identification in multi-armed bandit models},
  author={Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1--42},
  year={2016},
  publisher={JMLR. org}
}

@article{bhatia2020online,
  title={Online learning with dynamics: A minimax perspective},
  author={Bhatia, Kush and Sridharan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15020--15030},
  year={2020}
}

@inproceedings{malik2023weighted,
  title={Weighted tallying bandits: overcoming intractability via repeated exposure optimality},
  author={Malik, Dhruv and Igoe, Conor and Li, Yuanzhi and Singh, Aarti},
  booktitle={International Conference on Machine Learning},
  pages={23590--23609},
  year={2023},
  organization={PMLR}
}

@inproceedings{awasthi2022congested,
  title={Congested bandits: Optimal routing via short-term resets},
  author={Awasthi, Pranjal and Bhatia, Kush and Gollapudi, Sreenivas and Kollias, Kostas},
  booktitle={International Conference on Machine Learning},
  pages={1078--1100},
  year={2022},
  organization={PMLR}
}

@article{lindner2021addressing,
  title={Addressing the long-term impact of ML decisions via policy regret},
  author={Lindner, David and Heidari, Hoda and Krause, Andreas},
  journal={arXiv preprint arXiv:2106.01325},
  year={2021}
}

@inproceedings{seznec2019rotting,
  title={Rotting bandits are no harder than stochastic ones},
  author={Seznec, Julien and Locatelli, Andrea and Carpentier, Alexandra and Lazaric, Alessandro and Valko, Michal},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2564--2572},
  year={2019},
  organization={PMLR}
}

@article{levine2017rotting,
  title={Rotting bandits},
  author={Levine, Nir and Crammer, Koby and Mannor, Shie},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{malik2022complete,
  title={Complete policy regret bounds for tallying bandits},
  author={Malik, Dhruv and Li, Yuanzhi and Singh, Aarti},
  booktitle={Conference on Learning Theory},
  pages={5146--5174},
  year={2022},
  organization={PMLR}
}

@inproceedings{ramponi2022learning,
  title={Learning in {M}arkov Games: can we exploit a general-sum opponent?},
  author={Ramponi, Giorgia and Restelli, Marcello},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1665--1675},
  year={2022},
  organization={PMLR}
}

@article{bai2021sample,
  title={Sample-efficient learning of Stackelberg equilibria in general-sum games},
  author={Bai, Yu and Jin, Chi and Wang, Huan and Xiong, Caiming},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25799--25811},
  year={2021}
}

@article{
doi:10.1073/pnas.36.1.48,
author = {John F. Nash },
title = {Equilibrium points in $n$-person games},
journal = {Proceedings of the National Academy of Sciences},
volume = {36},
number = {1},
pages = {48-49},
year = {1950},
doi = {10.1073/pnas.36.1.48},

eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.36.1.48}}


@article{silver2017mastering,
  title={Mastering the game of {G}o without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}


@article{nguyen2018improved,
  title={Improved algorithms for collaborative PAC learning},
  author={Nguyen, Huy and Zakynthinou, Lydia},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{lu2023reinforcement,
  title={Reinforcement learning, bit by bit},
  author={Lu, Xiuyuan and Van Roy, Benjamin and Dwaracherla, Vikranth and Ibrahimi, Morteza and Osband, Ian and Wen, Zheng and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={16},
  number={6},
  pages={733--865},
  year={2023},
  publisher={Now Publishers, Inc.}
}




@article{lee2022oracle,
  title={Oracle inequalities for model selection in offline reinforcement learning},
  author={Lee, Jonathan N and Tucker, George and Nachum, Ofir and Dai, Bo and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28194--28207},
  year={2022}
}

@article{attias2023adversarially,
  title={Adversarially Robust PAC Learnability of Real-Valued Functions},
  author={Attias, Idan and Hanneke, Steve},
  year={2023}
}


@article{hanneke2022no,
  title={A no-free-lunch theorem for multitask learning},
  author={Hanneke, Steve and Kpotufe, Samory},
  journal={The Annals of Statistics},
  volume={50},
  number={6},
  pages={3119--3143},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{hanneke2015minimax,
  title={Minimax analysis of active learning.},
  author={Hanneke, Steve and Yang, Liu},
  journal={J. Mach. Learn. Res.},
  volume={16},
  number={1},
  pages={3487--3602},
  year={2015}
}

@inproceedings{montasser2019vc,
  title={Vc classes are adversarially robustly learnable, but only improperly},
  author={Montasser, Omar and Hanneke, Steve and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2512--2530},
  year={2019},
  organization={PMLR}
}

@inproceedings{block2021majorizing,
  title={Majorizing measures, sequential complexities, and online learning},
  author={Block, Adam and Dagan, Yuval and Rakhlin, Alexander},
  booktitle={Conference on Learning Theory},
  pages={587--590},
  year={2021},
  organization={PMLR}
}

@article{rakhlin2015online,
  title={Online learning via sequential complexities.},
  author={Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
  journal={J. Mach. Learn. Res.},
  volume={16},
  number={1},
  pages={155--186},
  year={2015}
}

@article{rakhlin2015sequential,
  title={Sequential complexities and uniform martingale laws of large numbers},
  author={Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
  journal={Probability theory and related fields},
  volume={161},
  pages={111--153},
  year={2015},
  publisher={Springer}
}

@inproceedings{rakhlin2014online,
  title={Online non-parametric regression},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Conference on Learning Theory},
  pages={1232--1264},
  year={2014},
  organization={PMLR}
}

@article{rakhlin2010online,
  title={Online learning: Random averages, combinatorial parameters, and learnability},
  author={Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}

@article{raman2023characterization,
  title={A Characterization of Online Multiclass Learnability},
  author={Raman, Vinod and Subedi, Unique and Tewari, Ambuj},
  journal={arXiv preprint arXiv:2303.17716},
  year={2023}
}

@article{daniely2015multiclass,
  title={Multiclass learnability and the ERM principle.},
  author={Daniely, Amit and Sabato, Sivan and Ben-David, Shai and Shalev-Shwartz, Shai},
  journal={J. Mach. Learn. Res.},
  volume={16},
  number={1},
  pages={2377--2404},
  year={2015}
}

@article{raman2023multiclass,
  title={Multiclass Online Learnability under Bandit Feedback},
  author={Raman, Ananth and Raman, Vinod and Subedi, Unique and Tewari, Ambuj},
  journal={arXiv preprint arXiv:2308.04620},
  year={2023}
}

@inproceedings{hanneke2023multiclass,
  title={Multiclass online learning and uniform convergence},
  author={Hanneke, Steve and Moran, Shay and Raman, Vinod and Subedi, Unique and Tewari, Ambuj},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={5682--5696},
  year={2023},
  organization={PMLR}
}

@inproceedings{alon2021adversarial,
  title={Adversarial laws of large numbers and optimal regret in online classification},
  author={Alon, Noga and Ben-Eliezer, Omri and Dagan, Yuval and Moran, Shay and Naor, Moni and Yogev, Eylon},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={447--455},
  year={2021}
}

@inproceedings{ben2009agnostic,
  title={Agnostic Online Learning.},
  author={Ben-David, Shai and P{\'a}l, D{\'a}vid and Shalev-Shwartz, Shai},
  booktitle={COLT},
  volume={3},
  pages={1},
  year={2009}
}

@article{wu2022power,
  title={The power and limitation of pretraining-finetuning for linear regression under covariate shift},
  author={Wu, Jingfeng and Zou, Difan and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33041--33053},
  year={2022}
}

@article{zhang2020robust,
  title={Robust deep reinforcement learning against adversarial perturbations on state observations},
  author={Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21024--21037},
  year={2020}
}

@article{wu2022copa,
  title={COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks},
  author={Wu, Fan and Li, Linyi and Xu, Chejian and Zhang, Huan and Kailkhura, Bhavya and Kenthapadi, Krishnaram and Zhao, Ding and Li, Bo},
  journal={arXiv preprint arXiv:2203.08398},
  year={2022}
}

@article{tnt2023-offlinerl,
  title={On The Theory of Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond},
  author={Nguyen-Tang, Thanh and Arora, Raman},
  journal={preprint},
  year={2023}
}

@article{zhang2023optimal,
  title={Optimal Multi-Distribution Learning},
  author={Zhang, Zihan and Zhan, Wenhao and Chen, Yuxin and Du, Simon S and Lee, Jason D},
  journal={arXiv preprint arXiv:2312.05134},
  year={2023}
}

@article{foster2021efficient,
  title={Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination},
  author={Foster, Dylan J and Krishnamurthy, Akshay},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18907--18919},
  year={2021}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@article{agarwal2021theory,
  title={On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@article{david2016statistical,
  title={On statistical learning via the lens of compression},
  author={David, Ofir and Moran, Shay and Yehudayoff, Amir},
  journal={arXiv preprint arXiv:1610.03592},
  year={2016}
}

@article{chen2022byzantine,
  title={Byzantine-Robust Online and Offline Distributed Reinforcement Learning},
  author={Chen, Yiding and Zhang, Xuezhou and Zhang, Kaiqing and Wang, Mengdi and Zhu, Xiaojin},
  journal={arXiv preprint arXiv:2206.00165},
  year={2022}
}

@inproceedings{lykouris2021corruption,
  title={Corruption-robust exploration in episodic reinforcement learning},
  author={Lykouris, Thodoris and Simchowitz, Max and Slivkins, Alex and Sun, Wen},
  booktitle={Conference on Learning Theory},
  pages={3242--3245},
  year={2021},
  organization={PMLR}
}

@article{wang2022policy,
  title={Policy Gradient Method For Robust Reinforcement Learning},
  author={Wang, Yue and Zou, Shaofeng},
  journal={arXiv preprint arXiv:2205.07344},
  year={2022}
}

@article{ma2022distributionally,
  title={Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation},
  author={Ma, Xiaoteng and Liang, Zhipeng and Xia, Li and Zhang, Jiheng and Blanchet, Jose and Liu, Mingwen and Zhao, Qianchuan and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2209.06620},
  year={2022}
}

@article{shi2022distributionally,
  title={Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity},
  author={Shi, Laixi and Chi, Yuejie},
  journal={arXiv preprint arXiv:2208.05767},
  year={2022}
}

@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13626--13640},
  year={2021}
}

@article{cheng2022adversarially,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2202.02446},
  year={2022}
}

@article{rigter2022rambo,
  title={Rambo-rl: Robust adversarial model-based offline reinforcement learning},
  author={Rigter, Marc and Lacerda, Bruno and Hawes, Nick},
  journal={arXiv preprint arXiv:2204.12581},
  year={2022}
}

@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={2817--2826},
  year={2017},
  organization={PMLR}
}

@inproceedings{zhang2022corruption,
  title={Corruption-robust offline reinforcement learning},
  author={Zhang, Xuezhou and Chen, Yiding and Zhu, Xiaojin and Sun, Wen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5757--5773},
  year={2022},
  organization={PMLR}
}

@article{hu2013kullback,
  title={Kullback-Leibler divergence constrained distributionally robust optimization},
  author={Hu, Zhaolin and Hong, L Jeff},
  journal={Available at Optimization Online},
  pages={1695--1724},
  year={2013}
}

@article{derman2021twice,
  title={Twice regularized MDPs and the equivalence between robustness and regularization},
  author={Derman, Esther and Geist, Matthieu and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22274--22287},
  year={2021}
}

@inproceedings{liu2023optimistic,
  title={Optimistic {MLE}: A generic model-based algorithm for partially observable sequential decision making},
  author={Liu, Qinghua and Netrapalli, Praneeth and Szepesvari, Csaba and Jin, Chi},
  booktitle={Proceedings of the 55th Annual ACM Symposium on Theory of Computing},
  pages={363--376},
  year={2023}
}

@book{geer2000empirical,
  title={Empirical Processes in M-estimation},
  author={Geer, Sara A},
  volume={6},
  year={2000},
  publisher={Cambridge university press}
}

@inproceedings{DBLP:conf/colt/AgarwalKLM20,
  author    = {Alekh Agarwal and
               Sham M. Kakade and
               Jason D. Lee and
               Gaurav Mahajan},
  editor    = {Jacob D. Abernethy and
               Shivani Agarwal},
  title     = {Optimality and Approximation with Policy Gradient Methods in Markov
               Decision Processes},
  booktitle = {Conference on Learning Theory, {COLT} 2020, 9-12 July 2020, Virtual
               Event [Graz, Austria]},
  series    = {Proceedings of Machine Learning Research},
  volume    = {125},
  pages     = {64--66},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v125/agarwal20a.html},
  timestamp = {Fri, 27 Nov 2020 16:13:27 +0100},
  biburl    = {https://dblp.org/rec/conf/colt/AgarwalKLM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{haghtalab2022demand,
  title={On-demand sampling: Learning optimally from multiple distributions},
  author={Haghtalab, Nika and Jordan, Michael and Zhao, Eric},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={406--419},
  year={2022}
}

@inproceedings{DBLP:conf/cdc/KhodadadianJVM21,
  author    = {Sajad Khodadadian and
               Prakirt Raj Jhunjhunwala and
               Sushil Mahavir Varma and
               Siva Theja Maguluri},
  title     = {On the Linear Convergence of Natural Policy Gradient Algorithm},
  booktitle = {2021 60th {IEEE} Conference on Decision and Control (CDC), Austin,
               TX, USA, December 14-17, 2021},
  pages     = {3794--3799},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/CDC45484.2021.9682908},
  doi       = {10.1109/CDC45484.2021.9682908},
  timestamp = {Tue, 17 May 2022 15:53:17 +0200},
  biburl    = {https://dblp.org/rec/conf/cdc/KhodadadianJVM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{awasthi2021existence,
  title={On the existence of the adversarial bayes classifier (extended version)},
  author={Awasthi, Pranjal and Frank, Natalie S and Mohri, Mehryar},
  journal={arXiv preprint arXiv:2112.01694},
  year={2021}
}

@article{DBLP:journals/corr/abs-2102-00135,
  author    = {Guanghui Lan},
  title     = {Policy Mirror Descent for Reinforcement Learning: Linear Convergence,
               New Sampling Complexity, and Generalized Problem Classes},
  journal   = {CoRR},
  volume    = {abs/2102.00135},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.00135},
  eprinttype = {arXiv},
  eprint    = {2102.00135},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-00135.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2007-11120,
  author    = {Jalaj Bhandari and
               Daniel Russo},
  title     = {A Note on the Linear Convergence of Policy Gradient Methods},
  journal   = {CoRR},
  volume    = {abs/2007.11120},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.11120},
  eprinttype = {arXiv},
  eprint    = {2007.11120},
  timestamp = {Tue, 28 Jul 2020 14:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-11120.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}