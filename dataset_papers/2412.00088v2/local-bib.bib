@InProceedings{skorski21_moder_analy_hutch_trace_estim,
  author          = {Skorski, Maciej},
  title           = {Modern Analysis of Hutchinson's Trace Estimator},
  booktitle       = {2021 55th Annual Conference on Information Sciences and
                  Systems (CISS)},
  year            = 2021,
  doi             = {10.1109/ciss50987.2021.9400306},
  url             = {http://dx.doi.org/10.1109/CISS50987.2021.9400306},
  month           = mar,
  publisher       = {IEEE},
}

@article{yu22_gradien_enhan_physic_infor_neural,
  author          = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis,
                  George Em},
  title           = {Gradient-Enhanced Physics-Informed Neural Networks for
                  Forward and Inverse {PDE} Problems},
  journal         = {Computer Methods in Applied Mechanics and Engineering},
  volume          = 393,
  pages           = 114823,
  year            = 2022,
  doi             = {10.1016/j.cma.2022.114823},
  url             = {http://arxiv.org/abs/2111.02801},
  abstract        = {Deep learning has been shown to be an effective tool in
                  solving partial differential equations (PDEs) through
                  physics-informed neural networks (PINNs). PINNs embed the PDE
                  residual into the loss function of the neural network, and
                  have been successfully employed to solve diverse forward and
                  inverse PDE problems. However, one disadvantage of the first
                  generation of PINNs is that they usually have limited accuracy
                  even with many training points. Here, we propose a new method,
                  gradient-enhanced physics-informed neural networks (gPINNs),
                  for improving the accuracy and training efficiency of PINNs.
                  gPINNs leverage gradient information of the PDE residual and
                  embed the gradient into the loss function. We tested gPINNs
                  extensively and demonstrated the effectiveness of gPINNs in
                  both forward and inverse PDE problems. Our numerical results
                  show that gPINN performs better than PINN with fewer training
                  points. Furthermore, we combined gPINN with the method of
                  residual-based adaptive refinement (RAR), a method for
                  improving the distribution of training points adaptively
                  during training, to further improve the performance of gPINN,
                  especially in PDEs with solutions that have steep gradients.},
  issn            = 00457825,
  keywords        = {Computer Science - Machine Learning, Physics -
                  Computational Physics},
  month           = apr,
  note            = {arXiv:2111.02801 [physics]},
  urldate         = {2023-09-10},
}

@misc{pu24_lax,
  abstract        = {Lax pairs are one of the most important features of
                  integrable system. In this work, we propose the Lax pairs
                  informed neural networks (LPNNs) tailored for the integrable
                  systems with Lax pairs by designing novel network
                  architectures and loss functions, comprising LPNN-v1 and
                  LPNN-v2. The most noteworthy advantage of LPNN-v1 is that it
                  can transform the solving of nonlinear integrable systems into
                  the solving of a linear Lax pairs spectral problems, and it
                  not only efficiently solves data-driven localized wave
                  solutions, but also obtains spectral parameter and
                  corresponding spectral function in Lax pairs spectral problems
                  of the integrable systems. On the basis of LPNN-v1, we
                  additionally incorporate the compatibility condition/zero
                  curvature equation of Lax pairs in LPNN-v2, its major
                  advantage is the ability to solve and explore high-accuracy
                  data-driven localized wave solutions and associated spectral
                  problems for integrable systems with Lax pairs. The numerical
                  experiments focus on studying abundant localized wave
                  solutions for very important and representative integrable
                  systems with Lax pairs spectral problems, including the
                  soliton solution of the Korteweg-de Vries (KdV) euqation and
                  modified KdV equation, rogue wave solution of the nonlinear
                  Schr{\textbackslash}"odinger equation, kink solution of the
                  sine-Gordon equation, non-smooth peakon solution of the
                  Camassa-Holm equation and pulse solution of the short pulse
                  equation, as well as the line-soliton solution of
                  Kadomtsev-Petviashvili equation and lump solution of
                  high-dimensional KdV equation. The innovation of this work
                  lies in the pioneering integration of Lax pairs informed of
                  integrable systems into deep neural networks, thereby
                  presenting a fresh methodology and pathway for investigating
                  data-driven localized wave solutions and Lax pairs spectral
                  problems.},
  author          = {Pu, Juncai and Chen, Yong},
  doi             = {10.48550/arXiv.2401.04982},
  keywords        = {Nonlinear Sciences - Exactly Solvable and Integrable
                  Systems},
  month           = jan,
  note            = {arXiv:2401.04982 [nlin]},
  publisher       = {arXiv},
  title           = {Lax pairs informed neural networks solving integrable
                  systems},
  url             = {http://arxiv.org/abs/2401.04982},
  urldate         = {2024-02-28},
  year            = 2024,
}

@misc{hutzenthaler18_overc,
  abstract        = {For a long time it is well-known that high-dimensional
                  linear parabolic partial differential equations (PDEs) can be
                  approximated by Monte Carlo methods with a computational
                  effort which grows polynomially both in the dimension and in
                  the reciprocal of the prescribed accuracy. In other words,
                  linear PDEs do not suffer from the curse of dimensionality.
                  For general semilinear PDEs with Lipschitz coefficients,
                  however, it remained an open question whether these suffer
                  from the curse of dimensionality. In this paper we partially
                  solve this open problem. More precisely, we prove in the case
                  of semilinear heat equations with gradient-independent and
                  globally Lipschitz continuous nonlinearities that the
                  computational effort of a variant of the recently introduced
                  multilevel Picard approximations grows polynomially both in
                  the dimension and in the reciprocal of the required accuracy.},
  author          = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas
                  and Nguyen, Tuan Anh and von Wurstemberger, Philippe},
  doi             = {10.1098/rspa.2019.0630},
  journal         = {arXiv.org},
  language        = {en},
  month           = jul,
  title           = {Overcoming the curse of dimensionality in the numerical
                  approximation of semilinear parabolic partial differential
                  equations},
  url             = {https://arxiv.org/abs/1807.01212v3},
  urldate         = {2024-01-05},
  year            = 2018,
}

@article{becker20_numer_simul_full_histor_recur,
  author          = {Becker, Sebastian and Braunwarth, Ramon and Hutzenthaler,
                  Martin and Jentzen, Arnulf and von Wurstemberger, Philippe},
  title           = {Numerical Simulations for Full History Recursive Multilevel
                  {Picard} Approximations for Systems of High-Dimensional
                  Partial Differential Equations},
  journal         = {Communications in Computational Physics},
  volume          = 28,
  number          = 5,
  pages           = {2109--2138},
  year            = 2020,
  doi             = {10.4208/cicp.OA-2020-0130},
  url             = {http://arxiv.org/abs/2005.10206},
  abstract        = {One of the most challenging issues in applied mathematics
                  is to develop and analyze algorithms which are able to
                  approximately compute solutions of high-dimensional nonlinear
                  partial differential equations (PDEs). In particular, it is
                  very hard to develop approximation algorithms which do not
                  suffer under the curse of dimensionality in the sense that the
                  number of computational operations needed by the algorithm to
                  compute an approximation of accuracy \${\textbackslash}epsilon
                  {\textgreater} 0\$ grows at most polynomially in both the
                  reciprocal \$1/{\textbackslash}epsilon\$ of the required
                  accuracy and the dimension \$d {\textbackslash}in
                  {\textbackslash}mathbb\{N\}\$ of the PDE. Recently, a new
                  approximation method, the so-called full history recursive
                  multilevel Picard (MLP) approximation method, has been
                  introduced and, until today, this approximation scheme is the
                  only approximation method in the scientific literature which
                  has been proven to overcome the curse of dimensionality in the
                  numerical approximation of semilinear PDEs with general time
                  horizons. It is a key contribution of this article to extend
                  the MLP approximation method to systems of semilinear PDEs and
                  to numerically test it on several example PDEs. More
                  specifically, we apply the proposed MLP approximation method
                  in the case of Allen-Cahn PDEs, Sine-Gordon-type PDEs, systems
                  of coupled semilinear heat PDEs, and semilinear Black-Scholes
                  PDEs in up to 1000 dimensions. The presented numerical
                  simulation results suggest in the case of each of these
                  example PDEs that the proposed MLP approximation method
                  produces very accurate results in short runtimes and, in
                  particular, the presented numerical simulation results
                  indicate that the proposed MLP approximation scheme
                  significantly outperforms certain deep learning based
                  approximation methods for high-dimensional semilinear PDEs.},
  issn            = {1815-2406, 1991-7120},
  keywords        = {Mathematics - Numerical Analysis, 65M75, G.1.8},
  month           = jun,
  note            = {arXiv:2005.10206 [cs, math]},
  urldate         = {2024-01-05},
}

@inproceedings{kingma15_adam,
  author          = {Diederik P. Kingma and Jimmy Ba},
  title           = {Adam: {A} Method for Stochastic Optimization},
  booktitle       = {3rd International Conference on Learning Representations,
                  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference
                  Track Proceedings},
  year            = 2015,
  url             = {http://arxiv.org/abs/1412.6980},
  bibsource       = {dblp computer science bibliography, https://dblp.org},
  biburl          = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  editor          = {Yoshua Bengio and Yann LeCun},
  timestamp       = {Thu, 25 Jul 2019 01:00:00 +0200},
}

@book{griewank08_evaluat_deriv,
  author          = {Griewank, Andreas and Walther, Andrea},
  title           = {Evaluating Derivatives},
  year            = 2008,
  publisher       = {Society for Industrial and Applied Mathematics},
  URL             = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
  doi             = {10.1137/1.9780898717761},
  edition         = {Second},
  eprint          = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717761},
}

@article{sirignano18_dgm,
  author          = {Sirignano, Justin and Spiliopoulos, Konstantinos},
  title           = {Dgm: a Deep Learning Algorithm for Solving Partial
                  Differential Equations},
  journal         = {Journal of computational physics},
  volume          = 375,
  pages           = {1339--1364},
  year            = 2018,
  publisher       = {Elsevier},
}

@Article{han18_solvin_high_dimen_partial_differ,
  author          = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
  title           = {Solving High-Dimensional Partial Differential Equations
                  Using Deep Learning},
  journal         = {Proceedings of the National Academy of Sciences},
  volume          = 115,
  number          = 34,
  pages           = {8505-8510},
  year            = 2018,
  doi             = {10.1073/pnas.1718942115},
  url             = {http://dx.doi.org/10.1073/pnas.1718942115},
  issn            = {1091-6490},
  month           = {Aug},
  publisher       = {Proceedings of the National Academy of Sciences},
}

@misc{raissi18_forwar_backw_stoch_neural_networ,
  abstract        = {Classical numerical methods for solving partial
                  differential equations suffer from the curse dimensionality
                  mainly due to their reliance on meticulously generated
                  spatio-temporal grids. Inspired by modern deep learning based
                  techniques for solving forward and inverse problems associated
                  with partial differential equations, we circumvent the tyranny
                  of numerical discretization by devising an algorithm that is
                  scalable to high-dimensions. In particular, we approximate the
                  unknown solution by a deep neural network which essentially
                  enables us to benefit from the merits of automatic
                  differentiation. To train the aforementioned neural network we
                  leverage the well-known connection between high-dimensional
                  partial differential equations and forward-backward stochastic
                  differential equations. In fact, independent realizations of a
                  standard Brownian motion will act as training data. We test
                  the effectiveness of our approach for a couple of benchmark
                  problems spanning a number of scientific domains including
                  Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman
                  equations, both in 100-dimensions.},
  author          = {Raissi, Maziar},
  doi             = {10.48550/arXiv.1804.07010},
  keywords        = {Statistics - Machine Learning, Computer Science - Machine
                  Learning, Electrical Engineering and Systems Science - Systems
                  and Control, Mathematics - Analysis of PDEs, Mathematics -
                  Optimization and Control},
  month           = apr,
  note            = {arXiv:1804.07010 [cs, math, stat]},
  publisher       = {arXiv},
  shorttitle      = {Forward-{Backward} {Stochastic} {Neural} {Networks}},
  title           = {Forward-{Backward} {Stochastic} {Neural} {Networks}: {Deep}
                  {Learning} of {High}-dimensional {Partial} {Differential}
                  {Equations}},
  url             = {http://arxiv.org/abs/1804.07010},
  urldate         = {2024-01-05},
  year            = 2018,
}

@article{beck21_deep_split_method_parab_pdes,
  author          = {Beck, Christian and Becker, Sebastian and Cheridito,
                  Patrick and Jentzen, Arnulf and Neufeld, Ariel},
  title           = {Deep Splitting Method for Parabolic {PDEs}},
  journal         = {SIAM Journal on Scientific Computing},
  volume          = 43,
  number          = 5,
  pages           = {A3135--A3154},
  year            = 2021,
  doi             = {10.1137/19M1297919},
  url             = {http://arxiv.org/abs/1907.03452},
  abstract        = {In this paper we introduce a numerical method for nonlinear
                  parabolic PDEs that combines operator splitting with deep
                  learning. It divides the PDE approximation problem into a
                  sequence of separate learning problems. Since the
                  computational graph for each of the subproblems is
                  comparatively small, the approach can handle extremely
                  high-dimensional PDEs. We test the method on different
                  examples from physics, stochastic control and mathematical
                  finance. In all cases, it yields very good results in up to
                  10,000 dimensions with short run times.},
  issn            = {1064-8275, 1095-7197},
  keywords        = {Mathematics - Numerical Analysis, Computer Science -
                  Machine Learning, Mathematics - Probability, Statistics -
                  Machine Learning, 35K15, 65C05, 65M22, 65M75, 91G20, 93E20},
  month           = jan,
  note            = {arXiv:1907.03452 [cs, math, stat]},
  urldate         = {2023-12-20},
}

@article{zang20_weak_adver_networ_high_partial_differ_equat,
  author          = {Zang, Yaohua and Bao, Gang and Ye, Xiaojing and Zhou,
                  Haomin},
  title           = {Weak {Adversarial} {Networks} for {High}-dimensional
                  {Partial} {Differential} {Equations}},
  journal         = {Journal of Computational Physics},
  volume          = 411,
  pages           = 109409,
  year            = 2020,
  doi             = {10.1016/j.jcp.2020.109409},
  url             = {http://arxiv.org/abs/1907.08272},
  abstract        = {Solving general high-dimensional partial differential
                  equations (PDE) is a long-standing challenge in numerical
                  mathematics. In this paper, we propose a novel approach to
                  solve high-dimensional linear and nonlinear PDEs defined on
                  arbitrary domains by leveraging their weak formulations. We
                  convert the problem of finding the weak solution of PDEs into
                  an operator norm minimization problem induced from the weak
                  formulation. The weak solution and the test function in the
                  weak formulation are then parameterized as the primal and
                  adversarial networks respectively, which are alternately
                  updated to approximate the optimal network parameter setting.
                  Our approach, termed as the weak adversarial network (WAN), is
                  fast, stable, and completely mesh-free, which is particularly
                  suitable for high-dimensional PDEs defined on irregular
                  domains where the classical numerical methods based on finite
                  differences and finite elements suffer the issues of slow
                  computation, instability and the curse of dimensionality. We
                  apply our method to a variety of test problems with
                  high-dimensional PDEs to demonstrate its promising
                  performance.},
  issn            = 00219991,
  keywords        = {Mathematics - Numerical Analysis},
  month           = jun,
  note            = {arXiv:1907.08272 [cs, math]},
  urldate         = {2024-04-12},
}

@article{weinan17_deep_ritz_method,
  author          = {E Weinan and Ting Yu},
  title           = {The Deep Ritz Method: a Deep Learning-Based Numerical
                  Algorithm for Solving Variational Problems},
  journal         = {Communications in Mathematics and Statistics},
  volume          = 6,
  pages           = {1 - 12},
  year            = 2017,
  url             = {https://api.semanticscholar.org/CorpusID:2988078},
}

@article{lu21_physic_infor_neural_networ_with,
  author          = {Lu, Lu and Pestourie, Rapha\"{e}l and Yao, Wenjie and Wang,
                  Zhicheng and Verdugo, Francesc and Johnson, Steven G.},
  title           = {Physics-Informed Neural Networks With Hard Constraints for
                  Inverse Design},
  journal         = {SIAM Journal on Scientific Computing},
  volume          = 43,
  number          = 6,
  pages           = {B1105-B1132},
  year            = 2021,
  doi             = {10.1137/21M1397908},
  URL             = { https://doi.org/10.1137/21M1397908 },
  abstract        = { Inverse design arises in a variety of areas in engineering
                  such as acoustic, mechanics, thermal/electronic transport,
                  electromagnetism, and optics. Topology optimization is an
                  important form of inverse design, where one optimizes a
                  designed geometry to achieve targeted properties parameterized
                  by the materials at every point in a design region. This
                  optimization is challenging, because it has a very high
                  dimensionality and is usually constrained by partial
                  differential equations (PDEs) and additional inequalities.
                  Here, we propose a new deep learning method---physics-informed
                  neural networks with hard constraints (hPINNs)---for solving
                  topology optimization. hPINN leverages the recent development
                  of PINNs for solving PDEs, and thus does not require a large
                  dataset (generated by numerical PDE solvers) for training.
                  However, all the constraints in PINNs are soft constraints,
                  and hence we impose hard constraints by using the penalty
                  method and the augmented Lagrangian method. We demonstrate the
                  effectiveness of hPINN for a holography problem in optics and
                  a fluid problem of Stokes flow. We achieve the same objective
                  as conventional PDE-constrained optimization methods based on
                  adjoint methods and numerical PDE solvers, but find that the
                  design obtained from hPINN is often smoother for problems
                  whose solution is not unique. Moreover, the implementation of
                  inverse design with hPINN can be easier than that of
                  conventional methods because it exploits the extensive
                  deep-learning software infrastructure. },
  eprint          = { https://doi.org/10.1137/21M1397908 },
}

@misc{song21_score_based_gener_model_stoch_differ_equat,
  abstract        = {Creating noise from data is easy; creating data from noise
                  is generative modeling. We present a stochastic differential
                  equation (SDE) that smoothly transforms a complex data
                  distribution to a known prior distribution by slowly injecting
                  noise, and a corresponding reverse-time SDE that transforms
                  the prior distribution back into the data distribution by
                  slowly removing the noise. Crucially, the reverse-time SDE
                  depends only on the time-dependent gradient field
                  ({\textbackslash}aka, score) of the perturbed data
                  distribution. By leveraging advances in score-based generative
                  modeling, we can accurately estimate these scores with neural
                  networks, and use numerical SDE solvers to generate samples.
                  We show that this framework encapsulates previous approaches
                  in score-based generative modeling and diffusion probabilistic
                  modeling, allowing for new sampling procedures and new
                  modeling capabilities. In particular, we introduce a
                  predictor-corrector framework to correct errors in the
                  evolution of the discretized reverse-time SDE. We also derive
                  an equivalent neural ODE that samples from the same
                  distribution as the SDE, but additionally enables exact
                  likelihood computation, and improved sampling efficiency. In
                  addition, we provide a new way to solve inverse problems with
                  score-based models, as demonstrated with experiments on
                  class-conditional generation, image inpainting, and
                  colorization. Combined with multiple architectural
                  improvements, we achieve record-breaking performance for
                  unconditional image generation on CIFAR-10 with an Inception
                  score of 9.89 and FID of 2.20, a competitive likelihood of
                  2.99 bits/dim, and demonstrate high fidelity generation of
                  1024 x 1024 images for the first time from a score-based
                  generative model.},
  author          = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik
                  P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  doi             = {10.48550/arXiv.2011.13456},
  keywords        = {Computer Science - Machine Learning, Statistics - Machine
                  Learning},
  month           = feb,
  note            = {arXiv:2011.13456 [cs, stat]},
  publisher       = {arXiv},
  title           = {Score-{Based} {Generative} {Modeling} through {Stochastic}
                  {Differential} {Equations}},
  url             = {http://arxiv.org/abs/2011.13456},
  urldate         = {2023-05-14},
  year            = 2021,
}

@misc{malladi24_fine_tunin_languag_model_just_forwar_passes,
  abstract        = {Fine-tuning language models (LMs) has yielded success on
                  diverse downstream tasks, but as LMs grow in size,
                  backpropagation requires a prohibitively large amount of
                  memory. Zeroth-order (ZO) methods can in principle estimate
                  gradients using only two forward passes but are theorized to
                  be catastrophically slow for optimizing large models. In this
                  work, we propose a memory-efficient zerothorder optimizer
                  (MeZO), adapting the classical ZO-SGD method to operate
                  in-place, thereby fine-tuning LMs with the same memory
                  footprint as inference. For example, with a single A100 80GB
                  GPU, MeZO can train a 30-billion parameter model, whereas
                  fine-tuning with backpropagation can train only a 2.7B LM with
                  the same budget. We conduct comprehensive experiments across
                  model types (masked and autoregressive LMs), model scales (up
                  to 66B), and downstream tasks (classification,
                  multiple-choice, and generation). Our results demonstrate that
                  (1) MeZO significantly outperforms in-context learning and
                  linear probing; (2) MeZO achieves comparable performance to
                  fine-tuning with backpropagation across multiple tasks, with
                  up to 12x memory reduction and up to 2x GPU-hour reduction in
                  our implementation; (3) MeZO is compatible with both
                  full-parameter and parameter-efficient tuning techniques such
                  as LoRA and prefix tuning; (4) MeZO can effectively optimize
                  non-differentiable objectives (e.g., maximizing accuracy or
                  F1). We support our empirical findings with theoretical
                  insights, highlighting how adequate pre-training and task
                  prompts enable MeZO to fine-tune huge models, despite
                  classical ZO analyses suggesting otherwise.},
  author          = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and
                  Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora,
                  Sanjeev},
  doi             = {10.48550/arXiv.2305.17333},
  keywords        = {Computer Science - Machine Learning, Computer Science -
                  Computation and Language},
  month           = jan,
  note            = {arXiv:2305.17333 [cs]},
  publisher       = {arXiv},
  title           = {Fine-{Tuning} {Language} {Models} with {Just} {Forward}
                  {Passes}},
  url             = {http://arxiv.org/abs/2305.17333},
  urldate         = {2024-01-19},
  year            = 2024,
}

@misc{liu20_primer_zerot_order_optim_signal,
  abstract        = {Zeroth-order (ZO) optimization is a subset of gradient-free
                  optimization that emerges in many signal processing and
                  machine learning applications. It is used for solving
                  optimization problems similarly to gradient-based methods.
                  However, it does not require the gradient, using only function
                  evaluations. Specifically, ZO optimization iteratively
                  performs three major steps: gradient estimation, descent
                  direction computation, and solution update. In this paper, we
                  provide a comprehensive review of ZO optimization, with an
                  emphasis on showing the underlying intuition, optimization
                  principles and recent advances in convergence analysis.
                  Moreover, we demonstrate promising applications of ZO
                  optimization, such as evaluating robustness and generating
                  explanations from black-box deep learning models, and
                  efficient online sensor management.},
  author          = {Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and
                  Zhang, Gaoyuan and Hero, Alfred and Varshney, Pramod K.},
  doi             = {10.48550/arXiv.2006.06224},
  keywords        = {Computer Science - Machine Learning, Electrical Engineering
                  and Systems Science - Signal Processing, Statistics - Machine
                  Learning},
  month           = jun,
  note            = {arXiv:2006.06224 [cs, eess, stat]},
  publisher       = {arXiv},
  title           = {A {Primer} on {Zeroth}-{Order} {Optimization} in {Signal}
                  {Processing} and {Machine} {Learning}},
  url             = {http://arxiv.org/abs/2006.06224},
  urldate         = {2023-12-02},
  year            = 2020,
}

@misc{song19_sliced_score_match,
  abstract        = {Score matching is a popular method for estimating
                  unnormalized statistical models. However, it has been so far
                  limited to simple, shallow models or low-dimensional data, due
                  to the difficulty of computing the Hessian of log-density
                  functions. We show this difficulty can be mitigated by
                  projecting the scores onto random vectors before comparing
                  them. This objective, called sliced score matching, only
                  involves Hessian-vector products, which can be easily
                  implemented using reverse-mode automatic differentiation.
                  Therefore, sliced score matching is amenable to more complex
                  models and higher dimensional data compared to score matching.
                  Theoretically, we prove the consistency and asymptotic
                  normality of sliced score matching estimators. Moreover, we
                  demonstrate that sliced score matching can be used to learn
                  deep score estimators for implicit distributions. In our
                  experiments, we show sliced score matching can learn deep
                  energy-based models effectively, and can produce accurate
                  score estimates for applications such as variational inference
                  with implicit distributions and training Wasserstein
                  Auto-Encoders.},
  author          = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon,
                  Stefano},
  doi             = {10.48550/arXiv.1905.07088},
  keywords        = {Computer Science - Machine Learning, Statistics - Machine
                  Learning},
  month           = jun,
  note            = {arXiv:1905.07088 [cs, stat]},
  publisher       = {arXiv},
  shorttitle      = {Sliced {Score} {Matching}},
  title           = {Sliced {Score} {Matching}: {A} {Scalable} {Approach} to
                  {Density} and {Score} {Estimation}},
  url             = {http://arxiv.org/abs/1905.07088},
  urldate         = {2023-12-14},
  year            = 2019,
}

@misc{amos23_tutor,
  abstract        = {Optimization is a ubiquitous modeling tool and is often
                  deployed in settings which repeatedly solve similar instances
                  of the same problem. Amortized optimization methods use
                  learning to predict the solutions to problems in these
                  settings, exploiting the shared structure between similar
                  problem instances. These methods have been crucial in
                  variational inference and reinforcement learning and are
                  capable of solving optimization problems many orders of
                  magnitudes times faster than traditional optimization methods
                  that do not use amortization. This tutorial presents an
                  introduction to the amortized optimization foundations behind
                  these advancements and overviews their applications in
                  variational inference, sparse coding, gradient-based
                  meta-learning, control, reinforcement learning, convex
                  optimization, optimal transport, and deep equilibrium
                  networks. The source code for this tutorial is available at
                  https://github.com/facebookresearch/amortized-optimization-tutorial.},
  author          = {Amos, Brandon},
  doi             = {10.48550/arXiv.2202.00665},
  keywords        = {Computer Science - Machine Learning, Computer Science -
                  Artificial Intelligence, Mathematics - Optimization and
                  Control},
  month           = apr,
  note            = {arXiv:2202.00665 [cs, math]},
  publisher       = {arXiv},
  title           = {Tutorial on amortized optimization},
  url             = {http://arxiv.org/abs/2202.00665},
  urldate         = {2024-05-13},
  year            = 2023,
}

@misc{ghojogh21_johns_linden_lemma_linear_nonlin,
  abstract        = {This is a tutorial and survey paper on the
                  Johnson-Lindenstrauss (JL) lemma and linear and nonlinear
                  random projections. We start with linear random projection and
                  then justify its correctness by JL lemma and its proof. Then,
                  sparse random projections with \${\textbackslash}ell\_1\$ norm
                  and interpolation norm are introduced. Two main applications
                  of random projection, which are low-rank matrix approximation
                  and approximate nearest neighbor search by random projection
                  onto hypercube, are explained. Random Fourier Features (RFF)
                  and Random Kitchen Sinks (RKS) are explained as methods for
                  nonlinear random projection. Some other methods for nonlinear
                  random projection, including extreme learning machine,
                  randomly weighted neural networks, and ensemble of random
                  projections, are also introduced.},
  author          = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and
                  Crowley, Mark},
  doi             = {10.48550/arXiv.2108.04172},
  keywords        = {Statistics - Machine Learning, Computer Science - Data
                  Structures and Algorithms, Computer Science - Machine
                  Learning, Mathematics - Probability},
  month           = aug,
  note            = {arXiv:2108.04172 [cs, math, stat]},
  publisher       = {arXiv},
  shorttitle      = {Johnson-{Lindenstrauss} {Lemma}, {Linear} and {Nonlinear}
                  {Random} {Projections}, {Random} {Fourier} {Features}, and
                  {Random} {Kitchen} {Sinks}},
  title           = {Johnson-{Lindenstrauss} {Lemma}, {Linear} and {Nonlinear}
                  {Random} {Projections}, {Random} {Fourier} {Features}, and
                  {Random} {Kitchen} {Sinks}: {Tutorial} and {Survey}},
  url             = {http://arxiv.org/abs/2108.04172},
  urldate         = {2023-12-10},
  year            = 2021,
}

@misc{murray23_random_numer_linear_algeb,
  abstract        = {Randomized numerical linear algebra - RandNLA, for short -
                  concerns the use of randomization as a resource to develop
                  improved algorithms for large-scale linear algebra
                  computations. The origins of contemporary RandNLA lay in
                  theoretical computer science, where it blossomed from a simple
                  idea: randomization provides an avenue for computing
                  approximate solutions to linear algebra problems more
                  efficiently than deterministic algorithms. This idea proved
                  fruitful in the development of scalable algorithms for machine
                  learning and statistical data analysis applications. However,
                  RandNLA's true potential only came into focus upon integration
                  with the fields of numerical analysis and "classical"
                  numerical linear algebra. Through the efforts of many
                  individuals, randomized algorithms have been developed that
                  provide full control over the accuracy of their solutions and
                  that can be every bit as reliable as algorithms that might be
                  found in libraries such as LAPACK. Recent years have even seen
                  the incorporation of certain RandNLA methods into MATLAB, the
                  NAG Library, NVIDIA's cuSOLVER, and SciKit-Learn. For all its
                  success, we believe that RandNLA has yet to realize its full
                  potential. In particular, we believe the scientific community
                  stands to benefit significantly from suitably defined
                  "RandBLAS" and "RandLAPACK" libraries, to serve as standards
                  conceptually analogous to BLAS and LAPACK. This 200-page
                  monograph represents a step toward defining such standards. In
                  it, we cover topics spanning basic sketching, least squares
                  and optimization, low-rank approximation, full matrix
                  decompositions, leverage score sampling, and sketching data
                  with tensor product structures (among others). Much of the
                  provided pseudo-code has been tested via publicly available
                  MATLAB and Python implementations.},
  author          = {Murray, Riley and Demmel, James and Mahoney, Michael W. and
                  Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman
                  Asif and Grigori, Laura and Luszczek, Piotr and Dereziński,
                  Michał and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui
                  and Dongarra, Jack},
  doi             = {10.48550/arXiv.2302.11474},
  keywords        = {Mathematics - Numerical Analysis, Computer Science -
                  Mathematical Software, Mathematics - Optimization and Control},
  month           = apr,
  note            = {arXiv:2302.11474 [cs, math]},
  publisher       = {arXiv},
  shorttitle      = {Randomized {Numerical} {Linear} {Algebra}},
  title           = {Randomized {Numerical} {Linear} {Algebra} : {A}
                  {Perspective} on the {Field} {With} an {Eye} to {Software}},
  url             = {http://arxiv.org/abs/2302.11474},
  urldate         = {2024-05-19},
  year            = 2023,
}

@misc{martinsson21_random_numer_linear_algeb,
  abstract        = {This survey describes probabilistic algorithms for linear
                  algebra computations, such as factorizing matrices and solving
                  linear systems. It focuses on techniques that have a proven
                  track record for real-world problem instances. The paper
                  treats both the theoretical foundations of the subject and the
                  practical computational issues. Topics covered include norm
                  estimation; matrix approximation by sampling; structured and
                  unstructured random embeddings; linear regression problems;
                  low-rank approximation; subspace iteration and Krylov methods;
                  error estimation and adaptivity; interpolatory and CUR
                  factorizations; Nystr{\textbackslash}"om approximation of
                  positive-semidefinite matrices; single view ("streaming")
                  algorithms; full rank-revealing factorizations; solvers for
                  linear systems; and approximation of kernel matrices that
                  arise in machine learning and in scientific computing.},
  author          = {Martinsson, Per-Gunnar and Tropp, Joel},
  doi             = {10.48550/arXiv.2002.01387},
  keywords        = {Mathematics - Numerical Analysis},
  month           = mar,
  note            = {arXiv:2002.01387 [cs, math]},
  publisher       = {arXiv},
  shorttitle      = {Randomized {Numerical} {Linear} {Algebra}},
  title           = {Randomized {Numerical} {Linear} {Algebra}: {Foundations} \&
                  {Algorithms}},
  url             = {http://arxiv.org/abs/2002.01387},
  urldate         = {2024-01-29},
  year            = 2021,
}

@misc{baydin22_gradien_backp,
  abstract        = {Using backpropagation to compute gradients of objective
                  functions for optimization has remained a mainstay of machine
                  learning. Backpropagation, or reverse-mode differentiation, is
                  a special case within the general family of automatic
                  differentiation algorithms that also includes the forward
                  mode. We present a method to compute gradients based solely on
                  the directional derivative that one can compute exactly and
                  efficiently via the forward mode. We call this formulation the
                  forward gradient, an unbiased estimate of the gradient that
                  can be evaluated in a single forward run of the function,
                  entirely eliminating the need for backpropagation in gradient
                  descent. We demonstrate forward gradient descent in a range of
                  problems, showing substantial savings in computation and
                  enabling training up to twice as fast in some cases.},
  author          = {Baydin, Atılım G{\"u}ne{\c{s}} and Pearlmutter, Barak A.
                  and Syme, Don and Wood, Frank and Torr, Philip},
  doi             = {10.48550/arXiv.2202.08587},
  keywords        = {Computer Science - Machine Learning, Statistics - Machine
                  Learning, 68T07, I.2.6, I.2.5},
  month           = feb,
  note            = {arXiv:2202.08587 [cs, stat]},
  publisher       = {arXiv},
  title           = {Gradients without {Backpropagation}},
  url             = {http://arxiv.org/abs/2202.08587},
  urldate         = {2023-12-12},
  year            = 2022,
}

@misc{oktay21_random_autom_differ,
  abstract        = {The successes of deep learning, variational inference, and
                  many other fields have been aided by specialized
                  implementations of reverse-mode automatic differentiation (AD)
                  to compute gradients of mega-dimensional objectives. The AD
                  techniques underlying these tools were designed to compute
                  exact gradients to numerical precision, but modern machine
                  learning models are almost always trained with stochastic
                  gradient descent. Why spend computation and memory on exact
                  (minibatch) gradients only to use them for stochastic
                  optimization? We develop a general framework and approach for
                  randomized automatic differentiation (RAD), which can allow
                  unbiased gradient estimates to be computed with reduced memory
                  in return for variance. We examine limitations of the general
                  approach, and argue that we must leverage problem specific
                  structure to realize benefits. We develop RAD techniques for a
                  variety of simple neural network architectures, and show that
                  for a fixed memory budget, RAD converges in fewer iterations
                  than using a small batch size for feedforward networks, and in
                  a similar number for recurrent networks. We also show that RAD
                  can be applied to scientific computing, and use it to develop
                  a low-memory stochastic gradient method for optimizing the
                  control parameters of a linear reaction-diffusion PDE
                  representing a fission reactor.},
  author          = {Oktay, Deniz and McGreivy, Nick and Aduol, Joshua and
                  Beatson, Alex and Adams, Ryan P.},
  doi             = {10.48550/arXiv.2007.10412},
  keywords        = {Computer Science - Machine Learning, Statistics - Machine
                  Learning},
  month           = mar,
  note            = {arXiv:2007.10412 [cs, stat]},
  publisher       = {arXiv},
  title           = {Randomized {Automatic} {Differentiation}},
  url             = {http://arxiv.org/abs/2007.10412},
  urldate         = {2023-12-18},
  year            = 2021,
}

@misc{li23_forwar_laplac,
  abstract        = {Neural network-based variational Monte Carlo (NN-VMC) has
                  emerged as a promising cutting-edge technique of ab initio
                  quantum chemistry. However, the high computational cost of
                  existing approaches hinders their applications in realistic
                  chemistry problems. Here, we report the development of a new
                  NN-VMC method that achieves a remarkable speed-up by more than
                  one order of magnitude, thereby greatly extending the
                  applicability of NN-VMC to larger systems. Our key design is a
                  novel computational framework named Forward Laplacian, which
                  computes the Laplacian associated with neural networks, the
                  bottleneck of NN-VMC, through an efficient forward propagation
                  process. We then demonstrate that Forward Laplacian is not
                  only versatile but also facilitates more developments of
                  acceleration methods across various aspects, including
                  optimization for sparse derivative matrix and efficient neural
                  network design. Empirically, our approach enables NN-VMC to
                  investigate a broader range of atoms, molecules and chemical
                  reactions for the first time, providing valuable references to
                  other ab initio methods. The results demonstrate a great
                  potential in applying deep learning methods to solve general
                  quantum mechanical problems.},
  author          = {Li, Ruichen and Ye, Haotian and Jiang, Du and Wen, Xuelan
                  and Wang, Chuwei and Li, Zhe and Li, Xiang and He, Di and
                  Chen, Ji and Ren, Weiluo and Wang, Liwei},
  doi             = {10.48550/arXiv.2307.08214},
  keywords        = {Physics - Computational Physics, Computer Science - Machine
                  Learning, Physics - Chemical Physics},
  month           = jul,
  note            = {arXiv:2307.08214 [physics]},
  publisher       = {arXiv},
  shorttitle      = {Forward {Laplacian}},
  title           = {Forward {Laplacian}: {A} {New} {Computational} {Framework}
                  for {Neural} {Network}-based {Variational} {Monte} {Carlo}},
  url             = {http://arxiv.org/abs/2307.08214},
  urldate         = {2023-12-16},
  year            = 2023,
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{laurel22_gener_const_abstr_inter_higher,
  author          = {Laurel, Jacob and Yang, Rem and Ugare, Shubham and Nagel,
                  Robert and Singh, Gagandeep and Misailovic, Sasa},
  title           = {A General Construction for Abstract Interpretation of
                  Higher-Order Automatic Differentiation},
  journal         = {Proc. ACM Program. Lang.},
  volume          = 6,
  number          = {OOPSLA2},
  year            = 2022,
  doi             = {10.1145/3563324},
  url             = {https://doi-org.libproxy1.nus.edu.sg/10.1145/3563324},
  abstract        = {We present a novel, general construction to abstractly
                  interpret higher-order automatic differentiation (AD). Our
                  construction allows one to instantiate an abstract interpreter
                  for computing derivatives up to a chosen order. Furthermore,
                  since our construction reduces the problem of abstractly
                  reasoning about derivatives to abstractly reasoning about
                  real-valued straight-line programs, it can be instantiated
                  with almost any numerical abstract domain, both relational and
                  non-relational. We formally establish the soundness of this
                  construction. We implement our technique by instantiating our
                  construction with both the non-relational interval domain and
                  the relational zonotope domain to compute both first and
                  higher-order derivatives. In the latter case, we are the first
                  to apply a relational domain to automatic differentiation for
                  abstracting higher-order derivatives, and hence we are also
                  the first abstract interpretation work to track correlations
                  across not only different variables, but different orders of
                  derivatives. We evaluate these instantiations on multiple case
                  studies, namely robustly explaining a neural network and more
                  precisely computing a neural network's Lipschitz constant. For
                  robust interpretation, first and second derivatives computed
                  via zonotope AD are up to 4.76\texttimes{} and
                  6.98\texttimes{} more precise, respectively, compared to
                  interval AD. For Lipschitz certification, we obtain bounds
                  that are up to 11,850\texttimes{} more precise with zonotopes,
                  compared to the state-of-the-art interval-based tool.},
  address         = {New York, NY, USA},
  articleno       = 161,
  issue_date      = {October 2022},
  keywords        = {Abstract Interpretation, Differentiable Programming},
  month           = {oct},
  numpages        = 29,
  publisher       = {Association for Computing Machinery},
}

@phdthesis{wang17_high_order_rever_mode_autom_differ,
  abstract        = {We study the high order reverse mode of Automatic
                  Differentiation (AD) in the dissertation. Automatic
                  Differentiation (AD) is a technique to augment a computer
                  program for computing a function so that the augmented program
                  computes the derivatives as well as the function values. AD is
                  employed to solve optimization problems and differential
                  equations from many application domains, and has been included
                  among the top twenty algorithms in scientific computing. The
                  reverse mode of AD propagates the derivatives in the reverse
                  order of the evaluation of the objective function. It has
                  optimal time complexity for computing first-order derivatives
                  since it satisfies the Baur-Strassen theorem, which states
                  that the complexity of evaluating all (first order) partial
                  derivatives of a scalar objective function is only a constant
                  factor greater than the complexity of computing the objective
                  function itself. We propose the generalized high order reverse
                  mode of which the first order reverse mode can be considered
                  as a special case. The key concept in the high order reverse
                  mode is live variables. The set of live variables at a step is
                  defined as currently active variables whose values will be
                  used in future steps of the computation. Then the invariant of
                  reverse mode AD can be stated as follows: the intermediate
                  results at a step k in the algorithm are the derivatives of a
                  suitably defined equivalent function fk(Sk), where S k
                  constitutes the current set of live variables, and fk is
                  obtained by composing the elemental functions by which the
                  function is computed. A general expression for the high order
                  chain rule for evaluating derivatives of fk( Sk) from values
                  at the previous step, fk +1(Sk +1), yields the high order
                  reverse mode. We have provided a thorough complexity analysis
                  of these algorithms. The algorithms are implemented to exploit
                  both sparsity and symmetry. Sparsity is exploited by
                  performing updates with only the nonzero values in the
                  algorithm. Symmetry is exploited by keeping only the unique
                  elements in the high order derivative tensor due to the
                  inherently high degree of symmetry. For second and third order
                  derivatives, we show that enabling preaccumulation can further
                  reduce the time complexity. The combinatorial properties of
                  the high order reverse mode are also discussed in the
                  dissertation. We prove that the second order reverse mode is
                  equivalent to a combinatorial model that performs vertex
                  elimination on the computational graph of the gradient. More
                  generally, performing vertex elimination in a specified order
                  on the computational graph corresponding to the procedure for
                  evaluating derivatives up to the d-th order yields the
                  (d+1)-th order reverse mode.},
  author          = {Wang,Mu},
  isbn            = {978-0-355-25686-4},
  journal         = {ProQuest Dissertations and Theses},
  keywords        = {Applied sciences; Automatic differentiation; Derivative
                  tensor; Hessian; High order; Reverse mode; Computer science;
                  0984:Computer science},
  language        = {English},
  note            = {Copyright - Database copyright ProQuest LLC; ProQuest does
                  not claim copyright in the individual underlying works; Last
                  updated - 2023-03-04},
  pages           = {155},
  title           = {High Order Reverse Mode of Automatic Differentiation},
  url             =
                  {http://libproxy1.nus.edu.sg/login?url=https://www.proquest.com/dissertations-theses/high-order-reverse-mode-automatic-differentiation/docview/1975367062/se-2},
  year            = {2017},
}

@inproceedings{karczmarczuk98_funct_differ_comput_progr,
  author          = {Karczmarczuk, Jerzy},
  title           = {Functional Differentiation of Computer Programs},
  booktitle       = {Proceedings of the Third ACM SIGPLAN International
                  Conference on Functional Programming},
  year            = {1998},
  pages           = {195-203},
  doi             = {10.1145/289423.289442},
  url             = {https://doi.org/10.1145/289423.289442},
  abstract        = {We present two purely functional implementations of the
                  computational differentiation tools -- the well known numeric
                  (not symbolic!) techniques which permit to compute pointwise
                  derivatives of functions defined by computer programs
                  economically and exactly. We show how the co-recursive (lazy)
                  algorithm formulation permits to construct in a transparent
                  and elegant manner the entire infinite tower of derivatives of
                  higher order for any expressions present in the program, and
                  we present a purely functional variant of the reverse (or
                  adjoint) mode of computational differentiation, using a chain
                  of delayed evaluations represented by closures. Some concrete
                  applications are also discussed.},
  address         = {New York, NY, USA},
  isbn            = {1581130244},
  keywords        = {differentiation, derivatives, lazy semantics, arithmetic,
                  Haskell},
  location        = {Baltimore, Maryland, USA},
  numpages        = {9},
  publisher       = {Association for Computing Machinery},
  series          = {ICFP '98},
}

@inproceedings{bendtsen97_tadif_flexib_c_packag_for,
  author          = {Claus Bendtsen and Ole Stauning},
  title           = {TADIFF , A FLEXIBLE C + + PACKAGE FOR AUTOMATIC
                  DIFFERENTIATION using Taylor series expansion},
  year            = {1997},
  url             = {https://api.semanticscholar.org/CorpusID:62828904},
}

@inproceedings{bettencourt19_taylor_mode_autom_differ_higher,
  author          = {Jesse Bettencourt and Matthew J. Johnson and David
                  Duvenaud},
  title           = {Taylor-Mode Automatic Differentiation for Higher-Order
                  Derivatives in {JAX}},
  booktitle       = {Program Transformations for ML Workshop at NeurIPS 2019},
  year            = 2019,
  url             = {https://openreview.net/forum?id=SkxEF3FNPH},
}

@article{stein81_estim_mean_multiv_normal_distr,
  author          = {Charles M. Stein},
  title           = {{Estimation of the Mean of a Multivariate Normal
                  Distribution}},
  journal         = {The Annals of Statistics},
  volume          = 9,
  number          = 6,
  pages           = {1135 -- 1151},
  year            = 1981,
  doi             = {10.1214/aos/1176345632},
  URL             = {https://doi.org/10.1214/aos/1176345632},
  keywords        = {Bayes estimate, confidence region, James-Stein estimate,
                  Minimax estimate, moving average, multivariate normal mean,
                  simultaneous estimation, trimmed mean},
  publisher       = {Institute of Mathematical Statistics},
}

@misc{hu23_bias_varian_trade_physic_infor,
  abstract        = {While physics-informed neural networks (PINNs) have been
                  proven effective for low-dimensional partial differential
                  equations (PDEs), the computational cost remains a hurdle in
                  high-dimensional scenarios. This is particularly pronounced
                  when computing high-order and high-dimensional derivatives in
                  the physics-informed loss. Randomized Smoothing PINN (RS-PINN)
                  introduces Gaussian noise for stochastic smoothing of the
                  original neural net model, enabling Monte Carlo methods for
                  derivative approximation, eliminating the need for costly
                  auto-differentiation. Despite its computational efficiency in
                  high dimensions, RS-PINN introduces biases in both loss and
                  gradients, negatively impacting convergence, especially when
                  coupled with stochastic gradient descent (SGD). We present a
                  comprehensive analysis of biases in RS-PINN, attributing them
                  to the nonlinearity of the Mean Squared Error (MSE) loss and
                  the PDE nonlinearity. We propose tailored bias correction
                  techniques based on the order of PDE nonlinearity. The
                  unbiased RS-PINN allows for a detailed examination of its pros
                  and cons compared to the biased version. Specifically, the
                  biased version has a lower variance and runs faster than the
                  unbiased version, but it is less accurate due to the bias. To
                  optimize the bias-variance trade-off, we combine the two
                  approaches in a hybrid method that balances the rapid
                  convergence of the biased version with the high accuracy of
                  the unbiased version. In addition, we present an enhanced
                  implementation of RS-PINN. Extensive experiments on diverse
                  high-dimensional PDEs, including Fokker-Planck, HJB, viscous
                  Burgers', Allen-Cahn, and Sine-Gordon equations, illustrate
                  the bias-variance trade-off and highlight the effectiveness of
                  the hybrid RS-PINN. Empirical guidelines are provided for
                  selecting biased, unbiased, or hybrid versions, depending on
                  the dimensionality and nonlinearity of the specific PDE
                  problem.},
  author          = {Hu, Zheyuan and Yang, Zhouhao and Wang, Yezhen and
                  Karniadakis, George Em and Kawaguchi, Kenji},
  doi             = {10.48550/arXiv.2311.15283},
  keywords        = {Computer Science - Machine Learning, Computer Science -
                  Artificial Intelligence, Mathematics - Dynamical Systems,
                  Mathematics - Numerical Analysis, Statistics - Machine
                  Learning, 14J60},
  month           = nov,
  note            = {arXiv:2311.15283 [cs, math, stat]},
  publisher       = {arXiv},
  title           = {Bias-{Variance} {Trade}-off in {Physics}-{Informed}
                  {Neural} {Networks} with {Randomized} {Smoothing} for
                  {High}-{Dimensional} {PDEs}},
  url             = {http://arxiv.org/abs/2311.15283},
  urldate         = {2024-04-12},
  year            = 2023,
}

@misc{he23_learn_physic_infor_neural_networ,
  abstract        = {Physics-Informed Neural Network (PINN) has become a
                  commonly used machine learning approach to solve partial
                  differential equations (PDE). But, facing high-dimensional
                  secondorder PDE problems, PINN will suffer from severe
                  scalability issues since its loss includes second-order
                  derivatives, the computational cost of which will grow along
                  with the dimension during stacked back-propagation. In this
                  work, we develop a novel approach that can significantly
                  accelerate the training of Physics-Informed Neural Networks.
                  In particular, we parameterize the PDE solution by the
                  Gaussian smoothed model and show that, derived from Stein's
                  Identity, the second-order derivatives can be efficiently
                  calculated without back-propagation. We further discuss the
                  model capacity and provide variance reduction methods to
                  address key limitations in the derivative estimation.
                  Experimental results show that our proposed method can achieve
                  competitive error compared to standard PINN training but is
                  significantly faster. Our code is released at
                  https://github.com/LithiumDA/PINN-without-Stacked-BP.},
  author          = {He, Di and Li, Shanda and Shi, Wenlei and Gao, Xiaotian and
                  Zhang, Jia and Bian, Jiang and Wang, Liwei and Liu, Tie-Yan},
  doi             = {10.48550/arXiv.2202.09340},
  keywords        = {Computer Science - Machine Learning},
  month           = feb,
  note            = {arXiv:2202.09340 [cs]},
  publisher       = {arXiv},
  title           = {Learning {Physics}-{Informed} {Neural} {Networks} without
                  {Stacked} {Back}-propagation},
  url             = {http://arxiv.org/abs/2202.09340},
  urldate         = {2023-08-12},
  year            = 2023,
}

@misc{pang20_effic_learn_gener_model_finit,
  abstract        = {Several machine learning applications involve the
                  optimization of higher-order derivatives (e.g., gradients of
                  gradients) during training, which can be expensive in respect
                  to memory and computation even with automatic differentiation.
                  As a typical example in generative modeling, score matching
                  (SM) involves the optimization of the trace of a Hessian. To
                  improve computing efficiency, we rewrite the SM objective and
                  its variants in terms of directional derivatives, and present
                  a generic strategy to efficiently approximate any-order
                  directional derivative with finite difference (FD). Our
                  approximation only involves function evaluations, which can be
                  executed in parallel, and no gradient computations. Thus, it
                  reduces the total computational cost while also improving
                  numerical stability. We provide two instantiations by
                  reformulating variants of SM objectives into the FD forms.
                  Empirically, we demonstrate that our methods produce results
                  comparable to the gradient-based counterparts while being much
                  more computationally efficient.},
  author          = {Pang, Tianyu and Xu, Kun and Li, Chongxuan and Song, Yang
                  and Ermon, Stefano and Zhu, Jun},
  doi             = {10.48550/arXiv.2007.03317},
  keywords        = {Computer Science - Machine Learning, Statistics - Machine
                  Learning},
  month           = nov,
  note            = {arXiv:2007.03317 [cs, stat]},
  publisher       = {arXiv},
  title           = {Efficient {Learning} of {Generative} {Models} via
                  {Finite}-{Difference} {Score} {Matching}},
  url             = {http://arxiv.org/abs/2007.03317},
  urldate         = {2023-12-14},
  year            = 2020,
}

@Article{hutchinson89_stoch_estim_trace_influen_matrix,
  author          = {Hutchinson, M.F.},
  title           = {A Stochastic Estimator of the Trace of the Influence Matrix
                  for Laplacian Smoothing Splines},
  journal         = {Communications in Statistics - Simulation and Computation},
  volume          = 18,
  number          = 3,
  pages           = {1059-1076},
  year            = 1989,
  doi             = {10.1080/03610918908812806},
  url             = {http://dx.doi.org/10.1080/03610918908812806},
  issn            = {1532-4141},
  month           = jan,
  publisher       = {Informa UK Limited},
}

@misc{hu24_score_based_physic_infor_neural,
  abstract        = {The Fokker-Planck (FP) equation is a foundational PDE in
                  stochastic processes. However, curse of dimensionality (CoD)
                  poses challenge when dealing with high-dimensional FP PDEs.
                  Although Monte Carlo and vanilla Physics-Informed Neural
                  Networks (PINNs) have shown the potential to tackle CoD, both
                  methods exhibit numerical errors in high dimensions when
                  dealing with the probability density function (PDF) associated
                  with Brownian motion. The point-wise PDF values tend to
                  decrease exponentially as dimension increases, surpassing the
                  precision of numerical simulations and resulting in
                  substantial errors. Moreover, due to its massive sampling,
                  Monte Carlo fails to offer fast sampling. Modeling the
                  logarithm likelihood (LL) via vanilla PINNs transforms the FP
                  equation into a difficult HJB equation, whose error grows
                  rapidly with dimension. To this end, we propose a novel
                  approach utilizing a score-based solver to fit the score
                  function in SDEs. The score function, defined as the gradient
                  of the LL, plays a fundamental role in inferring LL and PDF
                  and enables fast SDE sampling. Three fitting methods, Score
                  Matching (SM), Sliced SM (SSM), and Score-PINN, are
                  introduced. The proposed score-based SDE solver operates in
                  two stages: first, employing SM, SSM, or Score-PINN to acquire
                  the score; and second, solving the LL via an ODE using the
                  obtained score. Comparative evaluations across these methods
                  showcase varying trade-offs. The proposed method is evaluated
                  across diverse SDEs, including anisotropic OU processes,
                  geometric Brownian, and Brownian with varying eigenspace. We
                  also test various distributions, including Gaussian,
                  Log-normal, Laplace, and Cauchy. The numerical results
                  demonstrate the score-based SDE solver's stability, speed, and
                  performance across different settings, solidifying its
                  potential as a solution to CoD for high-dimensional FP
                  equations.},
  author          = {Hu, Zheyuan and Zhang, Zhongqiang and Karniadakis, George
                  Em and Kawaguchi, Kenji},
  doi             = {10.48550/arXiv.2402.07465},
  keywords        = {Computer Science - Machine Learning, Computer Science -
                  Artificial Intelligence, Mathematics - Dynamical Systems,
                  Mathematics - Numerical Analysis, Statistics - Machine
                  Learning, 14J60},
  month           = feb,
  note            = {arXiv:2402.07465 [cs, math, stat]},
  publisher       = {arXiv},
  title           = {Score-{Based} {Physics}-{Informed} {Neural} {Networks} for
                  {High}-{Dimensional} {Fokker}-{Planck} {Equations}},
  url             = {http://arxiv.org/abs/2402.07465},
  urldate         = {2024-04-12},
  year            = 2024,
}

@inproceedings{lai22_regul,
  author          = {Lai, Chieh-Hsin and Takida, Yuhta and Murata, Naoki and
                  Uesaka, Toshimitsu and Mitsufuji, Yuki and Ermon, Stefano},
  title           = {Regularizing score-based models with score fokker-planck
                  equations},
  booktitle       = {NeurIPS 2022 Workshop on Score-Based Methods},
  year            = 2022,
}

@article{hu24_hutch_trace_estim_high_dimen,
  author          = {Hu, Zheyuan and Shi, Zekun and Karniadakis, George Em and
                  Kawaguchi, Kenji},
  title           = {Hutchinson {Trace} {Estimation} for {High}-{Dimensional}
                  and {High}-{Order} {Physics}-{Informed} {Neural} {Networks}},
  journal         = {Computer Methods in Applied Mechanics and Engineering},
  volume          = 424,
  pages           = 116883,
  year            = 2024,
  doi             = {10.1016/j.cma.2024.116883},
  url             = {http://arxiv.org/abs/2312.14499},
  abstract        = {Physics-Informed Neural Networks (PINNs) have proven
                  effective in solving partial differential equations (PDEs),
                  especially when some data are available by seamlessly blending
                  data and physics. However, extending PINNs to high-dimensional
                  and even high-order PDEs encounters significant challenges due
                  to the computational cost associated with automatic
                  differentiation in the residual loss. Herein, we address the
                  limitations of PINNs in handling high-dimensional and
                  high-order PDEs by introducing Hutchinson Trace Estimation
                  (HTE). Starting with the second-order high-dimensional PDEs
                  ubiquitous in scientific computing, HTE transforms the
                  calculation of the entire Hessian matrix into a Hessian vector
                  product (HVP). This approach alleviates the computational
                  bottleneck via Taylor-mode automatic differentiation and
                  significantly reduces memory consumption from the Hessian
                  matrix to HVP. We further showcase HTE's convergence to the
                  original PINN loss and its unbiased behavior under specific
                  conditions. Comparisons with Stochastic Dimension Gradient
                  Descent (SDGD) highlight the distinct advantages of HTE,
                  particularly in scenarios with significant variance among
                  dimensions. We further extend HTE to higher-order and
                  higher-dimensional PDEs, specifically addressing the
                  biharmonic equation. By employing tensor-vector products
                  (TVP), HTE efficiently computes the colossal tensor associated
                  with the fourth-order high-dimensional biharmonic equation,
                  saving memory and enabling rapid computation. The
                  effectiveness of HTE is illustrated through experimental
                  setups, demonstrating comparable convergence rates with SDGD
                  under memory and speed constraints. Additionally, HTE proves
                  valuable in accelerating the Gradient-Enhanced PINN (gPINN)
                  version as well as the Biharmonic equation. Overall, HTE opens
                  up a new capability in scientific machine learning for
                  tackling high-order and high-dimensional PDEs.},
  issn            = 00457825,
  keywords        = {Computer Science - Machine Learning, Computer Science -
                  Artificial Intelligence, Mathematics - Dynamical Systems,
                  Mathematics - Numerical Analysis, Statistics - Machine
                  Learning, 14J60},
  month           = may,
  note            = {arXiv:2312.14499 [cs, math, stat]},
  urldate         = {2024-04-12},
}

@article{hu24_tackl_curse_dimen_with_physic,
  author          = {Hu, Zheyuan and Shukla, Khemraj and Karniadakis, George Em
                  and Kawaguchi, Kenji},
  title           = {Tackling the Curse of Dimensionality With Physics-Informed
                  Neural Networks},
  journal         = {Neural Networks},
  volume          = 176,
  pages           = 106369,
  year            = 2024,
  publisher       = {Elsevier},
}

@Article{raissi19_physic_infor_neural_networ,
  author          = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  title           = {Physics-Informed Neural Networks: a Deep Learning Framework
                  for Solving Forward and Inverse Problems Involving Nonlinear
                  Partial Differential Equations},
  journal         = {Journal of Computational Physics},
  volume          = 378,
  pages           = {686-707},
  year            = 2019,
  doi             = {10.1016/j.jcp.2018.10.045},
  url             = {http://dx.doi.org/10.1016/j.jcp.2018.10.045},
  issn            = {0021-9991},
  month           = feb,
  publisher       = {Elsevier BV},
}

@Article{karniadakis21_physic_infor_machin_learn,
  author          = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu,
                  Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  title           = {Physics-Informed Machine Learning},
  journal         = {Nature Reviews Physics},
  volume          = 3,
  number          = 6,
  pages           = {422-440},
  year            = 2021,
  doi             = {10.1038/s42254-021-00314-5},
  url             = {https://doi.org/10.1038/s42254-021-00314-5},
  abstract        = {Despite great progress in simulating multiphysics problems
                  using the numerical discretization of partial differential
                  equations (PDEs), one still cannot seamlessly incorporate
                  noisy data into existing algorithms, mesh generation remains
                  complex, and high-dimensional problems governed by
                  parameterized PDEs cannot be tackled. Moreover, solving
                  inverse problems with hidden physics is often prohibitively
                  expensive and requires different formulations and elaborate
                  computer codes. Machine learning has emerged as a promising
                  alternative, but training deep neural networks requires big
                  data, not always available for scientific problems. Instead,
                  such networks can be trained from additional information
                  obtained by enforcing the physical laws (for example, at
                  random points in the continuous space-time domain). Such
                  physics-informed learning integrates (noisy) data and
                  mathematical models, and implements them through neural
                  networks or other kernel-based regression networks. Moreover,
                  it may be possible to design specialized network architectures
                  that automatically satisfy some of the physical invariants for
                  better accuracy, faster training and improved generalization.
                  Here, we review some of the prevailing trends in embedding
                  physics into machine learning, present some of the current
                  capabilities and limitations and discuss diverse applications
                  of physics-informed learning both for forward and inverse
                  problems, including discovering hidden physics and tackling
                  high-dimensional problems.},
  day             = 01,
  issn            = {2522-5820},
  month           = {Jun},
}

@inproceedings{li24_dof,
  author          = {Ruichen Li and Chuwei Wang and Haotian Ye and Di He and
                  Liwei Wang},
  title           = {{DOF}: Accelerating High-order Differential Operators with
                  Forward Propagation},
  booktitle       = {ICLR 2024 Workshop on AI4DifferentialEquations In Science},
  year            = 2024,
  url             = {https://openreview.net/forum?id=yQsLKpkRoS},
}
