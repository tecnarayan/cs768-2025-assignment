\begin{thebibliography}{10}

\bibitem{wang2019satnet}
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter.
\newblock Satnet: Bridging deep learning and logical reasoning using a
  differentiable satisfiability solver.
\newblock In {\em International Conference on Machine Learning}, pages
  6545--6554, 2019.

\bibitem{chen2020rna}
Xinshi Chen, Yu~Li, Ramzan Umarov, Xin Gao, and Le~Song.
\newblock Rna secondary structure prediction by learning unrolled algorithms.
\newblock {\em arXiv preprint arXiv:2002.05810}, 2020.

\bibitem{cuturi2019differentiable}
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert.
\newblock Differentiable sorting using optimal transport: The sinkhorn cdf and
  quantile operator.
\newblock {\em arXiv preprint arXiv:1905.11885}, 2019.

\bibitem{shrivastava2020glad}
Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinivas Aluru,
  Han Liu, and Le~Song.
\newblock {GLAD}: Learning sparse graph recovery.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yang2017admm}
Y~Yang, J~Sun, H~Li, and Z~Xu.
\newblock Admm-net: A deep learning approach for compressive sensing mri. corr.
\newblock {\em arXiv preprint arXiv:1705.06869}, 2017.

\bibitem{ingraham2018learning}
John Ingraham, Adam Riesselman, Chris Sander, and Debora Marks.
\newblock Learning protein structure with a differentiable simulator.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{manhaeve2018deepproblog}
Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc
  De~Raedt.
\newblock Deepproblog: Neural probabilistic logic programming.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3749--3759, 2018.

\bibitem{mensch2018differentiable}
Arthur Mensch and Mathieu Blondel.
\newblock Differentiable dynamic programming for structured prediction and
  attention.
\newblock In {\em 35th International Conference on Machine Learning},
  volume~80, 2018.

\bibitem{wilder2019end}
Bryan Wilder, Eric Ewing, Bistra Dilkina, and Milind Tambe.
\newblock End to end learning and optimization on graphs.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4674--4685, 2019.

\bibitem{domke2011parameter}
Justin Domke.
\newblock Parameter learning with truncated message-passing.
\newblock In {\em CVPR 2011}, pages 2937--2943. IEEE, 2011.

\bibitem{paschalidou2018raynet}
Despoina Paschalidou, Osman Ulusoy, Carolin Schmitt, Luc Van~Gool, and Andreas
  Geiger.
\newblock Raynet: Learning volumetric 3d reconstruction with ray potentials.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3897--3906, 2018.

\bibitem{wang2019backpropagation}
Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua, and Mathieu Salzmann.
\newblock Backpropagation-friendly eigendecomposition.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3156--3164, 2019.

\bibitem{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6240--6249, 2017.

\bibitem{chen2019generalization}
Minshuo Chen, Xingguo Li, and Tuo Zhao.
\newblock On generalization bounds of a family of recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1910.12947}, 2019.

\bibitem{garg2020generalization}
Vikas~K Garg, Stefanie Jegelka, and Tommi Jaakkola.
\newblock Generalization and representational limits of graph neural networks.
\newblock {\em arXiv preprint arXiv:2002.06157}, 2020.

\bibitem{bartlett2005local}
Peter~L Bartlett, Olivier Bousquet, Shahar Mendelson, et~al.
\newblock Local rademacher complexities.
\newblock {\em The Annals of Statistics}, 33(4):1497--1537, 2005.

\bibitem{koltchinskii2006local}
Vladimir Koltchinskii et~al.
\newblock Local rademacher complexities and oracle inequalities in risk
  minimization.
\newblock {\em The Annals of Statistics}, 34(6):2593--2656, 2006.

\bibitem{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock {\em arXiv preprint arXiv:1804.01619}, 2018.

\bibitem{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3981--3989, 2016.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{maurer2016vector}
Andreas Maurer.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 3--17. Springer, 2016.

\bibitem{cortes2016structured}
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang.
\newblock Structured prediction theory based on factor graph complexity.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2514--2522, 2016.

\bibitem{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em Journal of machine learning research}, 2(Mar):499--526, 2002.

\bibitem{agarwal2009generalization}
Shivani Agarwal and Partha Niyogi.
\newblock Generalization bounds for ranking algorithms via algorithmic
  stability.
\newblock {\em Journal of Machine Learning Research}, 10(Feb):441--474, 2009.

\bibitem{hardt2015train}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:1509.01240}, 2015.

\bibitem{rivasplata2018pac}
Omar Rivasplata, Emilio Parrado-Hern{\'a}ndez, John~S Shawe-Taylor, Shiliang
  Sun, and Csaba Szepesv{\'a}ri.
\newblock Pac-bayes bounds for stable algorithms with instance-dependent
  priors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9214--9224, 2018.

\bibitem{verma2019stability}
Saurabh Verma and Zhi-Li Zhang.
\newblock Stability and generalization of graph convolutional neural networks.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1539--1548, 2019.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  113--124, 2019.

\bibitem{belanger2017end}
David Belanger, Bishan Yang, and Andrew McCallum.
\newblock End-to-end learning for structured prediction energy networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 429--439. JMLR. org, 2017.

\bibitem{donti2017task}
Priya Donti, Brandon Amos, and J~Zico Kolter.
\newblock Task-based end-to-end model learning in stochastic optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5484--5494, 2017.

\bibitem{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 136--145. JMLR. org, 2017.

\bibitem{poganvcic2019differentiation}
Marin~Vlastelica Pogan{\v{c}}i{\'c}, Anselm Paulus, Vit Musil, Georg Martius,
  and Michal Rolinek.
\newblock Differentiation of blackbox combinatorial solvers.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{rolinek2020deep}
Michal Rol{\'\i}nek, Paul Swoboda, Dominik Zietlow, Anselm Paulus, V{\'\i}t
  Musil, and Georg Martius.
\newblock Deep graph matching via blackbox differentiation of combinatorial
  solvers.
\newblock {\em arXiv preprint arXiv:2003.11657}, 2020.

\bibitem{berthet2020learning}
Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe
  Vert, and Francis Bach.
\newblock Learning with differentiable perturbed optimizers.
\newblock {\em arXiv preprint arXiv:2002.08676}, 2020.

\bibitem{chen2018theoretical}
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin.
\newblock Theoretical linear convergence of unfolded ista and its practical
  weights and thresholds.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9061--9071, 2018.

\bibitem{ferber2020mipaal}
Aaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe.
\newblock Mipaal: Mixed integer program as a layer.
\newblock In {\em AAAI}, pages 1504--1511, 2020.

\bibitem{knobelreiter2017end}
Patrick Knobelreiter, Christian Reinbacher, Alexander Shekhovtsov, and Thomas
  Pock.
\newblock End-to-end training of hybrid cnn-crf models for stereo.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2339--2348, 2017.

\bibitem{niculae2018sparsemap}
Vlad Niculae, Andre Martins, Mathieu Blondel, and Claire Cardie.
\newblock Sparsemap: Differentiable sparse structured inference.
\newblock In {\em International Conference on Machine Learning}, pages
  3799--3808, 2018.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{kawaguchi2017generalization}
Kenji Kawaguchi, Leslie~Pack Kaelbling, and Yoshua Bengio.
\newblock Generalization in deep learning.
\newblock {\em arXiv preprint arXiv:1710.05468}, 2017.

\bibitem{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017.

\bibitem{neyshabur2018a}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{koltchinskii2000rademacher}
Vladimir Koltchinskii and Dmitriy Panchenko.
\newblock Rademacher processes and bounding the risk of function learning.
\newblock In {\em High dimensional probability II}, pages 443--457. Springer,
  2000.

\bibitem{koltchinskii2001rademacher}
Vladimir Koltchinskii.
\newblock Rademacher penalties and structural risk minimization.
\newblock {\em IEEE Transactions on Information Theory}, 47(5):1902--1914,
  2001.

\bibitem{koltchinskii2002empirical}
Vladimir Koltchinskii, Dmitry Panchenko, et~al.
\newblock Empirical margin distributions and bounding the generalization error
  of combined classifiers.
\newblock {\em The Annals of Statistics}, 30(1):1--50, 2002.

\bibitem{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{talagrand1994sharper}
Michel Talagrand.
\newblock Sharper bounds for gaussian and empirical processes.
\newblock {\em The Annals of Probability}, pages 28--76, 1994.

\bibitem{dudley2014uniform}
Richard~M Dudley.
\newblock {\em Uniform central limit theorems}, volume 142.
\newblock Cambridge university press, 2014.

\bibitem{bai2019deep}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  688--699, 2019.

\bibitem{el2019implicit}
Laurent El~Ghaoui, Fangda Gu, Bertrand Travacca, and Armin Askari.
\newblock Implicit deep learning.
\newblock {\em arXiv preprint arXiv:1908.06315}, 2019.

\bibitem{arbelaez2010contour}
Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik.
\newblock Contour detection and hierarchical image segmentation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  33(5):898--916, 2010.

\bibitem{zhang2017beyond}
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.
\newblock Beyond a gaussian denoiser: Residual learning of deep cnn for image
  denoising.
\newblock {\em IEEE Transactions on Image Processing}, 26(7):3142--3155, 2017.

\end{thebibliography}
