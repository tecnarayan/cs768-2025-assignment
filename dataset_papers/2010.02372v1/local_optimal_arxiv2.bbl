\begin{thebibliography}{10}

\bibitem{agarwal2014lower}
Alekh Agarwal and Leon Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock {\em Proceedings of the 32nd International Conference on Machine
  Learning}, 2015.

\bibitem{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter~L Bartlett, and Pradeep~K Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--9, 2009.

\bibitem{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock {\em The Journal of Machine Learning Research}, 18(1):8194--8244,
  2017.

\bibitem{balkanski2018parallelization}
Eric Balkanski and Yaron Singer.
\newblock Parallelization does not accelerate convex optimization: Adaptivity
  lower bounds for non-smooth convex minimization.
\newblock {\em arXiv preprint arXiv:1808.03880}, 2018.

\bibitem{beck2017first}
Amir Beck.
\newblock {\em First-order methods in optimization}, volume~25.
\newblock SIAM, 2017.

\bibitem{beck2009fast}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM journal on imaging sciences}, 2(1):183--202, 2009.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: A library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):1--27, 2011.

\bibitem{corinzia2019variational}
Luca Corinzia and Joachim~M Buhmann.
\newblock Variational federated multi-task learning.
\newblock {\em arXiv preprint arXiv:1906.06268}, 2019.

\bibitem{defazio2016simple}
Aaron Defazio.
\newblock A simple practical accelerated method for finite sums.
\newblock In {\em Advances in neural information processing systems}, pages
  676--684, 2016.

\bibitem{deng2020adaptive}
Yuyang Deng, Mohammad~Mahdi Kamani, and Mehrdad Mahdavi.
\newblock Adaptive personalized federated learning.
\newblock {\em arXiv preprint arXiv:2003.13461}, 2020.

\bibitem{diakonikolas2018lower}
Jelena Diakonikolas and Crist{\'o}bal Guzm{\'a}n.
\newblock Lower bounds for parallel and randomized convex optimization.
\newblock {\em arXiv preprint arXiv:1811.01903}, 2018.

\bibitem{duchi2018minimax}
John Duchi, Feng Ruan, and Chulhee Yun.
\newblock Minimax bounds on stochastic batched convex optimization.
\newblock In {\em Conference On Learning Theory}, pages 3065--3162, 2018.

\bibitem{eichner2019semi}
Hubert Eichner, Tomer Koren, H~Brendan McMahan, Nathan Srebro, and Kunal
  Talwar.
\newblock Semi-cyclic stochastic gradient descent.
\newblock {\em Proceedings of the 36th International Conference on Machine
  Learning}, 2019.

\bibitem{fallah2020personalized}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Personalized federated learning: A meta-learning approach.
\newblock {\em arXiv preprint arXiv:2002.07948}, 2020.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem{gorbunov2019optimal}
Eduard Gorbunov, Darina Dvinskikh, and Alexander Gasnikov.
\newblock Optimal decentralized distributed algorithms for stochastic convex
  optimization.
\newblock {\em arXiv preprint arXiv:1911.07363}, 2019.

\bibitem{guzman2015lower}
Crist{\'o}bal Guzm{\'a}n and Arkadi Nemirovski.
\newblock On lower complexity bounds for large-scale smooth convex
  optimization.
\newblock {\em Journal of Complexity}, 31(1):1--14, 2015.

\bibitem{hanzely2020variance}
Filip Hanzely, Dmitry Kovalev, and Peter Richtarik.
\newblock Variance reduced coordinate descent with acceleration: New method
  with a surprising application to finite-sum problems.
\newblock {\em arXiv preprint arXiv:2002.04670}, 2020.

\bibitem{hanzely2020federated}
Filip Hanzely and Peter Richt{\'a}rik.
\newblock Federated learning of a mixture of global and local models.
\newblock {\em arXiv preprint arXiv:2002.05516}, 2020.

\bibitem{hard2018federated}
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran{\c{c}}oise
  Beaufays, Sean Augenstein, Hubert Eichner, Chlo{\'e} Kiddon, and Daniel
  Ramage.
\newblock Federated learning for mobile keyboard prediction.
\newblock {\em arXiv preprint arXiv:1811.03604}, 2018.

\bibitem{hendrikx2020optimal}
Hadrien Hendrikx, Francis Bach, and Laurent Massoulie.
\newblock An optimal algorithm for decentralized finite sum optimization.
\newblock {\em arXiv preprint arXiv:2005.10675}, 2020.

\bibitem{FL-big}
Peter Kairouz, H.~Brendan McMahan, and et~al.
\newblock Advances and open problems in federated learning.
\newblock {\em arXiv preprint arXiv:1912.04977v1}, 2019.

\bibitem{khodak2019adaptive}
Mikhail Khodak, Maria-Florina~F Balcan, and Ameet~S Talwalkar.
\newblock Adaptive gradient-based meta-learning methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5915--5926, 2019.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\'y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{kulkarni2020survey}
Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant.
\newblock Survey of personalization techniques for federated learning.
\newblock {\em arXiv preprint arXiv:2003.08673}, 2020.

\bibitem{lan2018communication}
Guanghui Lan, Soomin Lee, and Yi~Zhou.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock {\em Mathematical Programming}, pages 1--48, 2018.

\bibitem{lan2018optimal}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock {\em Mathematical programming}, 171(1-2):167--215, 2018.

\bibitem{li2018federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock {\em arXiv preprint arXiv:1812.06127}, 2018.

\bibitem{lin2015accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated randomized proximal coordinate gradient method and its
  application to regularized empirical risk minimization.
\newblock {\em SIAM Journal on Optimization}, 25(4):2244--2273, 2015.

\bibitem{liu2017distributed}
Sulin Liu, Sinno~Jialin Pan, and Qirong Ho.
\newblock Distributed multi-task relationship learning.
\newblock In {\em Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 937--946, 2017.

\bibitem{mansour2020three}
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda~Theertha Suresh.
\newblock Three approaches for personalization with applications to federated
  learning.
\newblock {\em arXiv preprint arXiv:2002.10619}, 2020.

\bibitem{mcmahan17a}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock {Communication-Efficient Learning of Deep Networks from Decentralized
  Data}.
\newblock In {\em Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, pages 1273--1282. PMLR, 2017.

\bibitem{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock {\em arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{nemirovski1994parallel}
Arkadi Nemirovski.
\newblock On parallel complexity of nonsmooth convex optimization.
\newblock {\em Journal of Complexity}, 10(4):451--463, 1994.

\bibitem{nemirovski1985optimal}
Arkadi~S Nemirovski and Yurii Nesterov.
\newblock Optimal methods of smooth convex minimization.
\newblock {\em Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  25(3):356--369, 1985.

\bibitem{nemirovsky1983problem}
Arkadi{\u\i}~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem{nesterov2018lectures}
Yurii Nesterov.
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem{nesterov1983method}
Yurii~E Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In {\em Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem{nguyen2019tight}
Phuong-Ha Ngyuen, Lam Nguyen, and Marten van Dijk.
\newblock Tight dimension independent lower bound on the expected convergence
  rate for diminishing step sizes in {SGD}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3660--3669, 2019.

\bibitem{pathak2020fedsplit}
Reese Pathak and Martin~J Wainwright.
\newblock {FedSplit}: An algorithmic framework for fast federated optimization.
\newblock {\em arXiv preprint arXiv:2005.05238}, 2020.

\bibitem{peterson2019private}
Daniel Peterson, Pallika Kanani, and Virendra~J Marathe.
\newblock Private federated learning with domain adaptation.
\newblock {\em arXiv preprint arXiv:1912.06733}, 2019.

\bibitem{qian2019svrg}
Xun Qian, Zheng Qu, and Peter Richt{\'a}rik.
\newblock L-{SVRG} and {L-Katyusha} with arbitrary sampling.
\newblock {\em arXiv preprint arXiv:1906.01481}, 2019.

\bibitem{raginsky2011information}
Maxim Raginsky and Alexander Rakhlin.
\newblock Information-based complexity, feedback and dynamics in convex
  programming.
\newblock {\em IEEE Transactions on Information Theory}, 57(10):7036--7056,
  2011.

\bibitem{scaman2018optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Laurent Massouli{\'e}, and
  Yin~Tat Lee.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2740--2749, 2018.

\bibitem{schmidt2011convergence}
Mark Schmidt, Nicolas~L Roux, and Francis~R Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock In {\em Advances in neural information processing systems}, pages
  1458--1466, 2011.

\bibitem{shalev2014accelerated}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In {\em International Conference on Machine Learning}, pages 64--72,
  2014.

\bibitem{smith2017interaction}
Adam Smith, Abhradeep Thakurta, and Jalaj Upadhyay.
\newblock Is interaction necessary for distributed private learning?
\newblock In {\em 2017 IEEE Symposium on Security and Privacy (SP)}, pages
  58--77. IEEE, 2017.

\bibitem{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet~S Talwalkar.
\newblock Federated multi-task learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4424--4434, 2017.

\bibitem{vanhaesebrouck2016decentralized}
Paul Vanhaesebrouck, Aur{\'e}lien Bellet, and Marc Tommasi.
\newblock Decentralized collaborative learning of personalized models over
  networks.
\newblock {\em International Conference on Artificial Intelligence and
  Statistics}, pages 509--517, 2017.

\bibitem{wang2018distributed}
Weiran Wang, Jialei Wang, Mladen Kolar, and Nathan Srebro.
\newblock Distributed stochastic multi-task learning with graph regularization.
\newblock {\em arXiv preprint arXiv:1802.03830}, 2018.

\bibitem{woodworth2016tight}
Blake~E Woodworth and Nati Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock In {\em Advances in neural information processing systems}, pages
  3639--3647, 2016.

\bibitem{woodworth2018graph}
Blake~E Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock In {\em Advances in neural information processing systems}, pages
  8496--8506, 2018.

\bibitem{wu2020personalized}
Qiong Wu, Kaiwen He, and Xu~Chen.
\newblock Personalized federated learning for intelligent {IoT} applications: A
  cloud-edge based framework.
\newblock {\em arXiv preprint arXiv:2002.10671}, 2020.

\bibitem{zhao2018federated}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock {\em arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{zhou2018direct}
Kaiwen Zhou.
\newblock Direct acceleration of {SAGA} using sampled negative momentum.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2018.

\bibitem{zhou2018simple}
Kaiwen Zhou, Fanhua Shang, and James Cheng.
\newblock A simple stochastic variance reduced algorithm with fast convergence
  rates.
\newblock In {\em The 35th International Conference on Machine Learning}, 2018.

\end{thebibliography}
