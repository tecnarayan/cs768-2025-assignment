\begin{thebibliography}{10}

\bibitem{Achiam2017ConstrainedPO}
J.~Achiam, D.~Held, A.~Tamar, and P.~Abbeel.
\newblock {Constrained Policy Optimization}.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{Agarwal2021DeepRL}
R.~Agarwal, M.~Schwarzer, P.~S. Castro, A.~C. Courville, and M.~G. Bellemare.
\newblock {Deep Reinforcement Learning at the Edge of the Statistical
  Precipice}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{ajay2021opal}
A.~Ajay, A.~Kumar, P.~Agrawal, S.~Levine, and O.~Nachum.
\newblock {OPAL: Offline Primitive Discovery for Accelerating Offline
  Reinforcement Learning}.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{bai2022pessimistic}
C.~Bai, L.~Wang, Z.~Yang, Z.-H. Deng, A.~Garg, P.~Liu, and Z.~Wang.
\newblock {Pessimistic Bootstrapping for Uncertainty-Driven Offline
  Reinforcement Learning}.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Bang2021MGGANSM}
D.~Bang and H.~Shim.
\newblock {MGGAN: Solving Mode Collapse Using Manifold-Guided Training}.
\newblock {\em IEEE/CVF International Conference on Computer Vision Workshops},
  pages 2347--2356, 2021.

\bibitem{Bau2019SeeingWA}
D.~Bau, J.-Y. Zhu, J.~Wulff, W.~S. Peebles, H.~Strobelt, B.~Zhou, and
  A.~Torralba.
\newblock {Seeing What a GAN Cannot Generate}.
\newblock In {\em IEEE/CVF International Conference on Computer Vision}, pages
  4501--4510, 2019.

\bibitem{Chen2021DecisionTR}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch.
\newblock Decision {T}ransformer: {R}einforcement {L}earning via {S}equence
  {M}odeling.
\newblock {\em ArXiv}, abs/2106.01345, 2021.

\bibitem{Chen2020BAILBI}
X.~Chen, Z.~Zhou, Z.~Wang, C.~Wang, Y.~Wu, Q.~Deng, and K.~W. Ross.
\newblock {BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement
  Learning}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Csiszr2011InformationT}
I.~Csisz{\'a}r and J.~K{\"o}rner.
\newblock {\em {Information Theory - Coding Theorems for Discrete Memoryless
  Systems, Second Edition}}.
\newblock Cambridge University Press, 2011.

\bibitem{Diehl2021UMBRELLAUM}
C.~P. Diehl, T.~Sievernich, M.~Kr{\"u}ger, F.~Hoffmann, and T.~Bertram.
\newblock U{MBRELLA}: {U}ncertainty-{A}ware {M}odel-{B}ased {O}ffline
  {R}einforcement {L}earning {L}everaging {P}lanning.
\newblock {\em ArXiv}, abs/2111.11097, 2021.

\bibitem{Fakoor2021ContinuousDC}
R.~Fakoor, J.~Mueller, P.~Chaudhari, and A.~Smola.
\newblock {Continuous Doubly Constrained Batch Reinforcement Learning}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{Fu2020D4RLDF}
J.~Fu, A.~Kumar, O.~Nachum, G.~Tucker, and S.~Levine.
\newblock D4{RL}: {D}atasets for {D}eep {D}ata-{D}riven {R}einforcement
  {L}earning.
\newblock {\em ArXiv}, abs/2004.07219, 2020.

\bibitem{Fujimoto2021AMA}
S.~Fujimoto and S.~S. Gu.
\newblock A {M}inimalist {A}pproach to {O}ffline {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{Fujimoto2019OffPolicyDR}
S.~Fujimoto, D.~Meger, and D.~Precup.
\newblock Off-{P}olicy {D}eep {R}einforcement {L}earning without {E}xploration.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{Fujimoto2018AddressingFA}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing {F}unction {A}pproximation {E}rror in {A}ctor-{C}ritic
  {M}ethods.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{Gelada2019OffPolicyDR}
C.~Gelada and M.~G. Bellemare.
\newblock Off-{P}olicy {D}eep {R}einforcement {L}earning by {B}ootstrapping the
  {C}ovariate {S}hift.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~33, 2019.

\bibitem{Ghasemipour2021EMaQEQ}
S.~K.~S. Ghasemipour, D.~Schuurmans, and S.~S. Gu.
\newblock {EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective
  Offline and Online RL}.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{Ghosh2022OfflineRP}
D.~Ghosh, A.~Ajay, P.~Agrawal, and S.~Levine.
\newblock {Offline RL Policies Should be Trained to be Adaptive}.
\newblock {\em ArXiv}, abs/2207.02200, 2022.

\bibitem{Goodfellow2014GenerativeAN}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~C. Courville, and Y.~Bengio.
\newblock {Generative Adversarial Nets}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2014.

\bibitem{Haarnoja2018SoftAO}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft {A}ctor-{C}ritic: {O}ff-{P}olicy {M}aximum {E}ntropy {D}eep
  {R}einforcement {L}earning with a {S}tochastic {A}ctor.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{Hester2018DeepQF}
T.~Hester, M.~Vecer{\'i}k, O.~Pietquin, M.~Lanctot, T.~Schaul, B.~Piot,
  D.~Horgan, J.~Quan, A.~Sendonaris, I.~Osband, G.~Dulac-Arnold, J.~P. Agapiou,
  J.~Z. Leibo, and A.~Gruslys.
\newblock {Deep Q-learning From Demonstrations}.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{Janner2019WhenTT}
M.~Janner, J.~Fu, M.~Zhang, and S.~Levine.
\newblock When to {T}rust {Y}our {M}odel: {M}odel-{B}ased {P}olicy
  {O}ptimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{Janner2021ReinforcementLA}
M.~Janner, Q.~Li, and S.~Levine.
\newblock Offline {R}einforcement {L}earning as {O}ne {B}ig {S}equence
  {M}odeling {P}roblem.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{jin2021pessimism}
Y.~Jin, Z.~Yang, and Z.~Wang.
\newblock {Is Pessimism Provably Efficient for Offline RL?}
\newblock In {\em International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem{Kakade2002ApproximatelyOA}
S.~M. Kakade and J.~Langford.
\newblock {Approximately Optimal Approximate Reinforcement Learning}.
\newblock In {\em International Conference on Machine Learning}, 2002.

\bibitem{Kang2018PolicyOW}
B.~Kang, Z.~Jie, and J.~Feng.
\newblock {Policy Optimization with Demonstrations}.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{Kidambi2020MOReLM}
R.~Kidambi, A.~Rajeswaran, P.~Netrapalli, and T.~Joachims.
\newblock {MOReL}: {M}odel-{B}ased {O}ffline {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Kingma2015AdamAM}
D.~P. Kingma and J.~Ba.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In {\em International Conference on Learning Representation}, 2015.

\bibitem{Kingma2014AutoEncodingVB}
D.~P. Kingma and M.~Welling.
\newblock Auto-{E}ncoding {V}ariational {B}ayes.
\newblock {\em ArXiv}, abs/1312.6114, 2013.

\bibitem{Konda1999ActorCriticT}
V.~R. Konda and V.~S. Borkar.
\newblock {Actor-Critic - Type Learning Algorithms for Markov Decision
  Processes}.
\newblock {\em SIAM J. Control. Optim.}, 38:94--123, 1999.

\bibitem{Konda1999ActorCriticA}
V.~R. Konda and J.~N. Tsitsiklis.
\newblock {Actor-Critic Algorithms}.
\newblock In {\em Advances in Neural Information Processing Systems}, 1999.

\bibitem{kostrikov2022offline}
I.~Kostrikov, A.~Nair, and S.~Levine.
\newblock {Offline Reinforcement Learning with Implicit Q-Learning}.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Kostrikov2021OfflineRL}
I.~Kostrikov, J.~Tompson, R.~Fergus, and O.~Nachum.
\newblock Offline {R}einforcement {L}earning with {F}isher {D}ivergence
  {C}ritic {R}egularization.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{Kumar2019StabilizingOQ}
A.~Kumar, J.~Fu, G.~Tucker, and S.~Levine.
\newblock Stabilizing {O}ff-{P}olicy {Q}-{L}earning via {B}ootstrapping {E}rror
  {R}eduction.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{Kumar2020ConservativeQF}
A.~Kumar, A.~Zhou, G.~Tucker, and S.~Levine.
\newblock Conservative {Q}-{L}earning for {O}ffline {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Lange2012BatchRL}
S.~Lange, T.~Gabel, and M.~A. Riedmiller.
\newblock Batch {R}einforcement {L}earning.
\newblock In {\em Reinforcement Learning}, 2012.

\bibitem{lee2021representation}
B.-J. Lee, J.~Lee, and K.-E. Kim.
\newblock Representation {B}alancing {O}ffline {M}odel-based {R}einforcement
  {L}earning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Lee2021OfflinetoOnlineRL}
S.~Lee, Y.~Seo, K.~Lee, P.~Abbeel, and J.~Shin.
\newblock {Offline-to-Online Reinforcement Learning via Balanced Replay and
  Pessimistic Q-Ensemble}.
\newblock In {\em Conference on Robot Learning}, 2021.

\bibitem{Liu2019OffPolicyPG}
Y.~Liu, A.~Swaminathan, A.~Agarwal, and E.~Brunskill.
\newblock Off-{P}olicy {P}olicy {G}radient with {S}tate {D}istribution
  {C}orrection.
\newblock {\em ArXiv}, abs/1904.08473, 2019.

\bibitem{ma2022offline}
X.~Ma, Y.~Yang, H.~Hu, J.~Yang, C.~Zhang, Q.~Zhao, B.~Liang, and Q.~Liu.
\newblock {Offline Reinforcement Learning with Value-based Episodic Memory}.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Ma2021ConservativeOD}
Y.~J. Ma, D.~Jayaraman, and O.~Bastani.
\newblock Conservative {O}ffline {D}istributional {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem{matsushima2021deploymentefficient}
T.~Matsushima, H.~Furuta, Y.~Matsuo, O.~Nachum, and S.~Gu.
\newblock Deployment-{E}fficient {R}einforcement {L}earning via {M}odel-{B}ased
  {O}ffline {O}ptimization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Meng2021OfflinePM}
L.~Meng, M.~Wen, Y.~Yang, C.~Le, X.~Li, W.~Zhang, Y.~Wen, H.~Zhang, J.~Wang,
  and B.~Xu.
\newblock Offline {P}re-trained {M}ulti-{A}gent {D}ecision {T}ransformer: {O}ne
  {B}ig {S}equence {M}odel {T}ackles {A}ll {SMAC} {T}asks.
\newblock {\em ArXiv}, abs/2112.02845, 2021.

\bibitem{Nachum2019DualDICEBE}
O.~Nachum, Y.~Chow, B.~Dai, and L.~Li.
\newblock Dual{DICE}: {B}ehavior-{A}gnostic {E}stimation of {D}iscounted
  {S}tationary {D}istribution {C}orrections.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{Nair2020AcceleratingOR}
A.~Nair, M.~Dalal, A.~Gupta, and S.~Levine.
\newblock Accelerating {O}nline {R}einforcement {L}earning with {O}ffline
  {D}atasets.
\newblock {\em ArXiv}, abs/2006.09359, 2020.

\bibitem{Ovadia2019CanYT}
Y.~Ovadia, E.~Fertig, J.~Ren, Z.~Nado, D.~Sculley, S.~Nowozin, J.~V. Dillon,
  B.~Lakshminarayanan, and J.~Snoek.
\newblock Can {Y}ou {T}rust {Y}our {M}odel's {U}ncertainty? {E}valuating
  {P}redictive {U}ncertainty {U}nder {D}ataset {S}hift.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{Precup2001OffPolicyTD}
D.~Precup, R.~S. Sutton, and S.~Dasgupta.
\newblock Off-{P}olicy {T}emporal {D}ifference {L}earning with {F}unction
  {A}pproximation.
\newblock In {\em International Conference on Machine Learning}, 2001.

\bibitem{Rafailov2021OfflineRL}
R.~Rafailov, T.~Yu, A.~Rajeswaran, and C.~Finn.
\newblock Offline {R}einforcement {L}earning from {I}mages with {L}atent
  {S}pace {M}odels.
\newblock In {\em Learning for Dynamics and Control (L4DC)}, 2021.

\bibitem{Rajeswaran2018LearningCD}
A.~Rajeswaran, V.~Kumar, A.~Gupta, J.~Schulman, E.~Todorov, and S.~Levine.
\newblock Learning {C}omplex {D}exterous {M}anipulation with {D}eep
  {R}einforcement {L}earning and {D}emonstrations.
\newblock {\em ArXiv}, abs/1709.10087, 2018.

\bibitem{rashidinejad2021bridging}
P.~Rashidinejad, B.~Zhu, C.~Ma, J.~Jiao, and S.~Russell.
\newblock {Bridging Offline reinforcement Learning and Imitation Learning: A
  Tale of Pessimism}.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{Sohn2015LearningSO}
K.~Sohn, H.~Lee, and X.~Yan.
\newblock Learning {S}tructured {O}utput {R}epresentation using {D}eep
  {C}onditional {G}enerative {M}odels.
\newblock In {\em Advances in Neural Information Processing Systems}, 2015.

\bibitem{Srivastava2017VEEGANRM}
A.~Srivastava, L.~Valkov, C.~Russell, M.~U. Gutmann, and C.~Sutton.
\newblock {VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational
  Learning}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock {\em {Reinforcement learning: An introduction}}.
\newblock MIT press, 2018.

\bibitem{Sutton2016AnEA}
R.~S. Sutton, A.~R. Mahmood, and M.~White.
\newblock An {E}mphatic {A}pproach to the {P}roblem of {O}ff-policy
  {T}emporal-{D}ifference {L}earning.
\newblock {\em Journal of Machine Learning Research}, 17:2603--2631, 2016.

\bibitem{Wang2021OfflineRL}
J.~Wang, W.~Li, H.~Jiang, G.~Zhu, S.~Li, and C.~Zhang.
\newblock Offline {R}einforcement {L}earning with {R}everse {M}odel-based
  {I}magination.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{Wang2018ExponentiallyWI}
Q.~Wang, J.~Xiong, L.~Han, P.~Sun, H.~Liu, and T.~Zhang.
\newblock {Exponentially Weighted Imitation Learning for Batched Historical
  Data}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{Wang2020CriticRR}
Z.~Wang, A.~Novikov, K.~Zolna, J.~T. Springenberg, S.~E. Reed, B.~Shahriari,
  N.~Siegel, J.~Merel, C.~Gulcehre, N.~M.~O. Heess, and N.~de~Freitas.
\newblock {Critic Regularized Regression}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Wu2019BehaviorRO}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock Behavior {R}egularized {O}ffline {R}einforcement {L}earning.
\newblock {\em ArXiv}, abs/1911.11361, 2019.

\bibitem{Wu2021UncertaintyWA}
Y.~Wu, S.~Zhai, N.~Srivastava, J.~M. Susskind, J.~Zhang, R.~Salakhutdinov, and
  H.~Goh.
\newblock Uncertainty {W}eighted {A}ctor-{C}ritic for {O}ffline {R}einforcement
  {L}earning.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{xie2021bellman}
T.~Xie, C.-A. Cheng, N.~Jiang, P.~Mineiro, and A.~Agarwal.
\newblock {Bellman-Consistent Pessimism for Offline Reinforcement Learning}.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{Yang2022ARI}
S.~Yang, Z.~Wang, H.~Zheng, Y.~Feng, and M.~Zhou.
\newblock {A Regularized Implicit Policy for Offline Reinforcement Learning}.
\newblock {\em ArXiv}, abs/2202.09673, 2022.

\bibitem{yang2021believe}
Y.~Yang, X.~Ma, L.~Chenghao, Z.~Zheng, Q.~Zhang, G.~Huang, J.~Yang, and
  Q.~Zhao.
\newblock {Believe what you see: Implicit constraint approach for offline
  multi-agent reinforcement learning}.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{Yu2021COMBOCO}
T.~Yu, A.~Kumar, R.~Rafailov, A.~Rajeswaran, S.~Levine, and C.~Finn.
\newblock {COMBO}: {C}onservative {O}ffline {M}odel-{B}ased {P}olicy
  {O}ptimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{Yu2020MOPOMO}
T.~Yu, G.~Thomas, L.~Yu, S.~Ermon, J.~Y. Zou, S.~Levine, C.~Finn, and T.~Ma.
\newblock {MOPO}: {M}odel-based {O}ffline {P}olicy {O}ptimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Zanette2021ProvableBO}
A.~Zanette, M.~J. Wainwright, and E.~Brunskill.
\newblock Provable {B}enefits of {A}ctor-{C}ritic {M}ethods for {O}ffline
  {R}einforcement {L}earning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{zhao2022adaptive}
Y.~Zhao, R.~Boney, A.~Ilin, J.~Kannala, and J.~Pajarinen.
\newblock {Adaptive Behavior Cloning Regularization for Stable
  Offline-to-Online Reinforcement Learning}, 2022.

\bibitem{Zhou2020PLASLA}
W.~Zhou, S.~Bajracharya, and D.~Held.
\newblock P{LAS}: {L}atent {A}ction {S}pace for {O}ffline {R}einforcement
  {L}earning.
\newblock In {\em Conference on Robot Learning}, 2020.

\end{thebibliography}
