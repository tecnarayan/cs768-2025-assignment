\begin{thebibliography}{1}

\bibitem{beltagy2019scibert}
Iz~Beltagy, Kyle Lo, and Arman Cohan.
\newblock {SciBERT}: A pretrained language model for scientific text.
\newblock In {\em EMNLP}. ACL, 2019.

\bibitem{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In {\em NeurIPS}, 2018.

\bibitem{guo2021tight}
Qing Guo, Junya Chen, Dong Wang, Yuewei Yang, Xinwei Deng, Fan Li, Lawrence
  Carin, and Chenyang Tao.
\newblock Tight mutual information estimation with contrastive
  {Fenchel-Legendre} optimization, 2021.
\newblock [Available online; accessed 28-May-2021].

\bibitem{hyvarinen2019nonlinear}
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner.
\newblock Nonlinear ica using auxiliary variables and generalized contrastive
  learning.
\newblock In {\em AISTATS}, pages 859--868, 2019.

\bibitem{li1991sliced}
Ker-Chau Li.
\newblock Sliced inverse regression for dimension reduction.
\newblock {\em Journal of the American Statistical Association},
  86(414):316--327, 1991.

\bibitem{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em NIPS}, 2017.

\bibitem{rezende2015variational}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em ICML}, 2015.

\bibitem{robinson2020strength}
Joshua Robinson, Stefanie Jegelka, and Suvrit Sra.
\newblock Strength from weakness: Fast learning using weak supervision.
\newblock In {\em ICML}, 2020.

\bibitem{teshima2020few}
Takeshi Teshima, Issei Sato, and Masashi Sugiyama.
\newblock Few-shot domain adaptation by causal mechanism transfer.
\newblock In {\em ICML}, 2020.

\end{thebibliography}
