
@InProceedings{pmlr-v133-sazanovich21a,
  title = 	 {Solving Black-Box Optimization Challenge via Learning Search Space Partition for Local Bayesian Optimization},
  author =       {Sazanovich, Mikita and Nikolskaya, Anastasiya and Belousov, Yury and Shpilman, Aleksei},
  booktitle = 	 {Proceedings of the NeurIPS 2020 Competition and Demonstration Track},
  pages = 	 {77--85},
  year = 	 {2021},
  editor = 	 {Escalante, Hugo Jair and Hofmann, Katja},
  volume = 	 {133},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--12 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v133/sazanovich21a/sazanovich21a.pdf},
  @url = 	 {https://proceedings.mlr.press/v133/sazanovich21a.html},
  abstract = 	 {Black-box optimization is one of the vital tasks in machine learning, since it approximates real-world conditions, in that we do not always know all the properties of a given system, up to knowing almost nothing but the results. This paper describes our approach to solving the black-box optimization challenge at NeurIPS 2020 through learning search space partition for local Bayesian optimization. We describe the task of the challenge as well as our algorithm for low budget optimization that we named SPBOpt. We optimize the hyper-parameters of our algorithm for the competition finals using multi-task Bayesian optimization on results from the first two evaluation settings. Our approach has ranked third in the competition finals.}
}

@inproceedings{eriksson2019scalable,
  title = {Scalable Global Optimization via Local {Bayesian} Optimization},
  author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {5496--5507},
  year = {2019},
  @url = {http://papers.nips.cc/paper/8788-scalable-global-optimization-via-local-bayesian-optimization.pdf},
}

@article{wang2020learning,
  title={Learning search space partition for black-box optimization using monte carlo tree search},
  author={Wang, Linnan and Fonseca, Rodrigo and Tian, Yuandong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19511--19522},
  year={2020},
  @url = {https://proceedings.neurips.cc/paper/2020/file/e2ce14e81dba66dbff9cbc35ecfdb704-Paper.pdf}
}


@InProceedings{pmlr-v37-sui15,
  title = 	 {Safe Exploration for Optimization with Gaussian Processes},
  author = 	 {Sui, Yanan and Gotovos, Alkis and Burdick, Joel and Krause, Andreas},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {997--1005},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sui15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sui15.html},
  abstract = 	 {We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified "safety" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.}
}

@inproceedings{10.5555/2540128.2540322,
author = {Gotovos, Alkis and Casati, Nathalie and Hitz, Gregory and Krause, Andreas},
    title = {Active Learning for Level Set Estimation},
    year = {2013},
    isbn = {9781577356332},
    publisher = {AAAI Press},
    abstract = {Many information gathering problems require determining the set of points, for which an unknown function takes value above or below some given threshold level. We formalize this task as a classification problem with sequential measurements, where the unknown function is modeled as a sample from a Gaussian process (GP). We propose LSE, an algorithm that guides both sampling and classification based on GP-derived confidence bounds, and provide theoretical guarantees about its sample complexity. Furthermore, we extend LSE and its theory to two more natural settings: (1) where the threshold level is implicitly defined as a percentage of the (unknown) maximum of the target function and (2) where samples are selected in batches. We evaluate the effectiveness of our proposed methods on two problems of practical interest, namely autonomous monitoring of algal populations in a lake environment and geolocating network latency.},
    booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
    pages = {1344–1350},
    numpages = {7},
    location = {Beijing, China},
    @url = {https://dl.acm.org/doi/10.5555/2540128.2540322},
    series = {IJCAI '13}
}

@article{makarova2021risk,
  title={Risk-averse Heteroscedastic Bayesian Optimization},
  author={Makarova, Anastasia and Usmanova, Ilnura and Bogunovic, Ilija and Krause, Andreas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{JMLR:v22:18-220,
  author  = {Erich Merrill and Alan Fern and Xiaoli Fern and Nima Dolatnia},
  title   = {An Empirical Study of Bayesian Optimization: Acquisition Versus Partition},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {4},
  pages   = {1--25},
  @url     = {http://jmlr.org/papers/v22/18-220.html}
}

@inproceedings{10.1145/1015330.1015367,
author = {Gramacy, Robert B. and Lee, Herbert K. H. and Macready, William G.},
title = {Parameter Space Exploration with Gaussian Process Trees},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015367},
doi = {10.1145/1015330.1015367},
abstract = {Computer experiments often require dense sweeps over input parameters to obtain a qualitative understanding of their response. Such sweeps can be prohibitively expensive, and are unnecessary in regions where the response is easy predicted; well-chosen designs could allow a mapping of the response with far fewer simulation runs. Thus, there is a need for computationally inexpensive surrogate models and an accompanying method for selecting small designs. We explore a general methodology for addressing this need that uses non-stationary Gaussian processes. Binary trees partition the input space to facilitate non-stationarity and a Bayesian interpretation provides an explicit measure of predictive uncertainty that can be used to guide sampling. Our methods are illustrated on several examples, including a motivating example involving computational fluid dynamics simulation of a NASA reentry vehicle.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {45},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{cheng2019regression,
  title={Regression clustering for improved accuracy and training costs with molecular-orbital-based machine learning},
  author={Cheng, Lixue and Kovachki, Nikola B and Welborn, Matthew and Miller III, Thomas F},
  journal={Journal of chemical theory and computation},
  volume={15},
  number={12},
  pages={6668--6677},
  year={2019},
  publisher={ACS Publications},
  url = {https://pubs.acs.org/doi/pdf/10.1021/acs.jctc.9b00884}
}

@ARTICLE{7352306,
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  journal={Proceedings of the IEEE}, 
  title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, 
  year={2016},
  volume={104},
  number={1},
  pages={148-175},
  doi={10.1109/JPROC.2015.2494218}}
  
@BOOK{8187198,
  author={Munos, Rémi},
  title={From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning},
  year={2014},
  volume={},
  number={},
  pages={},
  abstract={From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning covers several aspects of the "optimism in the face of uncertainty" principle for large scale optimization problems under finite numerical budget. The monograph's initial motivation came from the empirical success of the so-called "Monte-Carlo Tree Search" method popularized in Computer Go and further extended to many other games as well as optimization and planning problems. It lays out the theoretical foundations of the field by characterizing the complexity of the optimization problems and designing efficient algorithms with performance guarantees. The main direction followed in this monograph consists in decomposing a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a stochastic "multi-armed bandit" (mathematical model for decision making in stochastic environments). This defines a hierarchical search which possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing, at different scales, on the most promising areas (using the optimistic principle) until eventually performing a local search around the global optima of the function. This monograph considers the problem of function optimization in general search spaces (such as metric spaces, structured spaces, trees, and graphs) as well as the problem of planning in Markov decision processes. Its main contribution is a class of hierarchical optimistic algorithms with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the local ''smoothness'' of the function around the global maximum is known or unknown.},
  keywords={},
  doi={},
  ISSN={},
  publisher={now},
  isbn={},
  @url={https://ieeexplore.ieee.org/document/8187198},}
  
  @article{kawaguchi2016global,
  title={Global continuous optimization with error bound and fast convergence},
  author={Kawaguchi, Kenji and Maruyama, Yu and Zheng, Xiaoyu},
  journal={Journal of Artificial Intelligence Research},
  volume={56},
  pages={153--195},
  year={2016},
  @url = {https://www.jair.org/index.php/jair/article/view/11007/26166}
}

@inproceedings{snoek2012practical,
  title={Practical Bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={26th Annual Conference on Neural Information Processing Systems 2012},
  pages={2951--2959},
  year={2012}
}

@article{yang2019machine,
  title={Machine-learning-guided directed evolution for protein engineering},
  author={Yang, Kevin K and Wu, Zachary and Arnold, Frances H},
  journal={Nature methods},
  volume={16},
  number={8},
  pages={687--694},
  year={2019},
  publisher={Nature Publishing Group}
}

@book{rasmussen:williams:2006,
  added-at = {2009-03-05T08:49:50.000+0100},
  author = {Rasmussen, C. E. and Williams, C. K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/26771eaebbee7d852934f29aa33dea971/bcao},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {6771eaebbee7d852934f29aa33dea971},
  keywords = {},
  publisher = {MIT Press},
  timestamp = {2009-03-05T08:49:50.000+0100},
  title = {Gaussian Processes for Machine Learning},
  year = 2006,
  @URL={https://bit.ly/2tYpBix}
}

@article{srinivas2009gaussian,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  journal={arXiv preprint arXiv:0912.3995},
  year={2009}
}

@inproceedings{wang2017max,
  title={Max-value entropy search for efficient Bayesian optimization},
  author={Wang, Zi and Jegelka, Stefanie},
  booktitle={International Conference on Machine Learning},
  pages={3627--3635},
  year={2017},
  organization={PMLR}
}

@inproceedings{wang2016optimization,
  title={Optimization as estimation with Gaussian processes in bandit settings},
  author={Wang, Zi and Zhou, Bolei and Jegelka, Stefanie},
  booktitle={Artificial Intelligence and Statistics},
  pages={1022--1031},
  year={2016},
  organization={PMLR}
}

@article{bengio2005curse,
  title={The curse of dimensionality for local kernel machines},
  author={Bengio, Yoshua and Delalleau, Olivier and Le Roux, Nicolas},
  journal={Techn. Rep},
  volume={1258},
  pages={12},
  year={2005},
  publisher={Citeseer}
}

@article{djolonga2013high,
  title={High-dimensional gaussian process bandits},
  author={Djolonga, Josip and Krause, Andreas and Cevher, Volkan},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{kandasamy2015high,
  title={High dimensional Bayesian optimisation and bandits via additive models},
  author={Kandasamy, Kirthevasan and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  booktitle={International conference on machine learning},
  pages={295--304},
  year={2015},
  organization={PMLR}
}

@inproceedings{gardner2017discovering,
  title={Discovering and exploiting additive structure for Bayesian optimization},
  author={Gardner, Jacob and Guo, Chuan and Weinberger, Kilian and Garnett, Roman and Grosse, Roger},
  booktitle={Artificial Intelligence and Statistics},
  pages={1311--1319},
  year={2017},
  organization={PMLR}
}

@article{wang2016bayesian,
  title={Bayesian optimization in a billion dimensions via random embeddings},
  author={Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and de Feitas, Nando},
  journal={Journal of Artificial Intelligence Research},
  volume={55},
  pages={361--387},
  year={2016}
}

@inproceedings{sui2018stagewise,
  title={Stagewise safe bayesian optimization with gaussian processes},
  author={Sui, Yanan and Zhuang, Vincent and Burdick, Joel and Yue, Yisong},
  booktitle={International conference on machine learning},
  pages={4781--4789},
  year={2018},
  organization={PMLR}
}

@article{munos2011optimistic,
  title={Optimistic optimization of a deterministic function without the knowledge of its smoothness},
  author={Munos, R{\'e}mi},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@InProceedings{pmlr-v51-wilson16,
  title = 	 {Deep Kernel Learning},
  author = 	 {Andrew Gordon Wilson and Zhiting Hu and Ruslan Salakhutdinov and Eric P. Xing},
  pages = 	 {370--378},
  year = 	 {2016},
  editor = 	 {Arthur Gretton and Christian C. Robert},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/wilson16.pdf}},
}

@article{ferreira2020using,
  title={Using autoencoders as a weight initialization method on deep neural networks for disease detection},
  author={Ferreira, Mafalda Falc{\~a}o and Camacho, Rui and Teixeira, Lu{\'\i}s F},
  journal={BMC Medical Informatics and Decision Making},
  volume={20},
  number={5},
  pages={1--18},
  year={2020},
  publisher={Springer}
}

@article{chapelle2011empirical,
  title={An empirical evaluation of thompson sampling},
  author={Chapelle, Olivier and Li, Lihong},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{song2019general,
  title={A general framework for multi-fidelity bayesian optimization with gaussian processes},
  author={Song, Jialin and Chen, Yuxin and Yue, Yisong},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3158--3167},
  year={2019},
  organization={PMLR}
}

  
@Article{song:18.2,
  author = 	 {Jialin Song and Yury Tokpanov and Yuxin Chen and Dagny Fleischman and Kate Fountaine and Harry Atwater and Yisong Yue},
  title = 	 {Optimizing Photonic Nanostructures via Multi-fidelity Gaussian Processes},
  journal =  {NeurIPS Workshop on Machine Learning for Molecules and Materials},
  year = 	 {2018}
  }
  
 @misc{zhang20coflo,
  author       = {Fengxue Zhang, Yair Altas, Louise Fan, Kaustubh Vinchure, Brian Nord, Yuxin Chen},
  title        = {Design of Physical Experiments via Collision-Free Latent Space Optimization},
  howpublished = {NeurIPS Workshop on Machine Learning and the Physical Sciences},
  month        = December,
  year         = 2020
}


@article{zhang2022learning,
  title={Learning Representation for Bayesian Optimization with Collision-free Regularization},
  author={Zhang, Fengxue and Nord, Brian and Chen, Yuxin},
  journal={arXiv preprint arXiv:2203.08656},
  year={2022}
}

@inproceedings{Berkenkamp2016SafeOpt,
  title = {Safe Controller Optimization for Quadrotors with {G}aussian Processes},
  booktitle = {Proc. of the IEEE International Conference on Robotics and Automation (ICRA)},
  author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
  year = {2016},
  pages = {493--496},
  @url = {https://arxiv.org/abs/1509.01066}
}

@article{barlow2018flex,
  title={Flex ddG: Rosetta ensemble-based estimation of changes in protein--protein binding affinity upon mutation},
  author={Barlow, Kyle A and O Conchuir, Shane and Thompson, Samuel and Suresh, Pooja and Lucas, James E and Heinonen, Markus and Kortemme, Tanja},
  journal={The Journal of Physical Chemistry B},
  volume={122},
  number={21},
  pages={5389--5399},
  year={2018},
  publisher={ACS Publications}
}

@article{smith2008backrub,
  title={Backrub-like backbone simulation recapitulates natural protein conformational variability and improves mutant side-chain prediction},
  author={Smith, Colin A and Kortemme, Tanja},
  journal={Journal of molecular biology},
  volume={380},
  number={4},
  pages={742--756},
  year={2008},
  publisher={Elsevier}
}


@article{desautels2020rapid,
  title={Rapid in silico design of antibodies targeting SARS-CoV-2 using machine learning and supercomputing},
  author={Desautels, Thomas and Zemla, Adam and Lau, Edmond and Franco, Magdalena and Faissol, Daniel},
  journal={BioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{kirschner2019adaptive,
  title={Adaptive and safe Bayesian optimization in high dimensions via one-dimensional subspaces},
  author={Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and Ischebeck, Rasmus and Krause, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={3429--3438},
  year={2019},
  organization={PMLR}
}

@article{das2008macromolecular,
  title={Macromolecular modeling with rosetta},
  author={Das, Rhiju and Baker, David},
  journal={Annu. Rev. Biochem.},
  volume={77},
  pages={363--382},
  year={2008},
  publisher={Annual Reviews}
}


@article{ryzhov2016convergence,
  title={On the convergence rates of expected improvement methods},
  author={Ryzhov, Ilya O},
  journal={Operations Research},
  volume={64},
  number={6},
  pages={1515--1528},
  year={2016},
  publisher={INFORMS}
}

@phdthesis{frazier2009knowledge,
  title={Knowledge-gradient methods for statistical learning},
  author={Frazier, Peter I},
  year={2009},
  school={Citeseer}
}

@article{jones1998efficient,
  title={Efficient global optimization of expensive black-box functions},
  author={Jones, Donald R and Schonlau, Matthias and Welch, William J},
  journal={Journal of Global optimization},
  volume={13},
  number={4},
  pages={455--492},
  year={1998},
  publisher={Springer}
}

@article{bogunovic2016truncated,
  title={Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation},
  author={Bogunovic, Ilija and Scarlett, Jonathan and Krause, Andreas and Cevher, Volkan},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{wu2019machine,
  title={Machine learning-assisted directed protein evolution with combinatorial libraries},
  author={Wu, Zachary and Kan, SB Jennifer and Lewis, Russell D and Wittmann, Bruce J and Arnold, Frances H},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={18},
  pages={8852--8858},
  year={2019},
  publisher={National Acad Sciences}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{wistuba2021few,
  title={Few-shot Bayesian optimization with deep kernel surrogates},
  author={Wistuba, Martin and Grabocka, Josif},
  journal={arXiv preprint arXiv:2101.07667},
  year={2021}
}


@article{wabersich2016advancing,
  title={Advancing Bayesian optimization: The mixed-global-local (MGL) kernel and length-scale cool down},
  author={Wabersich, Kim Peter and Toussaint, Marc},
  journal={arXiv preprint arXiv:1612.03117},
  year={2016}
}

@article{salgia2021domain,
  title={A domain-shrinking based Bayesian optimization algorithm with order-optimal regret performance},
  author={Salgia, Sudeep and Vakili, Sattar and Zhao, Qing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28836--28847},
  year={2021}
}

@article{desautels2022computationally,
  title={Computationally restoring the potency of a clinical antibody against SARS-CoV-2 Omicron subvariants},
  author={Desautels, Thomas A and Arrildt, Kathryn T and Zemla, Adam T and Lau, Edmond Y and Zhu, Fangqiang and Ricci, Dante and Cronin, Stephanie and Zost, Seth and Binshtein, Elad and Scheaffer, Suzanne M and others},
  journal={bioRxiv},
  pages={2022--10},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{tripp2020sample,
  title={Sample-efficient optimization in the latent space of deep generative models via weighted retraining},
  author={Tripp, Austin and Daxberger, Erik and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11259--11272},
  year={2020}
}

@article{10.1145/3545611,
author = {Binois, Micka\"{e}l and Wycoff, Nathan},
title = {A Survey on High-Dimensional Gaussian Process Modeling with Application to Bayesian Optimization},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2688-299X},
url = {https://doi.org/10.1145/3545611},
doi = {10.1145/3545611},
abstract = {Bayesian Optimization (BO), the application of Bayesian function approximation to finding optima of expensive functions, has exploded in popularity in recent years. In particular, much attention has been paid to improving its efficiency on problems with many parameters to optimize. This attention has trickled down to the workhorse of high-dimensional BO, high-dimensional Gaussian process regression, which is also of independent interest. The great flexibility that the Gaussian process prior implies is a boon when modeling complicated, low-dimensional surfaces but simply says too little when dimension grows too large. A variety of structural model assumptions have been tested to tame high dimensions, from variable selection and additive decomposition to low-dimensional embeddings and beyond. Most of these approaches in turn require modifications of the acquisition function optimization strategy as well. Here, we review the defining structural model assumptions and discuss the benefits and drawbacks of these approaches in practice.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = {aug},
articleno = {8},
numpages = {26},
keywords = {active subspace, low-intrinsic dimensionality, additivity, variable selection, Black-box optimization}
}

@inproceedings{HeSBO19,
  author    = {Alex Munteanu and
               Amin Nayebi and
			   Matthias Poloczek},
  title     = {A Framework for Bayesian Optimization in Embedded Subspaces},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning, {(ICML)}},
  year      = {2019},
  note={Accepted for publication. The code is available at https://github.com/aminnayebi/HesBO.}
}

@inproceedings{Letham2020Re,
    author    = {Letham, Benjamin and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},        
    title     = {Re-Examining Linear Embeddings for High-Dimensional {B}ayesian Optimization},
    booktitle   = {Advances in Neural Information Processing Systems 33},
    year      = {2020},
    series = {NeurIPS},
}

@inproceedings{papenmeier2022increasing,
  title={Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces},
  author={Papenmeier, Leonard and Nardi, Luigi and Poloczek, Matthias},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{mcintire2016sparse,
  title={Sparse Gaussian Processes for Bayesian Optimization.},
  author={McIntire, Mitchell and Ratner, Daniel and Ermon, Stefano},
  booktitle={UAI},
  year={2016}
}

@inproceedings{moss2023inducing,
  title={Inducing point allocation for sparse gaussian processes in high-throughput bayesian optimisation},
  author={Moss, Henry B and Ober, Sebastian W and Picheny, Victor},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5213--5230},
  year={2023},
  organization={PMLR}
}

@article{song2022monte,
  title={Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization},
  author={Song, Lei and Xue, Ke and Huang, Xiaobin and Qian, Chao},
  journal={arXiv preprint arXiv:2210.01628},
  year={2022}
}

@inproceedings{wilson2015kernel,
  title={Kernel interpolation for scalable structured Gaussian processes (KISS-GP)},
  author={Wilson, Andrew and Nickisch, Hannes},
  booktitle={International conference on machine learning},
  pages={1775--1784},
  year={2015},
  organization={PMLR}
}