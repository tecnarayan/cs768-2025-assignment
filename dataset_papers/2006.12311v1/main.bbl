\begin{thebibliography}{42}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\text{Abbasi-Yadkori, Y.}, \text{P{\'a}l, D.} and \text{Szepesv{\'a}ri, C.}
  (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Auer and Ortner(2007)}]{auer2007logarithmic}
\text{Auer, P.} and \text{Ortner, R.} (2007).
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\text{Azar, M.~G.}, \text{Osband, I.} and \text{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Balke and Pearl(2013)}]{balke2013counterfactuals}
\text{Balke, A.} and \text{Pearl, J.} (2013).
\newblock Counterfactuals and policy analysis in structural models.
\newblock \textit{arXiv preprint arXiv:1302.4929}.

\bibitem[{Bradtke and Barto(1996)}]{bradtke1996linear}
\text{Bradtke, S.~J.} and \text{Barto, A.~G.} (1996).
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \textit{Machine {L}earning}, \textbf{22} 33--57.

\bibitem[{Buesing et~al.(2018)Buesing, Weber, Zwols, Racaniere, Guez, Lespiau
  and Heess}]{buesing2018woulda}
\text{Buesing, L.}, \text{Weber, T.}, \text{Zwols, Y.}, \text{Racaniere, S.},
  \text{Guez, A.}, \text{Lespiau, J.-B.} and \text{Heess, N.} (2018).
\newblock Woulda, coulda, shoulda: {C}ounterfactually-guided policy search.
\newblock \textit{arXiv preprint arXiv:1811.06272}.

\bibitem[{Cai et~al.(2019)Cai, Yang, Jin and Wang}]{cai2019provably}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Jin, C.} and \text{Wang, Z.} (2019).
\newblock Provably efficient exploration in policy optimization.
\newblock \textit{arXiv preprint arXiv:1912.05830}.

\bibitem[{Chakraborty and Murphy(2014)}]{chakraborty2014dynamic}
\text{Chakraborty, B.} and \text{Murphy, S.~A.} (2014).
\newblock Dynamic treatment regimes.
\newblock \textit{Annual {R}eview of {S}tatistics and {I}ts {A}pplication},
  \textbf{1} 447--464.

\bibitem[{de~Haan et~al.(2019)de~Haan, Jayaraman and Levine}]{de2019causal}
\text{de~Haan, P.}, \text{Jayaraman, D.} and \text{Levine, S.} (2019).
\newblock Causal confusion in imitation learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{D{\'\i}az and Hejazi(2019)}]{diaz2019causal}
\text{D{\'\i}az, I.} and \text{Hejazi, N.} (2019).
\newblock Causal mediation analysis for stochastic interventions.
\newblock \textit{arXiv preprint arXiv:1901.02776}.

\bibitem[{Forney et~al.(2017)Forney, Pearl and
  Bareinboim}]{forney2017counterfactual}
\text{Forney, A.}, \text{Pearl, J.} and \text{Bareinboim, E.} (2017).
\newblock Counterfactual data-fusion for online reinforcement learners.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Hallak et~al.(2015)Hallak, Di~Castro and
  Mannor}]{hallak2015contextual}
\text{Hallak, A.}, \text{Di~Castro, D.} and \text{Mannor, S.} (2015).
\newblock Contextual {M}arkov decision processes.
\newblock \textit{arXiv preprint arXiv:1502.02259}.

\bibitem[{Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar and Silver}]{hessel2018rainbow}
\text{Hessel, M.}, \text{Modayil, J.}, \text{Van~Hasselt, H.}, \text{Schaul,
  T.}, \text{Ostrovski, G.}, \text{Dabney, W.}, \text{Horgan, D.}, \text{Piot,
  B.}, \text{Azar, M.} and \text{Silver, D.} (2018).
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \textit{{AAAI} {C}onference on {A}rtificial {I}ntelligence}.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\text{Jaksch, T.}, \text{Ortner, R.} and \text{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research}, \textbf{11}
  1563--1600.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\text{Jin, C.}, \text{Allen-Zhu, Z.}, \text{Bubeck, S.} and \text{Jordan,
  M.~I.} (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jin et~al.(2019)Jin, Yang, Wang and Jordan}]{jin2019provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2019).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1907.05388}.

\bibitem[{Kallus and Zhou(2018{\natexlab{a}})}]{kallus2018confounding}
\text{Kallus, N.} and \text{Zhou, A.} (2018{\natexlab{a}}).
\newblock Confounding-robust policy improvement.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Kallus and Zhou(2018{\natexlab{b}})}]{kallus2018policy}
\text{Kallus, N.} and \text{Zhou, A.} (2018{\natexlab{b}}).
\newblock Policy evaluation and optimization with continuous treatments.
\newblock \textit{arXiv preprint arXiv:1802.06037}.

\bibitem[{Kober et~al.(2013)Kober, Bagnell and Peters}]{kober2013reinforcement}
\text{Kober, J.}, \text{Bagnell, J.~A.} and \text{Peters, J.} (2013).
\newblock Reinforcement learning in robotics: {A}survey.
\newblock \textit{{I}nternational {J}ournal of {R}obotics {R}esearch},
  \textbf{32} 1238--1274.

\bibitem[{Lattimore et~al.(2016)Lattimore, Lattimore and
  Reid}]{lattimore2016causal}
\text{Lattimore, F.}, \text{Lattimore, T.} and \text{Reid, M.~D.} (2016).
\newblock Causal bandits: {L}earning good interventions via causal inference.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Levine et~al.(2020)Levine, Kumar, Tucker and Fu}]{levine2020offline}
\text{Levine, S.}, \text{Kumar, A.}, \text{Tucker, G.} and \text{Fu, J.}
  (2020).
\newblock Offline reinforcement learning: {T}utorial, review, and perspectives
  on open problems.
\newblock \textit{arXiv preprint arXiv:2005.01643}.

\bibitem[{Li et~al.(2020)Li, Chan and Chen}]{li2020make}
\text{Li, C.}, \text{Chan, S.~H.} and \text{Chen, Y.-T.} (2020).
\newblock Who make drivers stop? {T}owards driver-centric risk assessment:
  {R}isk object identification via causal inference.
\newblock \textit{arXiv preprint arXiv:2003.02425}.

\bibitem[{Li et~al.(2016)Li, Monroe, Ritter, Galley, Gao and
  Jurafsky}]{li2016deep}
\text{Li, J.}, \text{Monroe, W.}, \text{Ritter, A.}, \text{Galley, M.},
  \text{Gao, J.} and \text{Jurafsky, D.} (2016).
\newblock Deep reinforcement learning for dialogue generation.
\newblock \textit{arXiv preprint arXiv:1606.01541}.

\bibitem[{Lu et~al.(2018)Lu, Sch{\"o}lkopf and
  Hern{\'a}ndez-Lobato}]{lu2018deconfounding}
\text{Lu, C.}, \text{Sch{\"o}lkopf, B.} and \text{Hern{\'a}ndez-Lobato, J.~M.}
  (2018).
\newblock Deconfounding reinforcement learning in observational settings.
\newblock \textit{arXiv preprint arXiv:1812.10576}.

\bibitem[{Lu et~al.(2019)Lu, Meisami, Tewari and Yan}]{lu2019regret}
\text{Lu, Y.}, \text{Meisami, A.}, \text{Tewari, A.} and \text{Yan, Z.} (2019).
\newblock Regret analysis of causal bandit problems.
\newblock \textit{arXiv preprint arXiv:1910.04938}.

\bibitem[{Manski(1990)}]{manski1990nonparametric}
\text{Manski, C.~F.} (1990).
\newblock Nonparametric bounds on treatment effects.
\newblock \textit{{A}merican {E}conomic {R}eview}, \textbf{80} 319--323.

\bibitem[{Mu{\~n}oz and van~der Laan(2012)}]{munoz2012population}
\text{Mu{\~n}oz, I.~D.} and \text{van~der Laan, M.} (2012).
\newblock Population intervention causal effects based on stochastic
  interventions.
\newblock \textit{Biometrics}, \textbf{68} 541--549.

\bibitem[{Murphy(2003)}]{murphy2003optimal}
\text{Murphy, S.~A.} (2003).
\newblock Optimal dynamic treatment regimes.
\newblock \textit{Journal of the {R}oyal {S}tatistical {S}ociety: {S}eries {B}
  ({S}tatistical {M}ethodology)}, \textbf{65} 331--355.

\bibitem[{Osband et~al.(2014)Osband, Van~Roy and
  Wen}]{osband2014generalization}
\text{Osband, I.}, \text{Van~Roy, B.} and \text{Wen, Z.} (2014).
\newblock Generalization and exploration via randomized value functions.
\newblock \textit{arXiv preprint arXiv:1402.0635}.

\bibitem[{Pearl(2009)}]{pearl2009causality}
\text{Pearl, J.} (2009).
\newblock \textit{Causality}.
\newblock Cambridge university press.

\bibitem[{Peters et~al.(2017)Peters, Janzing and
  Sch{\"o}lkopf}]{peters2017elements}
\text{Peters, J.}, \text{Janzing, D.} and \text{Sch{\"o}lkopf, B.} (2017).
\newblock \textit{Elements of {C}ausal {I}nference: {F}oundations and
  {L}earning {A}lgorithms}.
\newblock MIT press.

\bibitem[{Sen et~al.(2017)Sen, Shanmugam, Dimakis and
  Shakkottai}]{sen2017identifying}
\text{Sen, R.}, \text{Shanmugam, K.}, \text{Dimakis, A.~G.} and
  \text{Shakkottai, S.} (2017).
\newblock Identifying best interventions through online importance sampling.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot
  et~al.}]{silver2016mastering}
\text{Silver, D.}, \text{Huang, A.}, \text{Maddison, C.~J.}, \text{Guez, A.},
  \text{Sifre, L.}, \text{Van Den~Driessche, G.}, \text{Schrittwieser, J.},
  \text{Antonoglou, I.}, \text{Panneershelvam, V.}, \text{Lanctot, M.}
  \text{et~al.} (2016).
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \textit{Nature}, \textbf{529} 484.

\bibitem[{Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou,
  Huang, Guez, Hubert, Baker, Lai, Bolton et~al.}]{silver2017mastering}
\text{Silver, D.}, \text{Schrittwieser, J.}, \text{Simonyan, K.},
  \text{Antonoglou, I.}, \text{Huang, A.}, \text{Guez, A.}, \text{Hubert, T.},
  \text{Baker, L.}, \text{Lai, M.}, \text{Bolton, A.} \text{et~al.} (2017).
\newblock Mastering the game of {G}o without human knowledge.
\newblock \textit{Nature}, \textbf{550} 354.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement Learning: An Introduction}.
\newblock MIT press.

\bibitem[{Tan(2006)}]{tan2006distributional}
\text{Tan, Z.} (2006).
\newblock A distributional approach for causal inference using propensity
  scores.
\newblock \textit{{J}ournal of the {A}merican {S}tatistical {A}ssociation},
  \textbf{101} 1619--1637.

\bibitem[{Tennenholtz et~al.(2019)Tennenholtz, Mannor and
  Shalit}]{tennenholtz2019off}
\text{Tennenholtz, G.}, \text{Mannor, S.} and \text{Shalit, U.} (2019).
\newblock Off-policy evaluation in partially observable environments.
\newblock \textit{arXiv preprint arXiv:1909.03739}.

\bibitem[{Vershynin(2010)}]{vershynin2010introduction}
\text{Vershynin, R.} (2010).
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \textit{arXiv preprint arXiv:1011.3027}.

\bibitem[{Yang and Wang(2019{\natexlab{a}})}]{yang2019sample}
\text{Yang, L.} and \text{Wang, M.} (2019{\natexlab{a}}).
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Yang and Wang(2019{\natexlab{b}})}]{yang2019reinforcement}
\text{Yang, L.~F.} and \text{Wang, M.} (2019{\natexlab{b}}).
\newblock Reinforcement leaning in feature space: {M}atrix bandit, kernels, and
  regret bound.
\newblock \textit{arXiv preprint arXiv:1905.10389}.

\bibitem[{Zhang and Bareinboim(2017)}]{zhang2017transfer}
\text{Zhang, J.} and \text{Bareinboim, E.} (2017).
\newblock Transfer learning in multi-armed bandit: {A} causal approach.
\newblock In \textit{Autonomous {A}gents and {M}ulti-{A}gent {S}ystems}.

\bibitem[{Zhang and Bareinboim(2019)}]{zhang2019near}
\text{Zhang, J.} and \text{Bareinboim, E.} (2019).
\newblock Near-optimal reinforcement learning in dynamic treatment regimes.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
