\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2019)Bahdanau, Hill, Leike, Hughes, Hosseini, Kohli,
  and Grefenstette]{bahdanau2018learning}
Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and
  Grefenstette, E.
\newblock Learning to understand goal specifications by modelling reward.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Bechtle et~al.(2019)Bechtle, Molchanov, Chebotar, Grefenstette,
  Righetti, Sukhatme, and Meier]{bechtle2019meta}
Bechtle, S., Molchanov, A., Chebotar, Y., Grefenstette, E., Righetti, L.,
  Sukhatme, G., and Meier, F.
\newblock Meta-learning via learned loss.
\newblock \emph{arXiv preprint arXiv:1906.05374}, 2019.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1471--1479, 2016.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., and Wanderman-Milne, S.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Clark \& Amodei(2016)Clark and Amodei]{FaultyRewards}
Clark, J. and Amodei, D.
\newblock Faulty reward functions in the wild.
\newblock \emph{CoRR}, 2016.
\newblock URL \url{https://blog.openai.com/}.

\bibitem[Clavera et~al.(2019)Clavera, Nagabandi, Liu, Fearing, Abbeel, Levine,
  and Finn]{clavera2018learning}
Clavera, I., Nagabandi, A., Liu, S., Fearing, R.~S., Abbeel, P., Levine, S.,
  and Finn, C.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyztsoC5Y7}.

\bibitem[Cohen et~al.(2007)Cohen, McClure, and Yu]{cohen2007should}
Cohen, J.~D., McClure, S.~M., and Yu, A.~J.
\newblock Should i stay or should i go? how the human brain manages the
  trade-off between exploitation and exploration.
\newblock \emph{Philosophical Transactions of the Royal Society B: Biological
  Sciences}, 362\penalty0 (1481):\penalty0 933--942, 2007.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Duan, Y., Schulman, J., Chen, X., Bartlett, P.~L., Sutskever, I., and Abbeel,
  P.
\newblock R{L}$^2$: Fast reinforcement learning via slow reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1611.02779}, 2016.

\bibitem[Duan et~al.(2017)Duan, Andrychowicz, Stadie, Ho, Schneider, Sutskever,
  Abbeel, and Zaremba]{duan2017one}
Duan, Y., Andrychowicz, M., Stadie, B., Ho, O.~J., Schneider, J., Sutskever,
  I., Abbeel, P., and Zaremba, W.
\newblock One-shot imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1087--1098, 2017.

\bibitem[Dubey \& Griffiths(2019)Dubey and Griffiths]{dubey2019reconciling}
Dubey, R. and Griffiths, T.~L.
\newblock Reconciling novelty and complexity through a rational analysis of
  curiosity.
\newblock \emph{Psychological Review}, 2019.

\bibitem[Finn et~al.(2017{\natexlab{a}})Finn, Abbeel, and
  Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1126--1135. JMLR. org, 2017{\natexlab{a}}.

\bibitem[Finn et~al.(2017{\natexlab{b}})Finn, Yu, Zhang, Abbeel, and
  Levine]{finn2017one}
Finn, C., Yu, T., Zhang, T., Abbeel, P., and Levine, S.
\newblock One-shot visual imitation learning via meta-learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  357--368,
  2017{\natexlab{b}}.

\bibitem[Gittins(1974)]{gittins1974dynamic}
Gittins, J.
\newblock A dynamic allocation index for the sequential design of experiments.
\newblock \emph{Progress in statistics}, pp.\  241--266, 1974.

\bibitem[Gittins(1979)]{gittins1979bandit}
Gittins, J.~C.
\newblock Bandit processes and dynamic allocation indices.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 41\penalty0 (2):\penalty0 148--164, 1979.

\bibitem[Gordon \& Ahissar(2011)Gordon and Ahissar]{gordon2011reinforcement}
Gordon, G. and Ahissar, E.
\newblock Reinforcement active learning hierarchical loops.
\newblock In \emph{The 2011 International Joint Conference on Neural Networks},
  pp.\  3008--3015. IEEE, 2011.

\bibitem[Goyal et~al.(2018)Goyal, Islam, Strouse, Ahmed, Larochelle, Botvinick,
  Bengio, and Levine]{goyal2018infobot}
Goyal, A., Islam, R., Strouse, D., Ahmed, Z., Larochelle, H., Botvinick, M.,
  Bengio, Y., and Levine, S.
\newblock Infobot: Transfer and exploration via the information bottleneck.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Guo et~al.(2016)Guo, Singh, Lewis, and Lee]{guo2016deep}
Guo, X., Singh, S., Lewis, R., and Lee, H.
\newblock Deep learning for reward design to improve monte carlo tree search in
  atari games.
\newblock In \emph{Proceedings of the Twenty-Fifth International Joint
  Conference on Artificial Intelligence}, pp.\  1519--1525. AAAI Press, 2016.

\bibitem[Harutyunyan et~al.(2015)Harutyunyan, Devlin, Vrancx, and
  Nowe]{harutyunyan2015expressing}
Harutyunyan, A., Devlin, S., Vrancx, P., and Nowe, A.
\newblock Expressing arbitrary reward functions as potential-based advice.
\newblock In \emph{Proceedings of the Twenty-Ninth AAAI Conference on
  Artificial Intelligence}, pp.\  2652--2658. AAAI Press, 2015.

\bibitem[Itti \& Baldi(2006)Itti and Baldi]{itti2006bayesian}
Itti, L. and Baldi, P.~F.
\newblock Bayesian surprise attracts human attention.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  547--554, 2006.

\bibitem[Kirsch et~al.(2019)Kirsch, van Steenkiste, and
  Schmidhuber]{kirsch2019improving}
Kirsch, L., van Steenkiste, S., and Schmidhuber, J.
\newblock Improving generalization in meta reinforcement learning using learned
  objectives.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Linke et~al.(2019)Linke, Ady, White, Degris, and
  White]{linke2019adapting}
Linke, C., Ady, N.~M., White, M., Degris, T., and White, A.
\newblock Adapting behaviour via intrinsic reward: A survey and empirical
  study.
\newblock \emph{arXiv preprint arXiv:1906.07865}, 2019.

\bibitem[Metz et~al.(2019)Metz, Maheswaranathan, Cheung, and
  Sohl-Dickstein]{metz2019meta}
Metz, L., Maheswaranathan, N., Cheung, B., and Sohl-Dickstein, J.
\newblock Meta-learning update rules for unsupervised representation learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HkNDsiC9KQ}.

\bibitem[Mirolli \& Baldassarre(2013)Mirolli and
  Baldassarre]{mirolli2013functions}
Mirolli, M. and Baldassarre, G.
\newblock Functions and mechanisms of intrinsic motivations.
\newblock In \emph{Intrinsically Motivated Learning in Natural and Artificial
  Systems}, pp.\  49--72. Springer, 2013.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.~J.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Proceedings of the Sixteenth International Conference on
  Machine Learning}, pp.\  278--287. Morgan Kaufmann Publishers Inc., 1999.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, van~den Oord, and
  Munos]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., van~den Oord, A., and Munos, R.
\newblock Count-based exploration with neural density models.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2721--2730. JMLR. org, 2017.

\bibitem[Oudeyer et~al.(2007)Oudeyer, Kaplan, and Hafner]{oudeyer2007intrinsic}
Oudeyer, P.-Y., Kaplan, F., and Hafner, V.~V.
\newblock Intrinsic motivation systems for autonomous mental development.
\newblock \emph{IEEE transactions on evolutionary computation}, 11\penalty0
  (2):\penalty0 265--286, 2007.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2778--2787. JMLR. org, 2017.

\bibitem[Poupart et~al.(2006)Poupart, Vlassis, Hoey, and
  Regan]{poupart2006analytic}
Poupart, P., Vlassis, N., Hoey, J., and Regan, K.
\newblock An analytic solution to discrete bayesian reinforcement learning.
\newblock In \emph{Proceedings of the 23rd International Conference on Machine
  Learning}, pp.\  697--704. ACM, 2006.

\bibitem[Randl{\"o}v \& Alstr{\"o}m(1998)Randl{\"o}v and
  Alstr{\"o}m]{randlov1998learning}
Randl{\"o}v, J. and Alstr{\"o}m, P.
\newblock Learning to drive a bicycle using reinforcement learning and shaping.
\newblock In \emph{Proceedings of the Fifteenth International Conference on
  Machine Learning}, pp.\  463--471. Morgan Kaufmann Publishers Inc., 1998.

\bibitem[Schlegel et~al.(2018)Schlegel, Patterson, White, and
  White]{schlegel2018discovery}
Schlegel, M., Patterson, A., White, A., and White, M.
\newblock Discovery of predictive representations with a network of general
  value functions, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryZElGZ0Z}.

\bibitem[Schmidhuber(1991{\natexlab{a}})]{schmidhuber1991curious}
Schmidhuber, J.
\newblock Curious model-building control systems.
\newblock In \emph{Proc. international joint conference on neural networks},
  pp.\  1458--1463, 1991{\natexlab{a}}.

\bibitem[Schmidhuber(1991{\natexlab{b}})]{schmidhuber1991possibility}
Schmidhuber, J.
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In \emph{Proc. of the international conference on simulation of
  adaptive behavior: From animals to animats}, pp.\  222--227,
  1991{\natexlab{b}}.

\bibitem[Schmidhuber et~al.(1996)Schmidhuber, Zhao, and
  Wiering]{schmidhuber1996simple}
Schmidhuber, J., Zhao, J., and Wiering, M.
\newblock Simple principles of metalearning.
\newblock \emph{Technical report IDSIA}, 69:\penalty0 1--23, 1996.

\bibitem[Singh et~al.(2009)Singh, Lewis, and Barto]{singh2009rewards}
Singh, S., Lewis, R.~L., and Barto, A.~G.
\newblock Where do rewards come from.
\newblock In \emph{Proceedings of the annual conference of the cognitive
  science society}, pp.\  2601--2606. Cognitive Science Society, 2009.

\bibitem[Singh et~al.(2010)Singh, Lewis, Barto, and
  Sorg]{singh2010intrinsically}
Singh, S., Lewis, R.~L., Barto, A.~G., and Sorg, J.
\newblock Intrinsically motivated reinforcement learning: An evolutionary
  perspective.
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 2\penalty0
  (2):\penalty0 70--82, 2010.

\bibitem[Sorg et~al.(2010)Sorg, Lewis, and Singh]{sorg2010reward}
Sorg, J., Lewis, R.~L., and Singh, S.
\newblock Reward design via online gradient ascent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2190--2198, 2010.

\bibitem[Stadie et~al.(2018)Stadie, Yang, Houthooft, Chen, Duan, Wu, Abbeel,
  and Sutskever]{stadie2018importance}
Stadie, B., Yang, G., Houthooft, R., Chen, P., Duan, Y., Wu, Y., Abbeel, P.,
  and Sutskever, I.
\newblock The importance of sampling inmeta-reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9280--9290, 2018.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Sutton(1990)]{Sutton90integratedarchitectures}
Sutton, R.~S.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{Proceedings of the Seventh International Conference on
  Machine Learning}, pp.\  216--224. Morgan Kaufmann, 1990.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1057--1063, 2000.

\bibitem[Thompson(1933)]{thompson1933likelihood}
Thompson, W.~R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun1998learning}
Thrun, S. and Pratt, L.
\newblock Learning to learn: Introduction and overview.
\newblock In \emph{Learning to learn}, pp.\  3--17. Springer, 1998.

\bibitem[Veeriah et~al.(2019)Veeriah, Hessel, Xu, Rajendran, Lewis, Oh, van
  Hasselt, Silver, and Singh]{veeriah2019discovery}
Veeriah, V., Hessel, M., Xu, Z., Rajendran, J., Lewis, R.~L., Oh, J., van
  Hasselt, H.~P., Silver, D., and Singh, S.
\newblock Discovery of useful questions as auxiliary tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9306--9317, 2019.

\bibitem[Wang et~al.(2016)Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos,
  Blundell, Kumaran, and Botvinick]{Wang2016LearningTR}
Wang, J.~X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.~Z., Munos,
  R., Blundell, C., Kumaran, D., and Botvinick, M.~M.
\newblock Learning to reinforcement learn.
\newblock \emph{ArXiv}, abs/1611.05763, 2016.

\bibitem[Watkins(1989)]{watkins1989learning}
Watkins, C. J. C.~H.
\newblock Learning from delayed rewards.
\newblock 1989.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Wilson et~al.(2014)Wilson, Geana, White, Ludvig, and
  Cohen]{wilson2014humans}
Wilson, R.~C., Geana, A., White, J.~M., Ludvig, E.~A., and Cohen, J.~D.
\newblock Humans use directed and random exploration to solve the
  explore--exploit dilemma.
\newblock \emph{Journal of Experimental Psychology: General}, 143\penalty0
  (6):\penalty0 2074, 2014.

\bibitem[Xu et~al.(2019)Xu, Ratner, Dragan, Levine, and Finn]{xu2019learning}
Xu, K., Ratner, E., Dragan, A., Levine, S., and Finn, C.
\newblock Learning a prior over intent via meta-inverse reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  6952--6962, 2019.

\bibitem[Xu et~al.(2018{\natexlab{a}})Xu, Liu, Zhao, and Peng]{xu2018learning}
Xu, T., Liu, Q., Zhao, L., and Peng, J.
\newblock Learning to explore via meta-policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5459--5468, 2018{\natexlab{a}}.

\bibitem[Xu et~al.(2018{\natexlab{b}})Xu, van Hasselt, and Silver]{xu2018meta}
Xu, Z., van Hasselt, H.~P., and Silver, D.
\newblock Meta-gradient reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2396--2407, 2018{\natexlab{b}}.

\bibitem[Zheng et~al.(2018)Zheng, Oh, and Singh]{zheng2018learning}
Zheng, Z., Oh, J., and Singh, S.
\newblock On learning intrinsic rewards for policy gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4644--4654, 2018.

\end{thebibliography}
