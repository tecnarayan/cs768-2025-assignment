\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Azadi et~al.(2016)Azadi, Feng, Jegelka, and
  Darrell]{azadi2016auxiliary}
Azadi, S., Feng, J., Jegelka, S., and Darrell, T.
\newblock Auxiliary image regularization for deep cnns with noisy labels.
\newblock \emph{ICLR}, 2016.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
\newblock Curriculum learning.
\newblock In \emph{ICML}, 2009.

\bibitem[Candes et~al.(2008)Candes, Wakin, and Boyd]{candes2008enhancing}
Candes, E.~J., Wakin, M.~B., and Boyd, S.~P.
\newblock Enhancing sparsity by reweighted l1 minimization.
\newblock \emph{Journal of Fourier analysis and applications}, 14\penalty0
  (5-6):\penalty0 877--905, 2008.

\bibitem[Chang et~al.(2017)Chang, Learned-Miller, and
  McCallum]{chang2017active}
Chang, H.-S., Learned-Miller, E., and McCallum, A.
\newblock Active bias: Training a more accurate neural network by emphasizing
  high variance samples.
\newblock \emph{NIPS}, 2017.

\bibitem[Chen \& Gupta(2015)Chen and Gupta]{chen2015webly}
Chen, X. and Gupta, A.
\newblock Webly supervised learning of convolutional networks.
\newblock In \emph{ICCV}, 2015.

\bibitem[Chen et~al.(2018)Chen, Chi, Fan, and Ma]{chen2018gradient}
Chen, Y., Chi, Y., Fan, J., and Ma, C.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock \emph{arXiv preprint arXiv:1803.07726}, 2018.

\bibitem[Csiszar(1984)]{csiszar1984information}
Csiszar, I.
\newblock Information geometry and alternating minimization procedures.
\newblock \emph{Statistics and decisions}, 1:\penalty0 205--237, 1984.

\bibitem[Dehghani et~al.(2017)Dehghani, Severyn, Rothe, and
  Kamps]{dehghani2017avoiding}
Dehghani, M., Severyn, A., Rothe, S., and Kamps, J.
\newblock Avoiding your teacher's mistakes: Training neural networks with
  controlled weak supervision.
\newblock \emph{arXiv preprint arXiv:1711.00313}, 2017.

\bibitem[Dehghani et~al.(2018)Dehghani, Mehrjou, Gouws, Kamps, and
  Schölkopf]{dehghani2018fidelityweighted}
Dehghani, M., Mehrjou, A., Gouws, S., Kamps, J., and Schölkopf, B.
\newblock Fidelity-weighted learning.
\newblock In \emph{ICLR}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Fan et~al.(2017)Fan, He, Liang, and Hu]{fan2017self}
Fan, Y., He, R., Liang, J., and Hu, B.-G.
\newblock Self-paced learning: An implicit regularization perspective.
\newblock In \emph{AAAI}, 2017.

\bibitem[Fan et~al.(2018)Fan, Tian, Qin, Li, and Liu]{fan2018learning}
Fan, Y., Tian, F., Qin, T., Li, X.-Y., and Liu, T.-Y.
\newblock Learning to teach.
\newblock In \emph{ICLR}, 2018.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2017training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and
  Kavukcuoglu]{graves2017automated}
Graves, A., Bellemare, M.~G., Menick, J., Munos, R., and Kavukcuoglu, K.
\newblock Automated curriculum learning for neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huber(2011)]{huber2011robust}
Huber, P.~J.
\newblock Robust statistics.
\newblock In \emph{International Encyclopedia of Statistical Science}, pp.\
  1248--1251. Springer, 2011.

\bibitem[Huber et~al.(1964)]{huber1964robust}
Huber, P.~J. et~al.
\newblock Robust estimation of a location parameter.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73--101, 1964.

\bibitem[Jiang et~al.(2014)Jiang, Meng, Yu, Lan, Shan, and
  Hauptmann]{jiang2014self}
Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann, A.
\newblock Self-paced learning with diversity.
\newblock In \emph{NIPS}, 2014.

\bibitem[Jiang et~al.(2015)Jiang, Meng, Zhao, Shan, and
  Hauptmann]{jiang2015self}
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann, A.~G.
\newblock Self-paced curriculum learning.
\newblock In \emph{AAAI}, 2015.

\bibitem[Khan et~al.(2011)Khan, Mutlu, and Zhu]{khan2011humans}
Khan, F., Mutlu, B., and Zhu, X.
\newblock How do humans teach: On curriculum learning and teaching dimension.
\newblock In \emph{NIPS}, 2011.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, 2012.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
Kumar, M.~P., Packer, B., and Koller, D.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{NIPS}, 2010.

\bibitem[Lee et~al.(2017)Lee, He, Zhang, and Yang]{lee2017cleannet}
Lee, K.-H., He, X., Zhang, L., and Yang, L.
\newblock Cleannet: Transfer learning for scalable image classifier training
  with label noise.
\newblock \emph{arXiv preprint arXiv:1711.07131}, 2017.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Wang, Li, Agustsson, and
  Van~Gool]{li2017webvision}
Li, W., Wang, L., Li, W., Agustsson, E., and Van~Gool, L.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{arXiv preprint arXiv:1708.02862}, 2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Yang, Song, Cao, Li, and
  Luo]{Li2017ICCV}
Li, Y., Yang, J., Song, Y., Cao, L., Li, J., and Luo, J.
\newblock Learning from noisy labels with distillation.
\newblock In \emph{ICCV}, 2017{\natexlab{b}}.

\bibitem[Liang et~al.(2016)Liang, Jiang, Meng, and
  Hauptmann]{liang2016learning}
Liang, J., Jiang, L., Meng, D., and Hauptmann, A.~G.
\newblock Learning to detect concepts from webly-labeled video data.
\newblock In \emph{IJCAI}, 2016.

\bibitem[Lin et~al.(2017{\natexlab{a}})Lin, Wang, Meng, Zuo, and
  Zhang]{lin2017active}
Lin, L., Wang, K., Meng, D., Zuo, W., and Zhang, L.
\newblock Active self-paced learning for cost-effective and progressive face
  identification.
\newblock \emph{TPAMI}, 2017{\natexlab{a}}.

\bibitem[Lin et~al.(2017{\natexlab{b}})Lin, Goyal, Girshick, He, and
  Doll{\'a}r]{lin2017focal}
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Focal loss for dense object detection.
\newblock \emph{ICCV}, 2017{\natexlab{b}}.

\bibitem[Ma et~al.(2017{\natexlab{a}})Ma, Meng, Xie, Li, and Dong]{ma2017self}
Ma, F., Meng, D., Xie, Q., Li, Z., and Dong, X.
\newblock Self-paced co-training.
\newblock In \emph{ICML}, 2017{\natexlab{a}}.

\bibitem[Ma et~al.(2017{\natexlab{b}})Ma, Liu, and Meng]{ma2017convergence}
Ma, Z., Liu, S., and Meng, D.
\newblock On convergence property of implicit self-paced objective.
\newblock \emph{arXiv preprint arXiv:1703.09923}, 2017{\natexlab{b}}.

\bibitem[Meng et~al.(2015)Meng, Zhao, and Jiang]{meng2015objective}
Meng, D., Zhao, Q., and Jiang, L.
\newblock What objective does self-paced learning indeed optimize?
\newblock \emph{arXiv preprint arXiv:1511.06049}, 2015.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Menon, A., Van~Rooyen, B., Ong, C.~S., and Williamson, B.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{ICML}, 2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Sangineto et~al.(2016)Sangineto, Nabi, Culibrk, and
  Sebe]{sangineto2016self}
Sangineto, E., Nabi, M., Culibrk, D., and Sebe, N.
\newblock Self paced deep learning for weakly supervised object detection.
\newblock \emph{arXiv preprint arXiv:1605.07651}, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G.~E., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sukhbaatar et~al.(2014)Sukhbaatar, Bruna, Paluri, Bourdev, and
  Fergus]{sukhbaatar2014training}
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fergus, R.
\newblock Training convolutional networks with noisy labels.
\newblock \emph{arXiv preprint arXiv:1406.2080}, 2014.

\bibitem[Supancic \& Ramanan(2013)Supancic and Ramanan]{supancic2013self}
Supancic, J.~S. and Ramanan, D.
\newblock Self-paced learning for long-term tracking.
\newblock In \emph{CVPR}, 2013.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Szegedy et~al.(2017)Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2017inception}
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A.~A.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In \emph{AAAI}, 2017.

\bibitem[Turian et~al.(2010)Turian, Ratinov, and Bengio]{turian2010word}
Turian, J., Ratinov, L., and Bengio, Y.
\newblock Word representations: a simple and general method for semi-supervised
  learning.
\newblock In \emph{ACL}, 2010.

\bibitem[Vahdat(2017)]{vahdat2017toward}
Vahdat, A.
\newblock Toward robustness against label noise in training deep discriminative
  neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Veit et~al.(2017)Veit, Alldrin, Chechik, Krasin, Gupta, and
  Belongie]{Veit2017}
Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In \emph{CVPR}, 2017.

\bibitem[Wang et~al.(2017)Wang, Kucukelbir, and Blei]{wang2017robust}
Wang, Y., Kucukelbir, A., and Blei, D.~M.
\newblock Robust probabilistic modeling with bayesian data reweighting.
\newblock In \emph{ICML}, 2017.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Zhang et~al.(2017{\natexlab{a}})Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Zhang(2010)]{zhang2010nearly}
Zhang, C.-H.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of Statistics}, pp.\  894--942, 2010.

\bibitem[Zhang et~al.(2017{\natexlab{b}})Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017{\natexlab{b}}.

\bibitem[Zhou et~al.(2017{\natexlab{a}})Zhou, Mertikopoulos, Bambos, Boyd, and
  Glynn]{zhou2017mirror}
Zhou, Z., Mertikopoulos, P., Bambos, N., Boyd, S., and Glynn, P.
\newblock Mirror descent in non-convex stochastic programming.
\newblock \emph{arXiv preprint arXiv:1706.05681}, 2017{\natexlab{a}}.

\bibitem[Zhou et~al.(2017{\natexlab{b}})Zhou, Mertikopoulos, Bambos, Boyd, and
  Glynn]{zhou2017stochastic}
Zhou, Z., Mertikopoulos, P., Bambos, N., Boyd, S., and Glynn, P.~W.
\newblock Stochastic mirror descent in variationally coherent optimization
  problems.
\newblock In \emph{NIPS}, 2017{\natexlab{b}}.

\end{thebibliography}
