\begin{thebibliography}{10}

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies}, 2019.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1877--1901, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{openai2023gpt4}
OpenAI.
\newblock {GPT}-4 technical report, 2023.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock {PaLM}: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{xue2023repeat}
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
\newblock To repeat or not to repeat: Insights from scaling llm under token-crisis.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{chen2018lifelong}
Zhiyuan Chen and Bing Liu.
\newblock {\em Lifelong Machine Learning}, volume~1.
\newblock Springer, 2018.

\bibitem{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock In {\em Psychology of Learning and Motivation}, volume~24, pages 109--165. Elsevier, 1989.

\bibitem{cai2021online}
Zhipeng Cai, Ozan Sener, and Vladlen Koltun.
\newblock Online continual learning with natural distribution shifts: An empirical study with visual data.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 8281--8290, 2021.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In {\em International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{biderman2022datasheet}
Stella Biderman, Kieran Bicheno, and Leo Gao.
\newblock Datasheet for the {P}ile.
\newblock {\em arXiv preprint arXiv:2201.07311}, 2022.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, pages 1691--1703. PMLR, 2020.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, {\c{C}}a{\u{g}}lar Gul{\c{c}}ehre, and Bing Xiang.
\newblock Abstractive text summarization using sequence-to-sequence rnns and beyond.
\newblock In {\em Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning}, pages 280--290, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em University of Toronto}, 2009.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on Computer Vision and Pattern Recognition}, pages 248--255. IEEE, 2009.

\bibitem{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{ahn2020sgd}
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra.
\newblock {SGD} with shuffling: {O}ptimal rates without component convexity and large epoch requirements.
\newblock {\em {Advances in Neural Information Processing Systems}}, 33:17526--17535, 2020.

\bibitem{gurbuzbalaban2019convergence}
M~Gurbuzbalaban, Asu Ozdaglar, and Pablo~A Parrilo.
\newblock Convergence rate of incremental gradient and incremental newton methods.
\newblock {\em SIAM Journal on Optimization}, 29(4):2542--2565, 2019.

\bibitem{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled, and Peter Richt{\'a}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock {\em {Advances in Neural Information Processing Systems}}, 33:17309--17320, 2020.

\bibitem{safran2020good}
Itay Safran and Ohad Shamir.
\newblock How good is {SGD} with random shuffling?
\newblock In {\em Conference on Learning Theory}, pages 3250--3284. PMLR, 2020.

\bibitem{xu2022stochastic}
Lijie Xu, Shuang Qiu, Binhang Yuan, Jiawei Jiang, Cedric Renggli, Shaoduo Gan, Kaan Kara, Guoliang Li, Ji~Liu, Wentao Wu, et~al.
\newblock Stochastic gradient descent without full data shuffle.
\newblock {\em arXiv preprint arXiv:2206.05830}, 2022.

\bibitem{hannan1957approximation}
James Hannan.
\newblock Approximation to {B}ayes risk in repeated play.
\newblock {\em Contributions to the Theory of Games}, 3:97--139, 1957.

\bibitem{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient ascent.
\newblock In {\em International Conference on Machine Learning}, pages 928--936, 2003.

\bibitem{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock {\em Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.

\bibitem{shalev2012online}
Shai Shalev-Shwartz et~al.
\newblock Online learning and online convex optimization.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning}, 4(2):107--194, 2012.

\bibitem{denevi2019learning}
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil.
\newblock Learning-to-learn stochastic gradient descent with biased regularization.
\newblock In {\em International Conference on Machine Learning}, pages 1566--1575. PMLR, 2019.

\bibitem{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages 1920--1930. PMLR, 2019.

\bibitem{denevi2019online}
Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil.
\newblock Online-within-online meta-learning.
\newblock {\em {Advances in Neural Information Processing Systems}}, 32, 2019.

\bibitem{javed2019meta}
Khurram Javed and Martha White.
\newblock Meta-learning representations for continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{fini2020online}
Enrico Fini, St{\'e}phane Lathuiliere, Enver Sangineto, Moin Nabi, and Elisa Ricci.
\newblock Online continual learning under extreme memory constraints.
\newblock In {\em Proceedings of the European Conference on Computer Vision}, pages 720--735. Springer, 2020.

\bibitem{ren2020wandering}
Mengye Ren, Michael~Louis Iuzzolino, Michael~Curtis Mozer, and Richard~S. Zemel.
\newblock Wandering within a world: Online contextualized few-shot learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{wang2021wanderlust}
Jianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta.
\newblock Wanderlust: Online continual object detection in the real world.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 10829--10838, 2021.

\bibitem{biehl1995learning}
Michael Biehl and Holm Schwarze.
\newblock Learning by on-line gradient descent.
\newblock {\em Journal of Physics A: Mathematical and general}, 28(3):643, 1995.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 114(13):3521--3526, 2017.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em International Conference on Machine Learning}, pages 3987--3995. PMLR, 2017.

\bibitem{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision}, pages 139--154, 2018.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H Lampert.
\newblock {iCaRL}: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne.
\newblock Experience replay for continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{chaudhry2019tiny}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet~K Dokania, Philip~HS Torr, and Marc'Aurelio Ranzato.
\newblock On tiny episodic memories in continual learning.
\newblock {\em arXiv preprint arXiv:1902.10486}, 2019.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 40(12):2935--2947, 2017.

\bibitem{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.
\newblock Dark experience for general continual learning: a strong, simple baseline.
\newblock {\em Advances in Neural Information Processing Systems}, 33:15920--15930, 2020.

\bibitem{madaan2023heterogeneous}
Divyam Madaan, Hongxu Yin, Wonmin Byeon, Jan Kautz, and Pavlo Molchanov.
\newblock Heterogeneous continual learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15985--15995, 2023.

\bibitem{yoon2017lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{serra2018overcoming}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In {\em International Conference on Machine Learning}, pages 4548--4557. PMLR, 2018.

\bibitem{gurbuz2022nispa}
Mustafa~B Gurbuz and Constantine Dovrolis.
\newblock {NISPA}: Neuro-inspired stability-plasticity adaptation for continual learning in sparse networks.
\newblock In {\em International Conference on Machine Learning}, pages 8157--8174. PMLR, 2022.

\bibitem{kang2022forget}
Haeyong Kang, Rusty John~Lloyd Mina, Sultan Rizky~Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung~Ju Hwang, and Chang~D Yoo.
\newblock Forget-free continual learning with winning subnetworks.
\newblock In {\em International Conference on Machine Learning}, pages 10734--10750. PMLR, 2022.

\bibitem{mayo2023multitask}
David Mayo, Tyler Scott, Mengye Ren, Gamaleldin Elsayed, Katherine Hermann, Matt Jones, and Michael Mozer.
\newblock Multitask learning via interleaving: A neural network investigation.
\newblock In M.~Goldwater, F.~K. Anggoro, B.~K. Hayes, and D.~C. Ong, editors, {\em Proceedings of the 45th Annual Conference of the Cognitive Science Society}, volume~45, 2023.

\bibitem{madotto2021continual}
Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Pascale Fung, and Zhiguang Wang.
\newblock Continual learning in task-oriented dialogue systems.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 7452--7467, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{qin2022LFPT5}
Chengwei Qin and Shafiq~R. Joty.
\newblock {LFPT5:} {A} unified framework for lifelong few-shot language learning based on prompt tuning of {T5}.
\newblock In {\em International Conference on Learning Representations}. OpenReview.net, 2022.

\bibitem{razdaibiedina2023progressive}
Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi.
\newblock Progressive prompts: Continual learning for language models.
\newblock In {\em International Conference on Learning Representations}. OpenReview.net, 2023.

\bibitem{davidson2020sequential}
Guy Davidson and Michael~C Mozer.
\newblock Sequential mastery of multiple visual tasks: Networks naturally learn to learn and forget to forget.
\newblock In {\em Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition}, pages 9282--9293, 2020.

\bibitem{janson2022simple}
Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.
\newblock A simple baseline that questions the use of pretrained-models in continual learning.
\newblock In {\em NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications}, 2022.

\bibitem{lee2023pre}
Kuan-Ying Lee, Yuanyi Zhong, and Yu-Xiong Wang.
\newblock Do pre-trained models benefit equally in continual learning?
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 6485--6493, 2023.

\bibitem{fini2022self}
Enrico Fini, Victor G~Turrisi Da~Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal.
\newblock Self-supervised models are continual learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9621--9630, 2022.

\bibitem{scialom2022fine}
Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan.
\newblock Fine-tuned language models are continual learners.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 6107--6122, 2022.

\bibitem{ke2022continual}
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.
\newblock Continual pre-training of language models.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{jones2023learning}
Matt Jones, Tyler~R. Scott, Mengye Ren, Gamaleldin~Fathy Elsayed, Katherine~L. Hermann, David Mayo, and Michael~Curtis Mozer.
\newblock Learning in temporally structured environments.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em 5th International Conference on Learning Representations, Toulon, France, April 24-26, 2017, Conference Track Proceedings}, 2017.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock {\em Transactions on Machine Learning Research}, 2022.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et~al.
\newblock Predictability and surprise in large generative models.
\newblock In {\em Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 1747--1764, 2022.

\bibitem{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{carlini2022quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram{\`{e}}r, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{orhan2023recognition}
A~Emin Orhan.
\newblock Recognition, recall, and retention of few-shot memories in large language models.
\newblock {\em arXiv preprint arXiv:2303.17557}, 2023.

\end{thebibliography}
