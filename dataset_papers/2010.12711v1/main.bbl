\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in neural information processing systems}, pages
  6155--6166, 2019.

\bibitem[Arora et~al.(2020)Arora, Bartlett, Mianjy, and
  Srebro]{arora2020dropout}
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro.
\newblock Dropout: Explicit forms and capacity control.
\newblock \emph{arXiv preprint arXiv:2003.03397}, 2020.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019.

\bibitem[Baldi and Sadowski(2013)]{baldi2013understanding}
Pierre Baldi and Peter~J Sadowski.
\newblock Understanding dropout.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2814--2822, 2013.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer2011contextual}
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 19--26, 2011.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{arXiv preprint arXiv:1710.10174}, 2017.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10835--10845, 2019.

\bibitem[Chizat et~al.(2018)Chizat, Oyallon, and Bach]{chizat2018lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming. arxiv e-prints, page.
\newblock \emph{arXiv preprint arXiv:1812.07956}, 2018.

\bibitem[Dahl et~al.(2013)Dahl, Sainath, and Hinton]{dahl2013improving}
George~E Dahl, Tara~N Sainath, and Geoffrey~E Hinton.
\newblock Improving deep neural networks for {LVCSR} using rectified linear
  units and dropout.
\newblock In \emph{2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 8609--8613. IEEE, 2013.

\bibitem[Daniely(2017)]{daniely2017sgd}
Amit Daniely.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem[Dauber et~al.(2020)Dauber, Feder, Koren, and Livni]{dauber2020can}
Assaf Dauber, Meir Feder, Tomer Koren, and Roi Livni.
\newblock Can implicit bias explain generalization? stochastic convex
  optimization as a case study.
\newblock \emph{arXiv preprint arXiv:2003.06152}, 2020.

\bibitem[Du et~al.(2018)Du, Lee, Li, Wang, and Zhai]{du2018gradient1}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Frankle and Carbin(2019)]{frankle2018the}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{Int. Conf. Machine Learning (ICML)}, 2016.

\bibitem[Gao and Zhou(2016)]{gao2016dropout}
Wei Gao and Zhi-Hua Zhou.
\newblock Dropout rademacher complexity of deep neural networks.
\newblock \emph{Science China Information Sciences}, 59\penalty0 (7):\penalty0
  072104, 2016.

\bibitem[Gomez et~al.(2019)Gomez, Zhang, Swersky, Gal, and
  Hinton]{gomez2019learning}
Aidan~N Gomez, Ivan Zhang, Kevin Swersky, Yarin Gal, and Geoffrey~E Hinton.
\newblock Learning sparse networks using targeted dropout.
\newblock \emph{arXiv preprint arXiv:1905.13678}, 2019.

\bibitem[Helmbold and Long(2015)]{helmbold2015inductive}
David~P Helmbold and Philip~M Long.
\newblock On the inductive bias of dropout.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 16:\penalty0
  3403--3454, 2015.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving}
Geoffrey~E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Hu et~al.(2020)Hu, Li, and Yu]{Hu2020Simple}
Wei Hu, Zhiyuan Li, and Dingli Yu.
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Hke3gyHYwH}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Ji and Telgarsky(2019)]{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock \emph{arXiv preprint arXiv:1909.12292}, 2019.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  8570--8581, 2019.

\bibitem[Li et~al.(2019)Li, Soltanolkotabi, and Oymak]{li2019gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1903.11680}, 2019.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem[McAllester(2013)]{mcallester2013pac}
David McAllester.
\newblock A {PAC}-bayesian tutorial with a dropout bound.
\newblock \emph{arXiv preprint arXiv:1307.2118}, 2013.

\bibitem[Merity et~al.(2017)Merity, Keskar, and Socher]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017.

\bibitem[Mianjy and Arora(2019)]{mianjy2019dropout}
Poorya Mianjy and Raman Arora.
\newblock On dropout and nuclear norm regularization.
\newblock In \emph{International Conference on Machine Learning}, pages
  4575--4584, 2019.

\bibitem[Mianjy et~al.(2018)Mianjy, Arora, and Vidal]{mianjy2018implicit}
Poorya Mianjy, Raman Arora, and Rene Vidal.
\newblock On the implicit bias of dropout.
\newblock In \emph{International Conference on Machine Learning}, pages
  3540--3548, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2498--2507. JMLR. org, 2017.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Tomioka, Salakhutdinov, and
  Srebro]{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock \emph{arXiv preprint arXiv:1705.03071}, 2017.

\bibitem[Nitanda and Suzuki(2019)]{nitanda2019refined}
Atsushi Nitanda and Taiji Suzuki.
\newblock Refined generalization analysis of gradient descent for
  over-parameterized two-layer neural networks with smooth activations on
  classification problems.
\newblock \emph{arXiv preprint arXiv:1905.09870}, 2019.

\bibitem[Oymak and Soltanolkotabi(2020)]{oymak2020towards}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2020.

\bibitem[Senen-Cerda and Sanders(2020)]{senen2020almost}
Albert Senen-Cerda and Jaron Sanders.
\newblock Almost sure convergence of dropout algorithms for neural networks.
\newblock \emph{arXiv preprint arXiv:2002.02247}, 2020.

\bibitem[Song and Yang(2019)]{song2019quadratic}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 15\penalty0 (1),
  2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge University Press, 2018.

\bibitem[Wager et~al.(2013)Wager, Wang, and Liang]{wager2013dropout}
Stefan Wager, Sida Wang, and Percy~S Liang.
\newblock Dropout training as adaptive regularization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem[Wager et~al.(2014)Wager, Fithian, Wang, and Liang]{wager2014altitude}
Stefan Wager, William Fithian, Sida Wang, and Percy~S Liang.
\newblock Altitude training: Strong bounds for single-layer dropout.
\newblock In \emph{Adv. Neural Information Processing Systems}, 2014.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and
  Fergus]{wan2013regularization}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann Le~Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{International conference on machine learning}, pages
  1058--1066, 2013.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9709--9721, 2019.

\bibitem[Wei et~al.(2020)Wei, Kakade, and Ma]{wei2020implicit}
Colin Wei, Sham Kakade, and Tengyu Ma.
\newblock The implicit and explicit regularization effects of dropout.
\newblock \emph{arXiv preprint arXiv:2002.12915}, 2020.

\bibitem[Zhai and Wang(2018)]{zhai2018adaptive}
Ke~Zhai and Huan Wang.
\newblock Adaptive dropout with rademacher complexity regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=S1uxsye0Z}.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
