Which transformer architecture fits my data? A vocabulary bottleneck in self-attention