\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2014)Agarwal, Hsu, Kale, Langford, Li, and
  Schapire]{agarwal2014taming}
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert
  Schapire.
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  1638--1646. PMLR, 2014.

\bibitem[Anthony(2002)]{anthony2002uniform}
Martin Anthony.
\newblock Uniform glivenko-cantelli theorems and concentration of measure in
  the mathematical modelling of learning.
\newblock \emph{Research Report LSE-CDAM-2002--07}, 2002.

\bibitem[Ash et~al.(2021)Ash, Goel, Krishnamurthy, and Kakade]{ash2021gone}
Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade.
\newblock Gone fishing: Neural active learning with fisher embeddings.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ash et~al.(2019)Ash, Zhang, Krishnamurthy, Langford, and
  Agarwal]{ash2019deep}
Jordan~T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh
  Agarwal.
\newblock Deep batch active learning by diverse, uncertain gradient lower
  bounds.
\newblock \emph{arXiv preprint arXiv:1906.03671}, 2019.

\bibitem[Audibert and Tsybakov(2007)]{audibert2007fast}
Jean-Yves Audibert and Alexandre~B Tsybakov.
\newblock Fast learning rates for plug-in classifiers.
\newblock \emph{The Annals of statistics}, 35\penalty0 (2):\penalty0 608--633,
  2007.

\bibitem[Balcan et~al.(2006)Balcan, Beygelzimer, and
  Langford]{balcan2006agnostic}
Maria-Florina Balcan, Alina Beygelzimer, and John Langford.
\newblock Agnostic active learning.
\newblock In \emph{Proceedings of the 23rd international conference on Machine
  learning}, pages 65--72, 2006.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
Peter~L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2285--2301, 2019.

\bibitem[Beygelzimer et~al.(2009)Beygelzimer, Dasgupta, and
  Langford]{beygelzimer2009importance}
Alina Beygelzimer, Sanjoy Dasgupta, and John Langford.
\newblock Importance weighted active learning.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 49--56, 2009.

\bibitem[Beygelzimer et~al.(2010)Beygelzimer, Hsu, Langford, and
  Zhang]{beygelzimer2010agnostic}
Alina Beygelzimer, Daniel~J Hsu, John Langford, and Tong Zhang.
\newblock Agnostic active learning without constraints.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Boucheron et~al.(2005)Boucheron, Bousquet, and
  Lugosi]{boucheron2005theory}
St{\'e}phane Boucheron, Olivier Bousquet, and G{\'a}bor Lugosi.
\newblock Theory of classification: A survey of some recent advances.
\newblock \emph{ESAIM: probability and statistics}, 9:\penalty0 323--375, 2005.

\bibitem[Castro and Nowak(2008)]{castro2008minimax}
Rui~M Castro and Robert~D Nowak.
\newblock Minimax bounds for active learning.
\newblock \emph{IEEE Transactions on Information Theory}, 54\penalty0
  (5):\penalty0 2339--2353, 2008.

\bibitem[Chow(1970)]{chow1970optimum}
CK~Chow.
\newblock On optimum recognition error and reject tradeoff.
\newblock \emph{IEEE Transactions on information theory}, 16\penalty0
  (1):\penalty0 41--46, 1970.

\bibitem[Citovsky et~al.(2021)Citovsky, DeSalvo, Gentile, Karydas, Rajagopalan,
  Rostamizadeh, and Kumar]{citovsky2021batch}
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand
  Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar.
\newblock Batch active learning at scale.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Cohn et~al.(1994)Cohn, Atlas, and Ladner]{cohn1994improving}
David Cohn, Les Atlas, and Richard Ladner.
\newblock Improving generalization with active learning.
\newblock \emph{Machine learning}, 15\penalty0 (2):\penalty0 201--221, 1994.

\bibitem[Cortes et~al.(2019)Cortes, DeSalvo, Mohri, Zhang, and
  Gentile]{cortes2019active}
Corinna Cortes, Giulia DeSalvo, Mehryar Mohri, Ningshan Zhang, and Claudio
  Gentile.
\newblock Active learning with disagreement graphs.
\newblock In \emph{International Conference on Machine Learning}, pages
  1379--1387. PMLR, 2019.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Dasgupta et~al.(2007)Dasgupta, Hsu, and
  Monteleoni]{dasgupta2007general}
Sanjoy Dasgupta, Daniel~J Hsu, and Claire Monteleoni.
\newblock A general agnostic active learning algorithm.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Emam et~al.(2021)Emam, Chu, Chiang, Czaja, Leapman, Goldblum, and
  Goldstein]{emam2021active}
Zeyad Ali~Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard
  Leapman, Micah Goldblum, and Tom Goldstein.
\newblock Active learning at the imagenet scale.
\newblock \emph{arXiv preprint arXiv:2111.12880}, 2021.

\bibitem[Foster et~al.(2018)Foster, Agarwal, Dud{\'\i}k, Luo, and
  Schapire]{foster2018practical}
Dylan Foster, Alekh Agarwal, Miroslav Dud{\'\i}k, Haipeng Luo, and Robert
  Schapire.
\newblock Practical contextual bandits with regression oracles.
\newblock In \emph{International Conference on Machine Learning}, pages
  1539--1548. PMLR, 2018.

\bibitem[Foster et~al.(2020)Foster, Rakhlin, Simchi-Levi, and
  Xu]{foster2020instance}
Dylan~J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu.
\newblock Instance-dependent complexity of contextual bandits and reinforcement
  learning: A disagreement-based perspective.
\newblock \emph{arXiv preprint arXiv:2010.03104}, 2020.

\bibitem[Freedman(1975)]{freedman1975tail}
David~A Freedman.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pages 100--118, 1975.

\bibitem[Friedman(2009)]{friedman2009active}
Eric Friedman.
\newblock Active learning for smooth problems.
\newblock In \emph{COLT}. Citeseer, 2009.

\bibitem[Hanneke(2007)]{hanneke2007bound}
Steve Hanneke.
\newblock A bound on the label complexity of agnostic active learning.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pages 353--360, 2007.

\bibitem[Hanneke(2014)]{hanneke2014theory}
Steve Hanneke.
\newblock Theory of active learning.
\newblock \emph{Foundations and Trends in Machine Learning}, 7\penalty0 (2-3),
  2014.

\bibitem[Haussler(1989)]{haussler1989decision}
David Haussler.
\newblock Decision theoretic generalizations of the pac model for neural net
  and other learning applications.
\newblock 1989.

\bibitem[Haussler(1995)]{haussler1995sphere}
David Haussler.
\newblock Sphere packing numbers for subsets of the boolean n-cube with bounded
  vapnik-chervonenkis dimension.
\newblock \emph{Journal of Combinatorial Theory, Series A}, 69\penalty0
  (2):\penalty0 217--232, 1995.

\bibitem[Heinonen(2005)]{heinonen2005lectures}
Juha Heinonen.
\newblock \emph{Lectures on Lipschitz analysis}.
\newblock Number 100. University of Jyv{\"a}skyl{\"a}, 2005.

\bibitem[Hornik(1991)]{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Huang et~al.(2015)Huang, Agarwal, Hsu, Langford, and
  Schapire]{huang2015efficient}
Tzu-Kuo Huang, Alekh Agarwal, Daniel~J Hsu, John Langford, and Robert~E
  Schapire.
\newblock Efficient and parsimonious agnostic active learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[K{\"a}{\"a}ri{\"a}inen(2006)]{kaariainen2006active}
Matti K{\"a}{\"a}ri{\"a}inen.
\newblock Active learning in the non-realizable case.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 63--77. Springer, 2006.

\bibitem[Karzand and Nowak(2020)]{karzand2020maximin}
Mina Karzand and Robert~D Nowak.
\newblock Maximin active learning in overparameterized model classes.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 167--177, 2020.

\bibitem[Kothawade et~al.(2021)Kothawade, Beck, Killamsetty, and
  Iyer]{kothawade2021similar}
Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer.
\newblock Similar: Submodular information measures based active learning in
  realistic scenarios.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Kpotufe et~al.(2021)Kpotufe, Yuan, and Zhao]{kpotufe2021nuances}
Samory Kpotufe, Gan Yuan, and Yunfan Zhao.
\newblock Nuances in margin conditions determine gains in active learning.
\newblock \emph{arXiv preprint arXiv:2110.08418}, 2021.

\bibitem[Krishnamurthy et~al.(2017)Krishnamurthy, Agarwal, Huang,
  Daum{\'e}~III, and Langford]{krishnamurthy2017active}
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daum{\'e}~III, and John
  Langford.
\newblock Active learning for cost-sensitive classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  1915--1924. PMLR, 2017.

\bibitem[Krishnamurthy et~al.(2019)Krishnamurthy, Agarwal, Huang,
  Daum{\'e}~III, and Langford]{krishnamurthy2019active}
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daum{\'e}~III, and John
  Langford.
\newblock Active learning for cost-sensitive classification.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--50,
  2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Li et~al.(2021)Li, Kamath, Foster, and Srebro]{li2021eluder}
Gene Li, Pritish Kamath, Dylan~J Foster, and Nathan Srebro.
\newblock Eluder dimension and generalized rank.
\newblock \emph{arXiv preprint arXiv:2104.06970}, 2021.

\bibitem[Locatelli et~al.(2017)Locatelli, Carpentier, and
  Kpotufe]{locatelli2017adaptivity}
Andrea Locatelli, Alexandra Carpentier, and Samory Kpotufe.
\newblock Adaptivity to noise parameters in nonparametric active learning.
\newblock In \emph{Proceedings of the 2017 Conference on Learning Theory,
  PMLR}, 2017.

\bibitem[Locatelli et~al.(2018)Locatelli, Carpentier, and
  Kpotufe]{locatelli2018adaptive}
Andrea Locatelli, Alexandra Carpentier, and Samory Kpotufe.
\newblock An adaptive strategy for active learning with smooth decision
  boundary.
\newblock In \emph{Algorithmic Learning Theory}, pages 547--571. PMLR, 2018.

\bibitem[Lu et~al.(2021)Lu, Shen, Yang, and Zhang]{lu2021deep}
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation for smooth functions.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 53\penalty0
  (5):\penalty0 5465--5506, 2021.

\bibitem[Massart and N{\'e}d{\'e}lec(2006)]{massart2006risk}
Pascal Massart and {\'E}lodie N{\'e}d{\'e}lec.
\newblock Risk bounds for statistical learning.
\newblock \emph{The Annals of Statistics}, 34\penalty0 (5):\penalty0
  2326--2366, 2006.

\bibitem[Minsker(2012)]{minsker2012plug}
Stanislav Minsker.
\newblock Plug-in approach to active learning.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (1), 2012.

\bibitem[Ongie et~al.(2020)Ongie, Willett, Soudry, and
  Srebro]{ongie2020function}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width relu nets: The
  multivariate case.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Parhi and Nowak(2021)]{parhi2021banach}
Rahul Parhi and Robert~D Nowak.
\newblock Banach space representer theorems for neural networks and ridge
  splines.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (43):\penalty0 1--40, 2021.

\bibitem[Parhi and Nowak(2022{\natexlab{a}})]{parhi2022kinds}
Rahul Parhi and Robert~D Nowak.
\newblock What kinds of functions do deep neural networks learn? insights from
  variational spline theory.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0
  (2):\penalty0 464--489, 2022{\natexlab{a}}.

\bibitem[Parhi and Nowak(2022{\natexlab{b}})]{parhi2022near}
Rahul Parhi and Robert~D Nowak.
\newblock Near-minimax optimal estimation with shallow relu neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 2022{\natexlab{b}}.

\bibitem[Pollard(1984)]{pollard1984convergence}
D~Pollard.
\newblock \emph{Convergence of Stochastic Processes}.
\newblock David Pollard, 1984.

\bibitem[Puchkin and Zhivotovskiy(2021)]{puchkin2021exponential}
Nikita Puchkin and Nikita Zhivotovskiy.
\newblock Exponential savings in agnostic active learning through abstention.
\newblock \emph{arXiv preprint arXiv:2102.00451}, 2021.

\bibitem[Ren et~al.(2021)Ren, Xiao, Chang, Huang, Li, Gupta, Chen, and
  Wang]{ren2021survey}
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij~B Gupta,
  Xiaojiang Chen, and Xin Wang.
\newblock A survey of deep active learning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (9):\penalty0 1--40,
  2021.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \emph{NIPS}, pages 2256--2264. Citeseer, 2013.

\bibitem[Sener and Savarese(2018)]{sener2018active}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Settles(2009)]{settles2009active}
Burr Settles.
\newblock Active learning literature survey.
\newblock 2009.

\bibitem[Shekhar et~al.(2021)Shekhar, Ghavamzadeh, and
  Javidi]{shekhar2021active}
Shubhanshu Shekhar, Mohammad Ghavamzadeh, and Tara Javidi.
\newblock Active learning for classification with abstention.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  2\penalty0 (2):\penalty0 705--719, 2021.

\bibitem[Tsybakov(2004)]{tsybakov2004optimal}
Alexander~B Tsybakov.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 135--166,
  2004.

\bibitem[Unser(2022)]{unser2022ridges}
Michael Unser.
\newblock Ridges, neural networks, and the radon transform.
\newblock \emph{arXiv preprint arXiv:2203.02543}, 2022.

\bibitem[Vapnik and Chervonenkis(1971)]{vapnik1971uniform}
VN~Vapnik and A~Ya Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock \emph{Theory of Probability and its Applications}, 16\penalty0
  (2):\penalty0 264, 1971.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang(2011)]{wang2011smoothness}
Liwei Wang.
\newblock Smoothness, disagreement coefficient, and the label complexity of
  agnostic active learning.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (7), 2011.

\bibitem[Wang et~al.(2021)Wang, Awasthi, Dann, Sekhari, and
  Gentile]{wang2021neural}
Zhilei Wang, Pranjal Awasthi, Christoph Dann, Ayush Sekhari, and Claudio
  Gentile.
\newblock Neural active learning with performance guarantees.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Yao(1977)]{yao1977probabilistic}
Andrew Chi-Chin Yao.
\newblock Probabilistic computations: Toward a unified measure of complexity.
\newblock In \emph{18th Annual Symposium on Foundations of Computer Science
  (sfcs 1977)}, pages 222--227. IEEE Computer Society, 1977.

\bibitem[Yarotsky(2017)]{yarotsky2017error}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\bibitem[Yarotsky(2018)]{yarotsky2018optimal}
Dmitry Yarotsky.
\newblock Optimal approximation of continuous functions by very deep relu
  networks.
\newblock In \emph{Conference on learning theory}, pages 639--649. PMLR, 2018.

\bibitem[Zhu and Nowak(2022)]{zhu2022efficient}
Yinglun Zhu and Robert Nowak.
\newblock Efficient active learning with abstention.
\newblock \emph{arXiv preprint arXiv:2204.00043}, 2022.

\end{thebibliography}
