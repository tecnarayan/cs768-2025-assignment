@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{zhou2021learning,
  title={Learning N: M fine-grained structured sparse neural networks from scratch},
  author={Zhou, Aojun and Ma, Yukun and Zhu, Junnan and Liu, Jianbo and Zhang, Zhijie and Yuan, Kun and Sun, Wenxiu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2102.04010},
  year={2021}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}
@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}
@article{evci2019difficulty,
  title={The difficulty of training sparse neural networks},
  author={Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  journal={arXiv preprint arXiv:1906.10732},
  year={2019}
}
@inproceedings{zou2019improved,
  title={An improved analysis of training over-parameterized deep neural networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2055--2064},
  year={2019}
}
@article{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1911.12360},
  year={2019}
}
@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}
@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis}
}
@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}
@article{mocanu2016topological,
  title={A topological insight into restricted boltzmann machines},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Machine Learning},
  volume={104},
  number={2-3},
  pages={243--270},
  year={2016},
  publisher={Springer}
}
@article{you2019drawing,
  title={Drawing early-bird tickets: Towards more efficient training of deep networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G and Wang, Zhangyang and Lin, Yingyan},
  journal={arXiv preprint arXiv:1909.11957},
  year={2019}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{gomez2019learning,
  title={Learning sparse networks using targeted dropout},
  author={Gomez, Aidan N and Zhang, Ivan and Kamalakara, Siddhartha Rao and Madaan, Divyam and Swersky, Kevin and Gal, Yarin and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1905.13678},
  year={2019}
}
@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}
@inproceedings{
jorge2021progressive,
title={Progressive Skeletonization: Trimming more fat from a network at initialization},
author={Pau de Jorge and Amartya Sanyal and Harkirat Behl and Philip Torr and Gr{\'e}gory Rogez and Puneet K. Dokania},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=9GsFOUyUPi}
}
@inproceedings{
bellec2018deep,
title={Deep Rewiring: Training very sparse deep networks},
author={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJ_wN01C-},
}
@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}
@inproceedings{
LIU2020Dynamic,
title={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},
author={Junjie Liu and Zhe Xu and Runbin Shi and Ray C. C. Cheung and Hayden K.H. So},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJlbGJrtDB}
}
@article{savarese2019winning,
  title={Winning the Lottery with Continuous Sparsification},
  author={Savarese, Pedro and Silva, Hugo and Maire, Michael},
  journal={arXiv preprint arXiv:1912.04427},
  year={2019}
}
@inproceedings{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in neural information processing systems},
  pages={2074--2082},
  year={2016}
}
@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={International Conference on Machine Learning},
  year={2017}
}
@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}
@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={11264--11272},
  year={2019}
}
@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}
@inproceedings{lin2017runtime,
  title={Runtime neural pruning},
  author={Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  booktitle={Advances in neural information processing systems},
  pages={2181--2191},
  year={2017}
}

@inproceedings{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  booktitle={Advances in neural information processing systems},
  pages={164--171},
  year={1993}
}
@article{louizos2017learning,
  title={Learning Sparse Neural Networks through $ L\_0 $ Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1712.01312},
  year={2017}
}
@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{liu2021selfish,
  title={Selfish sparse RNN training},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:2101.09048},
  year={2021}
}
@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={International Conference on Machine Learning},
  year={2019}
}
@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@inproceedings{sagawa2020investigation,
  title={An investigation of why overparameterization exacerbates spurious correlations},
  author={Sagawa, Shiori and Raghunathan, Aditi and Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={8346--8356},
  year={2020},
  organization={PMLR}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@misc{zhang2021subnetwork,
    title={Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?},
    author={Dinghuai Zhang and Kartik Ahuja and Yilun Xu and Yisen Wang and Aaron Courville},
    year={2021},
    eprint={2106.02890},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{wong2021leveraging,
  title={Leveraging Sparse Linear Layers for Debuggable Deep Networks},
  author={Wong, Eric and Santurkar, Shibani and Madry, Aleksander},
  journal={arXiv preprint arXiv:2105.04857},
  year={2021}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}
@inproceedings{evci2019rigging,
  title={Rigging the Lottery: Making All Tickets Winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  year={2020}
}
@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
@inproceedings{
lee2019signal,
title={A signal propagation perspective for pruning neural networks at initialization},
author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip HS},
booktitle={International Conference on Learning Representations},
year={2019},
}
@inproceedings{
anonymous2021gradient,
title={Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=V1N4GEWki_E},
note={under review}
}
@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@inproceedings{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  booktitle={Advances in neural information processing systems},
  pages={1379--1387},
  year={2016}
}
@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{zhang2019one,
  title={One-shot pruning of recurrent neural networks by jacobian spectrum evaluation},
  author={Zhang, Matthew Shunshi and Stadie, Bradly},
  journal={arXiv preprint arXiv:1912.00120},
  year={2019}
}
@inproceedings{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  booktitle={International Conference on Learning Representations},
  year={2017},
}
@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}
@inproceedings{liu2019rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{dai2018grow,
  title={Grow and prune compact, fast, and accurate LSTMs},
  author={Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K},
  journal={arXiv preprint arXiv:1805.11797},
  year={2018}
}
@article{dai2019nest,
  title={NeST: A neural network synthesis tool based on a grow-and-prune paradigm},
  author={Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K},
  journal={IEEE Transactions on Computers},
  volume={68},
  number={10},
  pages={1487--1497},
  year={2019},
  publisher={IEEE}
} 
@article{jayakumar2020top,
  title={Top-KAST: Top-K Always Sparse Training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{raihan2020sparse,
  title={Sparse weight activation training},
  author={Raihan, Md Aamir and Aamodt, Tor M},
  journal={arXiv preprint arXiv:2001.01969},
  year={2020}
}
@article{atashgahi2020quick,
  title={Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders},
  author={Atashgahi, Zahra and Sokar, Ghada and van der Lee, Tim and Mocanu, Elena and Mocanu, Decebal Constantin and Veldhuis, Raymond and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:2012.00560},
  year={2020}
}
@article{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A},
  journal={arXiv preprint arXiv:1803.00885},
  year={2018}
}
@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8789--8798},
  year={2018}
}
@article{stanley2002evolving,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  publisher={MIT Press}
}
@article{evci2020gradient,
  title={Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win},
  author={Evci, Utku and Ioannou, Yani A and Keskin, Cem and Dauphin, Yann},
  journal={arXiv preprint arXiv:2010.03533},
  year={2020}
}
@article{frankle2020pruning,
  title={Pruning Neural Networks at Initialization: Why are We Missing the Mark?},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:2009.08576},
  year={2020}
}
@inproceedings{dai2018compressing,
  title={Compressing neural networks using the variational information bottleneck},
  author={Dai, Bin and Zhu, Chen and Guo, Baining and Wipf, David},
  booktitle={International Conference on Machine Learning},
  pages={1135--1144},
  year={2018},
  organization={PMLR}
}
@inproceedings{xiao2019autoprune,
  title={Autoprune: Automatic network pruning by regularizing auxiliary parameters},
  author={Xiao, Xia and Wang, Zigeng and Rajasekaran, Sanguthevar},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13681--13691},
  year={2019}
}
@inproceedings{kusupati2020soft,
  title={Soft Threshold Weight Reparameterization for Learnable Sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={International Conference on Machine Learning},
  year={2020}
}
@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={138--145},
  year={2017}
}
@inproceedings{prabhu2018deep,
  title={Deep expander networks: Efficient deep networks from graph theory},
  author={Prabhu, Ameya and Varma, Girish and Namboodiri, Anoop},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={20--35},
  year={2018}
}
@inproceedings{kepner2019radix,
  title={Radix-net: Structured sparse matrices for deep neural networks},
  author={Kepner, Jeremy and Robinett, Ryan},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={268--274},
  year={2019},
  organization={IEEE}
}
@inproceedings{liu2020topological,
  title={Topological Insights into Sparse Neural Networks},
  author={Liu, Shiwei and van der Lee, TT and Yaman, Anil and Atashgahi, Zahra and Ferrar, D and Sokar, Ghada and Pechenizkiy, Mykola and Mocanu, DC},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year={2020}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@inproceedings{goodfellow2014qualitatively,
  title={Qualitatively characterizing neural network optimization problems},
  author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  booktitle={International Conference on Learning Representations},
  year={2015}
}
@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}
@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}
@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@article{brutzkus2017sgd,
  title={Sgd learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1710.10174},
  year={2017}
}
@inproceedings{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{zhang2019taming,
  title={Taming the noisy gradient: train deep neural networks with small batch sizes},
  author={Zhang, Yikai and Qu, Hui},
  booktitle={IJCAI},
  year={2019}
}
@inproceedings{
lee2018snip,
title={{SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION} {SENSITIVITY}},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1VZqjAcYX},
}
@inproceedings{
Lee2020A,
title={A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Stephen Gould and Philip H. S. Torr},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJeTo2VFwH}
}
@inproceedings{
Wang2020Picking,
title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgsACVKPH}
}
@phdthesis{setphdthesis2017,
title = "Network computations in artificial intelligence",
author = "Decebal Constantin Mocanu",
year = "2017",
month = jun,
day = "29",
language = "English",
isbn = "978-90-386-4305-2",
school = "Technische Universiteit Eindhoven",
}

@article{onemillionneurons,
  author    = {Shiwei Liu and
               Decebal Constantin Mocanu and
               Amarsagar Reddy Ramapuram Matavalam and
               Yulong Pei and
               Mykola Pechenizkiy},
  title     = {Sparse evolutionary Deep Learning with over one million artificial
               neurons on commodity hardware},
  journal   = {Neural Computing and Applications},
  year      = {2020},
}

@inproceedings{ICML-2019-PetersonB0GR,
	author        = "Joshua C. Peterson and David Bourgin and Daniel Reichman 0001 and Thomas L. Griffiths and Stuart J. Russell",
	booktitle     = "{Proceedings of the 36th International Conference on Machine Learning}",
	pages         = "5133--5141",
	publisher     = "{PMLR}",
	title         = "{Cognitive model priors for predicting human decisions}",
	year          = 2019,
}
@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  journal={arXiv preprint arXiv:2006.05467},
  year={2020}
}
@article{de2020progressive,
  title={Progressive skeletonization: Trimming more fat from a network at initialization},
  author={de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
  journal={arXiv preprint arXiv:2006.09081},
  year={2020}
}