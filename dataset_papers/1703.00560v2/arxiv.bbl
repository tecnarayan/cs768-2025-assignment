\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bottou(1988)]{bottou-88b}
Bottou, {L\'eon}.
\newblock Reconnaissance de la parole par reseaux connexionnistes.
\newblock In \emph{Proceedings of Neuro Nimes 88}, pp.\  197--218, Nimes,
  France, 1988.
\newblock URL \url{http://leon.bottou.org/papers/bottou-88b}.

\bibitem[Brading \& Castellani(2003)Brading and
  Castellani]{brading2003symmetries}
Brading, Katherine and Castellani, Elena.
\newblock \emph{Symmetries in physics: philosophical reflections}.
\newblock Cambridge University Press, 2003.

\bibitem[Choromanska et~al.(2015{\natexlab{a}})Choromanska, Henaff, Mathieu,
  Arous, and LeCun]{choromanska2015loss}
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, G{\'e}rard~Ben, and
  LeCun, Yann.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{AISTATS}, 2015{\natexlab{a}}.

\bibitem[Choromanska et~al.(2015{\natexlab{b}})Choromanska, LeCun, and
  Arous]{choromanska2015open}
Choromanska, Anna, LeCun, Yann, and Arous, G{\'e}rard~Ben.
\newblock Open problem: The landscape of the loss surfaces of multilayer
  networks.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory, COLT
  2015, Paris, France, July 3}, volume~6, pp.\  1756--1760, 2015{\natexlab{b}}.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphin2014identifying}
Dauphin, Yann~N, Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli,
  Surya, and Bengio, Yoshua.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2933--2941, 2014.

\bibitem[Fukumizu \& Amari(2000)Fukumizu and Amari]{fukumizu2000local}
Fukumizu, Kenji and Amari, Shun-ichi.
\newblock Local minima and plateaus in hierarchical structures of multilayer
  perceptrons.
\newblock \emph{Neural Networks}, 13\penalty0 (3):\penalty0 317--327, 2000.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{xavier}
Glorot, Xavier and Bengio, Yoshua.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Aistats}, volume~9, pp.\  249--256, 2010.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{PReLU}
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
\newblock Deep residual learning for image recognition.
\newblock \emph{Computer Vision anad Pattern Recognition (CVPR)}, 2016.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}
Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George~E, Mohamed, Abdel-rahman,
  Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick,
  Sainath, Tara~N, et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Hochreiter et~al.(1995)Hochreiter, Schmidhuber,
  et~al.]{hochreiter1995simplifying}
Hochreiter, Sepp, Schmidhuber, J{\"u}rgen, et~al.
\newblock Simplifying neural nets by discovering flat minima.
\newblock \emph{Advances in Neural Information Processing Systems}, pp.\
  529--536, 1995.

\bibitem[Janzamin et~al.(2015)Janzamin, Sedghi, and
  Anandkumar]{janzamin2015beating}
Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock \emph{CoRR abs/1506.08473}, 2015.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, Kenji.
\newblock Deep learning without poor local minima.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LaSalle \& Lefschetz(1961)LaSalle and Lefschetz]{lyapunov}
LaSalle, J.~P. and Lefschetz, S.
\newblock Stability by lyapunov's second method with applications.
\newblock \emph{New York: Academic Press.}, 1961.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Yann~A, Bottou, L{\'e}on, Orr, Genevieve~B, and M{\"u}ller,
  Klaus-Robert.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Mei et~al.(2016)Mei, Bai, and Montanari]{mei2016landscape}
Mei, Song, Bai, Yu, and Montanari, Andrea.
\newblock The landscape of empirical risk for non-convex losses.
\newblock \emph{arXiv preprint arXiv:1607.06534}, 2016.

\bibitem[Saad \& Solla(1996)Saad and Solla]{saad1996dynamics}
Saad, David and Solla, Sara~A.
\newblock Dynamics of on-line gradient descent learning for multilayer neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, pp.\
  302--308, 1996.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, Andrew~M, McClelland, James~L, and Ganguli, Surya.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{vgg}
Simonyan, Karen and Zisserman, Andrew.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2015.

\bibitem[Soudry \& Carmon(2016)Soudry and Carmon]{soudry2016no}
Soudry, Daniel and Carmon, Yair.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc~V.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3104--3112, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{inception}
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott,
  Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich,
  Andrew.
\newblock Going deeper with convolutions.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, pp.\  1--9,
  2015.

\bibitem[Zhang et~al.(2017)Zhang, Panigrahy, and Sachdeva]{eletron-proton}
Zhang, Qiuyi, Panigrahy, Rina, and Sachdeva, Sushant.
\newblock Electron-proton dynamics in deep learning.
\newblock \emph{arXiv preprint arXiv:1702.00458}, 2017.

\end{thebibliography}
