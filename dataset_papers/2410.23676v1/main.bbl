\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[A~Ayyubi et~al.(2023)A~Ayyubi, Liu, Nagrani, Lin, Zhang, Arnab, Han,
  Zhu, Liu, and Chang]{ayyubi2023video}
Hammad A~Ayyubi, Tianqi Liu, Arsha Nagrani, Xudong Lin, Mingda Zhang, Anurag
  Arnab, Feng Han, Yukun Zhu, Jialu Liu, and Shih-Fu Chang.
\newblock Video summarization: towards entity-aware captions.
\newblock \emph{arXiv preprint arXiv:2312.02188}, 2023.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu,
  Marathe, Bitton, Gadre, Sagawa, Jitsev, Kornblith, Koh, Ilharco, Wortsman,
  and Schmidt]{awadalla2023openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu,
  Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev,
  Simon Kornblith, Pang~Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig
  Schmidt.
\newblock Openflamingo: An open-source framework for training large
  autoregressive vision-language models.
\newblock \emph{arXiv preprint arXiv:2308.01390}, 2023.

\bibitem[Beyer et~al.(2024)Beyer, Steiner, Pinto, Kolesnikov, Wang, Salz,
  Neumann, Alabdulmohsin, Tschannen, Bugliarello, et~al.]{beyer2024paligemma}
Lucas Beyer, Andreas Steiner, Andr{\'e}~Susano Pinto, Alexander Kolesnikov,
  Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael
  Tschannen, Emanuele Bugliarello, et~al.
\newblock Paligemma: A versatile 3b vlm for transfer.
\newblock \emph{arXiv preprint arXiv:2407.07726}, 2024.

\bibitem[Biten et~al.(2019)Biten, Gomez, Rusinol, and Karatzas]{biten2019good}
Ali~Furkan Biten, Lluis Gomez, Mar{\c{c}}al Rusinol, and Dimosthenis Karatzas.
\newblock Good news, everyone! context driven entity-aware captioning for news
  images.
\newblock In \emph{CVPR}, 2019.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and
  Van~Gool]{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In \emph{ECCV}, 2014.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Caron et~al.(2024)Caron, Iscen, Fathi, and
  Schmid]{caron2024generative}
Mathilde Caron, Ahmet Iscen, Alireza Fathi, and Cordelia Schmid.
\newblock A generative approach for wikipedia-scale visual entity recognition.
\newblock In \emph{CVPR}, 2024.

\bibitem[Changpinyo et~al.(2022)Changpinyo, Kukliansky, Szpektor, Chen, Ding,
  and Soricut]{changpinyo2022all}
Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, and
  Radu Soricut.
\newblock All you may need for vqa are image captions.
\newblock \emph{arXiv preprint arXiv:2205.01883}, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wang, Changpinyo, Piergiovanni,
  Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{pali2022}
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski,
  Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
  et~al.
\newblock Pali: A jointly-scaled multilingual language-image model.
\newblock \emph{ICLR}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Hu, Luan, Sun, Changpinyo,
  Ritter, and Chang]{chen2023can}
Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter,
  and Ming-Wei Chang.
\newblock Can pre-trained vision and language models answer visual
  information-seeking questions?
\newblock \emph{arXiv preprint arXiv:2302.11713}, 2023{\natexlab{b}}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, 2023.

\bibitem[De~Cao et~al.(2020)De~Cao, Izacard, Riedel, and
  Petroni]{de2020autoregressive}
Nicola De~Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni.
\newblock Autoregressive entity retrieval.
\newblock \emph{arXiv preprint arXiv:2010.00904}, 2020.

\bibitem[Everingham et~al.(2010)Everingham, Van~Gool, Williams, Winn, and
  Zisserman]{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock \emph{IJCV}, 88, 2010.

\bibitem[Fu et~al.(2021)Fu, Wang, and Yang]{fu2021mm}
Xiyan Fu, Jun Wang, and Zhenglu Yang.
\newblock Mm-avs: A full-scale dataset for multi-modal summarization.
\newblock In \emph{NAACL}, 2021.

\bibitem[{Gemini Team Google}(2023)]{team2023gemini}
{Gemini Team Google}.
\newblock Gemini: A family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna,
  Lee, and Pfister]{hsieh2023distilling}
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii,
  Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
\newblock Distilling step-by-step! outperforming larger language models with
  less training data and smaller model sizes.
\newblock \emph{arXiv preprint arXiv:2305.02301}, 2023.

\bibitem[Hu et~al.(2023)Hu, Luan, Chen, Khandelwal, Joshi, Lee, Toutanova, and
  Chang]{hu2023open}
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee,
  Kristina Toutanova, and Ming-Wei Chang.
\newblock Open-domain visual entity recognition: Towards recognizing millions
  of wikipedia entities.
\newblock \emph{ICCV}, 2023.

\bibitem[Iscen et~al.(2024)Iscen, Caron, Fathi, and Schmid]{iscen2023retrieval}
Ahmet Iscen, Mathilde Caron, Alireza Fathi, and Cordelia Schmid.
\newblock Retrieval-enhanced contrastive vision-text models.
\newblock \emph{ICLR}, 2024.

\bibitem[Khosla et~al.(2011)Khosla, Jayadevaprakash, Yao, and
  Fei-Fei]{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei.
\newblock Novel dataset for fine-grained image categorization.
\newblock In \emph{First Workshop on Fine-Grained Visual Categorization, CVPR},
  2011.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{krause20133d}
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{ICCV workshops}, 2013.

\bibitem[Kuznetsova et~al.(2020)Kuznetsova, Rom, Alldrin, Uijlings, Krasin,
  Pont-Tuset, Kamali, Popov, Malloci, Kolesnikov, et~al.]{kuznetsova2020open}
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
  Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
  Kolesnikov, et~al.
\newblock The open images dataset v4: Unified image classification, object
  detection, and visual relationship detection at scale.
\newblock \emph{IJCV}, 2020.

\bibitem[Lerner et~al.(2022)Lerner, Ferret, Guinaudeau, Le~Borgne,
  Besan{\c{c}}on, Moreno, and Lov{\'o}n~Melgarejo]{lerner2022viquae}
Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv{\'e} Le~Borgne, Romaric
  Besan{\c{c}}on, Jos{\'e}~G Moreno, and Jes{\'u}s Lov{\'o}n~Melgarejo.
\newblock Viquae, a dataset for knowledge-based visual question answering about
  named entities.
\newblock In \emph{ACM SIGIR}, 2022.

\bibitem[Lin et~al.(2022)Lin, Petroni, Bertasius, Rohrbach, Chang, and
  Torresani]{lin2022learning}
Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and
  Lorenzo Torresani.
\newblock Learning to recognize procedural activities with distant supervision.
\newblock In \emph{CVPR}, 2022.

\bibitem[Liu et~al.(2020)Liu, Wang, Wang, and Ordonez]{liu2020visual}
Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez.
\newblock Visual news: Benchmark and challenges in news image captioning.
\newblock \emph{arXiv preprint arXiv:2010.03743}, 2020.

\bibitem[Liu et~al.(2023)Liu, Son, Yang, Liu, Gao, Lee, and
  Li]{liu2023learning}
Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong~Jae Lee, and
  Chunyuan Li.
\newblock Learning customized visual models with retrieval-augmented knowledge.
\newblock In \emph{CVPR}, 2023.

\bibitem[Liu et~al.(2024)Liu, Li, Wu, and Lee]{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2018)Lu, Whitehead, Huang, Ji, and Chang]{lu2018entity}
Di Lu, Spencer Whitehead, Lifu Huang, Heng Ji, and Shih-Fu Chang.
\newblock Entity-aware image caption generation.
\newblock \emph{arXiv preprint arXiv:1804.07889}, 2018.

\bibitem[Maji et~al.(2013)Maji, Kannala, Rahtu, Blaschko, and
  Vedaldi]{maji13fine-grained}
S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock Technical report, 2013.

\bibitem[Mehta et~al.(2022)Mehta, Gupta, Tay, Dehghani, Tran, Rao, Najork,
  Strubell, and Metzler]{mehta2022dsi++}
Sanket~Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh~Q Tran, Jinfeng
  Rao, Marc Najork, Emma Strubell, and Donald Metzler.
\newblock Dsi++: Updating transformer memory with new documents.
\newblock \emph{arXiv preprint arXiv:2212.09744}, 2022.

\bibitem[Mensink et~al.(2023)Mensink, Uijlings, Castrejon, Goel, Cadar, Zhou,
  Sha, Araujo, and Ferrari]{mensink2023encyclopedic}
Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar,
  Howard Zhou, Fei Sha, Andr{\'e} Araujo, and Vittorio Ferrari.
\newblock Encyclopedic vqa: Visual questions about detailed properties of
  fine-grained categories.
\newblock In \emph{ICCV}, 2023.

\bibitem[Momeni et~al.(2023)Momeni, Caron, Nagrani, Zisserman, and
  Schmid]{momeni2023verbs}
Liliane Momeni, Mathilde Caron, Arsha Nagrani, Andrew Zisserman, and Cordelia
  Schmid.
\newblock Verbs in action: Improving verb understanding in video-language
  models.
\newblock In \emph{ICCV}, 2023.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock \emph{NeurIPS}, 2019.

\bibitem[Nguyen et~al.(2023)Nguyen, Biten, Mafla, Gomez, and
  Karatzas]{nguyen2023show}
Khanh Nguyen, Ali~Furkan Biten, Andres Mafla, Lluis Gomez, and Dimosthenis
  Karatzas.
\newblock Show, interpret and tell: entity-aware contextualised image
  captioning in wikipedia.
\newblock In \emph{AAAI}, 2023.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian conference on computer vision, graphics \&
  image processing}, 2008.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[{OpenAI}(2023)]{openai-gpt4vision}
{OpenAI}.
\newblock Gpt-4v(ision) system card.
\newblock System Card, 2023.
\newblock Version 1.0.

\bibitem[Oquab et~al.(2023)Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov,
  Fernandez, Haziza, Massa, El-Nouby, et~al.]{oquab2023dinov2}
Maxime Oquab, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni, Huy Vo, Marc
  Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa,
  Alaaeldin El-Nouby, et~al.
\newblock Dinov2: Learning robust visual features without supervision.
\newblock \emph{arXiv preprint arXiv:2304.07193}, 2023.

\bibitem[Pradeep et~al.(2023)Pradeep, Hui, Gupta, Lelkes, Zhuang, Lin, Metzler,
  and Tran]{pradeep2023does}
Ronak Pradeep, Kai Hui, Jai Gupta, Adam~D Lelkes, Honglei Zhuang, Jimmy Lin,
  Donald Metzler, and Vinh~Q Tran.
\newblock How does generative retrieval scale to millions of passages?
\newblock \emph{arXiv preprint arXiv:2305.11841}, 2023.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Rajput et~al.(2023)Rajput, Mehta, Singh, Keshavan, Vu, Heldt, Hong,
  Tay, Tran, Samost, et~al.]{rajput2023recommender}
Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan~H Keshavan, Trung Vu,
  Lukasz Heldt, Lichan Hong, Yi Tay, Vinh~Q Tran, Jonah Samost, et~al.
\newblock Recommender systems with generative retrieval.
\newblock \emph{arXiv preprint arXiv:2305.05065}, 2023.

\bibitem[Ridnik et~al.(2021)Ridnik, Ben-Baruch, Noy, and
  Zelnik-Manor]{ridnik2021imagenet}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock Imagenet-21k pretraining for the masses.
\newblock \emph{arXiv preprint arXiv:2104.10972}, 2021.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{IJCV}, 2015.

\bibitem[Santurkar et~al.(2022)Santurkar, Dubois, Taori, Liang, and
  Hashimoto]{athousandwords}
Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori
  Hashimoto.
\newblock Is a caption worth a thousand images? a controlled study for
  representation learning.
\newblock \emph{arXiv preprint arXiv:2207.07635}, 2022.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock \emph{arXiv preprint arXiv:2210.08402}, 2022.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{ACL}, 2018.

\bibitem[Shi et~al.(2023)Shi, Xu, Hu, and Zhang]{shi2023generative}
Senbao Shi, Zhenran Xu, Baotian Hu, and Min Zhang.
\newblock Generative multimodal entity linking.
\newblock \emph{arXiv preprint arXiv:2306.12725}, 2023.

\bibitem[Sun et~al.(2023)Sun, Yan, Chen, Wang, Zhu, Ren, Chen, Yin, Rijke, and
  Ren]{sun2023learning}
Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie
  Ren, Zhumin Chen, Dawei Yin, Maarten Rijke, and Zhaochun Ren.
\newblock Learning to tokenize for generative retrieval.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Tay et~al.(2022)Tay, Tran, Dehghani, Ni, Bahri, Mehta, Qin, Hui, Zhao,
  Gupta, et~al.]{tay2022transformer}
Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen
  Qin, Kai Hui, Zhe Zhao, Jai Gupta, et~al.
\newblock Transformer memory as a differentiable search index.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,
  Sifre, Rivi{\`e}re, Kale, Love, et~al.]{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
  Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,
  Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tran et~al.(2020)Tran, Mathews, and Xie]{tran2020transform}
Alasdair Tran, Alexander Mathews, and Lexing Xie.
\newblock Transform and tell: Entity-aware news image captioning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem[Wang et~al.(2022)Wang, Yang, Hu, Li, Lin, Gan, Liu, Liu, and
  Wang]{wang2022git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
  Zicheng Liu, Ce Liu, and Lijuan Wang.
\newblock Git: A generative image-to-text transformer for vision and language.
\newblock \emph{arXiv preprint arXiv:2205.14100}, 2022.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and
  Torralba]{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{CVPR}, 2010.

\bibitem[Xiao et~al.(2024)Xiao, Gong, Cascante-Bonilla, Zhang, Wu, and
  Ordonez]{xiao2024grounding}
Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, and
  Vicente Ordonez.
\newblock Grounding language models for visual entity recognition.
\newblock \emph{arXiv preprint arXiv:2402.18695}, 2024.

\bibitem[Yang et~al.(2021)Yang, Miech, Sivic, Laptev, and
  Schmid]{yang2021justask}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In \emph{ICCV}, 2021.

\bibitem[Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati,
  Hessel, Farhadi, and Choi]{zellers2022merlotreserve}
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza
  Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.
\newblock Merlot reserve: Multimodal neural script knowledge through vision and
  language and sound.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhao and Wu(2023)]{zhao2023boosting}
Wentian Zhao and Xinxiao Wu.
\newblock Boosting entity-aware image captioning with multi-modal knowledge
  graph.
\newblock \emph{IEEE Transactions on Multimedia}, 2023.

\bibitem[Zhao et~al.(2022)Zhao, Misra, Kr{\"a}henb{\"u}hl, and
  Girdhar]{zhao2022lavila}
Yue Zhao, Ishan Misra, Philipp Kr{\"a}henb{\"u}hl, and Rohit Girdhar.
\newblock Learning video representations from large language models.
\newblock \emph{arXiv preprint arXiv:2212.04501}, 2022.

\bibitem[Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{zhou2017places}
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE TPAMI}, 2017.

\end{thebibliography}
