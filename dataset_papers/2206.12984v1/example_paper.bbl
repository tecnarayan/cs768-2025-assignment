\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bram et~al.(2019)Bram, Brunner, Richter, and
  Wattenhofer]{bram2019attentive}
Bram, T., Brunner, G., Richter, O., and Wattenhofer, R.
\newblock Attentive multi-task deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.02874}, 2019.

\bibitem[Chen et~al.(2022)Chen, Xu, and Agrawal]{chen2022system}
Chen, T., Xu, J., and Agrawal, P.
\newblock A system for general in-hand object re-orientation.
\newblock In \emph{Conference on Robot Learning}, pp.\  297--307. PMLR, 2022.

\bibitem[Cobbe et~al.(2020{\natexlab{a}})Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020{\natexlab{a}}.

\bibitem[Cobbe et~al.(2020{\natexlab{b}})Cobbe, Hesse, Hilton, and
  Schulman]{procgen}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020{\natexlab{b}}.

\bibitem[Cobbe et~al.(2021)Cobbe, Hilton, Klimov, and
  Schulman]{cobbe2021phasic}
Cobbe, K.~W., Hilton, J., Klimov, O., and Schulman, J.
\newblock Phasic policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2020--2027. PMLR, 2021.

\bibitem[Czarnecki et~al.(2018)Czarnecki, Jayakumar, Jaderberg, Hasenclever,
  Teh, Heess, Osindero, and Pascanu]{czarnecki2018mix}
Czarnecki, W., Jayakumar, S., Jaderberg, M., Hasenclever, L., Teh, Y.~W.,
  Heess, N., Osindero, S., and Pascanu, R.
\newblock Mix \& match agent curricula for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1087--1095. PMLR, 2018.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019provably}
Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J.
\newblock Provably efficient rl with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1665--1674. PMLR, 2019.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1407--1416. PMLR, 2018.

\bibitem[Filos et~al.(2020)Filos, Tigkas, McAllister, Rhinehart, Levine, and
  Gal]{filos2020can}
Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine, S., and Gal, Y.
\newblock Can autonomous vehicles identify, recover from, and adapt to
  distribution shifts?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3145--3153. PMLR, 2020.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pp.\  49--58.
  PMLR, 2016.

\bibitem[Gan et~al.(2020)Gan, Schwartz, Alter, Schrimpf, Traer, De~Freitas,
  Kubilius, Bhandwaldar, Haber, Sano, et~al.]{gan2020threedworld}
Gan, C., Schwartz, J., Alter, S., Schrimpf, M., Traer, J., De~Freitas, J.,
  Kubilius, J., Bhandwaldar, A., Haber, N., Sano, M., et~al.
\newblock Threedworld: A platform for interactive multi-modal physical
  simulation.
\newblock \emph{arXiv preprint arXiv:2007.04954}, 2020.

\bibitem[Ghosh et~al.(2017)Ghosh, Singh, Rajeswaran, Kumar, and
  Levine]{ghosh2017divide}
Ghosh, D., Singh, A., Rajeswaran, A., Kumar, V., and Levine, S.
\newblock Divide-and-conquer reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.09874}, 2017.

\bibitem[Gupta et~al.(2018)Gupta, Murali, Gandhi, and Pinto]{gupta2018robot}
Gupta, A., Murali, A., Gandhi, D., and Pinto, L.
\newblock Robot learning in homes: Improving generalization and reducing
  dataset bias.
\newblock \emph{arXiv preprint arXiv:1807.07049}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{gail}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4565--4573, 2016.

\bibitem[Igl et~al.(2019)Igl, Ciosek, Li, Tschiatschek, Zhang, Devlin, and
  Hofmann]{igl2019generalization}
Igl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin, S., and
  Hofmann, K.
\newblock Generalization in reinforcement learning with selective noise
  injection and information bottleneck.
\newblock \emph{arXiv preprint arXiv:1910.12911}, 2019.

\bibitem[James et~al.(2020)James, Ma, Arrojo, and Davison]{james2020rlbench}
James, S., Ma, Z., Arrojo, D.~R., and Davison, A.~J.
\newblock Rlbench: The robot learning benchmark \& learning environment.
\newblock \emph{IEEE Robotics and Automation Letters}, 5\penalty0 (2):\penalty0
  3019--3026, 2020.

\bibitem[Jiang et~al.(2021)Jiang, Grefenstette, and
  Rockt{\"a}schel]{jiang2021prioritized}
Jiang, M., Grefenstette, E., and Rockt{\"a}schel, T.
\newblock Prioritized level replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4940--4950. PMLR, 2021.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Kurin et~al.(2022)Kurin, De~Palma, Kostrikov, Whiteson, and
  Kumar]{kurin2022defense}
Kurin, V., De~Palma, A., Kostrikov, I., Whiteson, S., and Kumar, M.~P.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock \emph{arXiv preprint arXiv:2201.04122}, 2022.

\bibitem[Mu et~al.(2020)Mu, Gu, Jia, Tang, and Su]{mu2020refactoring}
Mu, T., Gu, J., Jia, Z., Tang, H., and Su, H.
\newblock Refactoring policy for compositional generalizability using
  self-supervised object proposals.
\newblock \emph{arXiv preprint arXiv:2011.00971}, 2020.

\bibitem[Mu et~al.(2021)Mu, Ling, Xiang, Yang, Li, Tao, Huang, Jia, and
  Su]{mu2021maniskill}
Mu, T., Ling, Z., Xiang, F., Yang, D., Li, X., Tao, S., Huang, Z., Jia, Z., and
  Su, H.
\newblock Maniskill: Generalizable manipulation skill benchmark with
  large-scale demonstrations.
\newblock \emph{arXiv preprint arXiv:2107.14483}, 2021.

\bibitem[Raileanu \& Fergus(2021)Raileanu and Fergus]{raileanu2021decoupling}
Raileanu, R. and Fergus, R.
\newblock Decoupling value and policy for generalization in reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2102.10330}, 2021.

\bibitem[Raileanu et~al.(2020)Raileanu, Goldstein, Yarats, Kostrikov, and
  Fergus]{raileanu2020automatic}
Raileanu, R., Goldstein, M., Yarats, D., Kostrikov, I., and Fergus, R.
\newblock Automatic data augmentation for generalization in deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2006.12862}, 2020.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{dapg}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
Ross, S., Gordon, G., and Bagnell, D.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Rusu et~al.(2015)Rusu, Colmenarejo, Gulcehre, Desjardins, Kirkpatrick,
  Pascanu, Mnih, Kavukcuoglu, and Hadsell]{rusu2015policy}
Rusu, A.~A., Colmenarejo, S.~G., Gulcehre, C., Desjardins, G., Kirkpatrick, J.,
  Pascanu, R., Mnih, V., Kavukcuoglu, K., and Hadsell, R.
\newblock Policy distillation.
\newblock \emph{arXiv preprint arXiv:1511.06295}, 2015.

\bibitem[Schaul et~al.(2019)Schaul, Borsa, Modayil, and Pascanu]{schaul2019ray}
Schaul, T., Borsa, D., Modayil, J., and Pascanu, R.
\newblock Ray interference: a source of plateaus in deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1904.11455}, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{GAE}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015.

\bibitem[Schulman et~al.(2017{\natexlab{a}})Schulman, Wolski, Dhariwal,
  Radford, and Klimov]{PPO}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017{\natexlab{a}}.

\bibitem[Schulman et~al.(2017{\natexlab{b}})Schulman, Wolski, Dhariwal,
  Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017{\natexlab{b}}.

\bibitem[Shen et~al.(2022)Shen, Wan, and Wang]{shen2022learning}
Shen, H., Wan, W., and Wang, H.
\newblock Learning category-level generalizable object manipulation policy via
  generative adversarial self-imitation learning from demonstrations.
\newblock \emph{arXiv preprint arXiv:2203.02107}, 2022.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sodhani et~al.(2021{\natexlab{a}})Sodhani, Zhang, and
  Pineau]{pmlr-v139-sodhani21a}
Sodhani, S., Zhang, A., and Pineau, J.
\newblock Multi-task reinforcement learning with context-based representations.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9767--9779. PMLR,
  18--24 Jul 2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v139/sodhani21a.html}.

\bibitem[Sodhani et~al.(2021{\natexlab{b}})Sodhani, Zhang, and
  Pineau]{sodhani2021multi}
Sodhani, S., Zhang, A., and Pineau, J.
\newblock Multi-task reinforcement learning with context-based representations.
\newblock \emph{arXiv preprint arXiv:2102.06177}, 2021{\natexlab{b}}.

\bibitem[Szot et~al.(2021)Szot, Clegg, Undersander, Wijmans, Zhao, Turner,
  Maestre, Mukadam, Chaplot, Maksymets, et~al.]{szot2021habitat}
Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J.,
  Maestre, N., Mukadam, M., Chaplot, D.~S., Maksymets, O., et~al.
\newblock Habitat 2.0: Training home assistants to rearrange their habitat.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Team et~al.(2021)Team, Stooke, Mahajan, Barros, Deck, Bauer,
  Sygnowski, Trebacz, Jaderberg, Mathieu, et~al.]{team2021open}
Team, O. E.~L., Stooke, A., Mahajan, A., Barros, C., Deck, C., Bauer, J.,
  Sygnowski, J., Trebacz, M., Jaderberg, M., Mathieu, M., et~al.
\newblock Open-ended learning leads to generally capable agents.
\newblock \emph{arXiv preprint arXiv:2107.12808}, 2021.

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell,
  Heess, and Pascanu]{teh2017distral}
Teh, Y.~W., Bapst, V., Czarnecki, W.~M., Quan, J., Kirkpatrick, J., Hadsell,
  R., Heess, N., and Pascanu, R.
\newblock Distral: Robust multitask reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.04175}, 2017.

\bibitem[Urakami et~al.(2019)Urakami, Hodgkinson, Carlin, Leu, Rigazio, and
  Abbeel]{urakami2019doorgym}
Urakami, Y., Hodgkinson, A., Carlin, C., Leu, R., Rigazio, L., and Abbeel, P.
\newblock Doorgym: A scalable door opening environment and baseline agent.
\newblock \emph{arXiv preprint arXiv:1908.01887}, 2019.

\bibitem[Wijmans et~al.(2019)Wijmans, Kadian, Morcos, Lee, Essa, Parikh, Savva,
  and Batra]{wijmans2019dd}
Wijmans, E., Kadian, A., Morcos, A., Lee, S., Essa, I., Parikh, D., Savva, M.,
  and Batra, D.
\newblock Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion
  frames.
\newblock \emph{arXiv preprint arXiv:1911.00357}, 2019.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{yu2021reinforcement}
Yu, C., Liu, J., Nemati, S., and Yin, G.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.

\bibitem[Yu et~al.(2020{\natexlab{a}})Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{arXiv preprint arXiv:2001.06782}, 2020{\natexlab{a}}.

\bibitem[Yu et~al.(2020{\natexlab{b}})Yu, Quillen, He, Julian, Hausman, Finn,
  and Levine]{yu2020meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1094--1100. PMLR,
  2020{\natexlab{b}}.

\bibitem[Zakka et~al.(2022)Zakka, Zeng, Florence, Tompson, Bohg, and
  Dwibedi]{irl}
Zakka, K., Zeng, A., Florence, P., Tompson, J., Bohg, J., and Dwibedi, D.
\newblock Xirl: Cross-embodiment inverse reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  537--546. PMLR, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Lyle, Sodhani, Filos, Kwiatkowska, Pineau,
  Gal, and Precup]{zhang2020invariant}
Zhang, A., Lyle, C., Sodhani, S., Filos, A., Kwiatkowska, M., Pineau, J., Gal,
  Y., and Precup, D.
\newblock Invariant causal prediction for block mdps.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11214--11224. PMLR, 2020.

\bibitem[Zhao et~al.(2021)Zhao, Lin, Jia, Gao, Thattai, Thomason, and
  Sukhatme]{zhao2021luminous}
Zhao, Y., Lin, K., Jia, Z., Gao, Q., Thattai, G., Thomason, J., and Sukhatme,
  G.~S.
\newblock Luminous: Indoor scene generation for embodied ai challenges.
\newblock \emph{arXiv preprint arXiv:2111.05527}, 2021.

\end{thebibliography}
