\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson(1965)]{anderson1965iterative}
Donald~G Anderson.
\newblock Iterative procedures for nonlinear integral equations.
\newblock \emph{Journal of the ACM (JACM)}, 12\penalty0 (4):\penalty0 547--560,
  1965.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deep}
Shaojie Bai, J.~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Bai et~al.(2020)Bai, Koltun, and Kolter]{bai2020multiscale}
Shaojie Bai, Vladlen Koltun, and J.~Zico Kolter.
\newblock Multiscale deep equilibrium models.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Bai et~al.(2021)Bai, Koltun, and Kolter]{Bai2021StabilizingEM}
Shaojie Bai, Vladlen Koltun, and J.~Zico Kolter.
\newblock Stabilizing equilibrium models by jacobian regularization.
\newblock In \emph{ICML}, 2021.

\bibitem[Banino et~al.(2021)Banino, Balaguer, and
  Blundell]{banino2021pondernet}
Andrea Banino, Jan Balaguer, and Charles Blundell.
\newblock Pondernet: Learning to ponder.
\newblock \emph{arXiv preprint arXiv:2107.05407}, 2021.

\bibitem[Bansal et~al.(2022)Bansal, Schwarzschild, Borgnia, Emam, Huang,
  Goldblum, and Goldstein]{bansal2022end}
Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah
  Goldblum, and Tom Goldstein.
\newblock End-to-end algorithm synthesis with recurrent networks: Logical
  extrapolation without overthinking.
\newblock \emph{arXiv preprint arXiv:2202.05826}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Broyden(1965)]{broyden1965class}
Charles~G Broyden.
\newblock A class of methods for solving nonlinear simultaneous equations.
\newblock \emph{Mathematics of Computation}, 1965.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2019}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock 2019.
\newblock URL \url{https://openreview.net/pdf?id=HyzdRiR9Y7}.

\bibitem[Du et~al.(2022)Du, Li, Tenenbaum, and Mordatch]{du2022learning}
Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch.
\newblock Learning iterative reasoning through energy minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  5570--5582. PMLR, 2022.

\bibitem[Elbayad et~al.(2020{\natexlab{a}})Elbayad, Gu, Grave, and
  Auli]{Elbayad2020Depth-Adaptive}
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
\newblock Depth-adaptive transformer.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=SJg7KhVKPH}.

\bibitem[Elbayad et~al.(2020{\natexlab{b}})Elbayad, Gu, Grave, and
  Auli]{Elbayad2020DepthAdaptiveT}
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
\newblock Depth-adaptive transformer.
\newblock \emph{ArXiv}, abs/1910.10073, 2020{\natexlab{b}}.

\bibitem[Eyzaguirre and Soto(2020)]{eyzaguirre2020differentiable}
Cristobal Eyzaguirre and Alvaro Soto.
\newblock Differentiable adaptive computation time for visual reasoning.
\newblock In \emph{Proceedings of the ieee/cvf conference on computer vision
  and pattern recognition}, pages 12817--12825, 2020.

\bibitem[Figurnov et~al.(2017)Figurnov, Collins, Zhu, Zhang, Huang, Vetrov, and
  Salakhutdinov]{figurnov2017spatially}
Michael Figurnov, Maxwell~D Collins, Yukun Zhu, Li~Zhang, Jonathan Huang,
  Dmitry Vetrov, and Ruslan Salakhutdinov.
\newblock Spatially adaptive computation time for residual networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1039--1048, 2017.

\bibitem[Fung et~al.(2021)Fung, Heaton, Li, McKenzie, Osher, and
  Yin]{fung2021fixed}
Samy~Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and
  Wotao Yin.
\newblock Fixed point networks: Implicit depth models with jacobian-free
  backprop.
\newblock \emph{arXiv e-prints}, pages arXiv--2103, 2021.

\bibitem[Geng et~al.(2021{\natexlab{a}})Geng, Guo, Chen, Li, Wei, and
  Lin]{geng2021attention}
Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke~Wei, and Zhouchen Lin.
\newblock Is attention better than matrix decomposition?
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021{\natexlab{a}}.

\bibitem[Geng et~al.(2021{\natexlab{b}})Geng, Zhang, Bai, Wang, and
  Lin]{geng2021training}
Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin.
\newblock On training implicit models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Graves(2016{\natexlab{a}})]{DBLP:journals/corr/Graves16}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{CoRR}, abs/1603.08983, 2016{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1603.08983}.

\bibitem[Graves(2016{\natexlab{b}})]{graves2016adaptive}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1603.08983}, 2016{\natexlab{b}}.

\bibitem[Gu et~al.(2020)Gu, Chang, Zhu, Sojoudi, and El~Ghaoui]{gu2020implicit}
Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El~Ghaoui.
\newblock Implicit graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11984--11995, 2020.

\bibitem[Hu et~al.(2019)Hu, Liu, Gomes, Zitnik, Liang, Pande, and
  Leskovec]{hu2019strategies}
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
  and Jure Leskovec.
\newblock Strategies for pre-training graph neural networks.
\newblock \emph{arXiv preprint arXiv:1905.12265}, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Kolter et~al.(2020)Kolter, Duvenaud, and Johnson]{kolter2020tutorial}
Zico Kolter, David Duvenaud, and Matthew Johnson.
\newblock Tutorial: Deep implicit layers-neural odes, deep equilibirum models,
  and beyond, 2020.

\bibitem[Kouris et~al.(2021)Kouris, Venieris, Laskaridis, and
  Lane]{kouris2021multi}
Alexandros Kouris, Stylianos~I Venieris, Stefanos Laskaridis, and Nicholas~D
  Lane.
\newblock Multi-exit semantic segmentation networks.
\newblock \emph{arXiv preprint arXiv:2106.03527}, 2021.

\bibitem[Laskaridis et~al.(2021)Laskaridis, Kouris, and
  Lane]{laskaridis2021adaptive}
Stefanos Laskaridis, Alexandros Kouris, and Nicholas~D Lane.
\newblock Adaptive inference through early-exit networks: Design, challenges
  and directions.
\newblock In \emph{Proceedings of the 5th International Workshop on Embedded
  and Mobile Deep Learning}, pages 1--6, 2021.

\bibitem[Li et~al.(2017)Li, Liu, Luo, Change~Loy, and Tang]{li2017not}
Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change~Loy, and Xiaoou Tang.
\newblock Not all pixels are equal: Difficulty-aware semantic segmentation via
  deep layer cascade.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3193--3202, 2017.

\bibitem[Liang et~al.()Liang, Anil, Wu, and Grosse]{liangout}
Kaiqu Liang, Cem Anil, Yuhuai Wu, and Roger Grosse.
\newblock Out-of-distribution generalization with deep equilibrium models.
\newblock \emph{Uncertainty and Robustness in Deep Learning, ICML 2021}.

\bibitem[Liao et~al.(2018)Liao, Xiong, Fetaya, Zhang, Yoon, Pitkow, Urtasun,
  and Zemel]{liao2018reviving}
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow,
  Raquel Urtasun, and Richard Zemel.
\newblock Reviving and improving recurrent back-propagation.
\newblock In \emph{International Conference on Machine Learning}, pages
  3082--3091. PMLR, 2018.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1):\penalty0 503--528,
  1989.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Kawaguchi, Hooi, Wang, and
  Xiao]{Liu2021EIGNNEI}
Juncheng Liu, Kenji Kawaguchi, Bryan Hooi, Yiwei Wang, and X.~Xiao.
\newblock Eignn: Efficient infinite-depth graph neural networks.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2020)Liu, Zhou, Zhao, Wang, Deng, and Ju]{liu2020fastbert}
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi~Ju.
\newblock Fastbert: a self-distilling bert with adaptive inference time.
\newblock \emph{arXiv preprint arXiv:2004.02178}, 2020.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Meng, Zhou, Chen, and
  Xu]{liu2021faster}
Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, and Jinan Xu.
\newblock Faster depth-adaptive transformers.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 13424--13432, 2021{\natexlab{b}}.

\bibitem[McClelland and Rumelhart(1989)]{mcclelland1989explorations}
James~L McClelland and David~E Rumelhart.
\newblock \emph{Explorations in parallel distributed processing: A handbook of
  models, programs, and exercises}.
\newblock MIT press, 1989.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7721--7735. PMLR, 2021.

\bibitem[Neumann et~al.(2016)Neumann, Stenetorp, and
  Riedel]{neumann2016learning}
Mark Neumann, Pontus Stenetorp, and Sebastian Riedel.
\newblock Learning to reason with adaptive computation.
\newblock \emph{arXiv preprint arXiv:1610.07647}, 2016.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Revay et~al.(2020)Revay, Wang, and Manchester]{revay2020lipschitz}
Max Revay, Ruigang Wang, and Ian~R Manchester.
\newblock Lipschitz bounded equilibrium networks.
\newblock \emph{arXiv:2010.01732}, 2020.

\bibitem[Salimans and Kingma(2016)]{DBLP:journals/corr/SalimansK16}
Tim Salimans and Diederik~P. Kingma.
\newblock Weight normalization: {A} simple reparameterization to accelerate
  training of deep neural networks.
\newblock \emph{CoRR}, abs/1602.07868, 2016.
\newblock URL \url{http://arxiv.org/abs/1602.07868}.

\bibitem[Schmidhuber(2012)]{schmidhuber2012self}
J{\"u}rgen Schmidhuber.
\newblock Self-delimiting neural networks.
\newblock \emph{arXiv preprint arXiv:1210.0118}, 2012.

\bibitem[Schwartz et~al.(2020)Schwartz, Stanovsky, Swayamdipta, Dodge, and
  Smith]{schwartz2020right}
Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah~A
  Smith.
\newblock The right tool for the job: Matching model and instance complexities.
\newblock \emph{arXiv preprint arXiv:2004.07453}, 2020.

\bibitem[Schwarzschild et~al.(2021{\natexlab{a}})Schwarzschild, Borgnia, Gupta,
  Bansal, Emam, Huang, Goldblum, and Goldstein]{schwarzschild2021datasets}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Arpit Bansal, Zeyad Emam, Furong
  Huang, Micah Goldblum, and Tom Goldstein.
\newblock Datasets for studying generalization from easy to hard examples.
\newblock \emph{arXiv preprint arXiv:2108.06011}, 2021{\natexlab{a}}.

\bibitem[Schwarzschild et~al.(2021{\natexlab{b}})Schwarzschild, Borgnia, Gupta,
  Huang, Vishkin, Goldblum, and Goldstein]{schwarzschild2021can}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah
  Goldblum, and Tom Goldstein.
\newblock Can you learn an algorithm? generalizing from easy to hard problems
  with recurrent networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Selsam et~al.(2018)Selsam, Lamm, B{\"u}nz, Liang, de~Moura, and
  Dill]{selsam2018learning}
Daniel Selsam, Matthew Lamm, Benedikt B{\"u}nz, Percy Liang, Leonardo de~Moura,
  and David~L Dill.
\newblock Learning a sat solver from single-bit supervision.
\newblock \emph{arXiv preprint arXiv:1802.03685}, 2018.

\bibitem[Slotine et~al.(1991)Slotine, Li, et~al.]{slotine1991applied}
Jean-Jacques~E Slotine, Weiping Li, et~al.
\newblock \emph{Applied nonlinear control}, volume 199.
\newblock Prentice hall Englewood Cliffs, NJ, 1991.

\bibitem[Soldaini and Moschitti(2020)]{DBLP:journals/corr/abs-2005-02534}
Luca Soldaini and Alessandro Moschitti.
\newblock The cascade transformer: an application for efficient answer sentence
  selection.
\newblock \emph{CoRR}, abs/2005.02534, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.02534}.

\bibitem[Sriperumbudur and Lanckriet(2009)]{sriperumbudur2009convergence}
Bharath~K Sriperumbudur and Gert~RG Lanckriet.
\newblock On the convergence of the concave-convex procedure.
\newblock In \emph{Nips}, volume~9, pages 1759--1767. Citeseer, 2009.

\bibitem[Teerapittayanon et~al.(2016)Teerapittayanon, McDanel, and
  Kung]{teerapittayanon2016branchynet}
Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung.
\newblock Branchynet: Fast inference via early exiting from deep neural
  networks.
\newblock In \emph{2016 23rd International Conference on Pattern Recognition
  (ICPR)}, pages 2464--2469. IEEE, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2003)Wang, Lee, and Lin]{wang2003global}
Qing-Guo Wang, Tong~Heng Lee, and Chong Lin.
\newblock Global stability of limit cycles.
\newblock In \emph{Relay Feedback}, pages 57--83. Springer, 2003.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Sun]{wang2020implicit}
Tiancai Wang, Xiangyu Zhang, and Jian Sun.
\newblock Implicit feature pyramid network for object detection.
\newblock \emph{arXiv preprint arXiv:2012.13563}, 2020.

\bibitem[Wang et~al.(2018)Wang, Yu, Dou, Darrell, and
  Gonzalez]{wang2018skipnet}
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph~E Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 409--424, 2018.

\bibitem[Winston and Kolter(2020)]{winston2020monotone}
Ezra Winston and J~Zico Kolter.
\newblock Monotone operator equilibrium networks.
\newblock \emph{arXiv preprint arXiv:2006.08591}, 2020.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Xing et~al.(2020)Xing, Xu, Li, and Guan]{xing2020early}
Qunliang Xing, Mai Xu, Tianyi Li, and Zhenyu Guan.
\newblock Early exit or not: Resource-efficient blind quality enhancement for
  compressed images.
\newblock In \emph{European Conference on Computer Vision}, pages 275--292.
  Springer, 2020.

\bibitem[Zhou et~al.(2020)Zhou, Xu, Ge, McAuley, Xu, and Wei]{zhou2020bert}
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke~Xu, and Furu Wei.
\newblock Bert loses patience: Fast and robust inference with early exit.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18330--18341, 2020.

\end{thebibliography}
