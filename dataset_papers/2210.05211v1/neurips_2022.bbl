\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2018)Agrawal, Batra, Parikh, and Kembhavi]{VQA-CP}
A.~Agrawal, D.~Batra, D.~Parikh, and A.~Kembhavi.
\newblock Don't just assume; look and answer: Overcoming priors for visual
  question answering.
\newblock In \emph{{CVPR}}, pages 4971--4980. Computer Vision Foundation /
  {IEEE} Computer Society, 2018.

\bibitem[Beery et~al.(2018)Beery, Horn, and Perona]{BeeryHP18}
S.~Beery, G.~V. Horn, and P.~Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{{ECCV} {(16)}}, volume 11220 of \emph{Lecture Notes in
  Computer Science}, pages 472--489. Springer, 2018.

\bibitem[Bengio et~al.(2013)Bengio, L{\'{e}}onard, and
  Courville]{GradEstimator}
Y.~Bengio, N.~L{\'{e}}onard, and A.~C. Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{CoRR}, abs/1308.3432, 2013.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin]{BERT-LT}
T.~Chen, J.~Frankle, S.~Chang, S.~Liu, Y.~Zhang, Z.~Wang, and M.~Carbin.
\newblock The lottery ticket hypothesis for pre-trained {BERT} networks.
\newblock In \emph{NeurIPS}, pages 15834--15846, 2020.

\bibitem[Clark et~al.(2019)Clark, Yatskar, and Zettlemoyer]{DontTakeEasyWay}
C.~Clark, M.~Yatskar, and L.~Zettlemoyer.
\newblock Don{'}t take the easy way out: Ensemble based methods for avoiding
  known dataset biases.
\newblock In \emph{EMNLP/IJCNLP}, pages 4069--4082. Association for
  Computational Linguistics, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{BERT}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{{NAACL-HLT}}, pages 4171--4186, 2019.

\bibitem[Dinan et~al.(2020)Dinan, Fan, Williams, Urbanek, Kiela, and
  Weston]{QueensArePowerful}
E.~Dinan, A.~Fan, A.~Williams, J.~Urbanek, D.~Kiela, and J.~Weston.
\newblock Queens are powerful too: Mitigating gender bias in dialogue
  generation.
\newblock In \emph{{EMNLP} {(1)}}, pages 8173--8188. Association for
  Computational Linguistics, 2020.

\bibitem[Du et~al.(2021)Du, Mukherjee, Cheng, Shokouhi, Hu, and
  Awadallah]{WhatDoCompressLMForget}
M.~Du, S.~Mukherjee, Y.~Cheng, M.~Shokouhi, X.~Hu, and A.~H. Awadallah.
\newblock What do compressed large language models forget? robustness
  challenges in model compression.
\newblock \emph{CoRR}, abs/2110.08419, 2021.

\bibitem[Frankle and Carbin(2019)]{LTH}
J.~Frankle and M.~Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{{ICLR}}. OpenReview.net, 2019.

\bibitem[Fu et~al.(2021)Fu, Yu, Zhang, Wu, Ouyang, Cox, and
  Lin]{RobustScratchTickets}
Y.~Fu, Q.~Yu, Y.~Zhang, S.~Wu, X.~Ouyang, D.~D. Cox, and Y.~Lin.
\newblock Drawing robust scratch tickets: Subnetworks with inborn robustness
  are found within randomly initialized networks.
\newblock In \emph{NeurIPS}, pages 13059--13072, 2021.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{StateofSparsity}
T.~Gale, E.~Elsen, and S.~Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{CoRR}, abs/1902.09574, 2019.

\bibitem[Ganesh et~al.(2021)Ganesh, Chen, Lou, Khan, Yang, Sajjad, Nakov, Chen,
  and Winslett]{ganesh-etal-2021-compressing}
P.~Ganesh, Y.~Chen, X.~Lou, M.~A. Khan, Y.~Yang, H.~Sajjad, P.~Nakov, D.~Chen,
  and M.~Winslett.
\newblock Compressing large-scale transformer-based models: A case study on
  {BERT}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 1061--1080, 2021.

\bibitem[Ghaddar et~al.(2021)Ghaddar, Langlais, Rezagholizadeh, and
  Rashid]{EndToEndSelfDebias}
A.~Ghaddar, P.~Langlais, M.~Rezagholizadeh, and A.~Rashid.
\newblock End-to-end self-debiasing framework for robust {NLU} training.
\newblock In \emph{{ACL/IJCNLP} (Findings)}, volume {ACL/IJCNLP} 2021 of
  \emph{Findings of {ACL}}, pages 1923--1929. Association for Computational
  Linguistics, 2021.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{AdversarialAttack}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{{ICLR} (Poster)}, 2015.

\bibitem[Gordon et~al.(2020)Gordon, Duh, and Andrews]{CompressingBERT}
M.~A. Gordon, K.~Duh, and N.~Andrews.
\newblock Compressing {BERT:} studying the effects of weight pruning on
  transfer learning.
\newblock In \emph{RepL4NLP@ACL}, pages 143--155, 2020.

\bibitem[Gui et~al.(2019)Gui, Wang, Yang, Yu, Wang, and Liu]{ADMM1}
S.~Gui, H.~Wang, H.~Yang, C.~Yu, Z.~Wang, and J.~Liu.
\newblock Model compression with adversarial robustness: {A} unified
  optimization framework.
\newblock In \emph{NeurIPS}, pages 1283--1294, 2019.

\bibitem[Guo et~al.(2022)Guo, Yang, and Abbasi]{AutoDebias}
Y.~Guo, Y.~Yang, and A.~Abbasi.
\newblock Auto-debias: Debiasing masked language models with automated biased
  prompts.
\newblock In \emph{ACL}, pages 1012--1023. Association for Computational
  Linguistics, 2022.

\bibitem[Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz,
  Bowman, and Smith]{MNLI-hard}
S.~Gururangan, S.~Swayamdipta, O.~Levy, R.~Schwartz, S.~R. Bowman, and N.~A.
  Smith.
\newblock Annotation artifacts in natural language inference data.
\newblock In \emph{{NAACL-HLT}}, pages 107--112. Association for Computational
  Linguistics, 2018.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{HanNips}
S.~Han, J.~Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems 28}, pages
  1135--1143. Curran Associates, Inc., 2015.

\bibitem[He et~al.(2019)He, Zha, and Wang]{UnlearnDatasetBias}
H.~He, S.~Zha, and H.~Wang.
\newblock Unlearn dataset bias in natural language inference by fitting the
  residual.
\newblock In \emph{Proceedings of the 2nd Workshop on Deep Learning Approaches
  for Low-Resource NLP (DeepLo 2019)}, pages 132--142. Association for
  Computational Linguistics, 2019.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Liu, Wallace, Dziedzic, Krishnan, and
  Song]{HendrycksLWDKS20}
D.~Hendrycks, X.~Liu, E.~Wallace, A.~Dziedzic, R.~Krishnan, and D.~Song.
\newblock Pretrained transformers improve out-of-distribution robustness.
\newblock In \emph{{ACL}}, pages 2744--2751. Association for Computational
  Linguistics, 2020.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{KD}
G.~E. Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{CoRR}, abs/1503.02531, 2015.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El{-}Yaniv, and
  Bengio]{BinaryNet}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El{-}Yaniv, and Y.~Bengio.
\newblock Binarized neural networks.
\newblock In \emph{{NIPS}}, pages 4107--4115, 2016.

\bibitem[Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{TinyBERT}
X.~Jiao, Y.~Yin, L.~Shang, X.~Jiang, X.~Chen, L.~Li, F.~Wang, and Q.~Liu.
\newblock Tinybert: Distilling {BERT} for natural language understanding.
\newblock In \emph{{EMNLP} (Findings)}, pages 4163--4174, 2020.

\bibitem[Karimi~Mahabadi et~al.(2020)Karimi~Mahabadi, Belinkov, and
  Henderson]{EndToEndBias}
R.~Karimi~Mahabadi, Y.~Belinkov, and J.~Henderson.
\newblock End-to-end bias mitigation by modelling biases in corpora.
\newblock In \emph{ACL}, pages 8706--8716. Association for Computational
  Linguistics, 2020.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{ALBERT}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut.
\newblock {ALBERT:} {A} lite {BERT} for self-supervised learning of language
  representations.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\bibitem[Li et~al.(2020)Li, Wallace, Shen, Lin, Keutzer, Klein, and
  Gonzalez]{TrainLargeThenCompress}
Z.~Li, E.~Wallace, S.~Shen, K.~Lin, K.~Keutzer, D.~Klein, and J.~E. Gonzalez.
\newblock Train large, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock \emph{CoRR}, abs/2002.11794, 2020.

\bibitem[Liang et~al.(2021)Liang, Zuo, Chen, Jiang, Liu, He, Zhao, and
  Chen]{SuperTickets}
C.~Liang, S.~Zuo, M.~Chen, H.~Jiang, X.~Liu, P.~He, T.~Zhao, and W.~Chen.
\newblock Super tickets in pre-trained language models: From model compression
  to improving generalization.
\newblock In \emph{{ACL/IJCNLP}}, pages 6524--6538. Association for
  Computational Linguistics, 2021.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{RoBERTa}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{CoRR}, abs/1907.11692, 2019.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Lin, and Yuan]{rosita}
Y.~Liu, Z.~Lin, and F.~Yuan.
\newblock {ROSITA:} refined {BERT} compression with integrated techniques.
\newblock In \emph{{AAAI}}, pages 8715--8722. {AAAI} Press, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Meng, Lin, Wang, and Zhou]{MUD}
Y.~Liu, F.~Meng, Z.~Lin, W.~Wang, and J.~Zhou.
\newblock Marginal utility diminishes: Exploring the minimum knowledge for
  {BERT} knowledge distillation.
\newblock In \emph{{ACL/IJCNLP}}, pages 2928--2941, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Meng, Lin, Fu, Cao, Wang, and Zhou]{TAMT}
Y.~Liu, F.~Meng, Z.~Lin, P.~Fu, Y.~Cao, W.~Wang, and J.~Zhou.
\newblock Learning to win lottery tickets in {BERT} transfer via task-agnostic
  mask training.
\newblock \emph{CoRR}, abs/2204.11218, 2022.

\bibitem[Loshchilov and Hutter(2019)]{AdamW}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{{ICLR} (Poster)}. OpenReview.net, 2019.

\bibitem[Madaan et~al.(2020)Madaan, Shin, and Hwang]{VulnerabilitySuppression}
D.~Madaan, J.~Shin, and S.~J. Hwang.
\newblock Adversarial neural pruning with latent vulnerability suppression.
\newblock In \emph{{ICML}}, volume 119 of \emph{Proceedings of Machine Learning
  Research}, pages 6575--6585. {PMLR}, 2020.

\bibitem[Mallya et~al.(2018)Mallya, Davis, and Lazebnik]{Piggyback}
A.~Mallya, D.~Davis, and S.~Lazebnik.
\newblock Piggyback: Adapting a single network to multiple tasks by learning to
  mask weights.
\newblock In \emph{{ECCV}}, volume 11208 of \emph{Lecture Notes in Computer
  Science}, pages 72--88. Springer, 2018.

\bibitem[Mao et~al.(2020)Mao, Wang, Wu, Zhang, Wang, Zhang, Yang, Tong, and
  Bai]{LadaBERT}
Y.~Mao, Y.~Wang, C.~Wu, C.~Zhang, Y.~Wang, Q.~Zhang, Y.~Yang, Y.~Tong, and
  J.~Bai.
\newblock Ladabert: Lightweight adaptation of {BERT} through hybrid model
  compression.
\newblock In \emph{{COLING}}, pages 3225--3234, 2020.

\bibitem[McCoy et~al.(2019)McCoy, Pavlick, and Linzen]{HANS}
T.~McCoy, E.~Pavlick, and T.~Linzen.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock In \emph{{ACL}}, pages 3428--3448. Association for Computational
  Linguistics, 2019.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{AttnPrun}
P.~Michel, O.~Levy, and G.~Neubig.
\newblock Are sixteen heads really better than one?
\newblock In \emph{NeurIPS}, pages 14014--14024, 2019.

\bibitem[Mikolov et~al.(2018)Mikolov, Grave, Bojanowski, Puhrsch, and
  Joulin]{fasttext}
T.~Mikolov, E.~Grave, P.~Bojanowski, C.~Puhrsch, and A.~Joulin.
\newblock Advances in pre-training distributed word representations.
\newblock In \emph{{LREC}}. European Language Resources Association {(ELRA)},
  2018.

\bibitem[Prasanna et~al.(2020)Prasanna, Rogers, and Rumshisky]{AllTicketWin}
S.~Prasanna, A.~Rogers, and A.~Rumshisky.
\newblock When {BERT} plays the lottery, all tickets are winning.
\newblock In \emph{{EMNLP}}, pages 3208--3229, 2020.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskever]{GPT}
A.~Radford, K.~Narasimhan, T.~Salimans, and I.~Sutskever.
\newblock Improving language understanding with unsupervised learning.
\newblock In \emph{{Technical report, OpenAI}}, 2018.

\bibitem[Radiya{-}Dixit and Wang(2020)]{HowFine}
E.~Radiya{-}Dixit and X.~Wang.
\newblock How fine can fine-tuning be? learning efficient language models.
\newblock In \emph{{AISTATS}}, volume 108 of \emph{Proceedings of Machine
  Learning Research}, pages 2435--2443. {PMLR}, 2020.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{T5}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{DistillBERT}
V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf.
\newblock Distilbert, a distilled version of {BERT:} smaller, faster, cheaper
  and lighter.
\newblock \emph{CoRR}, abs/1910.01108, 2019.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{MovementPruning}
V.~Sanh, T.~Wolf, and A.~M. Rush.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock In \emph{NeurIPS}, pages 20378--20389, 2020.

\bibitem[Schuster et~al.(2019)Schuster, Shah, Yeo, Filizzola, Santus, and
  Barzilay]{FeverSym}
T.~Schuster, D.~J. Shah, Y.~J.~S. Yeo, D.~Filizzola, E.~Santus, and
  R.~Barzilay.
\newblock Towards debiasing fact verification models.
\newblock In \emph{{EMNLP/IJCNLP}}, pages 3417--3423. Association for
  Computational Linguistics, 2019.

\bibitem[Sehwag et~al.(2019)Sehwag, Wang, Mittal, and
  Jana]{TowardsCompactRobustDnn}
V.~Sehwag, S.~Wang, P.~Mittal, and S.~Jana.
\newblock Towards compact and robust deep neural networks.
\newblock \emph{CoRR}, abs/1906.06110, 2019.

\bibitem[Sehwag et~al.(2020)Sehwag, Wang, Mittal, and Jana]{HYDRA}
V.~Sehwag, S.~Wang, P.~Mittal, and S.~Jana.
\newblock {HYDRA:} pruning adversarially robust neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{EnergyPolicyConsiderationNLP}
E.~Strubell, A.~Ganesh, and A.~McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In \emph{{ACL}}, pages 3645--3650. Association for Computational
  Linguistics, 2019.

\bibitem[Sun et~al.(2019)Sun, Cheng, Gan, and Liu]{PKD}
S.~Sun, Y.~Cheng, Z.~Gan, and J.~Liu.
\newblock Patient knowledge distillation for {BERT} model compression.
\newblock In \emph{{EMNLP/IJCNLP}}, pages 4322--4331, 2019.

\bibitem[Tambe et~al.(2020)Tambe, Hooper, Pentecost, Yang, Donato, Sanh, Rush,
  Brooks, and Wei]{EdgeBERT}
T.~Tambe, C.~Hooper, L.~Pentecost, E.~Yang, M.~Donato, V.~Sanh, A.~M. Rush,
  D.~Brooks, and G.~Wei.
\newblock Edgebert: Optimizing on-chip inference for multi-task {NLP}.
\newblock \emph{CoRR}, abs/2011.14203, 2020.

\bibitem[Thorne et~al.(2018)Thorne, Vlachos, Cocarascu, Christodoulopoulos, and
  Mittal]{FEVER}
J.~Thorne, A.~Vlachos, O.~Cocarascu, C.~Christodoulopoulos, and A.~Mittal.
\newblock The fact extraction and verification {(FEVER)} shared task.
\newblock \emph{CoRR}, abs/1811.10971, 2018.

\bibitem[Tu et~al.(2020)Tu, Lalwani, Gella, and He]{EmpiricalStudyRobustness}
L.~Tu, G.~Lalwani, S.~Gella, and H.~He.
\newblock An empirical study on robustness to spurious correlations using
  pre-trained language models.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 8:\penalty0 621--633, 2020.

\bibitem[Utama et~al.(2020{\natexlab{a}})Utama, Moosavi, and
  Gurevych]{MindTradeOff}
P.~A. Utama, N.~S. Moosavi, and I.~Gurevych.
\newblock Mind the trade-off: Debiasing {NLU} models without degrading the
  in-distribution performance.
\newblock In \emph{{ACL}}, pages 8717--8729. Association for Computational
  Linguistics, 2020{\natexlab{a}}.

\bibitem[Utama et~al.(2020{\natexlab{b}})Utama, Moosavi, and
  Gurevych]{UnknownBias}
P.~A. Utama, N.~S. Moosavi, and I.~Gurevych.
\newblock Towards debiasing {NLU} models from unknown biases.
\newblock In \emph{{EMNLP}}, pages 7597--7610. Association for Computational
  Linguistics, 2020{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{{NIPS}}, pages 5998--6008, 2017.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{MNLI}
A.~Williams, N.~Nangia, and S.~R. Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{{NAACL-HLT}}, pages 1112--1122. Association for
  Computational Linguistics, 2018.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen,
  C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~L. Scao, S.~Gugger, M.~Drame, Q.~Lhoest,
  and A.~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  Oct. 2020.

\bibitem[Xu et~al.(2021)Xu, Zhou, Ge, Xu, McAuley, and
  Wei]{LoyaltyRobustnessOfBERT}
C.~Xu, W.~Zhou, T.~Ge, K.~Xu, J.~J. McAuley, and F.~Wei.
\newblock Beyond preserved accuracy: Evaluating loyalty and robustness of
  {BERT} compression.
\newblock In \emph{{EMNLP} {(1)}}, pages 10653--10659. Association for
  Computational Linguistics, 2021.

\bibitem[Ye et~al.(2019)Ye, Lin, Xu, Liu, Cheng, Lambrechts, Zhang, Zhou, Ma,
  and Wang]{ADMM2}
S.~Ye, X.~Lin, K.~Xu, S.~Liu, H.~Cheng, J.~Lambrechts, H.~Zhang, A.~Zhou,
  K.~Ma, and Y.~Wang.
\newblock Adversarial robustness vs. model compression, or both?
\newblock In \emph{{ICCV}}, pages 111--120. {IEEE}, 2019.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and Wasserblat]{Q8BERT}
O.~Zafrir, G.~Boudoukh, P.~Izsak, and M.~Wasserblat.
\newblock {Q8BERT:} quantized 8bit {BERT}.
\newblock In \emph{EMC2@NeurIPS}, pages 36--39. {IEEE}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Ahuja, Xu, Wang, and Courville]{MRM}
D.~Zhang, K.~Ahuja, Y.~Xu, Y.~Wang, and A.~C. Courville.
\newblock Can subnetwork structure be the key to out-of-distribution
  generalization?
\newblock In \emph{{ICML}}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pages 12356--12367. {PMLR}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Ye, Zhang, Tang, Wen, Fardad, and Wang]{ADMM}
T.~Zhang, S.~Ye, K.~Zhang, J.~Tang, W.~Wen, M.~Fardad, and Y.~Wang.
\newblock A systematic {DNN} weight pruning framework using alternating
  direction method of multipliers.
\newblock In \emph{{ECCV} {(8)}}, volume 11212 of \emph{Lecture Notes in
  Computer Science}, pages 191--207. Springer, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Hou, Yin, Shang, Chen, Jiang, and
  Liu]{TernaryBERT}
W.~Zhang, L.~Hou, Y.~Yin, L.~Shang, X.~Chen, X.~Jiang, and Q.~Liu.
\newblock Ternarybert: Distillation-aware ultra-low bit {BERT}.
\newblock In \emph{{EMNLP}}, pages 509--521. Association for Computational
  Linguistics, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Baldridge, and He]{PAWS}
Y.~Zhang, J.~Baldridge, and L.~He.
\newblock {PAWS:} paraphrase adversaries from word scrambling.
\newblock In \emph{{NAACL-HLT}}, pages 1298--1308. Association for
  Computational Linguistics, 2019.

\bibitem[Zhao et~al.(2020)Zhao, Lin, Mi, Jaggi, and Sch{\"{u}}tze]{Masking}
M.~Zhao, T.~Lin, F.~Mi, M.~Jaggi, and H.~Sch{\"{u}}tze.
\newblock Masking as an efficient alternative to finetuning for pretrained
  language models.
\newblock In \emph{{EMNLP}}, pages 2226--2241, 2020.

\bibitem[Zhu and Gupta(2018)]{ToPruneOrNotToPrune}
M.~Zhu and S.~Gupta.
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression.
\newblock In \emph{{ICLR} (Workshop)}. OpenReview.net, 2018.

\end{thebibliography}
