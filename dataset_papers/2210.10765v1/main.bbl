\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In \emph{International conference on machine learning}, pages 22--31.
  PMLR, 2017.

\bibitem[Akrour et~al.(2011)Akrour, Schoenauer, and
  Sebag]{akrour2011preference}
Riad Akrour, Marc Schoenauer, and Michele Sebag.
\newblock Preference-based policy learning.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 12--27. Springer, 2011.

\bibitem[Alshiekh et~al.(2018)Alshiekh, Bloem, Ehlers, K{\"o}nighofer, Niekum,
  and Topcu]{alshiekh2018safe}
Mohammed Alshiekh, Roderick Bloem, R{\"u}diger Ehlers, Bettina K{\"o}nighofer,
  Scott Niekum, and Ufuk Topcu.
\newblock Safe reinforcement learning via shielding.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Arzate~Cruz and Igarashi(2020)]{arzate2020survey}
Christian Arzate~Cruz and Takeo Igarashi.
\newblock A survey on interactive reinforcement learning: design principles and
  open challenges.
\newblock In \emph{Proceedings of the 2020 ACM designing interactive systems
  conference}, pages 1195--1209, 2020.

\bibitem[Badia et~al.(2020)Badia, Sprechmann, Vitvitskyi, Guo, Piot,
  Kapturowski, Tieleman, Arjovsky, Pritzel, Bolt, et~al.]{badia2020never}
Adri{\`a}~Puigdom{\`e}nech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel
  Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Mart{\'\i}n Arjovsky,
  Alexander Pritzel, Andew Bolt, et~al.
\newblock Never give up: Learning directed exploration strategies.
\newblock \emph{arXiv preprint arXiv:2002.06038}, 2020.

\bibitem[Bastani et~al.(2021)Bastani, Li, and Xu]{bastani2021safe}
Osbert Bastani, Shuo Li, and Anton Xu.
\newblock Safe reinforcement learning via statistical model predictive
  shielding.
\newblock In \emph{Robotics: Science and Systems}, 2021.

\bibitem[Bharadhwaj et~al.(2020)Bharadhwaj, Kumar, Rhinehart, Levine, Shkurti,
  and Garg]{bharadhwaj2020conservative}
Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian
  Shkurti, and Animesh Garg.
\newblock Conservative safety critics for exploration.
\newblock \emph{arXiv preprint arXiv:2010.14497}, 2020.

\bibitem[Biyik and Sadigh(2018)]{biyik2018batch}
Erdem Biyik and Dorsa Sadigh.
\newblock Batch active preference-based learning of reward functions.
\newblock In \emph{Conference on robot learning}, pages 519--528. PMLR, 2018.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chow et~al.(2017)Chow, Ghavamzadeh, Janson, and Pavone]{chow2017risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6070--6120, 2017.

\bibitem[Eysenbach et~al.(2017)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach2017leave}
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.06782}, 2017.

\bibitem[Faulkner et~al.(2020)Faulkner, Short, and
  Thomaz]{faulkner2020interactive}
Taylor A~Kessler Faulkner, Elaine~Schaertl Short, and Andrea~L Thomaz.
\newblock Interactive reinforcement learning with inaccurate feedback.
\newblock In \emph{2020 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 7498--7504. IEEE, 2020.

\bibitem[Finn et~al.(2016)Finn, Tan, Duan, Darrell, Levine, and
  Abbeel]{finn2016deep}
Chelsea Finn, Xin~Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter
  Abbeel.
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 512--519. IEEE, 2016.

\bibitem[Ghadirzadeh et~al.(2017)Ghadirzadeh, Maki, Kragic, and
  Bj{\"o}rkman]{ghadirzadeh2017deep}
Ali Ghadirzadeh, Atsuto Maki, Danica Kragic, and M{\aa}rten Bj{\"o}rkman.
\newblock Deep predictive policy training using reinforcement learning.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 2351--2358. IEEE, 2017.

\bibitem[Grinsztajn et~al.(2021)Grinsztajn, Ferret, Pietquin, Geist,
  et~al.]{grinsztajn2021there}
Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Matthieu Geist, et~al.
\newblock There is no turning back: A self-supervised approach for
  reversibility-aware reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pages 3389--3396. IEEE, 2017.

\bibitem[Gupta et~al.(2021)Gupta, Yu, Zhao, Kumar, Rovinsky, Xu, Devlin, and
  Levine]{Gupta2021ResetFreeRL}
Abhishek Gupta, Justin Yu, Tony Zhao, Vikash Kumar, Aaron Rovinsky, Kelvin Xu,
  Thomas Devlin, and Sergey Levine.
\newblock Reset-free reinforcement learning via multi-task learning: Learning
  dexterous manipulation behaviors without human intervention.
\newblock \emph{ArXiv}, abs/2104.11203, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Han et~al.(2015)Han, Levine, and Abbeel]{han2015learning}
Weiqiao Han, Sergey Levine, and Pieter Abbeel.
\newblock Learning compound multi-step controllers under unknown dynamics.
\newblock In \emph{2015 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 6435--6442. IEEE, 2015.

\bibitem[Hoque et~al.(2021{\natexlab{a}})Hoque, Balakrishna, Novoseller,
  Wilcox, Brown, and Goldberg]{hoque2021thriftydagger}
Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Albert Wilcox, Daniel~S
  Brown, and Ken Goldberg.
\newblock Thriftydagger: Budget-aware novelty and risk gating for interactive
  imitation learning.
\newblock \emph{arXiv preprint arXiv:2109.08273}, 2021{\natexlab{a}}.

\bibitem[Hoque et~al.(2021{\natexlab{b}})Hoque, Balakrishna, Putterman, Luo,
  Brown, Seita, Thananjeyan, Novoseller, and Goldberg]{hoque2021lazydagger}
Ryan Hoque, Ashwin Balakrishna, Carl Putterman, Michael Luo, Daniel~S Brown,
  Daniel Seita, Brijen Thananjeyan, Ellen Novoseller, and Ken Goldberg.
\newblock Lazydagger: Reducing context switching in interactive imitation
  learning.
\newblock In \emph{2021 IEEE 17th International Conference on Automation
  Science and Engineering (CASE)}, pages 502--509. IEEE, 2021{\natexlab{b}}.

\bibitem[Knox and Stone(2009)]{knox2009interactively}
W~Bradley Knox and Peter Stone.
\newblock Interactively shaping agents via human reinforcement: The tamer
  framework.
\newblock In \emph{Proceedings of the fifth international conference on
  Knowledge capture}, pages 9--16, 2009.

\bibitem[Krakovna et~al.(2018)Krakovna, Orseau, Kumar, Martic, and
  Legg]{krakovna2018penalizing}
Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, and Shane Legg.
\newblock Penalizing side effects using stepwise relative reachability.
\newblock \emph{arXiv preprint arXiv:1806.01186}, 2018.

\bibitem[Kruusmaa et~al.(2007)Kruusmaa, Gavshin, and
  Eppendahl]{kruusmaa2007don}
Maarja Kruusmaa, Yuri Gavshin, and Adam Eppendahl.
\newblock Don't do things you can't undo: reversibility models for generating
  safe behaviours.
\newblock In \emph{Proceedings 2007 IEEE International Conference on Robotics
  and Automation}, pages 1134--1139. IEEE, 2007.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{lee2021pebble}
Kimin Lee, Laura Smith, and Pieter Abbeel.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock \emph{arXiv preprint arXiv:2106.05091}, 2021.

\bibitem[MacGlashan et~al.(2017)MacGlashan, Ho, Loftin, Peng, Wang, Roberts,
  Taylor, and Littman]{macglashan2017interactive}
James MacGlashan, Mark~K Ho, Robert Loftin, Bei Peng, Guan Wang, David~L
  Roberts, Matthew~E Taylor, and Michael~L Littman.
\newblock Interactive learning from policy-dependent human feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  2285--2294. PMLR, 2017.

\bibitem[Menda et~al.(2019)Menda, Driggs-Campbell, and
  Kochenderfer]{menda2019ensembledagger}
Kunal Menda, Katherine Driggs-Campbell, and Mykel~J Kochenderfer.
\newblock Ensembledagger: A bayesian approach to safe imitation learning.
\newblock In \emph{2019 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 5041--5048. IEEE, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Moldovan and Abbeel(2012)]{moldovan2012safe}
Teodor~Mihai Moldovan and Pieter Abbeel.
\newblock Safe exploration in markov decision processes.
\newblock \emph{arXiv preprint arXiv:1205.4810}, 2012.

\bibitem[Rahaman et~al.(2020)Rahaman, Wolf, Goyal, Remme, and
  Bengio]{rahaman2020learning}
Nasim Rahaman, Steffen Wolf, Anirudh Goyal, Roman Remme, and Yoshua Bengio.
\newblock Learning the arrow of time for problems in reinforcement learning.
\newblock 2020.

\bibitem[Sadigh et~al.(2017)Sadigh, Dragan, Sastry, and
  Seshia]{sadigh2017active}
Dorsa Sadigh, Anca~D Dragan, Shankar Sastry, and Sanjit~A Seshia.
\newblock \emph{Active preference-based learning of reward functions}.
\newblock 2017.

\bibitem[Savinov et~al.(2018)Savinov, Raichuk, Marinier, Vincent, Pollefeys,
  Lillicrap, and Gelly]{savinov2018episodic}
Nikolay Savinov, Anton Raichuk, Rapha{\"e}l Marinier, Damien Vincent, Marc
  Pollefeys, Timothy Lillicrap, and Sylvain Gelly.
\newblock Episodic curiosity through reachability.
\newblock \emph{arXiv preprint arXiv:1810.02274}, 2018.

\bibitem[Sharma et~al.(2021{\natexlab{a}})Sharma, Gupta, Levine, Hausman, and
  Finn]{sharma2021autonomouscurr}
Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
\newblock Autonomous reinforcement learning via subgoal curricula.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Sharma et~al.(2021{\natexlab{b}})Sharma, Xu, Sardana, Gupta, Hausman,
  Levine, and Finn]{sharma2021autonomous}
Archit Sharma, Kelvin Xu, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey
  Levine, and Chelsea Finn.
\newblock Autonomous reinforcement learning: Formalism and benchmarking.
\newblock \emph{arXiv preprint arXiv:2112.09605}, 2021{\natexlab{b}}.

\bibitem[Sharma et~al.(2022)Sharma, Ahmad, and Finn]{sharma2022state}
Archit Sharma, Rehaan Ahmad, and Chelsea Finn.
\newblock A state-distribution matching approach to non-episodic reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2205.05212}, 2022.

\bibitem[Srinivasan et~al.(2020)Srinivasan, Eysenbach, Ha, Tan, and
  Finn]{srinivasan2020learning}
Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn.
\newblock Learning to be safe: Deep rl with a safety critic.
\newblock \emph{arXiv preprint arXiv:2010.14603}, 2020.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Meguro, and
  Minami]{sugiyama2012preference}
Hiroaki Sugiyama, Toyomi Meguro, and Yasuhiro Minami.
\newblock Preference-learning based inverse reinforcement learning for dialog
  control.
\newblock In \emph{Thirteenth Annual Conference of the International Speech
  Communication Association}, 2012.

\bibitem[Tessler et~al.(2018)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Chen Tessler, Daniel~J Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:1805.11074}, 2018.

\bibitem[Thananjeyan et~al.(2020)Thananjeyan, Balakrishna, Nair, Luo,
  Srinivasan, Hwang, Gonzalez, Ibarz, Finn, and
  Goldberg]{thananjeyan2020recovery}
Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan
  Srinivasan, Minho Hwang, Joseph~E Gonzalez, Julian Ibarz, Chelsea Finn, and
  Ken Goldberg.
\newblock Recovery rl: Safe reinforcement learning with learned recovery zones.
\newblock \emph{arXiv preprint arXiv:2010.15920}, 2020.

\bibitem[Thomas et~al.(2021)Thomas, Luo, and Ma]{thomas2021safe}
Garrett Thomas, Yuping Luo, and Tengyu Ma.
\newblock Safe reinforcement learning by imagining the near future.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Turchetta et~al.(2020)Turchetta, Kolobov, Shah, Krause, and
  Agarwal]{turchetta2020safe}
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh
  Agarwal.
\newblock Safe reinforcement learning via curriculum induction.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12151--12162, 2020.

\bibitem[Wagener et~al.(2021)Wagener, Boots, and Cheng]{wagener2021safe}
Nolan~C Wagener, Byron Boots, and Ching-An Cheng.
\newblock Safe reinforcement learning using advantage-based intervention.
\newblock In \emph{International Conference on Machine Learning}, pages
  10630--10640. PMLR, 2021.

\bibitem[Wang et~al.(2022)Wang, Lee, Hakhamaneshi, Abbeel, and
  Laskin]{wang2022skill}
Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, and Michael
  Laskin.
\newblock Skill preferences: Learning to extract and execute robotic skills
  from human feedback.
\newblock In \emph{Conference on Robot Learning}, pages 1259--1268. PMLR, 2022.

\bibitem[Wang and Taylor(2018)]{wang2018interactive}
Zhaodong Wang and Matthew~E Taylor.
\newblock Interactive reinforcement learning with dynamic reuse of prior
  knowledge from human/agent's demonstration.
\newblock \emph{arXiv preprint arXiv:1805.04493}, 2018.

\bibitem[Wirth and F{\"u}rnkranz(2013)]{wirth2013preference}
Christian Wirth and Johannes F{\"u}rnkranz.
\newblock Preference-based reinforcement learning: A preliminary survey.
\newblock In \emph{Proceedings of the ECML/PKDD-13 Workshop on Reinforcement
  Learning from Generalized Feedback: Beyond Numeric Rewards}. Citeseer, 2013.

\bibitem[Xu et~al.(2020)Xu, Verma, Finn, and Levine]{Xu2020ContinualLO}
Kelvin Xu, Siddharth Verma, Chelsea Finn, and Sergey Levine.
\newblock Continual learning of control primitives: Skill discovery via
  reset-games.
\newblock \emph{ArXiv}, abs/2011.05286, 2020.

\bibitem[Yarats et~al.(2021{\natexlab{a}})Yarats, Fergus, Lazaric, and
  Pinto]{yarats2021drqv2}
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2107.09645}, 2021{\natexlab{a}}.

\bibitem[Yarats et~al.(2021{\natexlab{b}})Yarats, Kostrikov, and
  Fergus]{yarats2021image}
Denis Yarats, Ilya Kostrikov, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=GY6-6sTvGaf}.

\bibitem[Zanger et~al.(2021)Zanger, Daaboul, and Z{\"o}llner]{zanger2021safe}
Moritz~A Zanger, Karam Daaboul, and J~Marius Z{\"o}llner.
\newblock Safe continuous control with constrained model-based policy
  optimization.
\newblock In \emph{2021 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 3512--3519. IEEE, 2021.

\bibitem[Zhang and Cho(2016)]{zhang2016query}
Jiakai Zhang and Kyunghyun Cho.
\newblock Query-efficient imitation learning for end-to-end autonomous driving.
\newblock \emph{arXiv preprint arXiv:1605.06450}, 2016.

\bibitem[Zhu et~al.(2019)Zhu, Gupta, Rajeswaran, Levine, and
  Kumar]{zhu2019dexterous}
Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar.
\newblock Dexterous manipulation with deep reinforcement learning: Efficient,
  general, and low-cost.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pages 3651--3657. IEEE, 2019.

\bibitem[Zhu et~al.(2020)Zhu, Yu, Gupta, Shah, Hartikainen, Singh, Kumar, and
  Levine]{zhu2020ingredients}
Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi
  Singh, Vikash Kumar, and Sergey Levine.
\newblock The ingredients of real-world robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.12570}, 2020.

\end{thebibliography}
