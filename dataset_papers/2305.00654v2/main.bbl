\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
A.~Abdolmaleki, J.~T. Springenberg, Y.~Tassa, R.~Munos, N.~Heess, and
  M.~Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock \emph{arXiv preprint arXiv:1806.06920}, 2018.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
A.~Agarwal, S.~Kakade, A.~Krishnamurthy, and W.~Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20095--20107, 2020.

\bibitem[Allen et~al.(2021)Allen, Parikh, Gottesman, and
  Konidaris]{allen2021learning}
C.~Allen, N.~Parikh, O.~Gottesman, and G.~Konidaris.
\newblock Learning markov state abstractions for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8229--8241, 2021.

\bibitem[Azar et~al.(2019)Azar, Piot, Pires, Grill, Altch{\'e}, and
  Munos]{azar2019world}
M.~G. Azar, B.~Piot, B.~A. Pires, J.-B. Grill, F.~Altch{\'e}, and R.~Munos.
\newblock World discovery models.
\newblock \emph{arXiv preprint arXiv:1902.07685}, 2019.

\bibitem[Azizzadenesheli et~al.(2016)Azizzadenesheli, Lazaric, and
  Anandkumar]{azizzadenesheli2016reinforcement}
K.~Azizzadenesheli, A.~Lazaric, and A.~Anandkumar.
\newblock Reinforcement learning of pomdps using spectral methods.
\newblock In \emph{Conference on Learning Theory}, pages 193--256. PMLR, 2016.

\bibitem[Badia et~al.(2020)Badia, Sprechmann, Vitvitskyi, Guo, Piot,
  Kapturowski, Tieleman, Arjovsky, Pritzel, Bolt, et~al.]{badia2020never}
A.~P. Badia, P.~Sprechmann, A.~Vitvitskyi, D.~Guo, B.~Piot, S.~Kapturowski,
  O.~Tieleman, M.~Arjovsky, A.~Pritzel, A.~Bolt, et~al.
\newblock Never give up: Learning directed exploration strategies.
\newblock \emph{arXiv preprint arXiv:2002.06038}, 2020.

\bibitem[Baird(1995)]{baird1995residual}
L.~Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pages 30--37. Elsevier,
  1995.

\bibitem[Beattie et~al.(2016)Beattie, Leibo, Teplyashin, Ward, Wainwright,
  K{\"u}ttler, Lefrancq, Green, Vald{\'e}s, Sadik, et~al.]{beattie2016deepmind}
C.~Beattie, J.~Z. Leibo, D.~Teplyashin, T.~Ward, M.~Wainwright, H.~K{\"u}ttler,
  A.~Lefrancq, S.~Green, V.~Vald{\'e}s, A.~Sadik, et~al.
\newblock Deepmind lab.
\newblock \emph{arXiv preprint arXiv:1612.03801}, 2016.

\bibitem[Behzadian et~al.(2019)Behzadian, Gharatappeh, and
  Petrik]{behzadian2019fast}
B.~Behzadian, S.~Gharatappeh, and M.~Petrik.
\newblock Fast feature selection for linear value function approximation.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, volume~29, pages 601--609, 2019.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
M.~Bellemare, S.~Srinivasan, G.~Ostrovski, T.~Schaul, D.~Saxton, and R.~Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Bellemare et~al.(2019)Bellemare, Dabney, Dadashi, Ali~Taiga, Castro,
  Le~Roux, Schuurmans, Lattimore, and Lyle]{bellemare2019geometric}
M.~Bellemare, W.~Dabney, R.~Dadashi, A.~Ali~Taiga, P.~S. Castro, N.~Le~Roux,
  D.~Schuurmans, T.~Lattimore, and C.~Lyle.
\newblock A geometric perspective on optimal representations for reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Boots and Gordon(2011)]{boots2011online}
B.~Boots and G.~Gordon.
\newblock An online spectral learning algorithm for partially observable
  nonlinear dynamical systems.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~25, pages 293--300, 2011.

\bibitem[Boots et~al.(2011)Boots, Siddiqi, and Gordon]{boots2011closing}
B.~Boots, S.~M. Siddiqi, and G.~J. Gordon.
\newblock Closing the learning-planning loop with predictive state
  representations.
\newblock \emph{The International Journal of Robotics Research}, 30\penalty0
  (7):\penalty0 954--966, 2011.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Y.~Burda, H.~Edwards, A.~Storkey, and O.~Klimov.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Dabney et~al.(2021)Dabney, Barreto, Rowland, Dadashi, Quan, Bellemare,
  and Silver]{dabney2021value}
W.~Dabney, A.~Barreto, M.~Rowland, R.~Dadashi, J.~Quan, M.~G. Bellemare, and
  D.~Silver.
\newblock The value-improvement path: Towards better representations for
  reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 7160--7168, 2021.

\bibitem[De~Handschutter et~al.(2021)De~Handschutter, Gillis, and
  Siebert]{de2021survey}
P.~De~Handschutter, N.~Gillis, and X.~Siebert.
\newblock A survey on deep matrix factorizations.
\newblock \emph{Computer Science Review}, 42:\penalty0 100423, 2021.

\bibitem[Deng et~al.(2022)Deng, Shi, and Zhu]{deng2022neuralef}
Z.~Deng, J.~Shi, and J.~Zhu.
\newblock Neuralef: Deconstructing kernels by deep neural networks.
\newblock \emph{arXiv preprint arXiv:2205.00165}, 2022.

\bibitem[Erraqabi et~al.(2022)Erraqabi, Machado, Zhao, Sukhbaatar, Lazaric,
  Denoyer, and Bengio]{erraqabi2022temporal}
A.~Erraqabi, M.~C. Machado, M.~Zhao, S.~Sukhbaatar, A.~Lazaric, L.~Denoyer, and
  Y.~Bengio.
\newblock Temporal abstractions-augmented temporally contrastive learning: An
  alternative to the laplacian in rl.
\newblock \emph{arXiv preprint arXiv:2203.11369}, 2022.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International conference on machine learning}, pages
  1407--1416. PMLR, 2018.

\bibitem[Garrido et~al.(2022)Garrido, Chen, Bardes, Najman, and
  Lecun]{garrido2022duality}
Q.~Garrido, Y.~Chen, A.~Bardes, L.~Najman, and Y.~Lecun.
\newblock On the duality between contrastive and non-contrastive
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2206.02574}, 2022.

\bibitem[Gemp et~al.(2020)Gemp, McWilliams, Vernade, and
  Graepel]{gemp2020eigengame}
I.~Gemp, B.~McWilliams, C.~Vernade, and T.~Graepel.
\newblock Eigengame: Pca as a nash equilibrium.
\newblock \emph{arXiv preprint arXiv:2010.00554}, 2020.

\bibitem[Ghosh and Bellemare(2020)]{ghosh2020representations}
D.~Ghosh and M.~G. Bellemare.
\newblock Representations for stable off-policy reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3556--3565. PMLR, 2020.

\bibitem[Gregor et~al.(2019)Gregor, Jimenez~Rezende, Besse, Wu, Merzic, and
  van~den Oord]{gregor2019shaping}
K.~Gregor, D.~Jimenez~Rezende, F.~Besse, Y.~Wu, H.~Merzic, and A.~van~den Oord.
\newblock Shaping belief states with generative environment models for rl.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Grinberg et~al.(2018)Grinberg, Aboutalebi, Lyman-Abramovitch, Balle,
  and Precup]{grinberg2018learning}
Y.~Grinberg, H.~Aboutalebi, M.~Lyman-Abramovitch, B.~Balle, and D.~Precup.
\newblock Learning predictive state representations from non-uniform sampling.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Gulcehre et~al.(2019)Gulcehre, Le~Paine, Shahriari, Denil, Hoffman,
  Soyer, Tanburn, Kapturowski, Rabinowitz, Williams,
  et~al.]{gulcehre2019making}
C.~Gulcehre, T.~Le~Paine, B.~Shahriari, M.~Denil, M.~Hoffman, H.~Soyer,
  R.~Tanburn, S.~Kapturowski, N.~Rabinowitz, D.~Williams, et~al.
\newblock Making efficient use of demonstrations to solve hard exploration
  problems.
\newblock In \emph{International conference on learning representations}, 2019.

\bibitem[Guo et~al.(2016)Guo, Doroudi, and Brunskill]{guo2016pac}
Z.~D. Guo, S.~Doroudi, and E.~Brunskill.
\newblock A pac rl algorithm for episodic pomdps.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 510--518.
  PMLR, 2016.

\bibitem[Guo et~al.(2020)Guo, Pires, Piot, Grill, Altch{\'e}, Munos, and
  Azar]{guo2020bootstrap}
Z.~D. Guo, B.~A. Pires, B.~Piot, J.-B. Grill, F.~Altch{\'e}, R.~Munos, and
  M.~G. Azar.
\newblock Bootstrap latent-predictive representations for multitask
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3875--3886. PMLR, 2020.

\bibitem[Guo et~al.(2022)Guo, Thakoor, P{\^\i}slar, Pires, Altch{\'e}, Tallec,
  Saade, Calandriello, Grill, Tang, et~al.]{guo2022byol}
Z.~D. Guo, S.~Thakoor, M.~P{\^\i}slar, B.~A. Pires, F.~Altch{\'e}, C.~Tallec,
  A.~Saade, D.~Calandriello, J.-B. Grill, Y.~Tang, et~al.
\newblock Byol-explore: Exploration by bootstrapped prediction.
\newblock \emph{arXiv preprint arXiv:2206.08332}, 2022.

\bibitem[Handschutter et~al.(2021)Handschutter, Gillis, and
  Siebert]{De_Handschutter_2021}
P.~D. Handschutter, N.~Gillis, and X.~Siebert.
\newblock A survey on deep matrix factorizations.
\newblock \emph{Computer Science Review}, 42:\penalty0 100423, nov 2021.
\newblock \doi{10.1016/j.cosrev.2021.100423}.
\newblock URL \url{https://doi.org/10.1016%2Fj.cosrev.2021.100423}.

\bibitem[Hessel et~al.(2019)Hessel, Soyer, Espeholt, Czarnecki, Schmitt, and
  van Hasselt]{hessel2019multi}
M.~Hessel, H.~Soyer, L.~Espeholt, W.~Czarnecki, S.~Schmitt, and H.~van Hasselt.
\newblock Multi-task deep reinforcement learning with popart.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3796--3803, 2019.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{houthooft2016vime}
R.~Houthooft, X.~Chen, Y.~Duan, J.~Schulman, F.~De~Turck, and P.~Abbeel.
\newblock Vime: Variational information maximizing exploration.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Hsu et~al.(2012)Hsu, Kakade, and Zhang]{hsu2012spectral}
D.~Hsu, S.~M. Kakade, and T.~Zhang.
\newblock A spectral algorithm for learning hidden markov models.
\newblock \emph{Journal of Computer and System Sciences}, 78\penalty0
  (5):\penalty0 1460--1480, 2012.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{jaderberg2016reinforcement}
M.~Jaderberg, V.~Mnih, W.~M. Czarnecki, T.~Schaul, J.~Z. Leibo, D.~Silver, and
  K.~Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{arXiv preprint arXiv:1611.05397}, 2016.

\bibitem[James and Singh(2004)]{james2004learning}
M.~R. James and S.~Singh.
\newblock Learning and discovery of predictive state representations in
  dynamical systems with reset.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~53, 2004.

\bibitem[Kwon et~al.(2021)Kwon, Efroni, Caramanis, and Mannor]{kwon2021rl}
J.~Kwon, Y.~Efroni, C.~Caramanis, and S.~Mannor.
\newblock Rl for latent mdps: Regret guarantees and a lower bound.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 24523--24534, 2021.

\bibitem[Lan et~al.(2022)Lan, Greaves, Farebrother, Rowland, Pedregosa,
  Agarwal, and Bellemare]{lan2022novel}
C.~L. Lan, J.~Greaves, J.~Farebrother, M.~Rowland, F.~Pedregosa, R.~Agarwal,
  and M.~G. Bellemare.
\newblock A novel stochastic gradient descent algorithm for learning principal
  subspaces.
\newblock \emph{arXiv preprint arXiv:2212.04025}, 2022.

\bibitem[Littman and Sutton(2001)]{littman2001predictive}
M.~Littman and R.~S. Sutton.
\newblock Predictive representations of state.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Liu et~al.(2017)Liu, Machado, Tesauro, and
  Campbell]{liu2017eigenoption}
M.~Liu, M.~C. Machado, G.~Tesauro, and M.~Campbell.
\newblock The eigenoption-critic framework.
\newblock \emph{arXiv preprint arXiv:1712.04065}, 2017.

\bibitem[Liu et~al.(2022)Liu, Chung, Szepesv{\'a}ri, and Jin]{liu2022partially}
Q.~Liu, A.~Chung, C.~Szepesv{\'a}ri, and C.~Jin.
\newblock When is partially observable reinforcement learning not scary?
\newblock \emph{arXiv preprint arXiv:2204.08967}, 2022.

\bibitem[Lyle et~al.(2021)Lyle, Rowland, Ostrovski, and Dabney]{lyle2021effect}
C.~Lyle, M.~Rowland, G.~Ostrovski, and W.~Dabney.
\newblock On the effect of auxiliary tasks on representation dynamics.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1--9. PMLR, 2021.

\bibitem[Lyle et~al.(2022{\natexlab{a}})Lyle, Rowland, and
  Dabney]{lyle2022understanding}
C.~Lyle, M.~Rowland, and W.~Dabney.
\newblock Understanding and preventing capacity loss in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2204.09560}, 2022{\natexlab{a}}.

\bibitem[Lyle et~al.(2022{\natexlab{b}})Lyle, Rowland, Dabney, Kwiatkowska, and
  Gal]{lyle2022learning}
C.~Lyle, M.~Rowland, W.~Dabney, M.~Kwiatkowska, and Y.~Gal.
\newblock Learning dynamics and generalization in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2206.02126}, 2022{\natexlab{b}}.

\bibitem[Machado et~al.(2017{\natexlab{a}})Machado, Bellemare, and
  Bowling]{machado2017laplacian}
M.~C. Machado, M.~G. Bellemare, and M.~Bowling.
\newblock A laplacian framework for option discovery in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2295--2304. PMLR, 2017{\natexlab{a}}.

\bibitem[Machado et~al.(2017{\natexlab{b}})Machado, Rosenbaum, Guo, Liu,
  Tesauro, and Campbell]{machado2017eigenoption}
M.~C. Machado, C.~Rosenbaum, X.~Guo, M.~Liu, G.~Tesauro, and M.~Campbell.
\newblock Eigenoption discovery through the deep successor representation.
\newblock \emph{arXiv preprint arXiv:1710.11089}, 2017{\natexlab{b}}.

\bibitem[Machado et~al.(2020)Machado, Bellemare, and Bowling]{machado2020count}
M.~C. Machado, M.~G. Bellemare, and M.~Bowling.
\newblock Count-based exploration with the successor representation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5125--5133, 2020.

\bibitem[Mahadevan(2005)]{mahadevan2005proto}
S.~Mahadevan.
\newblock Proto-value functions: Developmental reinforcement learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 553--560, 2005.

\bibitem[Mahadevan and Maggioni(2007)]{mahadevan2007proto}
S.~Mahadevan and M.~Maggioni.
\newblock Proto-value functions: A laplacian framework for learning
  representation and control in markov decision processes.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (10), 2007.

\bibitem[Mirowski et~al.(2016)Mirowski, Pascanu, Viola, Soyer, Ballard, Banino,
  Denil, Goroshin, Sifre, Kavukcuoglu, et~al.]{mirowski2016learning}
P.~Mirowski, R.~Pascanu, F.~Viola, H.~Soyer, A.~J. Ballard, A.~Banino,
  M.~Denil, R.~Goroshin, L.~Sifre, K.~Kavukcuoglu, et~al.
\newblock Learning to navigate in complex environments.
\newblock \emph{arXiv preprint arXiv:1611.03673}, 2016.

\bibitem[Misra et~al.(2020)Misra, Henaff, Krishnamurthy, and
  Langford]{misra2020kinematic}
D.~Misra, M.~Henaff, A.~Krishnamurthy, and J.~Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  6961--6971. PMLR, 2020.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum2018near}
O.~Nachum, S.~Gu, H.~Lee, and S.~Levine.
\newblock Near-optimal representation learning for hierarchical reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1810.01257}, 2018.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, Oord, and
  Munos]{ostrovski2017count}
G.~Ostrovski, M.~G. Bellemare, A.~Oord, and R.~Munos.
\newblock Count-based exploration with neural density models.
\newblock In \emph{International conference on machine learning}, pages
  2721--2730. PMLR, 2017.

\bibitem[Paine et~al.(2019)Paine, Gulcehre, Shahriari, Denil, Hoffman, Soyer,
  Tanburn, Kapturowski, Rabinowitz, Williams, et~al.]{paine2019making}
T.~L. Paine, C.~Gulcehre, B.~Shahriari, M.~Denil, M.~Hoffman, H.~Soyer,
  R.~Tanburn, S.~Kapturowski, N.~Rabinowitz, D.~Williams, et~al.
\newblock Making efficient use of demonstrations to solve hard exploration
  problems.
\newblock \emph{arXiv preprint arXiv:1909.01387}, 2019.

\bibitem[Parr et~al.(2007)Parr, Painter-Wakefield, Li, and
  Littman]{parr2007analyzing}
R.~Parr, C.~Painter-Wakefield, L.~Li, and M.~Littman.
\newblock Analyzing feature generation for value-function approximation.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pages 737--744, 2007.

\bibitem[Parr et~al.(2008)Parr, Li, Taylor, Painter-Wakefield, and
  Littman]{parr2008analysis}
R.~Parr, L.~Li, G.~Taylor, C.~Painter-Wakefield, and M.~L. Littman.
\newblock An analysis of linear models, linear value-function approximation,
  and feature selection for reinforcement learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 752--759, 2008.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
D.~Pathak, P.~Agrawal, A.~A. Efros, and T.~Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International conference on machine learning}, pages
  2778--2787. PMLR, 2017.

\bibitem[Petrik(2007)]{petrik2007analysis}
M.~Petrik.
\newblock An analysis of laplacian methods for value function approximation in
  mdps.
\newblock In \emph{IJCAI}, pages 2574--2579, 2007.

\bibitem[Ren et~al.(2022)Ren, Zhang, Lee, Gonzalez, Schuurmans, and
  Dai]{ren2022spectral}
T.~Ren, T.~Zhang, L.~Lee, J.~E. Gonzalez, D.~Schuurmans, and B.~Dai.
\newblock Spectral decomposition representation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2208.09515}, 2022.

\bibitem[Rosencrantz et~al.(2004)Rosencrantz, Gordon, and
  Thrun]{rosencrantz2004learning}
M.~Rosencrantz, G.~Gordon, and S.~Thrun.
\newblock Learning low dimensional predictive representations.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~88, 2004.

\bibitem[Roy et~al.(2005)Roy, Gordon, and Thrun]{roy2005finding}
N.~Roy, G.~Gordon, and S.~Thrun.
\newblock Finding approximate pomdp solutions through belief compression.
\newblock \emph{Journal of artificial intelligence research}, 23:\penalty0
  1--40, 2005.

\bibitem[Rudary and Singh(2003)]{rudary2003nonlinear}
M.~Rudary and S.~Singh.
\newblock A nonlinear predictive state representation.
\newblock \emph{Advances in neural information processing systems}, 16, 2003.

\bibitem[Schmidhuber(1991)]{schmidhuber1991possibility}
J.~Schmidhuber.
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In \emph{Proc. of the international conference on simulation of
  adaptive behavior: From animals to animats}, pages 222--227, 1991.

\bibitem[Sch{\"o}lkopf et~al.(1997)Sch{\"o}lkopf, Smola, and
  M{\"u}ller]{scholkopf1997kernel}
B.~Sch{\"o}lkopf, A.~Smola, and K.-R. M{\"u}ller.
\newblock Kernel principal component analysis.
\newblock In \emph{International conference on artificial neural networks},
  pages 583--588. Springer, 1997.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Schwarzer et~al.(2020)Schwarzer, Anand, Goel, Hjelm, Courville, and
  Bachman]{schwarzer2020data}
M.~Schwarzer, A.~Anand, R.~Goel, R.~D. Hjelm, A.~Courville, and P.~Bachman.
\newblock Data-efficient reinforcement learning with self-predictive
  representations.
\newblock \emph{arXiv preprint arXiv:2007.05929}, 2020.

\bibitem[Singh et~al.(2012)Singh, James, and Rudary]{singh2012predictive}
S.~Singh, M.~James, and M.~Rudary.
\newblock Predictive state representations: A new theory for modeling dynamical
  systems.
\newblock \emph{arXiv preprint arXiv:1207.4167}, 2012.

\bibitem[Singh et~al.(2003)Singh, Littman, Jong, Pardoe, and
  Stone]{singh2003learning}
S.~P. Singh, M.~L. Littman, N.~K. Jong, D.~Pardoe, and P.~Stone.
\newblock Learning predictive state representations.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pages 712--719, 2003.

\bibitem[Song et~al.(2019)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae,
  Noury, Ahuja, Liu, Tirumala, et~al.]{song2019v}
H.~F. Song, A.~Abdolmaleki, J.~T. Springenberg, A.~Clark, H.~Soyer, J.~W. Rae,
  S.~Noury, A.~Ahuja, S.~Liu, D.~Tirumala, et~al.
\newblock V-mpo: On-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock \emph{arXiv preprint arXiv:1909.12238}, 2019.

\bibitem[Song et~al.(2016)Song, Parr, Liao, and Carin]{song2016linear}
Z.~Song, R.~E. Parr, X.~Liao, and L.~Carin.
\newblock Linear feature encoding for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Srinivas et~al.(2020)Srinivas, Laskin, and Abbeel]{srinivas2020curl}
A.~Srinivas, M.~Laskin, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2004.04136}, 2020.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Xi~Chen, Duan,
  Schulman, DeTurck, and Abbeel]{tang2017exploration}
H.~Tang, R.~Houthooft, D.~Foote, A.~Stooke, O.~Xi~Chen, Y.~Duan, J.~Schulman,
  F.~DeTurck, and P.~Abbeel.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Tang et~al.(2022)Tang, Guo, Richemond, Pires, Chandak, Munos, Rowland,
  Azar, Lan, Lyle, et~al.]{tang2022understanding}
Y.~Tang, Z.~D. Guo, P.~H. Richemond, B.~{\'A}. Pires, Y.~Chandak, R.~Munos,
  M.~Rowland, M.~G. Azar, C.~L. Lan, C.~Lyle, et~al.
\newblock Understanding self-predictive learning for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2212.03319}, 2022.

\bibitem[Touati and Ollivier(2021)]{touati2021learning}
A.~Touati and Y.~Ollivier.
\newblock Learning one representation to optimize all rewards.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 13--23, 2021.

\bibitem[Van~Loan(1976)]{van1976generalizing}
C.~F. Van~Loan.
\newblock Generalizing the singular value decomposition.
\newblock \emph{SIAM Journal on numerical Analysis}, 13\penalty0 (1):\penalty0
  76--83, 1976.

\bibitem[Wang et~al.(2021)Wang, Zhou, Zhang, Shao, Hooi, and
  Feng]{wang2021towards}
K.~Wang, K.~Zhou, Q.~Zhang, J.~Shao, B.~Hooi, and J.~Feng.
\newblock Towards better laplacian representation in reinforcement learning
  with generalized graph drawing.
\newblock In \emph{International Conference on Machine Learning}, pages
  11003--11012. PMLR, 2021.

\bibitem[Wang et~al.(2022)Wang, Cai, Yang, and Wang]{wang2022embed}
L.~Wang, Q.~Cai, Z.~Yang, and Z.~Wang.
\newblock Embed to control partially observed systems: Representation learning
  with provable sample efficiency.
\newblock \emph{arXiv preprint arXiv:2205.13476}, 2022.

\bibitem[Weng(2020)]{weng2020exploration}
L.~Weng.
\newblock Exploration strategies in deep reinforcement learning.
\newblock \emph{Online: https://lilianweng. github.
  io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.
  html}, 2020.

\bibitem[Wu et~al.(2018)Wu, Tucker, and Nachum]{wu2018laplacian}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock The laplacian in rl: Learning representations with efficient
  approximations.
\newblock \emph{arXiv preprint arXiv:1810.04586}, 2018.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
J.~Zbontar, L.~Jing, I.~Misra, Y.~LeCun, and S.~Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{International Conference on Machine Learning}, pages
  12310--12320. PMLR, 2021.

\bibitem[Zhan et~al.(2022)Zhan, Uehara, Sun, and Lee]{zhan2022pac}
W.~Zhan, M.~Uehara, W.~Sun, and J.~D. Lee.
\newblock Pac reinforcement learning for predictive state representations.
\newblock \emph{arXiv preprint arXiv:2207.05738}, 2022.

\end{thebibliography}
