\begin{thebibliography}{10}

\bibitem{agarwal2020flambe}
A.~Agarwal, S.~Kakade, A.~Krishnamurthy, and W.~Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.05449}, 2017.

\bibitem{bai2020provable}
Y.~Bai and C.~Jin.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  551--560. PMLR, 2020.

\bibitem{bai2020near}
Y.~Bai, C.~Jin, and T.~Yu.
\newblock Near-optimal reinforcement learning with self-play.
\newblock {\em arXiv preprint arXiv:2006.12007}, 2020.

\bibitem{baker2020emergent}
B.~Baker, I.~Kanitscheider, T.~Markov, Y.~Wu, G.~Powell, B.~McGrew, and
  I.~Mordatch.
\newblock Emergent tool use from multi-agent autocurricula.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{berner2019dota}
C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~Debiak, C.~Dennison, D.~Farhi,
  Q.~Fischer, S.~Hashme, C.~Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1912.06680}, 2019.

\bibitem{brafman2002r}
R.~I. Brafman and M.~Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 3(Oct):213--231, 2002.

\bibitem{brambilla2013swarm}
M.~Brambilla, E.~Ferrante, M.~Birattari, and M.~Dorigo.
\newblock Swarm robotics: a review from the swarm engineering perspective.
\newblock {\em Swarm Intelligence}, 7(1):1--41, 2013.

\bibitem{brown2018superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman ai for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock {\em Science}, 359(6374):418--424, 2018.

\bibitem{brown2019superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman ai for multiplayer poker.
\newblock {\em Science}, 365(6456):885--890, 2019.

\bibitem{cai2019provably}
Q.~Cai, Z.~Yang, C.~Jin, and Z.~Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock {\em arXiv preprint arXiv:1912.05830}, 2019.

\bibitem{celli2020no}
A.~Celli, A.~Marchesi, G.~Farina, and N.~Gatti.
\newblock No-regret learning dynamics for extensive-form correlated
  equilibrium.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{chen2021almost}
Z.~Chen, D.~Zhou, and Q.~Gu.
\newblock Almost optimal algorithms for two-player markov games with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:2102.07404}, 2021.

\bibitem{dann2015sample}
C.~Dann and E.~Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem{dong2020root}
K.~Dong, J.~Peng, Y.~Wang, and Y.~Zhou.
\newblock Root-n-regret for learning in markov decision processes with function
  approximation and low bellman rank.
\newblock In {\em Conference on Learning Theory}, pages 1554--1557. PMLR, 2020.

\bibitem{du2021bilinear}
S.~S. Du, S.~M. Kakade, J.~D. Lee, S.~Lovett, G.~Mahajan, W.~Sun, and R.~Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock {\em arXiv preprint arXiv:2103.10897}, 2021.

\bibitem{filar2012competitive}
J.~Filar and K.~Vrieze.
\newblock {\em Competitive Markov decision processes}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{foster2020instance}
D.~J. Foster, A.~Rakhlin, D.~Simchi-Levi, and Y.~Xu.
\newblock Instance-dependent complexity of contextual bandits and reinforcement
  learning: A disagreement-based perspective.
\newblock {\em arXiv preprint arXiv:2010.03104}, 2020.

\bibitem{gilpin2006finding}
A.~Gilpin and T.~Sandholm.
\newblock Finding equilibria in large sequential games of imperfect
  information.
\newblock In {\em Proceedings of the 7th ACM conference on Electronic
  commerce}, pages 160--169, 2006.

\bibitem{hansen2013strategy}
T.~D. Hansen, P.~B. Miltersen, and U.~Zwick.
\newblock Strategy iteration is strongly polynomial for 2-player turn-based
  stochastic games with a constant discount factor.
\newblock {\em Journal of the ACM (JACM)}, 60(1):1--16, 2013.

\bibitem{hu2003nash}
J.~Hu and M.~P. Wellman.
\newblock Nash q-learning for general-sum stochastic games.
\newblock {\em Journal of machine learning research}, 4(Nov):1039--1069, 2003.

\bibitem{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(4), 2010.

\bibitem{jia2019feature}
Z.~Jia, L.~F. Yang, and M.~Wang.
\newblock Feature-based q-learning for two-player stochastic games.
\newblock {\em arXiv preprint arXiv:1906.00423}, 2019.

\bibitem{jiang2017contextual}
N.~Jiang, A.~Krishnamurthy, A.~Agarwal, J.~Langford, and R.~E. Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In {\em International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem{jin2018q}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is q-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem{jin2021bellman}
C.~Jin, Q.~Liu, and S.~Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em arXiv preprint arXiv:2102.00815}, 2021.

\bibitem{jin2020provably}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143, 2020.

\bibitem{koller1992complexity}
D.~Koller and N.~Megiddo.
\newblock The complexity of two-person zero-sum games in extensive form.
\newblock {\em Games and economic behavior}, 4(4):528--552, 1992.

\bibitem{krishnamurthy2016pac}
A.~Krishnamurthy, A.~Agarwal, and J.~Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock {\em arXiv preprint arXiv:1602.02722}, 2016.

\bibitem{li2020multi}
H.~Li and H.~He.
\newblock Multi-agent trust region policy optimization.
\newblock {\em arXiv preprint arXiv:2010.07916}, 2020.

\bibitem{littman1994markov}
M.~L. Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In {\em Machine learning proceedings 1994}, pages 157--163. Elsevier,
  1994.

\bibitem{littman2001friend}
M.~L. Littman.
\newblock Friend-or-foe q-learning in general-sum games.
\newblock In {\em ICML}, volume~1, pages 322--328, 2001.

\bibitem{liu2020sharp}
Q.~Liu, T.~Yu, Y.~Bai, and C.~Jin.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock {\em arXiv preprint arXiv:2010.01604}, 2020.

\bibitem{lowe2017multi}
R.~Lowe, Y.~Wu, A.~Tamar, J.~Harb, P.~Abbeel, and I.~Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock {\em arXiv preprint arXiv:1706.02275}, 2017.

\bibitem{neu2020unifying}
G.~Neu and C.~Pike-Burke.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{openaidota}
OpenAI.
\newblock Openai five.
\newblock \url{https://blog.openai.com/openai-five/}, 2018.

\bibitem{osband2014model}
I.~Osband and B.~Van~Roy.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1466--1474, 2014.

\bibitem{rashid2018qmix}
T.~Rashid, M.~Samvelyan, C.~Schroeder, G.~Farquhar, J.~Foerster, and
  S.~Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4295--4304. PMLR, 2018.

\bibitem{russo2013eluder}
D.~Russo and B.~Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2256--2264, 2013.

\bibitem{shalev2016safe}
S.~Shalev-Shwartz, S.~Shammah, and A.~Shashua.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock {\em arXiv preprint arXiv:1610.03295}, 2016.

\bibitem{shapley1953stochastic}
L.~S. Shapley.
\newblock Stochastic games.
\newblock {\em Proceedings of the national academy of sciences},
  39(10):1095--1100, 1953.

\bibitem{sidford2020solving}
A.~Sidford, M.~Wang, L.~Yang, and Y.~Ye.
\newblock Solving discounted stochastic two-player games with near-optimal time
  and sample complexity.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2992--3002. PMLR, 2020.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484--489, 2016.

\bibitem{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}, 550(7676):354--359, 2017.

\bibitem{sun2019model}
W.~Sun, N.~Jiang, A.~Krishnamurthy, A.~Agarwal, and J.~Langford.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In {\em Conference on Learning Theory}, pages 2898--2933, 2019.

\bibitem{szepesvari2010algorithms}
C.~Szepesv{\'a}ri.
\newblock Algorithms for reinforcement learning.
\newblock {\em Synthesis lectures on artificial intelligence and machine
  learning}, 4(1):1--103, 2010.

\bibitem{tian2020provably}
Y.~Tian, Y.~Wang, T.~Yu, and S.~Sra.
\newblock Online learning in unknown markov games.
\newblock {\em arXiv preprint arXiv:2010.15020}, 2021.

\bibitem{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354, 2019.

\bibitem{wang2020provably}
R.~Wang, R.~Salakhutdinov, and L.~F. Yang.
\newblock Provably efficient reinforcement learning with general value function
  approximation.
\newblock {\em arXiv preprint arXiv:2005.10804}, 2020.

\bibitem{wang2019optimism}
Y.~Wang, R.~Wang, S.~S. Du, and A.~Krishnamurthy.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock {\em arXiv preprint arXiv:1912.04136}, 2019.

\bibitem{wei2017online}
C.-Y. Wei, Y.-T. Hong, and C.-J. Lu.
\newblock Online reinforcement learning in stochastic games.
\newblock {\em arXiv preprint arXiv:1712.00579}, 2017.

\bibitem{wei2020linear}
C.-Y. Wei, C.-W. Lee, M.~Zhang, and H.~Luo.
\newblock Linear last-iterate convergence in constrained saddle-point
  optimization.
\newblock {\em arXiv e-prints}, pages arXiv--2006, 2020.

\bibitem{wei2021last}
C.-Y. Wei, C.-W. Lee, M.~Zhang, and H.~Luo.
\newblock Last-iterate convergence of decentralized optimistic gradient
  descent/ascent in infinite-horizon competitive markov games.
\newblock {\em arXiv preprint arXiv:2102.04540}, 2021.

\bibitem{weisz2020exponential}
G.~Weisz, P.~Amortila, and C.~Szepesv{\'a}ri.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock {\em arXiv preprint arXiv:2010.01374}, 2020.

\bibitem{xie2020learning}
Q.~Xie, Y.~Chen, Z.~Wang, and Z.~Yang.
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In {\em Conference on Learning Theory}, pages 3674--3682. PMLR, 2020.

\bibitem{yang2020bridging}
Z.~Yang, C.~Jin, Z.~Wang, M.~Wang, and M.~I. Jordan.
\newblock Bridging exploration and general function approximation in
  reinforcement learning: Provably efficient kernel and neural value
  iterations.
\newblock {\em arXiv preprint arXiv:2011.04622}, 2020.

\bibitem{yu2021surprising}
C.~Yu, A.~Velu, E.~Vinitsky, Y.~Wang, A.~Bayen, and Y.~Wu.
\newblock The surprising effectiveness of mappo in cooperative, multi-agent
  games.
\newblock {\em arXiv preprint arXiv:2103.01955}, 2021.

\bibitem{yu2021provably}
T.~Yu, Y.~Tian, J.~Zhang, and S.~Sra.
\newblock Provably efficient algorithms for multi-objective competitive rl.
\newblock {\em arXiv preprint arXiv:2102.03192}, 2021.

\bibitem{zanette2019tighter}
A.~Zanette and E.~Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock {\em arXiv preprint arXiv:1901.00210}, 2019.

\bibitem{zanette2020learning}
A.~Zanette, A.~Lazaric, M.~Kochenderfer, and E.~Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock {\em arXiv preprint arXiv:2003.00153}, 2020.

\bibitem{zanette2020provably}
A.~Zanette, A.~Lazaric, M.~J. Kochenderfer, and E.~Brunskill.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{zhang2020model}
K.~Zhang, S.~M. Kakade, T.~Ba{\c{s}}ar, and L.~F. Yang.
\newblock Model-based multi-agent rl in zero-sum markov games with near-optimal
  sample complexity.
\newblock {\em arXiv preprint arXiv:2007.07461}, 2020.

\bibitem{zhang2020almost}
Z.~Zhang, Y.~Zhou, and X.~Ji.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock {\em arXiv preprint arXiv:2004.10019}, 2020.

\bibitem{zinkevich2007regret}
M.~Zinkevich, M.~Johanson, M.~Bowling, and C.~Piccione.
\newblock Regret minimization in games with incomplete information.
\newblock {\em Advances in neural information processing systems},
  20:1729--1736, 2007.

\end{thebibliography}
