\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A., and Bellemare, M.~G.
\newblock Deep reinforcement learning at the edge of the statistical precipice,
  2021.

\bibitem[Allen-Zhu \& Li(2021)Allen-Zhu and Li]{allenzhu2021feature}
Allen-Zhu, Z. and Li, Y.
\newblock Feature purification: How adversarial training performs robust deep
  learning, 2021.

\bibitem[Alsallakh et~al.(2021)Alsallakh, Kokhlikyan, Miglani, Yuan, and
  Reblitz-Richardson]{mindThePad}
Alsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., and Reblitz-Richardson,
  O.
\newblock Mind the pad -- {\{}cnn{\}}s can develop blind spots.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=m1CD7tPubNy}.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, and Lacoste-Julien]{memorizationCNNs}
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.~S.,
  Maharaj, T., Fischer, A., Courville, A., Bengio, Y., and Lacoste-Julien, S.
\newblock A closer look at memorization in deep networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  233--242. PMLR, 06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/arpit17a.html}.

\bibitem[Baird(1999)]{gradient-TD-Baird-2}
Baird, L.
\newblock Reinforcement learning through gradient descent.
\newblock Technical report, Carnegie-Mellon University, Department of Computer
  Science, 1999.

\bibitem[Baird \& Moore(1998)Baird and Moore]{gradient-TD-Baird}
Baird, L. and Moore, A.
\newblock Gradient descent for general reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 11, 1998.

\bibitem[Ball et~al.(2020)Ball, Parker-Holder, Pacchiano, Choromanski, and
  Roberts]{rp1}
Ball, P., Parker-Holder, J., Pacchiano, A., Choromanski, K., and Roberts, S.
\newblock Ready policy one: World building through active learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML}}. 2020.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{atariALE}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellman(1957)]{mdp}
Bellman, R.
\newblock A markovian decision process.
\newblock \emph{Indiana Univ. Math. J.}, 6:\penalty0 679--684, 1957.
\newblock ISSN 0022-2518.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{OpenAI5}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and
  Bruna]{brandfonbrener2021offline}
Brandfonbrener, D., Whitney, W.~F., Ranganath, R., and Bruna, J.
\newblock Offline {RL} without off-policy evaluation.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=LU687itn08w}.

\bibitem[Bu{\c{s}}oniu et~al.(2018)Bu{\c{s}}oniu, de~Bruin, Toli{\'c}, Kober,
  and Palunko]{RL_stability_practical}
Bu{\c{s}}oniu, L., de~Bruin, T., Toli{\'c}, D., Kober, J., and Palunko, I.
\newblock Reinforcement learning for control: Performance, stability, and deep
  approximators.
\newblock \emph{Annual Reviews in Control}, 46:\penalty0 8--28, 2018.

\bibitem[Cetin \& Celiktutan(2021)Cetin and Celiktutan]{learningpessimism}
Cetin, E. and Celiktutan, O.
\newblock Learning pessimism for robust and efficient off-policy reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2110.03375}, 2021.

\bibitem[Chatterjee(2020)]{coherentgrads}
Chatterjee, S.
\newblock Coherent gradients: An approach to understanding generalization in
  gradient descent-based optimization.
\newblock \emph{arXiv preprint arXiv:2002.10657}, 2020.

\bibitem[Cobbe et~al.(2021)Cobbe, Hilton, Klimov, and Schulman]{ppg}
Cobbe, K.~W., Hilton, J., Klimov, O., and Schulman, J.
\newblock Phasic policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2020--2027. PMLR, 2021.

\bibitem[Dolan \& Mor{\'e}(2002)Dolan and Mor{\'e}]{perf-profile}
Dolan, E.~D. and Mor{\'e}, J.~J.
\newblock Benchmarking optimization software with performance profiles.
\newblock \emph{Mathematical programming}, 91\penalty0 (2):\penalty0 201--213,
  2002.

\bibitem[Dror et~al.(2019)Dror, Shlomov, and Reichart]{stoch-dom}
Dror, R., Shlomov, S., and Reichart, R.
\newblock Deep dominance-how to properly compare deep neural models.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2773--2785, 2019.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{unstable_off}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International conference on machine learning}, pp.\
  1329--1338. PMLR, 2016.

\bibitem[Dulac-Arnold et~al.(2019)Dulac-Arnold, Mankowitz, and
  Hester]{challenges-rl}
Dulac-Arnold, G., Mankowitz, D., and Hester, T.
\newblock Challenges of real-world reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1904.12901}, 2019.

\bibitem[Dwibedi et~al.(2018)Dwibedi, Tompson, Lynch, and
  Sermanet]{actionable-repr}
Dwibedi, D., Tompson, J., Lynch, C., and Sermanet, P.
\newblock Learning actionable representations from visual observations.
\newblock In \emph{2018 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  1577--1584. IEEE, 2018.

\bibitem[Efron(1992)]{bootstrap-cis}
Efron, B.
\newblock Bootstrap methods: another look at the jackknife.
\newblock In \emph{Breakthroughs in statistics}, pp.\  569--593. Springer,
  1992.

\bibitem[Finn et~al.(2015)Finn, Tan, Duan, Darrell, Levine, and
  Abbeel]{deepspatial-ae}
Finn, C., Tan, X.~Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P.
\newblock Learning visual feature spaces for robotic manipulation with deep
  spatial autoencoders.
\newblock \emph{arXiv preprint arXiv:1509.06113}, 25, 2015.

\bibitem[Fu et~al.(2021)Fu, Kumar, Nachum, Tucker, and Levine]{d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4{\{}rl{\}}: Datasets for deep data-driven reinforcement learning,
  2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{td3}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{ICML}, pp.\  1582--1591, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/fujimoto18a.html}.

\bibitem[Ghiasi et~al.(2018)Ghiasi, Lin, and Le]{dropblock}
Ghiasi, G., Lin, T.-Y., and Le, Q.~V.
\newblock Dropblock: A regularization method for convolutional networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper.pdf}.

\bibitem[Gogianu et~al.(2021)Gogianu, Berariu, Rosca, Clopath, Busoniu, and
  Pascanu]{specnormDQN}
Gogianu, F., Berariu, T., Rosca, M.~C., Clopath, C., Busoniu, L., and Pascanu,
  R.
\newblock Spectral normalisation for deep reinforcement learning: An
  optimisation perspective.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3734--3744. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/gogianu21a.html}.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1861--1870. PMLR, 10--15 Jul
  2018{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v80/haarnoja18b.html}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{sac-alg}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{planet}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2555--2565. PMLR, 2019.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and Norouzi]{dreamer}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hernandez-Garcia \& Sutton(2019)Hernandez-Garcia and
  Sutton]{nstepStudy}
Hernandez-Garcia, J.~F. and Sutton, R.~S.
\newblock Understanding multi-step deep reinforcement learning: A systematic
  study of the dqn target, 2019.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{rainbowDQN}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Kaiser et~al.(2020)Kaiser, Babaeizadeh, Milos, Osiński, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, Mohiuddin, Sepassi, Tucker, and
  Michalewski]{simple}
Kaiser, L., Babaeizadeh, M., Milos, P., Osiński, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin,
  A., Sepassi, R., Tucker, G., and Michalewski, H.
\newblock Model based reinforcement learning for {A}tari.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{qt-opt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Kearns \& Singh(2000)Kearns and Singh]{nstepBias}
Kearns, M.~J. and Singh, S.~P.
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In \emph{Proceedings of the Thirteenth Annual Conference on
  Computational Learning Theory}, COLT '00, pp.\  142–147, San Francisco, CA,
  USA, 2000. Morgan Kaufmann Publishers Inc.
\newblock ISBN 155860703X.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{sharpLossBadGeneralization}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Kielak(2019)]{otrainbow}
Kielak, K.~P.
\newblock Do recent advancements in model-based deep reinforcement learning
  really improve data efficiency?
\newblock 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Yarats, and Fergus]{drq}
Kostrikov, I., Yarats, D., and Fergus, R.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{International Conference on Learning Representations}. 2021.

\bibitem[Kumar et~al.(2021)Kumar, Agarwal, Ghosh, and
  Levine]{kumar2021implicit}
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S.
\newblock Implicit under-parameterization inhibits data-efficient deep
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=O9bnihsFfXU}.

\bibitem[Laskin et~al.(2020{\natexlab{a}})Laskin, Lee, Stooke, Pinto, Abbeel,
  and Srinivas]{rad}
Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A.
\newblock Reinforcement learming with augmented data.
\newblock In \emph{Advances in Neural Information Processing Systems 33}.
  2020{\natexlab{a}}.

\bibitem[Laskin et~al.(2020{\natexlab{b}})Laskin, Srinivas, and Abbeel]{curl}
Laskin, M., Srinivas, A., and Abbeel, P.
\newblock {CURL}: Contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020{\natexlab{b}}.

\bibitem[Lee et~al.(2019)Lee, Nagabandi, Abbeel, and Levine]{slac}
Lee, A.~X., Nagabandi, A., Abbeel, P., and Levine, S.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock \emph{arXiv preprint arXiv:1907.00953}, 2019.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{offlinerl_survey}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems, 2020.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{visualloss}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu et~al.(2020)Liu, Papailiopoulos, and
  Achlioptas]{badglobalminimaexist}
Liu, S., Papailiopoulos, D., and Achlioptas, D.
\newblock Bad global minima exist and sgd can reach them.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Machado et~al.(2018)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{machadoALEprotocol}
Machado, M.~C., Bellemare, M.~G., Talvitie, E., Veness, J., Hausknecht, M., and
  Bowling, M.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  523--562, 2018.

\bibitem[Maennel et~al.(2020)Maennel, Alabdulmohsin, Tolstikhin, Baldock,
  Bousquet, Gelly, and Keysers]{randomlabels}
Maennel, H., Alabdulmohsin, I.~M., Tolstikhin, I.~O., Baldock, R., Bousquet,
  O., Gelly, S., and Keysers, D.
\newblock What do neural networks learn when trained with random labels?
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  19693--19704. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/e4191d610537305de1d294adb121b513-Paper.pdf}.

\bibitem[Mann \& Whitney(1947)Mann and Whitney]{mannwhitneyUstat}
Mann, H.~B. and Whitney, D.~R.
\newblock {On a Test of Whether one of Two Random Variables is Stochastically
  Larger than the Other}.
\newblock \emph{The Annals of Mathematical Statistics}, 18\penalty0
  (1):\penalty0 50 -- 60, 1947.
\newblock \doi{10.1214/aoms/1177730491}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177730491}.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{spectralNorm}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1QRgziT-}.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Moskovitz et~al.(2021)Moskovitz, Parker-Holder, Pacchiano, Arbel, and
  Jordan]{moskovitz2021tactical}
Moskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M., and Jordan, M.
\newblock Tactical optimism and pessimism for deep reinforcement learning.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=a4WgjcLeZIn}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{exploringgeneralization}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, pp.\  5949–5958, Red Hook, NY,
  USA, 2017. Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[Parker-Holder et~al.(2022)Parker-Holder, Rajan, Song, Biedenkapp,
  Miao, Eimer, Zhang, Nguyen, Calandra, Faust, Hutter, and
  Lindauer]{parkerholder2022automated}
Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A., Miao, Y., Eimer, T.,
  Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F., and Lindauer, M.
\newblock Automated reinforcement learning (autorl): A survey and open
  problems, 2022.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{cnnspectralbias}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock On the spectral bias of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5301--5310. PMLR, 2019.

\bibitem[Rosca et~al.(2020)Rosca, Weber, Gretton, and Mohamed]{rosca2020a}
Rosca, M., Weber, T., Gretton, A., and Mohamed, S.
\newblock A case for new neural networks smoothness constraints.
\newblock In \emph{''I Can't Believe It's Not Better!'' NeurIPS 2020 workshop},
  2020.
\newblock URL \url{https://openreview.net/forum?id=_b-uT9wCI-7}.

\bibitem[Rummery \& Niranjan(1994)Rummery and Niranjan]{sarsa}
Rummery, G.~A. and Niranjan, M.
\newblock On-line {Q}-learning using connectionist systems.
\newblock Technical Report TR 166, Cambridge University Engineering Department,
  Cambridge, England, 1994.

\bibitem[Schaul et~al.(2021)Schaul, Ostrovski, Kemaev, and
  Borsa]{schaul2021returnbased}
Schaul, T., Ostrovski, G., Kemaev, I., and Borsa, D.
\newblock Return-based scaling: Yet another normalisation trick for deep rl,
  2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.06347}.

\bibitem[Schwarzer et~al.(2020)Schwarzer, Anand, Goel, Hjelm, Courville, and
  Bachman]{spr-atari}
Schwarzer, M., Anand, A., Goel, R., Hjelm, R.~D., Courville, A., and Bachman,
  P.
\newblock Data-efficient reinforcement learning with self-predictive
  representations.
\newblock \emph{arXiv preprint arXiv:2007.05929}, 2020.

\bibitem[Shorten \& Khoshgoftaar(2019)Shorten and
  Khoshgoftaar]{shorten2019augsurvey}
Shorten, C. and Khoshgoftaar, T.~M.
\newblock A survey on image data augmentation for deep learning.
\newblock \emph{Journal of Big Data}, 6\penalty0 (1):\penalty0 1--48, 2019.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{dpg}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock 2014.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{alphazero}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Song et~al.(2020)Song, Jiang, Tu, Du, and
  Neyshabur]{Song2020Observational}
Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B.
\newblock Observational overfitting in reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Student(1908)]{studentttest}
Student.
\newblock The probable error of a mean.
\newblock \emph{Biometrika}, 6\penalty0 (1):\penalty0 1--25, 1908.
\newblock ISSN 00063444.
\newblock URL \url{http://www.jstor.org/stable/2331554}.

\bibitem[Sutton(1988)]{suttonTDlearning}
Sutton, R.
\newblock Learning to predict by the method of temporal differences.
\newblock \emph{Machine Learning}, 3:\penalty0 9--44, 08 1988.
\newblock \doi{10.1007/BF00115009}.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and Mansour]{pg-thm}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden,
  Abdolmaleki, Merel, Lefrancq, et~al.]{dmc}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d.~L., Budden,
  D., Abdolmaleki, A., Merel, J., Lefrancq, A., et~al.
\newblock Deepmind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Van~Hasselt et~al.(2018)Van~Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{deadly_triad}
Van~Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{arXiv preprint arXiv:1812.02648}, 2018.

\bibitem[van Hasselt et~al.(2019)van Hasselt, Hessel, and
  Aslanides]{der-rainbow}
van Hasselt, H.~P., Hessel, M., and Aslanides, J.
\newblock When to use parametric models in reinforcement learning?
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 14322--14333, 2019.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{alphastar}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wilcoxon(1945)]{wilcoxon}
Wilcoxon, F.
\newblock Individual comparisons by ranking methods.
\newblock \emph{Biometrics Bulletin}, 1\penalty0 (6):\penalty0 80--83, 1945.
\newblock ISSN 00994987.
\newblock URL \url{http://www.jstor.org/stable/3001968}.

\bibitem[Yarats et~al.(2021)Yarats, Zhang, Kostrikov, Amos, Pineau, and
  Fergus]{sacae}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (12):\penalty0 10674--10681, May 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/17276}.

\bibitem[Yarats et~al.(2022)Yarats, Fergus, Lazaric, and Pinto]{drqv2}
Yarats, D., Fergus, R., Lazaric, A., and Pinto, L.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_SJ-_yyes8}.

\bibitem[Zhu et~al.(2020)Zhu, Yu, Gupta, Shah, Hartikainen, Singh, Kumar, and
  Levine]{rl_real_world_ingredients}
Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V.,
  and Levine, S.
\newblock The ingredients of real-world robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.12570}, 2020.

\end{thebibliography}
