\begin{thebibliography}{10}

\bibitem{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 41--48, 2009.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622, 2015.

\bibitem{cao2019learning}
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{chen2019understanding}
Pengfei Chen, Ben~Ben Liao, Guangyong Chen, and Shengyu Zhang.
\newblock Understanding and utilizing deep neural networks trained with noisy
  labels.
\newblock In {\em International Conference on Machine Learning}, pages
  1062--1070. PMLR, 2019.

\bibitem{chen2014stochastic}
Tianqi Chen, Emily Fox, and Carlos Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In {\em International Conference on Machine Learning}, pages
  1683--1691. PMLR, 2014.

\bibitem{coleman2019selection}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
  Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock {\em International Conference on Learning Representations}, 2020.

\bibitem{cui2019class}
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
\newblock Class-balanced loss based on effective number of samples.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 9268--9277, 2019.

\bibitem{daxberger2021laplace}
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
  Matthias Bauer, and Philipp Hennig.
\newblock Laplace redux-effortless bayesian deep learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:20089--20103, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{deng2023bayesadapter}
Zhijie Deng and Jun Zhu.
\newblock Bayesadapter: Being bayesian, inexpensively and reliably, via
  bayesian fine-tuning.
\newblock In {\em Asian Conference on Machine Learning}, pages 280--295. PMLR,
  2023.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{gupta2018matrix}
Arjun~K Gupta and Daya~K Nagar.
\newblock {\em Matrix variate distributions}.
\newblock Chapman and Hall/CRC, 2018.

\bibitem{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16000--16009, 2022.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hernandez2015probabilistic}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato and Ryan Adams.
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1869, 2015.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. pmlr, 2015.

\bibitem{jiang2019accelerating}
Angela~H Jiang, Daniel L-K Wong, Giulio Zhou, David~G Andersen, Jeffrey Dean,
  Gregory~R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary~C
  Lipton, et~al.
\newblock Accelerating deep learning by focusing on the biggest losers.
\newblock {\em arXiv preprint arXiv:1910.00762}, 2019.

\bibitem{jiang2018mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In {\em International Conference on Machine Learning}, pages
  2304--2313. PMLR, 2018.

\bibitem{katharopoulos2018not}
Angelos Katharopoulos and Fran{\c{c}}ois Fleuret.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In {\em International conference on machine learning}, pages
  2525--2534. PMLR, 2018.

\bibitem{kawaguchi2020ordered}
Kenji Kawaguchi and Haihao Lu.
\newblock Ordered sgd: A new stochastic optimization framework for empirical
  risk minimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 669--679. PMLR, 2020.

\bibitem{khan2018fast}
Mohammad~Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and
  Akash Srivastava.
\newblock Fast and scalable {B}ayesian deep learning by weight-perturbation in
  adam.
\newblock In {\em International Conference on Machine Learning}, pages
  2616--2625, 2018.

\bibitem{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock {\em arXiv preprint arXiv:2304.02643}, 2023.

\bibitem{kristiadi2020being}
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
\newblock Being bayesian, even just a bit, fixes overconfidence in relu
  networks.
\newblock In {\em International conference on machine learning}, pages
  5436--5446. PMLR, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{li2017webvision}
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van~Gool.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock {\em arXiv preprint arXiv:1708.02862}, 2017.

\bibitem{loshchilov2015online}
Ilya Loshchilov and Frank Hutter.
\newblock Online batch selection for faster training of neural networks.
\newblock {\em arXiv preprint arXiv:1511.06343}, 2015.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{louizos2016structured}
Christos Louizos and Max Welling.
\newblock Structured and efficient variational deep learning with matrix
  gaussian posteriors.
\newblock In {\em International Conference on Machine Learning}, pages
  1708--1716, 2016.

\bibitem{mackay1992bayesian}
David John~Cameron Mackay.
\newblock {\em Bayesian methods for adaptive models}.
\newblock California Institute of Technology, 1992.

\bibitem{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In {\em International conference on machine learning}, pages
  2408--2417. PMLR, 2015.

\bibitem{mindermann2022prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas
  Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot,
  Sebastian Farquhar, et~al.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learnt.
\newblock In {\em International Conference on Machine Learning}, pages
  15630--15649. PMLR, 2022.

\bibitem{OMS}
OpenAI.
\newblock Introducing chatgpt, 2022.
\newblock \url{https://openai.com/blog/chatgpt}, Last accessed on 2023-05-09.

\bibitem{paul2021deep}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock {\em Advances in Neural Information Processing Systems},
  34:20596--20607, 2021.

\bibitem{pleiss2020identifying}
Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kilian~Q Weinberger.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock {\em Advances in Neural Information Processing Systems},
  33:17044--17056, 2020.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{ritter2018online}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock Online structured laplace approximations for overcoming catastrophic
  forgetting.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{ritter2018scalable}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In {\em 6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, volume~6. International Conference
  on Representation Learning, 2018.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{smith2018understanding}
Lewis Smith and Yarin Gal.
\newblock Understanding measures of uncertainty for adversarial example
  detection.
\newblock {\em arXiv preprint arXiv:1803.08533}, 2018.

\bibitem{tonevaempirical}
Mariya Toneva, Alessandro Sordoni, Remi~Tachet des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em International Conference on Machine Learning}, pages
  681--688, 2011.

\bibitem{xia2022moderate}
Xiaobo Xia, Jiale Liu, Jun Yu, Xu~Shen, Bo~Han, and Tongliang Liu.
\newblock Moderate coreset: A universal method of data selection for real-world
  data-efficient deep learning.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem{zhang2018noisy}
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse.
\newblock Noisy natural gradient as variational inference.
\newblock In {\em International Conference on Machine Learning}, pages
  5847--5856, 2018.

\bibitem{zhang2019cyclical}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock {\em arXiv preprint arXiv:1902.03932}, 2019.

\bibitem{zheng2022coverage}
Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash.
\newblock Coverage-centric coreset selection for high pruning rates.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\end{thebibliography}
