\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
  Khirirat, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5973--5983, 2018.

\bibitem[Arjevani et~al.(2020)Arjevani, Bruna, Can, G{\"u}rb{\"u}zbalaban,
  Jegelka, and Lin]{arjevani2020ideal}
Yossi Arjevani, Joan Bruna, Bugra Can, Mert G{\"u}rb{\"u}zbalaban, Stefanie
  Jegelka, and Hongzhou Lin.
\newblock Ideal: Inexact decentralized accelerated augmented lagrangian method.
\newblock \emph{arXiv preprint arXiv:2006.06733}, 2020.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  344--353. PMLR, 2019.

\bibitem[Balu et~al.(2021)Balu, Jiang, Tan, Hedge, Lee, and
  Sarkar]{balu2021decentralized}
Aditya Balu, Zhanhong Jiang, Sin~Yong Tan, Chinmay Hedge, Young~M Lee, and
  Soumik Sarkar.
\newblock Decentralized deep learning using momentum-accelerated consensus.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3675--3679. IEEE, 2021.

\bibitem[Boyd et~al.(2006)Boyd, Ghosh, Prabhakar, and Shah]{boyd2006randomized}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Randomized gossip algorithms.
\newblock \emph{IEEE transactions on information theory}, 52\penalty0
  (6):\penalty0 2508--2530, 2006.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{The Journal of Machine Learning Research}, 13:\penalty0
  165--202, 2012.

\bibitem[Haghighat et~al.(2020)Haghighat, Ravichandra-Mouli, Chakraborty,
  Esfandiari, Arabi, and Sharma]{haghighat2020applications}
Arya~Ketabchi Haghighat, Varsha Ravichandra-Mouli, Pranamesh Chakraborty,
  Yasaman Esfandiari, Saeed Arabi, and Anuj Sharma.
\newblock Applications of deep learning in intelligent transportation systems.
\newblock \emph{Journal of Big Data Analytics in Transportation}, 2\penalty0
  (2):\penalty0 115--145, 2020.

\bibitem[Hsieh et~al.(2019)Hsieh, Phanishayee, Mutlu, and
  Gibbons]{hsieh2019non}
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip~B Gibbons.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock \emph{arXiv preprint arXiv:1910.00189}, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Balu, Hegde, and
  Sarkar]{jiang2017collaborative}
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar.
\newblock Collaborative deep learning in fixed topology networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  2017:\penalty0 5905--5915, 2017.

\bibitem[Jiang et~al.(2018)Jiang, Balu, Hegde, and Sarkar]{jiang2018consensus}
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar.
\newblock On consensus-optimality trade-offs in collaborative deep learning.
\newblock \emph{arXiv preprint arXiv:1805.12120}, 2018.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Karimireddy et~al.(2019{\natexlab{a}})Karimireddy, Kale, Mohri, Reddi,
  Stich, and Suresh]{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J Reddi,
  Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019{\natexlab{a}}.

\bibitem[Karimireddy et~al.(2019{\natexlab{b}})Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock \emph{arXiv preprint arXiv:1901.09847}, 2019{\natexlab{b}}.

\bibitem[Kempe et~al.(2003)Kempe, Dobra, and Gehrke]{kempe2003gossip}
David Kempe, Alin Dobra, and Johannes Gehrke.
\newblock Gossip-based computation of aggregate information.
\newblock In \emph{44th Annual IEEE Symposium on Foundations of Computer
  Science, 2003. Proceedings.}, pages 482--491. IEEE, 2003.

\bibitem[Koloskova et~al.(2019)Koloskova, Lin, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Tao Lin, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock \emph{arXiv preprint arXiv:1907.09356}, 2019.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock \emph{arXiv preprint arXiv:2003.10422}, 2020.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Li, Wang, Li, Zhou, Guo, and
  Li]{li2019gradient}
Chengjie Li, Ruixuan Li, Haozhao Wang, Yuhua Li, Pan Zhou, Song Guo, and Keqin
  Li.
\newblock Gradient scheduling with global momentum for non-iid data distributed
  asynchronous training.
\newblock \emph{arXiv preprint arXiv:1902.07848}, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2018)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2018federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 2018.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Huang, Yang, Wang, and
  Zhang]{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock \emph{arXiv preprint arXiv:1907.02189}, 2019{\natexlab{b}}.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem[Lu and De~Sa(2020)]{lu2020moniqua}
Yucheng Lu and Christopher De~Sa.
\newblock Moniqua: Modulo quantized communication in decentralized sgd.
\newblock \emph{arXiv preprint arXiv:2002.11787}, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Nadiradze et~al.(2019)Nadiradze, Sabour, Alistarh, Sharma, Markov, and
  Aksenov]{nadiradze2019swarmsgd}
Giorgi Nadiradze, Amirmojtaba Sabour, Dan Alistarh, Aditya Sharma, Ilia Markov,
  and Vitaly Aksenov.
\newblock {SwarmSGD}: Scalable decentralized {SGD} with local updates.
\newblock \emph{arXiv preprint arXiv:1910.12308}, 2019.

\bibitem[Nedi{\'c} et~al.(2018)Nedi{\'c}, Olshevsky, and
  Rabbat]{nedic2018network}
Angelia Nedi{\'c}, Alex Olshevsky, and Michael~G Rabbat.
\newblock Network topology and communication-computation tradeoffs in
  decentralized optimization.
\newblock \emph{Proceedings of the IEEE}, 106\penalty0 (5):\penalty0 953--976,
  2018.

\bibitem[Nesterov(2012)]{nesterov2012efficiency}
Yu~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Richt{\'a}rik and Tak{\'a}{\v{c}}(2016)]{richtarik2016distributed}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Distributed coordinate descent method for learning with big data.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2657--2681, 2016.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica,
  Braverman, Gonzalez, and Arora]{rothchild2020fetchsgd}
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
  Vladimir Braverman, Joseph Gonzalez, and Raman Arora.
\newblock Fetchsgd: Communication-efficient federated learning with sketching.
\newblock \emph{arXiv preprint arXiv:2007.07682}, 2020.

\bibitem[Sattler et~al.(2019)Sattler, Wiedemann, M{\"u}ller, and
  Samek]{sattler2019robust}
Felix Sattler, Simon Wiedemann, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Robust and communication-efficient federated learning from non-iid
  data.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  2019.

\bibitem[Scaman et~al.(2018)Scaman, Bach, Bubeck, Massouli{\'e}, and
  Lee]{scaman2018optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Laurent Massouli{\'e}, and
  Yin~Tat Lee.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2740--2749, 2018.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Shoham et~al.(2019)Shoham, Avidor, Keren, Israel, Benditkis,
  Mor-Yosef, and Zeitak]{shoham2019overcoming}
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron
  Mor-Yosef, and Itai Zeitak.
\newblock Overcoming forgetting in federated learning on non-iid data.
\newblock \emph{arXiv preprint arXiv:1910.07796}, 2019.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Tang et~al.(2019)Tang, Lian, Qiu, Yuan, Zhang, Zhang, and
  Liu]{DBLP:journals/corr/abs-1907-07346}
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce~Zhang, Tong Zhang, and
  Ji~Liu.
\newblock Deepsqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock \emph{CoRR}, abs/1907.07346, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.07346}.

\bibitem[Tong et~al.(2020)Tong, Liang, and Bi]{tong2020effective}
Qianqian Tong, Guannan Liang, and Jinbo Bi.
\newblock Effective federated adaptive gradient methods with non-iid
  decentralized data.
\newblock \emph{arXiv preprint arXiv:2009.06557}, 2020.

\bibitem[Vogels et~al.(2020)Vogels, Karimireddy, and
  Jaggi]{vogels2020practical}
Thijs Vogels, Sai~Praneeth Karimireddy, and Martin Jaggi.
\newblock Practical low-rank communication compression in decentralized deep
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Wang et~al.(2019)Wang, Tantia, Ballas, and Rabbat]{wang2019slowmo}
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat.
\newblock Slowmo: Improving communication-efficient distributed sgd with slow
  momentum.
\newblock \emph{arXiv preprint arXiv:1910.00643}, 2019.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{arXiv preprint arXiv:2007.07481}, 2020.

\bibitem[Xiao and Boyd(2004)]{xiao2004fast}
Lin Xiao and Stephen Boyd.
\newblock Fast linear iterations for distributed averaging.
\newblock \emph{Systems \& Control Letters}, 53\penalty0 (1):\penalty0 65--78,
  2004.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1905.03817}, 2019.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and
  Chandra]{zhao2018federated}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\end{thebibliography}
