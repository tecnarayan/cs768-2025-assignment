% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{International conference
  on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  8748--8763.

\bibitem{li2022grounded}
L.~H. Li, P.~Zhang, H.~Zhang, J.~Yang, C.~Li, Y.~Zhong, L.~Wang, L.~Yuan,
  L.~Zhang, J.-N. Hwang \emph{et~al.}, ``Grounded language-image
  pre-training,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, 2022, pp. 10\,965--10\,975.

\bibitem{zhong2022regionclip}
Y.~Zhong, J.~Yang, P.~Zhang, C.~Li, N.~Codella, L.~H. Li, L.~Zhou, X.~Dai,
  L.~Yuan, Y.~Li \emph{et~al.}, ``Regionclip: Region-based language-image
  pretraining,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, 2022, pp. 16\,793--16\,803.

\bibitem{yao2021filip}
L.~Yao, R.~Huang, L.~Hou, G.~Lu, M.~Niu, H.~Xu, X.~Liang, Z.~Li, X.~Jiang, and
  C.~Xu, ``Filip: Fine-grained interactive language-image pre-training,'' in
  \emph{International Conference on Learning Representations}, 2021.

\bibitem{cherti2023reproducible}
M.~Cherti, R.~Beaumont, R.~Wightman, M.~Wortsman, G.~Ilharco, C.~Gordon,
  C.~Schuhmann, L.~Schmidt, and J.~Jitsev, ``Reproducible scaling laws for
  contrastive language-image learning,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2023, pp. 2818--2829.

\bibitem{zhao2023clip}
Z.~Zhao, Y.~Liu, H.~Wu, Y.~Li, S.~Wang, L.~Teng, D.~Liu, X.~Li, Z.~Cui, Q.~Wang
  \emph{et~al.}, ``Clip in medical imaging: A comprehensive survey,''
  \emph{arXiv preprint arXiv:2312.07353}, 2023.

\bibitem{chen2023fine}
W.~Chen, X.~Li, L.~Shen, and Y.~Yuan, ``Fine-grained image-text alignment in
  medical imaging enables cyclic image-report generation,'' \emph{arXiv
  preprint arXiv:2312.08078}, 2023.

\bibitem{zhang2023multi}
K.~Zhang, Y.~Yang, J.~Yu, H.~Jiang, J.~Fan, Q.~Huang, and W.~Han, ``Multi-task
  paired masking with alignment modeling for medical vision-language
  pre-training,'' \emph{IEEE Transactions on Multimedia}, 2023.

\bibitem{wang2022medclip}
Z.~Wang, Z.~Wu, D.~Agarwal, and J.~Sun, ``Medclip: Contrastive learning from
  unpaired medical images and text,'' \emph{arXiv preprint arXiv:2210.10163},
  2022.

\bibitem{huang2021gloria}
S.-C. Huang, L.~Shen, M.~P. Lungren, and S.~Yeung, ``Gloria: A multimodal
  global-local representation learning framework for label-efficient medical
  image recognition,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 3942--3951.

\bibitem{wang2022multi}
F.~Wang, Y.~Zhou, S.~Wang, V.~Vardhanabhuti, and L.~Yu, ``Multi-granularity
  cross-modal alignment for generalized medical visual representation
  learning,'' \emph{arXiv preprint arXiv:2210.06044}, 2022.

\bibitem{RobertGeirhos2020ShortcutLI}
R.~Geirhos, J.-H. Jacobsen, C.~Michaelis, R.~S. Zemel, W.~Brendel, M.~Bethge,
  and F.~A. Wichmann, ``Shortcut learning in deep neural networks,''
  \emph{Nature Machine Intelligence}, vol.~2, no.~11, pp. 665--673, 2020.

\bibitem{ma2023rectify}
C.~Ma, L.~Zhao, Y.~Chen, L.~Guo, T.~Zhang, X.~Hu, D.~Shen, X.~Jiang, and
  T.~Liu, ``Rectify vit shortcut learning by visual saliency,'' \emph{IEEE
  Transactions on Neural Networks and Learning Systems}, 2023.

\bibitem{Ma2023}
C.~Ma \emph{et~al.}, ``Eye-gaze-guided vision transformer for rectifying
  shortcut learning,'' \emph{IEEE Transactions on Medical Imaging}, vol.~42,
  no.~11, pp. 3384--3394, Nov 2023.

\bibitem{Drew2013}
T.~Drew, K.~Evans, M.~L.-H. VÃµ, F.~L. Jacobson, and J.~M. Wolfe, ``Informatics
  in radiology: What can you see in a single glance and how might this guide
  visual search in medical images?'' \emph{RadioGraphics}, vol.~33, no.~1, pp.
  263--274, Jan 2013.

\bibitem{Khosravan2016}
N.~Khosravan \emph{et~al.}, ``Gaze2segment: A pilot study for integrating
  eye-tracking technology into medical image segmentation,'' in \emph{Medical
  Computer Vision and Bayesian and Graphical Models for Biomedical
  Imaging}.\hskip 1em plus 0.5em minus 0.4em\relax Cham, Switzerland: Springer,
  2016, pp. 94--104.

\bibitem{Wang2022}
S.~Wang, X.~Ouyang, T.~Liu, Q.~Wang, and D.~Shen, ``Follow my eye: Using gaze
  to supervise computer-aided diagnosis,'' \emph{IEEE Trans. Med. Imag.},
  vol.~41, no.~7, pp. 1688--1698, Jul 2022.

\bibitem{khosravan2019collaborative}
N.~Khosravan, H.~Celik, B.~Turkbey, E.~C. Jones, B.~Wood, and U.~Bagci, ``A
  collaborative computer aided diagnosis (c-cad) system with eye-tracking,
  sparse attentional model, and deep learning,'' \emph{Medical image analysis},
  vol.~51, pp. 101--115, 2019.

\bibitem{karargyris2020eye}
A.~Karargyris, S.~Kashyap, I.~Lourentzou, J.~Wu, M.~Tong, A.~Sharma, S.~Abedin,
  D.~Beymer, V.~Mukherjee, E.~Krupinski \emph{et~al.}, ``Eye gaze data for
  chest x-rays,'' \emph{PhysioNet https://doi. org/10.13026/QFDZ-ZR67}, 2020.

\bibitem{lanfredi2021reflacx}
R.~B. Lanfredi, M.~Zhang, W.~Auffermann, J.~Chan, P.-A. Duong, V.~Srikumar,
  T.~Drew, J.~Schroeder, and T.~Tasdizen, ``Reflacx: Reports and eye-tracking
  data for localization of abnormalities in chest x-rays,'' 2021.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{lu2019vilbert}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee, ``Vilbert: Pretraining task-agnostic
  visiolinguistic representations for vision-and-language tasks,'' in
  \emph{Advances in Neural Information Processing Systems}, 2019, pp. 13--23.

\bibitem{tan2019lxmert}
H.~Tan and M.~Bansal, ``Lxmert: Learning cross-modality encoder representations
  from transformers,'' in \emph{Proceedings of the 2019 Conference on Empirical
  Methods in Natural Language Processing and the 9th International Joint
  Conference on Natural Language Processing (EMNLP-IJCNLP)}.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2019.

\bibitem{zhang2022contrastive}
Y.~Zhang, H.~Jiang, Y.~Miura, C.~D. Manning, and C.~P. Langlotz, ``Contrastive
  learning of medical visual representations from paired images and text,'' in
  \emph{Machine Learning for Healthcare Conference}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2022, pp. 2--25.

\bibitem{boecking2022making}
B.~Boecking, N.~Usuyama, S.~Bannur, D.~C. Castro, A.~Schwaighofer, S.~Hyland,
  M.~Wetscherek, T.~Naumann, A.~Nori, J.~Alvarez-Valle \emph{et~al.}, ``Making
  the most of text semantics to improve biomedical vision--language
  processing,'' in \emph{Computer Vision--ECCV 2022: 17th European Conference,
  Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXVI}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2022, pp. 1--21.

\bibitem{wu2023medklip}
C.~Wu, X.~Zhang, Y.~Zhang, Y.~Wang, and W.~Xie, ``Medklip: Medical knowledge
  enhanced language-image pre-training,'' \emph{medRxiv}, pp. 2023--01, 2023.

\bibitem{Krupinski2010}
E.~A. Krupinski, ``Current perspectives in medical image perception,''
  \emph{Attention, Perception, \& Psychophysics}, vol.~72, no.~5, pp.
  1205--1217, Jul 2010.

\bibitem{Kundel2007}
H.~L. Kundel, C.~F. Nodine, E.~F. Conant, and S.~P. Weinstein, ``Holistic
  component of image perception in mammogram interpretation: Gaze-tracking
  study,'' \emph{Radiology}, vol. 242, no.~2, pp. 396--402, Feb 2007.

\bibitem{Kok2017}
E.~M. Kok and H.~Jarodzka, ``Before your very eyes: The value and limitations
  of eye tracking in medical education,'' \emph{Med. Educ.}, vol.~51, no.~1,
  pp. 114--122, Jan 2017.

\bibitem{Mall2019}
S.~Mall, E.~A. Krupinski, and C.~R. Mello-Thoms, ``Missed cancer and visual
  search of mammograms: What feature based machine-learning can tell us that
  deep-convolution learning cannot,'' in \emph{Proc. SPIE}, Mar 2019, pp.
  281--287.

\bibitem{Karargyris2021}
A.~Karargyris \emph{et~al.}, ``Creation and validation of a chest x-ray dataset
  with eye-tracking and report dictation for ai development,'' \emph{Sci.
  Data}, vol.~8, no.~1, pp. 1--18, Mar 2021.

\bibitem{men2023gaze}
Q.~Men, C.~Teng, L.~Drukker \emph{et~al.}, ``Gaze-probe joint guidance with
  multi-task learning in obstetric ultrasound scanning,'' \emph{Medical Image
  Analysis}, vol.~90, p. 102981, 2023.

\bibitem{hsieh2023mimic}
C.~Hsieh, C.~Ouyang, J.~C. Nascimento, J.~Pereira, J.~Jorge, and C.~Moreira,
  ``Mimic-eye: Integrating mimic datasets with reflacx and eye gaze for
  multimodal deep learning applications,'' \emph{PhysioNet (version 1.0. 0)},
  2023.

\bibitem{johnson13026mimic}
A.~Johnson, T.~Pollard, R.~Mark, S.~Berkowitz, and S.~Horng, ``Mimic-cxr
  database,'' \emph{PhysioNet10}, vol. 13026, p. C2JT1Q, 2019.

\bibitem{johnson2019mimic}
A.~E. Johnson, T.~J. Pollard, N.~R. Greenbaum, M.~P. Lungren, C.-y. Deng,
  Y.~Peng, Z.~Lu, R.~G. Mark, S.~J. Berkowitz, and S.~Horng, ``Mimic-cxr-jpg, a
  large publicly available database of labeled chest radiographs,'' \emph{arXiv
  preprint arXiv:1901.07042}, 2019.

\bibitem{johnson2020mimic}
A.~Johnson, L.~Bulgarelli, T.~Pollard, S.~Horng, L.~A. Celi, and R.~Mark,
  ``Mimic-iv,'' \emph{PhysioNet. Available online at: https://physionet.
  org/content/mimiciv/1.0/(accessed August 23, 2021)}, 2020.

\bibitem{johnson2021mimic}
A.~Johnson, L.~Bulgarelli, T.~Pollard, L.~A. Celi, R.~Mark, and S.~Horng,
  ``Mimic-iv-ed (version 1.0),'' \emph{PhysioNet}, 2021.

\bibitem{goldberger2000physiobank}
A.~L. Goldberger, L.~A. Amaral, L.~Glass, J.~M. Hausdorff, P.~C. Ivanov, R.~G.
  Mark, J.~E. Mietus, G.~B. Moody, C.-K. Peng, and H.~E. Stanley, ``Physiobank,
  physiotoolkit, and physionet: components of a new research resource for
  complex physiologic signals,'' \emph{circulation}, vol. 101, no.~23, pp.
  e215--e220, 2000.

\bibitem{irvin2019chexpert}
J.~Irvin, P.~Rajpurkar, M.~Ko, Y.~Yu, S.~Ciurea-Ilcus, C.~Chute, H.~Marklund,
  B.~Haghgoo, R.~Ball, K.~Shpanskaya \emph{et~al.}, ``Chexpert: A large chest
  radiograph dataset with uncertainty labels and expert comparison,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~33, 2019, pp. 590--597.

\bibitem{shih2019rsna}
G.~Shih, C.~C. Wu, S.~S. Halabi, M.~D. Kohli, L.~M. Prevedello, T.~S. Cook,
  A.~Sharma, J.~K. Amorosa, V.~Arteaga, M.~Galperin-Aizenberg \emph{et~al.},
  ``Augmenting the national institutes of health chest radiograph dataset with
  expert annotations of possible pneumonia,'' \emph{Radiology: Artificial
  Intelligence}, vol.~1, no.~1, 2019.

\bibitem{SIIM-ACR}
``{SIIM-ACR} pneumothorax segmentation,'' 2020, [online] Available:
  \url{https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation}.

\bibitem{KhaledSaab2021ObservationalSF}
K.~Saab, S.~M. Hooper, N.~S. Sohoni, J.~Parmar, B.~P. Pogatchnik, S.~Wu,
  J.~Dunnmon, H.~Zhang, D.~L. Rubin, and C.~R{\'e}, ``Observational supervision
  for medical image classification using gaze data.'' in \emph{Medical Image
  Computing and Computer-Assisted Intervention}, 2021.

\bibitem{PRIOR}
P.~Cheng, L.~Lin, J.~Lyu, Y.~Huang, W.~Luo, and X.~Tang, ``Prior: Prototype
  representation joint learning from medical images and reports,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 2023.

\bibitem{van2008visualizing}
L.~Van~der Maaten and G.~Hinton, ``Visualizing data using t-sne.''
  \emph{Journal of machine learning research}, vol.~9, no.~11, 2008.

\bibitem{kroger2020does}
J.~L. Kr{\"o}ger, O.~H.-M. Lutz, and F.~M{\"u}ller, ``What does your gaze
  reveal about you? on the privacy implications of eye tracking,'' in
  \emph{IFIP International Summer School on Privacy and Identity
  Management}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp.
  226--241.

\bibitem{katsini2020role}
C.~Katsini, Y.~Abdrabou, G.~E. Raptis, M.~Khamis, and F.~Alt, ``The role of eye
  gaze in security and privacy applications: Survey and future hci research
  directions,'' in \emph{Proceedings of the 2020 CHI Conference on Human
  Factors in Computing Systems}, 2020, pp. 1--21.

\bibitem{ZeLiu2021SwinTH}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``{Swin
  Transformer}: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, pp. 10\,012--10\,022.

\bibitem{alsentzer2019publicly}
E.~Alsentzer, J.~R. Murphy, W.~Boag, W.-H. Weng, D.~Jin, T.~Naumann, and
  M.~McDermott, ``Publicly available clinical bert embeddings,'' \emph{arXiv
  preprint arXiv:1904.03323}, 2019.

\bibitem{hansell2008fleischner}
D.~M. Hansell, A.~A. Bankier, H.~MacMahon, T.~C. McLoud, N.~L. Muller, and
  J.~Remy, ``Fleischner society: glossary of terms for thoracic imaging,''
  \emph{Radiology}, vol. 246, no.~3, pp. 697--722, 2008.

\end{thebibliography}
