@article{orwig_recent_2015,
	title = {Recent Trends in Variable Generation Forecasting and Its Value to the Power System},
	volume = {6},
	issn = {1949-3037},
	doi = {10.1109/TSTE.2014.2366118},
	abstract = {The rapid deployment of wind and solar energy generation systems has resulted in a need to better understand, predict, and manage variable generation. The uncertainty around wind and solar power forecasts is still viewed by the power industry as being quite high, and many barriers to forecast adoption by power system operators still remain. In response, the U.S. Department of Energy has sponsored, in partnership with the National Oceanic and Atmospheric Administration, public, private, and academic organizations, two projects to advance wind and solar power forecasts. Additionally, several utilities and grid operators have recognized the value of adopting variable generation forecasting and have taken great strides to enhance their usage of forecasting. In parallel, power system markets and operations are evolving to integrate greater amounts of variable generation. This paper will discuss the recent trends in wind and solar power forecasting technologies in the U.S., the role of forecasting in an evolving power system framework, and the benefits to intended forecast users.},
	pages = {924--933},
	number = {3},
	journal = {{IEEE} Transactions on Sustainable Energy},
	author = {Orwig, Kirsten D. and Ahlstrom, Mark L. and Banunarayanan, Venkat and Sharp, Justin and Wilczak, James M. and Freedman, Jeffrey and Haupt, Sue Ellen and Cline, Joel and Bartholomy, Obadiah and Hamann, Hendrik F. and Hodge, Bri-Mathias and Finley, Catherine and Nakafuji, Dora and Peterson, Jack L. and Maggio, David and Marquis, Melinda},
	date = {2015-07},
	note = {Conference Name: {IEEE} Transactions on Sustainable Energy},
	keywords = {Predictive models, Forecasting, Wind power generation, Accuracy, large-scale integration, market design, Measurement, power-system reliability, renewable energy, solar energy, variable generation, wind energy, Wind forecasting},
	file = {functional_relation_field_a_mo.pdf:/Users/pemami/Zotero/storage/MN3KE7RZ/functional_relation_field_a_mo.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/WW8B2YAB/6996049.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/CJY57WN6/Orwig et al. - 2015 - Recent Trends in Variable Generation Forecasting a.pdf:application/pdf},
}

@Article{zhang_baseline_2015,
  author       = {Zhang, Jie and Hodge, Bri-Mathias and Lu, Siyuan and Hamann, Hendrik F. and Lehman, Brad and Simmons, Joseph and Campos, Edwin and Banunarayanan, Venkat and Black, Jon and Tedesco, John},
  title        = {Baseline and target values for regional and point {PV} power forecasts: Toward improved solar forecasting},
  doi          = {10.1016/j.solener.2015.09.047},
  pages        = {804--819},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0038092X1500540X},
  volume       = {122},
  abstract     = {Accurate solar photovoltaic ({PV}) power forecasting allows utilities to reliably utilize solar resources on their systems. However, to truly measure the improvements that any new solar forecasting methods provide, it is important to develop a methodology for determining baseline and target values for the accuracy of solar forecasting at different spatial and temporal scales. This paper aims at developing a framework to derive baseline and target values for a suite of generally applicable, value-based, and custom-designed solar forecasting metrics. The work was informed by close collaboration with utility and independent system operator partners. The baseline values are established based on state-of-the-art numerical weather prediction models and persistence models in combination with a radiative transfer model. The target values are determined based on the reduction in the amount of reserves that must be held to accommodate the uncertainty of {PV} power output. The proposed reserve-based methodology is a reasonable and practical approach that can be used to assess the economic benefits gained from improvements in accuracy of solar forecasting. The financial baseline and targets can be translated back to forecasting accuracy metrics and requirements, which will guide research on solar forecasting improvements towards the areas that are most beneficial to power systems operations.},
  file         = {Zhang et al. - 2015 - Baseline and target values for regional and point .pdf:/Users/pemami/Zotero/storage/X5GFGAPZ/Zhang et al. - 2015 - Baseline and target values for regional and point .pdf:application/pdf},
  issn         = {0038092X},
  journal      = {Solar Energy},
  langid       = {english},
  month        = dec,
  shortjournal = {Solar Energy},
  shorttitle   = {Baseline and target values for regional and point {PV} power forecasts},
  urldate      = {2022-01-25},
  year         = {2015},
}

@article{chen_short-term_2019,
	title = {Short-Term Load Forecasting With Deep Residual Networks},
	volume = {10},
	issn = {1949-3061},
	doi = {10.1109/TSG.2018.2844307},
	abstract = {We present in this paper a model for forecasting short-term electric load based on deep residual networks. The proposed model is able to integrate domain knowledge and researchers' understanding of the task by virtue of different neural network building blocks. Specifically, a modified deep residual network is formulated to improve the forecast results. Further, a two-stage ensemble strategy is used to enhance the generalization capability of the proposed model. We also apply the proposed model to probabilistic load forecasting using Monte Carlo dropout. Three public datasets are used to prove the effectiveness of the proposed model. Multiple test cases and comparison with existing models show that the proposed model provides accurate load forecasting results and has high generalization capability.},
	pages = {3943--3952},
	number = {4},
	journal = {{IEEE} Transactions on Smart Grid},
	author = {Chen, Kunjin and Chen, Kunlong and Wang, Qin and He, Ziyu and Hu, Jun and He, Jinliang},
	date = {2019-07},
	note = {Conference Name: {IEEE} Transactions on Smart Grid},
	keywords = {Predictive models, Neural networks, Task analysis, Load modeling, deep learning, Forecasting, deep residual network, Load forecasting, probabilistic load forecasting, Probabilistic logic, Short-term load forecasting},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/JYW54KYP/8372953.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/WHIW8BJD/Chen et al. - 2019 - Short-Term Load Forecasting With Deep Residual Net.pdf:application/pdf},
}

@article{ahmad_generic_2019,
	title = {A generic data-driven technique for forecasting of reservoir inflow: Application for hydropower maximization},
	volume = {119},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815219302373},
	doi = {10.1016/j.envsoft.2019.06.008},
	shorttitle = {A generic data-driven technique for forecasting of reservoir inflow},
	abstract = {A generic and scalable scheme is proposed for forecasting reservoir inﬂow to optimize reservoir operations for hydropower maximization. Short-term weather forecasts and antecedent hydrological variables were inputs to a three-layered hydrologically-relevant Artiﬁcial Neural Network ({ANN}) to forecast inﬂow for 7-days of lead-time. Application of the scheme was demonstrated over 23 dams in U.S. with varying hydrological characteristics and climate regimes. Probabilistic forecast was also explored by feeding {ANN} with ensembles of weather forecast ﬁelds. Results suggest forecasting skill improves with decreasing coeﬃcient of variation in inﬂow and increasing drainage area. Forecast-informed operations were simulated using a rolling horizon scheme and assessed against benchmark control rules. Over two years of operations from Pensacola dam (Oklahoma), additional 47,253 {MWh} of energy could have been harvested without compromising ﬂood risk with optimal operations. This study reinforces the potential of a numerically eﬃcient and skillful reservoir inﬂow forecasting scheme to address water-energy security challenges.},
	pages = {147--165},
	journal = {Environmental Modelling \& Software},
	shortjournal = {Environmental Modelling \& Software},
	author = {Ahmad, Shahryar Khalique and Hossain, Faisal},
	urldate = {2022-01-25},
	date = {2019-09},
	langid = {english},
	file = {Ahmad and Hossain - 2019 - A generic data-driven technique for forecasting of.pdf:/Users/pemami/Zotero/storage/8JQF36VA/Ahmad and Hossain - 2019 - A generic data-driven technique for forecasting of.pdf:application/pdf},
}

@article{bendaoud_comparing_2021,
	title = {Comparing Generative Adversarial Networks architectures for electricity demand forecasting},
	volume = {247},
	issn = {03787788},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378778821004369},
	doi = {10.1016/j.enbuild.2021.111152},
	abstract = {This paper introduces short-term load forecasting ({STLF}) using Generative Adversarial Networks ({GAN}). {STLF} was explored using several Artiﬁcial Intelligence based methods that offered excellent results. However, the usage of {GAN} models in this ﬁeld is very limited, and usually works on creating synthetic load proﬁles to increase load datasets. This paper investigates the application of {GAN} for load forecasting by generating daily load proﬁles. Predicting the daily load is a challenging task that requires accurate and stable models that can capture seasonality and variation in load data. This paper presents a conditional {GAN} ({cGAN}) architecture, that uses only four exogenous variables (maximum and minimum temperature, day of the week and month), to predict a daily proﬁle (24 h of the day). Several types of {GAN} have been compared such as Deep Convolutional {GAN}, Least Squares {GAN} and Wasserstein {GAN}. The generated load proﬁles were tested on one year of data and compared to the real load proﬁles. The proposed {GAN} models provided excellent predictions, averaging a Mean Absolute Percentage Error ({MAPE}) of 4.99\%.},
	pages = {111152},
	journal = {Energy and Buildings},
	shortjournal = {Energy and Buildings},
	author = {Bendaoud, Nadjib Mohamed Mehdi and Farah, Nadir and Ben Ahmed, Samir},
	urldate = {2022-01-25},
	date = {2021-09},
	langid = {english},
	file = {Bendaoud et al. - 2021 - Comparing Generative Adversarial Networks architec.pdf:/Users/pemami/Zotero/storage/4Z8H5YAS/Bendaoud et al. - 2021 - Comparing Generative Adversarial Networks architec.pdf:application/pdf},
}

@article{james_unified_2017,
	title = {A unified high-resolution wind and solar dataset from a rapidly updating numerical weather prediction model},
	volume = {102},
	issn = {09601481},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960148116309363},
	doi = {10.1016/j.renene.2016.10.059},
	abstract = {A new gridded dataset for wind and solar resource estimation over the contiguous United States has been derived from hourly updated 1-h forecasts from the National Oceanic and Atmospheric Administration High-Resolution Rapid Refresh ({HRRR}) 3-km model composited over a three-year period (approximately 22 000 forecast model runs). The unique dataset features hourly data assimilation, and provides physically consistent wind and solar estimates for the renewable energy industry. The wind resource dataset shows strong similarity to that previously provided by a Department of Energy-funded study, and it includes estimates in southern Canada and northern Mexico. The solar resource dataset represents an initial step towards application-speciﬁc ﬁelds such as global horizontal and direct normal irradiance. This combined dataset will continue to be augmented with new forecast data from the advanced {HRRR} atmospheric/land-surface model.},
	pages = {390--405},
	journal = {Renewable Energy},
	shortjournal = {Renewable Energy},
	author = {James, Eric P. and Benjamin, Stanley G. and Marquis, Melinda},
	urldate = {2022-01-25},
	date = {2017-03},
	langid = {english},
	file = {James et al. - 2017 - A unified high-resolution wind and solar dataset f.pdf:/Users/pemami/Zotero/storage/APHPSVG9/James et al. - 2017 - A unified high-resolution wind and solar dataset f.pdf:application/pdf},
}

@article{rodriguez-benitez_assessment_2021,
	title = {Assessment of new solar radiation nowcasting methods based on sky-camera and satellite imagery},
	volume = {292},
	issn = {03062619},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261921003354},
	doi = {10.1016/j.apenergy.2021.116838},
	abstract = {This work proposes and evaluates methods for extending the forecasting horizon of all-sky imager ({ASI})based solar radiation nowcasts and estimating the uncertainty of these predictions. In addition, we evaluated procedures for improving the temporal resolution and latency of satellite-imagery-derived solar nowcasts. Based on these contributions, we assessed the reliability of {ASIs} and satellite-derived solar radiation nowcasts, with 1-min time-resolution and up-to-90-min ahead. The study was conducted in a location in Southern Spain using a set of cloudy days, specifically selected as representative of the most challenging conditions regarding solar radiation nowcasting. The results reveal that the use of {ASI}-based models provide low benefits compared to the use of satellite-based models for point solar radiation nowcasting. Given the frequency of occurrence of the different sky types in the study area, the results suggest that the use of a simple smart persistence algorithm, in combination with a low-resolution satellite nowcasting model could be an adequate choice, avoiding the challenges associated with the use of {ASIs}.},
	pages = {116838},
	journal = {Applied Energy},
	shortjournal = {Applied Energy},
	author = {Rodríguez-Benítez, Francisco J. and López-Cuesta, Miguel and Arbizu-Barrena, Clara and Fernández-León, María M. and Pamos-Ureña, Miguel Á. and Tovar-Pescador, Joaquín and Santos-Alamillos, Francisco J. and Pozo-Vázquez, David},
	urldate = {2022-01-25},
	date = {2021-06},
	langid = {english},
	file = {Rodríguez-Benítez et al. - 2021 - Assessment of new solar radiation nowcasting metho.pdf:/Users/pemami/Zotero/storage/RAN2P9ER/Rodríguez-Benítez et al. - 2021 - Assessment of new solar radiation nowcasting metho.pdf:application/pdf},
}

@article{gao_pile_2020,
	title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
	url = {http://arxiv.org/abs/2101.00027},
	shorttitle = {The Pile},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 {GiB} English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of {GPT}-2 and {GPT}-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw {CC} and {CC}-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	journal = {{arXiv}:2101.00027 [cs]},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	urldate = {2022-01-26},
	date = {2020-12-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.00027},
	keywords = {Computer Science - Computation and Language},
	file = {Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:/Users/pemami/Zotero/storage/W8ZVTYDC/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf},
}

@article{birhane_multimodal_2021,
	title = {Multimodal datasets: misogyny, pornography, and malignant stereotypes},
	url = {http://arxiv.org/abs/2110.01963},
	shorttitle = {Multimodal datasets},
	abstract = {We have now entered the era of trillion parameter machine learning models trained on billion-sized datasets scraped from the internet. The rise of these gargantuan datasets has given rise to formidable bodies of critical work that has called for caution while generating these large datasets. These address concerns surrounding the dubious curation practices used to generate these datasets, the sordid quality of alt-text data available on the world wide web, the problematic content of the {CommonCrawl} dataset often used as a source for training large language models, and the entrenched biases in large-scale visio-linguistic models (such as {OpenAI}’s {CLIP} model) trained on opaque datasets ({WebImageText}). In the backdrop of these speciﬁc calls of caution, we examine the recently released {LAION}-400M dataset, which is a {CLIP}-ﬁltered dataset of Image-Alt-text pairs parsed from the Common-Crawl dataset. We found that the dataset contains, troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content. We outline numerous implications, concerns and downstream harms regarding the current state of large scale datasets while raising open questions for various stakeholders including the {AI} community, regulators, policy makers and data subjects.},
	journal = {{arXiv}:2110.01963 [cs]},
	author = {Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
	urldate = {2022-01-26},
	date = {2021-10-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.01963},
	keywords = {Computer Science - Computers and Society},
	file = {Birhane et al. - 2021 - Multimodal datasets misogyny, pornography, and ma.pdf:/Users/pemami/Zotero/storage/6NXI7DAA/Birhane et al. - 2021 - Multimodal datasets misogyny, pornography, and ma.pdf:application/pdf},
}

@inproceedings{herruzo_high-resolution_2021,
	title = {High-resolution multi-channel weather forecasting – First insights on transfer learning from the Weather4cast Competitions 2021},
	doi = {10.1109/BigData52589.2021.9672063},
	abstract = {Weather forecasting is both a high impact application as well as a complex Big Data modelling challenge. Recent advances in machine learning have already demonstrated the power of non-physical modelling approaches for the prediction of rainfall. The Weather4cast competitions now provide a unique multi-channel benchmark for the prediction of up to 8 hours of weather with high temporal and spatial resolutions (15 min, 4 km) for a diverse set of large regions across Earth. This diversity, for the first time, also permits a meaningful spatial transfer learning challenge in weather forecasting.Weather4cast introduces multi-channel weather ‘movies’ that encode temperature, rainfall, cloud properties, and turbulence as derived from the meteorological satellites by the {EUMETSAT} {NWC} {SAF}. Inspired by the Traffic4cast competitions at the {NeurIPS} conferences in 2019 and 2020, weather forecasting is thus presented as a video frame prediction task. As then, the U-Net based models developed for photographic image analysis intriguingly did well on these artificial videos. In contrast, however, the winning submission did not employ a U-Net but a recurrent convolutional network with residual units.Weather4cast introduces the first spatial transfer learning challenge in weather forecasting: only one-hour short snippets from spatial regions never seen before were provided as input to models. We can thus now present first insights from submissions to this spatial transfer learning challenge. Notably, models with better core prediction performance also generalized better. Moreover, the two top-ranked models – one {RCN} based, one U-Net based – were further ahead of the remaining top-ranked submissions for spatial transfer learning (+6\%) than in the core prediction challenge (+1\%).While submissions tested varying strategies for input data selection and training, there remains a wide range of additional complementary approaches to be explored in future analyses. The competition and its leaderboards remain available and open for new submissions on the weather4cast.ai website.},
	eventtitle = {2021 {IEEE} International Conference on Big Data (Big Data)},
	pages = {5750--5757},
	booktitle = {2021 {IEEE} International Conference on Big Data (Big Data)},
	author = {Herruzo, Pedro and Gruca, Aleksandra and Lliso, Llorenç and Calbet, Xavier and Rípodas, Pilar and Hochreiter, Sepp and Kopp, Michael and Kreil, David P.},
	date = {2021-12},
	keywords = {Predictive models, Training, Analytical models, Big Data, cloud cover, competition, deep-learning, machine-learning, nowcasting, precipitation, satellite-imagery, temperature, Temperature, Transfer learning, transfer-learning, turbulence, video-prediction, weather benchmark, Weather forecasting, weather-forecast},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/J6USCNQJ/9672063.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/PMGS4WXE/Herruzo et al. - 2021 - High-resolution multi-channel weather forecasting .pdf:application/pdf},
}

@article{grigsby_long-range_2021,
	title = {Long-Range Transformers for Dynamic Spatiotemporal Forecasting},
	url = {http://arxiv.org/abs/2109.12218},
	abstract = {Multivariate Time Series Forecasting ({TSF}) focuses on the prediction of future values based on historical context. In these problems, dependent variables provide additional information or early warning signs of changes in future behavior. State-of-the-art forecasting models rely on neural attention between timesteps. This allows for temporal learning but fails to consider distinct spatial relationships between variables. This paper addresses the problem by translating multivariate {TSF} into a novel “spatiotemporal sequence” formulation where each input token represents the value of a single variable at a given timestep. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, scales to high dimensional forecasting problems dominated by Graph Neural Networks that rely on predeﬁned variable graphs. We achieve competitive results on benchmarks from trafﬁc forecasting to electricity demand and weather prediction while learning spatial and temporal relationships purely from data.},
	journal = {{arXiv}:2109.12218 [cs, stat]},
	author = {Grigsby, Jake and Wang, Zhe and Qi, Yanjun},
	urldate = {2022-01-28},
	date = {2021-09-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.12218},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Grigsby et al. - 2021 - Long-Range Transformers for Dynamic Spatiotemporal.pdf:/Users/pemami/Zotero/storage/WGHDU7F5/Grigsby et al. - 2021 - Long-Range Transformers for Dynamic Spatiotemporal.pdf:application/pdf},
}

@article{lai_modeling_2018,
	title = {Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks},
	url = {http://arxiv.org/abs/1703.07015},
	abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network ({LSTNet}), to address this open challenge. {LSTNet} uses the Convolution Neural Network ({CNN}) and the Recurrent Neural Network ({RNN}) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, {LSTNet} achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
	journal = {{arXiv}:1703.07015 [cs]},
	author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
	urldate = {2022-01-28},
	date = {2018-04-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.07015},
	keywords = {Computer Science - Machine Learning},
	file = {Lai et al. - 2018 - Modeling Long- and Short-Term Temporal Patterns wi.pdf:/Users/pemami/Zotero/storage/S9SZUKA4/Lai et al. - 2018 - Modeling Long- and Short-Term Temporal Patterns wi.pdf:application/pdf},
}

@inproceedings{bauer_libra_2021,
	location = {Virtual Event France},
	title = {Libra: A Benchmark for Time Series Forecasting Methods},
	isbn = {978-1-4503-8194-9},
	url = {https://dl.acm.org/doi/10.1145/3427921.3450241},
	doi = {10.1145/3427921.3450241},
	shorttitle = {Libra},
	abstract = {In many areas of decision making, forecasting is an essential pillar. Consequently, there are many different forecasting methods. According to the “No-Free-Lunch Theorem”, there is no single forecasting method that performs best for all time series. In other words, each method has its advantages and disadvantages depending on the specific use case. Therefore, the choice of the forecasting method remains a mandatory expert task. However, expert knowledge cannot be fully automated. To establish a level playing field for evaluating the performance of time series forecasting methods in a broad setting, we propose Libra, a forecasting benchmark that automatically evaluates and ranks forecasting methods based on their performance in a diverse set of evaluation scenarios. The benchmark comprises four different use cases, each covering 100 heterogeneous time series taken from different domains. The data set was assembled from publicly available time series and was designed to exhibit much higher diversity than existing forecasting competitions. Based on this benchmark, we perform a comprehensive evaluation to compare different existing time series forecasting methods.},
	eventtitle = {{ICPE} '21: {ACM}/{SPEC} International Conference on Performance Engineering},
	pages = {189--200},
	booktitle = {Proceedings of the {ACM}/{SPEC} International Conference on Performance Engineering},
	publisher = {{ACM}},
	author = {Bauer, André and Züfle, Marwin and Eismann, Simon and Grohmann, Johannes and Herbst, Nikolas and Kounev, Samuel},
	urldate = {2022-01-28},
	date = {2021-04-09},
	langid = {english},
	file = {Bauer et al. - 2021 - Libra A Benchmark for Time Series Forecasting Met.pdf:/Users/pemami/Zotero/storage/VRH8PL8M/Bauer et al. - 2021 - Libra A Benchmark for Time Series Forecasting Met.pdf:application/pdf},
}

@article{iwana_empirical_2021,
	title = {An empirical survey of data augmentation for time series classification with neural networks},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254841},
	doi = {10.1371/journal.pone.0254841},
	abstract = {In recent times, deep artificial neural networks have achieved many successes in pattern recognition. Part of this success can be attributed to the reliance on big data to increase generalization. However, in the field of time series recognition, many datasets are often very small. One method of addressing this problem is through the use of data augmentation. In this paper, we survey data augmentation techniques for time series and their application to time series classification with neural networks. We propose a taxonomy and outline the four families in time series data augmentation, including transformation-based methods, pattern mixing, generative models, and decomposition methods. Furthermore, we empirically evaluate 12 time series data augmentation methods on 128 time series classification datasets with six different types of neural networks. Through the results, we are able to analyze the characteristics, advantages and disadvantages, and recommendations of each data augmentation method. This survey aims to help in the selection of time series data augmentation for neural network applications.},
	pages = {e0254841},
	number = {7},
	journal = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Iwana, Brian Kenji and Uchida, Seiichi},
	urldate = {2022-01-28},
	date = {2021-07-15},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Neural networks, Archives, Convolution, Interpolation, Permutation, Recurrent neural networks, Sensory perception, Time domain analysis},
	file = {Full Text PDF:/Users/pemami/Zotero/storage/PRRF6NMD/Iwana and Uchida - 2021 - An empirical survey of data augmentation for time .pdf:application/pdf;Snapshot:/Users/pemami/Zotero/storage/9I3XXXZW/article.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	journal = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2022-01-31},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/pemami/Zotero/storage/LPFQQYWI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@Article{liu_generating_2018,
  author     = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  title      = {Generating Wikipedia by Summarizing Long Sequences},
  eprint     = {1801.10198},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/1801.10198},
  abstract   = {We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate ﬂuent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reﬂected in perplexity, {ROUGE} scores and human evaluations.},
  file       = {Liu et al. - 2018 - Generating Wikipedia by Summarizing Long Sequences.pdf:/Users/pemami/Zotero/storage/J7CQIM6S/Liu et al. - 2018 - Generating Wikipedia by Summarizing Long Sequences.pdf:application/pdf},
  journal    = {{arXiv}:1801.10198 [cs]},
  keywords   = {Computer Science - Computation and Language},
  langid     = {english},
  urldate    = {2022-01-31},
  year       = {2018},
}

@article{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3’s few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that {GPT}-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of {GPT}-3 in general.},
	journal = {{arXiv}:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2022-01-31},
	date = {2020-07-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {Computer Science - Computation and Language},
	file = {Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:/Users/pemami/Zotero/storage/4DXEZFYV/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{bommasani_opportunities_2021,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	journal = {{arXiv}:2108.07258 [cs]},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2022-02-01},
	date = {2021-08-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.07258},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf:/Users/pemami/Zotero/storage/WYTPUG7D/Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf},
}

@Article{kroposki_achieving_2017,
  author   = {Kroposki, Benjamin and Johnson, Brian and Zhang, Yingchen and Gevorgian, Vahan and Denholm, Paul and Hodge, Bri-Mathias and Hannegan, Bryan},
  title    = {Achieving a 100\% Renewable Grid: Operating Electric Power Systems with Extremely High Levels of Variable Renewable Energy},
  doi      = {10.1109/MPE.2016.2637122},
  number   = {2},
  pages    = {61--73},
  volume   = {15},
  journal  = {{IEEE} Power and Energy Magazine},
  keywords = {Renewable energy sources, Coal, Electricity supply industry, Hydroelectric power generation, Power grids, Power system planning, Synchronous generators, Wind power},
  year     = {2017},
}

@article{jiang_short-term_2018,
	title = {A Short-Term and High-Resolution Distribution System Load Forecasting Approach Using Support Vector Regression With Hybrid Parameters Optimization},
	volume = {9},
	issn = {1949-3061},
	doi = {10.1109/TSG.2016.2628061},
	abstract = {This paper proposes an approach for distribution system load forecasting, which aims to provide highly accurate short-term load forecasting with high resolution utilizing a support vector regression ({SVR}) based forecaster and a two-step hybrid parameters optimization method. Specifically, because the load profiles in distribution systems contain abrupt deviations, a data normalization is designed as the pretreatment for the collected historical load data. Then an {SVR} model is trained by the load data to forecast the future load. For better performance of {SVR}, a two-step hybrid optimization algorithm is proposed to determine the best parameters. In the first step of the hybrid optimization algorithm, a designed grid traverse algorithm ({GTA}) is used to narrow the parameters searching area from a global to local space. In the second step, based on the result of the {GTA}, particle swarm optimization is used to determine the best parameters in the local parameter space. After the best parameters are determined, the {SVR} model is used to forecast the short-term load deviation in the distribution system. The performance of the proposed approach is compared to some classic methods in later sections of this paper.},
	pages = {3341--3350},
	number = {4},
	journal = {{IEEE} Transactions on Smart Grid},
	author = {Jiang, Huaiguang and Zhang, Yingchen and Muljadi, Eduard and Zhang, Jun Jason and Gao, David Wenzhong},
	date = {2018-07},
	note = {Conference Name: {IEEE} Transactions on Smart Grid},
	keywords = {Optimization, Predictive models, Training, Load modeling, Forecasting, Load forecasting, distribution system, grid traverse algorithm, particle swarm optimization, Short-term load forecast, Support vector machines, support vector regression},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/RX5AXNR8/7748604.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/XXLFRQ58/Jiang et al. - 2018 - A Short-Term and High-Resolution Distribution Syst.pdf:application/pdf},
}

@article{voyant_machine_2017,
	title = {Machine learning methods for solar radiation forecasting: A review},
	volume = {105},
	issn = {09601481},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960148116311648},
	doi = {10.1016/j.renene.2016.12.095},
	shorttitle = {Machine learning methods for solar radiation forecasting},
	abstract = {Forecasting the output power of solar systems is required for the good operation of the power grid or for the optimal management of the energy ﬂuxes occurring into the solar system. Before forecasting the solar systems output, it is essential to focus the prediction on the solar irradiance. The global solar radiation forecasting can be performed by several methods; the two big categories are the cloud imagery combined with physical models, and the machine learning models. In this context, the objective of this paper is to give an overview of forecasting methods of solar irradiation using machine learning approaches. Although, a lot of papers describes methodologies like neural networks or support vector regression, it will be shown that other methods (regression tree, random forest, gradient boosting and many others) begin to be used in this context of prediction. The performance ranking of such methods is complicated due to the diversity of the data set, time step, forecasting horizon, set up and performance indicators. Overall, the error of prediction is quite equivalent. To improve the prediction performance some authors proposed the use of hybrid models or to use an ensemble forecast approach.},
	pages = {569--582},
	journal = {Renewable Energy},
	shortjournal = {Renewable Energy},
	author = {Voyant, Cyril and Notton, Gilles and Kalogirou, Soteris and Nivet, Marie-Laure and Paoli, Christophe and Motte, Fabrice and Fouilloy, Alexis},
	urldate = {2022-02-02},
	date = {2017-05},
	langid = {english},
	file = {Voyant et al. - 2017 - Machine learning methods for solar radiation forec.pdf:/Users/pemami/Zotero/storage/J7ZQUPDP/Voyant et al. - 2017 - Machine learning methods for solar radiation forec.pdf:application/pdf},
}

@article{kumler_physics-based_2019,
	title = {A Physics-based Smart Persistence model for Intra-hour forecasting of solar radiation ({PSPI}) using {GHI} measurements and a cloud retrieval technique},
	volume = {177},
	issn = {0038092X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0038092X18311460},
	doi = {10.1016/j.solener.2018.11.046},
	abstract = {Short-term solar forecasting models based solely on global horizontal irradiance ({GHI}) measurements are often unable to discriminate the forecasting of the factors affecting {GHI} from those that can be precisely computed by atmospheric models. This study introduces a Physics-based Smart Persistence model for Intra-hour forecasting of solar radiation ({PSPI}) that decomposes the forecasting of {GHI} into the computation of extraterrestrial solar radiation and solar zenith angle and the forecasting of cloud albedo and cloud fraction. The extraterrestrial solar radiation and solar zenith angle are accurately computed by the Solar Position Algorithm ({SPA}) developed at the National Renewable Energy Laboratory ({NREL}). A cloud retrieval technique is used to estimate cloud albedo and cloud fraction from surfacebased observations of {GHI}. With the assumption of persistent cloud structures, the cloud albedo and cloud fraction are predicted for future time steps using a twostream approximation and a 5-min exponential weighted moving average, respectively. Our model evaluation using the long-term observations of {GHI} at {NREL}’s Solar Radiation Research Laboratory ({SRRL}) shows that the {PSPI} has a better performance than the persistence and smart persistence models in all forecast time horizons between 5 and 60 min, which is more significant in cloudy-sky conditions. Compared to the persistence and smart persistence models, the {PSPI} does not require additional observations of various atmospheric parameters but is customizable in that additional observations, if available, can be ingested to further improve the {GHI} forecast. An advanced technology of cloud forecast is also expected to improve the future performance of the {PSPI}.},
	pages = {494--500},
	journal = {Solar Energy},
	shortjournal = {Solar Energy},
	author = {Kumler, Andrew and Xie, Yu and Zhang, Yingchen},
	urldate = {2022-02-02},
	date = {2019-01},
	langid = {english},
	file = {Kumler et al. - 2019 - A Physics-based Smart Persistence model for Intra-.pdf:/Users/pemami/Zotero/storage/C4G9CYFY/Kumler et al. - 2019 - A Physics-based Smart Persistence model for Intra-.pdf:application/pdf},
}

@Article{vaswani_attention_2017,
  author     = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title      = {Attention Is All You Need},
  eprint     = {1706.03762},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/1706.03762},
  journal    = {{arXiv}:1706.03762 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  urldate    = {2022-02-03},
  year       = {2017},
}

@article{salinas_deepar_2019,
	title = {{DeepAR}: Probabilistic Forecasting with Autoregressive Recurrent Networks},
	url = {http://arxiv.org/abs/1704.04110},
	shorttitle = {{DeepAR}},
	abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series’ future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose {DeepAR}, a methodology for producing accurate probabilistic forecasts, based on training an auto-regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
	journal = {{arXiv}:1704.04110 [cs, stat]},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
	urldate = {2022-02-03},
	date = {2019-02-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1704.04110},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Salinas et al. - 2019 - DeepAR Probabilistic Forecasting with Autoregress.pdf:/Users/pemami/Zotero/storage/JBNVSG32/Salinas et al. - 2019 - DeepAR Probabilistic Forecasting with Autoregress.pdf:application/pdf},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	journal = {{arXiv}:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2022-02-04},
	date = {2019-07-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.11692},
	keywords = {Computer Science - Computation and Language},
	file = {Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:/Users/pemami/Zotero/storage/54KG3CHE/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf},
}

@article{weyn_improving_2020,
	title = {Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere},
	volume = {12},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002109},
	doi = {10.1029/2020MS002109},
	abstract = {We present a significantly improved data-driven global weather forecasting framework using a deep convolutional neural network ({CNN}) to forecast several basic atmospheric variables on a global grid. New developments in this framework include an off-line volume-conservative mapping to a cubed-sphere grid, improvements to the {CNN} architecture and the minimization of the loss function over multiple steps in a prediction sequence. The cubed-sphere remapping minimizes the distortion on the cube faces on which convolution operations are performed and provides natural boundary conditions for padding in the {CNN}. Our improved model produces weather forecasts that are indefinitely stable and produce realistic weather patterns at lead times of several weeks and longer. For short- to medium-range forecasting, our model significantly outperforms persistence, climatology, and a coarse-resolution dynamical numerical weather prediction ({NWP}) model. Unsurprisingly, our forecasts are worse than those from a high-resolution state-of-the-art operational {NWP} system. Our data-driven model is able to learn to forecast complex surface temperature patterns from few input atmospheric state variables. On annual time scales, our model produces a realistic seasonal cycle driven solely by the prescribed variation in top-of-atmosphere solar forcing. Although it currently does not compete with operational weather forecasting models, our data-driven {CNN} executes much faster than those models, suggesting that machine learning could prove to be a valuable tool for large-ensemble forecasting.},
	pages = {e2020MS002109},
	number = {9},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
	urldate = {2022-02-28},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002109},
	keywords = {to-read},
	file = {Full Text PDF:/Users/pemami/Zotero/storage/AGK6F4H9/Weyn et al. - 2020 - Improving Data-Driven Global Weather Prediction Us.pdf:application/pdf;Snapshot:/Users/pemami/Zotero/storage/CJLJ2HBK/2020MS002109.html:text/html},
}

@article{tascikaraoglu_evaluation_2018,
	title = {Evaluation of spatio-temporal forecasting methods in various smart city applications},
	volume = {82},
	issn = {13640321},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364032117313308},
	doi = {10.1016/j.rser.2017.09.078},
	abstract = {Together with the increasing population and urbanization, cities have started to face challenges that hinder their socio-economic and sustainable development. The concept of smart cities, therefore, has emerged during the last years as a response to these problems. Advanced measurement and communication technologies enabled through smart cities have particularly played a key role in dealing with such economic, social and organizational challenges faced during the growing of cities. In this sense, using historical information provided with the mentioned technologies, various forecasting tools have been incorporated into smart city environment in order to manage more eﬀectively its essential components such as smart grids and Intelligent Transportation Systems ({ITS}). For a further improvement in forecasting accuracy and hence in the management of these smart systems, recently, the information available in space has been also introduced in forecasting tools in addition to that in time. These advanced forecasting approaches, called spatio-temporal methods, have the capability of making use of all the available data collected from diﬀerent locations. The potential beneﬁts of these approaches have been underlined in various recent studies in the literature. In this paper, a comprehensive overview and assessment of forecasting approaches including both spatial and temporal information have been presented for the purpose of supporting the ongoing eﬀorts for exploiting the available information in smart city applications. With this objective, the spatio-temporal forecasting methods presented in the literature are classiﬁed considering their implementation areas and model structures. Furthermore, the similarities and peculiarities of the methods classiﬁed are examined in detail, resulted in the compiling of valuable reference information for future studies on improving these approaches.},
	pages = {424--435},
	journal = {Renewable and Sustainable Energy Reviews},
	shortjournal = {Renewable and Sustainable Energy Reviews},
	author = {Tascikaraoglu, Akin},
	urldate = {2022-04-05},
	date = {2018-02},
	langid = {english},
	file = {Tascikaraoglu - 2018 - Evaluation of spatio-temporal forecasting methods .pdf:/Users/pemami/Zotero/storage/MQ7XWX56/Tascikaraoglu - 2018 - Evaluation of spatio-temporal forecasting methods .pdf:application/pdf},
}

@article{koch_reduced_2021,
	title = {Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research},
	url = {http://arxiv.org/abs/2112.01716},
	shorttitle = {Reduced, Reused and Recycled},
	abstract = {Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals. Despite the foundational role of benchmarking practices in this ﬁeld, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We ﬁnd increasing concentration on fewer and fewer datasets within task communities, signiﬁcant adoption of datasets from other tasks, and concentration across the ﬁeld on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientiﬁc evaluation, {AI} ethics, and equity/access within the ﬁeld.},
	journal = {{arXiv}:2112.01716 [cs, stat]},
	author = {Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob G.},
	urldate = {2022-04-06},
	date = {2021-12-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2112.01716},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
	file = {Koch et al. - 2021 - Reduced, Reused and Recycled The Life of a Datase.pdf:/Users/pemami/Zotero/storage/EHK6TNA6/Koch et al. - 2021 - Reduced, Reused and Recycled The Life of a Datase.pdf:application/pdf},
}

@article{feng_opensolar_2019,
	title = {{OpenSolar}: Promoting the openness and accessibility of diverse public solar datasets},
	volume = {188},
	issn = {0038-092X},
	url = {https://www.sciencedirect.com/science/article/pii/S0038092X19306693},
	doi = {10.1016/j.solener.2019.07.016},
	shorttitle = {{OpenSolar}},
	abstract = {Observational solar data is the foundation of data-driven research in solar power grid integration and power system operations. Compared to other fields in data science, the openness and accessibility of solar data is lacking, which prevents solar data science from catching up with the emerging trends of data science (e.g., deep learning). In this paper, {OpenSolar}, a package with both R and Python versions, is developed to enhance the openness and accessibility of publicly available solar datasets. The {OpenSolar} package provides access to multiple types of solar data, primarily from four datasets: (1) the National Renewable Energy Laboratory ({NREL}) Solar Power Data for Integration Studies dataset, (2) the {NREL} Solar Radiation Research Laboratory dataset, (3) the Sheffield Solar-Microgen database, and (4) the Dataport database. Unlike other open solar datasets that only contain meteorological data, the four datasets in the {OpenSolar} package also contain behind-the-meter power data, sky images, and solar power data with satisfactory temporal and spatial resolution and coverage. The overview, quality-control methods, and potential usage of the datasets, in conjunction with sample code implementing the {OpenSolar} functions, are described in this paper. The package is expected to assist in bridging the gaps between the research fields of solar energy, power systems, and data science.},
	pages = {1369--1379},
	journal = {Solar Energy},
	shortjournal = {Solar Energy},
	author = {Feng, Cong and Yang, Dazhi and Hodge, Bri-Mathias and Zhang, Jie},
	urldate = {2022-04-06},
	date = {2019-08-01},
	langid = {english},
	keywords = {Machine learning, Data-driven, Python, R, Solar data openness},
	file = {ScienceDirect Full Text PDF:/Users/pemami/Zotero/storage/XRCM698X/Feng et al. - 2019 - OpenSolar Promoting the openness and accessibilit.pdf:application/pdf;ScienceDirect Snapshot:/Users/pemami/Zotero/storage/8GTQ4SJ9/S0038092X19306693.html:text/html},
}

@article{arias_electric_2016,
	title = {Electric vehicle charging demand forecasting model based on big data technologies},
	volume = {183},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261916311667},
	doi = {10.1016/j.apenergy.2016.08.080},
	abstract = {This paper presents a forecasting model to estimate electric vehicle charging demand based on big data technologies. Most previous studies have not considered real-world traffic distribution data and weather conditions in predicting the electric vehicle charging demand. In this paper, the historical traffic data and weather data of South Korea were used to formulate the forecasting model. The forecasting processes include a cluster analysis to classify traffic patterns, a relational analysis to identify influential factors, and a decision tree to establish classification criteria. The considered variables in this study were the charging starting time determined by the real-world traffic patterns and the initial state-of-charge of a battery. Example case studies for electric vehicle charging demand during weekdays and weekends in summer and winter were presented to show the different charging load profiles of electric vehicles in the residential and commercial sites. The presented forecasting model may allow power system engineers to anticipate electric vehicle charging demand based on historical traffic data and weather data. Therefore, the proposed electric vehicle charging demand model can be the foundation for the research on the impact of charging electric vehicles on the power system.},
	pages = {327--339},
	journal = {Applied Energy},
	shortjournal = {Applied Energy},
	author = {Arias, Mariz B. and Bae, Sungwoo},
	urldate = {2022-04-08},
	date = {2016-12-01},
	langid = {english},
	keywords = {Big data, Cluster analysis, Electric vehicle charging demand forecasting model, Real-world traffic data, Weather data},
	file = {ScienceDirect Snapshot:/Users/pemami/Zotero/storage/97PTXJCK/S0306261916311667.html:text/html},
}

@article{radford_learning_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/{OpenAI}/{CLIP}.},
	journal = {{arXiv}:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2022-04-08},
	date = {2021-02-26},
	eprinttype = {arxiv},
	eprint = {2103.00020},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/pemami/Zotero/storage/R7QJI5HE/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/Users/pemami/Zotero/storage/ZJ9AXERW/2103.html:text/html},
}

@article{donti_machine_2021,
	title = {Machine Learning for Sustainable Energy Systems},
	volume = {46},
	issn = {1543-5938, 1545-2050},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-environ-020220-061831},
	doi = {10.1146/annurev-environ-020220-061831},
	abstract = {In recent years, machine learning has proven to be a powerful tool for deriving insights from data. In this review, we describe ways in which machine learning has been leveraged to facilitate the development and operation of sustainable energy systems. We first provide a taxonomy of machine learning paradigms and techniques, along with a discussion of their strengths and limitations. We then provide an overview of existing research using machine learning for sustainable energy production, delivery, and storage. Finally, we identify gaps in this literature, propose future research directions, and discuss important considerations for deployment.},
	pages = {719--747},
	number = {1},
	journal = {Annual Review of Environment and Resources},
	shortjournal = {Annu. Rev. Environ. Resour.},
	author = {Donti, Priya L. and Kolter, J. Zico},
	urldate = {2022-04-14},
	date = {2021-10-18},
	langid = {english},
	file = {Donti and Kolter - 2021 - Machine Learning for Sustainable Energy Systems.pdf:/Users/pemami/Zotero/storage/97CNHKX2/Donti and Kolter - 2021 - Machine Learning for Sustainable Energy Systems.pdf:application/pdf},
}

@article{kashinath_physics-informed_2021,
	title = {Physics-informed machine learning: case studies for weather and climate modelling},
	volume = {379},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0093},
	doi = {10.1098/rsta.2020.0093},
	shorttitle = {Physics-informed machine learning},
	abstract = {Machine learning ({ML}) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf {ML} models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into {ML} models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed {ML} models for weather and climate processes.
            This article is part of the theme issue ‘Machine learning for weather and climate modelling’.},
	pages = {20200093},
	number = {2194},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	shortjournal = {Phil. Trans. R. Soc. A.},
	author = {Kashinath, K. and Mustafa, M. and Albert, A. and Wu, J-L. and Jiang, C. and Esmaeilzadeh, S. and Azizzadenesheli, K. and Wang, R. and Chattopadhyay, A. and Singh, A. and Manepalli, A. and Chirila, D. and Yu, R. and Walters, R. and White, B. and Xiao, H. and Tchelepi, H. A. and Marcus, P. and Anandkumar, A. and Hassanzadeh, P. and {Prabhat}},
	urldate = {2022-05-24},
	date = {2021-04-05},
	langid = {english},
	file = {Kashinath et al. - 2021 - Physics-informed machine learning case studies fo.pdf:/Users/pemami/Zotero/storage/8RSJKSUW/Kashinath et al. - 2021 - Physics-informed machine learning case studies fo.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: A large-scale hierarchical image database},
	doi = {10.1109/CVPR.2009.5206848},
	shorttitle = {{ImageNet}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “{ImageNet}”, a large-scale ontology of images built upon the backbone of the {WordNet} structure. {ImageNet} aims to populate the majority of the 80,000 synsets of {WordNet} with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of {WordNet}. This paper offers a detailed analysis of {ImageNet} in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that {ImageNet} is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of {ImageNet} through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of {ImageNet} can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	eventtitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {248--255},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	date = {2009-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Large-scale systems, Robustness, Explosions, Image databases, Image retrieval, Information retrieval, Internet, Multimedia databases, Ontologies, Spine},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/E3MNKMEP/5206848.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/X8ZEEIMS/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf},
}

@article{habte_long-term_2020,
	title = {Long-term spatial and temporal solar resource variability over America using the {NSRDB} version 3 (1998–2017)},
	volume = {134},
	issn = {13640321},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364032120305736},
	doi = {10.1016/j.rser.2020.110285},
	abstract = {The study assesses the long-term spatial and temporal solar resource variability in America using the 20-year National Renewable Energy Laboratory’s ({NREL}’s) National Solar Radiation Database ({NSRDB}). The coeffi­ cient of variation ({COV}) is used to analyze the spatial and temporal (interannual and seasonal) variability. Further, both spatial and temporal long-term variability are analyzed using the Ko¨ppen-Geiger climate classifi­ cation. The temporal variability is found that, on average, the continental United States ({CONUS}) {COV} reaches up to 5\% for global horizontal irradiance ({GHI}) and 10\% for direct normal irradiance ({DNI}), and that the {NSRDB} domain’s {COV} is roughly twice that of {CONUS}. For the seasonal variability analysis, the winter months are found to exhibit higher {COV} than the other seasons. In particular, December exhibits the highest variability, reaching on average 30\% for {DNI} and 20\% for {GHI} over various areas. On the other hand, the summer months demon­ strate significantly lower variability, reaching only less than 20\% for {DNI} and 10\% for {GHI}, on average. Simi­ larly, the spatial variability is analyzed by comparing each pixel to its neighbors. The long-term spatial variability is found to increase with the number of neighboring pixels being considered, which is equivalent to an increase in distance (within a 100-km x 100-km square grid). As expected, the {DNI} spatial variability is higher than that of {GHI}. Moreover, the annual solar irradiance anomalies are found to reach ±25\% for both {GHI} and {DNI} (and even exceed those value in some instances) during each year of the 20-year period.},
	pages = {110285},
	journal = {Renewable and Sustainable Energy Reviews},
	shortjournal = {Renewable and Sustainable Energy Reviews},
	author = {Habte, Aron and Sengupta, Manajit and Gueymard, Christian and Golnas, Anastasios and Xie, Yu},
	urldate = {2022-09-14},
	date = {2020-12},
	langid = {english},
	file = {Habte et al. - 2020 - Long-term spatial and temporal solar resource vari.pdf:/Users/pemami/Zotero/storage/AZR8LIRF/Habte et al. - 2020 - Long-term spatial and temporal solar resource vari.pdf:application/pdf},
}

@article{draxl_wind_2015,
	title = {The Wind Integration National Dataset ({WIND}) Toolkit},
	volume = {151},
	issn = {03062619},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261915004237},
	doi = {10.1016/j.apenergy.2015.03.121},
	abstract = {Regional wind integration studies in the United States require detailed wind power output data at many locations to perform simulations of how the power system will operate under high-penetration scenarios. The wind data sets that serve as inputs into the study must realistically reﬂect the ramping characteristics, spatial and temporal correlations, and capacity factors of the simulated wind plants, as well as be time synchronized with available load proﬁles. The Wind Integration National Dataset ({WIND}) Toolkit described in this article fulﬁlls these requirements as the largest and most complete grid integration data set publicly available to date. A meteorological data set, wind power production time series, and simulated forecasts created using the Weather Research and Forecasting Model run on a 2-km grid over the continental United States at a 5-min resolution is now publicly available for more than 126,000 land-based and offshore wind power production sites. State-of-the-art forecast accuracy was mimicked by reforecasting the years 2007–2013 using industry-standard techniques. Our meteorological and power validation results show that the {WIND} Toolkit data is satisfactory for wind energy integration studies. Users are encouraged to validate according to their phenomena and application of interest.},
	pages = {355--366},
	journal = {Applied Energy},
	shortjournal = {Applied Energy},
	author = {Draxl, Caroline and Clifton, Andrew and Hodge, Bri-Mathias and {McCaa}, Jim},
	urldate = {2022-10-27},
	date = {2015-08},
	langid = {english},
	file = {Draxl et al. - 2015 - The Wind Integration National Dataset (WIND) Toolk.pdf:/Users/pemami/Zotero/storage/FC68I7TA/Draxl et al. - 2015 - The Wind Integration National Dataset (WIND) Toolk.pdf:application/pdf},
}

@article{lu_deeponet_2021,
	title = {{DeepONet}: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	shorttitle = {{DeepONet}},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks ({DeepONets}) to learn operators accurately and efficiently from a relatively small dataset. A {DeepONet} consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that {DeepONet} significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	pages = {218--229},
	number = {3},
	journal = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	urldate = {2022-11-07},
	date = {2021-03},
	eprinttype = {arxiv},
	eprint = {1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/pemami/Zotero/storage/N2UFX4RJ/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identif.pdf:application/pdf;arXiv.org Snapshot:/Users/pemami/Zotero/storage/LSTTR3H2/1910.html:text/html},
}

@article{ahmad_artificial_2021,
	title = {Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities},
	volume = {289},
	issn = {09596526},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959652621000548},
	doi = {10.1016/j.jclepro.2021.125834},
	shorttitle = {Artificial intelligence in sustainable energy industry},
	abstract = {The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artiﬁcial intelligence ({AI}) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. {AI} will play an integral role in achieving this goal. This study focuses on the use of {AI} techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their {AI} efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of {AI} in solar and hydrogen power generation; (ii) the use of {AI} in supply and demand management control; and (iii) recent advances in {AI} technology. This study explored how {AI} techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, {IoT}, robotics, energy efﬁciency optimization, predictive maintenance control, and computational efﬁciency. Big data, the development of a machine learning model, and {AI} will play an important role in the future energy market. Our study’s ﬁndings show that {AI} is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efﬁciency in an increasingly cutthroat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on {AI} technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and ﬂexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, {AI} and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efﬁciently as possible.},
	pages = {125834},
	journal = {Journal of Cleaner Production},
	shortjournal = {Journal of Cleaner Production},
	author = {Ahmad, Tanveer and Zhang, Dongdong and Huang, Chao and Zhang, Hongcai and Dai, Ningyi and Song, Yonghua and Chen, Huanxin},
	urldate = {2022-11-07},
	date = {2021-03},
	langid = {english},
	file = {Ahmad et al. - 2021 - Artificial intelligence in sustainable energy indu.pdf:/Users/pemami/Zotero/storage/9QY69G54/Ahmad et al. - 2021 - Artificial intelligence in sustainable energy indu.pdf:application/pdf},
}

@inproceedings{ayush_geography-aware_2021,
	location = {Montreal, {QC}, Canada},
	title = {Geography-Aware Self-Supervised Learning},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711401/},
	doi = {10.1109/ICCV48922.2021.01002},
	abstract = {Contrastive learning methods have signiﬁcantly narrowed the gap between supervised and unsupervised learning on computer vision tasks. In this paper, we explore their application to geo-located datasets, e.g. remote sensing, where unlabeled data is often abundant but labeled data is scarce. We ﬁrst show that due to their different characteristics, a non-trivial gap persists between contrastive and supervised learning on standard benchmarks. To close the gap, we propose novel training methods that exploit the spatio-temporal structure of remote sensing data. We leverage spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location to design pre-text tasks. Our experiments show that our proposed method closes the gap between contrastive and supervised learning on image classiﬁcation, object detection and semantic segmentation for remote sensing. Moreover, we demonstrate that the proposed method can also be applied to geo-tagged {ImageNet} images, improving downstream performance on various tasks.},
	eventtitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {10161--10170},
	booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Ayush, Kumar and Uzkent, Burak and Meng, Chenlin and Tanmay, Kumar and Burke, Marshall and Lobell, David and Ermon, Stefano},
	urldate = {2022-11-07},
	date = {2021-10},
	langid = {english},
	file = {Ayush et al. - 2021 - Geography-Aware Self-Supervised Learning.pdf:/Users/pemami/Zotero/storage/8YU9ZJ44/Ayush et al. - 2021 - Geography-Aware Self-Supervised Learning.pdf:application/pdf},
}

@inproceedings{vincenzi_color_2021,
	title = {The color out of space: learning self-supervised representations for Earth Observation imagery},
	doi = {10.1109/ICPR48806.2021.9413112},
	shorttitle = {The color out of space},
	abstract = {The recent growth in the number of satellite images fosters the development of effective deep-learning techniques for Remote Sensing ({RS}). However, their full potential is untapped due to the lack of large annotated datasets. Such a problem is usually countered by fine-tuning a feature extractor that is previously trained on the {ImageNet} dataset. Unfortunately, the domain of natural images differs from the {RS} one, which hinders the final performance. In this work, we propose to learn meaningful representations from satellite imagery, leveraging its high-dimensionality spectral bands to reconstruct the visible colors. We conduct experiments on land cover classification ({BigEarthNet}) and West Nile Virus detection, showing that colorization is a solid pretext task for training a feature extractor. Furthermore, we qualitatively observe that guesses based on natural images and colorization rely on different parts of the input. This paves the way to an ensemble model that eventually outperforms both the above-mentioned techniques.},
	eventtitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	pages = {3034--3041},
	booktitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	author = {Vincenzi, Stefano and Porrello, Angelo and Buzzega, Pietro and Cipriano, Marco and Fronte, Pietro and Cuccu, Roberto and Ippoliti, Carla and Conte, Annamaria and Calderara, Simone},
	date = {2021-01},
	note = {{ISSN}: 1051-4651},
	keywords = {Task analysis, Training, Feature extraction, Image color analysis, Proposals, Satellites, Solids},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/UUJ98WIC/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/6FDZNNGA/Vincenzi et al. - 2021 - The color out of space learning self-supervised r.pdf:application/pdf},
}

@article{rasp_weatherbench_2020,
	title = {{WeatherBench}: A Benchmark Data Set for Data‐Driven Weather Forecasting},
	volume = {12},
	issn = {1942-2466, 1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/10.1029/2020MS002203},
	doi = {10.1029/2020MS002203},
	shorttitle = {{WeatherBench}},
	abstract = {Data-driven approaches, most prominently deep learning, have become powerful tools for prediction in many domains. A natural question to ask is whether data-driven methods could also be used to predict global weather patterns days in advance. First studies show promise but the lack of a common data set and evaluation metrics make intercomparison between studies difficult. Here we present a benchmark data set for data-driven medium-range weather forecasting (specifically 3–5 days), a topic of high scientific interest for atmospheric and computer scientists alike. We provide data derived from the {ERA}5 archive that has been processed to facilitate the use in machine learning models. We propose simple and clear evaluation metrics which will enable a direct comparison between different methods. Further, we provide baseline scores from simple linear regression techniques, deep learning models, as well as purely physical forecasting models. The data set is publicly available at https://github.com/pangeo-data/ {WeatherBench} and the companion code is reproducible with tutorials for getting started. We hope that this data set will accelerate research in data-driven weather forecasting.},
	number = {11},
	journal = {Journal of Advances in Modeling Earth Systems},
	shortjournal = {J. Adv. Model. Earth Syst.},
	author = {Rasp, Stephan and Dueben, Peter D. and Scher, Sebastian and Weyn, Jonathan A. and Mouatadid, Soukayna and Thuerey, Nils},
	urldate = {2022-11-08},
	date = {2020-11},
	langid = {english},
	file = {Rasp et al. - 2020 - WeatherBench A Benchmark Data Set for Data‐Driven.pdf:/Users/pemami/Zotero/storage/7JF4D6G4/Rasp et al. - 2020 - WeatherBench A Benchmark Data Set for Data‐Driven.pdf:application/pdf},
}

@Article{oreshkin_meta-learning_2021,
  author       = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  title        = {Meta-Learning Framework with Applications to Zero-Shot Time-Series Forecasting},
  doi          = {10.1609/aaai.v35i10.17115},
  issn         = {2374-3468, 2159-5399},
  number       = {10},
  pages        = {9242--9250},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17115},
  urldate      = {2022-11-21},
  volume       = {35},
  abstract     = {Can meta-learning discover generic ways of processing time series ({TS}) from a diverse dataset so as to greatly improve generalization on new {TS} coming from different datasets? This work provides positive evidence to this using a broad meta-learning framework which we show subsumes many existing meta-learning algorithms. Our theoretical analysis suggests that residual connections act as a meta-learning adaptation mechanism, generating a subset of task-speciﬁc parameters based on a given {TS} input, thus gradually expanding the expressive power of the architecture on-the-ﬂy. The same mechanism is shown via linearization analysis to have the interpretation of a sequential update of the ﬁnal linear layer. Our empirical results on a wide range of data emphasize the importance of the identiﬁed meta-learning mechanisms for successful zero-shot univariate forecasting, suggesting that it is viable to train a neural network on a source {TS} dataset and deploy it on a different target {TS} dataset without retraining, resulting in performance that is at least as good as that of state-of-practice univariate forecasting models.},
  journal      = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  langid       = {english},
  shortjournal = {{AAAI}},
  year         = {2021},
}

@Article{semenoglou_investigating_2021,
  author       = {Semenoglou, Artemios-Anargyros and Spiliotis, Evangelos and Makridakis, Spyros and Assimakopoulos, Vassilios},
  title        = {Investigating the accuracy of cross-learning time series forecasting methods},
  doi          = {10.1016/j.ijforecast.2020.11.009},
  number       = {3},
  pages        = {1072--1084},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0169207020301850},
  volume       = {37},
  abstract     = {The M4 competition identified innovative forecasting methods, advancing the theory and practice of forecasting. One of the most promising innovations of M4 was the utilization of cross-learning approaches that allow models to learn from multiple series how to accurately predict individual ones. In this paper, we investigate the potential of crosslearning by developing various neural network models that adopt such an approach, and we compare their accuracy to that of traditional models that are trained in a seriesby-series fashion. Our empirical evaluation, which is based on the M4 monthly data, confirms that cross-learning is a promising alternative to traditional forecasting, at least when appropriate strategies for extracting information from large, diverse time series data sets are considered. Ways of combining traditional with cross-learning methods are also examined in order to initiate further research in the field.},
  issn         = {01692070},
  journal      = {International Journal of Forecasting},
  langid       = {english},
  shortjournal = {International Journal of Forecasting},
  urldate      = {2022-11-21},
  year         = {2021},
}

@article{zhang_joint_2015,
	title = {Joint Probability Distribution and Correlation Analysis of Wind and Solar Power Forecast Errors in the Western Interconnection},
	volume = {141},
	issn = {0733-9402, 1943-7897},
	url = {https://ascelibrary.org/doi/10.1061/%28ASCE%29EY.1943-7897.0000189},
	doi = {10.1061/(ASCE)EY.1943-7897.0000189},
	abstract = {Wind and solar power generation differ from conventional power generation because of the variable and uncertain nature of their power output. This can have significant impacts on grid operations. Short-term forecasting of wind and solar power generation is uniquely helpful for planning the balance of supply and demand in the electric power system because it allows for a reduction in the uncertainty associated with their output. As a step toward assessing the simultaneous integration of large amounts of wind and solar power, this article investigates the spatial and temporal correlation between wind and solar power forecast errors. The forecast and actual data analyzed are obtained from one of the world’s largest regional variable generation integration studies to date. Multiple spatial and temporal scales (day ahead, 4 h ahead, and 1 h ahead) of forecast errors for the Western Interconnection in the United States are analyzed. A joint probability distribution of wind and solar power forecast errors is estimated using kernel density estimation. The Pearson’s correlation coefficient and mutual information between wind and solar power forecast errors are also evaluated. The results show that wind and solar power forecast errors are inversely correlated, and the correlation between wind and solar power forecast errors becomes stronger as the geographic size of the analyzed region increases. The absolute value of the correlation coefficient is generally less than 0.1 in the case of small geographic regions, while it is generally between 0.15 and 0.6 in the case of large geographic regions. The forecast errors are less correlated on the dayahead timescale, which influences economic operations more than reliability, and more correlated on the 4-h-ahead timescale, where reliability is more impacted by the forecasts. It is also found that the correlation between wind and solar power forecast errors in summer (July) is relatively stronger than in winter (January). The inverse correlation implies that in systems with high penetrations of both wind and solar power, reserves that are held to accommodate the variability of wind or solar power can be at least partially shared. In addition, interesting results are found through time and seasonal variation analyses of wind and solar power forecast errors, and these insights may be uniquely useful to operators who maintain the reliability of the electric power system. {DOI}: 10.1061/({ASCE}){EY}.1943-7897 .0000189. © 2014 American Society of Civil Engineers.},
	pages = {B4014008},
	number = {1},
	journal = {Journal of Energy Engineering},
	shortjournal = {J. Energy Eng.},
	author = {Zhang, Jie and Hodge, Bri-Mathias and Florita, Anthony},
	urldate = {2022-11-22},
	date = {2015-03},
	langid = {english},
	keywords = {to-read},
	file = {Zhang et al. - 2015 - Joint Probability Distribution and Correlation Ana.pdf:/Users/pemami/Zotero/storage/6BEWL36J/Zhang et al. - 2015 - Joint Probability Distribution and Correlation Ana.pdf:application/pdf},
}

@misc{mai_multi-scale_2020,
	title = {Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells},
	url = {http://arxiv.org/abs/2003.00824},
	abstract = {Unsupervised text encoding models have recently fueled substantial progress in {NLP}. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as {POIs} into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from {GIS} data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of {POIs} given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established {ML} approaches such as {RBF} kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec's multi-scale representation can handle distributions at different scales.},
	number = {{arXiv}:2003.00824},
	publisher = {{arXiv}},
	author = {Mai, Gengchen and Janowicz, Krzysztof and Yan, Bo and Zhu, Rui and Cai, Ling and Lao, Ni},
	urldate = {2022-12-07},
	date = {2020-02-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.00824 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, J.2, I.2.6, I.2.0, I.5.1},
	file = {Mai et al. - 2020 - Multi-Scale Representation Learning for Spatial Fe.pdf:/Users/pemami/Zotero/storage/MGIRWKTE/Mai et al. - 2020 - Multi-Scale Representation Learning for Spatial Fe.pdf:application/pdf},
}

@article{ravuri_skilful_2021,
	title = {Skilful precipitation nowcasting using deep generative models of radar},
	volume = {597},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03854-z},
	doi = {10.1038/s41586-021-03854-z},
	abstract = {Abstract
            
              Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making
              1,2
              . State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations
              3,4
              . Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints
              5,6
              . While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km × 1,280 km and with lead times from 5–90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89\% of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
	pages = {672--677},
	number = {7878},
	journal = {Nature},
	shortjournal = {Nature},
	author = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
	urldate = {2022-12-20},
	date = {2021-09-30},
	langid = {english},
	file = {Ravuri et al. - 2021 - Skilful precipitation nowcasting using deep genera.pdf:/Users/pemami/Zotero/storage/HS2FVG8U/Ravuri et al. - 2021 - Skilful precipitation nowcasting using deep genera.pdf:application/pdf},
}

@article{montero-manso_principles_2021,
	title = {Principles and algorithms for forecasting groups of time series: Locality and globality},
	volume = {37},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207021000558},
	doi = {10.1016/j.ijforecast.2021.03.004},
	shorttitle = {Principles and algorithms for forecasting groups of time series},
	pages = {1632--1653},
	number = {4},
	journal = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Montero-Manso, Pablo and Hyndman, Rob J.},
	urldate = {2022-12-21},
	date = {2021-10},
	langid = {english},
	file = {Montero-Manso and Hyndman - 2021 - Principles and algorithms for forecasting groups o.pdf:/Users/pemami/Zotero/storage/W5LANZYK/Montero-Manso and Hyndman - 2021 - Principles and algorithms for forecasting groups o.pdf:application/pdf},
}

@article{zheng_short-term_2017,
	title = {Short-Term Load Forecasting Using {EMD}-{LSTM} Neural Networks with a Xgboost Algorithm for Feature Importance Evaluation},
	volume = {10},
	issn = {1996-1073},
	url = {http://www.mdpi.com/1996-1073/10/8/1168},
	doi = {10.3390/en10081168},
	abstract = {Accurate load forecasting is an important issue for the reliable and efﬁcient operation of a power system. This study presents a hybrid algorithm that combines similar days ({SD}) selection, empirical mode decomposition ({EMD}), and long short-term memory ({LSTM}) neural networks to construct a prediction model (i.e., {SD}-{EMD}-{LSTM}) for short-term load forecasting. The extreme gradient boosting-based weighted k-means algorithm is used to evaluate the similarity between the forecasting and historical days. The {EMD} method is employed to decompose the {SD} load to several intrinsic mode functions ({IMFs}) and residual. Separated {LSTM} neural networks were also employed to forecast each {IMF} and residual. Lastly, the forecasting values from each {LSTM} model were reconstructed. Numerical testing demonstrates that the {SD}-{EMD}-{LSTM} method can accurately forecast the electric load.},
	pages = {1168},
	number = {8},
	journal = {Energies},
	shortjournal = {Energies},
	author = {Zheng, Huiting and Yuan, Jiabin and Chen, Long},
	urldate = {2023-01-26},
	date = {2017-08-08},
	langid = {english},
	file = {Zheng et al. - 2017 - Short-Term Load Forecasting Using EMD-LSTM Neural .pdf:/Users/pemami/Zotero/storage/JLY9AQHZ/Zheng et al. - 2017 - Short-Term Load Forecasting Using EMD-LSTM Neural .pdf:application/pdf},
}

@InProceedings{chadoulos_one_2021,
  author     = {Chadoulos, Spiros and Koutsopoulos, Iordanis and Polyzos, George C.},
  booktitle  = {Proceedings of the Twelfth {ACM} International Conference on Future Energy Systems},
  title      = {One model fits all: Individualized household energy demand forecasting with a single deep learning model},
  doi        = {10.1145/3447555.3466587},
  eventtitle = {e-Energy '21: The Twelfth {ACM} International Conference on Future Energy Systems},
  isbn       = {978-1-4503-8333-2},
  location   = {Virtual Event Italy},
  pages      = {466--474},
  publisher  = {{ACM}},
  url        = {https://dl.acm.org/doi/10.1145/3447555.3466587},
  urldate    = {2023-02-16},
  langid     = {english},
  year       = {2021},
}

@misc{shi_machine_2018,
	title = {Machine Learning for Spatiotemporal Sequence Forecasting: A Survey},
	url = {http://arxiv.org/abs/1808.06865},
	shorttitle = {Machine Learning for Spatiotemporal Sequence Forecasting},
	abstract = {Spatiotemporal systems are common in the real-world. Forecasting the multi-step future of these spatiotemporal systems based on the past observations, or, Spatiotemporal Sequence Forecasting ({STSF}), is a signiﬁcant and challenging problem. Although lots of real-world problems can be viewed as {STSF} and many research works have proposed machine learning based methods for them, no existing work has summarized and compared these methods from a uniﬁed perspective. This survey aims to provide a systematic review of machine learning for {STSF}. In this survey, we deﬁne the {STSF} problem and classify it into three subcategories: Trajectory Forecasting of Moving Point Cloud ({TF}-{MPC}), {STSF} on Regular Grid ({STSF}-{RG}) and {STSF} on Irregular Grid ({STSF}-{IG}). We then introduce the two major challenges of {STSF}: 1) how to learn a model for multi-step forecasting and 2) how to adequately model the spatial and temporal structures. After that, we review the existing works for solving these challenges, including the general learning strategies for multi-step forecasting, the classical machine learning based methods for {STSF}, and the deep learning based methods for {STSF}. We also compare these methods and point out some potential research directions.},
	number = {{arXiv}:1808.06865},
	publisher = {{arXiv}},
	author = {Shi, Xingjian and Yeung, Dit-Yan},
	urldate = {2023-02-23},
	date = {2018-08-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.06865 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Shi and Yeung - 2018 - Machine Learning for Spatiotemporal Sequence Forec.pdf:/Users/pemami/Zotero/storage/XHWMIRCY/Shi and Yeung - 2018 - Machine Learning for Spatiotemporal Sequence Forec.pdf:application/pdf},
}

@article{doubleday_benchmark_2020,
	title = {Benchmark probabilistic solar forecasts: Characteristics and recommendations},
	volume = {206},
	issn = {0038092X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0038092X20305429},
	doi = {10.1016/j.solener.2020.05.051},
	shorttitle = {Benchmark probabilistic solar forecasts},
	abstract = {We illustrate and compare commonly used benchmark, or reference, methods for probabilistic solar forecasting that researchers use to measure the performance of their proposed techniques. A thorough review of the literature indicates wide variation in the benchmarks implemented in probabilistic solar forecast studies. To promote consistent and sensible methodological comparisons, we implement and compare ten variants from six common benchmark classes at two temporal scales: intra-hourly forecasts and hourly resolution forecasts. Using open-source Surface Radiation Budget Network ({SURFRAD}) data from 2018, these benchmark methods are compared using proper probabilistic metrics and common diagnostic tools. Practical implementation issues, such as the impact of missing data and applicability for operational forecasting, are also discussed. We make recommendations for practitioners on the appropriate selection of benchmark methods to properly showcase stateof-the-art improvements in forecast reliability and sharpness. All code and open-source data are available on Github for reproducibility and for other researchers to apply the same benchmark methods to their own data.},
	pages = {52--67},
	journal = {Solar Energy},
	shortjournal = {Solar Energy},
	author = {Doubleday, Kate and Van Scyoc Hernandez, Vanessa and Hodge, Bri-Mathias},
	urldate = {2023-03-04},
	date = {2020-08},
	langid = {english},
	file = {Doubleday et al. - 2020 - Benchmark probabilistic solar forecasts Character.pdf:/Users/pemami/Zotero/storage/9ZIMZGMW/Doubleday et al. - 2020 - Benchmark probabilistic solar forecasts Character.pdf:application/pdf},
}

@Article{wu_deep_2020,
  author     = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  title      = {Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case},
  eprint     = {2001.08317 [cs, stat]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2001.08317},
  journal    = {{arXiv}:2001.08317},
  langid     = {english},
  publisher  = {{arXiv}},
  year       = {2020},
}

@misc{lim_temporal_2020,
	title = {Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
	url = {http://arxiv.org/abs/1912.09363},
	abstract = {Multi-horizon forecasting often contains a complex mix of inputs – including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past – without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically ‘black-box’ models which do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer ({TFT}) – a novel attentionbased architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at diﬀerent scales, {TFT} uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. {TFT} utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate signiﬁcant performance improvements over existing benchmarks, and showcase three practical interpretability use cases of {TFT}.},
	number = {{arXiv}:1912.09363},
	publisher = {{arXiv}},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	urldate = {2023-03-16},
	date = {2020-09-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.09363 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Mul.pdf:/Users/pemami/Zotero/storage/RPUIFPBE/Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Mul.pdf:application/pdf},
}

@Article{miller_building_2020,
  author       = {Miller, Clayton and Kathirgamanathan, Anjukan and Picchetti, Bianca and Arjunan, Pandarasamy and Park, June Young and Nagy, Zoltan and Raftery, Paul and Hobson, Brodie W. and Shi, Zixiao and Meggers, Forrest},
  date         = {2020-10-27},
  journal = {Scientific Data},
  title        = {The Building Data Genome Project 2, energy meter data from the {ASHRAE} Great Energy Predictor {III} competition},
  doi          = {10.1038/s41597-020-00712-x},
  issn         = {2052-4463},
  number       = {1},
  pages        = {368},
  url          = {https://www.nature.com/articles/s41597-020-00712-x},
  volume       = {7},
  langid       = {english},
  year         = {2020},
}

@Misc{iwata_few-shot_2020,
  author     = {Iwata, Tomoharu and Kumagai, Atsutoshi},
  title      = {Few-shot Learning for Time-series Forecasting},
  eprint     = {2009.14379 [cs, stat]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2009.14379},
  urldate    = {2023-03-18},
  abstract   = {Time-series forecasting is important for many applications. Forecasting models are usually trained using time-series data in a speciﬁc target task. However, sufﬁcient data in the target task might be unavailable, which leads to performance degradation. In this paper, we propose a few-shot learning method that forecasts a future value of a time-series in a target task given a few time-series in the target task. Our model is trained using time-series data in multiple training tasks that are different from target tasks. Our model uses a few time-series to build a forecasting function based on a recurrent neural network with an attention mechanism. With the attention mechanism, we can retrieve useful patterns in a small number of time-series for the current situation. Our model is trained by minimizing an expected test error of forecasting next timestep values. We demonstrate the effectiveness of the proposed method using 90 time-series datasets.},
  file       = {Iwata and Kumagai - 2020 - Few-shot Learning for Time-series Forecasting.pdf:/Users/pemami/Zotero/storage/ECCX3KNH/Iwata and Kumagai - 2020 - Few-shot Learning for Time-series Forecasting.pdf:application/pdf},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid     = {english},
  number     = {{arXiv}:2009.14379},
  publisher  = {{arXiv}},
  year       = {2020},
}

@inproceedings{marino_building_2016,
	title = {Building energy load forecasting using Deep Neural Networks},
	doi = {10.1109/IECON.2016.7793413},
	abstract = {Ensuring sustainability demands more efficient energy management with minimized energy wastage. Therefore, the power grid of the future should provide an unprecedented level of flexibility in energy management. To that end, intelligent decision making requires accurate predictions of future energy demand/load, both at aggregate and individual site level. Thus, energy load forecasting have received increased attention in the recent past. However, it has proven to be a difficult problem. This paper presents a novel energy load forecasting methodology based on Deep Neural Networks, specifically, Long Short Term Memory ({LSTM}) algorithms. The presented work investigates two {LSTM} based architectures: 1) standard {LSTM} and 2) {LSTM}-based Sequence to Sequence (S2S) architecture. Both methods were implemented on a benchmark data set of electricity consumption data from one residential customer. Both architectures were trained and tested on one hour and one-minute time-step resolution datasets. Experimental results showed that the standard {LSTM} failed at one-minute resolution data while performing well in one-hour resolution data. It was shown that S2S architecture performed well on both datasets. Further, it was shown that the presented methods produced comparable results with the other deep learning methods for energy forecasting in literature.},
	eventtitle = {{IECON} 2016 - 42nd Annual Conference of the {IEEE} Industrial Electronics Society},
	pages = {7046--7051},
	booktitle = {{IECON} 2016 - 42nd Annual Conference of the {IEEE} Industrial Electronics Society},
	author = {Marino, Daniel L. and Amarasinghe, Kasun and Manic, Milos},
	date = {2016-10},
	keywords = {Predictive models, Load modeling, Load forecasting, Deep Learning, Computer architecture, Buildings, Building Energy, Deep Neural Networks, Energy, Energy Load forecasting, Logic gates, Long-Short-Term memory, {LSTM}, Standards},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/6T4LA7LJ/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/LBKZHKKI/Marino et al. - 2016 - Building energy load forecasting using Deep Neural.pdf:application/pdf},
}

@Article{zhang_review_2021,
  author       = {Zhang, Liang and Wen, Jin and Li, Yanfei and Chen, Jianli and Ye, Yunyang and Fu, Yangyang and Livingood, William},
  title        = {A review of machine learning in building load prediction},
  doi          = {10.1016/j.apenergy.2021.116452},
  pages        = {116452},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0306261921000209},
  volume       = {285},
  abstract     = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machinelearning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E.},
  file         = {Zhang et al. - 2021 - A review of machine learning in building load pred.pdf:/Users/pemami/Zotero/storage/SEJ2WSQG/Zhang et al. - 2021 - A review of machine learning in building load pred.pdf:application/pdf},
  issn         = {03062619},
  journal      = {Applied Energy},
  langid       = {english},
  shortjournal = {Applied Energy},
  year         = {2021},
}

@article{kim_predicting_2019,
	title = {Predicting residential energy consumption using {CNN}-{LSTM} neural networks},
	volume = {182},
	issn = {03605442},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0360544219311223},
	doi = {10.1016/j.energy.2019.05.230},
	abstract = {The rapid increase in human population and development in technology have sharply raised power consumption in today's world. Since electricity is consumed simultaneously as it is generated at the power plant, it is important to accurately predict the energy consumption in advance for stable power supply. In this paper, we propose a {CNN}-{LSTM} neural network that can extract spatial and temporal features to effectively predict the housing energy consumption. Experiments have shown that the {CNNLSTM} neural network, which combines convolutional neural network ({CNN}) and long short-term memory ({LSTM}), can extract complex features of energy consumption. The {CNN} layer can extract the features between several variables affecting energy consumption, and the {LSTM} layer is appropriate for modeling temporal information of irregular trends in time series components. The proposed {CNN}-{LSTM} method achieves almost perfect prediction performance for electric energy consumption that was previously difﬁcult to predict. Also, it records the smallest value of root mean square error compared to the conventional forecasting methods for the dataset on individual household power consumption. The empirical analysis of the variables conﬁrms what affects to forecast the power consumption most.},
	pages = {72--81},
	journal = {Energy},
	shortjournal = {Energy},
	author = {Kim, Tae-Young and Cho, Sung-Bae},
	urldate = {2023-03-18},
	date = {2019-09},
	langid = {english},
	file = {Kim and Cho - 2019 - Predicting residential energy consumption using CN.pdf:/Users/pemami/Zotero/storage/YEBNJ982/Kim and Cho - 2019 - Predicting residential energy consumption using CN.pdf:application/pdf},
}

@Article{amasyali_review_2018,
  author       = {Amasyali, Kadir and El-Gohary, Nora M.},
  title        = {A review of data-driven building energy consumption prediction studies},
  doi          = {10.1016/j.rser.2017.04.095},
  pages        = {1192--1205},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1364032117306093},
  volume       = {81},
  abstract     = {Energy is the lifeblood of modern societies. In the past decades, the world's energy consumption and associated {CO}2 emissions increased rapidly due to the increases in population and comfort demands of people. Building energy consumption prediction is essential for energy planning, management, and conservation. Data-driven models provide a practical approach to energy consumption prediction. This paper oﬀers a review of the studies that developed data-driven building energy consumption prediction models, with a particular focus on reviewing the scopes of prediction, the data properties and the data preprocessing methods used, the machine learning algorithms utilized for prediction, and the performance measures used for evaluation. Based on this review, existing research gaps are identiﬁed and future research directions in the area of data-driven building energy consumption prediction are highlighted.},
  file         = {Amasyali and El-Gohary - 2018 - A review of data-driven building energy consumptio.pdf:/Users/pemami/Zotero/storage/4MB4PBGH/Amasyali and El-Gohary - 2018 - A review of data-driven building energy consumptio.pdf:application/pdf},
  issn         = {13640321},
  journal      = {Renewable and Sustainable Energy Reviews},
  langid       = {english},
  shortjournal = {Renewable and Sustainable Energy Reviews},
  urldate      = {2023-03-21},
  year         = {2018},
}

@misc{zhang_first_2022,
	title = {First De-Trend then Attend: Rethinking Attention for Time-Series Forecasting},
	url = {http://arxiv.org/abs/2212.08151},
	shorttitle = {First De-Trend then Attend},
	abstract = {Transformer-based models have gained large popularity and demonstrated promising results in long-term time-series forecasting in recent years. In addition to learning attention in time domain, recent works also explore learning attention in frequency domains (e.g., Fourier domain, wavelet domain), given that seasonal patterns can be better captured in these domains. In this work, we seek to understand the relationships between attention models in different time and frequency domains. Theoretically, we show that attention models in different domains are equivalent under linear conditions (i.e., linear kernel to attention scores). Empirically, we analyze how attention models of different domains show different behaviors through various synthetic experiments with seasonality, trend and noise, with emphasis on the role of softmax operation therein. Both these theoretical and empirical analyses motivate us to propose a new method: {TDformer} (Trend Decomposition Transformer), that ﬁrst applies seasonal-trend decomposition, and then additively combines an {MLP} which predicts the trend component with Fourier attention which predicts the seasonal component to obtain the ﬁnal prediction. Extensive experiments on benchmark time-series forecasting datasets demonstrate that {TDformer} achieves state-of-the-art performance against existing attention-based models.},
	number = {{arXiv}:2212.08151},
	publisher = {{arXiv}},
	author = {Zhang, Xiyuan and Jin, Xiaoyong and Gopalswamy, Karthick and Gupta, Gaurav and Park, Youngsuk and Shi, Xingjian and Wang, Hao and Maddix, Danielle C. and Wang, Yuyang},
	urldate = {2023-03-22},
	date = {2022-12-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2212.08151 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Zhang et al. - 2022 - First De-Trend then Attend Rethinking Attention f.pdf:/Users/pemami/Zotero/storage/DE4W2KXD/Zhang et al. - 2022 - First De-Trend then Attend Rethinking Attention f.pdf:application/pdf},
}

@InProceedings{Yuqietal-2023-PatchTST,
  author    = {Nie, Yuqi and H. Nguyen, Nam and Sinthong, Phanwadee and Kalagnanam, Jayant},
  booktitle = {International Conference on Learning Representations},
  title     = {A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  year      = {2023},
}

@article{yue_ts2vec_2022,
	title = {{TS}2Vec: Towards Universal Representation of Time Series},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20881},
	doi = {10.1609/aaai.v36i8.20881},
	shorttitle = {{TS}2Vec},
	abstract = {This paper presents {TS}2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, {TS}2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classiﬁcation tasks to evaluate the quality of time series representations. As a result, {TS}2Vec achieves signiﬁcant improvement over existing {SOTAs} of unsupervised time series representation on 125 {UCR} datasets and 29 {UEA} datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous {SOTAs} of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes {SOTA} results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
	pages = {8980--8987},
	number = {8},
	journal = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
	urldate = {2023-03-23},
	date = {2022-06-28},
	langid = {english},
	file = {Yue et al. - 2022 - TS2Vec Towards Universal Representation of Time S.pdf:/Users/pemami/Zotero/storage/TJX6A6RM/Yue et al. - 2022 - TS2Vec Towards Universal Representation of Time S.pdf:application/pdf},
}

@article{makridakis_m5_2022,
	title = {M5 accuracy competition: Results, findings, and conclusions},
	volume = {38},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207021001874},
	doi = {10.1016/j.ijforecast.2021.11.013},
	shorttitle = {M5 accuracy competition},
	abstract = {In this study, we present the results of the M5 ‘‘Accuracy’’ competition, which was the first of two parallel challenges in the latest M competition with the aim of advancing the theory and practice of forecasting. The main objective in the M5 ‘‘Accuracy’’ competition was to accurately predict 42,840 time series representing the hierarchical unit sales for the largest retail company in the world by revenue, Walmart. The competition required the submission of 30,490 point forecasts for the lowest cross-sectional aggregation level of the data, which could then be summed up accordingly to estimate forecasts for the remaining upward levels. We provide details of the implementation of the M5 ‘‘Accuracy’’ challenge, as well as the results and best performing methods, and summarize the major findings and conclusions. Finally, we discuss the implications of these findings and suggest directions for future research.},
	pages = {1346--1364},
	number = {4},
	journal = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	urldate = {2023-03-28},
	date = {2022-10},
	langid = {english},
	file = {Makridakis et al. - 2022 - M5 accuracy competition Results, findings, and co.pdf:/Users/pemami/Zotero/storage/WTJIIKG4/Makridakis et al. - 2022 - M5 accuracy competition Results, findings, and co.pdf:application/pdf},
}

@misc{hutchinson_towards_2021,
	title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
	url = {http://arxiv.org/abs/2010.13561},
	shorttitle = {Towards Accountability for Machine Learning Datasets},
	abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decisionmaking, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
	number = {{arXiv}:2010.13561},
	publisher = {{arXiv}},
	author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
	urldate = {2023-03-28},
	date = {2021-01-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.13561 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Databases, Computer Science - Software Engineering},
	file = {Hutchinson et al. - 2021 - Towards Accountability for Machine Learning Datase.pdf:/Users/pemami/Zotero/storage/YAFGE8UM/Hutchinson et al. - 2021 - Towards Accountability for Machine Learning Datase.pdf:application/pdf},
}

@article{pereira_residential_2022,
	title = {A residential labeled dataset for smart meter data analytics},
	volume = {9},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01252-2},
	doi = {10.1038/s41597-022-01252-2},
	abstract = {Abstract
            Smart meter data is a cornerstone for the realization of next-generation electrical power grids by enabling the creation of novel energy data-based services like providing recommendations on how to save energy or predictive maintenance of electric appliances. Most of these services are developed on top of advanced machine-learning algorithms, which rely heavily on datasets for training, testing, and validation purposes. A limitation of most existing datasets, however, is the scarcity of labels. The {SustDataED}2 dataset described in this paper contains 96 days of aggregated and individual appliance consumption from one household in Portugal. The current and voltage waveforms were sampled at 12.8 {kHz}, and the individual consumption of 18 appliances was sampled at 0.5 Hz. The dataset also contains the timestamps of the {ON}-{OFF} transitions of the monitored appliances for the entire deployment duration, providing the necessary ground truth for the evaluation of machine learning problems, particularly Non-Intrusive Load Monitoring. The data is accessible in easy-to-use audio and comma-separated formats.},
	pages = {134},
	number = {1},
	journal = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Pereira, Lucas and Costa, Donovan and Ribeiro, Miguel},
	urldate = {2023-04-19},
	date = {2022-03-31},
	langid = {english},
	file = {Pereira et al. - 2022 - A residential labeled dataset for smart meter data.pdf:/Users/pemami/Zotero/storage/WW7UF4M2/Pereira et al. - 2022 - A residential labeled dataset for smart meter data.pdf:application/pdf},
}

@inproceedings{stamatescu_multiscale_2022,
	title = {Multiscale Data Analytics for Residential Active Power Measurements through Time Series Data Mining},
	doi = {10.1109/ENERGYCON53164.2022.9830170},
	abstract = {Modern measurement and automation equipment for energy systems collect, store, process and communicate ever increasing quantities of raw data which can be used to build data-driven prediction and classification models. Directly using these large and unprocessed data sets can be inefficient especially in imbalanced class problems, where the positive class is sparsely represented in the training examples, such as classification of micro-scale transients. This is due to the longer time required for model training, and due to the increased possibility of obfuscating the useful information behind noisy readings. The matrix profile represents a computationally efficient and general purpose time series data mining technique which is suitable for embedded deployment in future generation smart meters, and in embedded energy gateways. Our analysis concerns the application of this technique on two types of residential electric power measurement data sets: a detached single family house and an apartment, with variable reporting rate and subsequence size parametrisation. Quantitative results support the findings that such approaches serve as practical instrument for measurement time series preprocessing in energy analytics.},
	eventtitle = {2022 {IEEE} 7th International Energy Conference ({ENERGYCON})},
	pages = {1--5},
	booktitle = {2022 {IEEE} 7th International Energy Conference ({ENERGYCON})},
	author = {Stamatescu, Grigore and Plamanescu, Radu and Dumitrescu, Ana-Maria and Ciomei, Irina and Albu, Mihaela},
	date = {2022-05},
	keywords = {Training, Time measurement, Time series analysis, Computational efficiency, Data analysis, data analytics, Energy measurement, feature extraction, Power measurement, power measurements, pre-processing, time series},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/4SPSQHR2/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/ZNDNXAYE/Stamatescu et al. - 2022 - Multiscale Data Analytics for Residential Active P.pdf:application/pdf},
}

@article{himeur_building_2020,
	title = {Building power consumption datasets: Survey, taxonomy and future directions},
	volume = {227},
	issn = {0378-7788},
	url = {https://www.sciencedirect.com/science/article/pii/S037877882030815X},
	doi = {10.1016/j.enbuild.2020.110404},
	shorttitle = {Building power consumption datasets},
	abstract = {In the last decade, extended efforts have been poured into energy efficiency. Several energy consumption datasets were henceforth published, with each dataset varying in properties, uses and limitations. For instance, building energy consumption patterns are sourced from several sources, including ambient conditions, user occupancy, weather conditions and consumer preferences. Thus, a proper understanding of the available datasets will result in a strong basis for improving energy efficiency. Starting from the necessity of a comprehensive review of existing databases, this work is proposed to survey, study and visualize the numerical and methodological nature of building energy consumption datasets. A total of thirty-one databases are examined and compared in terms of several features, such as the geographical location, period of collection, number of monitored households, sampling rate of collected data, number of sub-metered appliances, extracted features and release date. Furthermore, data collection platforms and related modules for data transmission, data storage and privacy concerns used in different datasets are also analyzed and compared. Based on the analytical study, a novel dataset has been presented, namely Qatar university dataset, which is an annotated power consumption anomaly detection dataset. The latter will be very useful for testing and training anomaly detection algorithms, and hence reducing wasted energy. Moving forward, a set of recommendations is derived to improve datasets collection, such as the adoption of multi-modal data collection, smart Internet of things data collection, low-cost hardware platforms and privacy and security mechanisms. In addition, future directions to improve datasets exploitation and utilization are identified, including the use of novel machine learning solutions, innovative visualization tools and explainable mobile recommender systems. Accordingly, a novel visualization strategy based on using power consumption micro-moments has been presented along with an example of deploying machine learning algorithms to classify the micro-moment classes and identify anomalous power usage.},
	pages = {110404},
	journal = {Energy and Buildings},
	shortjournal = {Energy and Buildings},
	author = {Himeur, Yassine and Alsalemi, Abdullah and Bensaali, Faycal and Amira, Abbes},
	urldate = {2023-04-19},
	date = {2020-11-15},
	langid = {english},
	keywords = {Energy efficiency, Building power consumption datasets, Dataset collection, Micro-moments, Recommender systems, Visualization},
	file = {ScienceDirect Full Text PDF:/Users/pemami/Zotero/storage/B5L4QAVV/Himeur et al. - 2020 - Building power consumption datasets Survey, taxon.pdf:application/pdf;ScienceDirect Snapshot:/Users/pemami/Zotero/storage/TELEXEEI/S037877882030815X.html:text/html},
}

@article{haben_review_2021,
	title = {Review of low voltage load forecasting: Methods, applications, and recommendations},
	volume = {304},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261921011326},
	doi = {10.1016/j.apenergy.2021.117798},
	shorttitle = {Review of low voltage load forecasting},
	abstract = {The increased digitalisation and monitoring of the energy system opens up numerous opportunities to decarbonise the energy system. Applications on low voltage, local networks, such as community energy markets and smart storage will facilitate decarbonisation, but they will require advanced control and management. Reliable forecasting will be a necessary component of many of these systems to anticipate key features and uncertainties. Despite this urgent need, there has not yet been an extensive investigation into the current state-of-the-art of low voltage level forecasts, other than at the smart meter level. This paper aims to provide a comprehensive overview of the landscape, current approaches, core applications, challenges and recommendations. Another aim of this paper is to facilitate the continued improvement and advancement in this area. To this end, the paper also surveys some of the most relevant and promising trends. It establishes an open, community-driven list of the known low voltage level open datasets to encourage further research and development.},
	pages = {117798},
	journal = {Applied Energy},
	shortjournal = {Applied Energy},
	author = {Haben, Stephen and Arora, Siddharth and Giasemidis, Georgios and Voss, Marcus and Vukadinović Greetham, Danica},
	urldate = {2023-04-19},
	date = {2021-12-15},
	langid = {english},
	keywords = {Neural networks, Load forecasting, Machine learning, Smart grid, Demand forecasting, Low voltage, Review, Smart meter, Substations, Survey, Time series},
	file = {ScienceDirect Full Text PDF:/Users/pemami/Zotero/storage/C6JE5NRQ/Haben et al. - 2021 - Review of low voltage load forecasting Methods, a.pdf:application/pdf;ScienceDirect Snapshot:/Users/pemami/Zotero/storage/7CQB46S4/S0306261921011326.html:text/html},
}

@Article{barker2012smart,
  author  = {Barker, Sean and Mishra, Aditya and Irwin, David and Cecchet, Emmanuel and Shenoy, Prashant and Albrecht, Jeannie and others},
  title   = {Smart*: An open data set and tools for enabling research in sustainable homes},
  number  = {112},
  pages   = {108},
  volume  = {111},
  journal = {SustKDD, August},
  year    = {2012},
}

@misc{rasul_vq-ar_2022,
	title = {{VQ}-{AR}: Vector Quantized Autoregressive Probabilistic Time Series Forecasting},
	url = {http://arxiv.org/abs/2205.15894},
	shorttitle = {{VQ}-{AR}},
	abstract = {Time series models aim for accurate predictions of the future given the past, where the forecasts are used for important downstream tasks like business decision making. In practice, deep learning based time series models come in many forms, but at a high level learn some continuous representation of the past and use it to output point or probabilistic forecasts. In this paper, we introduce a novel autoregressive architecture, {VQ}-{AR}, which instead learns a discrete set of representations that are used to predict the future. Extensive empirical comparison with other competitive deep learning models shows that surprisingly such a discrete set of representations gives state-of-the-art or equivalent results on a wide variety of time series datasets. We also highlight the shortcomings of this approach, explore its zero-shot generalization capabilities, and present an ablation study on the number of representations. The full source code of the method will be available at the time of publication with the hope that researchers can further investigate this important but overlooked inductive bias for the time series domain.},
	number = {{arXiv}:2205.15894},
	publisher = {{arXiv}},
	author = {Rasul, Kashif and Park, Young-Jin and Ramström, Max Nihlén and Kim, Kyung-Min},
	urldate = {2023-04-23},
	date = {2022-05-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2205.15894 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Rasul et al. - 2022 - VQ-AR Vector Quantized Autoregressive Probabilist.pdf:/Users/pemami/Zotero/storage/YQX4LXLM/Rasul et al. - 2022 - VQ-AR Vector Quantized Autoregressive Probabilist.pdf:application/pdf},
}

@Article{hong_probabilistic_2016,
  author       = {Hong, Tao and Fan, Shu},
  title        = {Probabilistic electric load forecasting: A tutorial review},
  doi          = {10.1016/j.ijforecast.2015.11.011},
  issn         = {01692070},
  number       = {3},
  pages        = {914--938},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0169207015001508},
  urldate      = {2023-05-08},
  volume       = {32},
  file         = {Hong and Fan - 2016 - Probabilistic electric load forecasting A tutoria.pdf:/Users/pemami/Zotero/storage/ZIM7YEZK/Hong and Fan - 2016 - Probabilistic electric load forecasting A tutoria.pdf:application/pdf},
  journal      = {International Journal of Forecasting},
  langid       = {english},
  shortjournal = {International Journal of Forecasting},
  shorttitle   = {Probabilistic electric load forecasting},
  year         = {2016},
}

@article{lee_individualized_2021,
	title = {Individualized Short-Term Electric Load Forecasting With Deep Neural Network Based Transfer Learning and Meta Learning},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3053317},
	abstract = {While the general belief is that the best way to predict electric load is through individualized models, the existing studies have focused on one-for-all models because the individual models are difficult to train and require a significantly larger data accumulation time per individual. In recent years, applying deep learning for forecasting electric load has become an important research topic but still one-for-all has been the main approach. In this work, we adopt transfer learning and meta learning that can be smoothly integrated into deep neural networks, and show how a high-performance individualized model can be formed using the individual's data collected over just several days. This is made possible by extracting the common patterns of many individuals using a sufficiently large dataset, and then customizing each individual model using the specific individual's small dataset. The proposed methods are evaluated over residential and non-residential datasets. When compared to the conventional methods, the meta learning model shows 7.84\% and 15.07\% {RMSE} improvements over the residential and non-residential datasets, respectively. Our results suggest that the individualized models can be used as effective tools for many short-term load forecasting tasks.},
	pages = {15413--15425},
	journal = {{IEEE} Access},
	author = {Lee, Eunjung and Rhee, Wonjong},
	date = {2021},
	note = {Conference Name: {IEEE} Access},
	keywords = {Predictive models, Task analysis, Load modeling, Data models, Deep learning, Load forecasting, Buildings, transfer learning, Adaptation models, individualized models, meta learning, short-term load forecasting},
	file = {IEEE Xplore Abstract Record:/Users/pemami/Zotero/storage/QBIL59VU/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/pemami/Zotero/storage/CEWN94Z2/Lee and Rhee - 2021 - Individualized Short-Term Electric Load Forecastin.pdf:application/pdf},
}

@Article{he_transferrable_2022,
  author       = {He, Yu and Luo, Fengji and Ranzi, Gianluca},
  journal = {{IEEE} Transactions on Power Systems},
  title        = {Transferrable Model-Agnostic Meta-learning for Short-Term Household Load Forecasting With Limited Training Data},
  doi          = {10.1109/TPWRS.2022.3169389},
  number       = {4},
  pages        = {3177--3180},
  volume       = {37},
  abstract     = {This letter proposes a transferrable model-agnostic meta-learning (T-{MAML}) approach for short-term load forecasting for single households. The proposed approach enables multiple households to collaboratively train a generic artificial neural network ({ANN}) model. The generic {ANN} model is then further trained at each target household node for the {STLF} purpose. The proposed T-{MAML} based {STLF} approach is featured by: (1) significant reduction of computation and communication costs on the household side; and (2) superior {STLF} performance, especially when there is limited load data for training in a target household. Experiments based on a real Australian residential dataset are conducted to validate the effectiveness of the proposed approach.},
  year         = {2022},
}

@Article{xu_hybrid_2020,
  author       = {Xu, Xianze and Meng, Zhaorui},
  journal = {Electrical Engineering},
  title        = {A hybrid transfer learning model for short-term electric load forecasting},
  doi          = {10.1007/s00202-020-00930-x},
  number       = {3},
  pages        = {1371--1381},
  url          = {http://link.springer.com/10.1007/s00202-020-00930-x},
  volume       = {102},
  langid       = {english},
  shortjournal = {Electr Eng},
  year         = {2020},
}

@Article{huy_short-term_2022,
  author  = {Huy, Pham Canh and Minh, Nguyen Quoc and Tien, Nguyen Dang and Anh, Tao Thi Quynh},
  title   = {Short-Term Electricity Load Forecasting Based on Temporal Fusion Transformer Model},
  doi     = {10.1109/ACCESS.2022.3211941},
  pages   = {106296--106304},
  url     = {https://ieeexplore.ieee.org/document/9910162/},
  urldate = {2023-05-19},
  volume  = {10},
  journal = {{IEEE} Access},
  langid  = {english},
  year    = {2022},
}

@report{granderson_assessment_2015,
	title = {Assessment of Automated Measurement and Verification (M\&V) Methods},
	url = {http://www.osti.gov/servlets/purl/1236174/},
	pages = {LBNL--187225, 1236174},
	number = {{LBNL}--187225, 1236174},
	author = {Granderson, Jessica and Touzani, Samir and Custodio, Claudine and Sohn, Michael and Fernandes, Samuel and Jump, David},
	urldate = {2023-05-19},
	date = {2015-07-01},
	langid = {english},
	doi = {10.2172/1236174},
	file = {Granderson et al. - 2015 - Assessment of Automated Measurement and Verificati.pdf:/Users/pemami/Zotero/storage/25ENFMBW/Granderson et al. - 2015 - Assessment of Automated Measurement and Verificati.pdf:application/pdf},
}

@TechReport{wilson2021end,
  author      = {Wilson, Eric JH and Parker, Andrew and Fontanini, Anthony and Present, Elaina and Reyna, Janet L and Adhikari, Rajendra and Bianchi, Carlo and CaraDonna, Christopher and Dahlhausen, Matthew and Kim, Janghyun and others},
  date        = {2021},
  institution = {National Renewable Energy Lab (NREL)},
  title       = {End-Use Load Profiles for the US Building Stock: Methodology and Results of Model Calibration, Validation, and Uncertainty Quantification},
  url         = {https://www.nrel.gov/docs/fy22osti/80889.pdf},
  comment     = {NREL/TP-5500-80889},
  year        = {2021},
}

@Misc{pecanstreet2021,
  author = {Pecan Street},
  title  = {Pecan Street Dataport},
  url    = {https://www. pecanstreet.org/dataport},
  year   = {2021},
}

@Article{gebru2021datasheets,
  author    = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  title     = {Datasheets for datasets},
  number    = {12},
  pages     = {86--92},
  volume    = {64},
  journal   = {Communications of the ACM},
  publisher = {ACM New York, NY, USA},
  year      = {2021},
}

@Misc{electricityloaddiagrams20112014_,
  author       = {Trindade,Artur},
  title        = {{ElectricityLoadDiagrams20112014}},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: \url{10.24432/C58C86}},
  year         = {2015},
}

@Misc{misc_individual_household_electric_power_consumption_235,
  author       = {Hebrail,Georges and Berard,Alice},
  title        = {{Individual household electric power consumption}},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: \url{10.24432/C58K54}},
  year         = {2012},
}

@Misc{Borealis,
  author    = {Omid Ardakanian and S. Huo and S. Keshav},
  title     = {{6-second load measurement dataset}},
  doi       = {10.5683/SP2/R4SVBF},
  url       = {https://doi.org/10.5683/SP2/R4SVBF},
  version   = {V1},
  publisher = {Borealis},
  year      = {2020},
}

@Article{pullinger2021ideal,
  author    = {Pullinger, Martin and Kilgour, Jonathan and Goddard, Nigel and Berliner, Niklas and Webb, Lynda and Dzikovska, Myroslava and Lovell, Heather and Mann, Janek and Sutton, Charles and Webb, Janette and others},
  title     = {The IDEAL household energy dataset, electricity, gas, contextual sensor data and survey data for 255 UK homes},
  number    = {1},
  pages     = {146},
  volume    = {8},
  journal   = {Scientific Data},
  publisher = {Nature Publishing Group UK London},
  year      = {2021},
}

@Misc{datastore2018smartmeter,
  author = {UK Power Networks},
  title  = {Smartmeter energy consumption data in london households},
  url    = {https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households},
  year   = {2018},
}

@Article{crawley2001energyplus,
  author    = {Crawley, Drury B and Lawrie, Linda K and Winkelmann, Frederick C and Buhl, Walter F and Huang, Y Joe and Pedersen, Curtis O and Strand, Richard K and Liesen, Richard J and Fisher, Daniel E and Witte, Michael J and others},
  title     = {EnergyPlus: creating a new-generation building energy simulation program},
  number    = {4},
  pages     = {319--331},
  volume    = {33},
  journal   = {Energy and buildings},
  publisher = {Elsevier},
  year      = {2001},
}

@Article{granderson2015automated,
  author    = {Granderson, Jessica and Price, Phillip N and Jump, David and Addy, Nathan and Sohn, Michael D},
  title     = {Automated measurement and verification: Performance of public domain whole-building electric baseline models},
  pages     = {106--113},
  volume    = {144},
  journal   = {Applied Energy},
  publisher = {Elsevier},
  year      = {2015},
}

@Article{hertel_transformer_2022,
  author  = {Hertel, Matthias and Ott, Simon and Neumann, Oliver and Sch{\"a}fer, Benjamin and Mikut, Ralf and Hagenmeyer, Veit},
  title   = {Transformer Neural Networks for Building Load Forecasting},
  journal = {Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022},
  year    = {2022},
}

@Article{zhang2022short,
  author    = {Zhang, Guangqi and Wei, Chuyuan and Jing, Changfeng and Wang, Yanxue},
  title     = {Short-Term Electrical Load Forecasting Based on Time Augmented Transformer},
  number    = {1},
  pages     = {67},
  volume    = {15},
  journal   = {International Journal of Computational Intelligence Systems},
  publisher = {Springer},
  year      = {2022},
}

@Article{pelekis2023comparative,
  author  = {Pelekis, Sotiris and Seisopoulos, Ioannis-Konstantinos and Spiliotis, Evangelos and Pountridis, Theodosios and Karakolis, Evangelos and Mouzakitis, Spiros and Askounis, Dimitris},
  title   = {A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers},
  journal = {arXiv preprint arXiv:2302.12168},
  year    = {2023},
}

@Article{eia2023,
  author = {EIA},
  title  = {Energy Information Administration May 2023 Monthly Energy Review},
  url    = {https://www.eia.gov/totalenergy/data/monthly/archive/00352305.pdf},
  year   = {2023},
}

@InProceedings{rathnayaka2022specialist,
  author       = {Rathnayaka, Prabod and Moraliyage, Harsha and Mills, Nishan and De Silva, Daswin and Jennings, Andrew},
  booktitle    = {2022 15th International Conference on Human System Interaction (HSI)},
  title        = {Specialist vs Generalist: A Transformer Architecture for Global Forecasting Energy Time Series},
  organization = {IEEE},
  pages        = {1--5},
  year         = {2022},
}

@InProceedings{wei2017deep,
  author    = {Wei, Tianshu and Wang, Yanzhi and Zhu, Qi},
  booktitle = {Proceedings of the 54th annual design automation conference 2017},
  title     = {Deep reinforcement learning for building HVAC control},
  pages     = {1--6},
  year      = {2017},
}

@Article{afram2014theory,
  author    = {Afram, Abdul and Janabi-Sharifi, Farrokh},
  title     = {Theory and applications of HVAC control systems--A review of model predictive control (MPC)},
  pages     = {343--355},
  volume    = {72},
  journal   = {Building and Environment},
  publisher = {Elsevier},
  year      = {2014},
}

@Article{suberu2014energy,
  author    = {Suberu, Mohammed Yekini and Mustafa, Mohd Wazir and Bashir, Nouruddeen},
  title     = {Energy storage systems for renewable energy power sector integration and mitigation of intermittency},
  pages     = {499--514},
  volume    = {35},
  journal   = {Renewable and Sustainable Energy Reviews},
  publisher = {Elsevier},
  year      = {2014},
}

@Article{drgovna2020all,
  author    = {Drgo{\v{n}}a, J{\'a}n and Arroyo, Javier and Figueroa, Iago Cupeiro and Blum, David and Arendt, Krzysztof and Kim, Donghun and Oll{\'e}, Enric Perarnau and Oravec, Juraj and Wetter, Michael and Vrabie, Draguna L and others},
  title     = {All you need to know about model predictive control for buildings},
  pages     = {190--232},
  volume    = {50},
  journal   = {Annual Reviews in Control},
  publisher = {Elsevier},
  year      = {2020},
}

@Article{iea2022,
  author = {IEA},
  title  = {Buildings},
  url    = {https://www.iea.org/reports/buildings},
  year   = {2022},
}

@Article{miller2020ashrae,
  author    = {Miller, Clayton and Arjunan, Pandarasamy and Kathirgamanathan, Anjukan and Fu, Chun and Roth, Jonathan and Park, June Young and Balbach, Chris and Gowri, Krishnan and Nagy, Zoltan and Fontanini, Anthony D and others},
  title     = {The ASHRAE great energy predictor III competition: Overview and results},
  number    = {10},
  pages     = {1427--1447},
  volume    = {26},
  journal   = {Science and Technology for the Built Environment},
  publisher = {Taylor \& Francis},
  year      = {2020},
}

@Article{mocanu2016deep,
  author    = {Mocanu, Elena and Nguyen, Phuong H and Gibescu, Madeleine and Kling, Wil L},
  title     = {Deep learning for estimating building energy consumption},
  pages     = {91--99},
  volume    = {6},
  journal   = {Sustainable Energy, Grids and Networks},
  publisher = {Elsevier},
  year      = {2016},
}

@Article{zeng2022transformers,
  author  = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  title   = {Are transformers effective for time series forecasting?},
  journal = {arXiv preprint arXiv:2205.13504},
  year    = {2022},
}

@INPROCEEDINGS{prev_day,
  author={Tsakoumis, A.C. and Vladov, S.S. and Mladenov, V.M.},
  booktitle={6th Seminar on Neural Network Applications in Electrical Engineering}, 
  title={Daily load forecasting based on previous day load}, 
  year={2002},
  volume={},
  number={},
  pages={83-86},
  doi={10.1109/NEUREL.2002.1057973}}

@Software{skforecast,
  author  = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},
  title   = {{skforecast}},
  url     = {https://github.com/JoaquinAmatRodrigo/skforecast},
  version = {0.8.1},
  license = {BSD 3-Clause License},
  month   = {5},
  year    = {2023},
}

@Article{lightgbm_lf,
author={Park, Sungwoo
and Jung, Seungmin
and Jung, Seungwon
and Rho, Seungmin
and Hwang, Eenjun},
title={Sliding window-based LightGBM model for electric load forecasting using anomaly repair},
journal={The Journal of Supercomputing},
year={2021},
month={Nov},
day={01},
volume={77},
number={11},
pages={12857-12878},
issn={1573-0484},
doi={10.1007/s11227-021-03787-4},
url={https://doi.org/10.1007/s11227-021-03787-4}
}

@Article{radford2022robust,
  author  = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title   = {Robust speech recognition via large-scale weak supervision},
  journal = {arXiv preprint arXiv:2212.04356},
  year    = {2022},
}

@Article{baevski2020wav2vec,
  author  = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  title   = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  pages   = {12449--12460},
  volume  = {33},
  journal = {Advances in neural information processing systems},
  year    = {2020},
}

@Article{zhang2022bigssl,
  author    = {Zhang, Yu and Park, Daniel S and Han, Wei and Qin, James and Gulati, Anmol and Shor, Joel and Jansen, Aren and Xu, Yuanzhong and Huang, Yanping and Wang, Shibo and others},
  title     = {Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition},
  number    = {6},
  pages     = {1519--1532},
  volume    = {16},
  journal   = {IEEE Journal of Selected Topics in Signal Processing},
  publisher = {IEEE},
  year      = {2022},
}

@Article{gneiting2014probabilistic,
  author    = {Gneiting, Tilmann and Katzfuss, Matthias},
  title     = {Probabilistic forecasting},
  pages     = {125--151},
  volume    = {1},
  journal   = {Annual Review of Statistics and Its Application},
  publisher = {Annual Reviews},
  year      = {2014},
}

@Article{razavi2019generating,
  author  = {Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
  title   = {Generating diverse high-fidelity images with vq-vae-2},
  volume  = {32},
  journal = {Advances in neural information processing systems},
  year    = {2019},
}

@Article{Hong2020,
  author   = {Tianzhen Hong and Zhe Wang and Xuan Luo and Wanni Zhang},
  title    = {State-of-the-art on research and applications of machine learning in the building life cycle},
  doi      = {https://doi.org/10.1016/j.enbuild.2020.109831},
  pages    = {109831},
  url      = {https://www.sciencedirect.com/science/article/pii/S0378778819337879},
  volume   = {212},
  abstract = {Fueled by big data, powerful and affordable computing resources, and advanced algorithms, machine learning has been explored and applied to buildings research for the past decades and has demonstrated its potential to enhance building performance. This study systematically surveyed how machine learning has been applied at different stages of building life cycle. By conducting a literature search on the Web of Knowledge platform, we found 9579 papers in this field and selected 153 papers for an in-depth review. The number of published papers is increasing year by year, with a focus on building design, operation, and control. However, no study was found using machine learning in building commissioning. There are successful pilot studies on fault detection and diagnosis of HVAC equipment and systems, load prediction, energy baseline estimate, load shape clustering, occupancy prediction, and learning occupant behaviors and energy use patterns. None of the existing studies were adopted broadly by the building industry, due to common challenges including (1) lack of large scale labeled data to train and validate the model, (2) lack of model transferability, which limits a model trained with one data-rich building to be used in another building with limited data, (3) lack of strong justification of costs and benefits of deploying machine learning, and (4) the performance might not be reliable and robust for the stated goals, as the method might work for some buildings but could not be generalized to others. Findings from the study can inform future machine learning research to improve occupant comfort, energy efficiency, demand flexibility, and resilience of buildings, as well as to inspire young researchers in the field to explore multidisciplinary approaches that integrate building science, computing science, data science, and social science.},
  journal  = {Energy and Buildings},
  keywords = {Machine learning, Artificial intelligence, Buildings, Building life cycle, Building control, Building performance},
  year     = {2020},
}

@Article{kaplan2020scaling,
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title   = {Scaling laws for neural language models},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020},
}

@Article{hernandez2021scaling,
  author  = {Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  title   = {Scaling laws for transfer},
  journal = {arXiv preprint arXiv:2102.01293},
  year    = {2021},
}

@Article{johnson2019billion,
  author    = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  title     = {Billion-scale similarity search with {GPUs}},
  number    = {3},
  pages     = {535--547},
  volume    = {7},
  journal   = {IEEE Transactions on Big Data},
  publisher = {IEEE},
  year      = {2019},
}

@Article{ke2017lightgbm,
  author  = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  title   = {Lightgbm: A highly efficient gradient boosting decision tree},
  volume  = {30},
  journal = {Advances in neural information processing systems},
  year    = {2017},
}

@Article{box1964analysis,
  author    = {Box, George EP and Cox, David R},
  title     = {An analysis of transformations},
  number    = {2},
  pages     = {211--243},
  volume    = {26},
  journal   = {Journal of the Royal Statistical Society: Series B (Methodological)},
  publisher = {Wiley Online Library},
  year      = {1964},
}

@Article{ePlus,
  author = {{National Renewable Energy Lab}},
  title  = {EnergyPlus},
  url    = {https://energyplus.net/},
  year   = {2023},
}

@Article{stephen2015incorporating,
  author    = {Stephen, Bruce and Tang, Xiaoqing and Harvey, Poppy R and Galloway, Stuart and Jennett, Kyle I},
  title     = {Incorporating practice theory in sub-profile models for short term aggregated residential load forecasting},
  number    = {4},
  pages     = {1591--1598},
  volume    = {8},
  journal   = {IEEE Transactions on Smart Grid},
  publisher = {IEEE},
  year      = {2015},
}

@Article{HONG2014357,
  author   = {Tao Hong and Pierre Pinson and Shu Fan},
  title    = {Global Energy Forecasting Competition 2012},
  doi      = {https://doi.org/10.1016/j.ijforecast.2013.07.001},
  number   = {2},
  pages    = {357-363},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169207013000745},
  volume   = {30},
  abstract = {The Global Energy Forecasting Competition (GEFCom2012) attracted hundreds of participants worldwide, who contributed many novel ideas to the energy forecasting field. This paper introduces both tracks of GEFCom2012, hierarchical load forecasting and wind power forecasting, with details on the aspects of the problem, the data, and a summary of the methods used by selected top entries. We also discuss the lessons learned from this competition from the organizers’ perspective. The complete data set, including the solution data, is published along with this paper, in an effort to establish a benchmark data pool for the community.},
  issn     = {0169-2070},
  journal  = {International Journal of Forecasting},
  year     = {2014},
}

@InCollection{torch,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  title     = {{P}y{T}orch: {A}n {I}mperative {S}tyle, {H}igh-{P}erformance {D}eep {L}earning {L}ibrary},
  pages     = {8024--8035},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  year      = {2019},
}

@Article{radford2018improving,
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  title     = {Improving language understanding by generative pre-training},
  publisher = {OpenAI},
  year      = {2018},
}

@InProceedings{loshchilovdecoupled2019,
  author    = {Loshchilov, Ilya and Hutter, Frank},
  booktitle = {International Conference on Learning Representations},
  title     = {Decoupled Weight Decay Regularization},
  year      = {2019},
}

@Article{hendrycks2016bridging,
  author  = {Hendrycks, Dan and Gimpel, Kevin},
  title   = {Bridging nonlinearities and stochastic regularizers with gaussian error linear units},
  journal = {arXiv preprint arXiv:1606.08415},
  year    = {2016},
}

@Article{gage1994new,
  author    = {Gage, Philip},
  title     = {A new algorithm for data compression},
  number    = {2},
  pages     = {23--38},
  volume    = {12},
  journal   = {C Users Journal},
  publisher = {McPherson, KS: R \& D Publications, c1987-1994.},
  year      = {1994},
}

@Article{hochreiter1997long,
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  title     = {Long short-term memory},
  number    = {8},
  pages     = {1735--1780},
  volume    = {9},
  journal   = {Neural computation},
  publisher = {MIT press},
  year      = {1997},
}

@InProceedings{wu2023timesnet,
  author    = {Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
  booktitle = {International Conference on Learning Representations},
  title     = {TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  year      = {2023},
}

@InProceedings{knox1998algorithms,
  author       = {Knox, Edwin M and Ng, Raymond T},
  booktitle    = {Proceedings of the international conference on very large data bases},
  title        = {Algorithms for mining distancebased outliers in large datasets},
  organization = {Citeseer},
  pages        = {392--403},
  year         = {1998},
}

@Article{zhang2021high,
  author    = {Zhang, Liang and Plathottam, Siby and Reyna, Janet and Merket, Noel and Sayers, Kevin and Yang, Xinshuo and Reynolds, Matthew and Parker, Andrew and Wilson, Eric and Fontanini, Anthony and others},
  title     = {High-resolution hourly surrogate modeling framework for physics-based large-scale building stock modeling},
  pages     = {103292},
  volume    = {75},
  journal   = {Sustainable Cities and Society},
  publisher = {Elsevier},
  year      = {2021},
}

@Comment{jabref-meta: databaseType:biblatex;}
