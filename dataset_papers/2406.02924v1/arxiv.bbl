\begin{thebibliography}{91}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Jacobs, and etc.]{Abdin2024Phi3TR}
Abdin, M., Jacobs, S.~A., and etc., A. A.~A.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{ArXiv}, abs/2404.14219, 2024.

\bibitem[Akhauri et~al.(2022)Akhauri, Munoz, Jain, and Iyer]{akhauri2022eznas}
Akhauri, Y., Munoz, J.~P., Jain, N., and Iyer, R.
\newblock {EZNAS}: Evolving zero-cost proxies for neural architecture scoring.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{NeurIPS}, 2022.

\bibitem[Ashkboos et~al.(2024)Ashkboos, Croci, do~Nascimento, Hoefler, and Hensman]{slicegpt_iclr24}
Ashkboos, S., Croci, M.~L., do~Nascimento, M.~G., Hoefler, T., and Hensman, J.
\newblock Slice{GPT}: Compress large language models by deleting rows and columns.
\newblock In \emph{ICLR}, 2024.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and Carbin]{chen2020lottery}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2023)Chen, Liang, DING, Zhu, and Zharkov]{chen2023otov}
Chen, T., Liang, L., DING, T., Zhu, Z., and Zharkov, I.
\newblock {OTO}v2: Automatic, generic, user-friendly.
\newblock In \emph{ICLR}, 2023.

\bibitem[Chijiwa et~al.(2021)Chijiwa, Yamaguchi, Ida, Umakoshi, and INOUE]{NEURIPS2021_23e582ad}
Chijiwa, D., Yamaguchi, S.~y., Ida, Y., Umakoshi, K., and INOUE, T.
\newblock Pruning randomly initialized neural networks with iterative randomization.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{NeurIPS}, volume~34, pp.\  4503--4513. Curran Associates, Inc., 2021.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{Clark2019BoolQET}
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{ArXiv}, abs/1905.10044, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{Clark2018ThinkYH}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{ArXiv}, abs/1803.05457, 2018.

\bibitem[Das et~al.(2023)Das, Ma, and Shen]{Das2023BeyondSH_GBLM}
Das, R.~J., Ma, L., and Shen, Z.
\newblock Beyond size: How gradients shape pruning decisions in large language models.
\newblock \emph{ArXiv}, abs/2311.04902, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Diao et~al.(2023)Diao, Wang, Zhang, Yang, Ding, and Tarokh]{diao2023pruning}
Diao, E., Wang, G., Zhang, J., Yang, Y., Ding, J., and Tarokh, V.
\newblock Pruning deep neural networks from a sparsity perspective.
\newblock In \emph{ICLR}, 2023.

\bibitem[Dong et~al.(2022)Dong, Niu, Li, Xie, Zou, Ye, Wei, and Pan]{linas2}
Dong, P., Niu, X., Li, L., Xie, L., Zou, W., Ye, T., Wei, Z., and Pan, H.
\newblock Prior-guided one-shot neural architecture search.
\newblock \emph{arXiv preprint arXiv:2206.13329}, 2022.

\bibitem[Dong et~al.(2023{\natexlab{a}})Dong, Li, and Wei]{Dong2023diswot}
Dong, P., Li, L., and Wei, Z.
\newblock Diswot: Student architecture search for distillation without training.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Dong et~al.(2023{\natexlab{b}})Dong, Li, Wei, Niu, Tian, and Pan]{dong2023emq}
Dong, P., Li, L., Wei, Z., Niu, X., Tian, Z., and Pan, H.
\newblock Emq: Evolving training-free proxies for automated mixed precision quantization.
\newblock In \emph{ICCV}, pp.\  17076--17086, 2023{\natexlab{b}}.

\bibitem[Dong et~al.(2024)Dong, Li, Pan, Wei, Liu, Wang, and Chu]{dong2024parzc}
Dong, P., Li, L., Pan, X., Wei, Z., Liu, X., Wang, Q., and Chu, X.
\newblock Parzc: Parametric zero-cost proxies for efficient nas.
\newblock \emph{arXiv preprint arXiv:2402.02105}, 2024.

\bibitem[Fang et~al.(2023)Fang, Ma, Song, Mi, and Wang]{Fang2023DepGraphTA}
Fang, G., Ma, X., Song, M., Mi, M.~B., and Wang, X.
\newblock Depgraph: Towards any structural pruning.
\newblock \emph{CVPR}, pp.\  16091--16101, 2023.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{Frantar2023SparseGPTML}
Frantar, E. and Alistarh, D.
\newblock Sparsegpt: Massive language models can be accurately pruned in one-shot.
\newblock In \emph{ICML}, 2023.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{Frantar2022GPTQAP}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock In \emph{ICLR}, 2023.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, September 2021.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015LearningBW}
Han, S., Pool, J., Tran, J., and Dally, W.~J.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Han et~al.(2016{\natexlab{a}})Han, Mao, and Dally]{han2015deep_compression_magnitude}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.
\newblock \emph{ICLR}, 2016{\natexlab{a}}.

\bibitem[Han et~al.(2016{\natexlab{b}})Han, Mao, and Dally]{song2016deep_compression}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.
\newblock In \emph{ICLR}, 2016{\natexlab{b}}.

\bibitem[He et~al.(2022)He, Yao, Wang, Tang, Cheung, See, Han, and Chu]{He2022NASLIDEN}
He, X., Yao, J., Wang, Y., Tang, Z., Cheung, K.~C., See, S., Han, B., and Chu, X.
\newblock Nas-lid: Efficient neural architecture search with local intrinsic dimension.
\newblock In \emph{AAAI}, 2022.

\bibitem[Hoang et~al.(2023)Hoang, Liu, Marculescu, and Wang]{hoang2023revisiting}
Hoang, D.~N., Liu, S., Marculescu, R., and Wang, Z.
\newblock revisiting pruning at initialization through the lens of ramanujan graph.
\newblock In \emph{ICLR}, 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{ICLR}, 2022.

\bibitem[Hu et~al.(2021)Hu, Wang, Li, and Gu]{hu2021improving}
Hu, Y., Wang, X., Li, L., and Gu, Q.
\newblock Improving one-shot nas with shrinking-and-expanding supernet.
\newblock \emph{Pattern Recognition}, 2021.

\bibitem[Koza(1994)]{Koza1994GeneticPA}
Koza, J.~R.
\newblock Genetic programming as a means for programming computers by natural selection.
\newblock \emph{Statistics and Computing}, 4:\penalty0 87--112, 1994.

\bibitem[Kwon et~al.(2022)Kwon, Kim, Mahoney, Hassoun, Keutzer, and Gholami]{kwon2022a}
Kwon, W., Kim, S., Mahoney, M.~W., Hassoun, J., Keutzer, K., and Gholami, A.
\newblock A fast post-training pruning framework for transformers.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{NeurIPS}, 2022.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{Lee2018SNIPSN}
Lee, N., Ajanthan, T., and Torr, P.~H.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{ICLR}, 2019.

\bibitem[Li(2022)]{li2022self}
Li, L.
\newblock Self-regulated feature learning via teacher-free feature distillation.
\newblock In \emph{ECCV}, 2022.

\bibitem[Li \& Jin(2022)Li and Jin]{lishadow}
Li, L. and Jin, Z.
\newblock Shadow knowledge distillation: Bridging offline and online knowledge transfer.
\newblock In \emph{NeuIPS}, 2022.

\bibitem[Li et~al.(2023)Li, Dong, Wei, and Yang]{li2023auto}
Li, L., Dong, P., Wei, Z., and Yang, Y.
\newblock Automated knowledge distillation via monte carlo tree search.
\newblock In \emph{ICCV}, 2023.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Dong, Li, Wei, and Yang]{li2024kd}
Li, L., Dong, P., Li, A., Wei, Z., and Yang, Y.
\newblock Kd-zero: Evolving knowledge distiller for any teacher-student pairs.
\newblock \emph{NeuIPS}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Han, and Bai]{Li2024NutePruneEP}
Li, S., Han, X., and Bai, J.
\newblock Nuteprune: Efficient progressive pruning with numerous teachers for large language models.
\newblock \emph{ArXiv}, abs/2402.09773, 2024{\natexlab{b}}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Ning, Wang, Liu, Shi, Yan, Dai, Yang, and Wang]{Li2024EvaluatingQL}
Li, S., Ning, X., Wang, L., Liu, T., Shi, X., Yan, S., Dai, G., Yang, H., and Wang, Y.
\newblock Evaluating quantized large language models.
\newblock \emph{ICML}, abs/2402.18158, 2024{\natexlab{c}}.

\bibitem[Lin et~al.(2024)Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan, and Han]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock In \emph{MLSys}, 2024.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Liu, Huang, Dong, and Cheng]{liu-etal-2023-llm-fp4}
Liu, S.-y., Liu, Z., Huang, X., Dong, P., and Cheng, K.-T.
\newblock {LLM}-{FP}4: 4-bit floating-point quantized transformers.
\newblock In \emph{EMNLP}, pp.\  592--605, Singapore, December 2023{\natexlab{a}}. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.39}.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{ICLR}, 2019.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi, Krishnamoorthi, and Chandra]{liu2023llm_qat}
Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V.
\newblock Llm-qat: Data-free quantization aware training for large language models.
\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023{\natexlab{b}}.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2018learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through l\_0 regularization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Lu et~al.(2024)Lu, Chen, Lu, Rao, Li, and Pang]{lu2024uniads}
Lu, L., Chen, Z., Lu, X., Rao, Y., Li, L., and Pang, S.
\newblock Uniads: Universal architecture-distiller search for distillation gap.
\newblock In \emph{AAAI}, 2024.

\bibitem[Lu et~al.(2022)Lu, Luo, Chen, Chen, Liu, and Wang]{miao2022learning}
Lu, M., Luo, X., Chen, T., Chen, W., Liu, D., and Wang, Z.
\newblock Learning pruning-friendly networks via frank-wolfe: One-shot, any-sparsity, and no retraining.
\newblock In \emph{ICLR}, 2022.

\bibitem[Luccioni et~al.(2023)Luccioni, Viguier, and Ligozat]{luccioni2023estimating}
Luccioni, A.~S., Viguier, S., and Ligozat, A.-L.
\newblock Estimating the carbon footprint of bloom, a 176b parameter language model.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (253):\penalty0 1--15, 2023.

\bibitem[Ma et~al.(2023)Ma, Fang, and Wang]{Ma2023LLMPrunerOT}
Ma, X., Fang, G., and Wang, X.
\newblock Llm-pruner: On the structural pruning of large language models.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Martius \& Lampert(2016)Martius and Lampert]{Martius2016ExtrapolationAL}
Martius, G. and Lampert, C.~H.
\newblock Extrapolation and learning equations.
\newblock \emph{ArXiv}, abs/1610.02995, 2016.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{merity2017pointer_wikitext2}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Byj72udxe}.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{Mihaylov2018CanAS}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[ModelTC(2023)]{lightllm}
ModelTC.
\newblock Lightllm.
\newblock \url{https://github.com/ModelTC/lightllm}, 2023.
\newblock A Python-based LLM inference and serving framework.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean]{ENAS}
Pham, H., Guan, M.~Y., Zoph, B., Le, Q.~V., and Dean, J.
\newblock Efficient neural architecture search via parameter sharing.
\newblock In \emph{ICML}, 2018.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2019t5_c4dataset}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{arXiv e-prints}, 2019.

\bibitem[Real et~al.(2020)Real, Liang, So, and Le]{real2020automlzero}
Real, E., Liang, C., So, D., and Le, Q.
\newblock Automl-zero: Evolving machine learning algorithms from scratch.
\newblock In \emph{ICML}, 2020.

\bibitem[Ren \& Zhu(2023)Ren and Zhu]{ren-zhu-2023-pruning}
Ren, S. and Zhu, K.
\newblock Pruning pre-trained language models with principled importance and self-regularization.
\newblock In \emph{ACL}, pp.\  8995--9008, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.573}.

\bibitem[Sahoo et~al.(2018)Sahoo, Lampert, and Martius]{Sahoo2018LearningEF}
Sahoo, S.~S., Lampert, C.~H., and Martius, G.
\newblock Learning equations for extrapolation and control.
\newblock In \emph{ICML}, 2018.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{Sakaguchi2019WinoGrande}
Sakaguchi, K., Bras, R.~L., Bhagavatula, C., and Choi, Y.
\newblock Winogrande.
\newblock \emph{Communications of the ACM}, 64:\penalty0 99--106, 2019.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Sanh, V., Wolf, T., and Rush, A.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{NeurIPS}, 33:\penalty0 20378--20389, 2020.

\bibitem[Schwartz et~al.(2020)Schwartz, Dodge, Smith, and Etzioni]{schwartz2020green}
Schwartz, R., Dodge, J., Smith, N.~A., and Etzioni, O.
\newblock Green ai.
\newblock \emph{Communications of the ACM}, 63\penalty0 (12):\penalty0 54--63, 2020.

\bibitem[Sharma et~al.(2024)Sharma, Ash, and Misra]{sharma2024the}
Sharma, P., Ash, J.~T., and Misra, D.
\newblock The truth is in there: Improving reasoning in language models with layer-selective rank reduction.
\newblock In \emph{ICLR}, 2024.

\bibitem[Sreenivasan et~al.(2022)Sreenivasan, yong Sohn, Yang, Grinde, Nagle, Wang, Xing, Lee, and Papailiopoulos]{sreenivasan2022rare}
Sreenivasan, K., yong Sohn, J., Yang, L., Grinde, M., Nagle, A., Wang, H., Xing, E., Lee, K., and Papailiopoulos, D.
\newblock Rare gems: Finding lottery tickets at initialization.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{NeurIPS}, 2022.

\bibitem[Srinivas \& Babu(2015)Srinivas and Babu]{Srinivas2015DatafreePP}
Srinivas, S. and Babu, R.~V.
\newblock Data-free parameter pruning for deep neural networks.
\newblock In \emph{BMVC}, 2015.

\bibitem[Sun et~al.(2024)Sun, Liu, Bair, and Kolter]{Sun2023ASA_wanda}
Sun, M., Liu, Z., Bair, A., and Kolter, J.~Z.
\newblock A simple and effective pruning approach for large language models.
\newblock In \emph{ICLR}, 2024.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving synaptic flow.
\newblock \emph{NeurIPS}, 33:\penalty0 6377--6389, 2020.

\bibitem[Tang et~al.(2019)Tang, Wang, Wang, and Chu]{tangDVFS}
Tang, Z., Wang, Y., Wang, Q., and Chu, X.
\newblock The impact of gpu dvfs on the energy and performance of deep learning: An empirical study.
\newblock In \emph{Proceedings of the Tenth ACM International Conference on Future Energy Systems}, e-Energy '19, pp.\  315–325, New York, NY, USA, 2019. Association for Computing Machinery.
\newblock ISBN 9781450366717.
\newblock \doi{10.1145/3307772.3328315}.

\bibitem[Tang et~al.(2020)Tang, Shi, Chu, Wang, and Li]{tang2020survey}
Tang, Z., Shi, S., Chu, X., Wang, W., and Li, B.
\newblock Communication-efficient distributed deep learning: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2003.06307}, 2020.

\bibitem[Tang et~al.(2023)Tang, Wang, He, Zhang, Pan, Wang, Zeng, Zhao, Shi, He, et~al.]{tang2023fusionai}
Tang, Z., Wang, Y., He, X., Zhang, L., Pan, X., Wang, Q., Zeng, R., Zhao, K., Shi, S., He, B., et~al.
\newblock Fusionai: Decentralized training and deploying llms with massive consumer-level gpus.
\newblock \emph{arXiv preprint arXiv:2309.01172}, 2023.

\bibitem[Tang et~al.(2024)Tang, Zhang, Shi, Tian, Liu, Han, and Chu]{tang2024fedimpro}
Tang, Z., Zhang, Y., Shi, S., Tian, X., Liu, T., Han, B., and Chu, X.
\newblock Fedimpro: Measuring and improving client update in federated learning.
\newblock In \emph{ICLR}, 2024.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[van~der Ouderaa et~al.(2024)van~der Ouderaa, Nagel, van Baalen, Asano, and Blankevoort]{Ouderaa2023TheLS}
van~der Ouderaa, T. F.~A., Nagel, M., van Baalen, M., Asano, Y.~M., and Blankevoort, T.
\newblock The llm surgeon.
\newblock In \emph{ICLR}, 2024.

\bibitem[Virgolin \& Pissis(2022)Virgolin and Pissis]{virgolin2022symbolic}
Virgolin, M. and Pissis, S.~P.
\newblock Symbolic regression is np-hard.
\newblock \emph{arXiv preprint arXiv:2207.01018}, 2022.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{Wang2018GLUEAM}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{BlackboxNLP@EMNLP}, 2018.

\bibitem[Wang et~al.(2020)Wang, Qin, Zhang, and Fu]{Wang2020NeuralPV}
Wang, H., Qin, C., Zhang, Y., and Fu, Y.~R.
\newblock Neural pruning via growing regularization.
\newblock \emph{ICLR}, abs/2012.09243, 2020.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_VjQlMeSB_J}.

\bibitem[Wei et~al.(2023)Wei, Pan, Li, Dong, Tian, Niu, and Li]{wei2023tvt}
Wei, Z., Pan, H., Li, L., Dong, P., Tian, Z., Niu, X., and Li, D.
\newblock Tvt: Training-free vision transformer search on tiny datasets.
\newblock \emph{arXiv preprint arXiv:2311.14337}, 2023.

\bibitem[Wei et~al.(2024)Wei, Dong, Hui, Li, Li, Lu, Pan, and Li]{wei2024auto}
Wei, Z., Dong, P., Hui, Z., Li, A., Li, L., Lu, M., Pan, H., and Li, D.
\newblock Auto-prox: Training-free vision transformer architecture search via automatic proxy discovery.
\newblock In \emph{AAAI}, 2024.

\bibitem[Werner et~al.(2021)Werner, Junginger, Hennig, and Martius]{Werner2021InformedEL}
Werner, M., Junginger, A., Hennig, P., and Martius, G.
\newblock Informed equation learning.
\newblock \emph{ArXiv}, abs/2105.06331, 2021.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
\newblock {S}mooth{Q}uant: Accurate and efficient post-training quantization for large language models.
\newblock In \emph{ICML}, 2023.

\bibitem[Xiaolong et~al.(2023)Xiaolong, Lujun, Chao, and Yao]{li2022norm}
Xiaolong, L., Lujun, L., Chao, L., and Yao, A.
\newblock Norm: Knowledge distillation via n-to-one representation matching.
\newblock In \emph{ICLR}, 2023.

\bibitem[Xu et~al.(2024)Xu, Shao, Chen, Tang, Zhang, Gao, An, Qiao, and Luo]{xu2024besa}
Xu, P., Shao, W., Chen, M., Tang, S., Zhang, K., Gao, P., An, F., Qiao, Y., and Luo, P.
\newblock {BESA}: Pruning large language models with blockwise parameter-efficient sparsity allocation.
\newblock In \emph{ICLR}, 2024.

\bibitem[Yang et~al.(2022)Yang, Cui, Yao, and Wang]{Yang2022GradientbasedIP}
Yang, Z., Cui, Y., Yao, X., and Wang, S.
\newblock Gradient-based intra-attention pruning on pre-trained language models.
\newblock In \emph{ACL}, 2022.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, Wu, Li, and He]{NEURIPS2022_adf7fa39_zero_quant}
Yao, Z., Yazdani~Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{NeurIPS}, volume~35, pp.\  27168--27183. Curran Associates, Inc., 2022.

\bibitem[Yin et~al.(2024)Yin, Wu, Zhang, Hsieh, Wang, Jia, Pechenizkiy, Liang, Wang, and Liu]{yin2024outlier_owl}
Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia, Y., Pechenizkiy, M., Liang, Y., Wang, Z., and Liu, S.
\newblock Outlier weighed layerwise sparsity ({OWL}): A missing secret sauce for pruning {LLM}s to high sparsity.
\newblock In \emph{ICML}, 2024.

\bibitem[Yvinec et~al.(2021)Yvinec, Dapogny, Cord, and Bailly]{Yvinec2021REDD}
Yvinec, E., Dapogny, A., Cord, M., and Bailly, K.
\newblock Red++ : Data-free pruning of deep neural networks via input splitting and output merging.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45:\penalty0 3664--3676, 2021.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{Zellers2019HellaSwagCA}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{ACL}, 2019.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Zeng, Wang, and Lu]{Zhang2024TinyLlamaAO}
Zhang, P., Zeng, G., Wang, T., and Lu, W.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{ArXiv}, abs/2401.02385, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Zuo, Liang, Bukharin, He, Chen, and Zhao]{zhang2022platon}
Zhang, Q., Zuo, S., Liang, C., Bukharin, A., He, P., Chen, W., and Zhao, T.
\newblock Platon: Pruning large transformer models with upper confidence bound of weight importance.
\newblock In \emph{ICML}, pp.\  26809--26823. PMLR, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{Zhang2022OPTOP}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M.~T., Li, X., Lin, X.~V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P.~S., Sridhar, A., Wang, T., and Zettlemoyer, L.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{ArXiv}, abs/2205.01068, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Bai, Lin, Zhao, Hou, and Cannistraci]{zhang2024plugandplay}
Zhang, Y., Bai, H., Lin, H., Zhao, J., Hou, L., and Cannistraci, C.~V.
\newblock Plug-and-play: An efficient post-training pruning method for large language models.
\newblock In \emph{ICLR}, 2024{\natexlab{b}}.

\bibitem[Zhu et~al.(2024)Zhu, Li, Wu, and Sun]{zhu2024saswot}
Zhu, C., Li, L., Wu, Y., and Sun, Z.
\newblock Saswot: Real-time semantic segmentation architecture search without training.
\newblock In \emph{AAAI}, 2024.

\bibitem[Zimmer et~al.(2023)Zimmer, Andoni, Spiegel, and Pokutta]{Zimmer2023PERPRT}
Zimmer, M., Andoni, M., Spiegel, C., and Pokutta, S.
\newblock Perp: Rethinking the prune-retrain paradigm in the era of llms.
\newblock \emph{ArXiv}, abs/2312.15230, 2023.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{nasnet}
Zoph, B., Vasudevan, V., Shlens, J., and Le, Q.~V.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{CVPR}, 2018.

\end{thebibliography}
