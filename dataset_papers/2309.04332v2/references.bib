@inproceedings{NEURIPS2018_0e98aeeb,
 author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Bias of Gradient Descent on Linear Convolutional Networks},
 volume = {31},
 year = {2018}
}

@misc{graphsage,
  doi = {10.48550/ARXIV.1706.02216},
  
  url = {https://arxiv.org/abs/1706.02216},
  
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  
  keywords = {Social and Information Networks (cs.SI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Inductive Representation Learning on Large Graphs},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{rashmi2015dart,
      title={DART: Dropouts meet Multiple Additive Regression Trees}, 
      author={K. V. Rashmi and Ran Gilad-Bachrach},
      year={2015},
      eprint={1505.01866},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{settrees,
  title = 	 {Trees with Attention for Set Prediction Tasks},
  author =       {Hirsch, Roy and Gilad-Bachrach, Ran},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4250--4261},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hirsch21a/hirsch21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hirsch21a.html},
  abstract = 	 {In many machine learning applications, each record represents a set of items. For example, when making predictions from medical records, the medications prescribed to a patient are a set whose size is not fixed and whose order is arbitrary. However, most machine learning algorithms are not designed to handle set structures and are limited to processing records of fixed size. Set-Tree, presented in this work, extends the support for sets to tree-based models, such as Random-Forest and Gradient-Boosting, by introducing an attention mechanism and set-compatible split criteria. We evaluate the new method empirically on a wide range of problems ranging from making predictions on sub-atomic particle jets to estimating the redshift of galaxies. The new method outperforms existing tree-based methods consistently and significantly. Moreover, it is competitive and often outperforms Deep Learning. We also discuss the theoretical properties of Set-Trees and explain how they enable item-level explainability.}
}

@inproceedings{10.1145/3292500.3330676,
author = {Li, Pan and Qin, Zhen and Wang, Xuanhui and Metzler, Donald},
title = {Combining Decision Trees and Neural Networks for Learning-to-Rank in Personal Search},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330676},
doi = {10.1145/3292500.3330676},
abstract = {Decision Trees (DTs) like LambdaMART have been one of the most effective types of learning-to-rank algorithms in the past decade. They typically work well with hand-crafted dense features (e.g., BM25 scores). Recently, Neural Networks (NNs) have shown impressive results in leveraging sparse and complex features (e.g., query and document keywords) directly when a large amount of training data is available. While there is a large body of work on how to use NNs for semantic matching between queries and documents, relatively less work has been conducted to compare NNs with DTs for general learning-to-rank tasks, where dense features are also available and DTs can achieve state-of-the-art performance. In this paper, we study how to combine DTs and NNs to effectively bring the benefits from both sides in the learning-to-rank setting. Specifically, we focus our study on personal search where clicks are used as the primary labels with unbiased learning-to-rank algorithms and a significantly large amount of training data is easily available. Our combination methods are based on ensemble learning. We design 12 variants and compare them based on two aspects, ranking effectiveness and ease-of-deployment, using two of the largest personal search services: Gmail search and Google Drive search. We show that direct application of existing ensemble methods can not achieve both aspects. We thus design a novel method that uses NNs to compensate DTs via boosting. We show that such a method is not only easier to deploy, but also gives comparable or better ranking accuracy.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2032–2040},
numpages = {9},
keywords = {learning to rank, ensemble methods, decision trees, personal search, neural networks},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article {mutagenicaffect,
	Title = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity},
	Author = {Debnath, AK and Lopez de Compadre, RL and Debnath, G and Shusterman, AJ and Hansch, C},
	DOI = {10.1021/jm00106a046},
	Number = {2},
	Volume = {34},
	Month = {February},
	Year = {1991},
	Journal = {Journal of medicinal chemistry},
	ISSN = {0022-2623},
	Pages = {786—797},
	Abstract = {A review of the literature yielded data on over 200 aromatic and heteroaromatic nitro compounds tested for mutagenicity in the Ames test using S. typhimurium TA98. From the data, a quantitative structure-activity relationship (QSAR) has been derived for 188 congeners. The main determinants of mutagenicity are the hydrophobicity (modeled by octanol/water partition coefficients) and the energies of the lowest unoccupied molecular orbitals calculated using the AM1 method. It is also shown that chemicals possessing three or more fused rings possess much greater mutagenic potency than compounds with one or two fused rings. Since the QSAR is based on a very wide range in structural variation, aromatic rings from benzene to coronene are included as well as many different types of heterocycles, it is a significant step toward a predictive toxicology of value in the design of less mutagenic bioactive compounds.},
	URL = {https://doi.org/10.1021/jm00106a046},
}












@article{xgraphboost,
author = {Deng, Daiguo and Chen, Xiaowei and Zhang, Ruochi and Lei, Zengrong and Wang, Xiaojian and Zhou, Fengfeng},
title = {XGraphBoost: Extracting Graph Neural Network-Based Features for a Better Prediction of Molecular Properties},
journal = {Journal of Chemical Information and Modeling},
volume = {61},
number = {6},
pages = {2697-2705},
year = {2021},
doi = {10.1021/acs.jcim.0c01489},
    note ={PMID: 34009965},

URL = { 
        https://doi.org/10.1021/acs.jcim.0c01489
    
},
eprint = { 
        https://doi.org/10.1021/acs.jcim.0c01489
    
}

}

@article{STOKES2020688,
title = {A Deep Learning Approach to Antibiotic Discovery},
journal = {Cell},
volume = {180},
number = {4},
pages = {688-702.e13},
year = {2020},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2020.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0092867420301021},
author = {Jonathan M. Stokes and Kevin Yang and Kyle Swanson and Wengong Jin and Andres Cubillos-Ruiz and Nina M. Donghia and Craig R. MacNair and Shawn French and Lindsey A. Carfrae and Zohar Bloom-Ackermann and Victoria M. Tran and Anush Chiappino-Pepe and Ahmed H. Badran and Ian W. Andrews and Emma J. Chory and George M. Church and Eric D. Brown and Tommi S. Jaakkola and Regina Barzilay and James J. Collins},
keywords = {antibiotics, antibiotic resistance, antibiotic tolerance, machine learning, drug discovery},
abstract = {Summary
Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub—halicin—that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from >107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules.}
}

@article{friedman2000greedy,
  added-at = {2012-10-21T11:58:58.000+0200},
  author = {Friedman, Jerome H.},
  biburl = {https://www.bibsonomy.org/bibtex/237ca72b4c7f9383050b7c50da4356802/nosebrain},
  interhash = {11df73eca7ccfc0e74d70d3de429965c},
  intrahash = {37ca72b4c7f9383050b7c50da4356802},
  journal = {Annals of Statistics},
  keywords = {MART approximation boosting function greedy},
  pages = {1189--1232},
  timestamp = {2012-10-21T12:01:26.000+0200},
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  volume = 29,
  year = 2000
}

@article{StochasticGradientBoosting,
author = {Friedman, Jerome H.},
title = {Stochastic Gradient Boosting},
year = {2002},
issue_date = {28 February 2002},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {4},
issn = {0167-9473},
url = {https://doi.org/10.1016/S0167-9473(01)00065-2},
doi = {10.1016/S0167-9473(01)00065-2},
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo'-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
journal = {Comput. Stat. Data Anal.},
month = {feb},
pages = {367–378},
numpages = {12}
}

@misc{patel2021graph,
      title={Graph Based Link Prediction between Human Phenotypes and Genes}, 
      author={Rushabh Patel and Yanhui Guo},
      year={2021},
      eprint={2105.11989},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ivanov2021boost,
      title={Boost then Convolve: Gradient Boosting Meets Graph Neural Networks}, 
      author={Sergei Ivanov and Liudmila Prokhorenkova},
      year={2021},
      eprint={2101.08543},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{8530225,
  author={Wu, Mingcong and Huang, Yong and Zhao, Liang and He, Yue},
  booktitle={2018 10th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)}, 
  title={Link Prediction Based on Random Forest in Signed Social Networks}, 
  year={2018},
  volume={02},
  number={},
  pages={251-256},
  doi={10.1109/IHMSC.2018.10164}}
  
  
@article{article,
author = {Li, Kuanyang and Tu, Lilan},
year = {2019},
month = {08},
pages = {022030},
title = {Link Prediction for Complex Networks via Random Forest},
volume = {1302},
journal = {Journal of Physics: Conference Series},
doi = {10.1088/1742-6596/1302/2/022030}
}



@article{mutagenic_affect,
author = {Debnath, Asim Kumar and Lopez de Compadre, Rosa L. and Debnath, Gargi and Shusterman, Alan J. and Hansch, Corwin},
title = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity},
journal = {Journal of Medicinal Chemistry},
volume = {34},
number = {2},
pages = {786-797},
year = {1991},
doi = {10.1021/jm00106a046},

URL = { 
        https://doi.org/10.1021/jm00106a046
    
},
eprint = { 
        https://doi.org/10.1021/jm00106a046
    
}

}


@InProceedings{mutagenicity_dataset,
author="Riesen, Kaspar
and Bunke, Horst",
editor="da Vitoria Lobo, Niels
and Kasparis, Takis
and Roli, Fabio
and Kwok, James T.
and Georgiopoulos, Michael
and Anagnostopoulos, Georgios C.
and Loog, Marco",
title="IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning",
booktitle="Structural, Syntactic, and Statistical Pattern Recognition",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="287--297",
abstract="In recent years the use of graph based representation has gained popularity in pattern recognition and machine learning. As a matter of fact, object representation by means of graphs has a number of advantages over feature vectors. Therefore, various algorithms for graph based machine learning have been proposed in the literature. However, in contrast with the emerging interest in graph based representation, a lack of standardized graph data sets for benchmarking can be observed. Common practice is that researchers use their own data sets, and this behavior cumbers the objective evaluation of the proposed methods. In order to make the different approaches in graph based machine learning better comparable, the present paper aims at introducing a repository of graph data sets and corresponding benchmarks, covering a wide spectrum of different applications.",
isbn="978-3-540-89689-0"
}

@article {mutag_dataset,
	Title = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity},
	Author = {Debnath, AK and Lopez de Compadre, RL and Debnath, G and Shusterman, AJ and Hansch, C},
	DOI = {10.1021/jm00106a046},
	Number = {2},
	Volume = {34},
	Month = {February},
	Year = {1991},
	Journal = {Journal of medicinal chemistry},
	ISSN = {0022-2623},
	Pages = {786—797},
	Abstract = {A review of the literature yielded data on over 200 aromatic and heteroaromatic nitro compounds tested for mutagenicity in the Ames test using S. typhimurium TA98. From the data, a quantitative structure-activity relationship (QSAR) has been derived for 188 congeners. The main determinants of mutagenicity are the hydrophobicity (modeled by octanol/water partition coefficients) and the energies of the lowest unoccupied molecular orbitals calculated using the AM1 method. It is also shown that chemicals possessing three or more fused rings possess much greater mutagenic potency than compounds with one or two fused rings. Since the QSAR is based on a very wide range in structural variation, aromatic rings from benzene to coronene are included as well as many different types of heterocycles, it is a significant step toward a predictive toxicology of value in the design of less mutagenic bioactive compounds.},
	URL = {https://doi.org/10.1021/jm00106a046},
}


@INPROCEEDINGS{nci1_dataset,
  author={Wale, Nikil and Karypis, George},
  booktitle={Sixth International Conference on Data Mining (ICDM'06)}, 
  title={Comparison of Descriptor Spaces for Chemical Compound Retrieval and Classification}, 
  year={2006},
  volume={},
  number={},
  pages={678-689},
  doi={10.1109/ICDM.2006.39}}

@inproceedings{tudataset,
    title={TUDataset: A collection of benchmark datasets for learning with graphs},
    author={Christopher Morris and Nils M. Kriege and Franka Bause and Kristian Kersting and Petra Mutzel and Marion Neumann},
    booktitle={ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)},
    archivePrefix={arXiv},
    eprint={2007.08663},
    url={www.graphlearning.io},
    year={2020}
}

@misc{gnnlimitations,
      title={Generalization and Representational Limits of Graph Neural Networks}, 
      author={Vikas K. Garg and Stefanie Jegelka and Tommi Jaakkola},
      year={2020},
      eprint={2002.06157},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gratedgraph,
      title={Gated Graph Sequence Neural Networks}, 
      author={Yujia Li and Daniel Tarlow and Marc Brockschmidt and Richard Zemel},
      year={2017},
      eprint={1511.05493},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{inductivegraph,
 author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inductive Representation Learning on Large Graphs},
 url = {https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{mulecularfingerprints,
      title={Convolutional Networks on Graphs for Learning Molecular Fingerprints}, 
      author={David Duvenaud and Dougal Maclaurin and Jorge Aguilera-Iparraguirre and Rafael Gómez-Bombarelli and Timothy Hirzel and Alán Aspuru-Guzik and Ryan P. Adams},
      year={2015},
      eprint={1509.09292},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{gcn,
      title={Semi-Supervised Classification with Graph Convolutional Networks}, 
      author={Thomas N. Kipf and Max Welling},
      year={2017},
      eprint={1609.02907},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{gat,
  title={Graph Attention Networks},
  author={Veli{\v{c}}ković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{gat-bak,
  title={Graph Attention Networks},
  author={Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@misc{gnnlowerbounds,
      title={What graph neural networks cannot learn: depth vs width}, 
      author={Andreas Loukas},
      year={2020},
      eprint={1907.03199},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cnngnn,
 author = {Defferrard, Micha\"{e}l and Bresson, Xavier and Vandergheynst, Pierre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
 url = {https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
 volume = {29},
 year = {2016}
}

@misc{barrett2018measuring,
      title={Measuring abstract reasoning in neural networks}, 
      author={David G. T. Barrett and Felix Hill and Adam Santoro and Ari S. Morcos and Timothy Lillicrap},
      year={2018},
      eprint={1807.04225},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{Erdos:1960,
  added-at = {2008-02-10T00:02:41.000+0100},
  author = {Erdos, Paul and Renyi, Alfred},
  biburl = {https://www.bibsonomy.org/bibtex/2dbd46a051f4453bf4e05a15d80c42b7e/marymcglo},
  interhash = {4d8b0a55d11fccf94b813271b962d17e},
  intrahash = {dbd46a051f4453bf4e05a15d80c42b7e},
  journal = {Publ. Math. Inst. Hungary. Acad. Sci.},
  keywords = {imported},
  pages = {17-61},
  timestamp = {2008-02-10T00:02:41.000+0100},
  title = {On the evolution of random graphs},
  volume = 5,
  year = 1960
}

@inproceedings{rf,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@article{gbt,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}


@inproceedings{
gin,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryGs6iA5Km},
}

@inproceedings{dgcnn,
  title={An End-to-End Deep Learning Architecture for Graph Classification},
  author={Muhan Zhang and Zhicheng Cui and Marion Neumann and Yixin Chen},
  booktitle={AAAI},
  year={2018}
}


@inproceedings{rw,
  added-at = {2010-11-11T20:45:19.000+0100},
  author = {Gärtner, Thomas and Flach, Peter and Wrobel, Stefan},
  biburl = {https://www.bibsonomy.org/bibtex/21c0dcf75e5c12031501bee47f84fa4c6/sb3000},
  booktitle = {Computational Learning Theory and Kernel Machines --- Proceedings of the 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop (COLT/Kernel 2003) August 24-27, 2003, Washington, DC, USA},
  editor = {Schölkopf, Bernhard and Warmuth, Manfred K.},
  interhash = {af61cfa4342af108adb3cdf2c32feea0},
  intrahash = {1c0dcf75e5c12031501bee47f84fa4c6},
  keywords = {graph-kernel ml},
  month = {August},
  pages = {129--143},
  publisher = {Springer, Berlin--Heidelberg, Germany},
  series = {Lecture Notes in Computer Science},
  timestamp = {2010-11-11T20:45:19.000+0100},
  title = {On Graph Kernels: Hardness Results and Efficient Alternatives},
  volume = 2777,
  year = 2003
}

@article{wl,
  added-at = {2019-07-10T00:00:00.000+0200},
  author = {Shervashidze, Nino and Schweitzer, Pascal and van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M.},
  biburl = {https://www.bibsonomy.org/bibtex/2aae4bef7d308f1ded5f2ff0a8bd7f4b5/dblp},
  ee = {http://dl.acm.org/citation.cfm?id=2078187},
  interhash = {5606aa2c652c8d931e0a9597c0c48b63},
  intrahash = {aae4bef7d308f1ded5f2ff0a8bd7f4b5},
  journal = {J. Mach. Learn. Res.},
  keywords = {dblp},
  pages = {2539-2561},
  timestamp = {2019-07-11T11:38:58.000+0200},
  title = {Weisfeiler-Lehman Graph Kernels.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlr12.html#ShervashidzeSLMB11},
  volume = 12,
  year = 2011
}

@article{gk,
  added-at = {2011-08-24T13:37:03.000+0200},
  author = {Shervashidze, Nino and Vishwanathan, S. V. N. and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten M.},
  biburl = {https://www.bibsonomy.org/bibtex/2ee01b18014f58932623a9b064f86900e/utahell},
  ee = {http://www.jmlr.org/proceedings/papers/v5/shervashidze09a.html},
  interhash = {cd658fc19c7109ae2e732e30c830a85f},
  intrahash = {ee01b18014f58932623a9b064f86900e},
  journal = {Journal of Machine Learning Research - Proceedings Track},
  keywords = {graph-kernel},
  pages = {488--495},
  timestamp = {2011-08-24T13:37:52.000+0200},
  title = {Efficient graphlet kernels for large graph comparison},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlrp5.html#ShervashidzeVPMB09},
  volume = 5,
  year = 2009
}

@article{cuseofdimensionality,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={Science},
  volume={153},
  number={3731},
  pages={34--37},
  year={1966},
  publisher={American Association for the Advancement of Science}
}


@misc{explanationsimportance,
      title={Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead}, 
      author={Cynthia Rudin},
      year={2019},
      eprint={1811.10154},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{polypharmacy,
    author = {Zitnik, Marinka and Agrawal, Monica and Leskovec, Jure},
    title = "{Modeling polypharmacy side effects with graph convolutional networks}",
    journal = {Bioinformatics},
    volume = {34},
    number = {13},
    pages = {i457-i466},
    year = {2018},
    month = {06},
    abstract = "{The use of drug combinations, termed polypharmacy, is common to treat patients with complex diseases or co-existing conditions. However, a major consequence of polypharmacy is a much higher risk of adverse side effects for the patient. Polypharmacy side effects emerge because of drug–drug interactions, in which activity of one drug may change, favorably or unfavorably, if taken with another drug. The knowledge of drug interactions is often limited because these complex relationships are rare, and are usually not observed in relatively small clinical testing. Discovering polypharmacy side effects thus remains an important challenge with significant implications for patient mortality and morbidity.Here, we present Decagon, an approach for modeling polypharmacy side effects. The approach constructs a multimodal graph of protein–protein interactions, drug–protein target interactions and the polypharmacy side effects, which are represented as drug–drug interactions, where each side effect is an edge of a different type. Decagon is developed specifically to handle such multimodal graphs with a large number of edge types. Our approach develops a new graph convolutional neural network for multirelational link prediction in multimodal networks. Unlike approaches limited to predicting simple drug–drug interaction values, Decagon can predict the exact side effect, if any, through which a given drug combination manifests clinically. Decagon accurately predicts polypharmacy side effects, outperforming baselines by up to 69\\%. We find that it automatically learns representations of side effects indicative of co-occurrence of polypharmacy in patients. Furthermore, Decagon models particularly well polypharmacy side effects that have a strong molecular basis, while on predominantly non-molecular side effects, it achieves good performance because of effective sharing of model parameters across edge types. Decagon opens up opportunities to use large pharmacogenomic and patient population data to flag and prioritize polypharmacy side effects for follow-up analysis via formal pharmacological studies.Source code and preprocessed datasets are at: http://snap.stanford.edu/decagon.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bty294},
    url = {https://doi.org/10.1093/bioinformatics/bty294},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/34/13/i457/25098429/bty294.pdf},
}


@inproceedings{
retrosynthesis,
title={Learning Graph Models for Retrosynthesis Prediction},
author={Vignesh Ram Somnath and Charlotte Bunne and Connor W. Coley and Andreas Krause and Regina Barzilay},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=SnONpXZ_uQ_}
}


@misc{geomodel,
      title={GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles}, 
      author={Octavian-Eugen Ganea and Lagnajit Pattanaik and Connor W. Coley and Regina Barzilay and Klavs F. Jensen and William H. Green and Tommi S. Jaakkola},
      year={2021},
      eprint={2106.07802},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph}
}

@misc{rossi2020temporal,
      title={Temporal Graph Networks for Deep Learning on Dynamic Graphs}, 
      author={Emanuele Rossi and Ben Chamberlain and Fabrizio Frasca and Davide Eynard and Federico Monti and Michael Bronstein},
      year={2020},
      eprint={2006.10637},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{generative,
 author = {Ingraham, John and Garg, Vikas and Barzilay, Regina and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Models for Graph-Based Protein Design},
 url = {https://proceedings.neurips.cc/paper/2019/file/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{hgcc, title={Hypergraph Neural Networks}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4235}, DOI={10.1609/aaai.v33i01.33013558}, abstractNote={&lt;p&gt;In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-theart methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue}, year={2019}, month={Jul.}, pages={3558-3565} }

@misc{mpgnn,
      title={Neural Message Passing for Quantum Chemistry}, 
      author={Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
      year={2017},
      eprint={1704.01212},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{gcnnlp,
    title = "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
    author = "Bastings, Jasmijn  and
      Titov, Ivan  and
      Aziz, Wilker  and
      Marcheggiani, Diego  and
      Sima{'}an, Khalil",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1209",
    doi = "10.18653/v1/D17-1209",
    pages = "1957--1967",
    abstract = "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.",
}
@article{pinsage,
   title={Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
   url={http://dx.doi.org/10.1145/3219819.3219890},
   DOI={10.1145/3219819.3219890},
   journal={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
   publisher={ACM},
   author={Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
   year={2018},
   month={Jul}
}

@inproceedings{dgk,
author = {Yanardag, Pinar and Vishwanathan, S.V.N.},
title = {Deep Graph Kernels},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783417},
doi = {10.1145/2783258.2783417},
abstract = {In this paper, we present Deep Graph Kernels, a unified framework to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information between sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve significant improvements in classification accuracy over state-of-the-art graph kernels.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1365–1374},
numpages = {10},
keywords = {string kernels, graph kernels, bioinformatics, social networks, structured data, collaboration networks, deep learning, r-convolution kernels},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article {mutag,
	Title = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity},
	Author = {Debnath, AK and Lopez de Compadre, RL and Debnath, G and Shusterman, AJ and Hansch, C},
	DOI = {10.1021/jm00106a046},
	Number = {2},
	Volume = {34},
	Month = {February},
	Year = {1991},
	Journal = {Journal of medicinal chemistry},
	ISSN = {0022-2623},
	Pages = {786—797},
	Abstract = {A review of the literature yielded data on over 200 aromatic and heteroaromatic nitro compounds tested for mutagenicity in the Ames test using S. typhimurium TA98. From the data, a quantitative structure-activity relationship (QSAR) has been derived for 188 congeners. The main determinants of mutagenicity are the hydrophobicity (modeled by octanol/water partition coefficients) and the energies of the lowest unoccupied molecular orbitals calculated using the AM1 method. It is also shown that chemicals possessing three or more fused rings possess much greater mutagenic potency than compounds with one or two fused rings. Since the QSAR is based on a very wide range in structural variation, aromatic rings from benzene to coronene are included as well as many different types of heterocycles, it is a significant step toward a predictive toxicology of value in the design of less mutagenic bioactive compounds.},
	URL = {https://doi.org/10.1021/jm00106a046},
}

@article{proteins,
  title={Distinguishing enzyme structures from non-enzymes without alignments.},
  author={Paul D. Dobson and Andrew J. Doig},
  journal={Journal of molecular biology},
  year={2003},
  volume={330 4},
  pages={
          771-83
        }
}

@inproceedings{ptcmr,
author = {Kriege, Nils and Mutzel, Petra},
title = {Subgraph Matching Kernels for Attributed Graphs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose graph kernels based on subgraph matchings, i.e. structure-preserving bijections between subgraphs. While recently proposed kernels based on common subgraphs (Wale et al., 2008; Shervashidze et al., 2009) in general can not be applied to attributed graphs, our approach allows to rate mappings of subgraphs by a exible scoring scheme comparing vertex and edge attributes by kernels. We show that subgraph matching kernels generalize several known kernels. To compute the kernel we propose a graph-theoretical algorithm inspired by a classical relation between common subgraphs of two graphs and cliques in their product graph observed by Levi (1973). Encouraging experimental results on a classification task of real-world graphs are presented.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {291–298},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{
shwartz-ziv2021tabular,
title={Tabular Data: Deep Learning is Not All You Need},
author={Ravid Shwartz-Ziv and Amitai Armon},
booktitle={8th ICML Workshop on Automated Machine Learning (AutoML) },
year={2021},
url={https://openreview.net/forum?id=vdgtepS1pV}
}






@article{nnvsgbt,
author = {Te Han and Dongxiang Jiang and Qi Zhao and Lei Wang and Kai Yin},
title ={Comparison of random forest, artificial neural networks and support vector machine for intelligent diagnosis of rotating machinery},
journal = {Transactions of the Institute of Measurement and Control},
volume = {40},
number = {8},
pages = {2681-2693},
year = {2018},
doi = {10.1177/0142331217708242},

URL = { 
        https://doi.org/10.1177/0142331217708242
    
},
eprint = { 
        https://doi.org/10.1177/0142331217708242
    
}
,
    abstract = { Nowadays, the data-driven diagnosis method, exploiting pattern recognition method to diagnose the fault patterns automatically, achieves much success for rotating machinery. Some popular classification algorithms such as artificial neural networks and support vector machine have been extensively studied and tested with many application cases, while the random forest, one of the present state-of-the-art classifiers based on ensemble learning strategy, is relatively unknown in this field. In this paper, the behavior of random forest for the intelligent diagnosis of rotating machinery is investigated with various features on two datasets. A framework for the comparison of different methods, that is, random forest, extreme learning machine, probabilistic neural network and support vector machine, is presented to find the most efficient one. Random forest has been proven to outperform the comparative classifiers in terms of recognition accuracy, stability and robustness to features, especially with a small training set. Additionally, compared with traditional methods, random forest is not easily influenced by environmental noise. Furthermore, the user-friendly parameters in random forest offer great convenience for practical engineering. These results suggest that random forest is a promising pattern recognition method for the intelligent diagnosis of rotating machinery. }
}

@inbook{inbook,
author = {Miah, Yunus and Prima, Chowdhury and Seema, Sharmeen and Mahmud, Mufti and Kaiser, M. Shamim},
year = {2021},
month = {01},
pages = {79-89},
title = {Performance Comparison of Machine Learning Techniques in Identifying Dementia from Open Access Clinical Datasets},
isbn = {978-981-15-6047-7},
doi = {10.1007/978-981-15-6048-4_8}
}

@INPROCEEDINGS{gori2005,  author={Gori, M. and Monfardini, G. and Scarselli, F.},  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},   title={A new model for learning in graph domains},   year={2005},  volume={2},  number={},  pages={729-734 vol. 2},  doi={10.1109/IJCNN.2005.1555942}}


@book{cart,
  added-at = {2020-05-07T22:53:11.000+0200},
  address = {Monterey, CA},
  author = {Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  biburl = {https://www.bibsonomy.org/bibtex/27f293aa2bdfd10960ef36928f2795f1d/flashspys},
  interhash = {61f3e6d61ba17bb493014bd1c6dfa670},
  intrahash = {7f293aa2bdfd10960ef36928f2795f1d},
  keywords = {ma treelearning},
  publisher = {Wadsworth and Brooks},
  serial = {bre84a},
  timestamp = {2020-05-07T22:53:11.000+0200},
  title = {Classification and Regression Trees},
  year = 1984
}



@article{sperduti1997supervised,
  title={Supervised neural networks for the classification of structures},
  author={Sperduti, Alessandro and Starita, Antonina},
  journal={IEEE Transactions on Neural Networks},
  volume={8},
  number={3},
  pages={714--735},
  year={1997},
  publisher={IEEE}
}

@article{wu2020comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={1},
  pages={4--24},
  year={2020},
  publisher={IEEE}
}

@article{pathak2021learning,
  title={Learning Atomic Interactions through Solvation Free Energy Prediction Using Graph Neural Networks},
  author={Pathak, Yashaswi and Mehta, Sarvesh and Priyakumar, U Deva},
  journal={Journal of Chemical Information and Modeling},
  volume={61},
  number={2},
  pages={689--698},
  year={2021},
  publisher={ACS Publications}
}

@inproceedings{heaton2016empirical,
  title={An empirical analysis of feature engineering for predictive modeling},
  author={Heaton, Jeff},
  booktitle={SoutheastCon 2016},
  pages={1--6},
  year={2016},
  organization={IEEE}
}

@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@article{singer1982chemical,
  title={Chemical mutagenesis},
  author={Singer, B and Kusmierek, JT},
  journal={Annual review of biochemistry},
  volume={51},
  number={1},
  pages={655--691},
  year={1982},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{wu2021mining,
  title={Mining Toxicity Information from Large Amounts of Toxicity Data},
  author={Wu, Zhenxing and Jiang, Dejun and Wang, Jike and Hsieh, Chang-Yu and Cao, Dongsheng and Hou, Tingjun},
  journal={Journal of Medicinal Chemistry},
  volume={64},
  number={10},
  pages={6924--6936},
  year={2021},
  publisher={ACS Publications}
}

@misc{adam,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.},
  added-at = {2022-01-22T18:57:35.000+0100},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/2d53bcfff0fe1a1d3a4a171352ee6e92c/leophill},
  description = {Adam: A Method for Stochastic Optimization},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
  keywords = {Thesis},
  note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
  timestamp = {2022-01-22T18:57:35.000+0100},
  title = {Adam: A Method for Stochastic Optimization},
  url = {http://arxiv.org/abs/1412.6980},
  year = 2014
}


@misc{ogb,
  doi = {10.48550/ARXIV.2005.00687},
  
  url = {https://arxiv.org/abs/2005.00687},
  
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  
  keywords = {Machine Learning (cs.LG), Social and Information Networks (cs.SI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{gksurvey,
	doi = {10.1613/jair.1.13225},
  
	url = {https://doi.org/10.1613%2Fjair.1.13225},
  
	year = 2021,
	month = {nov},
  
	publisher = {{AI} Access Foundation},
  
	volume = {72},
  
	pages = {943--1027},
  
	author = {Giannis Nikolentzos and Giannis Siglidis and Michalis Vazirgiannis},
  
	title = {Graph Kernels: A Survey},
  
	journal = {Journal of Artificial Intelligence Research}
}

@misc{attention,
  doi = {10.48550/ARXIV.1409.0473},
  
  url = {https://arxiv.org/abs/1409.0473},
  
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{planetoid,
  doi = {10.48550/ARXIV.1603.08861},
  
  url = {https://arxiv.org/abs/1603.08861},
  
  author = {Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Revisiting Semi-Supervised Learning with Graph Embeddings},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cyclesgnns,
  doi = {10.48550/ARXIV.2002.06157},
  
  url = {https://arxiv.org/abs/2002.06157},
  
  author = {Garg, Vikas K. and Jegelka, Stefanie and Jaakkola, Tommi},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generalization and Representational Limits of Graph Neural Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{o2017synthetic,
  title={Synthetic lethality and cancer},
  author={O'Neil, Nigel J and Bailey, Melanie L and Hieter, Philip},
  journal={Nature Reviews Genetics},
  volume={18},
  number={10},
  pages={613--623},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{he2017simboost,
  title={SimBoost: a read-across approach for predicting drug--target binding affinities using gradient boosting machines},
  author={He, Tong and Heidemeyer, Marten and Ban, Fuqiang and Cherkasov, Artem and Ester, Martin},
  journal={Journal of cheminformatics},
  volume={9},
  number={1},
  pages={1--14},
  year={2017},
  publisher={BioMed Central}
}

@article{lei2019gbdtcda,
  title={GBDTCDA: predicting circRNA-disease associations based on gradient boosting decision tree with multiple biological data fusion},
  author={Lei, Xiujuan and Fang, Zengqiang},
  journal={International journal of biological sciences},
  volume={15},
  number={13},
  pages={2911},
  year={2019},
  publisher={Ivyspring International Publisher}
}

@article{de2018simple,
  title={A simple baseline algorithm for graph classification},
  author={de Lara, Nathan and Pineau, Edouard},
  journal={arXiv preprint arXiv:1810.09155},
  year={2018}
}

@article{barnett2016feature,
  title={Feature-based classification of networks},
  author={Barnett, Ian and Malik, Nishant and Kuijjer, Marieke L and Mucha, Peter J and Onnela, Jukka-Pekka},
  journal={arXiv preprint arXiv:1610.05868},
  year={2016}
}

@misc{idgnn,
  doi = {10.48550/ARXIV.2101.10320},
  
  url = {https://arxiv.org/abs/2101.10320},
  
  author = {You, Jiaxuan and Gomes-Selman, Jonathan and Ying, Rex and Leskovec, Jure},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Identity-aware Graph Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lap_features_1,
  doi = {10.48550/ARXIV.1810.09155},
  
  url = {https://arxiv.org/abs/1810.09155},
  
  author = {de Lara, Nathan and Pineau, Edouard},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Simple Baseline Algorithm for Graph Classification},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lap_features_2,
author = {Li, Peng and Fanrang, Kong and He, Qingbo and Liu, Yongbin},
year = {2013},
month = {01},
pages = {497–505},
title = {Multiscale slope feature extraction for rotating machinery fault diagnosis using wavelet analysis},
volume = {46},
journal = {Measurement},
doi = {10.1016/j.measurement.2012.08.007}
}

@article{linegraphs,
  title={Some properties of line digraphs},
  author={Frank Harary and R. Z. Norman},
  journal={Rendiconti del Circolo Matematico di Palermo},
  year={1960},
  volume={9},
  pages={161-168}
}

@misc{gnns_local_structures,
  doi = {10.48550/ARXIV.2010.08853},
  
  url = {https://arxiv.org/abs/2010.08853},
  
  author = {Yehudai, Gilad and Fetaya, Ethan and Meirom, Eli and Chechik, Gal and Maron, Haggai},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {From Local Structures to Size Generalization in Graph Neural Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
gnns_extrapolation,
title={How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
author={Keyulu Xu and Mozhi Zhang and Jingling Li and Simon Shaolei Du and Ken-Ichi Kawarabayashi and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=UH-cmocLJC}
}

@misc{implcit_bias_separable_data,
  doi = {10.48550/ARXIV.1710.10345},
  
  url = {https://arxiv.org/abs/1710.10345},
  
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Implicit Bias of Gradient Descent on Separable Data},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{gine,
      title={Strategies for Pre-training Graph Neural Networks}, 
      author={Weihua Hu and Bowen Liu and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},
      year={2020},
      eprint={1905.12265},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2017understanding,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      eprint={1611.03530},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{morris2021weisfeiler,
      title={Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks}, 
      author={Christopher Morris and Martin Ritzert and Matthias Fey and William L. Hamilton and Jan Eric Lenssen and Gaurav Rattan and Martin Grohe},
      year={2021},
      eprint={1810.02244},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{gnp,
  added-at = {2010-05-05T00:38:27.000+0200},
  author = {Erd\"{o}s, P. and R\'{e}nyi, A.},
  biburl = {https://www.bibsonomy.org/bibtex/25aab47a7be9ec47644735f8e0a4607b6/alex},
  interhash = {99061fc859ba540d4485abfbce44f298},
  intrahash = {5aab47a7be9ec47644735f8e0a4607b6},
  journal = {Publicationes Mathematicae Debrecen},
  keywords = {graph sna},
  pages = 290,
  timestamp = {2010-05-05T00:38:27.000+0200},
  title = {On Random Graphs I},
  volume = 6,
  year = 1959
}

@article{badist,
  added-at = {2008-12-15T20:32:54.000+0100},
  author = {Barabasi, Albert-Laszlo and Albert, Reka},
  biburl = {https://www.bibsonomy.org/bibtex/2cd72a4b9b18025fd856f94e830530e50/lee_peck},
  description = {Emergence of Scaling in Random Networks -- Barabási and Albert 286 (5439): 509 -- Science},
  doi = {10.1126/science.286.5439.509},
  eprint = {http://www.sciencemag.org/cgi/reprint/286/5439/509.pdf},
  interhash = {89d3f086051d18093558698788063dfe},
  intrahash = {cd72a4b9b18025fd856f94e830530e50},
  journal = {Science},
  keywords = {99 Barabasi attachment graph modelling preferential},
  number = 5439,
  pages = {509-512},
  timestamp = {2008-12-15T20:32:54.000+0100},
  title = {Emergence of Scaling in Random Networks},
  url = {http://www.sciencemag.org/cgi/content/abstract/286/5439/509},
  volume = 286,
  year = 1999
}





@article{size_generalization,
  author       = {Gilad Yehudai and
                  Ethan Fetaya and
                  Eli A. Meirom and
                  Gal Chechik and
                  Haggai Maron},
  title        = {On Size Generalization in Graph Neural Networks},
  journal      = {CoRR},
  volume       = {abs/2010.08853},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.08853},
  eprinttype    = {arXiv},
  eprint       = {2010.08853},
  timestamp    = {Wed, 21 Oct 2020 12:11:48 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-08853.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v139-azulay21a,
  title = 	 {On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent},
  author =       {Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {468--477},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/azulay21a/azulay21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/azulay21a.html},
  abstract = 	 {Recent work has highlighted the role of initialization scale in determining the structure of the solutions that gradient methods converge to. In particular, it was shown that large initialization leads to the neural tangent kernel regime solution, whereas small initialization leads to so called “rich regimes”. However, the initialization structure is richer than the overall scale alone and involves relative magnitudes of different weights and layers in the network. Here we show that these relative scales, which we refer to as initialization shape, play an important role in determining the learned model. We develop a novel technique for deriving the inductive bias of gradient-flow and use it to obtain closed-form implicit regularizers for multiple cases of interest.}
}


@misc{brutzkus2017sgd,
      title={SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data}, 
      author={Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
      year={2017},
      eprint={1710.10174},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{errica2022fair,
      title={A Fair Comparison of Graph Neural Networks for Graph Classification}, 
      author={Federico Errica and Marco Podda and Davide Bacciu and Alessio Micheli},
      year={2022},
      eprint={1912.09893},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zaheer2018deep,
      title={Deep Sets}, 
      author={Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnabas Poczos and Ruslan Salakhutdinov and Alexander Smola},
      year={2018},
      eprint={1703.06114},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{motifsdataset,
      title={Parameterized Explainer for Graph Neural Network}, 
      author={Dongsheng Luo and Wei Cheng and Dongkuan Xu and Wenchao Yu and Bo Zong and Haifeng Chen and Xiang Zhang},
      year={2020},
      eprint={2011.04573},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{pyg,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}

@inproceedings{
kipf2017semisupervised,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}

@article{razin2021implicit,
  title={Implicit Regularization in Tensor Factorization},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  journal={International Conference on Machine Learning (ICML)},
  year={2021}
}

@misc{alon2021bottleneck,
      title={On the Bottleneck of Graph Neural Networks and its Practical Implications}, 
      author={Uri Alon and Eran Yahav},
      year={2021},
      eprint={2006.05205},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cohenkarlik2023learning,
      title={Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets}, 
      author={Edo Cohen-Karlik and Itamar Menuhin-Gruman and Raja Giryes and Nadav Cohen and Amir Globerson},
      year={2023},
      eprint={2210.14064},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Rong2019DropEdgeTD,
  title={DropEdge: Towards Deep Graph Convolutional Networks on Node Classification},
  author={Yu Rong and Wen-bing Huang and Tingyang Xu and Junzhou Huang},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:212859361}
}

@article{Wu_2021,
	doi = {10.1109/tnnls.2020.2978386},
  
	url = {https://doi.org/10.1109%2Ftnnls.2020.2978386},
  
	year = 2021,
	month = {jan},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {32},
  
	number = {1},
  
	pages = {4--24},
  
	author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  
	title = {A Comprehensive Survey on Graph Neural Networks},
  
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@misc{brody2022attentive,
      title={How Attentive are Graph Attention Networks?}, 
      author={Shaked Brody and Uri Alon and Eran Yahav},
      year={2022},
      eprint={2105.14491},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tarzanagh2023transformers,
      title={Transformers as Support Vector Machines}, 
      author={Davoud Ataee Tarzanagh and Yingcong Li and Christos Thrampoulidis and Samet Oymak},
      year={2023},
      eprint={2308.16898},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tarzanagh2023maxmargin,
      title={Max-Margin Token Selection in Attention Mechanism}, 
      author={Davoud Ataee Tarzanagh and Yingcong Li and Xuechen Zhang and Samet Oymak},
      year={2023},
      eprint={2306.13596},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lyu2020gradient,
      title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks}, 
      author={Kaifeng Lyu and Jian Li},
      year={2020},
      eprint={1906.05890},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2020_c76e4b2f,
 author = {Ji, Ziwei and Telgarsky, Matus},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17176--17186},
 publisher = {Curran Associates, Inc.},
 title = {Directional convergence and alignment in deep learning},
 volume = {33},
 year = {2020}
}

@misc{shi2021masked,
      title={Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification}, 
      author={Yunsheng Shi and Zhengjie Huang and Shikun Feng and Hui Zhong and Wenjin Wang and Yu Sun},
      year={2021},
      eprint={2009.03509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{seddik2022node,
      title={Node Feature Kernels Increase Graph Convolutional Network Robustness}, 
      author={Mohamed El Amine Seddik and Changmin Wu and Johannes F. Lutzeyer and Michalis Vazirgiannis},
      year={2022},
      eprint={2109.01785},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.1145/3394486.3403076,
author = {Liu, Meng and Gao, Hongyang and Ji, Shuiwang},
title = {Towards Deeper Graph Neural Networks},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403076},
doi = {10.1145/3394486.3403076},
abstract = {Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {338–348},
numpages = {11},
keywords = {deep learning, graph neural networks, graph representation learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@misc{chen2020graph,
      title={On Graph Neural Networks versus Graph-Augmented MLPs}, 
      author={Lei Chen and Zhengdao Chen and Joan Bruna},
      year={2020},
      eprint={2010.15116},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}