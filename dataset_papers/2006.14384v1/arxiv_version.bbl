\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bauschke et~al.(2017)Bauschke, Bolte, and
  Teboulle]{bauschke2017descent}
Heinz~H. Bauschke, J{\'e}r{\^o}me Bolte, and Marc Teboulle.
\newblock A descent lemma beyond {L}ipschitz gradient continuity: first-order
  methods revisited and applications.
\newblock \emph{Mathematics of Operations Research}, 42\penalty0 (2):\penalty0
  330--348, 2017.

\bibitem[Berthier et~al.(2020)Berthier, Bach, and
  Gaillard]{berthier2020accelerated}
Rapha{\"e}l Berthier, Francis Bach, and Pierre Gaillard.
\newblock Accelerated gossip in networks of given dimension using jacobi
  polynomial iterations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (1):\penalty0 24--47, 2020.

\bibitem[Bollob{\'a}s(2001)]{bollobas2001random}
B{\'e}la Bollob{\'a}s.
\newblock \emph{Random graphs}.
\newblock Number~73 in Cambridge studies in advanced mathematics. Cambridge
  University Press, 2001.

\bibitem[Bottou(2010)]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT}, pages 177--186. Springer, 2010.

\bibitem[Boyd et~al.(2006)Boyd, Ghosh, Prabhakar, and Shah]{boyd2006randomized}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Randomized gossip algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (6):\penalty0 2508--2530, 2006.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and
  Eckstein]{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{duchi2012dual}
John~C. Duchi, Alekh Agarwal, and Martin~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic Control}, 57\penalty0
  (3):\penalty0 592--606, 2012.

\bibitem[Dvinskikh and Gasnikov(2019)]{dvinskikh2019decentralized}
Darina Dvinskikh and Alexander Gasnikov.
\newblock Decentralized and parallelized primal and dual accelerated methods
  for stochastic convex programming problems.
\newblock \emph{arXiv preprint arXiv:1904.09015}, 2019.

\bibitem[Hendrikx et~al.(2019{\natexlab{a}})Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2018accelerated}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock Accelerated decentralized optimization with local updates for smooth
  and strongly convex objectives.
\newblock In \emph{Artificial Intelligence and Statistics}, 2019{\natexlab{a}}.

\bibitem[Hendrikx et~al.(2019{\natexlab{b}})Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2019accelerated}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock An accelerated decentralized stochastic proximal algorithm for finite
  sums.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Hendrikx et~al.(2020)Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2020optimal}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock An optimal algorithm for decentralized finite sum optimization.
\newblock \emph{arXiv preprint arXiv:2005.10675}, 2020.

\bibitem[Jakoveti{\'c}(2018)]{jakovetic2018unification}
Du{\v{s}}an Jakoveti{\'c}.
\newblock A unification and generalization of exact distributed first-order
  methods.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 5\penalty0 (1):\penalty0 31--46, 2018.

\bibitem[Jakoveti{\'c} et~al.(2014)Jakoveti{\'c}, Moura, and
  Xavier]{jakovetic2014linear}
Du{\v{s}}an Jakoveti{\'c}, Jos{\'e} M.~F. Moura, and Joao Xavier.
\newblock Linear convergence rate of a class of distributed augmented
  {L}agrangian algorithms.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (4):\penalty0 922--936, 2014.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem[Lan and Zhou(2017)]{lan2017optimal}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{Mathematical Programming}, pages 1--49, 2017.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Rose, and Li]{lewis2004rcv1}
David~D Lewis, Yiming Yang, Tony~G Rose, and Fan Li.
\newblock Rcv1: A new benchmark collection for text categorization research.
\newblock \emph{Journal of machine learning research}, 5\penalty0
  (Apr):\penalty0 361--397, 2004.

\bibitem[Li and Lin(2020)]{li2020revisiting}
Huan Li and Zhouchen Lin.
\newblock Revisiting {EXTRA} for smooth distributed optimization.
\newblock \emph{arXiv preprint arXiv:2002.10110}, 2020.

\bibitem[Li et~al.(2018)Li, Fang, Yin, and Lin]{li2018sharp}
Huan Li, Cong Fang, Wotao Yin, and Zhouchen Lin.
\newblock A sharp convergence rate analysis for distributed accelerated
  gradient methods.
\newblock \emph{arXiv preprint arXiv:1810.01053}, 2018.

\bibitem[Li et~al.(2019)Li, Shi, and Yan]{li2019decentralized}
Zhi Li, Wei Shi, and Ming Yan.
\newblock A decentralized proximal-gradient method with network independent
  step-sizes and separated convergence rates.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (17):\penalty0 4494--4506, 2019.

\bibitem[Lin et~al.(2015{\natexlab{a}})Lin, Mairal, and
  Harchaoui]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3384--3392, 2015{\natexlab{a}}.

\bibitem[Lin et~al.(2017)Lin, Mairal, and Harchaoui]{lin2017catalyst}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock Catalyst acceleration for first-order convex optimization: from
  theory to practice.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 7854--7907, 2017.

\bibitem[Lin et~al.(2015{\natexlab{b}})Lin, Lu, and Xiao]{lin2015accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated randomized proximal coordinate gradient method and its
  application to regularized empirical risk minimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  2244--2273, 2015{\natexlab{b}}.

\bibitem[Lu et~al.(2018)Lu, Freund, and Nesterov]{lu2018relatively}
Haihao Lu, Robert~M. Freund, and Yurii Nesterov.
\newblock Relatively smooth convex optimization by first-order methods, and
  applications.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  333--354, 2018.

\bibitem[Mohar(1997)]{mohar1997some}
Bojan Mohar.
\newblock Some applications of laplace eigenvalues of graphs.
\newblock In \emph{Graph Symmetry}, pages 225--275. Springer, 1997.

\bibitem[Mokhtari and Ribeiro(2016)]{mokhtari2016dsa}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock {DSA}: Decentralized double stochastic averaging gradient algorithm.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2165--2199, 2016.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Angelia Nedic, Alex Olshevsky, and Wei Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  3027--3036, 2017.

\bibitem[Scaman et~al.(2019)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2019optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal convergence rates for convex distributed optimization in
  networks.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--31,
  2019.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Shalev-Shwartz(2016)]{shalev2016sdca}
Shai Shalev-Shwartz.
\newblock {SDCA} without duality, regularization, and individual convexity.
\newblock In \emph{International Conference on Machine Learning}, pages
  747--754, 2016.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Feb):\penalty0 567--599, 2013.

\bibitem[Shen et~al.(2018)Shen, Mokhtari, Zhou, Zhao, and
  Qian]{shen2018towards}
Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  4631--4640, 2018.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Uribe et~al.(2020)Uribe, Lee, Gasnikov, and Nedi{\'c}]{uribe2020dual}
C{\'e}sar~A. Uribe, Soomin Lee, Alexander Gasnikov, and Angelia Nedi{\'c}.
\newblock A dual approach for optimal algorithms in distributed optimization
  over networks.
\newblock \emph{Optimization Methods and Software}, pages 1--40, 2020.

\bibitem[Wang and Xiao(2017)]{wang2017exploiting}
Jialei Wang and Lin Xiao.
\newblock Exploiting strong convexity from data with primal-dual first-order
  algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  3694--3702, 2017.

\bibitem[Xiao et~al.(2019)Xiao, Yu, Lin, and Chen]{xiao2017dscovr}
Lin Xiao, Adams~Wei Yu, Qihang Lin, and Weizhu Chen.
\newblock {DSCOVR}: Randomized primal-dual block coordinate algorithms for
  asynchronous distributed optimization.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (43):\penalty0 1--58, 2019.

\bibitem[Xin et~al.(2020)Xin, Kar, and Khan]{xin2020decentralized}
Ran Xin, Soummya Kar, and Usman~A Khan.
\newblock Decentralized stochastic optimization and machine learning: A unified
  variance-reduction framework for robust performance and fast convergence.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  102--113, 2020.

\bibitem[Xu et~al.(2020)Xu, Tian, Sun, and Scutari]{xu2020distributed}
Jinming Xu, Ye~Tian, Ying Sun, and Gesualdo Scutari.
\newblock Distributed algorithms for composite optimization: Unified and tight
  convergence analysis.
\newblock \emph{arXiv preprint arXiv:2002.11534}, 2020.

\end{thebibliography}
