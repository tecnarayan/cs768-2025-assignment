\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achituve et~al.(2021{\natexlab{a}})Achituve, Maron, and
  Chechik]{achituve2021self}
Achituve, I., Maron, H., and Chechik, G.
\newblock Self-supervised learning for domain adaptation on point clouds.
\newblock In \emph{Proceedings of the IEEE/CVF winter conference on
  applications of computer vision}, pp.\  123--133, 2021{\natexlab{a}}.

\bibitem[Achituve et~al.(2021{\natexlab{b}})Achituve, Navon, Yemini, Chechik,
  and Fetaya]{achituve2021gp}
Achituve, I., Navon, A., Yemini, Y., Chechik, G., and Fetaya, E.
\newblock {GP}-{T}ree: A {G}aussian process classifier for few-shot incremental
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  54--65.
  PMLR, 2021{\natexlab{b}}.

\bibitem[Achituve et~al.(2021{\natexlab{c}})Achituve, Shamsian, Navon, Chechik,
  and Fetaya]{achituve2021personalized}
Achituve, I., Shamsian, A., Navon, A., Chechik, G., and Fetaya, E.
\newblock Personalized federated learning with {G}aussian processes.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8392--8406, 2021{\natexlab{c}}.

\bibitem[Achituve et~al.(2023)Achituve, Chechik, and
  Fetaya]{achituve2023guided}
Achituve, I., Chechik, G., and Fetaya, E.
\newblock Guided deep kernel learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}. PMLR, 2023.

\bibitem[Baxter(2000)]{baxter2000model}
Baxter, J.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Bishop(2006)]{bishop2006pattern}
Bishop, C.
\newblock Pattern recognition and machine learning.
\newblock \emph{Springer google schola}, 2:\penalty0 531--537, 2006.

\bibitem[Brier(1950)]{brier1950verification}
Brier, G.~W.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly weather review}, 78\penalty0 (1):\penalty0 1--3, 1950.

\bibitem[Brookes(2020)]{GaussMoments}
Brookes, M.
\newblock The matrix reference manual.
\newblock \url{http://www.ee.imperial.ac.uk/hp/staff/dmb/matrix/intro.html},
  2020.

\bibitem[Calandra et~al.(2016)Calandra, Peters, Rasmussen, and
  Deisenroth]{calandra2016manifold}
Calandra, R., Peters, J., Rasmussen, C.~E., and Deisenroth, M.~P.
\newblock Manifold {G}aussian processes for regression.
\newblock In \emph{2016 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  3338--3345. IEEE, 2016.

\bibitem[Caruana(1997)]{caruana1997multitask}
Caruana, R.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28:\penalty0 41--75, 1997.

\bibitem[Chen et~al.(2018)Chen, Badrinarayanan, Lee, and
  Rabinovich]{GradNorm2018}
Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A.
\newblock {G}rad{N}orm: Gradient normalization for adaptive loss balancing in
  deep multitask networks.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  794--803. PMLR, 10--15 Jul 2018.

\bibitem[Chen et~al.(2020)Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and
  Anguelov]{NEURIPS2020_GradDrop}
Chen, Z., Ngiam, J., Huang, Y., Luong, T., Kretzschmar, H., Chai, Y., and
  Anguelov, D.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  2039--2050. Curran Associates, Inc., 2020.

\bibitem[Chennupati et~al.(2019)Chennupati, Sistu, Yogamani, and
  Rawashdeh]{GLS2019_CVPR}
Chennupati, S., Sistu, G., Yogamani, S., and Rawashdeh, S.
\newblock Multinet++: Multi-stream feature aggregation and geometric loss
  strategy for multi-task learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR) Workshops}, June 2019.

\bibitem[Daheim et~al.(2023)Daheim, M{\"o}llenhoff, Ponti, Gurevych, and
  Khan]{daheim2023model}
Daheim, N., M{\"o}llenhoff, T., Ponti, E., Gurevych, I., and Khan, M.~E.
\newblock Model merging by uncertainty-based gradient matching.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Dai et~al.(2023)Dai, Fei, and Lu]{dai2023improvable}
Dai, Y., Fei, N., and Lu, Z.
\newblock Improvable gap balancing for multi-task learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  496--506.
  PMLR, 2023.

\bibitem[D'Angelo \& Fortuin(2021)D'Angelo and Fortuin]{d2021repulsive}
D'Angelo, F. and Fortuin, V.
\newblock Repulsive deep ensembles are {B}ayesian.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3451--3465, 2021.

\bibitem[Davies(1973)]{davies1973numerical}
Davies, R.~B.
\newblock Numerical inversion of a characteristic function.
\newblock \emph{Biometrika}, 60\penalty0 (2):\penalty0 415--417, 1973.

\bibitem[Daxberger et~al.(2021)Daxberger, Kristiadi, Immer, Eschenhagen, Bauer,
  and Hennig]{daxberger2021laplace}
Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and
  Hennig, P.
\newblock Laplace redux-effortless bayesian deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20089--20103, 2021.

\bibitem[D{\'e}sid{\'e}ri(2012)]{desideri2012multiple}
D{\'e}sid{\'e}ri, J.-A.
\newblock Multiple-gradient descent algorithm ({MGDA}) for multiobjective
  optimization.
\newblock \emph{Comptes Rendus Mathematique}, 350\penalty0 (5-6):\penalty0
  313--318, 2012.

\bibitem[Devin et~al.(2017)Devin, Gupta, Darrell, Abbeel, and
  Levine]{devin2017learning}
Devin, C., Gupta, A., Darrell, T., Abbeel, P., and Levine, S.
\newblock Learning modular neural network policies for multi-task and
  multi-robot transfer.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  2169--2176. IEEE, 2017.

\bibitem[Dimitriadis et~al.(2023)Dimitriadis, Frossard, and
  Fleuret]{dimitriadis2023pareto}
Dimitriadis, N., Frossard, P., and Fleuret, F.
\newblock Pareto manifold learning: Tackling multiple tasks via ensembles of
  single-task models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8015--8052. PMLR, 2023.

\bibitem[Elich et~al.(2023)Elich, Kirchdorfer, K{\"o}hler, and
  Schott]{elich2023challenging}
Elich, C., Kirchdorfer, L., K{\"o}hler, J.~M., and Schott, L.
\newblock Challenging common assumptions in multi-task learning.
\newblock \emph{arXiv preprint arXiv:2311.04698}, 2023.

\bibitem[Fernando et~al.(2023)Fernando, Shen, Liu, Chaudhury, Murugesan, and
  Chen]{Fernando2023MitigatingGB}
Fernando, H., Shen, H., Liu, M., Chaudhury, S., Murugesan, K., and Chen, T.
\newblock Mitigating gradient bias in multi-objective learning: A provably
  convergent stochastic approach.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{fey2019fast}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with pytorch geometric.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem[Fortuin et~al.(2021)Fortuin, Garriga-Alonso, Ober, Wenzel, Ratsch,
  Turner, van~der Wilk, and Aitchison]{fortuin2021bayesian}
Fortuin, V., Garriga-Alonso, A., Ober, S.~W., Wenzel, F., Ratsch, G., Turner,
  R.~E., van~der Wilk, M., and Aitchison, L.
\newblock Bayesian neural network priors revisited.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International conference on machine learning}, pp.\
  1263--1272. PMLR, 2017.

\bibitem[Guo et~al.(2018)Guo, Haque, Huang, Yeung, and
  Fei-Fei]{DTP_Guo_2018_ECCV}
Guo, M., Haque, A., Huang, D.-A., Yeung, S., and Fei-Fei, L.
\newblock Dynamic task prioritization for multitask learning.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem[Immer et~al.(2021)Immer, Bauer, Fortuin, R{\"a}tsch, and
  Emtiyaz]{immer2021scalable}
Immer, A., Bauer, M., Fortuin, V., R{\"a}tsch, G., and Emtiyaz, K.~M.
\newblock Scalable marginal likelihood estimation for model selection in deep
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4563--4573. PMLR, 2021.

\bibitem[Javaloy \& Valera(2022)Javaloy and Valera]{javaloy2022rotograd}
Javaloy, A. and Valera, I.
\newblock Rotograd: Gradient homogenization in multitask learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kendall et~al.(2018)Kendall, Gal, and Cipolla]{kendall2018multi}
Kendall, A., Gal, Y., and Cipolla, R.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7482--7491, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Kingma2014AdamAM}
Kingma, D.~P. and Ba, J.
\newblock {ADAM}: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations}, 2015.

\bibitem[Kristiadi et~al.(2020)Kristiadi, Hein, and Hennig]{kristiadi2020being}
Kristiadi, A., Hein, M., and Hennig, P.
\newblock Being {B}ayesian, even just a bit, fixes overconfidence in {R}elu
  networks.
\newblock In \emph{International conference on machine learning}, pp.\
  5436--5446. PMLR, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Kurin et~al.(2022)Kurin, De~Palma, Kostrikov, Whiteson, and
  Mudigonda]{kurin2022defense}
Kurin, V., De~Palma, A., Kostrikov, I., Whiteson, S., and Mudigonda, P.~K.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 12169--12183, 2022.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lin et~al.(2022)Lin, Ye, Zhang, and Tsang]{lin2022reasonable}
Lin, B., Ye, F., Zhang, Y., and Tsang, I.~W.
\newblock Reasonable effectiveness of random weighting: A litmus test for
  multi-task learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Liu et~al.(2021)Liu, Liu, Jin, Stone, and Liu]{liu2021conflict}
Liu, B., Liu, X., Jin, X., Stone, P., and Liu, Q.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18878--18890, 2021.

\bibitem[Liu et~al.(2023)Liu, Feng, Stone, and Liu]{liu2023famo}
Liu, B., Feng, Y., Stone, P., and Liu, Q.
\newblock Famo: Fast adaptive multitask optimization, 2023.

\bibitem[Liu et~al.(2020)Liu, Li, Kuang, Xue, Chen, Yang, Liao, and
  Zhang]{liu2020towards}
Liu, L., Li, Y., Kuang, Z., Xue, J.-H., Chen, Y., Yang, W., Liao, Q., and
  Zhang, W.
\newblock Towards impartial multi-task learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Johns, and Davison]{liu2019end}
Liu, S., Johns, E., and Davison, A.~J.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  1871--1880, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, He, Chen, and Gao]{liu2019multi}
Liu, X., He, P., Chen, W., and Gao, J.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4487--4496, 2019{\natexlab{b}}.

\bibitem[MacKay(1992)]{mackay1992bayesian}
MacKay, D.~J.
\newblock Bayesian interpolation.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 415--447, 1992.

\bibitem[Maninis et~al.(2019)Maninis, Radosavovic, and
  Kokkinos]{maninis2019attentive}
Maninis, K.-K., Radosavovic, I., and Kokkinos, I.
\newblock Attentive single-tasking of multiple tasks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  1851--1860, 2019.

\bibitem[Martens \& Sutskever(2011)Martens and Sutskever]{martens2011learning}
Martens, J. and Sutskever, I.
\newblock Learning recurrent neural networks with hessian-free optimization.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pp.\  1033--1040, 2011.

\bibitem[Matena \& Raffel(2022)Matena and Raffel]{matena2022merging}
Matena, M.~S. and Raffel, C.~A.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17703--17716, 2022.

\bibitem[Michelsanti et~al.(2021)Michelsanti, Tan, Zhang, Xu, Yu, Yu, and
  Jensen]{michelsanti2021overview}
Michelsanti, D., Tan, Z.-H., Zhang, S.-X., Xu, Y., Yu, M., Yu, D., and Jensen,
  J.
\newblock An overview of deep-learning-based audio-visual speech enhancement
  and separation.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29:\penalty0 1368--1396, 2021.

\bibitem[Minka(2001)]{minka2001expectation}
Minka, T.~P.
\newblock Expectation propagation for approximate bayesian inference.
\newblock In \emph{Proceedings of the Seventeenth conference on Uncertainty in
  artificial intelligence}, pp.\  362--369, 2001.

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and
  Hebert]{CrossStitch_2016}
Misra, I., Shrivastava, A., Gupta, A., and Hebert, M.
\newblock Cross-stitch networks for multi-task learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  3994--4003, 06 2016.
\newblock \doi{10.1109/CVPR.2016.433}.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Naeini, M.~P., Cooper, G.~F., and Hauskrecht, M.
\newblock Obtaining well calibrated probabilities using {B}ayesian binning.
\newblock In \emph{Proceedings of the Twenty-Ninth {AAAI} Conference on
  Artificial Intelligence, January 25-30, 2015, Austin, Texas, {USA}}, pp.\
  2901--2907. {AAAI} Press, 2015.

\bibitem[Navon et~al.(2022)Navon, Shamsian, Achituve, Maron, Kawaguchi,
  Chechik, and Fetaya]{navon2022multi}
Navon, A., Shamsian, A., Achituve, I., Maron, H., Kawaguchi, K., Chechik, G.,
  and Fetaya, E.
\newblock Multi-task learning as a bargaining game.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  16428--16446. PMLR, 2022.

\bibitem[Neal \& Hinton(1998)Neal and Hinton]{neal1998view}
Neal, R.~M. and Hinton, G.~E.
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock In \emph{Learning in graphical models}, pp.\  355--368. Springer,
  1998.

\bibitem[Ramakrishnan et~al.(2014)Ramakrishnan, Dral, Rupp, and
  Von~Lilienfeld]{ramakrishnan2014quantum}
Ramakrishnan, R., Dral, P.~O., Rupp, M., and Von~Lilienfeld, O.~A.
\newblock Quantum chemistry structures and properties of 134 kilo molecules.
\newblock \emph{Scientific data}, 1\penalty0 (1):\penalty0 1--7, 2014.

\bibitem[Rosenbaum et~al.(2018)Rosenbaum, Klinger, and
  Riemer]{rosenbaum2018routing}
Rosenbaum, C., Klinger, T., and Riemer, M.
\newblock Routing networks: Adaptive selection of non-linear functions for
  multi-task learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ruder(2017)]{ruder2017overview}
Ruder, S.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Image{N}et large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115:\penalty0
  211--252, 2015.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[S{\"{a}}rkk{\"{a}}(2013)]{SimoBayes}
S{\"{a}}rkk{\"{a}}, S.
\newblock \emph{Bayesian Filtering and Smoothing}, volume~3 of \emph{Institute
  of Mathematical Statistics textbooks}.
\newblock Cambridge University Press, 2013.

\bibitem[Saul et~al.(1996)Saul, Jaakkola, and Jordan]{saul1996mean}
Saul, L.~K., Jaakkola, T., and Jordan, M.~I.
\newblock Mean field theory for {S}igmoid {B}elief {N}etworks.
\newblock \emph{Journal of artificial intelligence research}, 4:\penalty0
  61--76, 1996.

\bibitem[Schaul et~al.(2019)Schaul, Borsa, Modayil, and Pascanu]{schaul2019ray}
Schaul, T., Borsa, D., Modayil, J., and Pascanu, R.
\newblock Ray interference: a source of plateaus in deep reinforcement
  learning, 2019.

\bibitem[Schraudolph(2002)]{schraudolph2002fast}
Schraudolph, N.~N.
\newblock Fast curvature matrix-vector products for second-order gradient
  descent.
\newblock \emph{Neural computation}, 14\penalty0 (7):\penalty0 1723--1738,
  2002.

\bibitem[Sener \& Koltun(2018)Sener and Koltun]{sener2018multi}
Sener, O. and Koltun, V.
\newblock Multi-task learning as multi-objective optimization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Senushkin et~al.(2023)Senushkin, Patakin, Kuznetsov, and
  Konushin]{senushkin2023independent}
Senushkin, D., Patakin, N., Kuznetsov, A., and Konushin, A.
\newblock Independent component alignment for multi-task learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  20083--20093, 2023.

\bibitem[Shamshad et~al.(2023)Shamshad, Khan, Zamir, Khan, Hayat, Khan, and
  Fu]{shamshad2023transformers}
Shamshad, F., Khan, S., Zamir, S.~W., Khan, M.~H., Hayat, M., Khan, F.~S., and
  Fu, H.
\newblock Transformers in medical imaging: A survey.
\newblock \emph{Medical Image Analysis}, pp.\  102802, 2023.

\bibitem[Shamsian et~al.(2023)Shamsian, Navon, Glazer, Kawaguchi, Chechik, and
  Fetaya]{shamsian2023auxiliary}
Shamsian, A., Navon, A., Glazer, N., Kawaguchi, K., Chechik, G., and Fetaya, E.
\newblock Auxiliary learning as an asymmetric bargaining game.
\newblock \emph{arXiv preprint arXiv:2301.13501}, 2023.

\bibitem[Shi et~al.(2023)Shi, Ren, Zhang, and Pan]{shi2023deep}
Shi, H., Ren, S., Zhang, T., and Pan, S.~J.
\newblock Deep multitask learning with progressive parameter sharing.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  19924--19935, 2023.

\bibitem[Shu et~al.(2018)Shu, Xiong, and Socher]{shu2018hierarchical}
Shu, T., Xiong, C., and Socher, R.
\newblock Hierarchical and interpretable skill acquisition in multi-task
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N.,
  Patwary, M., Prabhat, M., and Adams, R.
\newblock Scalable {B}ayesian optimization using deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  2171--2180. PMLR, 2015.

\bibitem[Standley et~al.(2020)Standley, Zamir, Chen, Guibas, Malik, and
  Savarese]{standley2020tasks}
Standley, T., Zamir, A., Chen, D., Guibas, L., Malik, J., and Savarese, S.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9120--9132. PMLR, 2020.

\bibitem[Taslimi et~al.(2022)Taslimi, Taslimi, Fathi, Salehi, and
  Rohban]{taslimi2022swinchex}
Taslimi, S., Taslimi, S., Fathi, N., Salehi, M., and Rohban, M.~H.
\newblock Swinche{X}: Multi-label classification on chest {X}-ray images with
  transformers.
\newblock \emph{arXiv preprint arXiv:2206.04246}, 2022.

\bibitem[Vinyals et~al.(2016)Vinyals, Bengio, and Kudlur]{VinyalsBK15}
Vinyals, O., Bengio, S., and Kudlur, M.
\newblock Order matters: Sequence to sequence for sets.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{4th International
  Conference on Learning Representations, {ICLR}}, 2016.

\bibitem[Wang et~al.(2017)Wang, Peng, Lu, Lu, Bagheri, and
  Summers]{wang2017chestx}
Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., and Summers, R.~M.
\newblock Chest{X}-ray8: Hospital-scale chest {X}-ray database and benchmarks
  on weakly-supervised classification and localization of common thorax
  diseases.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2097--2106, 2017.

\bibitem[Wang et~al.(2020)Wang, Tsvetkov, Firat, and Cao]{wang2020gradient}
Wang, Z., Tsvetkov, Y., Firat, O., and Cao, Y.
\newblock Gradient vaccine: Investigating and improving multi-task optimization
  in massively multilingual models.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wild et~al.(2024)Wild, Ghalebikesabi, Sejdinovic, and
  Knoblauch]{wild2024rigorous}
Wild, V.~D., Ghalebikesabi, S., Sejdinovic, D., and Knoblauch, J.
\newblock A rigorous link between deep ensembles and (variational) {B}ayesian
  methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Wilson \& Izmailov(2020)Wilson and Izmailov]{wilson2020bayesian}
Wilson, A.~G. and Izmailov, P.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 4697--4708, 2020.

\bibitem[Wilson et~al.(2016{\natexlab{a}})Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Deep kernel learning.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  370--378.
  PMLR, 2016{\natexlab{a}}.

\bibitem[Wilson et~al.(2016{\natexlab{b}})Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016stochastic}
Wilson, A.~G., Hu, Z., Salakhutdinov, R.~R., and Xing, E.~P.
\newblock Stochastic variational deep kernel learning.
\newblock \emph{Advances in neural information processing systems}, 29,
  2016{\natexlab{b}}.

\bibitem[Wu et~al.(2018)Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
  Leswing, and Pande]{wu2018moleculenet}
Wu, Z., Ramsundar, B., Feinberg, E.~N., Gomes, J., Geniesse, C., Pappu, A.~S.,
  Leswing, K., and Pande, V.
\newblock Molecule{N}et: a benchmark for molecular machine learning.
\newblock \emph{Chemical science}, 9\penalty0 (2):\penalty0 513--530, 2018.

\bibitem[Xin et~al.(2022)Xin, Ghorbani, Gilmer, Garg, and
  Firat]{xin2022current}
Xin, D., Ghorbani, B., Gilmer, J., Garg, A., and Firat, O.
\newblock Do current multi-task optimization methods in deep learning even
  help?
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 13597--13609, 2022.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5824--5836, 2020.

\bibitem[Yun \& Cho(2023)Yun and Cho]{Yun_2023_ICCV}
Yun, H. and Cho, H.
\newblock Achievement-based training progress balancing for multi-task
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pp.\  16935--16944, October 2023.

\bibitem[Zhang et~al.(2017)Zhang, Song, and Qi]{zhang2017age}
Zhang, Z., Song, Y., and Qi, H.
\newblock Age progression/regression by conditional adversarial autoencoder.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  5810--5818, 2017.

\bibitem[Zheng et~al.(2023)Zheng, Wu, Chen, Yang, Zhu, Shen, Kehtarnavaz, and
  Shah]{zheng2023deep}
Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J., Kehtarnavaz, N., and
  Shah, M.
\newblock Deep learning-based human pose estimation: A survey.
\newblock \emph{ACM Computing Surveys}, 56\penalty0 (1):\penalty0 1--37, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Li, Li, Yu, Liu, Wang, Zhang, Ji, Yan, He,
  et~al.]{zhou2023comprehensive}
Zhou, C., Li, Q., Li, C., Yu, J., Liu, Y., Wang, G., Zhang, K., Ji, C., Yan,
  Q., He, L., et~al.
\newblock A comprehensive survey on pretrained foundation models: A history
  from {BERT} to {ChatGPT}.
\newblock \emph{arXiv preprint arXiv:2302.09419}, 2023.

\end{thebibliography}
