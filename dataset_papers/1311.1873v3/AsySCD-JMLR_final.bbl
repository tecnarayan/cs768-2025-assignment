\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Duchi(2011)]{AgarwalD12}
A.~Agarwal and J.~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages
  873--881. 2011.
\newblock URL
  \url{http://papers.nips.cc/paper/4247-distributed-delayed-stochastic-optimization.pdf}.

\bibitem[{Avron} et~al.(2014){Avron}, {Druinsky}, and {Gupta}]{Avron13arXiv}
H.~{Avron}, A.~{Druinsky}, and A.~{Gupta}.
\newblock Revisiting asynchronous linear solvers: Provable convergence rate
  through randomization.
\newblock \emph{IPDPS}, 2014.

\bibitem[Beck and Teboulle(2009)]{BeckT09}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM J. Imaging Sciences}, 2\penalty0 (1):\penalty0 183--202,
  2009.

\bibitem[Beck and Tetruashvili(2013)]{Beck13}
A.~Beck and L.~Tetruashvili.
\newblock On the convergence of block coordinate descent type methods.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2037--2060, 2013.

\bibitem[Bertsekas and Tsitsiklis(1989)]{Bertsekas89}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock \emph{Parallel and Distributed Computation: Numerical Methods}.
\newblock Pentice Hall, 1989.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and Eckstein]{Boyd11}
S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends in Machine Learning}, 3\penalty0
  (1):\penalty0 1--122, 2011.

\bibitem[Chang and Lin(2011)]{LIBSVM11}
C.-C. Chang and C.-J. Lin.
\newblock {LIBSVM}: A library for support vector machines, 2011.
\newblock URL \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}.

\bibitem[Cortes and Vapnik(1995)]{CortesVapnik95}
C.~Cortes and V.~Vapnik.
\newblock Support vector networks.
\newblock \emph{Machine Learning}, pages 273--297, 1995.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and Sridharan]{Cotter11}
A.~Cotter, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages
  1647--1655. 2011.
\newblock URL
  \url{http://papers.nips.cc/paper/4432-better-mini-batch-algorithms-via-accelerated-gradient-methods.pdf}.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{Duchi12}
J.~C. Duchi, A.~Agarwal, and M.~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic Control}, 57\penalty0
  (3):\penalty0 592--606, 2012.

\bibitem[Ferris and Mangasarian(1994)]{Ferris94}
M.~C. Ferris and O.~L. Mangasarian.
\newblock Parallel variable distribution.
\newblock \emph{SIAM Journal on Optimization}, 4\penalty0 (4):\penalty0
  815--832, 1994.

\bibitem[Goldfarb and Ma(2012)]{Ma12}
D.~Goldfarb and S.~Ma.
\newblock Fast multiple-splitting algorithms for convex optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  533--556, 2012.

\bibitem[Liu and Wright(2014)]{liu2014asynchronous}
J.~Liu and S.~J. Wright.
\newblock Asynchronous stochastic coordinate descent: Parallelism and
  convergence properties.
\newblock Technical Report arXiv:1403.3862, 2014.

\bibitem[Lu and Xiao(2013)]{LuXiao13}
Z.~Lu and L.~Xiao.
\newblock On the complexity analysis of randomized block-coordinate descent
  methods.
\newblock {Technical Report} arXiv:1305.4723, Simon Fraser University, 2013.

\bibitem[Luo and Tseng(1992)]{LuoTseng92}
Z.~Q. Luo and P.~Tseng.
\newblock On the convergence of the coordinate descent method for convex
  differentiable minimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 72:\penalty0
  7--35, 1992.

\bibitem[Mangasarian(1995)]{Mangasarian95}
O.~L. Mangasarian.
\newblock Parallel gradient distribution in unconstrained optimization.
\newblock \emph{SIAM Journal on Optimization}, 33\penalty0 (1):\penalty0
  916--1925, 1995.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{Nemirovski09}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19:\penalty0 1574--1609, 2009.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Y.~Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nesterov(2012)]{Nesterov12}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Niu et~al.(2011)Niu, Recht, R{\'e}, and Wright]{Hogwild11nips}
F.~Niu, B.~Recht, C.~R{\'e}, and S.~J. Wright.
\newblock {\sc Hogwild!}: A lock-free approach to parallelizing stochastic
  gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems 24}, pages
  693--701, 2011.

\bibitem[Peng et~al.(2013)Peng, Yan, and Yin]{Yin13}
Z.~Peng, M.~Yan, and W.~Yin.
\newblock Parallel and distributed sparse optimization.
\newblock Preprint, 2013.

\bibitem[{Richt{\'a}rik} and {Tak{\'a}{\v c}}(2012{\natexlab{a}})]{Richtarik11}
P.~{Richt{\'a}rik} and M.~{Tak{\'a}{\v c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematrical Programming}, 144:\penalty0 1--38,
  2012{\natexlab{a}}.

\bibitem[{Richt{\'a}rik} and {Tak{\'a}{\v
  c}}(2012{\natexlab{b}})]{Richtarik12arXiv}
P.~{Richt{\'a}rik} and M.~{Tak{\'a}{\v c}}.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock Technical Report arXiv:1212.0873, 2012{\natexlab{b}}.

\bibitem[Scherrer et~al.(2012)Scherrer, Tewari, Halappanavar, and
  Haglin]{ScherrerTHH12}
C.~Scherrer, A.~Tewari, M.~Halappanavar, and D.~Haglin.
\newblock Feature clustering for accelerating parallel coordinate descent.
\newblock \emph{Advances in Neural Information Processing Systems 25}, pages
  28--36, 2012.

\bibitem[Shalev-Shwartz and Zhang(2013)]{Shalev-Shwartz2013}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock \emph{Advances in Neural Information Processing Systems 26}, pages
  378--385, 2013.

\bibitem[Shamir and Zhang(2013)]{Shamir2013icml}
O.~Shamir and T.~Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, 2013.

\bibitem[Tibshirani(1996)]{Tibshirani96LASSO}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society, Series B},
  58:\penalty0 267--288, 1996.

\bibitem[Tseng(2001)]{Tseng01}
P.~Tseng.
\newblock Convergence of a block coordinate descent method for
  nondifferentiable minimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 109:\penalty0
  475--494, 2001.

\bibitem[Tseng and Yun(2009)]{TseY06}
P.~Tseng and S.~Yun.
\newblock A coordinate gradient descent method for nonsmooth separable
  minimization.
\newblock \emph{Mathematical Programming, Series {B}}, 117:\penalty0 387--423,
  June 2009.

\bibitem[Tseng and Yun(2010)]{TseY07a}
P.~Tseng and S.~Yun.
\newblock A coordinate gradient descent method for linearly constrained smooth
  optimization and support vector machines training.
\newblock \emph{Computational Optimization and Applications}, 47\penalty0
  (2):\penalty0 179--206, 2010.

\bibitem[Wang and Lin(2014)]{WangLin13}
P.-W. Wang and C.-J. Lin.
\newblock Iteration complexity of feasible descent methods for convex
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1523--1548,
  2014.

\bibitem[Wright(2012)]{Wright12}
S.~J. Wright.
\newblock Accelerated block-coordinate relaxation for regularized optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (1):\penalty0
  159--186, 2012.

\bibitem[Yang(2013)]{Yang13}
T.~Yang.
\newblock Trading computation for communication: Distributed stochastic dual
  coordinate ascent.
\newblock \emph{Advances in Neural Information Processing Systems 26}, pages
  629--637, 2013.

\end{thebibliography}
