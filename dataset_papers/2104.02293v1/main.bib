@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@misc{AS88,
  title={Handbook of mathematical functions with formulas, graphs, and mathematical tables},
  author={Abramowitz, Milton and Stegun, Irene A and Romer, Robert H},
  year={1988},
  publisher={American Association of Physics Teachers}
}

@unpublished{SuVPKuhn20,
  title         = "A General Framework for Optimal {Data-Driven} Optimization",
  author        = "Sutter, Tobias and Van Parys, Bart P G and Kuhn, Daniel",
  month         =  oct,
  year          =  2020,
  url           = "http://arxiv.org/abs/2010.06606",
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2010.06606"
}


@unpublished{BuGeBe20,
  title         = "The Importance of Pessimism in {Fixed-Dataset} Policy
                   Optimization",
  author        = "Buckman, Jacob and Gelada, Carles and Bellemare, Marc G",
  month         =  sep,
  year          =  2020,
  url           = "http://arxiv.org/abs/2009.06799",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2009.06799"
}


@ARTICLE{DeKiWi19,
  title     = {``Dice''-sion--Making Under Uncertainty: When Can a Random
               Decision Reduce Risk?},
  author    = "Delage, Erick and Kuhn, Daniel and Wiesemann, Wolfram",
  journal   = "Management Science",
  publisher = "INFORMS",
  volume    =  65,
  number    =  7,
  pages     = "3282--3301",
  month     =  jul,
  year      =  2019,
}




@unpublished{FaTaVaSmDo19,
  title         = "Distributionally Robust Counterfactual Risk Minimization",
  author        = "Faury, Louis and Tanielian, Ugo and Vasile, Flavian and
                   Smirnova, Elena and Dohmatob, Elvis",
  abstract      = "This manuscript introduces the idea of using
                   Distributionally Robust Optimization (DRO) for the
                   Counterfactual Risk Minimization (CRM) problem. Tapping into
                   a rich existing literature, we show that DRO is a principled
                   tool for counterfactual decision making. We also show that
                   well-established solutions to the CRM problem like sample
                   variance penalization schemes are special instances of a
                   more general DRO problem. In this unifying framework, a
                   variety of distributionally robust counterfactual risk
                   estimators can be constructed using various probability
                   distances and divergences as uncertainty measures. We
                   propose the use of Kullback-Leibler divergence as an
                   alternative way to model uncertainty in CRM and derive a new
                   robust counterfactual objective. In our experiments, we show
                   that this approach outperforms the state-of-the-art on four
                   benchmark datasets, validating the relevance of using other
                   uncertainty measures in practical applications.",
  month         =  jun,
  year          =  2019,
  url           = "http://arxiv.org/abs/1906.06211",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1906.06211"
}


@unpublished{DuGlNam16,
  title    = "Statistics of Robust Optimization: A Generalized Empirical
              Likelihood Approach",
  author   = "Duchi, John and Glynn, Peter and Namkoong, Hongseok",
  abstract = "We study statistical inference and distributionally robust
              solution methods for stochastic optimization problems, focusing
              on confidence intervals for optimal values and solutions that
              achieve exact coverage asymptotically. We develop a generalized
              empirical likelihood framework---based on distributional
              uncertainty sets constructed from nonparametric $f$-divergence
              balls---for Hadamard differentiable functionals, and in
              particular, stochastic optimization problems. As consequences of
              this theory, we provide a principled method for choosing the size
              of distributional uncertainty regions to provide one- and
              two-sided confidence intervals that achieve exact coverage. We
              also give an asymptotic expansion for our distributionally robust
              formulation, showing how robustification regularizes problems by
              their variance. Finally, we show that optimizers of the
              distributionally robust formulations we study enjoy (essentially)
              the same consistency properties as those in classical sample
              average approximations. Our general approach applies to quickly
              mixing stationary sequences, including geometrically ergodic
              Harris recurrent Markov chains.",
  month    =  oct,
  year     =  2016,
  url      = "https://arxiv.org/abs/1610.03425v3"
}

@incollection{Scarf58,
author = {Scarf, H.},
year = {1958},
title = {A min-max solution of an inventory problem},
editor = {K. S. Arrow and S. Karlin and H. E. Scarf},
booktitle = {Studies in the Mathematical Theory of Inventory and Production},
publisher = {Stanford University Press}, 
address = {Stanford, CA}, 
pages = {201--209}
}


@unpublished{RaMe19,
  title         = "Distributionally Robust Optimization: A Review",
  author        = "Rahimian, Hamed and Mehrotra, Sanjay",
  abstract      = "The concepts of risk-aversion, chance-constrained
                   optimization, and robust optimization have developed
                   significantly over the last decade. Statistical learning
                   community has also witnessed a rapid theoretical and applied
                   growth by relying on these concepts. A modeling framework,
                   called distributionally robust optimization (DRO), has
                   recently received significant attention in both the
                   operations research and statistical learning communities.
                   This paper surveys main concepts and contributions to DRO,
                   and its relationships with robust optimization,
                   risk-aversion, chance-constrained optimization, and function
                   regularization.",
  month         =  aug,
  year          =  2019,
  url           = "http://arxiv.org/abs/1908.05659",
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1908.05659"
}


@BOOK{Berger85,
  title     = "Statistical Decision Theory and Bayesian Analysis",
  author    = "Berger, James O",
  publisher = "Springer New York",
  series    = "Springer Series in Statistics",
  month     =  jan,
  year      =  1985,
  url       = "http://link.springer.com/10.1007/978-1-4757-4286-2",
  address   = "New York, NY",
  keywords  = "Mathematics"
}


@ARTICLE{DeYe10,
  title     = "Distributionally Robust Optimization Under Moment Uncertainty
               with Application to Data-Driven Problems",
  author    = "Delage, Erick and Ye, Yinyu",
  abstract  = "Stochastic programming can effectively describe many
               decision-making problems in uncertain environments.
               Unfortunately, such programs are often computationally demanding
               to solve. In addition, their solution can be misleading when
               there is ambiguity in the choice of a distribution for the
               random parameters. In this paper, we propose a model that
               describes uncertainty in both the distribution form (discrete,
               Gaussian, exponential, etc.) and moments (mean and covariance
               matrix). We demonstrate that for a wide range of cost functions
               the associated distributionally robust (or min-max) stochastic
               program can be solved efficiently. Furthermore, by deriving a
               new confidence region for the mean and the covariance matrix of
               a random vector, we provide probabilistic arguments for using
               our model in problems that rely heavily on historical data.
               These arguments are confirmed in a practical example of
               portfolio selection, where our framework leads to
               better-performing policies on the ?true? distribution underlying
               the daily returns of financial assets.",
  journal   = "Oper. Res.",
  publisher = "INFORMS",
  volume    =  58,
  number    =  3,
  pages     = "595--612",
  month     =  jun,
  year      =  2010,
  url       = "https://doi.org/10.1287/opre.1090.0741"
}



@unpublished{DuNam18,
  title         = "Learning Models with Uniform Performance via
                   Distributionally Robust Optimization",
  author        = "Duchi, John and Namkoong, Hongseok",
  abstract      = "A common goal in statistics and machine learning is to learn
                   models that can perform well against distributional shifts,
                   such as latent heterogeneous subpopulations, unknown
                   covariate shifts, or unmodeled temporal effects. We develop
                   and analyze a distributionally robust stochastic
                   optimization (DRO) framework that learns a model providing
                   good performance against perturbations to the
                   data-generating distribution. We give a convex formulation
                   for the problem, providing several convergence guarantees.
                   We prove finite-sample minimax upper and lower bounds,
                   showing that distributional robustness sometimes comes at a
                   cost in convergence rates. We give limit theorems for the
                   learned parameters, where we fully specify the limiting
                   distribution so that confidence intervals can be computed.
                   On real tasks including generalizing to unknown
                   subpopulations, fine-grained recognition, and providing good
                   tail performance, the distributionally robust approach often
                   exhibits improved performance.",
  month         =  oct,
  year          =  2018,
  url           = "http://arxiv.org/abs/1810.08750",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1810.08750"
}


@unpublished{DeSh20,
  title         = "Distributional Robustness and Regularization in
                   Reinforcement Learning",
  author        = "Derman, Esther and Mannor, Shie",
  abstract      = "Distributionally Robust Optimization (DRO) has enabled to
                   prove the equivalence between robustness and regularization
                   in classification and regression, thus providing an
                   analytical reason why regularization generalizes well in
                   statistical learning. Although DRO's extension to sequential
                   decision-making overcomes $\textit\{external uncertainty\}$
                   through the robust Markov Decision Process (MDP) setting,
                   the resulting formulation is hard to solve, especially on
                   large domains. On the other hand, existing regularization
                   methods in reinforcement learning only address
                   $\textit\{internal uncertainty\}$ due to stochasticity. Our
                   study aims to facilitate robust reinforcement learning by
                   establishing a dual relation between robust MDPs and
                   regularization. We introduce Wasserstein distributionally
                   robust MDPs and prove that they hold out-of-sample
                   performance guarantees. Then, we introduce a new regularizer
                   for empirical value functions and show that it lower bounds
                   the Wasserstein distributionally robust value function. We
                   extend the result to linear value function approximation for
                   large state spaces. Our approach provides an alternative
                   formulation of robustness with guaranteed finite-sample
                   performance. Moreover, it suggests using regularization as a
                   practical tool for dealing with $\textit\{external
                   uncertainty\}$ in reinforcement learning methods.",
  month         =  mar,
  year          =  2020,
  url           = "http://arxiv.org/abs/2003.02894",
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2003.02894"
}


@ARTICLE{GiSch89,
  title    = "Maxmin expected utility with non-unique prior",
  author   = "Gilboa, Itzhak and Schmeidler, David",
  journal  = "Journal of Mathematical Economics",
  volume   =  18,
  number   =  2,
  pages    = "141--153",
  year     =  1989,
}

@unpublished{VPEKuhn17,
  title         = "From Data to Decisions: Distributionally Robust Optimization
                   is Optimal",
  author        = "Van Parys, Bart P G and Esfahani, Peyman Mohajerin and Kuhn,
                   Daniel",
  month         =  apr,
  year          =  2017,
  url           = "http://arxiv.org/abs/1704.04118",
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1704.04118"
}

@unpublished{argenson2020model,
  title={Model-based offline planning},
  author={Argenson, Arthur and Dulac-Arnold, Gabriel},
  journal={arXiv preprint arXiv:2008.05556},
  year={2020}
}

@INPROCEEDINGS{KiRaNeJo20,
  title     = "{MOReL}: Model-Based Offline Reinforcement Learning",
  booktitle = "{NeurIPS}",
  author    = "Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth
               and Joachims, Thorsten",
  year      =  2020,
  url       = "https://papers.nips.cc/paper/2020/hash/f7efa4f864ae9b88d43527f4b14f750f-Abstract.html"
}

@unpublished{jin2020pessimism,
  title={Is Pessimism Provably Efficient for Offline {RL}?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2012.15085},
  year={2020}
}


@ARTICLE{SmiWi06,
  title     = "The Optimizer's Curse: Skepticism and Postdecision Surprise in
               Decision Analysis",
  author    = "Smith, James E and Winkler, Robert L",
  abstract  = "Decision analysis produces measures of value such as expected
               net present values or expected utilities and ranks alternatives
               by these value estimates. Other optimization-based processes
               operate in a similar manner. With uncertainty and limited
               resources, an analysis is never perfect, so these value
               estimates are subject to error. We show that if we take these
               value estimates at face value and select accordingly, we should
               expect the value of the chosen alternative to be less than its
               estimate, even if the value estimates are unbiased. Thus, when
               comparing actual outcomes to value estimates, we should expect
               to be disappointed on average, not because of any inherent bias
               in the estimates themselves, but because of the
               optimization-based selection process. We call this phenomenon
               the optimizer?s curse and argue that it is not well understood
               or appreciated in the decision analysis and management science
               communities. This curse may be a factor in creating skepticism
               in decision makers who review the results of an analysis. In
               this paper, we study the optimizer?s curse and show that the
               resulting expected disappointment may be substantial. We then
               propose the use of Bayesian methods to adjust value estimates.
               These Bayesian methods can be viewed as disciplined skepticism
               and provide a method for avoiding this postdecision
               disappointment.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  52,
  number    =  3,
  pages     = "311--322",
  month     =  mar,
  year      =  2006,
  url       = "https://doi.org/10.1287/mnsc.1050.0451"
}


@ARTICLE{Lam2019,
  title     = "Recovering Best Statistical Guarantees via the Empirical
               {Divergence-Based} Distributionally Robust Optimization",
  author    = "Lam, Henry",
  abstract  = "Distributionally robust optimization (DRO), a recent methodology
               to handle stochastic optimization problems in the presence of
               data, is based on robustifications of stochastic constraints
               that are enforced to hold over suitably constructed sets of
               underlying probability distributions. Although DRO enjoys valid
               feasibility guarantees, it often leads to over-conservative
               solutions. The paper ?Recovering best statistical guarantees via
               the empirical divergence-based distributionally robust
               optimization? by Lam studies a calibration method for
               distributional sets to combat conservativeness via a new
               interpretation of DRO through the statistical angle of empirical
               likelihood and empirical processes. The proposed method targets
               achieving precise confidence level guarantees that lead to
               superior performances over previous approaches.",
  journal   = "Oper. Res.",
  publisher = "INFORMS",
  volume    =  67,
  number    =  4,
  pages     = "1090--1105",
  month     =  jul,
  year      =  2019,
  url       = "https://doi.org/10.1287/opre.2018.1786"
}




@ARTICLE{Kuhn2019-if,
  title     = "Wasserstein distributionally robust optimization: Theory and
               applications in machine learning",
  author    = "Kuhn, D and Esfahani, P M and Nguyen, V A and {others}",
  journal   = "Science in the Age �",
  publisher = "pubsonline.informs.org",
  year      =  2019,
  url       = "https://pubsonline.informs.org/doi/abs/10.1287/educ.2019.0198"
}


@article{lai1985asymptotically,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, Tze Leung and Robbins, Herbert},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985},
  publisher={Academic Press}
}

@unpublished{yu2020mopo,
  title={MOPO: Model-based Offline Policy Optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={arXiv preprint arXiv:2005.13239},
  year={2020}
}


@article{swaminathan2015batch,
  title={Batch learning from logged bandit feedback through counterfactual risk minimization},
  author={Swaminathan, Adith and Joachims, Thorsten},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1731--1755},
  year={2015},
  publisher={JMLR. org}
}

@inproceedings{covington2016deep,
  title={Deep neural networks for {Youtube} recommendations},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM conference on recommender systems},
  pages={191--198},
  year={2016}
}


@unpublished{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@unpublished{kumar2020conservative,
  title={Conservative Q-Learning for Offline Reinforcement Learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@unpublished{liu2020provably,
  title={Provably good batch reinforcement learning without great exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:2007.08202},
  year={2020}
}

@inproceedings{strehl11learning,
  author = {Alexander L. Strehl and John Langford and Lihong Li and Sham M. Kakade},
  title = {Learning from Logged Implicit Exploration Data},
  booktitle = {Advances in Neural Information Processing Systems 23},
  year = {2011},
  pages = {2217--2225}
}

@unpublished{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@article{blumenthal1968estimation,
  title={Estimation of the larger of two normal means},
  author={Blumenthal, Saul and Cohen, Arthur},
  journal={Journal of the American Statistical Association},
  volume={63},
  number={323},
  pages={861--876},
  year={1968},
  publisher={Taylor \& Francis}
}

@unpublished{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:1906.00949},
  year={2019}
}

@unpublished{jaques2019way,
  title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@article{neyman1933ix,
  title={IX. On the problem of the most efficient tests of statistical hypotheses},
  author={Neyman, Jerzy and Pearson, Egon Sharpe},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume={231},
  number={694-706},
  pages={289--337},
  year={1933},
  publisher={The Royal Society London}
}

@article{karampatziakis2019empirical,
  title={Empirical likelihood for contextual bandits},
  author={Karampatziakis, Nikos and Langford, John and Mineiro, Paul},
  journal={arXiv preprint arXiv:1906.03323},
  year={2019}
}


@article{nilim2005robust,
  title={Robust control of Markov decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}

@inproceedings{xu2010distributionally,
  title={Distributionally robust Markov decision processes},
  author={Xu, Huan and Mannor, Shie},
  booktitle={Proceedings of the 23rd International Conference on Neural Information Processing Systems-Volume 2},
  pages={2505--2513},
  year={2010}
}

@article{yu2015distributionally,
  title={Distributionally robust counterpart in Markov decision processes},
  author={Yu, Pengqian and Xu, Huan},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={9},
  pages={2538--2543},
  year={2015},
  publisher={IEEE}
}

@article{yang2017convex,
  title={A convex optimization approach to distributionally robust Markov decision processes with Wasserstein distance},
  author={Yang, Insoon},
  journal={IEEE control systems letters},
  volume={1},
  number={1},
  pages={164--169},
  year={2017},
  publisher={IEEE}
}

@article{chen2019distributionally,
  title={Distributionally robust optimization for sequential decision-making},
  author={Chen, Zhi and Yu, Pengqian and Haskell, William B},
  journal={Optimization},
  volume={68},
  number={12},
  pages={2397--2426},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{dai2020coindice,
  title={CoinDICE: Off-policy confidence interval estimation},
  author={Dai, Bo and Nachum, Ofir and Chow, Yinlam and Li, Lihong and Szepesv{\'a}ri, Csaba and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2010.11652},
  year={2020}
}

@article{smith2006optimizer,
  title={The optimizer’s curse: Skepticism and postdecision surprise in decision analysis},
  author={Smith, James E and Winkler, Robert L},
  journal={Management Science},
  volume={52},
  number={3},
  pages={311--322},
  year={2006},
  publisher={INFORMS}
}

@article{siegel2020keep,
  title={Keep doing what worked: Behavioral modelling priors for offline reinforcement learning},
  author={Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2002.08396},
  year={2020}
}

@article{yin2021near,
  title={Near-Optimal Offline Reinforcement Learning via Double Variance Reduction},
  author={Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2102.01748},
  year={2021}
}