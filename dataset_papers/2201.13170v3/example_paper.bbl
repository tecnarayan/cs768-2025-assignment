\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adler \& Blue(2002)Adler and Blue]{adler2002cooperative}
Adler, J.~L. and Blue, V.~J.
\newblock A cooperative multi-agent transportation management and route
  guidance system.
\newblock \emph{Transportation Research Part C: Emerging Technologies},
  10\penalty0 (5-6):\penalty0 433--454, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  263--272. JMLR. org, 2017.

\bibitem[Bai \& Jin(2020)Bai and Jin]{bai2020provable}
Bai, Y. and Jin, C.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  551--560. PMLR, 2020.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Bai, Y., Jin, C., and Yu, T.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bar-On \& Mansour(2019)Bar-On and Mansour]{bar2019individual}
Bar-On, Y. and Mansour, Y.
\newblock Individual regret in cooperative nonstochastic multi-armed bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 3116--3126, 2019.

\bibitem[Bubeck et~al.(2021)Bubeck, Budzinski, and
  Sellke]{bubeck2021cooperative}
Bubeck, S., Budzinski, T., and Sellke, M.
\newblock Cooperative and stochastic multi-player multi-armed bandit: Optimal
  regret with neither communication nor collisions.
\newblock In \emph{Conference on Learning Theory}, pp.\  821--822. PMLR, 2021.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1283--1294. PMLR, 2020.

\bibitem[Cesa-Bianchi et~al.(2016)Cesa-Bianchi, Gentile, Mansour, and
  Minora]{cesa2016delay}
Cesa-Bianchi, N., Gentile, C., Mansour, Y., and Minora, A.
\newblock Delay and cooperation in nonstochastic bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  605--622. PMLR, 2016.

\bibitem[Cesa-Bianchi et~al.(2019)Cesa-Bianchi, Gentile, and
  Mansour]{cesa2019delay}
Cesa-Bianchi, N., Gentile, C., and Mansour, Y.
\newblock Delay and cooperation in nonstochastic bandits.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (17):\penalty0 1--38, 2019.

\bibitem[Choi et~al.(2009)Choi, Oh, and Horowitz]{choi2009distributed}
Choi, J., Oh, S., and Horowitz, R.
\newblock Distributed learning and cooperative control for multi-agent systems.
\newblock \emph{Automatica}, 45\penalty0 (12):\penalty0 2802--2814, 2009.

\bibitem[Cohen et~al.(2021)Cohen, Efroni, Mansour, and
  Rosenberg]{cohen2021minimax}
Cohen, A., Efroni, Y., Mansour, Y., and Rosenberg, A.
\newblock Minimax regret for stochastic shortest path.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Cortes et~al.(2004)Cortes, Martinez, Karatas, and
  Bullo]{cortes2004coverage}
Cortes, J., Martinez, S., Karatas, T., and Bullo, F.
\newblock Coverage control for mobile sensing networks.
\newblock \emph{IEEE Transactions on robotics and Automation}, 20\penalty0
  (2):\penalty0 243--255, 2004.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  5717--5727, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1507--1516. PMLR, 2019.

\bibitem[Dick et~al.(2014)Dick, Gyorgy, and Szepesvari]{dick2014online}
Dick, T., Gyorgy, A., and Szepesvari, C.
\newblock Online learning in markov decision processes with changing cost
  sequences.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  512--520. PMLR, 2014.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2021episodic}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  578--598. PMLR, 2021.

\bibitem[Dubey et~al.(2020)]{dubey2020cooperative}
Dubey, A. et~al.
\newblock Cooperative multi-agent bandits with heavy tails.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2730--2739. PMLR, 2020.

\bibitem[Efroni et~al.(2021)Efroni, Merlis, Saha, and
  Mannor]{efroni2021confidence}
Efroni, Y., Merlis, N., Saha, A., and Mannor, S.
\newblock Confidence-budget matching for sequential budgeted learning.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pp.\  2937--2947. {PMLR}, 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/efroni21a.html}.

\bibitem[Howson et~al.(2021)Howson, Pike-Burke, and Filippi]{howson2021delayed}
Howson, B., Pike-Burke, C., and Filippi, S.
\newblock Delayed feedback in episodic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2111.07615}, 2021.

\bibitem[Ito et~al.(2020)Ito, Hatano, Sumita, Takemura, Fukunaga, Kakimura, and
  Kawarabayashi]{ito2020delay}
Ito, S., Hatano, D., Sumita, H., Takemura, K., Fukunaga, T., Kakimura, N., and
  Kawarabayashi, K.-I.
\newblock Delay and cooperation in nonstochastic linear bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4872--4883, 2020.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4863--4873, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and
  Yu]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4860--4869. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143,
  2020{\natexlab{b}}.

\bibitem[Jin et~al.(2021)Jin, Liu, Wang, and Yu]{jin2021v}
Jin, C., Liu, Q., Wang, Y., and Yu, T.
\newblock V-learning--a simple, efficient, decentralized algorithm for
  multiagent rl.
\newblock \emph{arXiv preprint arXiv:2110.14555}, 2021.

\bibitem[Jin \& Luo(2020)Jin and Luo]{jin2020simultaneously}
Jin, T. and Luo, H.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Lancewicki et~al.(2020)Lancewicki, Rosenberg, and
  Mansour]{lancewicki2020learning}
Lancewicki, T., Rosenberg, A., and Mansour, Y.
\newblock Learning adversarial markov decision processes with delayed feedback.
\newblock \emph{arXiv preprint arXiv:2012.14843}, 2020.

\bibitem[Landgren et~al.(2021)Landgren, Srivastava, and
  Leonard]{landgren2021distributed}
Landgren, P., Srivastava, V., and Leonard, N.~E.
\newblock Distributed cooperative decision making in multi-agent multi-armed
  bandits.
\newblock \emph{Automatica}, 125:\penalty0 109445, 2021.

\bibitem[Lee et~al.(2002)Lee, Zhang, et~al.]{lee2002stock}
Lee, J.~W., Zhang, B.-T., et~al.
\newblock Stock trading system using reinforcement learning with cooperative
  agents.
\newblock In \emph{Proceedings of the Nineteenth International Conference on
  Machine Learning}, pp.\  451--458, 2002.

\bibitem[Lee et~al.(2007)Lee, Park, Jangmin, Lee, and Hong]{lee2007multiagent}
Lee, J.~W., Park, J., Jangmin, O., Lee, J., and Hong, E.
\newblock A multiagent approach to $ q $-learning for daily stock trading.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics-Part A:
  Systems and Humans}, 37\penalty0 (6):\penalty0 864--877, 2007.

\bibitem[Lidard et~al.(2021)Lidard, Madhushani, and
  Leonard]{lidard2021provably}
Lidard, J., Madhushani, U., and Leonard, N.~E.
\newblock Provably efficient multi-agent reinforcement learning with fully
  decentralized communication.
\newblock \emph{arXiv preprint arXiv:2110.07392}, 2021.

\bibitem[Littman(1994)]{littman1994markov}
Littman, M.~L.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pp.\  157--163.
  Elsevier, 1994.

\bibitem[Liu \& Zhao(2010)Liu and Zhao]{liu2010distributed}
Liu, K. and Zhao, Q.
\newblock Distributed learning in multi-armed bandit with multiple players.
\newblock \emph{IEEE transactions on signal processing}, 58\penalty0
  (11):\penalty0 5667--5681, 2010.

\bibitem[Liu et~al.(2021)Liu, Yu, Bai, and Jin]{liu2021sharp}
Liu, Q., Yu, T., Bai, Y., and Jin, C.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7001--7010. PMLR, 2021.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Madhushani et~al.(2021)Madhushani, Dubey, Leonard, and
  Pentland]{madhushani2021one}
Madhushani, U., Dubey, A., Leonard, N., and Pentland, A.
\newblock One more step towards reality: Cooperative bandits with imperfect
  communication.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Neu(2015)]{neu2015explore}
Neu, G.
\newblock Explore no more: Improved high-probability regret bounds for
  non-stochastic bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 3168--3176, 2015.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{a}})Rosenberg and
  Mansour]{rosenberg2019online}
Rosenberg, A. and Mansour, Y.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5478--5486. PMLR, 2019{\natexlab{a}}.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{b}})Rosenberg and
  Mansour]{rosenberg2019onlineb}
Rosenberg, A. and Mansour, Y.
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 2212--2221, 2019{\natexlab{b}}.

\bibitem[Rosenberg \& Mansour(2021)Rosenberg and
  Mansour]{rosenberg2021stochastic}
Rosenberg, A. and Mansour, Y.
\newblock Stochastic shortest path with adversarially changing costs.
\newblock In Zhou, Z. (ed.), \emph{Proceedings of the Thirtieth International
  Joint Conference on Artificial Intelligence, {IJCAI} 2021, Virtual Event /
  Montreal, Canada, 19-27 August 2021}, pp.\  2936--2942. ijcai.org, 2021.
\newblock \doi{10.24963/ijcai.2021/404}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2021/404}.

\bibitem[Rosenberg et~al.(2020)Rosenberg, Cohen, Mansour, and
  Kaplan]{rosenberg2020near}
Rosenberg, A., Cohen, A., Mansour, Y., and Kaplan, H.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8210--8219. PMLR, 2020.

\bibitem[Seldin et~al.(2014)Seldin, Bartlett, Crammer, and
  Abbasi-Yadkori]{seldin2014prediction}
Seldin, Y., Bartlett, P., Crammer, K., and Abbasi-Yadkori, Y.
\newblock Prediction with limited advice and multiarmed bandits with paid
  observations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  280--287. PMLR, 2014.

\bibitem[Shalev-Shwartz et~al.(2011)]{shalev2011online}
Shalev-Shwartz, S. et~al.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8604--8613. PMLR, 2020.

\bibitem[Streeter \& McMahan(2010)Streeter and McMahan]{streeter2010less}
Streeter, M. and McMahan, H.~B.
\newblock Less regret via online conditioning.
\newblock \emph{arXiv preprint arXiv:1002.4862}, 2010.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Wang et~al.(2020)Wang, Proutiere, Ariu, Jedra, and
  Russo]{wang2020optimal}
Wang, P.-A., Proutiere, A., Ariu, K., Jedra, Y., and Russo, A.
\newblock Optimal algorithms for multiplayer multi-armed bandits.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4120--4129. PMLR, 2020.

\bibitem[Wang et~al.(2016)Wang, Wan, Zhang, Li, and Zhang]{wang2016towards}
Wang, S., Wan, J., Zhang, D., Li, D., and Zhang, C.
\newblock Towards smart factory for industry 4.0: a self-organized multi-agent
  system with big data based feedback and coordination.
\newblock \emph{Computer networks}, 101:\penalty0 158--168, 2016.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Xie, Q., Chen, Y., Wang, Z., and Yang, Z.
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In \emph{Conference on Learning Theory}, pp.\  3674--3682. PMLR,
  2020.

\bibitem[Xu et~al.(2021)Xu, Ma, and Du]{xu2021fine}
Xu, H., Ma, T., and Du, S.~S.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock In Belkin, M. and Kpotufe, S. (eds.), \emph{Conference on Learning
  Theory, {COLT} 2021, 15-19 August 2021, Boulder, Colorado, {USA}}, volume 134
  of \emph{Proceedings of Machine Learning Research}, pp.\  4438--4472. {PMLR},
  2021.
\newblock URL \url{http://proceedings.mlr.press/v134/xu21a.html}.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Yang, and
  Basar]{zhang2018networked}
Zhang, K., Yang, Z., and Basar, T.
\newblock Networked multi-agent reinforcement learning in continuous spaces.
\newblock In \emph{2018 IEEE conference on decision and control (CDC)}, pp.\
  2771--2776. IEEE, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Yang, Liu, Zhang, and
  Basar]{zhang2018fully}
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
\newblock Fully decentralized multi-agent reinforcement learning with networked
  agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5872--5881. PMLR, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, Kakade, Basar, and Yang]{zhang2020model}
Zhang, K., Kakade, S., Basar, T., and Yang, L.
\newblock Model-based multi-agent rl in zero-sum markov games with near-optimal
  sample complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Yang, and
  Ba{\c{s}}ar]{zhang2021multi}
Zhang, K., Yang, Z., and Ba{\c{s}}ar, T.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, pp.\
  321--384, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Liu, Zhang, and
  Basar]{zhang2021finite}
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
\newblock Finite-sample analysis for decentralized batch multi-agent
  reinforcement learning with networked agents.
\newblock \emph{IEEE Transactions on Automatic Control}, 2021{\natexlab{b}}.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In \emph{Neural Information Processing Systems 26}, 2013.

\end{thebibliography}
