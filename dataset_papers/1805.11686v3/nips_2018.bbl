\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'{e}}]{Amodei16}
Amodei, Dario, Olah, Chris, Steinhardt, Jacob, Christiano, Paul, Schulman,
  John, and Man{\'{e}}, Dan.
\newblock Concrete problems in {AI} safety.
\newblock \emph{ArXiv Preprint}, abs/1606.06565, 2016.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and Browning]{Argall09}
Argall, Brenna~D., Chernova, Sonia, Veloso, Manuela, and Browning, Brett.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57(5):\penalty0 469--483,
  2009.

\bibitem[Finn et~al.(2016)Finn, Tan, Duan, Darrell, Levine, and
  Abbeel]{finn16dsae}
Finn, C., Tan, X., Duan, Y., Darrell, T., Levine, S., and Abbeel, P.
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In \emph{ICRA}, 2016.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{Fu18}
Fu, Justin, Luo, Katie, and Levine, Sergey.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{Haarnoja2017}
Haarnoja, Tuomas, Tang, Haoran, Abbeel, Pieter, and Levine, Sergey.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Hadfield{-}Menell et~al.(2017)Hadfield{-}Menell, Milli, Abbeel,
  Russell, and Dragan]{Hadfield-Menell17}
Hadfield{-}Menell, Dylan, Milli, Smitha, Abbeel, Pieter, Russell, Stuart~J.,
  and Dragan, Anca~D.
\newblock Inverse reward design.
\newblock In \emph{NIPS}, 2017.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{Ho16b}
Ho, Jonathan and Ermon, Stefano.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Kalman(1960)]{Kalman60}
Kalman, Rudolf.
\newblock A new approach to linear filtering and prediction problems.
\newblock 82:\penalty0 35--45, 1960.

\bibitem[Kappen et~al.(2009)Kappen, Gomez, and Opper]{Kappen09}
Kappen, Hilbert~J., Gomez, Vicenc, and Opper, Manfred.
\newblock Optimal control as a graphical model inference problem.
\newblock 2009.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{Levine16}
Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{Journal of Machine Learning (JMLR)}, 2016.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih2015}
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei~A, Veness,
  Joel, Bellemare, Marc~G, Graves, Alex, Riedmiller, Martin, Fidjeland,
  Andreas~K, Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir,
  Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg,
  Shane, and Hassabis, Demis.
\newblock {Human-level control through deep reinforcement learning}.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, feb 2015.
\newblock ISSN 0028-0836.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and Schuurmans]{Nachum17}
Nachum, Ofir, Norouzi, Mohammad, Xu, Kelvin, and Schuurmans, Dale.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Ng \& Russell(2000)Ng and Russell]{Ng2000}
Ng, Andrew and Russell, Stuart.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2000.

\bibitem[O'Donoghue et~al.(2016)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{ODonoghue16}
O'Donoghue, Brendan, Munos, Remi, Kavukcuoglu, Koray, and Mnih, Volodymyr.
\newblock Combining policy gradient and q-learning.
\newblock 2016.

\bibitem[Peng et~al.(2017)Peng, Andrychowicz, Zaremba, and Abbeel]{jason17}
Peng, Xue~Bin, Andrychowicz, Marcin, Zaremba, Wojciech, and Abbeel, Pieter.
\newblock Sim-to-real transfer of robotic control with dynamics randomization.
\newblock \emph{CoRR}, abs/1710.06537, 2017.

\bibitem[Pinto \& Gupta(2016)Pinto and Gupta]{Pinto16}
Pinto, Lerrel and Gupta, Abhinav.
\newblock Supersizing self-supervision: Learning to grasp from 50k tries and
  700 robot hours.
\newblock In \emph{IEEE International Conference on Robotics and Automation
  (ICRA)}, 2016.

\bibitem[Rawlik et~al.(2012)Rawlik, Toussaint, and Vijayakumar]{Rawlik12}
Rawlik, Konrad, Toussaint, Marc, and Vijayakumar, Sethu.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference.
\newblock In \emph{Robotics: Science and Systems (RSS)}, 2012.

\bibitem[Russell \& Norvig(2003)Russell and Norvig]{RussellNorvigAI}
Russell, Stuart~J. and Norvig, Peter.
\newblock \emph{Artificial Intelligence: A Modern Approach}.
\newblock Pearson Education, 2 edition, 2003.
\newblock ISBN 0137903952.

\bibitem[Rusu et~al.(2017)Rusu, Vecerik, Roth{\"{o}}rl, Heess, Pascanu, and
  Hadsell]{progressive}
Rusu, Andrei~A., Vecerik, Matej, Roth{\"{o}}rl, Thomas, Heess, Nicolas,
  Pascanu, Razvan, and Hadsell, Raia.
\newblock Sim-to-real robot learning from pixels with progressive nets.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{Schulman15}
Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael~I., and
  Abbeel, Pieter.
\newblock {Trust Region Policy Optimization}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and Abbeel]{Schulman17}
Schulman, John, Chen, Xi, and Abbeel, Pieter.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock 2017.

\bibitem[Singh et~al.(2010)Singh, Lewis, and Barto]{Singh10}
Singh, S., Lewis, R., and Barto, A.
\newblock Where do rewards come from?
\newblock In \emph{Proceedings of the International Symposium on AI Inspired
  Biology - A Symposium at the AISB 2010 Convention}, 2010.

\bibitem[Sorg et~al.(2010)Sorg, Singh, and Lewis]{Sorg10}
Sorg, Jonathan, Singh, Satinder~P., and Lewis, Richard~L.
\newblock Reward design via online gradient ascent.
\newblock In \emph{NIPS}, 2010.

\bibitem[Todorov(2007)]{Todorov07}
Todorov, Emo.
\newblock Linearly-solvable markov decision problems.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2007.

\bibitem[Todorov(2008)]{Todorov08}
Todorov, Emo.
\newblock General duality between optimal control and estimation.
\newblock In \emph{IEEE Conference on Decision and Control (CDC)}, 2008.

\bibitem[Toussaint(2009)]{Toussaint09}
Toussaint, Marc.
\newblock Robot trajectory optimization using approximate inference.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2009.

\bibitem[Tung et~al.(2018)Tung, Harley, Huang, and Fragkiadaki]{Tung18}
Tung, Hsiao-Yu~Fish, Harley, Adam~W., Huang, Liang-Kang, and Fragkiadaki,
  Katerina.
\newblock Reward learning from narrated demonstrations.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2018.

\bibitem[Ziebart(2010)]{Ziebart10}
Ziebart, Brian.
\newblock Modeling purposeful adaptive behavior with the principle of maximum
  causal entropy.
\newblock \emph{PhD thesis, Carnegie Mellon University}, 2010.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and Dey]{Ziebart08}
Ziebart, Brian, Maas, Andrew, Bagnell, Andrew, and Dey, Anind.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2008.

\end{thebibliography}
