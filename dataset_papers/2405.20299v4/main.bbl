\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Blumensath and Davies(2008)]{blumensath2008iterative}
Thomas Blumensath and Mike~E Davies.
\newblock Iterative thresholding for sparse approximations.
\newblock \emph{Journal of Fourier analysis and Applications}, 14:\penalty0 629--654, 2008.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2018)Chen, Liu, Wang, and Yin]{chen2018theoretical}
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin.
\newblock Theoretical linear convergence of unfolded ista and its practical weights and thresholds.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Cover(1999)]{cover1999elements}
Thomas~M Cover.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pages 7480--7512. PMLR, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Derksen et~al.(2007)Derksen, Ma, Hong, and Wright]{derksen2007segmentation}
Harm Derksen, Yi~Ma, Wei Hong, and John Wright.
\newblock Segmentation of multivariate mixed data via lossy coding and compression.
\newblock In \emph{Visual Communications and Image Processing 2007}, volume 6508, pages 170--181. SPIE, 2007.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Gadre et~al.(2024)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2024datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Gregor and LeCun(2010)]{gregor2010learning}
Karol Gregor and Yann LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock In \emph{Proceedings of the 27th international conference on international conference on machine learning}, pages 399--406, 2010.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16000--16009, 2022.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini, Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and Schmidt]{openclip}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock Openclip.
\newblock July 2021.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Karpathy(2022)]{author_github_repo}
Andrej Karpathy.
\newblock nanogpt, 2022.
\newblock URL \url{https://github.com/karpathy/nanoGPT}.
\newblock GitHub repository.

\bibitem[Li et~al.(2022)Li, Zhai, Tong, Gao, Huang, Zhu, You, Ma, et~al.]{li2022revisiting}
Mingyang Li, Pengyuan Zhai, Shengbang Tong, Xingjian Gao, Shao-Lun Huang, Zhihui Zhu, Chong You, Yi~Ma, et~al.
\newblock Revisiting sparse convolutional model for visual recognition.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 10492--10504, 2022.

\bibitem[Li et~al.(2024)Li, Wang, and Xie]{li2024inverse}
Xianhang Li, Zeyu Wang, and Cihang Xie.
\newblock An inverse scaling law for clip training.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Li et~al.(2023)Li, Fan, Hu, Feichtenhofer, and He]{li2022flip}
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
\newblock Scaling language-image pre-training via masking.
\newblock In \emph{CVPR}, 2023.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11976--11986, 2022.

\bibitem[Loshchilov and Hutter(2017{\natexlab{a}})]{loshchilov2016warmup}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Loshchilov and Hutter(2017{\natexlab{b}})]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017{\natexlab{b}}.

\bibitem[Loshchilov and Hutter(2018)]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Pai et~al.(2023)Pai, Wu, Buchanan, Chu, Yu, and Ma]{pai2023masked}
Druv Pai, Ziyang Wu, Sam Buchanan, Tianzhe Chu, Yaodong Yu, and Yi~Ma.
\newblock Masked completion via structured diffusion with white-box transformers.
\newblock In \emph{Conference on Parsimony and Learning (Recent Spotlight Track)}, 2023.

\bibitem[Parikh et~al.(2014)Parikh, Boyd, et~al.]{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock \emph{Foundations and trends{\textregistered} in Optimization}, 1\penalty0 (3):\penalty0 127--239, 2014.

\bibitem[Qu et~al.(2019)Qu, Zhai, Li, Zhang, and Zhu]{qu2019geometric}
Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu.
\newblock Geometric analysis of nonconvex optimization landscapes for overcomplete learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Ren et~al.(2023)Ren, Wei, Zhang, and Hu]{ren2023tinymim}
Sucheng Ren, Fangyun Wei, Zheng Zhang, and Han Hu.
\newblock Tinymim: An empirical study of distilling mim pre-trained models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3687--3697, 2023.

\bibitem[Singh et~al.(2023)Singh, Duval, Alwala, Fan, Aggarwal, Adcock, Joulin, Doll{\'a}r, Feichtenhofer, Girshick, et~al.]{singh2023effectiveness}
Mannat Singh, Quentin Duval, Kalyan~Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Doll{\'a}r, Christoph Feichtenhofer, Ross Girshick, et~al.
\newblock The effectiveness of mae pre-pretraining for billion-scale pretraining.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5484--5494, 2023.

\bibitem[Sun et~al.(2023)Sun, Fang, Wu, Wang, and Cao]{sun2023eva}
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock \emph{arXiv preprint arXiv:2303.15389}, 2023.

\bibitem[Sun et~al.(2024)Sun, Wang, Yu, Cui, Zhang, Zhang, and Wang]{sun2024eva}
Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang.
\newblock Eva-clip-18b: Scaling clip to 18 billion parameters.
\newblock \emph{arXiv preprint arXiv:2402.04252}, 2024.

\bibitem[Sun et~al.(2018)Sun, Nasrabadi, and Tran]{sun2018supervised}
Xiaoxia Sun, Nasser~M Nasrabadi, and Trac~D Tran.
\newblock Supervised deep sparse coding networks.
\newblock In \emph{2018 25th IEEE International Conference on Image Processing (ICIP)}, pages 346--350. IEEE, 2018.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 1--9, 2015.

\bibitem[Tolooshams and Ba(2021)]{tolooshams2021stable}
Bahareh Tolooshams and Demba Ba.
\newblock Stable and interpretable unrolled dictionary learning.
\newblock \emph{arXiv preprint arXiv:2106.00058}, 2021.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 24261--24272, 2021.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{International conference on machine learning}, pages 10347--10357. PMLR, 2021.

\bibitem[Touvron et~al.(2022)Touvron, Cord, and J{\'e}gou]{touvron2022deit}
Hugo Touvron, Matthieu Cord, and Herv{\'e} J{\'e}gou.
\newblock Deit iii: Revenge of the vit.
\newblock In \emph{European conference on computer vision}, pages 516--533. Springer, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2023)Wang, Girdhar, Yu, and Misra]{wang2023cut}
Xudong Wang, Rohit Girdhar, Stella~X Yu, and Ishan Misra.
\newblock Cut and learn for unsupervised object detection and instance segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 3124--3134, 2023.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Yu et~al.(2023{\natexlab{a}})Yu, Buchanan, Pai, Chu, Wu, Tong, Bai, Zhai, Haeffele, and Ma]{yu2023white_journal}
Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin~D Haeffele, and Yi~Ma.
\newblock White-box transformers via sparse rate reduction: Compression is all there is?
\newblock \emph{arXiv preprint arXiv:2311.13110}, 2023{\natexlab{a}}.

\bibitem[Yu et~al.(2023{\natexlab{b}})Yu, Buchanan, Pai, Chu, Wu, Tong, Haeffele, and Ma]{yu2023white}
Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin Haeffele, and Yi~Ma.
\newblock White-box transformers via sparse rate reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2024)Yu, Chu, Tong, Wu, Pai, Buchanan, and Ma]{yu2024emergence}
Yaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, and Yi~Ma.
\newblock Emergence of segmentation with minimalistic white-box transformers.
\newblock In \emph{Conference on Parsimony and Learning}, pages 72--93. PMLR, 2024.

\bibitem[Zarka et~al.(2019)Zarka, Thiry, Angles, and Mallat]{zarka2019deep}
John Zarka, Louis Thiry, Tom{\'a}s Angles, and St{\'e}phane Mallat.
\newblock Deep network classification by scattering and homotopy dictionary learning.
\newblock \emph{arXiv preprint arXiv:1910.03561}, 2019.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12104--12113, 2022.

\bibitem[Zhang and Li(2024)]{zhang2024white}
Bowen Zhang and Geoffrey~Ye Li.
\newblock White-box 3d-omp-transformer for isac.
\newblock \emph{arXiv preprint arXiv:2407.02251}, 2024.

\bibitem[Zhou et~al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and Torralba]{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 633--641, 2017.

\end{thebibliography}
