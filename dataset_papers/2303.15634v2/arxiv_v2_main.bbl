\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock {TensorFlow}: A system for large-scale machine learning.
\newblock In \emph{12th USENIX Symposium on Operating Systems Design and
  Implementation}, pages 265--283, 2016.

\bibitem[Anil et~al.(2022)Anil, Gadanho, Huang, Jacob, Li, Lin, Phillips, Pop,
  Regan, Shamir, et~al.]{anil2022factory}
R.~Anil, S.~Gadanho, D.~Huang, N.~Jacob, Z.~Li, D.~Lin, T.~Phillips, C.~Pop,
  K.~Regan, G.~I. Shamir, et~al.
\newblock On the factory floor: {ML} engineering for industrial-scale ads
  recommendation models.
\newblock \emph{arXiv preprint arXiv:2209.05310}, 2022.

\bibitem[Bastidas-Ponce et~al.(2019)Bastidas-Ponce, Sophie~Tritschler,
  Scheibner, Tarquis-Medina, Salinno, Schirge, Burtscher, Böttcher, Theis,
  Lickert, and Bakht]{Bastdas-Ponce}
A.~Bastidas-Ponce, L.~D. Sophie~Tritschler, K.~Scheibner, M.~Tarquis-Medina,
  C.~Salinno, S.~Schirge, I.~Burtscher, A.~Böttcher, F.~J. Theis, H.~Lickert,
  and M.~Bakht.
\newblock Comprehensive single cell {mRNA} profiling reveals a detailed roadmap
  for pancreatic endocrinogenesis.
\newblock \emph{Development}, 146\penalty0 (12):\penalty0 dev173849, 2019.

\bibitem[Bedi et~al.(2018)Bedi, Sarma, and Rajawat]{bedi2018tracking}
A.~S. Bedi, P.~Sarma, and K.~Rajawat.
\newblock Tracking moving agents via inexact online gradient descent algorithm.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  12\penalty0 (1):\penalty0 202--217, 2018.

\bibitem[Bellman(1956)]{bellman1956dynamic}
R.~Bellman.
\newblock Dynamic programming and lagrange multipliers.
\newblock \emph{Proceedings of the National Academy of Sciences}, 42\penalty0
  (10):\penalty0 767--769, 1956.

\bibitem[Bengio(2012)]{Bengio}
Y.~Bengio.
\newblock \emph{Practical Recommendations for Gradient-Based Training of Deep
  Architectures}, pages 437--478.
\newblock Springer Berlin Heidelberg, 2012.

\bibitem[Bergen et~al.(2020)Bergen, Lange, Peidli, Wolf, and Theis]{Bergen20}
V.~Bergen, M.~Lange, S.~Peidli, F.~A. Wolf, and F.~Theis.
\newblock Generalizing rna velocity to transient cell states through dynamical
  modeling.
\newblock \emph{Nature Biotechnology}, 38:\penalty0 1408--1414, 2020.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and Zeevi]{besbes2015non}
O.~Besbes, Y.~Gur, and A.~Zeevi.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Operations Research}, 63\penalty0 (5):\penalty0 1227--1244,
  2015.

\bibitem[Chaudhari et~al.(2018)Chaudhari, Oberman, Osher, Soatto, and
  Carlier]{chaudhari2018deep}
P.~Chaudhari, A.~Oberman, S.~Osher, S.~Soatto, and G.~Carlier.
\newblock Deep relaxation: {P}artial differential equations for optimizing deep
  neural networks.
\newblock \emph{Research in the Mathematical Sciences}, 5\penalty0
  (3):\penalty0 1--30, 2018.

\bibitem[Chollet et~al.(2015)]{chollet2015keras}
F.~Chollet et~al.
\newblock Keras.
\newblock \url{https://github.com/fchollet/keras}, 2015.

\bibitem[Coleman et~al.(2023)Coleman, Kang, Fahrbach, Wang, Hong, Chi, and
  Cheng]{coleman2023unified}
B.~Coleman, W.-C. Kang, M.~Fahrbach, R.~Wang, L.~Hong, E.~H. Chi, and D.~Z.
  Cheng.
\newblock {Unified Embedding}: {B}attle-tested feature representations for
  web-scale {ML} systems.
\newblock \emph{arXiv preprint arXiv:2305.12102}, 2023.

\bibitem[Fan and Zhang(2008)]{fan2008statistical}
J.~Fan and W.~Zhang.
\newblock Statistical methods with varying coefficient models.
\newblock \emph{Statistics and its Interface}, 1\penalty0 (1):\penalty0
  179--195, 2008.

\bibitem[Fang et~al.(2018)Fang, Xu, and Yang]{JMLR:v19:17-370}
Y.~Fang, J.~Xu, and L.~Yang.
\newblock Online bootstrap confidence intervals for the stochastic gradient
  descent estimator.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (78):\penalty0 1--21, 2018.

\bibitem[Ge et~al.(2019)Ge, Kakade, Kidambi, and Netrapalli]{ge2019step}
R.~Ge, S.~M. Kakade, R.~Kidambi, and P.~Netrapalli.
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Gronwall(1919)]{gronwall1919note}
T.~H. Gronwall.
\newblock Note on the derivatives with respect to a parameter of the solutions
  of a system of differential equations.
\newblock \emph{Annals of Mathematics}, pages 292--296, 1919.

\bibitem[Hastie and Tibshirani(1993)]{hastie1993varying}
T.~Hastie and R.~Tibshirani.
\newblock Varying-coefficient models.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 55\penalty0 (4):\penalty0 757--779, 1993.

\bibitem[Hu et~al.(2020)Hu, Tang, Singh, and Butte]{cyt3}
Z.~Hu, A.~Tang, J.~Singh, and A.~J. Butte.
\newblock A robust and interpretable end-to-end deep learning model for
  cytometry data.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (35):\penalty0 21373--21380, 2020.

\bibitem[Hu et~al.(2022)Hu, Bhattacharya, and Butte]{cyt2}
Z.~Hu, S.~Bhattacharya, and A.~J. Butte.
\newblock Application of machine learning for cytometry data.
\newblock \emph{Frontiers in immunology}, 12:\penalty0 787574, 2022.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{jadbabaie2015online}
A.~Jadbabaie, A.~Rakhlin, S.~Shahrampour, and K.~Sridharan.
\newblock Online optimization: Competing with dynamic comparators.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 398--406.
  PMLR, 2015.

\bibitem[Jain et~al.(2019)Jain, Nagaraj, and Netrapalli]{jain2019making}
P.~Jain, D.~Nagaraj, and P.~Netrapalli.
\newblock Making the last iterate of {SGD} information theoretically optimal.
\newblock In \emph{Conference on Learning Theory}, pages 1752--1755. PMLR,
  2019.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
C.~Jin, R.~Ge, P.~Netrapalli, S.~M. Kakade, and M.~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Kamath et~al.(2020)Kamath, Deshpande, and Subrahmanyam]{kamath2020sgd}
S.~Kamath, A.~Deshpande, and K.~Subrahmanyam.
\newblock How do {SGD} hyperparameters in natural training affect adversarial
  robustness?
\newblock \emph{arXiv preprint arXiv:2006.11604}, 2020.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krichene and Bartlett(2017)]{krichene2017acceleration}
W.~Krichene and P.~L. Bartlett.
\newblock Acceleration and averaging in stochastic descent dynamics.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
J.~D. Lee, M.~Simchowitz, M.~I. Jordan, and B.~Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Conference on Learning Theory}, pages 1246--1257. PMLR,
  2016.

\bibitem[Li et~al.(2017)Li, Tai, and Weinan]{li2017stochastic}
Q.~Li, C.~Tai, and E.~Weinan.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  2101--2110. PMLR, 2017.

\bibitem[Li et~al.(2019)Li, Mahjoubfar, Chen, Niazi, Pei, and Jalali]{LMC19}
Y.~Li, A.~Mahjoubfar, C.~L. Chen, K.~R. Niazi, L.~Pei, and B.~Jalali.
\newblock Deep cytometry: {D}eep learning with real-time inference in cell
  sorting and flow cytometry.
\newblock \emph{Scientific Reports}, 2019.

\bibitem[Li and Arora(2019)]{li2019exponential}
Z.~Li and S.~Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock \emph{arXiv preprint arXiv:1910.07454}, 2019.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: {S}tochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Oksendal(2013)]{oksendal2013stochastic}
B.~Oksendal.
\newblock \emph{Stochastic Differential Equations: An Introduction with
  Applications}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Shi et~al.(2020)Shi, Su, and Jordan]{shi2020learning}
B.~Shi, W.~J. Su, and M.~I. Jordan.
\newblock On learning rates and {S}chr\"odinger operators.
\newblock \emph{arXiv preprint arXiv:2004.06977}, 2020.

\bibitem[Smith(2015)]{Smith}
L.~N. Smith.
\newblock No more pesky learning rate guessing games.
\newblock \emph{CoRR, abs/1506.01186}, 5:\penalty0 575, 2015.

\bibitem[Smith et~al.(2018)Smith, Kindermans, Ying, and Le]{smith2018don}
S.~L. Smith, P.-J. Kindermans, C.~Ying, and Q.~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Tripuraneni et~al.(2018)Tripuraneni, Flammarion, Bach, and
  Jordan]{pmlr-v75-tripuraneni18a}
N.~Tripuraneni, N.~Flammarion, F.~Bach, and M.~I. Jordan.
\newblock Averaging stochastic gradient descent on {R}iemannian manifolds.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory},
  volume~75, pages 650--687. PMLR, 2018.

\bibitem[Yang et~al.(2016)Yang, Zhang, Jin, and Yi]{yang2016tracking}
T.~Yang, L.~Zhang, R.~Jin, and J.~Yi.
\newblock Tracking slowly moving clairvoyant: Optimal dynamic regret of online
  learning with true and noisy gradient.
\newblock In \emph{International Conference on Machine Learning}, pages
  449--457. PMLR, 2016.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and
  Mahoney]{yao2018hessian}
Z.~Yao, A.~Gholami, Q.~Lei, K.~Keutzer, and M.~W. Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Zhang et~al.(2015)Zhang, Choromanska, and LeCun]{zhang2015deep}
S.~Zhang, A.~E. Choromanska, and Y.~LeCun.
\newblock Deep learning with elastic averaging {SGD}.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Zhou(2018)]{zhou2018fenchel}
X.~Zhou.
\newblock On the {F}enchel duality between strong convexity and {L}ipschitz
  continuous gradient.
\newblock \emph{arXiv preprint arXiv:1803.06573}, 2018.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
M.~Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning}, pages 928--936, 2003.

\end{thebibliography}
