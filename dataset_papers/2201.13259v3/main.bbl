\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bender \& Glen(2004)Bender and Glen]{bender2004molecular}
Bender, A. and Glen, R.~C.
\newblock Molecular similarity: a key technique in molecular informatics.
\newblock \emph{Organic \& biomolecular chemistry}, 2\penalty0 (22):\penalty0
  3204--3218, 2004.

\bibitem[Bengio et~al.(2020)Bengio, Pineau, and Precup]{bengio2020interference}
Bengio, E., Pineau, J., and Precup, D.
\newblock Interference and generalization in temporal difference learning.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Bengio et~al.(2021{\natexlab{a}})Bengio, Jain, Korablyov, Precup, and
  Bengio]{bengio2021flow}
Bengio, E., Jain, M., Korablyov, M., Precup, D., and Bengio, Y.
\newblock Flow network based generative models for non-iterative diverse
  candidate generation.
\newblock \emph{Neural Information Processing Systems (NeurIPS)},
  2021{\natexlab{a}}.

\bibitem[Bengio et~al.(2021{\natexlab{b}})Bengio, Lahlou, Deleu, Hu, Tiwari,
  and Bengio]{bengio2021foundations}
Bengio, Y., Lahlou, S., Deleu, T., Hu, E., Tiwari, M., and Bengio, E.
\newblock {GFlowNet} foundations.
\newblock \emph{arXiv preprint 2111.09266}, 2021{\natexlab{b}}.

\bibitem[Buesing et~al.(2019)Buesing, Heess, and Weber]{buesing2019approximate}
Buesing, L., Heess, N., and Weber, T.
\newblock Approximate inference in discrete distributions with {Monte Carlo}
  tree search and value functions.
\newblock \emph{Artificial Intelligence and Statistics (AISTATS)}, 2019.

\bibitem[Christodoulou(2019)]{christodoulou2019soft}
Christodoulou, P.
\newblock Soft actor-critic for discrete action settings.
\newblock \emph{arXiv preprint arXiv:1910.07207}, 2019.

\bibitem[Dai et~al.(2020)Dai, Singh, Dai, Sutton, and Schuurmans]{dai2020aloe}
Dai, H., Singh, R., Dai, B., Sutton, C., and Schuurmans, D.
\newblock Learning discrete energy-based models via auxiliary-variable local
  exploration.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Deleu et~al.(2022)Deleu, G\'{o}is, Emezue, Rankawat, Lacoste-Julien,
  Bauer, and Bengio]{deleu2022bayesian}
Deleu, T., G\'{o}is, A., Emezue, C., Rankawat, M., Lacoste-Julien, S., Bauer,
  S., and Bengio, Y.
\newblock Bayesian structure learning with generative flow networks.
\newblock \emph{Uncertainty in Artificial Intelligence (UAI)}, 2022.

\bibitem[Ford \& Fulkerson(1956)Ford and Fulkerson]{ford-fulkerson}
Ford, L.~R. and Fulkerson, D.~R.
\newblock Maximal flow through a network.
\newblock \emph{Canadian Journal of Mathematics}, 8:\penalty0 243--248, 1956.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Swersky, Hashemi, Duvenaud, and
  Maddison]{grathwohl2021oops}
Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D.~K., and Maddison, C.~J.
\newblock Oops {I} took a gradient: Scalable sampling for discrete
  distributions.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Jain et~al.(2022)Jain, Bengio, Hernandez-Garcia, Rector-Brooks,
  Dossou, Ekbote, Fu, Zhang, Kilgour, Zhang, Simine, Das, and
  Bengio]{jain2022biological}
Jain, M., Bengio, E., Hernandez-Garcia, A., Rector-Brooks, J., Dossou, B.~F.,
  Ekbote, C., Fu, J., Zhang, T., Kilgour, M., Zhang, D., Simine, L., Das, P.,
  and Bengio, Y.
\newblock Biological sequence design with {GFlowNets}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Jin et~al.(2020)Jin, Barzilay, and Jaakkola]{Jin_2020}
Jin, W., Barzilay, R., and Jaakkola, T.
\newblock Chapter 11. junction tree variational autoencoder for molecular graph
  generation.
\newblock \emph{Drug Discovery}, pp.\  228â€“249, 2020.
\newblock ISSN 2041-3211.

\bibitem[Kumar et~al.(2012)Kumar, Voet, and Zhang]{kumar2012fragment}
Kumar, A., Voet, A., and Zhang, K.
\newblock Fragment based drug design: from experimental to computational
  approaches.
\newblock \emph{Current medicinal chemistry}, 19\penalty0 (30):\penalty0
  5128--5147, 2012.

\bibitem[Kumar et~al.(2021)Kumar, Agarwal, Ghosh, and
  Levine]{kumar2021implicit}
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S.
\newblock Implicit under-parameterization inhibits data-efficient deep
  reinforcement learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Madan et~al.(2022)Madan, Rector-Brooks, Korablyov, Bengio, Jain, Nica,
  Bosc, Bengio, and Malkin]{subtb}
Madan, K., Rector-Brooks, J., Korablyov, M., Bengio, E., Jain, M., Nica, A.,
  Bosc, T., Bengio, Y., and Malkin, N.
\newblock Learning {GFlowNets} from partial episodes for improved convergence
  and stability.
\newblock \emph{arXiv preprint 2209.12782}, 2022.

\bibitem[Malkin et~al.(2022)Malkin, Lahlou, Deleu, Ji, Hu, Everett, Zhang, and
  Bengio]{gfn-hvi}
Malkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett, K., Zhang, D., and
  Bengio, Y.
\newblock {GFlowNets} and variational inference.
\newblock \emph{arXiv preprint 2210.00580}, 2022.

\bibitem[Mnih \& Gregor(2014)Mnih and Gregor]{mnih2014neural}
Mnih, A. and Gregor, K.
\newblock Neural variational inference and learning in belief networks.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2014.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Pirtskhalava et~al.(2021)Pirtskhalava, Amstrong, Grigolava,
  Chubinidze, Alimbarashvili, Vishnepolsky, Gabrielian, Rosenthal, Hurt, and
  Tartakovsky]{pirtskhalava2021dbaasp}
Pirtskhalava, M., Amstrong, A.~A., Grigolava, M., Chubinidze, M.,
  Alimbarashvili, E., Vishnepolsky, B., Gabrielian, A., Rosenthal, A., Hurt,
  D.~E., and Tartakovsky, M.
\newblock Dbaasp v3: Database of antimicrobial/cytotoxic activity and structure
  of peptides as a resource for development of new therapeutics.
\newblock \emph{Nucleic Acids Research}, 49\penalty0 (D1):\penalty0 D288--D297,
  2021.

\bibitem[Shi et~al.(2021)Shi, Huang, Feng, Zhong, Wang, and Sun]{shi2021masked}
Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., and Sun, Y.
\newblock Masked label prediction: Unified message passing model for
  semi-supervised classification.
\newblock \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2021.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT Press, 2018.

\bibitem[Trott \& Olson(2010)Trott and Olson]{trott2010autodock}
Trott, O. and Olson, A.~J.
\newblock {AutoDock Vina}: improving the speed and accuracy of docking with a
  new scoring function, efficient optimization, and multithreading.
\newblock \emph{Journal of computational chemistry}, 31\penalty0 (2):\penalty0
  455--461, 2010.

\bibitem[van Hasselt et~al.(2018)van Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{van2018deep}
van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{arXiv preprint 1812.02648}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Williams \& Peng(1991)Williams and Peng]{williams1991function}
Williams, R.~J. and Peng, J.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock \emph{Connection Science}, 3\penalty0 (3):\penalty0 241--268, 1991.

\bibitem[Xie et~al.(2021)Xie, Shi, Zhou, Yang, Zhang, Yu, and Li]{xie2021mars}
Xie, Y., Shi, C., Zhou, H., Yang, Y., Zhang, W., Yu, Y., and Li, L.
\newblock {MARS}: Markov molecular sampling for multi-objective drug discovery.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Chen, Malkin, , and
  Bengio]{unifying}
Zhang, D., Chen, R. T.~Q., Malkin, N., , and Bengio, Y.
\newblock Unifying generative models with {GFlowNets}.
\newblock \emph{arXiv preprint 2209.02606}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Malkin, Liu, Volokhova,
  Courville, and Bengio]{zhang2022generative}
Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y.
\newblock Generative flow networks for discrete probabilistic modeling.
\newblock \emph{International Conference on Machine Learning (ICML)},
  2022{\natexlab{b}}.

\end{thebibliography}
