\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Commission(2018)]{eugdpr}
The~European Commission.
\newblock General data protection regulation (gdpr).
\newblock 2018.
\newblock URL \url{https://commission.europa.eu/law/law-topic/data-protection/reform/rules-business-and-organisations/principles-gdpr_en}.

\bibitem[House(2022)]{aibillofrights}
The~White House.
\newblock Blueprint for an ai bill of rights.
\newblock 2022.
\newblock URL \url{https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf}.

\bibitem[Han et~al.(2022)Han, Srinivas, and Lakkaraju]{han2022explanation}
Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju.
\newblock Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations.
\newblock \emph{arXiv preprint arXiv:2206.01254}, 2022.

\bibitem[Shah et~al.(2021)Shah, Jain, and Netrapalli]{shah2021input}
Harshay Shah, Prateek Jain, and Praneeth Netrapalli.
\newblock Do input gradients highlight discriminative features?
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2046--2059, 2021.

\bibitem[Hooker et~al.(2019)Hooker, Erhan, Kindermans, and Kim]{hooker2019benchmark}
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim.
\newblock A benchmark for interpretability methods in deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock " why should i trust you?" explaining the predictions of any classifier.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining}, pages 1135--1144, 2016.

\bibitem[Lundberg and Lee(2017)]{lundberg2017shap}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems 30}, pages 4765--4774. Curran Associates, Inc., 2017.
\newblock URL \url{http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'e}gas, and Wattenberg]{smilkov2017smoothgrad}
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi{\'e}gas, and Martin Wattenberg.
\newblock Smoothgrad: removing noise by adding noise.
\newblock \emph{arXiv preprint arXiv:1706.03825}, 2017.

\bibitem[Srinivas and Fleuret(2019)]{srinivas2019full}
Suraj Srinivas and Fran{\c{c}}ois Fleuret.
\newblock Full-gradient representation for neural network visualization.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and Batra]{selvaraju2017grad}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based localization.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 618--626, 2017.

\bibitem[Zeiler and Fergus(2014)]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13}, pages 818--833. Springer, 2014.

\bibitem[Fong and Vedaldi(2017)]{fong2017interpretable}
Ruth~C Fong and Andrea Vedaldi.
\newblock Interpretable explanations of black boxes by meaningful perturbation.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 3429--3437, 2017.

\bibitem[Fong et~al.(2019)Fong, Patrick, and Vedaldi]{fong2019understanding}
Ruth Fong, Mandela Patrick, and Andrea Vedaldi.
\newblock Understanding deep networks via extremal perturbations and smooth masks.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 2950--2958, 2019.

\bibitem[Dabkowski and Gal(2017)]{dabkowski2017real}
Piotr Dabkowski and Yarin Gal.
\newblock Real time image saliency for black box classifiers.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hastie(2017)]{hastie2017generalized}
Trevor~J Hastie.
\newblock Generalized additive models.
\newblock In \emph{Statistical models in S}, pages 249--307. Routledge, 2017.

\bibitem[Chen et~al.(2018)Chen, Song, Wainwright, and Jordan]{chen2018learning}
Jianbo Chen, Le~Song, Martin Wainwright, and Michael Jordan.
\newblock Learning to explain: An information-theoretic perspective on model interpretation.
\newblock In \emph{International Conference on Machine Learning}, pages 883--892. PMLR, 2018.

\bibitem[Yoon et~al.(2019)Yoon, Jordon, and van~der Schaar]{yoon2019invase}
Jinsung Yoon, James Jordon, and Mihaela van~der Schaar.
\newblock Invase: Instance-wise variable selection using neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Jethani et~al.(2021)Jethani, Sudarshan, Aphinyanaphongs, and Ranganath]{jethani2021have}
Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath.
\newblock Have we learned to explain?: How interpretability methods can learn to encode predictions in their interpretations.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1459--1467. PMLR, 2021.

\bibitem[Chen et~al.(2019)Chen, Li, Tao, Barnett, Rudin, and Su]{chen2019looks}
Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan~K Su.
\newblock This looks like that: deep learning for interpretable image recognition.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Koh et~al.(2020)Koh, Nguyen, Tang, Mussmann, Pierson, Kim, and Liang]{koh2020concept}
Pang~Wei Koh, Thao Nguyen, Yew~Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang.
\newblock Concept bottleneck models.
\newblock In \emph{International Conference on Machine Learning}, pages 5338--5348. PMLR, 2020.

\bibitem[B{\"o}hle et~al.(2022)B{\"o}hle, Fritz, and Schiele]{bohle2022b}
Moritz B{\"o}hle, Mario Fritz, and Bernt Schiele.
\newblock B-cos networks: Alignment is all we need for interpretability.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10329--10338, 2022.

\bibitem[Lakkaraju and Bastani(2020)]{lakkaraju2020fool}
Himabindu Lakkaraju and Osbert Bastani.
\newblock " how do i fool you?" manipulating user trust via misleading black box explanations.
\newblock In \emph{Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}, pages 79--85, 2020.

\bibitem[Samek et~al.(2016)Samek, Binder, Montavon, Lapuschkin, and M{\"u}ller]{samek2016evaluating}
Wojciech Samek, Alexander Binder, Gr{\'e}goire Montavon, Sebastian Lapuschkin, and Klaus-Robert M{\"u}ller.
\newblock Evaluating the visualization of what a deep neural network has learned.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 28\penalty0 (11):\penalty0 2660--2673, 2016.

\bibitem[Agarwal et~al.(2022)Agarwal, Krishna, Saxena, Pawelczyk, Johnson, Puri, Zitnik, and Lakkaraju]{agarwal2022openxai}
Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, and Himabindu Lakkaraju.
\newblock Openxai: Towards a transparent evaluation of model explanations.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 15784--15799, 2022.

\bibitem[Jain and Kar(2017)]{JainK17}
Prateek Jain and Purushottam Kar.
\newblock Non-convex optimization for machine learning.
\newblock \emph{Foundations and TrendsÂ® in Machine Learning}, 10, 2017.
\newblock URL \url{all_papers/JainK17_FTML.pdf}.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Kermany et~al.(2018)Kermany, Zhang, Goldbaum, et~al.]{kermany2018labeled}
Daniel Kermany, Kang Zhang, Michael Goldbaum, et~al.
\newblock Labeled optical coherence tomography (oct) and chest x-ray images for classification.
\newblock \emph{Mendeley data}, 2\penalty0 (2):\penalty0 651, 2018.

\bibitem[Liu et~al.(2018)Liu, Luo, Wang, and Tang]{liu2018large}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Large-scale celebfaces attributes (celeba) dataset.
\newblock \emph{Retrieved August}, 15\penalty0 (2018):\penalty0 11, 2018.

\bibitem[Heo et~al.(2019)Heo, Joo, and Moon]{heo2019fooling}
Juyeon Heo, Sunghwan Joo, and Taesup Moon.
\newblock Fooling neural network interpretations via adversarial model manipulation.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
