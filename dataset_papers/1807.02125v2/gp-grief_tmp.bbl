\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baker(1977)]{baker_nystrom}
Baker, C. T.~H.
\newblock \emph{The numerical treatment of integral equations}.
\newblock Oxford: Clarendon press, 1977.

\bibitem[Belabbas \& Wolfe(2009)Belabbas and Wolfe]{belabbas_nystrom}
Belabbas, M.-A. and Wolfe, P.~J.
\newblock Spectral methods in machine learning: New strategies for very large
  datasets.
\newblock In \emph{National Academy of Sciences of the USA}, number 106, pp.\
  369–374, 2009.

\bibitem[Bilionis(2014)]{bilionis_pymcmc}
Bilionis, I.
\newblock {P}redictive{S}cience lab: py-mcmc.
\newblock \url{https://github.com/PredictiveScienceLab/py-mcmc}, December 2014.

\bibitem[Buhmann(2003)]{buhmann_rbf}
Buhmann, M.~D.
\newblock \emph{Radial basis functions: theory and implementations}.
\newblock Cambridge university press, 2003.

\bibitem[Cutajar et~al.(2016)Cutajar, Osborne, Cunningham, and
  Filippone]{cutajar_preconditioning}
Cutajar, K., Osborne, M.~A., Cunningham, J.~P., and Filippone, M.
\newblock Preconditioning kernel matrices.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2529–2538, 2016.

\bibitem[Drineas \& Mahoney(2005)Drineas and Mahoney]{drineas_nystrom}
Drineas, P. and Mahoney, M.~W.
\newblock On the {N}yström method for approximating a {G}ram matrix for
  improved kernel-based learning.
\newblock \emph{Journal of Machine Learning Research}, \penalty0 (6):\penalty0
  2153–2175, 2005.

\bibitem[Girolami \& Calderhead(2011)Girolami and Calderhead]{girolami_MALA}
Girolami, M. and Calderhead, B.
\newblock {R}iemann manifold {L}angevin and {H}amiltonian {M}onte {C}arlo
  methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73\penalty0 (2):\penalty0 123--214, 2011.

\bibitem[Gittens \& Mahoney(2013)Gittens and Mahoney]{gittens_nystrom}
Gittens, A. and Mahoney, M.~W.
\newblock Revisiting the {N}yström method for improved large-scale machine
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[{GPy}(since 2012)]{GPy}
{GPy}.
\newblock {GPy}: A gaussian process framework in python.
\newblock \url{http://github.com/SheffieldML/GPy}, since 2012.

\bibitem[Kumar et~al.(2012)Kumar, Mohri, and Talwalkar]{kumar_nystrom_sampling}
Kumar, S., Mohri, M., and Talwalkar, A.
\newblock Sampling methods for the {N}yström method.
\newblock \emph{Journal of Machine Learning Research}, \penalty0 (13):\penalty0
  981--1006, 2012.

\bibitem[L{\'a}zaro-Gredilla et~al.(2010)L{\'a}zaro-Gredilla,
  Qui{\~n}onero-Candela, Rasmussen, and Figueiras-Vidal]{quin_ssgp}
L{\'a}zaro-Gredilla, M., Qui{\~n}onero-Candela, J., Rasmussen, C.~E., and
  Figueiras-Vidal, A.
\newblock Sparse spectrum {G}aussian process regression.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (6):\penalty0 1865--1881, 2010.

\bibitem[Li et~al.(2016)Li, Jegelka, and Sra]{li_nystrom}
Li, C., Jegelka, S., and Sra, S.
\newblock Fast {DPP} sampling for {N}yström with application to kernel
  methods.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Liu \& Trenkler(2008)Liu and Trenkler]{liu_kron}
Liu, S. and Trenkler, G.
\newblock {H}adamard, {K}hatri-{R}ao, {K}ronecker and other matrix products.
\newblock \emph{International Journal of Information and Systems Sciences},
  4\penalty0 (1):\penalty0 160--177, 2008.

\bibitem[Musco \& Musco(2017)Musco and Musco]{musco_leverage_nystrom}
Musco, C. and Musco, C.
\newblock Recursive sampling for the {N}yström method.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Neal(1997)]{neal_mcmc_gp}
Neal, R.~M.
\newblock {M}onte {C}arlo implementation of {G}aussian process models for
  {B}ayesian regression and classification.
\newblock Technical Report 9702, University of Toronto, 1997.

\bibitem[Nickson et~al.(2015)Nickson, Gunter, Lloyd, Osborne, and
  Roberts]{nickson_blitzkriging}
Nickson, T., Gunter, T., Lloyd, C., Osborne, M.~A., and Roberts, S.
\newblock Blitzkriging: {K}ronecker-structured stochastic {G}aussian processes.
\newblock \emph{arXiv preprint arXiv:1510.07965}, 2015.

\bibitem[Peng \& Qi(2015)Peng and Qi]{peng_eigengp}
Peng, H. and Qi, Y.
\newblock {EigenGP}: {G}aussian process models with adaptive eigenfunctions.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  pp.\  3763--3769, 2015.

\bibitem[Pinheiro \& Bates(1996)Pinheiro and
  Bates]{pinheiro_cov_parameterization}
Pinheiro, J.~C. and Bates, D.~M.
\newblock Unconstrained parametrizations for variance-covariance matrices.
\newblock \emph{Statistics and computing}, 6\penalty0 (3):\penalty0 289--296,
  1996.

\bibitem[Qui{\~n}onero-Candela \& Rasmussen(2005)Qui{\~n}onero-Candela and
  Rasmussen]{quinonero_sparse_gpm}
Qui{\~n}onero-Candela, J. and Rasmussen, C.~E.
\newblock A unifying view of sparse approximate {G}aussian process regression.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (12):\penalty0 1939--1959, 2005.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{rahimi_rff}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1177--1184, 2007.

\bibitem[Rasmussen \& Williams(2006)Rasmussen and Williams]{rasmussen_gpml}
Rasmussen, C.~E. and Williams, C. K.~I.
\newblock \emph{{G}aussian {P}rocesses for Machine Learning}.
\newblock MIT Press, 2006.

\bibitem[Saat{\c{c}}i(2011)]{saatci_phd}
Saat{\c{c}}i, Y.
\newblock \emph{Scalable inference for structured {G}aussian process models}.
\newblock PhD thesis, University of Cambridge, 2011.

\bibitem[Silverman(1985)]{silverman_sor}
Silverman, B.~W.
\newblock Some aspects of the spline smoothing approach to non-parametric
  regression curve fitting.
\newblock \emph{Journal of the Royal Statistical Society B}, 47\penalty0
  (1):\penalty0 1--52, 1985.

\bibitem[Smola \& Bartlett(2000)Smola and Bartlett]{smola_sor}
Smola, A.~J. and Bartlett, P.
\newblock Sparse greedy {G}aussian process regression.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  619--625, 2000.

\bibitem[Smola \& Schökopf(2000)Smola and Schökopf]{smola_greedy_nystrom}
Smola, A.~J. and Schökopf, B.
\newblock Sparse greedy matrix approximation for machine learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  911--918, 2000.

\bibitem[Snelson \& Ghahramani(2006)Snelson and Ghahramani]{snelson_fitc}
Snelson, E. and Ghahramani, Z.
\newblock Sparse {G}aussian processes using pseudo-inputs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1257--1264, 2006.

\bibitem[Titsias(2009)]{titsias_vfe}
Titsias, M.
\newblock Variational learning of inducing variables in sparse {G}aussian
  processes.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  567--574,
  2009.

\bibitem[Van~Loan(2000)]{van_loan_kron}
Van~Loan, C.~F.
\newblock The ubiquitous {K}ronecker product.
\newblock \emph{Journal of Computational and Applied Mathematics}, 123\penalty0
  (1):\penalty0 85--100, 2000.

\bibitem[Wang \& Zhang(2013)Wang and Zhang]{wang_nystrom}
Wang, S. and Zhang, Z.
\newblock Improving {CUR} matrix decomposition and the {N}yström approximation
  via adaptive sampling.
\newblock \emph{Journal of Machine Learning Research}, \penalty0 (14):\penalty0
  2729–2769, 2013.

\bibitem[Williams \& Seeger(2001)Williams and Seeger]{williams_nystrom}
Williams, C. K.~I. and Seeger, M.
\newblock Using the {N}ystr{\"o}m method to speed up kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  682--688, 2001.

\bibitem[Wilson \& Nickisch(2015)Wilson and Nickisch]{wilson_kiss}
Wilson, A.~G. and Nickisch, H.
\newblock Kernel interpolation for scalable structured {G}aussian processes
  ({KISS-GP}).
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1775--1784, 2015.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and Xing]{wilson_deep}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Deep kernel learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  370--378,
  2016.

\bibitem[Yang et~al.(2015)Yang, Smola, Song, and Wilson]{yang_a_la_carte}
Yang, Z., Smola, A.~J., Song, L., and Wilson, A.~G.
\newblock \`{A} la carte -- learning fast kernels.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1098--1106,
  2015.

\bibitem[Zhang et~al.(2008)Zhang, Tsang, and Kwok]{zhang_nystrom}
Zhang, K., Tsang, I.~W., and Kwok, J.~T.
\newblock Improved {N}yström low-rank approximation and error analysis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1232–1239, 2008.

\end{thebibliography}
