\begin{thebibliography}{10}

\bibitem{bai2023longbench}
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et~al.
\newblock Longbench: A bilingual, multitask benchmark for long context understanding.
\newblock {\em arXiv preprint arXiv:2308.14508}, 2023.

\bibitem{bajaj2018ms}
Payal Bajaj, Daniel Campos, Nick Craswell, Li~Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang.
\newblock Ms marco: A human generated machine reading comprehension dataset, 2018.

\bibitem{Beltagy2020Longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv:2004.05150}, 2020.

\bibitem{chen2017retrieveread}
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
\newblock Reading wikipedia to answer open-domain questions.
\newblock In {\em Association for Computational Linguistics (ACL)}, 2017.

\bibitem{chen2022perfectly}
Mayee Chen, Daniel~Y Fu, Avanika Narayan, Michael Zhang, Zhao Song, Kayvon Fatahalian, and Christopher R{\'e}.
\newblock Perfectly balanced: Improving transfer and robustness of supervised contrastive learning.
\newblock In {\em International Conference on Machine Learning}, pages 3090--3122. PMLR, 2022.

\bibitem{together2023redpajama}
Together Computer.
\newblock Redpajama: an open dataset for training large language models, October 2023.

\bibitem{dasigi2021dataset}
Pradeep Dasigi, Kyle Lo, Iz~Beltagy, Arman Cohan, Noah~A Smith, and Matt Gardner.
\newblock A dataset of information-seeking questions and answers anchored in research papers.
\newblock {\em arXiv preprint arXiv:2105.03011}, 2021.

\bibitem{dinan2019wow}
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston.
\newblock Wizard of wikipedia: Knowledge-powered conversational agents.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2019.

\bibitem{wikidump}
Wikimedia Foundation.
\newblock Wikimedia downloads, 2022.

\bibitem{fu2023monarch}
Daniel~Y Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin~W Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher R{\'e}.
\newblock Monarch mixer: A simple sub-quadratic gemm-based architecture.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{fu2022details}
Daniel~Y. Fu, Mayee~F. Chen, Michael Zhang, Kayvon Fatahalian, and Christopher R\'e.
\newblock The details matter: Preventing class collapse in supervised contrastive learning.
\newblock 2022.

\bibitem{fu2023hungry}
Daniel~Y. Fu, Tri Dao, Khaled~K. Saab, Armin~W. Thomas, Atri Rudra, and Christopher R{\'e}.
\newblock Hungry {H}ungry {H}ippos: Towards language modeling with state space models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{misc_legal_case_reports_239}
Filippo Galgani.
\newblock {Legal Case Reports}.
\newblock UCI Machine Learning Repository, 2012.
\newblock {DOI}: https://doi.org/10.24432/C5ZS41.

\bibitem{gao-etal-2021-scaling}
Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan.
\newblock Scaling deep contrastive learning batch size under memory limited setup.
\newblock In Anna Rogers, Iacer Calixto, Ivan Vuli{\'c}, Naomi Saphra, Nora Kassner, Oana-Maria Camburu, Trapit Bansal, and Vered Shwartz, editors, {\em Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)}, pages 316--321, Online, August 2021. Association for Computational Linguistics.

\bibitem{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces, 2023.

\bibitem{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher Ré.
\newblock Efficiently modeling long sequences with structured state spaces, 2022.

\bibitem{günther2023jina}
Michael Günther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo~Wang, and Han Xiao.
\newblock Jina embeddings: A novel set of high-performance sentence embedding models, 2023.

\bibitem{günther2024jina}
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad~Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo~Wang, Maximilian Werk, Nan Wang, and Han Xiao.
\newblock Jina embeddings 2: 8192-token general-purpose text embeddings for long documents, 2024.

\bibitem{hasani2022liquid}
Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus.
\newblock Liquid structural state-space models.
\newblock {\em arXiv preprint arXiv:2209.12951}, 2022.

\bibitem{henderson2017efficient}
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil.
\newblock Efficient natural language response suggestion for smart reply, 2017.

\bibitem{jones2000probabilistic}
K~Sparck Jones, Steve Walker, and Stephen~E. Robertson.
\newblock A probabilistic model of information retrieval: development and comparative experiments: Part 2.
\newblock {\em Information processing \& management}, 36(6):809--840, 2000.

\bibitem{karpukhin-etal-2020-dense}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 6769--6781, Online, November 2020. Association for Computational Linguistics.

\bibitem{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7:453--466, 2019.

\bibitem{10.1145/3477495.3531833}
Carlos Lassance and St\'{e}phane Clinchant.
\newblock An efficiency study for splade models.
\newblock In {\em Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, SIGIR '22, page 2220–2226, New York, NY, USA, 2022. Association for Computing Machinery.

\bibitem{leszczynski2022tabi}
Megan Leszczynski, Daniel~Y. Fu, Mayee~F. Chen, and Christopher Ré.
\newblock Tabi: Type-aware bi-encoders for open-domain entity retrieval, 2022.

\bibitem{lewis2021retrievalaugmented}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

\bibitem{li2021prototypical}
Junnan Li, Pan Zhou, Caiming Xiong, and Steven C.~H. Hoi.
\newblock Prototypical contrastive learning of unsupervised representations, 2021.

\bibitem{li2023don}
Zehua Li, Neel Guha, and Julian Nyarko.
\newblock Don’t use a cannon to kill a fly: An efficient cascading pipeline for long documents.
\newblock 2023.

\bibitem{liu2023lost}
Nelson~F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts, 2023.

\bibitem{muennighoff2022sgpt}
Niklas Muennighoff.
\newblock Sgpt: Gpt sentence embeddings for semantic search, 2022.

\bibitem{muennighoff2022mteb}
Niklas Muennighoff, Nouamane Tazi, Lo{\"\i}c Magne, and Nils Reimers.
\newblock Mteb: Massive text embedding benchmark.
\newblock {\em arXiv preprint arXiv:2210.07316}, 2022.

\bibitem{nussbaum2024nomic}
Zach Nussbaum, John~X Morris, Brandon Duderstadt, and Andriy Mulyar.
\newblock Nomic embed: Training a reproducible long context text embedder.
\newblock {\em Technical Report}, 2024.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library, 2019.

\bibitem{petroni-etal-2021-kilt}
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De~Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt{\"a}schel, and Sebastian Riedel.
\newblock {KILT}: a benchmark for knowledge intensive language tasks.
\newblock In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz~Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 2523--2544, Online, June 2021. Association for Computational Linguistics.

\bibitem{poli2023hyena}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R{\'e}.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock {\em arXiv preprint arXiv:2302.10866}, 2023.

\bibitem{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em arXiv e-prints}, 2019.

\bibitem{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock {\em arXiv preprint arXiv:1806.03822}, 2018.

\bibitem{ranasinghe2021orthogonal}
Kanchana Ranasinghe, Muzammal Naseer, Munawar Hayat, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Orthogonal projection loss, 2021.

\bibitem{Reimers2019SentenceBERTSE}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In {\em Conference on Empirical Methods in Natural Language Processing}, 2019.

\bibitem{saad-falcon-etal-2023-udapdr}
Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md~Sultan, and Christopher Potts.
\newblock {UDAPDR}: Unsupervised domain adaptation via {LLM} prompting and distillation of rerankers.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 11265--11279, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{santhanam-etal-2022-colbertv2}
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia.
\newblock {C}ol{BERT}v2: Effective and efficient retrieval via lightweight late interaction.
\newblock In Marine Carpuat, Marie-Catherine de~Marneffe, and Ivan~Vladimir Meza~Ruiz, editors, {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 3715--3734, Seattle, United States, July 2022. Association for Computational Linguistics.

\bibitem{shaham2022scrolls}
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.
\newblock Scrolls: Standardized comparison over long language sequences, 2022.

\bibitem{smith2023simplified}
Jimmy~TH Smith, Andrew Warrington, and Scott Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{thakur2021beir}
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.
\newblock Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models, 2021.

\bibitem{vaswani2023attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2023.

\bibitem{voorhees2005trec}
Ellen~M Voorhees, Donna~K Harman, et~al.
\newblock {\em TREC: Experiment and evaluation in information retrieval}, volume~63.
\newblock MIT press Cambridge, 2005.

\bibitem{wang2022pretraining}
Junxiong Wang, Jing~Nathan Yan, Albert Gu, and Alexander~M Rush.
\newblock Pretraining without attention.
\newblock {\em arXiv preprint arXiv:2212.10544}, 2022.

\bibitem{wang2023improving}
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.
\newblock Improving text embeddings with large language models.
\newblock {\em arXiv preprint arXiv:2401.00368}, 2023.

\bibitem{wang2022understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment and uniformity on the hypersphere, 2022.

\bibitem{wang2013theoretical}
Yining Wang, Liwei Wang, Yuanzhi Li, Di~He, Tie-Yan Liu, and Wei Chen.
\newblock A theoretical analysis of ndcg type ranking measures, 2013.

\bibitem{bge_embedding}
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
\newblock C-pack: Packaged resources to advance general chinese embedding, 2023.

\bibitem{xu2023retrieval}
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Retrieval meets long context large language models, 2023.

\bibitem{zhang2023retrieve}
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie.
\newblock Retrieve anything to augment large language models, 2023.

\bibitem{Zhu_2015_ICCV}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.
\newblock In {\em The IEEE International Conference on Computer Vision (ICCV)}, December 2015.

\end{thebibliography}
