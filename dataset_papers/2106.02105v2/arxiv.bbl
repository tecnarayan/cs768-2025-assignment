\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2020)]{allen2020feature}
Z.~Allen-Zhu and Y.~Li.
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock \emph{arXiv preprint arXiv:2005.10190}, 2020.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
D.~Arpit, S.~Jastrz{\k{e}}bski, N.~Ballas, D.~Krueger, E.~Bengio, M.~S. Kanwal,
  T.~Maharaj, A.~Fischer, A.~Courville, Y.~Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock \emph{arXiv preprint arXiv:1706.05394}, 2017.

\bibitem[Athalye et~al.(2018)Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2018synthesizing}
A.~Athalye, L.~Engstrom, A.~Ilyas, and K.~Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In \emph{International conference on machine learning}, pages
  284--293. PMLR, 2018.

\bibitem[Aubry and Russell(2015)]{aubry2015understanding}
M.~Aubry and B.~C. Russell.
\newblock Understanding deep features with computer-generated imagery.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2875--2883, 2015.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
B.~Biggio, I.~Corona, D.~Maiorca, B.~Nelson, N.~{\v{S}}rndi{\'c}, P.~Laskov,
  G.~Giacinto, and F.~Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pages 387--402. Springer, 2013.

\bibitem[Carlini and Wagner(2017{\natexlab{a}})]{carlini2017adversarial}
N.~Carlini and D.~Wagner.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pages 3--14, 2017{\natexlab{a}}.

\bibitem[Carlini and Wagner(2017{\natexlab{b}})]{carlini2017towards}
N.~Carlini and D.~Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 ieee symposium on security and privacy (sp)}, pages
  39--57. IEEE, 2017{\natexlab{b}}.

\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,
  Tsipras, Goodfellow, Madry, and Kurakin]{carlini2019evaluating}
N.~Carlini, A.~Athalye, N.~Papernot, W.~Brendel, J.~Rauber, D.~Tsipras,
  I.~Goodfellow, A.~Madry, and A.~Kurakin.
\newblock On evaluating adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1902.06705}, 2019.

\bibitem[Chollet(2017)]{chollet2017xception}
F.~Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1251--1258, 2017.

\bibitem[Chollet et~al.(2015)]{chollet2015keras}
F.~Chollet et~al.
\newblock Keras.
\newblock \url{https://keras.io}, 2015.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
J.~Cohen, E.~Rosenfeld, and Z.~Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{International Conference on Machine Learning}, pages
  1310--1320. PMLR, 2019.

\bibitem[De~Palma et~al.(2019)De~Palma, Kiani, and Lloyd]{de2019random}
G.~De~Palma, B.~Kiani, and S.~Lloyd.
\newblock Random deep neural networks are biased towards simple functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1964--1976, 2019.

\bibitem[Dong et~al.(2018)Dong, Liao, Pang, Su, Zhu, Hu, and
  Li]{dong2018boosting}
Y.~Dong, F.~Liao, T.~Pang, H.~Su, J.~Zhu, X.~Hu, and J.~Li.
\newblock Boosting adversarial attacks with momentum.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 9185--9193, 2018.

\bibitem[Dong et~al.(2019)Dong, Pang, Su, and Zhu]{dong2019evading}
Y.~Dong, T.~Pang, H.~Su, and J.~Zhu.
\newblock Evading defenses to transferable adversarial examples by
  translation-invariant attacks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4312--4321, 2019.

\bibitem[Dosovitskiy and Brox(2016)]{dosovitskiy2016inverting}
A.~Dosovitskiy and T.~Brox.
\newblock Inverting visual representations with convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4829--4837, 2016.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Engstrom et~al.(2019{\natexlab{a}})Engstrom, Ilyas, Salman, Santurkar,
  and Tsipras]{robustness}
L.~Engstrom, A.~Ilyas, H.~Salman, S.~Santurkar, and D.~Tsipras.
\newblock Robustness (python library), 2019{\natexlab{a}}.
\newblock URL \url{https://github.com/MadryLab/robustness}.

\bibitem[Engstrom et~al.(2019{\natexlab{b}})Engstrom, Ilyas, Santurkar,
  Tsipras, Tran, and Madry]{engstrom2019adversarial}
L.~Engstrom, A.~Ilyas, S.~Santurkar, D.~Tsipras, B.~Tran, and A.~Madry.
\newblock Adversarial robustness as a prior for learned representations.
\newblock \emph{arXiv preprint arXiv:1906.00945}, 2019{\natexlab{b}}.

\bibitem[Feinman et~al.(2017)Feinman, Curtin, Shintre, and
  Gardner]{feinman2017detecting}
R.~Feinman, R.~R. Curtin, S.~Shintre, and A.~B. Gardner.
\newblock Detecting adversarial samples from artifacts.
\newblock \emph{arXiv preprint arXiv:1703.00410}, 2017.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos2020shortcut}
R.~Geirhos, J.-H. Jacobsen, C.~Michaelis, R.~Zemel, W.~Brendel, M.~Bethge, and
  F.~A. Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:2004.07780}, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Graham et~al.(2021)Graham, El-Nouby, Touvron, Stock, Joulin,
  J{\'e}gou, and Douze]{graham2021levit}
B.~Graham, A.~El-Nouby, H.~Touvron, P.~Stock, A.~Joulin, H.~J{\'e}gou, and
  M.~Douze.
\newblock Levit: a vision transformer in convnet's clothing for faster
  inference.
\newblock \emph{arXiv preprint arXiv:2104.01136}, 2021.

\bibitem[Guo et~al.(2020)Guo, Li, and Chen]{guo2020backpropagating}
Y.~Guo, Q.~Li, and H.~Chen.
\newblock Backpropagating linearly improves transferability of adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:2012.03528}, 2020.

\bibitem[Hassani et~al.(2021)Hassani, Walton, Shah, Abuduweili, Li, and
  Shi]{hassani2021escaping}
A.~Hassani, S.~Walton, N.~Shah, A.~Abuduweili, J.~Li, and H.~Shi.
\newblock Escaping the big data paradigm with compact transformers.
\newblock \emph{arXiv preprint arXiv:2104.05704}, 2021.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pages 630--645.
  Springer, 2016{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Zhao, Basart, Steinhardt, and
  Song]{hendrycks2019natural}
D.~Hendrycks, K.~Zhao, S.~Basart, J.~Steinhardt, and D.~Song.
\newblock Natural adversarial examples.
\newblock \emph{arXiv preprint arXiv:1907.07174}, 2019.

\bibitem[Hermann et~al.(2020)Hermann, Chen, and Kornblith]{hermann2020origins}
K.~Hermann, T.~Chen, and S.~Kornblith.
\newblock The origins and prevalence of texture bias in convolutional neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Huang et~al.(2019)Huang, Katsman, He, Gu, Belongie, and
  Lim]{huang2019enhancing}
Q.~Huang, I.~Katsman, H.~He, Z.~Gu, S.~Belongie, and S.-N. Lim.
\newblock Enhancing adversarial example transferability with an intermediate
  level attack.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 4733--4742, 2019.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
A.~Ilyas, S.~Santurkar, D.~Tsipras, L.~Engstrom, B.~Tran, and A.~Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  125--136, 2019.

\bibitem[Inkawhich et~al.(2019)Inkawhich, Wen, Li, and
  Chen]{inkawhich2019feature}
N.~Inkawhich, W.~Wen, H.~H. Li, and Y.~Chen.
\newblock Feature space perturbations yield more transferable adversarial
  examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7066--7074, 2019.

\bibitem[Inkawhich et~al.(2020{\natexlab{a}})Inkawhich, Liang, Carin, and
  Chen]{inkawhich2020transferable}
N.~Inkawhich, K.~J. Liang, L.~Carin, and Y.~Chen.
\newblock Transferable perturbations of deep feature distributions.
\newblock \emph{arXiv preprint arXiv:2004.12519}, 2020{\natexlab{a}}.

\bibitem[Inkawhich et~al.(2020{\natexlab{b}})Inkawhich, Liang, Wang, Inkawhich,
  Carin, and Chen]{inkawhich2020perturbing}
N.~Inkawhich, K.~J. Liang, B.~Wang, M.~Inkawhich, L.~Carin, and Y.~Chen.
\newblock Perturbing across the feature hierarchy to improve standard and
  strict blackbox attack transferability.
\newblock \emph{arXiv preprint arXiv:2004.14861}, 2020{\natexlab{b}}.

\bibitem[Jo and Bengio(2017)]{jo2017measuring}
J.~Jo and Y.~Bengio.
\newblock Measuring the tendency of cnns to learn surface statistical
  regularities.
\newblock \emph{arXiv preprint arXiv:1711.11561}, 2017.

\bibitem[Kaur et~al.(2019)Kaur, Cohen, and Lipton]{kaur2019perceptually}
S.~Kaur, J.~Cohen, and Z.~C. Lipton.
\newblock Are perceptually-aligned gradients a general property of robust
  classifiers?
\newblock \emph{arXiv preprint arXiv:1910.08640}, 2019.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblith2019similarity}
S.~Kornblith, M.~Norouzi, H.~Lee, and G.~Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International Conference on Machine Learning}, pages
  3519--3529. PMLR, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
A.~Kurakin, I.~Goodfellow, and S.~Bengio.
\newblock Adversarial examples in the physical world.
\newblock \emph{arXiv preprint arXiv:1607.02533}, 2016.

\bibitem[Li et~al.(2020)Li, Deng, Li, Yan, Gao, and Huang]{li2020towards}
M.~Li, C.~Deng, T.~Li, J.~Yan, X.~Gao, and H.~Huang.
\newblock Towards transferable targeted attack.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 641--649, 2020.

\bibitem[Li et~al.(2015)Li, Yosinski, Clune, Lipson, and
  Hopcroft]{li2015convergent}
Y.~Li, J.~Yosinski, J.~Clune, H.~Lipson, and J.~E. Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same
  representations?
\newblock In \emph{FE@ NIPS}, pages 196--212, 2015.

\bibitem[Liang et~al.(2020)Liang, Zhang, Koyejo, and Li]{liang2020does}
K.~Liang, J.~Y. Zhang, O.~Koyejo, and B.~Li.
\newblock Does adversarial transferability indicate knowledge transferability?
\newblock \emph{arXiv preprint arXiv:2006.14512}, 2020.

\bibitem[Liu et~al.(2016)Liu, Chen, Liu, and Song]{liu2016delving}
Y.~Liu, X.~Chen, C.~Liu, and D.~Song.
\newblock Delving into transferable adversarial examples and black-box attacks.
\newblock \emph{arXiv preprint arXiv:1611.02770}, 2016.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Mahendran and Vedaldi(2015)]{mahendran2015understanding}
A.~Mahendran and A.~Vedaldi.
\newblock Understanding deep image representations by inverting them.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5188--5196, 2015.

\bibitem[McCoy et~al.(2019)McCoy, Pavlick, and Linzen]{mccoy2019right}
R.~T. McCoy, E.~Pavlick, and T.~Linzen.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock \emph{arXiv preprint arXiv:1902.01007}, 2019.

\bibitem[Melas-Kyriazi(2020)]{pytorch_pretrained_vit}
L.~Melas-Kyriazi.
\newblock Vit pytorch.
\newblock \url{https://github.com/lukemelas/PyTorch-Pretrained-ViT}, 2020.

\bibitem[Metzen et~al.(2017)Metzen, Genewein, Fischer, and
  Bischoff]{metzen2017detecting}
J.~H. Metzen, T.~Genewein, V.~Fischer, and B.~Bischoff.
\newblock On detecting adversarial perturbations.
\newblock \emph{arXiv preprint arXiv:1702.04267}, 2017.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and
  Frossard]{moosavi2016deepfool}
S.-M. Moosavi-Dezfooli, A.~Fawzi, and P.~Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2574--2582, 2016.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kalimeris, Kaplun, Edelman, Yang,
  Barak, and Zhang]{nakkiran2019sgd}
P.~Nakkiran, D.~Kalimeris, G.~Kaplun, B.~Edelman, T.~Yang, B.~Barak, and
  H.~Zhang.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3496--3506, 2019.

\bibitem[Olah et~al.(2017)Olah, Mordvintsev, and Schubert]{olah2017feature}
C.~Olah, A.~Mordvintsev, and L.~Schubert.
\newblock Feature visualization.
\newblock \emph{Distill}, 2\penalty0 (11):\penalty0 e7, 2017.

\bibitem[Olah et~al.(2020)Olah, Cammarata, Schubert, Goh, Petrov, and
  Carter]{olah2020zoom}
C.~Olah, N.~Cammarata, L.~Schubert, G.~Goh, M.~Petrov, and S.~Carter.
\newblock Zoom in: An introduction to circuits.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00024.001}.
\newblock https://distill.pub/2020/circuits/zoom-in.

\bibitem[Papernot et~al.(2016{\natexlab{a}})Papernot, McDaniel, and
  Goodfellow]{papernot2016transferability}
N.~Papernot, P.~McDaniel, and I.~Goodfellow.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples.
\newblock \emph{arXiv preprint arXiv:1605.07277}, 2016{\natexlab{a}}.

\bibitem[Papernot et~al.(2016{\natexlab{b}})Papernot, McDaniel, Jha,
  Fredrikson, Celik, and Swami]{papernot2016limitations}
N.~Papernot, P.~McDaniel, S.~Jha, M.~Fredrikson, Z.~B. Celik, and A.~Swami.
\newblock The limitations of deep learning in adversarial settings.
\newblock In \emph{2016 IEEE European symposium on security and privacy
  (EuroS\&P)}, pages 372--387. IEEE, 2016{\natexlab{b}}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Raghu et~al.(2017)Raghu, Gilmer, Yosinski, and
  Sohl-Dickstein]{raghu2017svcca}
M.~Raghu, J.~Gilmer, J.~Yosinski, and J.~Sohl-Dickstein.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock \emph{arXiv preprint arXiv:1706.05806}, 2017.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
A.~Raghunathan, J.~Steinhardt, and P.~Liang.
\newblock Certified defenses against adversarial examples.
\newblock \emph{arXiv preprint arXiv:1801.09344}, 2018.

\bibitem[Rozsa et~al.(2017)Rozsa, G{\"u}nther, and Boult]{rozsa2017lots}
A.~Rozsa, M.~G{\"u}nther, and T.~E. Boult.
\newblock Lots about attacking deep features.
\newblock In \emph{2017 IEEE International Joint Conference on Biometrics
  (IJCB)}, pages 168--176. IEEE, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
H.~Salman, A.~Ilyas, L.~Engstrom, A.~Kapoor, and A.~Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{arXiv preprint arXiv:2007.08489}, 2020.

\bibitem[Santurkar et~al.(2019)Santurkar, Ilyas, Tsipras, Engstrom, Tran, and
  Madry]{santurkar2019image}
S.~Santurkar, A.~Ilyas, D.~Tsipras, L.~Engstrom, B.~Tran, and A.~Madry.
\newblock Image synthesis with a single (robust) classifier.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1262--1273, 2019.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Studer, Feizi, and
  Goldstein]{shafahi2018adversarial}
A.~Shafahi, W.~R. Huang, C.~Studer, S.~Feizi, and T.~Goldstein.
\newblock Are adversarial examples inevitable?
\newblock \emph{arXiv preprint arXiv:1809.02104}, 2018.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
H.~Shah, K.~Tamuly, A.~Raghunathan, P.~Jain, and P.~Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock \emph{arXiv preprint arXiv:2006.07710}, 2020.

\bibitem[Shao et~al.(2021)Shao, Shi, Yi, Chen, and Hsieh]{shao2021adversarial}
R.~Shao, Z.~Shi, J.~Yi, P.-Y. Chen, and C.-J. Hsieh.
\newblock On the adversarial robustness of visual transformers.
\newblock \emph{arXiv preprint arXiv:2103.15670}, 2021.

\bibitem[Sharma et~al.(2019)Sharma, Ding, and
  Brubaker]{sharma2019effectiveness}
Y.~Sharma, G.~W. Ding, and M.~Brubaker.
\newblock On the effectiveness of low frequency perturbations.
\newblock \emph{arXiv preprint arXiv:1903.00073}, 2019.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013deep}
K.~Simonyan, A.~Vedaldi, and A.~Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock \emph{arXiv preprint arXiv:1312.6034}, 2013.

\bibitem[Song et~al.(2018)Song, Shu, Kushman, and Ermon]{song2018constructing}
Y.~Song, R.~Shu, N.~Kushman, and S.~Ermon.
\newblock Constructing unrestricted adversarial examples with generative
  models.
\newblock \emph{arXiv preprint arXiv:1805.07894}, 2018.

\bibitem[Springer et~al.(2021)Springer, Mitchell, and
  Kenyon]{springer2021adversarial}
J.~M. Springer, M.~Mitchell, and G.~T. Kenyon.
\newblock Adversarial perturbations are not so weird: Entanglement of robust
  and non-robust features in neural network classifiers.
\newblock \emph{arXiv preprint arXiv:2102.05110}, 2021.

\bibitem[Stutz et~al.(2019)Stutz, Hein, and Schiele]{stutz2019disentangling}
D.~Stutz, M.~Hein, and B.~Schiele.
\newblock Disentangling adversarial robustness and generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6976--6987, 2019.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2014intriguing}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{2nd International Conference on Learning Representations,
  ICLR 2014}, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
M.~Tan and Q.~Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  6105--6114. PMLR, 2019.

\bibitem[Terzi et~al.(2020)Terzi, Achille, Maggipinto, and
  Susto]{terzi2020adversarial}
M.~Terzi, A.~Achille, M.~Maggipinto, and G.~A. Susto.
\newblock Adversarial training reduces information and improves
  transferability.
\newblock \emph{arXiv preprint arXiv:2007.11259}, 2020.

\bibitem[Tram{\`e}r et~al.(2018)Tram{\`e}r, Boneh, Kurakin, Goodfellow,
  Papernot, and McDaniel]{tramer2018ensemble}
F.~Tram{\`e}r, D.~Boneh, A.~Kurakin, I.~Goodfellow, N.~Papernot, and
  P.~McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, 2018.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2019robustness}
D.~Tsipras, S.~Santurkar, L.~Engstrom, A.~Turner, and A.~Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Uesato et~al.(2018)Uesato, O’donoghue, Kohli, and
  Oord]{uesato2018adversarial}
J.~Uesato, B.~O’donoghue, P.~Kohli, and A.~Oord.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5025--5034. PMLR, 2018.

\bibitem[Utrera et~al.(2020)Utrera, Kravitz, Erichson, Khanna, and
  Mahoney]{utrera2020adversarially}
F.~Utrera, E.~Kravitz, N.~B. Erichson, R.~Khanna, and M.~W. Mahoney.
\newblock Adversarially-trained deep nets transfer better.
\newblock \emph{arXiv preprint arXiv:2007.05869}, 2020.

\bibitem[Valle-P{\'e}rez et~al.(2018)Valle-P{\'e}rez, Camargo, and
  Louis]{valle2018deep}
G.~Valle-P{\'e}rez, C.~Q. Camargo, and A.~A. Louis.
\newblock Deep learning generalizes because the parameter-function map is
  biased towards simple functions.
\newblock \emph{arXiv preprint arXiv:1805.08522}, 2018.

\bibitem[Wang et~al.(2020)Wang, Wu, Huang, and Xing]{wang2020high}
H.~Wang, X.~Wu, Z.~Huang, and E.~P. Xing.
\newblock High-frequency component helps explain the generalization of
  convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8684--8694, 2020.

\bibitem[Warde-Farley and Goodfellow(2016)]{warde201611}
D.~Warde-Farley and I.~Goodfellow.
\newblock Adversarial perturbations of deep neural networks.
\newblock \emph{Perturbations, Optimization, and Statistics}, 311, 2016.

\bibitem[Wei(2020)]{wei2020understanding}
K.-A.~A. Wei.
\newblock \emph{Understanding non-robust features in image classification}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2020.

\bibitem[Wu et~al.(2020)Wu, Wang, Xia, Bailey, and Ma]{wu2020skip}
D.~Wu, Y.~Wang, S.-T. Xia, J.~Bailey, and X.~Ma.
\newblock Skip connections matter: On the transferability of adversarial
  examples generated with resnets.
\newblock \emph{arXiv preprint arXiv:2002.05990}, 2020.

\bibitem[Wu et~al.(2017)Wu, Zhu, et~al.]{wu2017towards}
L.~Wu, Z.~Zhu, et~al.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock \emph{arXiv preprint arXiv:1706.10239}, 2017.

\bibitem[Xie et~al.(2019)Xie, Zhang, Zhou, Bai, Wang, Ren, and
  Yuille]{xie2019improving}
C.~Xie, Z.~Zhang, Y.~Zhou, S.~Bai, J.~Wang, Z.~Ren, and A.~L. Yuille.
\newblock Improving transferability of adversarial examples with input
  diversity.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2730--2739, 2019.

\bibitem[Yang et~al.(2021)Yang, Li, Xu, Zuo, Chen, Rubinstein, Zhang, and
  Li]{yang2021trs}
Z.~Yang, L.~Li, X.~Xu, S.~Zuo, Q.~Chen, B.~Rubinstein, C.~Zhang, and B.~Li.
\newblock Trs: Transferability reduced ensemble via encouraging gradient
  diversity and model smoothness.
\newblock \emph{arXiv preprint arXiv:2104.00671}, 2021.

\bibitem[Zhang and Zhu(2018)]{zhang2018visual}
Q.-s. Zhang and S.-C. Zhu.
\newblock Visual interpretability for deep learning: a survey.
\newblock \emph{Frontiers of Information Technology \& Electronic Engineering},
  19\penalty0 (1):\penalty0 27--39, 2018.

\bibitem[Zhao et~al.(2017)Zhao, Dua, and Singh]{zhao2017generating}
Z.~Zhao, D.~Dua, and S.~Singh.
\newblock Generating natural adversarial examples.
\newblock \emph{arXiv preprint arXiv:1710.11342}, 2017.

\bibitem[Zhao et~al.(2020)Zhao, Liu, and Larson]{zhao2020success}
Z.~Zhao, Z.~Liu, and M.~Larson.
\newblock On success and simplicity: A second look at transferable targeted
  attacks.
\newblock \emph{arXiv preprint arXiv:2012.11207}, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Hou, Chen, Tang, Huang, Gan, and
  Yang]{zhou2018transferable}
W.~Zhou, X.~Hou, Y.~Chen, M.~Tang, X.~Huang, X.~Gan, and Y.~Yang.
\newblock Transferable adversarial perturbations.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 452--467, 2018.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2018learning}
B.~Zoph, V.~Vasudevan, J.~Shlens, and Q.~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8697--8710, 2018.

\end{thebibliography}
