\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in {M}arkov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR, 2020.

\bibitem[Baxter \& Bartlett(2001)Baxter and Bartlett]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{journal of artificial intelligence research}, 15:\penalty0
  319--350, 2001.

\bibitem[Charles \& Kone{\v{c}}n{\`y}(2021)Charles and
  Kone{\v{c}}n{\`y}]{charles2021convergence}
Charles, Z. and Kone{\v{c}}n{\`y}, J.
\newblock Convergence and accuracy trade-offs in federated learning and
  meta-learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2575--2583. PMLR, 2021.

\bibitem[Cheng et~al.(2024)Cheng, Huang, and Yuan]{cheng2023momentum}
Cheng, Z., Huang, X., and Yuan, K.
\newblock Momentum benefits non-iid federated learning simply and provably.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Cortes et~al.(2010)Cortes, Mansour, and Mohri]{cortes2010learning}
Cortes, C., Mansour, Y., and Mohri, M.
\newblock Learning bounds for importance weighting.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Ding et~al.(2021)Ding, Zhang, and Lavaei]{ding2021beyond}
Ding, Y., Zhang, J., and Lavaei, J.
\newblock Beyond exact gradients: Convergence of stochastic soft-max policy
  gradient methods with entropy regularization.
\newblock \emph{arXiv preprint arXiv:2110.10117}, 2021.

\bibitem[Fan et~al.(2021)Fan, Ma, Dai, Jing, Tan, and Low]{fan2021fault}
Fan, X., Ma, Y., Dai, Z., Jing, W., Tan, C., and Low, B. K.~H.
\newblock Fault-tolerant federated reinforcement learning with theoretical
  guarantee.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1007--1021, 2021.

\bibitem[Fatkhullin et~al.(2023)Fatkhullin, Barakat, Kireeva, and
  He]{fatkhullin2023stochastic}
Fatkhullin, I., Barakat, A., Kireeva, A., and He, N.
\newblock Stochastic policy gradient methods: Improved sample complexity for
  {F}isher-non-degenerate policies.
\newblock \emph{arXiv preprint arXiv:2302.01734}, 2023.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1126--1135. PMLR, 2017.

\bibitem[Furmston et~al.(2016)Furmston, Lever, and
  Barber]{furmston2016approximate}
Furmston, T., Lever, G., and Barber, D.
\newblock Approximate {N}ewton methods for policy search in markov decision
  processes.
\newblock \emph{Journal of Machine Learning Research}, 17, 2016.

\bibitem[Gargiani et~al.(2022)Gargiani, Zanelli, Martinelli, Summers, and
  Lygeros]{gargiani2022page}
Gargiani, M., Zanelli, A., Martinelli, A., Summers, T., and Lygeros, J.
\newblock {PAGE-PG}: A simple and loopless variance-reduced policy gradient
  method with probabilistic gradient estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7223--7240. PMLR, 2022.

\bibitem[Huang et~al.(2020)Huang, Gao, Pei, and Huang]{huang2020momentum}
Huang, F., Gao, S., Pei, J., and Huang, H.
\newblock Momentum-based policy gradient methods.
\newblock In \emph{International conference on machine learning}, pp.\
  4422--4433. PMLR, 2020.

\bibitem[Huang et~al.(2024)Huang, Li, and Li]{huang2023stochastic}
Huang, X., Li, P., and Li, X.
\newblock Stochastic controlled averaging for federated learning with
  communication compression.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Jin et~al.(2022)Jin, Peng, Yang, Wang, and Zhang]{jin2022federated}
Jin, H., Peng, Y., Yang, W., Wang, S., and Zhang, Z.
\newblock Federated reinforcement learning with environment heterogeneity.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  18--37. PMLR, 2022.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock {SCAFFOLD}: Stochastic controlled averaging for federated learning.
\newblock In \emph{International conference on machine learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Khodadadian et~al.(2022)Khodadadian, Sharma, Joshi, and
  Maguluri]{khodadadian2022federated}
Khodadadian, S., Sharma, P., Joshi, G., and Maguluri, S.~T.
\newblock Federated reinforcement learning: Linear speedup under {M}arkovian
  sampling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10997--11057. PMLR, 2022.

\bibitem[Lan et~al.(2023)Lan, Wang, Anderson, Brinton, and
  Aggarwal]{lan2023improved}
Lan, G., Wang, H., Anderson, J., Brinton, C., and Aggarwal, V.
\newblock Improved communication efficiency in federated natural policy
  gradient via admm-based gradient updates.
\newblock \emph{arXiv preprint arXiv:2310.19807}, 2023.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0
  429--450, 2020.

\bibitem[Liu et~al.(2019)Liu, Wang, and Liu]{liu2019lifelong}
Liu, B., Wang, L., and Liu, M.
\newblock Lifelong federated reinforcement learning: a learning architecture
  for navigation in cloud robotic systems.
\newblock \emph{IEEE Robotics and Automation Letters}, 4\penalty0 (4):\penalty0
  4555--4562, 2019.

\bibitem[Liu et~al.(2020)Liu, Zhang, Basar, and Yin]{liu2020improved}
Liu, Y., Zhang, K., Basar, T., and Yin, W.
\newblock An improved analysis of (variance-reduced) policy gradient and
  natural policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7624--7636, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Mitra et~al.(2021)Mitra, Jaafar, Pappas, and Hassani]{mitra2021linear}
Mitra, A., Jaafar, R., Pappas, G.~J., and Hassani, H.
\newblock Linear convergence in federated learning: Tackling client
  heterogeneity and sparse gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14606--14619, 2021.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and
  Restelli]{papini2018stochastic}
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and Restelli, M.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{International conference on machine learning}, pp.\
  4026--4035. PMLR, 2018.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, and
  Bascetta]{pirotta2013adaptive}
Pirotta, M., Restelli, M., and Bascetta, L.
\newblock Adaptive step-size for policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Qi et~al.(2021)Qi, Zhou, Lei, and Zheng]{qi2021federated}
Qi, J., Zhou, Q., Lei, L., and Zheng, K.
\newblock Federated reinforcement learning: Techniques, applications, and open
  challenges.
\newblock \emph{arXiv preprint arXiv:2108.11887}, 2021.

\bibitem[Shen et~al.(2019)Shen, Ribeiro, Hassani, Qian, and
  Mi]{shen2019hessian}
Shen, Z., Ribeiro, A., Hassani, H., Qian, H., and Mi, C.
\newblock Hessian aided policy gradient.
\newblock In \emph{International conference on machine learning}, pp.\
  5729--5738. PMLR, 2019.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ international conference on intelligent robots
  and systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Wang et~al.(2022)Wang, Marella, and Anderson]{wang2022fedadmm}
Wang, H., Marella, S., and Anderson, J.
\newblock Fedadmm: A federated primal-dual algorithm allowing partial
  participation.
\newblock In \emph{2022 IEEE 61st Conference on Decision and Control (CDC)},
  pp.\  287--294. IEEE, 2022.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Mitra, Hassani, Pappas, and
  Anderson]{wang2023federated}
Wang, H., Mitra, A., Hassani, H., Pappas, G.~J., and Anderson, J.
\newblock Federated temporal difference learning with linear function
  approximation under environmental heterogeneity.
\newblock \emph{arXiv preprint arXiv:2302.02212}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Toso, Mitra, and
  Anderson]{wang2023model}
Wang, H., Toso, L.~F., Mitra, A., and Anderson, J.
\newblock Model-free learning with heterogeneous dynamical systems: A federated
  lqr approach.
\newblock \emph{arXiv preprint arXiv:2308.11743}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Tantia, Ballas, and Rabbat]{wang2019slowmo}
Wang, J., Tantia, V., Ballas, N., and Rabbat, M.
\newblock {SlowMo}: Improving communication-efficient distributed sgd with slow
  momentum.
\newblock \emph{arXiv preprint arXiv:1910.00643}, 2019.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 7611--7623, 2020.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8:\penalty0 229--256, 1992.

\bibitem[Woo et~al.(2023)Woo, Joshi, and Chi]{woo2023blessing}
Woo, J., Joshi, G., and Chi, Y.
\newblock The blessing of heterogeneity in federated {Q}-learning: Linear
  speedup and beyond.
\newblock \emph{arXiv preprint arXiv:2305.10697}, 2023.

\bibitem[Xie \& Song(2023{\natexlab{a}})Xie and Song]{xie2023client}
Xie, Z. and Song, S.
\newblock Client selection for federated policy optimization with environment
  heterogeneity.
\newblock \emph{arXiv preprint arXiv:2305.10978}, 2023{\natexlab{a}}.

\bibitem[Xie \& Song(2023{\natexlab{b}})Xie and Song]{xie2023fedkl}
Xie, Z. and Song, S.
\newblock {FedKL}: Tackling data heterogeneity in federated reinforcement
  learning by penalizing {KL} divergence.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 41\penalty0
  (4):\penalty0 1227--1242, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2019)Xu, Gao, and Gu]{xu2019sample}
Xu, P., Gao, F., and Gu, Q.
\newblock Sample efficient policy gradient methods with recursive variance
  reduction.
\newblock \emph{arXiv preprint arXiv:1909.08610}, 2019.

\bibitem[Xu et~al.(2020)Xu, Gao, and Gu]{xu2020improved}
Xu, P., Gao, F., and Gu, Q.
\newblock An improved convergence analysis of stochastic variance-reduced
  policy gradient.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  541--551.
  PMLR, 2020.

\bibitem[Yang et~al.(2021)Yang, Fang, and Liu]{yang2021achieving}
Yang, H., Fang, M., and Liu, J.
\newblock Achieving linear speedup with partial worker participation in non-iid
  federated learning.
\newblock \emph{arXiv preprint arXiv:2101.11203}, 2021.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{yu2020meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock {Meta-World}: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on robot learning}, pp.\  1094--1100. PMLR, 2020.

\bibitem[Yuan et~al.(2020)Yuan, Lian, Liu, and Zhou]{yuan2020stochastic}
Yuan, H., Lian, X., Liu, J., and Zhou, Y.
\newblock Stochastic recursive momentum for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2003.04302}, 2020.

\bibitem[Zeng et~al.(2021)Zeng, Anwar, Doan, Raychowdhury, and
  Romberg]{zeng2021decentralized}
Zeng, S., Anwar, M.~A., Doan, T.~T., Raychowdhury, A., and Romberg, J.
\newblock A decentralized policy gradient approach to multi-task reinforcement
  learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1002--1012.
  PMLR, 2021.

\bibitem[Zhang et~al.(2024)Zhang, Wang, Mitra, and Anderson]{zhang2024finite}
Zhang, C., Wang, H., Mitra, A., and Anderson, J.
\newblock Finite-time analysis of on-policy heterogeneous federated
  reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2024.

\end{thebibliography}
