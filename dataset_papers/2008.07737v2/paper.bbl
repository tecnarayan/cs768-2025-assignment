\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, Pal, and
  Szepesvari]{Abbasi11}
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2011.

\bibitem[Agarwal et~al.(2020)Agarwal, Henaff, Kakade, and Sun]{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock {PC-PG}: Policy cover directed exploration for provable policy
  gradient learning.
\newblock \emph{arXiv preprint arXiv:2007.08459}, 2020.

\bibitem[Agrawal and Jia(2017)]{Agrawal2017}
Shipra Agrawal and Randy Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems (NIPS)}, pages 1184--1194. Curran Associates,
  Inc., 2017.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{Azar17}
Mohammad~Gheshlaghi Azar, Ian Osband, and Remi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1042--1051, 2019.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1516, 2019.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Krishnamurthy, Jiang, Agarwal, Dudik,
  and Langford]{du2019provablyefficient}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and
  John Langford.
\newblock Provably efficient {RL} with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  volume~97, pages 1665--1674, Long Beach, California, USA, 09--15 Jun
  2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Kakade, Wang, and Yang]{du2019good}
Simon~S Du, Sham~M Kakade, Ruosong Wang, and Lin~F Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019{\natexlab{b}}.

\bibitem[Du et~al.(2019{\natexlab{c}})Du, Luo, Wang, and Zhang]{du2019provably}
Simon~S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8058--8068, 2019{\natexlab{c}}.

\bibitem[Du et~al.(2020)Du, Lee, Mahajan, and Wang]{du2020agnostic}
Simon~S. Du, Jason~D. Lee, Gaurav Mahajan, and Ruosong Wang.
\newblock Agnostic q-learning with function approximation in deterministic
  systems: Tight bounds on approximation error and sample complexity.
\newblock \emph{arXiv preprint arXiv:2002.07125}, 2020.

\bibitem[Efroni et~al.(2019)Efroni, Merlis, Ghavamzadeh, and
  Mannor]{efroni2019tight}
Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Gopalan and Mannor(2015)]{gopalan2015thompson}
Aditya Gopalan and Shie Mannor.
\newblock Thompson sampling for learning parameterized markov decision
  processes.
\newblock In \emph{Conference on Learning Theory}, pages 861--898, 2015.

\bibitem[Hazan et~al.(2018)Hazan, Kakade, Singh, and Soest]{hazan2018provably}
Elad Hazan, Sham~M. Kakade, Karan Singh, and Abby~Van Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock \emph{arXiv preprint arXiv:1812.02690}, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{Jaksch10}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 2010.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang17contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E.
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  volume~70, pages 1704--1713, International Convention Centre, Sydney,
  Australia, 06--11 Aug 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Krishnamurthy, Simchowitz, and
  Yu]{jin2020rewardfree}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, 2020{\natexlab{b}}.

\bibitem[Kaufmann et~al.(2020)Kaufmann, M{\'e}nard, Domingues, Jonsson,
  Leurent, and Valko]{kaufmann2020adaptive}
Emilie Kaufmann, Pierre M{\'e}nard, Omar~Darwiche Domingues, Anders Jonsson,
  Edouard Leurent, and Michal Valko.
\newblock Adaptive reward-free exploration.
\newblock \emph{arXiv preprint arXiv:2006.06294}, 2020.

\bibitem[Kiefer and Wolfowitz(1960)]{kiefer1960equivalence}
Jack Kiefer and Jacob Wolfowitz.
\newblock The equivalence of two extremum problems.
\newblock \emph{Canadian Journal of Mathematics}, 12:\penalty0 363--366, 1960.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1840--1848, 2016.

\bibitem[Lagoudakis and Parr(2003)]{lagoudakis2003least}
Michail~G Lagoudakis and Ronald Parr.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lattimore and Szepesvari(2020)]{lattimore2020learning}
Tor Lattimore and Csaba Szepesvari.
\newblock Learning with good feature representations in bandits and in rl with
  a generative model.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Lazaric et~al.(2012)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2012finite}
Alessandro Lazaric, Mohammad Ghavamzadeh, and R{\'e}mi Munos.
\newblock Finite-sample analysis of least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Oct):\penalty0 3041--3074, 2012.

\bibitem[M{\'e}nard et~al.(2020)M{\'e}nard, Domingues, Jonsson, Kaufmann,
  Leurent, and Valko]{menard2020fast}
Pierre M{\'e}nard, Omar~Darwiche Domingues, Anders Jonsson, Emilie Kaufmann,
  Edouard Leurent, and Michal Valko.
\newblock Fast active learning for pure exploration in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.13442}, 2020.

\bibitem[Misra et~al.(2020)Misra, Henaff, Krishnamurthy, and
  Langford]{misra2019kinematic}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Munos(2005)]{munos2005error}
R{\'e}mi Munos.
\newblock Error bounds for approximate value iteration.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2005.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (May):\penalty0 815--857, 2008.

\bibitem[Osband et~al.(2016{\natexlab{a}})Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016{\natexlab{a}}.

\bibitem[Osband et~al.(2016{\natexlab{b}})Osband, Van~Roy, and
  Wen]{osband2016generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2016{\natexlab{b}}.

\bibitem[Ouyang et~al.(2017)Ouyang, Gagrani, Nayyar, and
  Jain]{ouyang2017learning}
Yi~Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain.
\newblock Learning unknown markov decision processes: A thompson sampling
  approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1333--1342, 2017.

\bibitem[Puterman(1994)]{puterman1994markov}
Martin~L. Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock Wiley, 1994.

\bibitem[Russo(2019)]{russo2019worst}
Daniel Russo.
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT Press, 2018.

\bibitem[Tarbouriech et~al.()Tarbouriech, Pirotta, Valko, and
  Lazaric]{tarbouriechreward}
Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric.
\newblock Reward-free exploration beyond finite-horizon.
\newblock \emph{arXiv preprint arXiv:2002.02794}.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Du, Yang, and
  Salakhutdinov]{wang2020reward}
Ruosong Wang, Simon~S Du, Lin~F Yang, and Ruslan Salakhutdinov.
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2006.11274}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Salakhutdinov, and
  Yang]{wang2020provably}
Ruosong Wang, Ruslan Salakhutdinov, and Lin~F. Yang.
\newblock Provably efficient reinforcement learning with general value function
  approximation.
\newblock \emph{arXiv preprint arXiv:2005.10804}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Wang, Du, and Krishnamurthy]{wang2019optimism}
Yining Wang, Ruosong Wang, Simon~S Du, and Akshay Krishnamurthy.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1912.04136}, 2019.

\bibitem[Yang and Wang(2020)]{yang2020reinforcement}
Lin~F Yang and Mengdi Wang.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Pirotta,
  and Lazaric]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Matteo Pirotta, and Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2020{\natexlab{b}}.

\end{thebibliography}
