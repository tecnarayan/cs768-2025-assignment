\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams et~al.(2018)Adams, Pennington, Johnson, Smith, Ovadia, Patton,
  and Saunderson]{adams2018estimating}
Ryan~P Adams, Jeffrey Pennington, Matthew~J Johnson, Jamie Smith, Yaniv Ovadia,
  Brian Patton, and James Saunderson.
\newblock Estimating the spectral density of large implicit matrices.
\newblock \emph{arXiv preprint arXiv:1802.03451}, 2018.

\bibitem[Baykal et~al.(2019{\natexlab{a}})Baykal, Liebenwein, Gilitschenski,
  Feldman, and Rus]{baykal2018datadependent}
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela
  Rus.
\newblock Data-dependent coresets for compressing neural networks with
  applications to generalization bounds.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=HJfwJ2A5KX}.

\bibitem[Baykal et~al.(2019{\natexlab{b}})Baykal, Liebenwein, Gilitschenski,
  Feldman, and Rus]{sipp2019}
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela
  Rus.
\newblock Sipping neural networks: Sensitivity-informed provable pruning of
  neural networks.
\newblock \emph{arXiv preprint arXiv:1910.05422}, 2019{\natexlab{b}}.

\bibitem[Berg et~al.(2018)Berg, Hasenclever, Tomczak, and
  Welling]{berg2018sylvester}
Rianne van~den Berg, Leonard Hasenclever, Jakub~M Tomczak, and Max Welling.
\newblock Sylvester normalizing flows for variational inference.
\newblock \emph{arXiv preprint arXiv:1803.05649}, 2018.

\bibitem[Bottou and Bousquet(2008)]{NIPS2007_0d3180d6}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2008.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf}.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and
  Jacobsen]{chen2019residual}
Ricky T.~Q. Chen, Jens Behrmann, David~K Duvenaud, and Joern-Henrik Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf}.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in neural information processing systems}, pages
  6571--6583, 2018.

\bibitem[Dinh et~al.(2015)Dinh, Krueger, and Bengio]{dinh2015nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Dormand and Prince(1980)]{dormand1980family}
John~R Dormand and Peter~J Prince.
\newblock A family of embedded runge-kutta formulae.
\newblock \emph{Journal of computational and applied mathematics}, 6\penalty0
  (1):\penalty0 19--26, 1980.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
Emilien Dupont, Arnaud Doucet, and Yee~Whye Teh.
\newblock Augmented neural odes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3134--3144, 2019.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and
  Papamakarios]{durkan2019neural}
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
\newblock Neural spline flows.
\newblock \emph{arXiv preprint arXiv:1906.04032}, 2019.

\bibitem[Erichson et~al.(2021)Erichson, Azencot, Queiruga, Hodgkinson, and
  Mahoney]{erichson2021lipschitz}
N.~Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and
  Michael~W. Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=-N7PBXqOUJZ}.

\bibitem[Finlay et~al.(2020)Finlay, Jacobsen, Nurbekyan, and
  Oberman]{finlay2020train}
Chris Finlay, J{\"o}rn-Henrik Jacobsen, Levon Nurbekyan, and Adam~M Oberman.
\newblock How to train your neural ode.
\newblock \emph{arXiv preprint arXiv:2002.02798}, 2020.

\bibitem[Frankle and Carbin(2019)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and
  Larochelle]{pmlr-v37-germain15}
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 881--889, Lille,
  France, 07--09 Jul 2015. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v37/germain15.html}.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and
  Xiao]{ghorbani2019investigation}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In \emph{International Conference on Machine Learning}, pages
  2232--2241. PMLR, 2019.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Bettencourt, Sutskever, and
  Duvenaud]{grathwohl2019ffjord}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David
  Duvenaud.
\newblock Ffjord: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{Han15}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock \emph{CoRR}, abs/1510.00149, 2015{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1510.00149}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J Dally.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 1135--1143,
  2015{\natexlab{b}}.

\bibitem[Hasani et~al.(2020)Hasani, Lechner, Amini, Rus, and
  Grosu]{hasani2020natural}
Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu.
\newblock A natural lottery ticket winner: Reinforcement learning with ordinary
  neural circuits.
\newblock In \emph{International Conference on Machine Learning}, pages
  4082--4093. PMLR, 2020.

\bibitem[Hasani et~al.(2021)Hasani, Lechner, Amini, Rus, and
  Grosu]{Hasani2021liquid}
Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu.
\newblock Liquid time-constant networks.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (9):\penalty0 7657--7666, May 2021.

\bibitem[Hassibi and Stork(1993)]{hassibi1993second}
Babak Hassibi and David~G Stork.
\newblock \emph{Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Huang et~al.(2018)Huang, Krueger, Lacoste, and
  Courville]{pmlr-v80-huang18d}
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville.
\newblock Neural autoregressive flows.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 2078--2087. PMLR,
  10--15 Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/huang18d.html}.

\bibitem[Huang et~al.(2020)Huang, Chen, Tsirigotis, and
  Courville]{huang2020convex}
Chin-Wei Huang, Ricky~TQ Chen, Christos Tsirigotis, and Aaron Courville.
\newblock Convex potential flows: Universal probability distributions with
  optimal transport and convex optimization.
\newblock \emph{arXiv preprint arXiv:2012.05942}, 2020.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
Michael~F Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation},
  18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Jaini et~al.(2019)Jaini, Selby, and Yu]{jaini2019sum}
Priyank Jaini, Kira~A Selby, and Yaoliang Yu.
\newblock Sum-of-squares polynomial flow.
\newblock In \emph{International Conference on Machine Learning}, pages
  3009--3018. PMLR, 2019.

\bibitem[Keskar et~al.(2017)Keskar, Nocedal, Tang, Mudigere, and
  Smelyanskiy]{keskar2017large}
Nitish~Shirish Keskar, Jorge Nocedal, Ping Tak~Peter Tang, Dheevatsa Mudigere,
  and Mikhail Smelyanskiy.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR 2017}, 2017.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Diederik~P Kingma and Prafulla Dhariwal.
\newblock Glow: generative flow with invertible 1$\times$ 1 convolutions.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 10236--10245, 2018.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improving}
Diederik~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and
  Max Welling.
\newblock Improving variational inference with inverse autoregressive flow.
\newblock \emph{arXiv preprint arXiv:1606.04934}, 2016.

\bibitem[Kong and Chaudhuri(2020)]{kong2020expressive}
Zhifeng Kong and Kamalika Chaudhuri.
\newblock The expressive power of a class of normalizing flow models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3599--3609. PMLR, 2020.

\bibitem[Lechner and Hasani(2020)]{lechner2020learning}
Mathias Lechner and Ramin Hasani.
\newblock Learning long-term dependencies in irregularly-sampled time series.
\newblock \emph{arXiv preprint arXiv:2006.04418}, 2020.

\bibitem[Lechner et~al.(2020{\natexlab{a}})Lechner, Hasani, Amini, Henzinger,
  Rus, and Grosu]{lechner2020neural}
Mathias Lechner, Ramin Hasani, Alexander Amini, Thomas~A Henzinger, Daniela
  Rus, and Radu Grosu.
\newblock Neural circuit policies enabling auditable autonomy.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (10):\penalty0
  642--652, 2020{\natexlab{a}}.

\bibitem[Lechner et~al.(2020{\natexlab{b}})Lechner, Hasani, Rus, and
  Grosu]{lechner2020gershgorin}
Mathias Lechner, Ramin Hasani, Daniela Rus, and Radu Grosu.
\newblock Gershgorin loss stabilizes the recurrent neural network compartment
  of an end-to-end robot learning scheme.
\newblock In \emph{2020 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 5446--5452. IEEE, 2020{\natexlab{b}}.

\bibitem[Lechner et~al.(2021)Lechner, Hasani, Grosu, Rus, and
  Henzinger]{lechner2021adversarial}
Mathias Lechner, Ramin Hasani, Radu Grosu, Daniela Rus, and Thomas~A Henzinger.
\newblock Adversarial training is not ready for robot learning.
\newblock \emph{arXiv preprint arXiv:2103.08187}, 2021.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Li et~al.(2020)Li, Wong, Chen, and Duvenaud]{li2020scalable}
Xuechen Li, Ting-Kam~Leonard Wong, Ricky~TQ Chen, and David Duvenaud.
\newblock Scalable gradients for stochastic differential equations.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3870--3882. PMLR, 2020.

\bibitem[Liebenwein et~al.(2020)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{liebenwein2020provable}
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJxkOlSYDH}.

\bibitem[Liebenwein et~al.(2021{\natexlab{a}})Liebenwein, Baykal, Carter,
  Gifford, and Rus]{liebenwein2021lost}
Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, and Daniela Rus.
\newblock Lost in pruning: The effects of pruning neural networks beyond test
  accuracy.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3,
  2021{\natexlab{a}}.

\bibitem[Liebenwein et~al.(2021{\natexlab{b}})Liebenwein, Maalouf, Gal,
  Feldman, and Rus]{liebenwein2021compressing}
Lucas Liebenwein, Alaa Maalouf, Oren Gal, Dan Feldman, and Daniela Rus.
\newblock Compressing neural networks: Towards determining the optimal
  layer-wise decomposition.
\newblock \emph{arXiv preprint arXiv:2107.11442}, 2021{\natexlab{b}}.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 5058--5066, 2017.

\bibitem[Maalouf et~al.(2021)Maalouf, Lang, Rus, and Feldman]{maalouf2021deep}
Alaa Maalouf, Harry Lang, Daniela Rus, and Dan Feldman.
\newblock Deep learning meets projective clustering.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=EQfpYwF3-b}.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Park, Yamashita, and
  Asma]{massaroli2020dissecting}
Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime
  Asma.
\newblock Dissecting neural odes.
\newblock In \emph{34th Conference on Neural Information Processing Systems,
  NeurIPS 2020}. The Neural Information Processing Systems, 2020.

\bibitem[Molchanov et~al.(2019)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2019pruning}
P~Molchanov, S~Tyree, T~Karras, T~Aila, and J~Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR 2017-Conference Track Proceedings}, 2019.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, McWilliams, Rousselle, Gross, and
  Nov{\'a}k]{muller2019neural}
Thomas M{\"u}ller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan
  Nov{\'a}k.
\newblock Neural importance sampling.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 38\penalty0 (5):\penalty0
  1--19, 2019.

\bibitem[Oliva et~al.(2018{\natexlab{a}})Oliva, Dubey, Zaheer, Poczos,
  Salakhutdinov, Xing, and Schneider]{oliva2018transformation}
Junier Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos, Ruslan
  Salakhutdinov, Eric Xing, and Jeff Schneider.
\newblock Transformation autoregressive networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  3898--3907. PMLR, 2018{\natexlab{a}}.

\bibitem[Oliva et~al.(2018{\natexlab{b}})Oliva, Dubey, Zaheer, Poczos,
  Salakhutdinov, Xing, and Schneider]{pmlr-v80-oliva18a}
Junier Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos, Ruslan
  Salakhutdinov, Eric Xing, and Jeff Schneider.
\newblock Transformation autoregressive networks.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 3898--3907. PMLR,
  10--15 Jul 2018{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v80/oliva18a.html}.

\bibitem[Onken et~al.(2020)Onken, Fung, Li, and Ruthotto]{onken2020ot}
Derek Onken, Samy~Wu Fung, Xingjian Li, and Lars Ruthotto.
\newblock Ot-flow: Fast and accurate continuous normalizing flows via optimal
  transport.
\newblock \emph{arXiv preprint arXiv:2006.00104}, 2020.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock \emph{arXiv preprint arXiv:1705.07057}, 2017.

\bibitem[Poli et~al.(2020{\natexlab{a}})Poli, Massaroli, Yamashita, Asama, and
  Park]{poli2020torchdyn}
Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, and Jinkyoo
  Park.
\newblock Torchdyn: A neural differential equations library.
\newblock \emph{arXiv preprint arXiv:2009.09346}, 2020{\natexlab{a}}.

\bibitem[Poli et~al.(2020{\natexlab{b}})Poli, Massaroli, Yamashita, Asama,
  Park, et~al.]{poli2020hypersolvers}
Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, Jinkyoo Park,
  et~al.
\newblock Hypersolvers: Toward fast continuous-depth models.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Pontryagin(2018)]{pontryagin2018mathematical}
Lev~Semenovich Pontryagin.
\newblock \emph{Mathematical theory of optimal processes}.
\newblock Routledge, 2018.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{renda2020comparing}
Alex Renda, Jonathan Frankle, and Michael Carbin.
\newblock Comparing fine-tuning and rewinding in neural network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1gSj0NKvB}.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pages
  1530--1538. PMLR, 2015.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Srivastava et~al.(2017{\natexlab{a}})Srivastava, Valkov, Russell,
  Gutmann, and Sutton]{NIPS2017_44a2e080}
Akash Srivastava, Lazar Valkov, Chris Russell, Michael~U. Gutmann, and Charles
  Sutton.
\newblock Veegan: Reducing mode collapse in gans using implicit variational
  learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc.,
  2017{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf}.

\bibitem[Srivastava et~al.(2017{\natexlab{b}})Srivastava, Valkov, Russell,
  Gutmann, and Sutton]{srivastava2017veegan}
Akash Srivastava, Lazar Valkov, Chris Russell, Michael~U Gutmann, and Charles
  Sutton.
\newblock Veegan: Reducing mode collapse in gans using implicit variational
  learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 3310--3320, 2017{\natexlab{b}}.

\bibitem[Teshima et~al.(2020)Teshima, Ishikawa, Tojo, Oono, Ikeda, and
  Sugiyama]{teshima2020coupling}
Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and
  Masashi Sugiyama.
\newblock Coupling-based invertible neural networks are universal
  diffeomorphism approximators.
\newblock \emph{arXiv preprint arXiv:2006.11469}, 2020.

\bibitem[Vorbach et~al.(2021)Vorbach, Hasani, Amini, Lechner, and
  Rus]{vorbach2021causal}
Charles Vorbach, Ramin Hasani, Alexander Amini, Mathias Lechner, and Daniela
  Rus.
\newblock Causal navigation by continuous-time neural networks.
\newblock \emph{arXiv preprint arXiv:2106.08314}, 2021.

\bibitem[Wehenkel and Louppe(2019)]{wehenkel2019unconstrained}
Antoine Wehenkel and Gilles Louppe.
\newblock Unconstrained monotonic neural networks.
\newblock \emph{arXiv preprint arXiv:1908.05164}, 2019.

\bibitem[Yang and Karniadakis(2020)]{yang2020potential}
Liu Yang and George~Em Karniadakis.
\newblock Potential flow generator with l2 optimal transport regularity for
  generative models.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{yang2017designing}
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5687--5695, 2017.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael~W Mahoney.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE International Conference on Big Data (Big Data)},
  pages 581--590. IEEE, 2020.

\end{thebibliography}
