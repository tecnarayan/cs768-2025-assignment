\begin{thebibliography}{10}

\bibitem{chu2021slowed}
Johan~SG Chu and James~A Evans.
\newblock Slowed canonical progress in large fields of science.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(41):e2021636118, 2021.

\bibitem{yu2012citation}
Xiao Yu, Quanquan Gu, Mianwei Zhou, and Jiawei Han.
\newblock Citation prediction in heterogeneous bibliographic networks.
\newblock In {\em Proceedings of the 2012 SIAM international conference on data mining}, pages 1119--1130. SIAM, 2012.

\bibitem{DBLP:conf/acl/CohanFBDW20}
Arman Cohan, Sergey Feldman, Iz~Beltagy, Doug Downey, and Daniel~S. Weld.
\newblock {SPECTER:} document-level representation learning using citation-informed transformers.
\newblock In {\em {ACL}}, pages 2270--2282. Association for Computational Linguistics, 2020.

\bibitem{DBLP:conf/emnlp/OstendorffRAGR22}
Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, and Georg Rehm.
\newblock Neighborhood contrastive learning for scientific document representations with citation embeddings.
\newblock In {\em {EMNLP}}, pages 11670--11688. Association for Computational Linguistics, 2022.

\bibitem{DBLP:conf/acl/JinZZ000023}
Bowen Jin, Wentao Zhang, Yu~Zhang, Yu~Meng, Xinyang Zhang, Qi~Zhu, and Jiawei Han.
\newblock Patton: Language model pretraining on text-rich networks.
\newblock In {\em {ACL} {(1)}}, pages 7005--7020. Association for Computational Linguistics, 2023.

\bibitem{jin2023learning}
Bowen Jin, Wentao Zhang, Yu~Zhang, Yu~Meng, Han Zhao, and Jiawei Han.
\newblock Learning multiplex embeddings on text-rich networks with one text encoder.
\newblock {\em arXiv preprint arXiv:2310.06684}, 2023.

\bibitem{seoh2023encoding}
Ronald Seoh, Haw-Shiuan Chang, and Andrew McCallum.
\newblock Encoding multi-domain scientific papers by ensembling multiple cls tokens.
\newblock {\em arXiv preprint arXiv:2309.04333}, 2023.

\bibitem{ding2024artificial}
Jingtao Ding, Chang Liu, Yu~Zheng, Yunke Zhang, Zihan Yu, Ruikun Li, Hongyi Chen, Jinghua Piao, Huandong Wang, Jiazhen Liu, et~al.
\newblock Artificial intelligence for complex network: Potential, methodology and application.
\newblock {\em arXiv preprint arXiv:2402.16887}, 2024.

\bibitem{fortunato2018science}
Santo Fortunato, Carl~T Bergstrom, Katy B{\"o}rner, James~A Evans, Dirk Helbing, Sta{\v{s}}a Milojevi{\'c}, Alexander~M Petersen, Filippo Radicchi, Roberta Sinatra, Brian Uzzi, et~al.
\newblock Science of science.
\newblock {\em Science}, 359(6379):eaao0185, 2018.

\bibitem{wu2019large}
Lingfei Wu, Dashun Wang, and James~A Evans.
\newblock Large teams develop and small teams disrupt science and technology.
\newblock {\em Nature}, 566(7744):378--382, 2019.

\bibitem{alvarez2021evolutionary}
Unai Alvarez-Rodriguez, Federico Battiston, Guilherme~Ferraz de~Arruda, Yamir Moreno, Matja{\v{z}} Perc, and Vito Latora.
\newblock Evolutionary dynamics of higher-order interactions in social networks.
\newblock {\em Nature Human Behaviour}, 5(5):586--595, 2021.

\bibitem{park2023papers}
Michael Park, Erin Leahey, and Russell~J Funk.
\newblock Papers and patents are becoming less disruptive over time.
\newblock {\em Nature}, 613(7942):138--144, 2023.

\bibitem{xu2022flat}
Fengli Xu, Lingfei Wu, and James Evans.
\newblock Flat teams drive scientific innovation.
\newblock {\em Proceedings of the National Academy of Sciences}, 119(23):e2200927119, 2022.

\bibitem{lin2023remote}
Yiling Lin, Carl~Benedikt Frey, and Lingfei Wu.
\newblock Remote collaboration fuses fewer breakthrough ideas.
\newblock {\em Nature}, 623(7989):987--991, 2023.

\bibitem{sinha2015overview}
Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang.
\newblock An overview of microsoft academic service (mas) and applications.
\newblock In {\em Proceedings of the 24th international conference on world wide web}, pages 243--246, 2015.

\bibitem{li2023towards}
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.
\newblock Towards general text embeddings with multi-stage contrastive learning.
\newblock {\em arXiv preprint arXiv:2308.03281}, 2023.

\bibitem{muennighoff2022mteb}
Niklas Muennighoff, Nouamane Tazi, Lo{\"\i}c Magne, and Nils Reimers.
\newblock Mteb: Massive text embedding benchmark.
\newblock {\em arXiv preprint arXiv:2210.07316}, 2022.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{DBLP:conf/iclr/0007WKWA21}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q. Weinberger, and Yoav Artzi.
\newblock Revisiting few-sample {BERT} fine-tuning.
\newblock In {\em {ICLR}}. OpenReview.net, 2021.

\bibitem{pobrotyn2021neuralndcg}
Przemys{\l}aw Pobrotyn and Rados{\l}aw Bia{\l}obrzeski.
\newblock Neuralndcg: Direct optimisation of a ranking metric via differentiable relaxation of sorting.
\newblock {\em arXiv preprint arXiv:2102.07831}, 2021.

\bibitem{DBLP:conf/emnlp/KarpukhinOMLWEC20}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.~H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen{-}tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In {\em {EMNLP} {(1)}}, pages 6769--6781. Association for Computational Linguistics, 2020.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in neural information processing systems}, 35:24824--24837, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems}, 35:22199--22213, 2022.

\bibitem{li2024limp}
Songwei Li, Jie Feng, Jiawei Chi, Xinyuan Hu, Xiaomeng Zhao, and Fengli Xu.
\newblock Limp: Large language model enhanced intent-aware mobility prediction.
\newblock {\em arXiv preprint arXiv:2408.12832}, 2024.

\bibitem{parvez2024evidence}
Md~Rizwan Parvez.
\newblock Evidence to generate (e2g): A single-agent two-step prompting for context grounded and retrieval augmented reasoning.
\newblock {\em arXiv preprint arXiv:2401.05787}, 2024.

\bibitem{chen2024large}
Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, and Pan Hui.
\newblock Large language model-driven meta-structure discovery in heterogeneous information network.
\newblock In {\em Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 307--318, 2024.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context.
\newblock {\em arXiv preprint arXiv:1901.02860}, 2019.

\bibitem{bengio2000neural}
Yoshua Bengio, R{\'e}jean Ducharme, and Pascal Vincent.
\newblock A neural probabilistic language model.
\newblock {\em Advances in neural information processing systems}, 13, 2000.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{DBLP:conf/emnlp/BeltagyLC19}
Iz~Beltagy, Kyle Lo, and Arman Cohan.
\newblock Scibert: {A} pretrained language model for scientific text.
\newblock In {\em {EMNLP/IJCNLP} {(1)}}, pages 3613--3618. Association for Computational Linguistics, 2019.

\bibitem{DBLP:conf/emnlp/SinghDCDF23}
Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, and Sergey Feldman.
\newblock Scirepeval: {A} multi-format benchmark for scientific document representations.
\newblock In {\em {EMNLP}}, pages 5548--5566. Association for Computational Linguistics, 2023.

\bibitem{DBLP:conf/emnlp/0044CSLWG23}
Yu~Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye{-}Yi Wang, and Jianfeng Gao.
\newblock Pre-training multi-task contrastive learning models for scientific literature understanding.
\newblock In {\em {EMNLP} (Findings)}, pages 12259--12275. Association for Computational Linguistics, 2023.

\bibitem{zhang2024mgte}
Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, et~al.
\newblock mgte: Generalized long-context text representation and reranking models for multilingual text retrieval.
\newblock {\em arXiv preprint arXiv:2407.19669}, 2024.

\bibitem{li2018dna}
Suping Li, Qiao Jiang, Shaoli Liu, Yinlong Zhang, Yanhua Tian, Chen Song, Jing Wang, Yiguo Zou, Gregory~J Anderson, Jing-Yan Han, et~al.
\newblock A dna nanorobot functions as a cancer therapeutic in response to a molecular trigger in vivo.
\newblock {\em Nature biotechnology}, 36(3):258--264, 2018.

\bibitem{pinheiro2011challenges}
Andre~V Pinheiro, Dongran Han, William~M Shih, and Hao Yan.
\newblock Challenges and opportunities for structural dna nanotechnology.
\newblock {\em Nature nanotechnology}, 6(12):763--772, 2011.

\bibitem{lee2012molecularly}
Hyukjin Lee, Abigail~KR Lytton-Jean, Yi~Chen, Kevin~T Love, Angela~I Park, Emmanouil~D Karagiannis, Alfica Sehgal, William Querbes, Christopher~S Zurenko, Muthusamy Jayaraman, et~al.
\newblock Molecularly self-assembled nucleic acid nanoparticles for targeted in vivo sirna delivery.
\newblock {\em Nature nanotechnology}, 7(6):389--393, 2012.

\bibitem{xing2011self}
Yongzheng Xing, Enjun Cheng, Yang Yang, Ping Chen, Tao Zhang, Yawei Sun, Zhongqiang Yang, and Dongsheng Liu.
\newblock Self-assembled dna hydrogels with designable thermal and enzymatic responsiveness.
\newblock {\em Advanced Materials}, 23(9):1117--1121, 2011.

\bibitem{peng2012cells}
Songming Peng, Thomas~L Derrien, Jinhui Cui, Chuanying Xu, and Dan Luo.
\newblock From cells to dna materials.
\newblock {\em Materials Today}, 15(5):190--194, 2012.

\bibitem{DBLP:conf/emnlp/MadaanHY23}
Aman Madaan, Katherine Hermann, and Amir Yazdanbakhsh.
\newblock What makes chain-of-thought prompting effective? {A} counterfactual study.
\newblock In {\em {EMNLP} (Findings)}, pages 1448--1535. Association for Computational Linguistics, 2023.

\bibitem{beltagy2019scibert}
Iz~Beltagy, Kyle Lo, and Arman Cohan.
\newblock Scibert: A pretrained language model for scientific text.
\newblock {\em arXiv preprint arXiv:1903.10676}, 2019.

\bibitem{meng2021coco}
Yu~Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett, Jiawei Han, Xia Song, et~al.
\newblock Coco-lm: Correcting and contrasting text sequences for language model pretraining.
\newblock {\em Advances in Neural Information Processing Systems}, 34:23102--23114, 2021.

\bibitem{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov, and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language understanding.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{chew2023llm}
Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice Kim.
\newblock Llm-assisted content analysis: Using large language models to support deductive coding.
\newblock {\em arXiv preprint arXiv:2306.14924}, 2023.

\bibitem{chan2023chateval}
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.
\newblock Chateval: Towards better llm-based evaluators through multi-agent debate.
\newblock {\em arXiv preprint arXiv:2308.07201}, 2023.

\bibitem{lan2023stance}
Xiaochong Lan, Chen Gao, Depeng Jin, and Yong Li.
\newblock Stance detection with collaborative role-infused llm-based agents.
\newblock {\em arXiv preprint arXiv:2310.10467}, 2023.

\bibitem{ouyang2023llm}
Shuyin Ouyang, Jie~M Zhang, Mark Harman, and Meng Wang.
\newblock Llm is like a box of chocolates: the non-determinism of chatgpt in code generation.
\newblock {\em arXiv preprint arXiv:2308.02828}, 2023.

\bibitem{liu2024your}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{imani2023mathprompter}
Shima Imani, Liang Du, and Harsh Shrivastava.
\newblock Mathprompter: Mathematical reasoning using large language models.
\newblock {\em arXiv preprint arXiv:2303.05398}, 2023.

\bibitem{wu2023empirical}
Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin~Tat Lee, Richard Peng, and Chi Wang.
\newblock An empirical study on challenging math problem solving with gpt-4.
\newblock {\em arXiv preprint arXiv:2306.01337}, 2023.

\bibitem{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock {\em arXiv preprint arXiv:2305.16291}, 2023.

\bibitem{zhu2023ghost}
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et~al.
\newblock Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory.
\newblock {\em arXiv preprint arXiv:2305.17144}, 2023.

\bibitem{DBLP:conf/corl/HuangXXCLFZTMCS22}
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.
\newblock Inner monologue: Embodied reasoning through planning with language models.
\newblock In {\em CoRL}, volume 205 of {\em Proceedings of Machine Learning Research}, pages 1769--1782. {PMLR}, 2022.

\bibitem{zeng2024perceive}
Qingbin Zeng, Qinglong Yang, Shunan Dong, Heming Du, Liang Zheng, Fengli Xu, and Yong Li.
\newblock Perceive, reflect, and plan: Designing llm agent for goal-directed city navigation without instructions.
\newblock {\em arXiv preprint arXiv:2408.04168}, 2024.

\bibitem{DBLP:conf/uist/ParkOCMLB23}
Joon~Sung Park, Joseph~C. O'Brien, Carrie~Jun Cai, Meredith~Ringel Morris, Percy Liang, and Michael~S. Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In {\em {UIST}}, pages 2:1--2:22. {ACM}, 2023.

\bibitem{shao2024beyond}
Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng Wang, and Yong Li.
\newblock Beyond imitation: Generating human mobility from context-aware reasoning with large language models.
\newblock {\em arXiv preprint arXiv:2402.09836}, 2024.

\bibitem{li2023large}
Nian Li, Chen Gao, Yong Li, and Qingmin Liao.
\newblock Large language model-empowered agents for simulating macroeconomic activities.
\newblock {\em arXiv preprint arXiv:2310.10436}, 2023.

\bibitem{lan2024depression}
Xiaochong Lan, Yiming Cheng, Li~Sheng, Chen Gao, and Yong Li.
\newblock Depression detection on social media with large language models.
\newblock {\em arXiv preprint arXiv:2403.10750}, 2024.

\end{thebibliography}
