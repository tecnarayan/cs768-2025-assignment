\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Bartlett, Ravikumar, and
  Wainwright]{agarwal:12}
Alekh Agarwal, Peter~L. Bartlett, Pradeep Ravikumar, and Martin~J. Wainwright.
\newblock Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (5):\penalty0 3235--3249, 2012.

\bibitem[Arnold et~al.(2019)Arnold, Iqbal, and Sha]{arnold:19}
S\'{e}bastien M.~R. Arnold, Shariq Iqbal, and Fei Sha.
\newblock Decoupling adaptation from modeling with meta-optimizers for meta
  learning.
\newblock arXiv, 2019.

\bibitem[Balcan et~al.(2015)Balcan, Blum, and Vempala]{balcan:15}
Maria-Florina Balcan, Avrim Blum, and Santosh Vempala.
\newblock Efficient representations for lifelong learning and autoencoding.
\newblock In \emph{Proceedings of the 28th Annual Conference on Learning
  Theory}, 2015.

\bibitem[Baxter(2000)]{baxter:00}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of Artificial Intelligence Research}, 12:\penalty0
  149--198, 2000.

\bibitem[Bullins et~al.(2019)Bullins, Hazan, Kalai, and Livni]{bullins:19}
Brian Bullins, Elad Hazan, Adam Kalai, and Roi Livni.
\newblock Generalize across tasks: Efficient algorithms for linear
  representation learning.
\newblock In \emph{Proceedings of the 30th International Conference on
  Algorithmic Learning Theory}, 2019.

\bibitem[Davis and Kahan(1970)]{davis1970rotation}
Chandler Davis and William~Morton Kahan.
\newblock The rotation of eigenvectors by a perturbation. iii.
\newblock \emph{SIAM Journal on Numerical Analysis}, 7\penalty0 (1):\penalty0
  1--46, 1970.

\bibitem[Denevi et~al.(2018)Denevi, Ciliberto, Stamos, and Pontil]{denevi:18a}
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil.
\newblock Incremental learning-to-learn with statistical guarantees.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, 2018.

\bibitem[Denevi et~al.(2019)Denevi, Ciliberto, Grazzi, and Pontil]{denevi:19a}
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil.
\newblock Learning-to-learn stochastic gradient descent with biased
  regularization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, 2019.

\bibitem[Evgeniou and Pontil(2004)]{evgeniou:04}
Theodoros Evgeniou and Massimiliano Pontil.
\newblock Regularized multi-task learning.
\newblock In \emph{Proceedings of the 10th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, 2004.

\bibitem[Fallah et~al.(2019)Fallah, Mokhtari, and Ozdaglar]{fallah:19}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock arXiv, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn:17}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel:19}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in deep linear
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar:18}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Khodak et~al.(2019)Khodak, Balcan, and Talwalkar]{khodak:19b}
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar.
\newblock Adaptive gradient-based meta-learning methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Lake et~al.(2017)Lake, Salakhutdinov, Gross, and Tenenbaum]{lake:11}
Brenden~M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua~B. Tenenbaum.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the Conference of the Cognitive Science
  Society (CogSci)}, 2017.

\bibitem[Lampinen and Ganguli(2019)]{lampinen:19}
Andrew Lampinen and Surya Ganguli.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks, 2019.

\bibitem[Maurer(2005)]{maurer:05}
Andreas Maurer.
\newblock Algorithmic stability and meta-learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 967--994,
  2005.

\bibitem[Maurer(2009)]{maurer:09}
Andreas Maurer.
\newblock Transfer bounds for linear feature learning.
\newblock \emph{Machine Learning}, 2009.

\bibitem[Maurer and Pontil(2013)]{maurer:13}
Andreas Maurer and Massimiliano Pontil.
\newblock Excess risk bounds for multitask learning with trace norm
  regularization.
\newblock In \emph{Proceedings of the 26th Annual Conference on Learning
  Theory}, 2013.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and Romera-Paredes]{maurer:16}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2853--2884, 2016.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and {Aguera y
  Arcas}]{mcmahan:17}
H.~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
  {Aguera y Arcas}.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artifical Intelligence and Statistics}, 2017.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol:18}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock arXiv, 2018.

\bibitem[Raghu et~al.(2019)Raghu, Raghu, Bengio, and Vinyals]{raghu:19}
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.
\newblock Rapid learning or feature reuse? {T}owards understanding the
  effectiveness of {MAML}.
\newblock arXiv, 2019.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran:19}
Aravind Rajeswaran, Chelsea Finn, Sham~M. Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Ravi and Larochelle(2017)]{ravi:17}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{Proceedings of the 5th International Conference on Learning
  Representations}, 2017.

\bibitem[Ruvolo and Eaton(2013)]{ruvolo:13}
Paul Ruvolo and Eric Eaton.
\newblock {ELLA}: An efficient lifelong learning algorithm.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, 2013.

\bibitem[Saxe et~al.(2014)Saxe, Mcclelland, and Ganguli]{saxe:14}
Andrew~M. Saxe, James~L. Mcclelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural network.
\newblock In \emph{In International Conference on Learning Representations},
  2014.

\bibitem[Saxe et~al.(2019)Saxe, McClelland, and Ganguli]{saxe:19}
Andrew~M. Saxe, James~L. McClelland, and Surya Ganguli.
\newblock A mathematical theory of semantic development in deep neural
  networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (23):\penalty0 11537--11546, 2019.

\bibitem[Thrun and Pratt(1998)]{thrun:98}
Sebastian Thrun and Lorien Pratt.
\newblock \emph{Learning to Learn}.
\newblock Springer Science \& Business Media, 1998.

\bibitem[Zhou et~al.(2019)Zhou, Yuan, Xu, Yan, and Feng]{zhou:19}
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng.
\newblock Efficient meta learning via minibatch proximal update.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\end{thebibliography}
