
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{chen2020multispeech,
  title={Multispeech: Multi-speaker text to speech with transformer},
  author={Chen, Mingjian and Tan, Xu and Ren, Yi and Xu, Jin and Sun, Hao and Zhao, Sheng and Qin, Tao and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2006.04664},
  year={2020}
}

@inproceedings{tachibana2018efficiently,
  title={Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention},
  author={Tachibana, Hideyuki and Uenoyama, Katsuya and Aihara, Shunsuke},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4784--4788},
  year={2018},
  organization={IEEE}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@misc{chu2006objective,
  title={Objective measure for estimating mean opinion score of synthesized speech},
  author={Chu, Min and Peng, Hu},
  year={2006},
  month=apr # "~4",
  publisher={Google Patents},
  note={US Patent 7,024,362}
}

@inproceedings{chung2016out,
  title={Out of time: automated lip sync in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Asian conference on computer vision},
  pages={251--263},
  year={2016},
  organization={Springer}
}
@inproceedings{wan2018generalized,
  title={Generalized end-to-end loss for speaker verification},
  author={Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4879--4883},
  year={2018},
  organization={IEEE}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{wang2017tacotron,
  title={Tacotron: Towards end-to-end speech synthesis},
  author={Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and others},
  journal={arXiv preprint arXiv:1703.10135},
  year={2017}
}

@inproceedings{shen2018natural,
  title={Natural tts synthesis by conditioning wavenet on mel spectrogram predictions},
  author={Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4779--4783},
  year={2018},
  organization={IEEE}
}

@article{assael2016lipnet,
  title={Lipnet: End-to-end sentence-level lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and De Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01599},
  year={2016}
}

@inproceedings{chung2017lip,
  title={Lip reading sentences in the wild},
  author={Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3444--3453},
  year={2017},
  organization={IEEE}
}

@InProceedings{Chung17a,
  author       = "Joon~Son Chung and Andrew Zisserman",
  title        = "Lip Reading in Profile",
  booktitle    = "British Machine Vision Conference",
  year         = "2017",
}

@article{kello2004neural,
  title={A neural network model of the articulatory-acoustic forward mapping trained on recordings of articulatory parameters},
  author={Kello, Christopher T and Plaut, David C},
  journal={The Journal of the Acoustical Society of America},
  volume={116},
  number={4},
  pages={2354--2364},
  year={2004},
  publisher={Acoustical Society of America}
}

@inproceedings{ephrat2017vid2speech,
  title={Vid2speech: speech reconstruction from silent video},
  author={Ephrat, Ariel and Peleg, Shmuel},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5095--5099},
  year={2017},
  organization={IEEE}
}

@inproceedings{kumar2019lipper,
  title={Lipper: Synthesizing thy speech using multi-view lipreading},
  author={Kumar, Yaman and Jain, Rohit and Salik, Khwaja Mohd and Shah, Rajiv Ratn and Yin, Yifang and Zimmermann, Roger},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={2588--2595},
  year={2019}
}

@inproceedings{prajwal2020learning,
  title={Learning individual speaking styles for accurate lip to speech synthesis},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13796--13805},
  year={2020}
}

@inproceedings{ren2019fastspeech,
  title={Fastspeech: Fast, robust and controllable text to speech},
  author={Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{hunt1996unit,
  title={Unit selection in a concatenative speech synthesis system using a large speech database},
  author={Hunt, Andrew J and Black, Alan W},
  booktitle={1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings},
  pages={373--376},
  year={1996},
  organization={IEEE}
}

@inproceedings{wu2016merlin,
  title={Merlin: An Open Source Neural Network Speech Synthesis System.},
  author={Wu, Zhizheng and Watts, Oliver and King, Simon},
  booktitle={SSW},
  pages={202--207},
  year={2016}
}

@inproceedings{ping2018clarinet,
  title={Clarinet: Parallel wave generation in end-to-end text-to-speech},
  author={Ping, Wei and Peng, Kainan and Chen, Jitong},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{ping2018deep,
  title={Deep voice 3: 2000-speaker neural text-to-speech},
  author={Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
  journal={Proc. ICLR},
  pages={214--217},
  year={2018}
}

@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@inproceedings{prenger2019waveglow,
  title={Waveglow: A flow-based generative network for speech synthesis},
  author={Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3617--3621},
  year={2019},
  organization={IEEE}
}

@inproceedings{yamamoto2020parallel,
  title={Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram},
  author={Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6199--6203},
  year={2020},
  organization={IEEE}
}

@article{kumar2019melgan,
  title={Melgan: Generative adversarial networks for conditional waveform synthesis},
  author={Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Br{\'e}bisson, Alexandre and Bengio, Yoshua and Courville, Aaron},
  journal={arXiv preprint arXiv:1910.06711},
  year={2019}
}

@article{griffin1984signal,
  title={Signal estimation from modified short-time Fourier transform},
  author={Griffin, Daniel and Lim, Jae},
  journal={IEEE Transactions on acoustics, speech, and signal processing},
  volume={32},
  number={2},
  pages={236--243},
  year={1984},
  publisher={IEEE}
}

@inproceedings{li2019neural,
  title={Neural speech synthesis with transformer network},
  author={Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6706--6713},
  year={2019}
}

@inproceedings{ren2021fastspeech,
  title={Fastspeech 2: Fast and high-quality end-to-end text to speech},
  author={Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{kim2020glow,
  title={Glow-TTS: A generative flow for text-to-speech via monotonic alignment search},
  author={Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2005.11129},
  year={2020}
}

@inproceedings{petridis2018end,
  title={End-to-end audiovisual speech recognition},
  author={Petridis, Stavros and Stafylakis, Themos and Ma, Pingehuan and Cai, Feipeng and Tzimiropoulos, Georgios and Pantic, Maja},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={6548--6552},
  year={2018},
  organization={IEEE}
}

@inproceedings{jia2018transfer,
  title={Transfer learning from speaker verification to multispeaker text-to-speech synthesis},
  author={Jia, Ye and Zhang, Yu and Weiss, Ron J and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{afouras2018deep,
  title={Deep lip reading: a comparison of models and an online application},
  author={Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
  booktitle={Interspeech},
  year={2018}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{chorowski2015attention,
  title={Attention-based models for speech recognition},
  author={Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1506.07503},
  year={2015}
}

@inproceedings{cao2018vggface2,
  title={Vggface2: A dataset for recognising faces across pose and age},
  author={Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M and Zisserman, Andrew},
  booktitle={2018 13th IEEE international conference on automatic face \& gesture recognition (FG 2018)},
  pages={67--74},
  year={2018},
  organization={IEEE}
}

@article{edwards2016jali,
  title={JALI: an animator-centric viseme model for expressive lip synchronization},
  author={Edwards, Pif and Landreth, Chris and Fiume, Eugene and Singh, Karan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={35},
  number={4},
  pages={1--11},
  year={2016}
}

@article{zhou2018visemenet,
  title={Visemenet: Audio-driven animator-centric speech animation},
  author={Zhou, Yang and Xu, Zhan and Landreth, Chris and Kalogerakis, Evangelos and Maji, Subhransu and Singh, Karan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--10},
  year={2018}
}

@article{suwajanakorn2017synthesizing,
  title={Synthesizing obama: learning lip sync from audio},
  author={Suwajanakorn, Supasorn and Seitz, Steven M and Kemelmacher-Shlizerman, Ira},
  journal={ACM Transactions on Graphics (ToG)},
  volume={36},
  number={4},
  pages={1--13},
  year={2017}
}

@inproceedings{wiles2018x2face,
  title={X2face: A network for controlling face generation using images, audio, and pose codes},
  author={Wiles, Olivia and Koepke, A and Zisserman, Andrew},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={670--686},
  year={2018}
}

@inproceedings{thies2020neural,
  title={Neural voice puppetry: Audio-driven facial reenactment},
  author={Thies, Justus and Elgharib, Mohamed and Tewari, Ayush and Theobalt, Christian and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={716--731},
  year={2020}
}

@article{taylor2017deep,
  title={A deep learning approach for generalized speech animation},
  author={Taylor, Sarah and Kim, Taehwan and Yue, Yisong and Mahler, Moshe and Krahe, James and Rodriguez, Anastasio Garcia and Hodgins, Jessica and Matthews, Iain},
  journal={ACM Transactions on Graphics (TOG)},
  volume={36},
  number={4},
  pages={1--11},
  year={2017}
}

@article{karras2017audio,
  title={Audio-driven facial animation by joint end-to-end learning of pose and emotion},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Herva, Antti and Lehtinen, Jaakko},
  journal={ACM Transactions on Graphics (TOG)},
  volume={36},
  number={4},
  pages={1--12},
  year={2017}
}

@article{richard2021meshtalk,
  title={MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement},
  author={Richard, Alexander and Zollhoefer, Michael and Wen, Yandong and de la Torre, Fernando and Sheikh, Yaser},
  journal={arXiv preprint arXiv:2104.08223},
  year={2021}
}

@inproceedings{chung2017you,
  title={You said that?},
  author={Chung, Joon Son and Jamaludin, Amir and Zisserman, Andrew},
  booktitle={British Machine Vision Conference},
  year={2017}
}

@article{parkhi2015deep,
  title={Deep face recognition},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
  year={2015},
  publisher={British Machine Vision Association}
}

@inproceedings{zhou2019talking,
  title={Talking face generation by adversarially disentangled audio-visual representation},
  author={Zhou, Hang and Liu, Yu and Liu, Ziwei and Luo, Ping and Wang, Xiaogang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={9299--9306},
  year={2019}
}

@inproceedings{prajwal2020lip,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={484--492},
  year={2020}
}

@article{stafylakis2017combining,
  title={Combining residual networks with LSTMs for lipreading},
  author={Stafylakis, Themos and Tzimiropoulos, Georgios},
  journal={arXiv preprint arXiv:1703.04105},
  year={2017}
}

@article{afouras2018deepav,
  title={Deep audio-visual speech recognition},
  author={Afouras, Triantafyllos and Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2018},
  publisher={IEEE}
}

@article{zen2019libritts,
  title={LibriTTS: A corpus derived from LibriSpeech for text-to-speech},
  author={Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J and Jia, Ye and Chen, Zhifeng and Wu, Yonghui},
  journal={arXiv preprint arXiv:1904.02882},
  year={2019}
}

@inproceedings{arik2017deep,
  title={Deep voice: Real-time neural text-to-speech},
  author={Ar{\i}k, Sercan {\"O} and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and others},
  booktitle={International Conference on Machine Learning},
  pages={195--204},
  year={2017},
  organization={PMLR}
}

@inproceedings{zhang2017s3fd,
  title={S3fd: Single shot scale-invariant face detector},
  author={Zhang, Shifeng and Zhu, Xiangyu and Lei, Zhen and Shi, Hailin and Wang, Xiaobo and Li, Stan Z},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={192--201},
  year={2017}
}

@inproceedings{nagrani2018learnable,
  title={Learnable pins: Cross-modal embeddings for person identity},
  author={Nagrani, Arsha and Albanie, Samuel and Zisserman, Andrew},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={71--88},
  year={2018}
}

@inproceedings{kim2018learning,
  title={On learning associations of faces and voices},
  author={Kim, Changil and Shin, Hijung Valentina and Oh, Tae-Hyun and Kaspar, Alexandre and Elgharib, Mohamed and Matusik, Wojciech},
  booktitle={Asian Conference on Computer Vision},
  pages={276--292},
  year={2018},
  organization={Springer}
}
@article{chung2020seeing,
  title={Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision},
  author={Chung, Soo-Whan and Kang, Hong Goo and Chung, Joon Son},
  journal={arXiv preprint arXiv:2004.14326},
  year={2020}
}

@inproceedings{halperin2019dynamic,
  title={Dynamic temporal alignment of speech to lips},
  author={Halperin, Tavi and Ephrat, Ariel and Peleg, Shmuel},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3980--3984},
  year={2019},
  organization={IEEE}
}

@article{vougioukas2020realistic,
  title={Realistic speech-driven facial animation with gans},
  author={Vougioukas, Konstantinos and Petridis, Stavros and Pantic, Maja},
  journal={International Journal of Computer Vision},
  volume={128},
  number={5},
  pages={1398--1413},
  year={2020},
  publisher={Springer}
}

@article{yang2020large,
  title={Large-scale multilingual audio visual dubbing},
  author={Yang, Yi and Shillingford, Brendan and Assael, Yannis and Wang, Miaosen and Liu, Wendi and Chen, Yutian and Zhang, Yu and Sezener, Eren and Cobo, Luis C and Denil, Misha and others},
  journal={arXiv preprint arXiv:2011.03530},
  year={2020}
}