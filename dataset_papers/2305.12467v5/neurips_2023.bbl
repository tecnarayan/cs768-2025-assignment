\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2022{\natexlab{a}})Abbe, Adsera, and Misiakiewicz]{abbe2022merged}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 4782--4887. PMLR, 2022{\natexlab{a}}.

\bibitem[Abbe et~al.(2022{\natexlab{b}})Abbe, Cornacchia, Hazla, and Marquis]{abbe2022initial}
Emmanuel Abbe, Elisabetta Cornacchia, Jan Hazla, and Christopher Marquis.
\newblock An initial alignment between neural network and target is needed for gradient descent to learn.
\newblock In \emph{International Conference on Machine Learning}, pages 33--52. PMLR, 2022{\natexlab{b}}.

\bibitem[Abbe et~al.(2023)Abbe, Boix-Adsera, and Misiakiewicz]{abbe2023sgd}
Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz.
\newblock Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics.
\newblock \emph{arXiv preprint arXiv:2302.11055}, 2023.

\bibitem[Ahn et~al.(2022{\natexlab{a}})Ahn, Bubeck, Chewi, Lee, Suarez, and Zhang]{ahn2022learning}
Kwangjun Ahn, S{\'e}bastien Bubeck, Sinho Chewi, Yin~Tat Lee, Felipe Suarez, and Yi~Zhang.
\newblock Learning threshold neurons via the" edge of stability".
\newblock \emph{arXiv preprint arXiv:2212.07469}, 2022{\natexlab{a}}.

\bibitem[Ahn et~al.(2022{\natexlab{b}})Ahn, Zhang, and Sra]{ahn2022understanding}
Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra.
\newblock Understanding the unstable convergence of gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 247--257. PMLR, 2022{\natexlab{b}}.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages 242--252. PMLR, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 322--332. PMLR, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio, Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International conference on machine learning}, pages 233--242. PMLR, 2017.

\bibitem[Atanasov et~al.(2022)Atanasov, Bordelon, and Pehlevan]{atanasov2021neural}
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan.
\newblock Neural networks as kernel learners: The silent alignment effect.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and Valiant]{blanc2020implicit}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process.
\newblock In \emph{Conference on learning theory}, pages 483--513. PMLR, 2020.

\bibitem[Bolte et~al.(2010)Bolte, Daniilidis, Ley, and Mazet]{bolte2010characterizations}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet.
\newblock Characterizations of {\l}ojasiewicz inequalities: subgradient flows, talweg, convexity.
\newblock \emph{Transactions of the American Mathematical Society}, 362\penalty0 (6):\penalty0 3319--3363, 2010.

\bibitem[Boursier et~al.(2022)Boursier, Pillaud-Vivien, and Flammarion]{boursier2022gradient}
Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and Shalev-Shwartz]{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on linearly separable data.
\newblock \emph{arXiv preprint arXiv:1710.10174}, 2017.

\bibitem[Chatterji et~al.(2021)Chatterji, Long, and Bartlett]{chatterji2021doesA}
Niladri~S Chatterji, Philip~M Long, and Peter~L Bartlett.
\newblock When does gradient descent with logistic loss find interpolating two-layer networks?
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (159):\penalty0 1--48, 2021.

\bibitem[Chen et~al.(2023)Chen, Li, Luo, Zhou, and Xu]{chen2023phase}
Zhengan Chen, Yuqing Li, Tao Luo, Zhangchen Zhou, and Zhi-Qin~John Xu.
\newblock Phase diagram of initial condensation for two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:2303.06561}, 2023.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR, 2020.

\bibitem[Clarke et~al.(2008)Clarke, Ledyaev, Stern, and Wolenski]{clarke2008nonsmooth}
Francis~H Clarke, Yuri~S Ledyaev, Ronald~J Stern, and Peter~R Wolenski.
\newblock \emph{Nonsmooth analysis and control theory}, volume 178.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Damian et~al.(2022)Damian, Nichani, and Lee]{damian2022self}
Alex Damian, Eshaan Nichani, and Jason~D Lee.
\newblock Self-stabilization: The implicit bias of gradient descent at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2209.15594}, 2022.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Dutta et~al.(2013)Dutta, Deb, Tulshyan, and Arora]{dutta2013approximate}
Joydeep Dutta, Kalyanmoy Deb, Rupesh Tulshyan, and Ramnik Arora.
\newblock Approximate kkt points and a proximity measure for termination.
\newblock \emph{Journal of Global Optimization}, 56\penalty0 (4):\penalty0 1463--1499, 2013.

\bibitem[Fang et~al.(2021)Fang, He, Long, and Su]{fang2021exploring}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0 (43):\penalty0 e2103091118, 2021.

\bibitem[Feng and Tu(2021)]{feng2021inverse}
Yu~Feng and Yuhai Tu.
\newblock The inverse variance--flatness relation in stochastic gradient descent is critical for finding flat minima.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0 (9), 2021.

\bibitem[Filippov(2013)]{filippov2013differential}
Aleksei~Fedorovich Filippov.
\newblock \emph{Differential equations with discontinuous righthand sides: control systems}, volume~18.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Han et~al.(2021)Han, Papyan, and Donoho]{han2021neural}
XY~Han, Vardan Papyan, and David~L Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the central path.
\newblock \emph{arXiv preprint arXiv:2106.02073}, 2021.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Jacot et~al.(2021)Jacot, Ged, {\c{S}}im{\c{s}}ek, Hongler, and Gabriel]{jacot2021saddle}
Arthur Jacot, Fran{\c{c}}ois Ged, Berfin {\c{S}}im{\c{s}}ek, Cl{\'e}ment Hongler, and Franck Gabriel.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity.
\newblock \emph{arXiv preprint arXiv:2106.15933}, 2021.

\bibitem[Jastrz{\k{e}}bski et~al.(2019)Jastrz{\k{e}}bski, Kenton, Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2018relation}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock On the relation between the sharpest directions of dnn loss and the sgd step length.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ji and Telgarsky(2019)]{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks.
\newblock \emph{arXiv preprint arXiv:1909.12292}, 2019.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 17176--17186, 2020.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Kunin et~al.(2023)Kunin, Yamamura, Ma, and Ganguli]{kunin2022asymmetric}
Daniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli.
\newblock The asymmetric maximum margin bias of quasi-homogeneous neural networks.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Li et~al.(2017)Li, Tai, and E]{li2017stochastic}
Qianxiao Li, Cheng Tai, and Weinan E.
\newblock Stochastic modified equations and adaptive stochastic gradient algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages 2101--2110. PMLR, 2017.

\bibitem[Li et~al.(2019)Li, Tai, and E]{li2019stochastic}
Qianxiao Li, Cheng Tai, and Weinan E.
\newblock Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0 (1):\penalty0 1474--1520, 2019.

\bibitem[Li et~al.(2021)Li, Wang, and Arora]{li2021happens}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after sgd reaches zero loss?--a mathematical framework.
\newblock \emph{arXiv preprint arXiv:2110.06914}, 2021.

\bibitem[Li et~al.(2022)Li, Wang, and Li]{li2022analyzing}
Zhouzi Li, Zixuan Wang, and Jian Li.
\newblock Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability.
\newblock \emph{arXiv preprint arXiv:2207.12678}, 2022.

\bibitem[Liu et~al.(2021)Liu, Ziyin, and Ueda]{liu2021noise}
Kangqiao Liu, Liu Ziyin, and Masahito Ueda.
\newblock Noise and fluctuation of finite learning rate stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 7045--7056. PMLR, 2021.

\bibitem[Luo et~al.(2021)Luo, Xu, Ma, and Zhang]{luo2021phase}
Tao Luo, Zhi-Qin~John Xu, Zheng Ma, and Yaoyu Zhang.
\newblock Phase diagram for two-layer relu neural networks at infinite-width limit.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 3327--3373, 2021.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ma and Ying(2021)]{ma2021linear}
Chao Ma and Lexing Ying.
\newblock On linear stability of sgd and input-smoothness of neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 16805--16817, 2021.

\bibitem[Ma et~al.(2022)Ma, Kunin, Wu, and Ying]{ma2022beyond}
Chao Ma, Daniel Kunin, Lei Wu, and Lexing Ying.
\newblock Beyond the quadratic approximation: the multiscale structure of neural network loss landscapes.
\newblock \emph{arXiv preprint arXiv:2204.11326}, 2022.

\bibitem[Maennel et~al.(2018)Maennel, Bousquet, and Gelly]{maennel2018gradient}
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly.
\newblock Gradient descent quantizes relu network features.
\newblock \emph{arXiv preprint arXiv:1803.08367}, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pages 2388--2464. PMLR, 2019.

\bibitem[Mulayoff et~al.(2021)Mulayoff, Michaeli, and Soudry]{mulayoff2021implicit}
Rotem Mulayoff, Tomer Michaeli, and Daniel Soudry.
\newblock The implicit bias of minima stability: A view from function space.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17749--17761, 2021.

\bibitem[Nacson et~al.(2019)Nacson, Gunasekar, Lee, Srebro, and Soudry]{nacson2019lexicographic}
Mor~Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry.
\newblock Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models.
\newblock In \emph{International Conference on Machine Learning}, pages 4683--4692. PMLR, 2019.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Kalimeris, Yang, Edelman, Zhang, and Barak]{nakkiran2019sgd}
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin~L Edelman, Fred Zhang, and Boaz Barak.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{arXiv preprint arXiv:1905.11604}, 2019.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (40):\penalty0 24652--24663, 2020.

\bibitem[Pesme and Flammarion(2023)]{pesme2023saddle}
Scott Pesme and Nicolas Flammarion.
\newblock Saddle-to-saddle dynamics in diagonal linear networks.
\newblock \emph{arXiv preprint arXiv:2304.00488}, 2023.

\bibitem[Phuong and Lampert(2021)]{phuong2021inductive}
Mary Phuong and Christoph~H Lampert.
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Rahaman et~al.(2019)Rahaman, Arpit, Draxler, Lin, Hamprecht, Bengio, and Courville]{rahaman2019spectral}
Aristide Rahaman, Nasim xd~Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 5301--5310. PMLR, 2019.

\bibitem[Safran et~al.(2022)Safran, Vardi, and Lee]{safran2022effective}
Itay Safran, Gal Vardi, and Jason~D Lee.
\newblock On the effective number of linear regions in shallow univariate relu networks: Convergence guarantees and implicit bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 32667--32679, 2022.

\bibitem[Saxe et~al.(2022)Saxe, Sodhani, and Lewallen]{saxe2022neural}
Andrew Saxe, Shagun Sodhani, and Sam~Jay Lewallen.
\newblock The neural race reduction: Dynamics of abstraction in gated networks.
\newblock In \emph{International Conference on Machine Learning}, pages 19287--19309. PMLR, 2022.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0 (1):\penalty0 2822--2878, 2018.

\bibitem[Thomas et~al.(2020)Thomas, Pedregosa, Merri{\"e}nboer, Manzagol, Bengio, and Le~Roux]{thomas2020interplay}
Valentin Thomas, Fabian Pedregosa, Bart Merri{\"e}nboer, Pierre-Antoine Manzagol, Yoshua Bengio, and Nicolas Le~Roux.
\newblock On the interplay between noise and curvature and its effect on optimization and generalization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3503--3513. PMLR, 2020.

\bibitem[Vardi(2023)]{vardi2023implicit}
Gal Vardi.
\newblock On the implicit bias in deep-learning algorithms.
\newblock \emph{Communications of the ACM}, 66\penalty0 (6):\penalty0 86--93, 2023.

\bibitem[Vardi et~al.(2022)Vardi, Shamir, and Srebro]{vardi2022margin}
Gal Vardi, Ohad Shamir, and Nati Srebro.
\newblock On margin maximization in linear and relu networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 37024--37036, 2022.

\bibitem[Wang and Ma(2022)]{wang2022early}
Mingze Wang and Chao Ma.
\newblock Early stage convergence and global convergence of training mildly parameterized neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wang and Wu(2023)]{wang2023noise}
Mingze Wang and Lei Wu.
\newblock The noise geometry of stochastic gradient descent: A quantitative and analytical characterization.
\newblock \emph{arXiv preprint arXiv:2310.00692}, 2023.

\bibitem[Wojtowytsch(2023)]{wojtowytsch2023stochastic}
Stephan Wojtowytsch.
\newblock Stochastic gradient descent with noise of machine learning type part i: Discrete time analysis.
\newblock \emph{Journal of Nonlinear Science}, 33\penalty0 (3):\penalty0 45, 2023.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese, Golan, Soudry, and Srebro]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, pages 3635--3673. PMLR, 2020.

\bibitem[Wu and Su(2023)]{wu2023implicitstability}
Lei Wu and Weijie~J Su.
\newblock The implicit regularization of dynamical stability in stochastic gradient descent.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 37656--37684. PMLR, 2023.

\bibitem[Wu et~al.(2018)Wu, Ma, and E]{wu2018sgd}
Lei Wu, Chao Ma, and Weinan E.
\newblock How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 31:\penalty0 8279--8288, 2018.

\bibitem[Wu et~al.(2022)Wu, Wang, and Su]{wu2022does}
Lei Wu, Mingze Wang, and Weijie Su.
\newblock When does sgd favor flat minima? a quantitative characterization via linear stability.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Xu et~al.(2019)Xu, Zhang, Luo, Xiao, and Ma]{xu2019frequency}
Zhi-Qin~John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma.
\newblock Frequency principle: Fourier analysis sheds light on deep neural networks.
\newblock \emph{arXiv preprint arXiv:1901.06523}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Li, Zhang, Luo, and John~Xu]{zhang2021embedding}
Yaoyu Zhang, Yuqing Li, Zhongwang Zhang, Tao Luo, and Zhi-Qin John~Xu.
\newblock Embedding principle: A hierarchical structure of loss landscape of deep neural networks.
\newblock \emph{Journal of Machine Learning}, 1\penalty0 (1):\penalty0 60--113, 2022.
\newblock ISSN 2790-2048.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Qixuan, Jin, Luo, Zhang, and Xu]{zhou2022empirical}
Hanxu Zhou, Zhou Qixuan, Zhenyuan Jin, Tao Luo, Yaoyu Zhang, and Zhi-Qin Xu.
\newblock Empirical phase diagram for three-layer neural networks with infinite width.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26021--26033, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Qixuan, Luo, Zhang, and Xu]{zhou2022towards}
Hanxu Zhou, Zhou Qixuan, Tao Luo, Yaoyu Zhang, and Zhi-Qin Xu.
\newblock Towards understanding the condensation of neural networks at initial training.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 2184--2196, 2022{\natexlab{b}}.

\bibitem[Zhu et~al.(2022)Zhu, Wang, Wang, Zhou, and Ge]{zhu2022understanding}
Xingyu Zhu, Zixuan Wang, Xiang Wang, Mo~Zhou, and Rong Ge.
\newblock Understanding edge-of-stability training dynamics with a minimalist example.
\newblock \emph{arXiv preprint arXiv:2210.03294}, 2022.

\bibitem[Zhu et~al.(2019)Zhu, Wu, Yu, Wu, and Ma]{zhu2019anisotropic}
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects.
\newblock In \emph{International Conference on Machine Learning}, pages 7654--7663. PMLR, 2019.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and Qu]{zhu2021geometric}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 29820--29834, 2021.

\bibitem[Ziyin et~al.(2022)Ziyin, Liu, Mori, and Ueda]{ziyin2021minibatch}
Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda.
\newblock Strength of minibatch noise in {SGD}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
