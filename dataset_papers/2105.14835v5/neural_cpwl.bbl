\providecommand\CheckAccent[1]{\accent20 #1}
\begin{thebibliography}{10}

\bibitem{abrahamsen2021training}
M.~Abrahamsen, L.~Kleist, and T.~Miltzow.
\newblock Training neural networks is {ER}-complete.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{alfarra2022decision}
M.~Alfarra, A.~Bibi, H.~Hammoud, M.~Gaafar, and B.~Ghanem.
\newblock On the decision boundaries of neural networks: A tropical geometry
  perspective.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2022.

\bibitem{alvarez2017machine}
A.~M. Alvarez, Q.~Louveaux, and L.~Wehenkel.
\newblock A machine learning-based approximation of strong branching.
\newblock {\em INFORMS Journal on Computing}, 29(1):185--195, 2017.

\bibitem{anderson2020strong}
R.~Anderson, J.~Huchette, W.~Ma, C.~Tjandraatmadja, and J.~P. Vielma.
\newblock Strong mixed-integer programming formulations for trained neural
  networks.
\newblock {\em Mathematical Programming}, pages 1--37, 2020.

\bibitem{anthony2009neural}
M.~Anthony and P.~L. Bartlett.
\newblock {\em Neural network learning: Theoretical foundations}.
\newblock Cambridge University Press, 1999.

\bibitem{Arora:DNNwithReLU}
R.~Arora, A.~Basu, P.~Mianjy, and A.~Mukherjee.
\newblock Understanding deep neural networks with rectified linear units.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{BagnaraHZ08SCP}
R.~Bagnara, P.~M. Hill, and E.~Zaffanella.
\newblock The {Parma Polyhedra Library}: Toward a complete set of numerical
  abstractions for the analysis and verification of hardware and software
  systems.
\newblock {\em Science of Computer Programming}, 72(1--2):3--21, 2008.

\bibitem{barron1993universal}
A.~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information theory}, 39(3):930--945, 1993.

\bibitem{barron1994approximation}
A.~R. Barron.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock {\em Machine learning}, 14(1):115--133, 1994.

\bibitem{bengio2020machine}
Y.~Bengio, A.~Lodi, and A.~Prouvost.
\newblock Machine learning for combinatorial optimization: a methodological
  tour d'horizon.
\newblock {\em European Journal of Operational Research}, 2020.

\bibitem{bertschinger2022training}
D.~Bertschinger, C.~Hertrich, P.~Jungeblut, T.~Miltzow, and S.~Weber.
\newblock Training fully connected neural networks is {ER}-complete.
\newblock {\em arXiv:2204.01368}, 2022.

\bibitem{bienstock2018principled}
D.~Bienstock, G.~Mu{\~n}oz, and S.~Pokutta.
\newblock Principled deep neural network training through linear programming.
\newblock {\em arXiv:1810.03218}, 2018.

\bibitem{bonami2018learning}
P.~Bonami, A.~Lodi, and G.~Zarpellon.
\newblock Learning a classification of mixed-integer quadratic programming
  problems.
\newblock In {\em International Conference on the Integration of Constraint
  Programming, Artificial Intelligence, and Operations Research}, pages
  595--604. Springer, 2018.

\bibitem{boob2022complexity}
D.~Boob, S.~S. Dey, and G.~Lan.
\newblock Complexity of training relu neural network.
\newblock {\em Discrete Optimization}, 44, 2022.

\bibitem{charisopoulos2018tropical}
V.~Charisopoulos and P.~Maragos.
\newblock A tropical approach to neural networks with piecewise linear
  activations.
\newblock {\em arXiv preprint arXiv:1805.08749}, 2018.

\bibitem{chen2022improved}
K.-L. Chen, H.~Garudadri, and B.~D. Rao.
\newblock Improved bounds on neural complexity for representing piecewise
  linear functions.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{Chen2022_LearningFTP}
S.~Chen, A.~R. Klivans, and R.~Meka.
\newblock {Learning Deep ReLU Networks Is Fixed-Parameter Tractable}.
\newblock In N.~K. Vishnoi, editor, {\em 2021 IEEE 62nd Annual Symposium on
  Foundations of Computer Science (FOCS)}, pages 696--707, 2022.

\bibitem{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314,
  1989.

\bibitem{dey2020approximation}
S.~S. Dey, G.~Wang, and Y.~Xie.
\newblock Approximation algorithms for training one-node relu neural networks.
\newblock {\em IEEE Transactions on Signal Processing}, 68:6696--6706, 2020.

\bibitem{edelsbrunner1987algorithms}
H.~Edelsbrunner.
\newblock {\em Algorithms in Combinatorial Geometry}.
\newblock Springer Science \& Business Media, 1987.

\bibitem{eldan2016power}
R.~Eldan and O.~Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In {\em Conference on Learning Theory}, pages 907--940, 2016.

\bibitem{fischetti2017deep}
M.~Fischetti and J.~Jo.
\newblock Deep neural networks as 0-1 mixed integer linear programs: A
  feasibility study.
\newblock {\em arXiv preprint arXiv:1712.06174}, 2017.

\bibitem{froese2022computational}
V.~Froese, C.~Hertrich, and R.~Niedermeier.
\newblock The computational complexity of {ReLU} network training parameterized
  by data dimensionality.
\newblock {\em Journal of Artificial Intelligence Research}, 74:1775--1790,
  2022.

\bibitem{gasse2019exact}
M.~Gasse, D.~Ch{\'e}telat, N.~Ferroni, L.~Charlin, and A.~Lodi.
\newblock Exact combinatorial optimization with graph convolutional neural
  networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{goel2017reliably}
S.~Goel, V.~Kanade, A.~Klivans, and J.~Thaler.
\newblock Reliably learning the relu in polynomial time.
\newblock In {\em Conference on Learning Theory}, pages 1004--1042. PMLR, 2017.

\bibitem{goel2018learning}
S.~Goel, A.~Klivans, and R.~Meka.
\newblock Learning one convolutional layer with overlapping patches.
\newblock In {\em International Conference on Machine Learning}, pages
  1783--1791. PMLR, 2018.

\bibitem{goel2019learning}
S.~Goel and A.~R. Klivans.
\newblock Learning neural networks with two nonlinear layers in polynomial
  time.
\newblock In {\em Conference on Learning Theory}, pages 1470--1499. PMLR, 2019.

\bibitem{GKMR21}
S.~Goel, A.~R. Klivans, P.~Manurangsi, and D.~Reichman.
\newblock Tight hardness results for training depth-2 {ReLU} networks.
\newblock In {\em 12th Innovations in Theoretical Computer Science Conference
  (ITCS~'21)}, volume 185 of {\em LIPIcs}, pages 22:1--22:14. Schloss Dagstuhl
  - Leibniz-Zentrum f{\"{u}}r Informatik, 2021.

\bibitem{gribonval2021approximation}
R.~Gribonval, G.~Kutyniok, M.~Nielsen, and F.~Voigtlaender.
\newblock Approximation spaces of deep neural networks.
\newblock {\em Constructive Approximation}, pages 1--109, 2021.

\bibitem{gurobi}
{Gurobi Optimization, LLC}.
\newblock Gurobi optimizer reference manual, 2021.

\bibitem{haase2023lower}
C.~A. Haase, C.~Hertrich, and G.~Loho.
\newblock Lower bounds on the depth of integral {ReLU} neural networks via
  lattice polytopes.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{hanin2019universal}
B.~Hanin.
\newblock Universal function approximation by deep neural nets with bounded
  width and {ReLU} activations.
\newblock {\em Mathematics}, 7(10):992, 2019.

\bibitem{hanin2017approximating}
B.~Hanin and M.~Sellke.
\newblock Approximating continuous functions by {ReLU} nets of minimal width.
\newblock {\em arXiv:1710.11278}, 2017.

\bibitem{he2014learning}
H.~He, H.~Daume~III, and J.~M. Eisner.
\newblock Learning to search in branch and bound algorithms.
\newblock {\em Advances in neural information processing systems},
  27:3293--3301, 2014.

\bibitem{he2020relu}
J.~He, L.~Li, J.~Xu, and C.~Zheng.
\newblock Relu deep neural networks and linear finite elements.
\newblock {\em Journal of Computational Mathematics}, 38(3):502--527, 2020.

\bibitem{maxflowPaper}
C.~Hertrich and L.~Sering.
\newblock {ReLU} neural networks of polynomial size for exact maximum flow
  computation.
\newblock In {\em International Conference on Integer Programming and
  Combinatorial Optimization}, 2023.

\bibitem{knapsackPaper}
C.~Hertrich and M.~Skutella.
\newblock Provably good solutions to the knapsack problem via neural networks
  of bounded size.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2021.

\bibitem{lemarechal1996convex}
J.-B. Hiriart-Urruty and C.~Lemar{\'e}chal.
\newblock {\em Convex analysis and minimization algorithms {I}}, volume 305 of
  {\em Grundlehren der mathematischen Wissenschaften}.
\newblock Springer-Verlag, Berlin, 1993.

\bibitem{HiriartUrrutyLemarechal93b}
J.-B. Hiriart-Urruty and C.~Lemar{\'e}chal.
\newblock {\em Convex {A}nalysis and {M}inimization {A}lgorithms {II}}, volume
  306 of {\em Grundlehren der mathematischen Wissenschaften}.
\newblock Springer-Verlag, Berlin, 1993.

\bibitem{hornik1991approximation}
K.~Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural networks}, 4(2):251--257, 1991.

\bibitem{ETC}
M.~Joswig.
\newblock {\em Essentials of tropical combinatorics}.
\newblock Graduate Studies in Mathematics. American Mathematical Society,
  Providence, RI, 2022.
\newblock To appear.

\bibitem{khalife2022neural}
S.~Khalife and A.~Basu.
\newblock Neural networks with linear threshold activations: structure and
  algorithms.
\newblock In {\em International Conference on Integer Programming and
  Combinatorial Optimization}, pages 347--360. Springer, 2022.

\bibitem{khalil2016learning}
E.~Khalil, P.~Le~Bodic, L.~Song, G.~Nemhauser, and B.~Dilkina.
\newblock Learning to branch in mixed integer programming.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem{khalil2017learning}
E.~B. Khalil, B.~Dilkina, G.~L. Nemhauser, S.~Ahmed, and Y.~Shao.
\newblock Learning to run heuristics in tree search.
\newblock In {\em IJCAI}, pages 659--666, 2017.

\bibitem{kruber2017learning}
M.~Kruber, M.~E. L{\"u}bbecke, and A.~Parmentier.
\newblock Learning when to use a decomposition.
\newblock In {\em International Conference on AI and OR Techniques in
  Constraint Programming for Combinatorial Optimization Problems}, pages
  202--210. Springer, 2017.

\bibitem{liang2017deep}
S.~Liang and R.~Srikant.
\newblock Why deep neural networks for function approximation?
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{lodi2017learning}
A.~Lodi and G.~Zarpellon.
\newblock On learning and branching: a survey.
\newblock {\em {TOP}}, 25(2):207--236, 2017.

\bibitem{lu2021note}
Z.~Lu.
\newblock A note on the representation power of {GHH}s.
\newblock {\em arXiv:2101.11286}, 2021.

\bibitem{maclagan2015introduction}
D.~Maclagan and B.~Sturmfels.
\newblock {\em Introduction to tropical geometry}, volume 161 of {\em Graduate
  Studies in Mathematics}.
\newblock American Mathematical Soc., 2015.

\bibitem{maragos2021tropical}
P.~Maragos, V.~Charisopoulos, and E.~Theodosis.
\newblock Tropical geometry and machine learning.
\newblock {\em Proceedings of the IEEE}, 109(5):728--755, 2021.

\bibitem{mhaskar1993approximation}
H.~Mhaskar.
\newblock Approximation of real functions using neural networks.
\newblock In {\em Proc. Intl. Conf. Comp. Math., New Delhi, India, World
  Scientific Press}, pages 267--278. World Scientific, 1993.

\bibitem{mhaskar1996neural}
H.~N. Mhaskar.
\newblock Neural networks for optimal approximation of smooth and analytic
  functions.
\newblock {\em Neural computation}, 8(1):164--177, 1996.

\bibitem{mhaskar1995degree}
H.~N. Mhaskar and C.~A. Micchelli.
\newblock Degree of approximation by neural and translation networks with a
  single hidden layer.
\newblock {\em Advances in applied mathematics}, 16(2):151--183, 1995.

\bibitem{montufar2022sharp}
G.~Mont{\'u}far, Y.~Ren, and L.~Zhang.
\newblock Sharp bounds for the number of regions of maxout networks and
  vertices of minkowski sums.
\newblock {\em SIAM Journal on Applied Algebra and Geometry}, 6(4):618--649,
  2022.

\bibitem{montufar2014regions}
G.~F. Mont{\'u}far, R.~Pascanu, K.~Cho, and Y.~Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems
  27}, pages 2924--2932. 2014.

\bibitem{mukherjee2017lower}
A.~Mukherjee and A.~Basu.
\newblock Lower bounds over boolean inputs for deep neural networks with {ReLU}
  gates.
\newblock {\em arXiv:1711.03073}, 2017.

\bibitem{nguyen2018neural}
Q.~Nguyen, M.~C. Mukkamala, and M.~Hein.
\newblock Neural networks should be wide enough to learn disconnected decision
  regions.
\newblock In {\em International Conference on Machine Learning}, pages
  3737--3746, 2018.

\bibitem{virtual}
G.~Y. Panina and I.~Stre\u{\i}nu.
\newblock Virtual polytopes.
\newblock {\em Uspekhi Mat. Nauk}, 70(6(426)):139--202, 2015.

\bibitem{pascanu2014number}
R.~Pascanu, G.~Mont{\'u}far, and Y.~Bengio.
\newblock On the number of inference regions of deep feed forward networks with
  piece-wise linear activations.
\newblock In {\em International Conference on Learning Representations}, 2014.

\bibitem{pinkus1999approximation}
A.~Pinkus.
\newblock Approximation theory of the mlp model.
\newblock {\em Acta Numerica 1999: Volume 8}, 8:143--195, 1999.

\bibitem{raghu2017expressive}
M.~Raghu, B.~Poole, J.~Kleinberg, S.~Ganguli, and J.~S. Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2847--2854, 2017.

\bibitem{rosenblatt1958perceptron}
F.~Rosenblatt.
\newblock The perceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological review}, 65(6):386, 1958.

\bibitem{safran2017depth}
I.~Safran and O.~Shamir.
\newblock Depth-width tradeoffs in approximating natural functions with neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2979--2987, 2017.

\bibitem{sch}
A.~Schrijver.
\newblock {\em Theory of Linear and Integer Programming}.
\newblock John Wiley and Sons, New York, 1986.

\bibitem{serra2020lossless}
T.~Serra, A.~Kumar, and S.~Ramalingam.
\newblock Lossless compression of deep neural networks.
\newblock In {\em International Conference on Integration of Constraint
  Programming, Artificial Intelligence, and Operations Research}, pages
  417--430. Springer, 2020.

\bibitem{serra2020empirical}
T.~Serra and S.~Ramalingam.
\newblock Empirical bounds on linear regions of deep rectifier networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5628--5635, 2020.

\bibitem{serra2018bounding}
T.~Serra, C.~Tjandraatmadja, and S.~Ramalingam.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  4565--4573, 2018.

\bibitem{stanley2004introduction}
R.~P. Stanley.
\newblock An introduction to hyperplane arrangements.
\newblock In {\em Lecture notes, IAS/Park City Mathematics Institute}, 2004.

\bibitem{Telgarsky15}
M.~Telgarsky.
\newblock Representation benefits of deep feedforward networks.
\newblock {\em arXiv:1509.08101}, 2015.

\bibitem{telgarsky2016benefits}
M.~Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In {\em Conference on Learning Theory}, pages 1517--1539, 2016.

\bibitem{sagemath}
{The Sage Developers}.
\newblock {\em {S}ageMath, the {S}age {M}athematics {S}oftware {S}ystem
  ({V}ersion 9.0)}, 2020.
\newblock {\tt https://www.sagemath.org}.

\bibitem{vardi2021size}
G.~Vardi, D.~Reichman, T.~Pitassi, and O.~Shamir.
\newblock Size and depth separation in approximating benign functions with
  neural networks.
\newblock In {\em Conference on Learning Theory}, pages 4195--4223. PMLR, 2021.

\bibitem{wang2004general}
S.~Wang.
\newblock General constructive representations for continuous piecewise-linear
  functions.
\newblock {\em IEEE Transactions on Circuits and Systems I: Regular Papers},
  51(9):1889--1896, 2004.

\bibitem{wang2005generalization}
S.~Wang and X.~Sun.
\newblock Generalization of hinging hyperplanes.
\newblock {\em IEEE Transactions on Information Theory}, 51(12):4425--4431,
  2005.

\bibitem{yarotsky2017error}
D.~Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\bibitem{Zhang:Tropical}
L.~Zhang, G.~Naitzat, and L.-H. Lim.
\newblock Tropical geometry of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  5819--5827, 2018.

\end{thebibliography}
