\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
  Heess, and Martin Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock \emph{arXiv preprint arXiv:1806.06920}, 2018.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Brenna~D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Beliaev et~al.(2022)Beliaev, Shih, Ermon, Sadigh, and
  Pedarsani]{beliaev2022imitation}
Mark Beliaev, Andy Shih, Stefano Ermon, Dorsa Sadigh, and Ramtin Pedarsani.
\newblock Imitation learning by estimating expertise of demonstrators.
\newblock \emph{arXiv preprint arXiv:2202.01288}, 2022.

\bibitem[Brys et~al.(2015)Brys, Harutyunyan, Suay, Chernova, Taylor, and
  Now{\'e}]{brys2015reinforcement}
Tim Brys, Anna Harutyunyan, Halit~Bener Suay, Sonia Chernova, Matthew~E Taylor,
  and Ann Now{\'e}.
\newblock Reinforcement learning from demonstration through shaping.
\newblock In \emph{Twenty-fourth international joint conference on artificial
  intelligence}, 2015.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cao et~al.(2022)Cao, Wang, and Sadigh]{cao2022learning}
Zhangjie Cao, Zihan Wang, and Dorsa Sadigh.
\newblock Learning from imperfect demonstrations via adversarial confidence
  transfer.
\newblock \emph{arXiv preprint arXiv:2202.02967}, 2022.

\bibitem[Eysenbach et~al.(2017)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach2017leave}
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.06782}, 2017.

\bibitem[Eysenbach et~al.(2020)Eysenbach, Asawa, Chaudhari, Levine, and
  Salakhutdinov]{eysenbach2020off}
Benjamin Eysenbach, Swapnil Asawa, Shreyas Chaudhari, Sergey Levine, and Ruslan
  Salakhutdinov.
\newblock Off-dynamics reinforcement learning: Training for transfer with
  domain classifiers.
\newblock \emph{arXiv preprint arXiv:2006.13916}, 2020.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Chelsea Finn, Sergey Levine, and Pieter Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pages 49--58.
  PMLR, 2016.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1920--1930. PMLR, 2019.

\bibitem[Fu et~al.(2017)Fu, Luo, and Levine]{fu2017learning}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1710.11248}, 2017.

\bibitem[Fu et~al.(2019)Fu, Kumar, Soh, and Levine]{fu2019diagnosing}
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine.
\newblock Diagnosing bottlenecks in deep q-learning algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  2021--2030. PMLR, 2019.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and
  Gu]{ghasemipour2020divergence}
Seyed Kamyar~Seyed Ghasemipour, Richard Zemel, and Shixiang Gu.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, pages 1259--1277. PMLR, 2020.

\bibitem[Gupta et~al.(2019)Gupta, Kumar, Lynch, Levine, and
  Hausman]{gupta2019relay}
Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.11956}, 2019.

\bibitem[Gupta et~al.(2021)Gupta, Yu, Zhao, Kumar, Rovinsky, Xu, Devlin, and
  Levine]{gupta2021reset}
Abhishek Gupta, Justin Yu, Tony~Z Zhao, Vikash Kumar, Aaron Rovinsky, Kelvin
  Xu, Thomas Devlin, and Sergey Levine.
\newblock Reset-free reinforcement learning via multi-task learning: Learning
  dexterous manipulation behaviors without human intervention.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 6664--6671. IEEE, 2021.

\bibitem[Gupta et~al.(2022)Gupta, Lynch, Kinman, Peake, Levine, and
  Hausman]{gupta2022bootstrapped}
Abhishek Gupta, Corey Lynch, Brandon Kinman, Garrett Peake, Sergey Levine, and
  Karol Hausman.
\newblock Bootstrapped autonomous practicing via multi-task reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2203.15755}, 2022.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Han et~al.(2015)Han, Levine, and Abbeel]{han2015learning}
Weiqiao Han, Sergey Levine, and Pieter Abbeel.
\newblock Learning compound multi-step controllers under unknown dynamics.
\newblock In \emph{2015 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 6435--6442. IEEE, 2015.

\bibitem[Hansen et~al.(2020)Hansen, Jangir, Sun, Aleny{\`a}, Abbeel, Efros,
  Pinto, and Wang]{hansen2020self}
Nicklas Hansen, Rishabh Jangir, Yu~Sun, Guillem Aleny{\`a}, Pieter Abbeel,
  Alexei~A Efros, Lerrel Pinto, and Xiaolong Wang.
\newblock Self-supervised policy adaptation during deployment.
\newblock \emph{arXiv preprint arXiv:2007.04309}, 2020.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Khetarpal et~al.(2020)Khetarpal, Riemer, Rish, and
  Precup]{khetarpal2020towards}
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup.
\newblock Towards continual reinforcement learning: A review and perspectives.
\newblock \emph{arXiv preprint arXiv:2012.13490}, 2020.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21810--21823, 2020.

\bibitem[Kim et~al.(2022)Kim, hyeon Park, Cho, and Kim]{kim2022automating}
Jigang Kim, J~hyeon Park, Daesol Cho, and H~Jin Kim.
\newblock Automating reinforcement learning with example-based resets.
\newblock \emph{IEEE Robotics and Automation Letters}, 2022.

\bibitem[Kostrikov et~al.(2018)Kostrikov, Agrawal, Dwibedi, Levine, and
  Tompson]{kostrikov2018discriminator}
Ilya Kostrikov, Kumar~Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and
  Jonathan Tompson.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1809.02925}, 2018.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lomonaco et~al.(2020)Lomonaco, Desai, Culurciello, and
  Maltoni]{Lomonaco_2020_CVPR_Workshops}
Vincenzo Lomonaco, Karan Desai, Eugenio Culurciello, and Davide Maltoni.
\newblock Continual reinforcement learning in 3d non-stationary environments.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR) Workshops}, June 2020.

\bibitem[Mahadevan(1996)]{mahadevan1996average}
Sridhar Mahadevan.
\newblock Average reward reinforcement learning: Foundations, algorithms, and
  empirical results.
\newblock \emph{Machine learning}, 22\penalty0 (1):\penalty0 159--195, 1996.

\bibitem[Mehta et~al.(2020)Mehta, Diaz, Golemo, Pal, and
  Paull]{mehta2020active}
Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher~J Pal, and Liam Paull.
\newblock Active domain randomization.
\newblock In \emph{Conference on Robot Learning}, pages 1162--1176. PMLR, 2020.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Clavera, Liu, Fearing, Abbeel,
  Levine, and Finn]{nagabandi2018learning}
Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald~S Fearing, Pieter Abbeel,
  Sergey Levine, and Chelsea Finn.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11347}, 2018.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter
  Abbeel.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pages 6292--6299. IEEE, 2018.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Andrew~Y Ng, Stuart~J Russell, et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, page~2, 2000.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Peng et~al.(2018)Peng, Andrychowicz, Zaremba, and Abbeel]{peng2018sim}
Xue~Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel.
\newblock Sim-to-real transfer of robotic control with dynamics randomization.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pages 3803--3810. IEEE, 2018.

\bibitem[Peters et~al.(2010)Peters, Mulling, and Altun]{peters2010relative}
Jan Peters, Katharina Mulling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In \emph{Twenty-Fourth AAAI Conference on Artificial Intelligence},
  2010.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{rajeswaran2017learning}
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John
  Schulman, Emanuel Todorov, and Sergey Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Rolnick et~al.(2019)Rolnick, Ahuja, Schwarz, Lillicrap, and
  Wayne]{NEURIPS2019_fa7cdfad}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf}.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Sadeghi and Levine(2016)]{sadeghi2016cad2rl}
Fereshteh Sadeghi and Sergey Levine.
\newblock Cad2rl: Real single-image flight without a single real image.
\newblock \emph{arXiv preprint arXiv:1611.04201}, 2016.

\bibitem[Schwartz(1993)]{schwartz1993reinforcement}
Anton Schwartz.
\newblock A reinforcement learning method for maximizing undiscounted rewards.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, volume 298, pages 298--305, 1993.

\bibitem[Sharma et~al.(2021{\natexlab{a}})Sharma, Gupta, Levine, Hausman, and
  Finn]{sharma2021subgoal}
Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
\newblock Autonomous reinforcement learning via subgoal curricula.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 18474--18486. Curran Associates, Inc., 2021{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/99c83c904d0d64fbef50d919a5c66a80-Paper.pdf}.

\bibitem[Sharma et~al.(2021{\natexlab{b}})Sharma, Xu, Sardana, Gupta, Hausman,
  Levine, and Finn]{sharma2021autonomous}
Archit Sharma, Kelvin Xu, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey
  Levine, and Chelsea Finn.
\newblock Autonomous reinforcement learning: Formalism and benchmarking.
\newblock \emph{arXiv preprint arXiv:2112.09605}, 2021{\natexlab{b}}.

\bibitem[Sharma et~al.(2022)Sharma, Ahmad, and Finn]{sharma2022state}
Archit Sharma, Rehaan Ahmad, and Chelsea Finn.
\newblock A state-distribution matching approach to non-episodic reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2205.05212}, 2022.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019end}
Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock \emph{arXiv preprint arXiv:1904.07854}, 2019.

\bibitem[Sun and Ma(2019)]{sun2019adversarial}
Mingfei Sun and Xiaojuan Ma.
\newblock Adversarial imitation learning from incomplete demonstrations.
\newblock \emph{arXiv preprint arXiv:1905.12310}, 2019.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Syed et~al.(2008)Syed, Bowling, and Schapire]{syed2008apprenticeship}
Umar Syed, Michael Bowling, and Robert~E Schapire.
\newblock Apprenticeship learning using linear programming.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1032--1039, 2008.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{tobin2017domain}
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
  Pieter Abbeel.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{2017 IEEE/RSJ international conference on intelligent robots
  and systems (IROS)}, pages 23--30. IEEE, 2017.

\bibitem[Torabi et~al.(2019)Torabi, Warnell, and Stone]{torabi2019adversarial}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Adversarial imitation learning from state-only demonstrations.
\newblock In \emph{Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 2229--2231, 2019.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot,
  Heess, Roth{\"o}rl, Lampe, and Riedmiller]{vecerik2017leveraging}
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal
  Piot, Nicolas Heess, Thomas Roth{\"o}rl, Thomas Lampe, and Martin Riedmiller.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Wang et~al.(2018)Wang, Xiong, Han, Liu, Zhang,
  et~al.]{wang2018exponentially}
Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et~al.
\newblock Exponentially weighted imitation learning for batched historical
  data.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Ciliberto, Amadori, and
  Demiris]{wang2020support}
Ruohan Wang, Carlo Ciliberto, Pierluigi Amadori, and Yiannis Demiris.
\newblock Support-weighted adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:2002.08803}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Xu, and Du]{wang2021robust}
Yunke Wang, Chang Xu, and Bo~Du.
\newblock Robust adversarial imitation learning via adaptively-selected
  demonstrations.
\newblock In Zhi-Hua Zhou, editor, \emph{Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence, {IJCAI-21}}, pages
  3155--3161. International Joint Conferences on Artificial Intelligence
  Organization, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Xu, Du, and
  Lee]{wang2021learning}
Yunke Wang, Chang Xu, Bo~Du, and Honglak Lee.
\newblock Learning to weight imperfect demonstrations.
\newblock In \emph{International Conference on Machine Learning}, pages
  10961--10970. PMLR, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Novikov, Zolna, Merel,
  Springenberg, Reed, Shahriari, Siegel, Gulcehre, Heess,
  et~al.]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh~S Merel, Jost~Tobias
  Springenberg, Scott~E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7768--7778, 2020{\natexlab{b}}.

\bibitem[Wei et~al.(2020)Wei, Jahromi, Luo, Sharma, and Jain]{pmlr-v119-wei20c}
Chen-Yu Wei, Mehdi~Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul
  Jain.
\newblock Model-free reinforcement learning in infinite-horizon average-reward
  {M}arkov decision processes.
\newblock In Hal~Daum√© III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10170--10180. PMLR,
  13--18 Jul 2020.

\bibitem[Wu et~al.(2019{\natexlab{a}})Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019{\natexlab{a}}.

\bibitem[Wu et~al.(2019{\natexlab{b}})Wu, Charoenphakdee, Bao, Tangkaratt, and
  Sugiyama]{wu2019imitation}
Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi
  Sugiyama.
\newblock Imitation learning from imperfect demonstration.
\newblock In \emph{International Conference on Machine Learning}, pages
  6818--6827. PMLR, 2019{\natexlab{b}}.

\bibitem[Xie and Finn(2021)]{xie2021lifelong}
Annie Xie and Chelsea Finn.
\newblock Lifelong robotic reinforcement learning by retaining experiences.
\newblock \emph{arXiv preprint arXiv:2109.09180}, 2021.

\bibitem[Xie et~al.(2020)Xie, Harrison, and Finn]{xie2020deep}
Annie Xie, James Harrison, and Chelsea Finn.
\newblock Deep reinforcement learning amidst lifelong non-stationarity.
\newblock \emph{arXiv preprint arXiv:2006.10701}, 2020.

\bibitem[Xu and Zhu(2018)]{xu2018reinforced}
Ju~Xu and Zhanxing Zhu.
\newblock Reinforced continual learning.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf}.

\bibitem[Yoneda et~al.(2021)Yoneda, Yang, Walter, and
  Stadie]{yoneda2021invariance}
Takuma Yoneda, Ge~Yang, Matthew~R Walter, and Bradly Stadie.
\newblock Invariance through inference.
\newblock \emph{arXiv preprint arXiv:2112.08526}, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhu et~al.(2020{\natexlab{a}})Zhu, Yu, Gupta, Shah, Hartikainen,
  Singh, Kumar, and Levine]{zhu2020ingredients}
Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi
  Singh, Vikash Kumar, and Sergey Levine.
\newblock The ingredients of real-world robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.12570}, 2020{\natexlab{a}}.

\bibitem[Zhu et~al.(2020{\natexlab{b}})Zhu, Lin, Dai, and Zhou]{zhu2020off}
Zhuangdi Zhu, Kaixiang Lin, Bo~Dai, and Jiayu Zhou.
\newblock Off-policy imitation learning from observations.
\newblock In \emph{the Thirty-fourth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2020)}, 2020{\natexlab{b}}.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010modeling}
Brian~D Ziebart, J~Andrew Bagnell, and Anind~K Dey.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{ICML}, 2010.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2019varibad}
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin
  Gal, Katja Hofmann, and Shimon Whiteson.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1910.08348}, 2019.

\end{thebibliography}
