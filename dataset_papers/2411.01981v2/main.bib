@article{tang2024mind,
  title={Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models}, 
  author={Tang, Longxiang and Tian, Zhuotao and Li, Kai and He, Chunming and Zhou, Hantao and Zhao, Hengshuang and Li, Xiu and Jia, Jiaya},
  journal={arXiv preprint arXiv:2407.05342},
  year={2024}
}

@inproceedings{peng2024oacnns,
  title={OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation},
  author={Peng, Bohao and Wu, Xiaoyang and Jiang, Li and Chen, Yukang and Zhao, Hengshuang and Tian, Zhuotao and Jia, Jiaya},
  booktitle={CVPR},
  year={2024}
}

@article{tian2020pfenet,
  title={Prior Guided Feature Enrichment Network for Few-Shot Segmentation},
  author={Tian, Zhuotao and Zhao, Hengshuang and Shu, Michelle and Yang, Zhicheng and Li, Ruiyu and Jia, Jiaya},
  journal={TPAMI},
  year={2020}
}

@InProceedings{peng2023hierarchical,
  title={Hierarchical Dense Correlation Distillation for Few-Shot Segmentation},
  author={Peng, Bohao and Tian, Zhuotao and Wu, Xiaoyang and Wang, Chenyao and Liu, Shu and Su, Jingyong and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023}
}

@InProceedings{tian2022gfsseg,
    title={Generalized Few-shot Semantic Segmentation},
    author={Zhuotao Tian and Xin Lai and Li Jiang and Shu Liu and Michelle Shu and Hengshuang Zhao and Jiaya Jia},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}

@article{lai2023lisa,
  title={LISA: Reasoning Segmentation via Large Language Model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  journal={arXiv preprint arXiv:2308.00692},
  year={2023}
}
@article{yang2023improved,
  title={An Improved Baseline for Reasoning Segmentation with Large Language Model},
  author={Yang, Senqiao and Qu, Tianyuan and Lai, Xin and Tian, Zhuotao and Peng, Bohao and Liu, Shu and Jia, Jiaya},
  journal={arXiv preprint arXiv:2312.17240},
  year={2023}
}

@InProceedings{Tian_2019_CVPR,
author = {Tian, Zhuotao and Shu, Michelle and Lyu, Pengyuan and Li, Ruiyu and Zhou, Chao and Shen, Xiaoyong and Jia, Jiaya},
title = {Learning Shape-Aware Embedding for Scene Text Detection},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@ARTICLE{luo2023pfenet++,
  author={Luo, Xiaoliu and Tian, Zhuotao and Zhang, Taiping and Yu, Bei and Tang, Yuan Yan and Jia, Jiaya},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={PFENet++: Boosting Few-Shot Semantic Segmentation With the Noise-Filtered Context-Aware Prior Mask}, 
  year={2024},
  volume={46},
  number={2},
  pages={1273-1289},
  doi={10.1109/TPAMI.2023.3329725}
}

@InProceedings{tian2023cac,
    title={Learning Context-aware Classifier for Semantic Segmentation},
    author={Zhuotao Tian and Jiequan Cui and Li Jiang and Xiaojuan Qi and Xin Lai and Yixin Chen and Shu Liu and Jiaya Jia},
    booktitle={Proceedings of the Thirty-Seventh {AAAI} Conference on Artificial Intelligence},
    year={2023}
}

@article{mahalanobis,
  title={The mahalanobis distance},
  author={De Maesschalck, Roy and Jouan-Rimbaud, Delphine and Massart, D{\'e}sir{\'e} L},
  journal={Chemometrics and intelligent laboratory systems},
  volume={50},
  number={1},
  pages={1--18},
  year={2000},
  publisher={Elsevier}
}
@article{gradnorm,
  title={On the importance of gradients for detecting distributional shifts in the wild},
  author={Huang, Rui and Geng, Andrew and Li, Yixuan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={677--689},
  year={2021}
}
@inproceedings{textures,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3606--3613},
  year={2014}
}
@inproceedings{wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International conference on machine learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}
@inproceedings{moon2020confidence,
  title={Confidence-aware learning for deep neural networks},
  author={Moon, Jooyoung and Kim, Jihyo and Shin, Younghak and Hwang, Sangheum},
  booktitle={international conference on machine learning},
  pages={7034--7044},
  year={2020},
  organization={PMLR}
}
@inproceedings{sirc,
  title={Augmenting softmax information for selective classification with out-of-distribution data},
  author={Xia, Guoxuan and Bouganis, Christos-Savvas},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={1995--2012},
  year={2022}
}
@article{yang2023lidar,
  title={Lidar-llm: Exploring the potential of large language models for 3d lidar understanding},
  author={Yang, Senqiao and Liu, Jiaming and Zhang, Ray and Pan, Mingjie and Guo, Zoey and Li, Xiaoqi and Chen, Zehui and Gao, Peng and Guo, Yandong and Zhang, Shanghang},
  journal={arXiv preprint arXiv:2312.14074},
  year={2023}
}
@article{narasimhan2023plugin,
  title={Plugin estimators for selective classification with out-of-distribution detection},
  author={Narasimhan, Harikrishna and Menon, Aditya Krishna and Jitkrittum, Wittawat and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2301.12386},
  year={2023}
}

@article{kim2023unified,
  title={A unified benchmark for the unknown detection capability of deep neural networks},
  author={Kim, Jihyo and Koo, Jiin and Hwang, Sangheum},
  journal={Expert Systems with Applications},
  volume={229},
  pages={120461},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{tang2024cores,
  title={CORES: Convolutional Response-based Score for Out-of-distribution Detection},
  author={Tang, Keke and Hou, Chao and Peng, Weilong and Chen, Runnan and Zhu, Peican and Wang, Wenping and Tian, Zhihong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10916--10925},
  year={2024}
}
@article{yuksekgonul2024beyond,
  title={Beyond confidence: Reliable models should also consider atypicality},
  author={Yuksekgonul, Mert and Zhang, Linjun and Zou, James Y and Guestrin, Carlos},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yang2024unified,
  title={Unified Language-driven Zero-shot Domain Adaptation},
  author={Yang, Senqiao and Tian, Zhuotao and Jiang, Li and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23407--23415},
  year={2024}
}

@ARTICLE{9556483,
  author={MacÃªdo, David and Ren, Tsang Ing and Zanchettin, Cleber and Oliveira, Adriano L. I. and Ludermir, Teresa},
  journal={IEEE Transactions on Neural Networks and Learning Systems (TNNLS)}, 
  title={Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples}, 
  year={2022},
  volume={33},
  number={6},
  pages={2350-2364},
  keywords={Entropy;Training;Neural networks;Feature extraction;Data preprocessing;Measurement;Prototypes;Entropic score;isotropy maximization loss;maximum entropy principle;out-of-distribution (OOD) detection},
  doi={10.1109/TNNLS.2021.3112897}}
@ARTICLE{10244211,
  author={Hong, Jie and Fang, Pengfei and Li, Weihao and Han, Junlin and Petersson, Lars and Harandi, Mehrtash},
  journal={IEEE Transactions on Neural Networks and Learning Systems (TNNLS)}, 
  title={Curved Geometric Networks for Visual Anomaly Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-14},
  keywords={Geometry;Visualization;Task analysis;Manifolds;Australia;Training;Object recognition;Anomaly recognition;geometric learning;hyperbolic space;mixed-curvature space;open-set recognition;out-of-distribution (OOD) detection;spherical space},
  doi={10.1109/TNNLS.2023.3309846}}

@ARTICLE{9777854,
  author={Liu, Zihuan and Bhattacharya, Shrijita and Maiti, Tapabrata},
  journal={IEEE Transactions on Neural Networks and Learning Systems (TNNLS)}, 
  title={Variational Bayes Ensemble Learning Neural Networks With Compressed Feature Space}, 
  year={2024},
  volume={35},
  number={1},
  pages={1379-1385},
  keywords={Computational modeling;Bayes methods;Artificial neural networks;Uncertainty;Training;Predictive models;Numerical models;Intrinsic dimensionality;model averaging;random compression;variational inference (VI)},
  doi={10.1109/TNNLS.2022.3172276}}

@inproceedings{ece,
  title={Obtaining well calibrated probabilities using bayesian binning},
  author={Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence (AAAI)},
  volume={29},
  year={2015}
}

@inproceedings{CRL,
	title = {Confidence-{Aware} {Learning} for {Deep} {Neural} {Networks}},
	language = {en},
	urldate = {2024-05-19},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning} (ICML)},
	publisher = {PMLR},
	author = {Moon, Jooyoung and Kim, Jihyo and Shin, Younghak and Hwang, Sangheum},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7034--7044}
}

@article{history,
  title={Learning under concept drift: A review},
  author={Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
  journal={IEEE Transactions on Knowledge and Data Engineering (TKDE)},
  volume={31},
  number={12},
  pages={2346--2363},
  year={2018},
  publisher={IEEE}
}
@inproceedings{
	a_call,
	title={A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification},
	author={Paul F Jaeger and Carsten Tim L{\"u}th and Lukas Klein and Till J. Bungert},
	booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
	year={2023}
}
@article{michaelis2019dragon,
  title={Benchmarking Robustness in Object Detection: 
    Autonomous Driving when Winter is Coming},
  author={Michaelis, Claudio and Mitzkus, Benjamin and 
    Geirhos, Robert and Rusak, Evgenia and 
    Bringmann, Oliver and Ecker, Alexander S. and 
    Bethge, Matthias and Brendel, Wieland},
  journal={arXiv preprint arXiv:1907.07484},
  year={2019}
}
@inproceedings{what_can,
	title={What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?},
	author={Galil, Ido and Dabbah, Mohammed and El-Yaniv, Ran},
	booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
	year={2023}
}
@article{corbiere2019addressing,
	title={Addressing failure prediction by learning model confidence},
	author={Corbi{\`e}re, Charles and Thome, Nicolas and Bar-Hen, Avner and Cord, Matthieu and P{\'e}rez, Patrick},
	journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	volume={32},
	year={2019}
}
@article{baseline,
	title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
	author={Hendrycks, Dan and Gimpel, Kevin},
	journal={arXiv preprint arXiv:1610.02136},
	year={2016}
}
@article{malinin2018predictive,
	title={Predictive uncertainty estimation via prior networks},
	author={Malinin, Andrey and Gales, Mark},
	journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	volume={31},
	year={2018}
}
@inproceedings{cos,
	title={Decoupling MaxLogit for Out-of-Distribution Detection},
	author={Zhang, Zihan and Xiang, Xiang},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages={3388--3397},
	year={2023}
}

@inproceedings{corr,
	title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
	author={Hendrycks, Dan and Dietterich, Thomas},
	booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
	year={2019}
}

@inproceedings{SML,
	title={Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation},
	author={Jung, Sanghun and Lee, Jungsoo and Gwak, Daehoon and Choi, Sungha and Choo, Jaegul},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	pages={15425--15434},
	year={2021}
}
@misc{kahya_mcrood_2023,
	title = {{MCROOD}: {Multi}-{Class} {Radar} {Out}-{Of}-{Distribution} {Detection}},
	shorttitle = {{MCROOD}},
	url = {http://arxiv.org/abs/2303.06232},
	abstract = {Out-of-distribution (OOD) detection has recently received special attention due to its critical role in safely deploying modern deep learning (DL) architectures. This work proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. We also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The simple idea is called respiration detector (RESPD) and eases the OOD detection, especially for human sitting and standing classes. On our dataset collected by 60GHz short-range FMCW Radar, we achieve AUROCs of 97.45\%, 92.13\%, and 96.58\% for sitting, standing, and walking classes, respectively. We perform extensive experiments and show that our method outperforms state-of-the-art (SOTA) OOD detection methods. Also, our pipeline performs 24 times faster than the second-best method and is very suitable for real-time processing.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Kahya, Sabri Mustafa and Yavuz, Muhammet Sami and Steinbach, Eckehard},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06232 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: Accepted at ICASSP 2023},
	file = {arXiv.org Snapshot:C\:\\Users\\dell\\Zotero\\storage\\XK58IZBM\\2303.html:text/html;Full Text PDF:C\:\\Users\\dell\\Zotero\\storage\\DCILD9GV\\Kahya ç­ - 2023 - MCROOD Multi-Class Radar Out-Of-Distribution Dete.pdf:application/pdf},
}

@InProceedings{resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}
@ARTICLE{rfnet,
  author={Sun, Lei and Yang, Kailun and Hu, Xinxin and Hu, Weijian and Wang, Kaiwei},
  journal={IEEE Robotics and Automation Letters (RA-L)}, 
  title={Real-Time Fusion Network for RGB-D Semantic Segmentation Incorporating Unexpected Obstacle Detection for Road-Driving Images}, 
  year={2020},
  volume={5},
  number={4},
  pages={5558-5565},
  doi={10.1109/LRA.2020.3007457}}
@article{confidence1,
  title={Towards robust pattern recognition: A review},
  author={Zhang, Xu-Yao and Liu, Cheng-Lin and Suen, Ching Y},
  journal={Proceedings of the IEEE (P-IEEE)},
  volume={108},
  number={6},
  pages={894--922},
  year={2020},
  publisher={IEEE}
}
@inproceedings{cs-kd,
  title={Regularizing class-wise predictions via self-knowledge distillation},
  author={Yun, Sukmin and Park, Jongjin and Lee, Kimin and Shin, Jinwoo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={13876--13885},
  year={2020}
}
@inproceedings{focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages={2980--2988},
  year={2017}
}
@article{ls,
  title={When does label smoothing help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019}
}
@article{mixup,
  title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
  author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
  journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019}
}



@article{background2,
  title={Computer vision for autonomous vehicles: Problems, datasets and state of the art},
  author={Janai, Joel and G{\"u}ney, Fatma and Behl, Aseem and Geiger, Andreas and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision (FOUND TRENDS COMPUT)},
  volume={12},
  number={1--3},
  pages={1--308},
  year={2020},
  publisher={Now Publishers, Inc.}
}
@article{background1,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={Nature},
  volume={542},
  number={7639},
  pages={115--118},
  year={2017},
  publisher={Nature Publishing Group}
}
@inproceedings{zaeemzadeh_out--distribution_2021,
	address = {Nashville, TN, USA},
	title = {Out-of-{Distribution} {Detection} {Using} {Union} of 1-{Dimensional} {Subspaces}},
	isbn = {978-1-66544-509-2},
	doi = {10.1109/CVPR46437.2021.00933},
	abstract = {The goal of out-of-distribution (OOD) detection is to handle the situations where the test samples are drawn from a different distribution than the training data. In this paper, we argue that OOD samples can be detected more easily if the training data is embedded into a low-dimensional space, such that the embedded training samples lie on a union of 1-dimensional subspaces. We show that such embedding of the in-distribution (ID) samples provides us with two main advantages. First, due to compact representation in the feature space, OOD samples are less likely to occupy the same region as the known classes. Second, the ï¬rst singular vector of ID samples belonging to a 1-dimensional subspace can be used as their robust representative. Motivated by these observations, we train a deep neural network such that the ID samples are embedded onto a union of 1-dimensional subspaces. At the test time, employing sampling techniques used for approximate Bayesian inference in deep learning, input samples are detected as OOD if they occupy the region corresponding to the ID samples with probability 0. Spectral components of the ID samples are used as robust representative of this region. Our method does not have any hyperparameter to be tuned using extra information and it can be applied on different modalities with minimal change. The effectiveness of the proposed method is demonstrated on different benchmark datasets, both in the image and video classiï¬cation domains.},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zaeemzadeh, Alireza and Bisagno, Niccolo and Sambugaro, Zeno and Conci, Nicola and Rahnavard, Nazanin and Shah, Mubarak},
	month = jun,
	year = {2021},
	pages = {9447--9456},
	file = {Zaeemzadeh ç­ - 2021 - Out-of-Distribution Detection Using Union of 1-Dim.pdf:C\:\\Users\\dell\\Zotero\\storage\\8EXBL6IW\\Zaeemzadeh ç­ - 2021 - Out-of-Distribution Detection Using Union of 1-Dim.pdf:application/pdf},
}

@article{line,
	title = {{LINe}: {Out}-of-{Distribution} {Detection} by {Leveraging} {Important} {Neurons}},
	shorttitle = {{LINe}},
	journal = {arXiv preprint arXiv:2303.13995},
	author = {Ahn, Yong Hyun and Park, Gyeong-Moon and Kim, Seong Tae},
	year = {2023},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\IY59C5F4\\Ahn ç­ - 2023 - LINe Out-of-Distribution Detection by Leveraging .pdf:application/pdf;Snapshot:C\:\\Users\\dell\\Zotero\\storage\\LR3AQ3E4\\2303.html:text/html},
}
@inproceedings{pebal,
	title={Pixel-wise energy-biased abstention learning for anomaly segmentation on complex urban driving scenes},
	author={Tian, Yu and Liu, Yuyuan and Pang, Guansong and Liu, Fengbei and Chen, Yuanhong and Carneiro, Gustavo},
	booktitle={Proceedings of the Europeon Conference on Computer Vision (ECCV)},
	pages={246--263},
	year={2022},
	organization={Springer}
}
@inproceedings{densenet,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	url = {https://ieeexplore.ieee.org/document/8099726/},
	doi = {10.1109/CVPR.2017.243},
	abstract = {densenet},
	language = {en},
	urldate = {2024-03-06},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	pages = {2261--2269},
	file = {Huang ç­ - 2017 - Densely Connected Convolutional Networks.pdf:D\:\\Yijun\\latex\\Zotero\\storage\\2TEYGB45\\Huang ç­ - 2017 - Densely Connected Convolutional Networks.pdf:application/pdf},
}
@incollection{res110,
	address = {Cham},
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_38},
	abstract = {resnet110},
	language = {en},
	urldate = {2024-03-06},
	booktitle = {Proceedings of the Europeon Conference on Computer Vision (ECCV)},
	publisher = {Springer International Publishing},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	doi = {10.1007/978-3-319-46493-0_38},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {630--645},
	file = {He ç­ - 2016 - Identity Mappings in Deep Residual Networks.pdf:D\:\\Yijun\\latex\\Zotero\\storage\\FTQN6JS8\\He ç­ - 2016 - Identity Mappings in Deep Residual Networks.pdf:application/pdf},
}
@inproceedings{wrn,
	address = {York, France},
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {WideResNet,wrn},
	language = {en},
	urldate = {2024-03-06},
    booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},
	publisher = {British Machine Vision Association},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jan,
	year = {2016},
	note = {arXiv:1605.07146 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Zagoruyko å Komodakis - 2017 - Wide Residual Networks.pdf:D\:\\Yijun\\latex\\Zotero\\storage\\A9LG3MBA\\Zagoruyko å Komodakis - 2017 - Wide Residual Networks.pdf:application/pdf},
}
@inproceedings{energy,
	title = {Energy-based {Out}-of-distribution {Detection}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f5496252609c43eb8a3d147ab9b9c006-Abstract.html},
	abstract = {energy},
	urldate = {2024-03-06},
	booktitle = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
	year = {2020},
	keywords = {â No DOI found},
	pages = {21464--21475},
	file = {Full Text PDF:D\:\\Yijun\\latex\\Zotero\\storage\\XVV5QMNU\\Liu ç­ - 2020 - Energy-based Out-of-distribution Detection.pdf:application/pdf},
}
@inproceedings{wang2022partial,
	title={Partial and asymmetric contrastive learning for out-of-distribution detection in long-tailed recognition},
	author={Wang, Haotao and Zhang, Aston and Zhu, Yi and Zheng, Shuai and Li, Mu and Smola, Alex J and Wang, Zhangyang},
	booktitle={Proceedings of the {International} {Conference} on {Machine} {Learning} (ICML)},
	pages={23446--23458},
	year={2022},
	organization={PMLR}
}
@article{ppt5,
	author       = {Matej Grcic and
	Petra Bevandic and
	Zoran Kalafatic and
	Sinisa Segvic},
	title        = {Dense anomaly detection by robust learning on synthetic negative data},
	journal      = {CoRR},
	volume       = {abs/2112.12833},
	year         = {2021},
	url          = {https://arxiv.org/abs/2112.12833},
	eprinttype    = {arXiv},
	eprint       = {2112.12833},
	timestamp    = {Tue, 04 Jan 2022 15:59:27 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2112-12833.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{li2022asymmetric,
	title={Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again},
	author={Li, Xin-Chun and Fan, Wen-Shu and Song, Shaoming and Li, Yinchuan and Yunfeng, Shao and Zhan, De-Chuan and others},
	journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	volume={35},
	pages={3830--3842},
	year={2022}
}
@inproceedings{lu_uncertainty-aware_2023,
	title = {Uncertainty-{Aware} {Optimal} {Transport} for {Semantically} {Coherent} {Out}-of-{Distribution} {Detection}},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Lu, Fan and Zhu, Kai and Zhai, Wei and Zheng, Kecheng and Cao, Yang},
	year = {2023},
	pages = {3282--3291},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\TR6YACH5\\Lu ç­ - 2023 - Uncertainty-Aware Optimal Transport for Semantical.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\Zotero\\storage\\ZX84VMJH\\Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_.html:text/html},
}

@article{bitterwolf_or_2023,
	title = {In or {Out}? {Fixing} {ImageNet} {Out}-of-{Distribution} {Detection} {Evaluation}},
	shorttitle = {In or {Out}?},
	journal = {arXiv preprint arXiv:2306.00826},
	author = {Bitterwolf, Julian and MÃ¼ller, Maximilian and Hein, Matthias},
	year = {2023},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\TCMCX3QJ\\Bitterwolf ç­ - 2023 - In or Out Fixing ImageNet Out-of-Distribution Det.pdf:application/pdf},
}


@article{liu_deep_2019,
	title = {Deep gamblers: {Learning} to abstain with portfolio theory},
	volume = {32},
	shorttitle = {Deep gamblers},
	journal = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	author = {Liu, Ziyin and Wang, Zhikang and Liang, Paul Pu and Salakhutdinov, Russ R. and Morency, Louis-Philippe and Ueda, Masahito},
	year = {2019},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\677BXG58\\Liu ç­ - 2019 - Deep gamblers Learning to abstain with portfolio .pdf:application/pdf;Snapshot:C\:\\Users\\dell\\Zotero\\storage\\YJABR3BV\\0c4b1eeb45c90b52bfb9d07943d855ab-Abstract.html:text/html},
}

@inproceedings{geifman_selectivenet_2019,
	title = {Selectivenet: {A} deep neural network with an integrated reject option},
	shorttitle = {Selectivenet},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning} (ICML)},
	publisher = {PMLR},
	author = {Geifman, Yonatan and El-Yaniv, Ran},
	year = {2019},
	pages = {2151--2159},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\PVIC6YWG\\Geifman å El-Yaniv - 2019 - Selectivenet A deep neural network with an integr.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\Zotero\\storage\\LG672A2Z\\geifman19a.html:text/html},
}

@article{geifman_selective_2017,
	title = {Selective classification for deep neural networks},
	volume = {30},
	journal = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	author = {Geifman, Yonatan and El-Yaniv, Ran},
	year = {2017},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\BUJMTL4K\\Geifman å El-Yaniv - 2017 - Selective classification for deep neural networks.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\Zotero\\storage\\2FZP7EZL\\4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html:text/html},
}

@article{ovadia_can_2019,
	title = {Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
	volume = {32},
	shorttitle = {Can you trust your model's uncertainty?},
	journal = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
	year = {2019},
}
@inproceedings{feng2022towards,
	title={Towards Better Selective Classification},
	author={Feng, Leo and Ahmed, Mohamed Osama and Hajimirsadeghi, Hossein and Abdi, Amir H},
	booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
	year={2022}
}
@article{kendall_what_2017,
	title = {What uncertainties do we need in bayesian deep learning for computer vision?},
	volume = {30},
	journal = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	author = {Kendall, Alex and Gal, Yarin},
	year = {2017},
	file = {Full Text:C\:\\Users\\dell\\Zotero\\storage\\GHZ2MXRD\\Kendall å Gal - 2017 - What uncertainties do we need in bayesian deep lea.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\Zotero\\storage\\Y2LASX2Y\\2650d6089a6d640c5e85b2b88265dc2b-Abstract.html:text/html},
}

@inproceedings{logitNorm,
	title = {Mitigating {Neural} {Network} {Overconfidence} with {Logit} {Normalization}},
	abstract = {Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)âa simple fix to the cross-entropy lossâby enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of outputâs norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30\% on common benchmarks.},
	language = {en},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning} (ICML)},
	publisher = {PMLR},
	author = {Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {unseen},
	pages = {23631--23644},
	file = {Full Text PDF:C\:\\Users\\dell\\Zotero\\storage\\WEYMZTJU\\Wei ç­ - 2022 - Mitigating Neural Network Overconfidence with Logi.pdf:application/pdf},
}

@misc{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	doi = {10.48550/arXiv.1506.02142},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = oct,
	year = {2016},
	note = {arXiv:1506.02142 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 6 figures; fixed a mistake with standard error and added a new table with updated results (marked "Update [October 2016]"); Published in ICML 2016},
	file = {arXiv Fulltext PDF:C\:\\Users\\dell\\Zotero\\storage\\X8YKIQ9N\\Gal å Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\dell\\Zotero\\storage\\577W78BU\\1506.html:text/html},
}

@inproceedings{openmix,
  title={OpenMix: Exploring Outlier Samples for Misclassification Detection},
  author={Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12074--12083},
  year={2023}
}

@inproceedings{fmfp,
	title={Rethinking confidence calibration for failure prediction},
	author={Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
	booktitle={Proceedings of the Europeon Conference on Computer Vision (ECCV)},
	pages={518--536},
	year={2022},
	organization={Springer}
}
@article{65thulasidasan2019mixup,
	title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
	author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
	journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	volume={32},
	year={2019}
}
@article{51muller2019does,
	title={When does label smoothing help?},
	author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
	journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	volume={32},
	year={2019}
}
@article{50mukhoti2020calibrating,
	title={Calibrating deep neural networks using focal loss},
	author={Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip and Dokania, Puneet},
	journal={Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
	volume={33},
	pages={15288--15299},
	year={2020}
}
@article{zhu2022learning,
	title={Learning by seeing more classes},
	author={Zhu, Fei and Zhang, Xu-Yao and Wang, Rui-Qi and Liu, Cheng-Lin},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
	volume={45},
	number={6},
	pages={7477--7493},
	year={2022},
	publisher={IEEE}
}


@article{LostAndFound,
	author       = {Hermann Blum and
	Paul{-}Edouard Sarlin and
	Juan I. Nieto and
	Roland Siegwart and
	Cesar Cadena},
	title        = {The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation},
	journal      = {International Journal of Computer Vision (IJCV)},
	volume       = {129},
	number       = {11},
	pages        = {3119--3135},
	year         = {2021},
	url          = {https://doi.org/10.1007/s11263-021-01511-6},
	doi          = {10.1007/s11263-021-01511-6},
	timestamp    = {Tue, 28 Feb 2023 10:48:09 +0100},
	biburl       = {https://dblp.org/rec/journals/ijcv/BlumSNSC21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{RoadAnomaly,
	author       = {Krzysztof Lis and
	Krishna Kanth Nakka and
	Pascal Fua and
	Mathieu Salzmann},
	title        = {Detecting the Unexpected via Image Resynthesis},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
	pages        = {2152--2161},
	publisher    = {{IEEE}},
	year         = {2019},
	url          = {https://doi.org/10.1109/ICCV.2019.00224},
	doi          = {10.1109/ICCV.2019.00224},
	timestamp    = {Sun, 25 Oct 2020 23:16:02 +0100},
	biburl       = {https://dblp.org/rec/conf/iccv/LisNFS19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{auroc,
	title={The use of the area under the ROC curve in the evaluation of machine learning algorithms},
	author={Bradley, Andrew P},
	journal={Pattern Recognition (PR)},
	volume={30},
	number={7},
	pages={1145--1159},
	year={1997},
	publisher={Elsevier}
}
@inproceedings{aupr,
  title={Area under the precision-recall curve: point estimates and confidence intervals},
  author={Boyd, Kendrick and Eng, Kevin H and Page, C David},
  booktitle={Proceedings of the  European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)},
  pages={451--466},
  year={2013},
  organization={Springer}
}

@article{eaurc,
	title={Bias-reduced uncertainty estimation for deep neural classifiers},
	author={Geifman, Yonatan and Uziel, Guy and El-Yaniv, Ran},
	journal={arXiv preprint arXiv:1805.08206},
	year={2018}
}
@article{aurc,
	title={On the Foundations of Noise-free Selective Classification.},
	author={El-Yaniv, Ran and others},
	journal={Journal of Machine Learning Research (JMLR)},
	volume={11},
	number={5},
	year={2010}
}
@inproceedings{Kitti,
	title={Are we ready for autonomous driving? the kitti vision benchmark suite},
	author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages={3354--3361},
	year={2012},
	organization={IEEE}
}
@article{glcm,
	title={Image texture classification using gray level co-occurrence matrix based statistical features},
	author={Suresh, Annamalai and Shunmuganathan, KL and others},
	journal={European Journal of Scientific Research (EJSR)},
	volume={75},
	number={4},
	pages={591--597},
	year={2012}
}
@inproceedings{mobilenetv32019,
	title={Searching for MobileNetV3},
	author={Andrew Howard and Mark Sandler and Grace Chu and Liang-Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam},
	booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
	year={2019}
}
@inproceedings{cheng2021mask2former,
	title={Masked-attention Mask Transformer for Universal Image Segmentation},
	author={Bowen Cheng and Ishan Misra and Alexander G. Schwing and Alexander Kirillov and Rohit Girdhar},
	journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2022}
}
@inproceedings{cordts2016cityscapes,
	title={The cityscapes dataset for semantic urban scene understanding},
	author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages={3213--3223},
	year={2016}
}
@article{mnist,
	title={Gradient-based learning applied to document recognition},
	author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	journal={Proceedings of the IEEE (P-IEEE)},
	volume={86},
	number={11},
	pages={2278--2324},
	year={1998},
	publisher={Ieee}
}
@article{svhn,
	title={Multi-digit number recognition from street view imagery using deep convolutional neural networks},
	author={Goodfellow, Ian J and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
	journal={arXiv preprint arXiv:1312.6082},
	year={2013}
}

@article{Krizhevsky09,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, A. and Hinton, G.},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009},
  publisher={Citeseer}
}


@article{deeplabv3,
	title={Rethinking atrous convolution for semantic image segmentation},
	author={Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	journal={arXiv preprint arXiv:1706.05587},
	year={2017}
}

@inproceedings{deeplabv3plus2018,
	title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
	author={Liang-Chieh Chen and Yukun Zhu and George Papandreou and Florian Schroff and Hartwig Adam},
	booktitle={Proceedings of the Europeon Conference on Computer Vision (ECCV)},
	year={2018}
}

@misc{deit,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {http://arxiv.org/abs/2012.12877},
	abstract = {deit-small},
	language = {en},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and JÃ©gou, HervÃ©},
	month = jan,
	year = {2021},
	note = {arXiv:2012.12877 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Touvron ç­ - 2021 - Training data-efficient image transformers & disti.pdf:D\:\\Yijun\\latex\\Zotero\\storage\\BBISFIZE\\Touvron ç­ - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf},
}

@misc{SAM,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {http://arxiv.org/abs/2010.01412},
	abstract = {SAM, In todayâs heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a minmax optimization problem on which gradient descent can be performed efï¬ciently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-\{10, 100\}, ImageNet, ï¬netuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we ï¬nd that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that speciï¬cally target learning with noisy labels. We open source our code at https: //github.com/google-research/sam.},
	language = {en},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	month = apr,
	year = {2021},
	note = {arXiv:2010.01412 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2010.pdf:D\:\\Yijun\\latex\\Zotero\\storage\\ES4929HS\\2010.pdf:application/pdf},
}

@article{SWA,
  title = {Averaging {Weights} {Leads} to {Wider} {Optima} and {Better} {Generalization}},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}


@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{umask,
  title={Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability},
  author={Zhu, Jianing and Li, Hengzhuang and Yao, Jiangchao and Liu, Tongliang and Xu, Jianliang and Han, Bo},
  booktitle={Proceedings of the {International} {Conference} on {Machine} {Learning} (ICML)},
  pages={43068--43104},
  year={2023},
  organization={PMLR}
}


@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
