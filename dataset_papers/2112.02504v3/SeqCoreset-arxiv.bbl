\begin{thebibliography}{10}

\bibitem{DBLP:conf/approx/AvronCW17}
Haim Avron, Kenneth~L. Clarkson, and David~P. Woodruff.
\newblock Sharper bounds for regularized data fitting.
\newblock In {\em Approximation, Randomization, and Combinatorial Optimization.
  Algorithms and Techniques, {APPROX/RANDOM} 2017}, volume~81, pages
  27:1--27:22, 2017.

\bibitem{baraniuk2006johnson}
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin.
\newblock The johnson-lindenstrauss lemma meets compressed sensing.
\newblock {\em preprint}, 100(1):1--9, 2006.

\bibitem{FISTA}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM Journal on Imaging Sciences}, 2(1):183--202, 2009.

\bibitem{subgradient}
Dimitri~P. Bertsekas.
\newblock {\em Convex Optimization Algorithms}.
\newblock Athena Scientific Belmont, MA, 2015.

\bibitem{bishop2006pattern}
Christopher~M Bishop.
\newblock {\em Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem{DBLP:conf/nips/BorsosM020}
Zal{\'{a}}n Borsos, Mojmir Mutny, and Andreas Krause.
\newblock Coresets via bilevel optimization for continual learning and
  streaming.
\newblock In {\em Advances in Neural Information Processing Systems, NeurIPS},
  2020.

\bibitem{DBLP:journals/siamrev/BottouCN18}
L{\'{e}}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em {SIAM} Rev.}, 60(2):223--311, 2018.

\bibitem{DBLP:conf/nips/CampbellB19}
Trevor Campbell and Boyan Beronov.
\newblock Sparse variational inference: Bayesian coresets from scratch.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 11457--11468, 2019.

\bibitem{DBLP:conf/icml/CampbellB18}
Trevor Campbell and Tamara Broderick.
\newblock Bayesian coreset construction via greedy iterative geodesic ascent.
\newblock In Jennifer~G. Dy and Andreas Krause, editors, {\em Proceedings of
  the 35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  {\em Proceedings of Machine Learning Research}, pages 697--705. {PMLR}, 2018.

\bibitem{candanedo2017data}
Luis~M Candanedo, V{\'e}ronique Feldheim, and Dominique Deramaix.
\newblock Data driven prediction models of energy use of appliances in a
  low-energy house.
\newblock {\em Energy and buildings}, 140:81--97, 2017.

\bibitem{chen2009coresets}
Ke~Chen.
\newblock On coresets for k-median and k-means clustering in metric and
  euclidean spaces and their applications.
\newblock {\em SIAM Journal on Computing}, 39(3):923--947, 2009.

\bibitem{DBLP:conf/icml/Chhaya0S20}
Rachit Chhaya, Anirban Dasgupta, and Supratim Shit.
\newblock On coresets for regularized regression.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML}}, volume 119, pages 1866--1876, 2020.

\bibitem{DBLP:conf/icml/ChowdhuryYD18}
Agniva Chowdhury, Jiasen Yang, and Petros Drineas.
\newblock An iterative, sketching-based framework for ridge regression.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML}}, volume~80, pages 988--997, 2018.

\bibitem{DBLP:conf/iclr/ColemanYMMBLLZ20}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
  Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR}}. OpenReview.net, 2020.

\bibitem{CRAMER2004613}
J.~S. Cramer.
\newblock The early origins of the logit model.
\newblock {\em Studies in History and Philosophy of Science Part C: Studies in
  History and Philosophy of Biological and Biomedical Sciences}, 35(4):613 --
  626, 2004.

\bibitem{gdcurry}
Haskell~B. Curry.
\newblock The method of steepest descent for non-linear minimization problems.
\newblock {\em Quart. Appl. Math.}, 2:258--261, 1944.

\bibitem{dasgupta2009sampling}
Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael~W
  Mahoney.
\newblock Sampling algorithms and coresets for $\ell_p$ regression.
\newblock {\em SIAM Journal on Computing}, 38(5):2060--2078, 2009.

\bibitem{DBLP:conf/icml/DingW20}
Hu~Ding and Zixiu Wang.
\newblock Layered sampling for robust optimization problems.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML}}, volume 119, pages 2556--2566, 2020.

\bibitem{drineas2006sampling}
Petros Drineas, Michael~W Mahoney, and Shan Muthukrishnan.
\newblock Sampling algorithms for $l_2$ regression and applications.
\newblock In {\em Proceedings of the 17th annual ACM-SIAM symposium on Discrete
  algorithms}, pages 1127--1136, 2006.

\bibitem{DBLP:journals/jmlr/DuchiHS11}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em J. Mach. Learn. Res.}, 12:2121--2159, 2011.

\bibitem{DBLP:journals/widm/Feldman20}
Dan Feldman.
\newblock Core-sets: An updated survey.
\newblock {\em Wiley Interdiscip. Rev. Data Min. Knowl. Discov.}, 10(1), 2020.

\bibitem{DBLP:conf/stoc/FeldmanL11}
Dan Feldman and Michael Langberg.
\newblock A unified framework for approximating and clustering data.
\newblock In {\em Proceedings of the 43rd {ACM} Symposium on Theory of
  Computing, {STOC}}, pages 569--578, 2011.

\bibitem{G85}
Teofilo~F Gonzalez.
\newblock Clustering to minimize the maximum intercluster distance.
\newblock {\em Theoretical Computer Science}, 38:293--306, 1985.

\bibitem{hoeffding1994probability}
Wassily Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock In {\em The Collected Works of Wassily Hoeffding}, pages 409--426.
  Springer, 1994.

\bibitem{huang2018epsilon}
Lingxiao Huang, Shaofeng Jiang, Jian Li, and Xuan Wu.
\newblock Epsilon-coresets for clustering (with outliers) in doubling metrics.
\newblock In {\em IEEE 59th Annual Symposium on Foundations of Computer Science
  (FOCS)}, pages 814--825, 2018.

\bibitem{huggins2016coresets}
Jonathan Huggins, Trevor Campbell, and Tamara Broderick.
\newblock Coresets for scalable bayesian logistic regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4080--4088, 2016.

\bibitem{DBLP:conf/aistats/KachamW20}
Praneeth Kacham and David~P. Woodruff.
\newblock Optimal deterministic coresets for ridge regression.
\newblock In {\em The 23rd International Conference on Artificial Intelligence
  and Statistics, {AISTATS}}, volume 108, pages 4141--4150, 2020.

\bibitem{DBLP:journals/corr/KingmaB14}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em 3rd International Conference on Learning Representations,
  {ICLR}}, 2015.

\bibitem{langberg2010universal}
Michael Langberg and Leonard~J Schulman.
\newblock Universal $\varepsilon$-approximators for integrals.
\newblock In {\em Proceedings of the twenty-first annual ACM-SIAM symposium on
  Discrete Algorithms}, pages 598--607. SIAM, 2010.

\bibitem{DBLP:conf/aaai/LeeLAN06}
Su{-}In Lee, Honglak Lee, Pieter Abbeel, and Andrew~Y. Ng.
\newblock Efficient {L1} regularized logistic regression.
\newblock In {\em Proceedings, The 21st National Conference on Artificial
  Intelligence and the 18th Innovative Applications of Artificial Intelligence
  Conference}, pages 401--408. {AAAI} Press, 2006.

\bibitem{li2001improved}
Yi~Li, Philip~M Long, and Aravind Srinivasan.
\newblock Improved bounds on the sample complexity of learning.
\newblock {\em Journal of Computer and System Sciences}, 62(3):516--527, 2001.

\bibitem{lucic2017training}
Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman.
\newblock Training {G}aussian mixture models at scale via coresets.
\newblock {\em The Journal of Machine Learning Research}, 18(1):5885--5909,
  2017.

\bibitem{JMLR:v18:15-506}
Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman.
\newblock Training gaussian mixture models at scale via coresets.
\newblock {\em Journal of Machine Learning Research}, 18(160):1--25, 2018.

\bibitem{DBLP:conf/nips/MaaloufJF19}
Alaa Maalouf, Ibrahim Jubran, and Dan Feldman.
\newblock Fast and accurate least-mean-squares solvers.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  NeurIPS}, pages 8305--8316, 2019.

\bibitem{DBLP:books/daglib/0021593}
Christopher~D. Manning, Prabhakar Raghavan, and Hinrich Sch{\"{u}}tze.
\newblock {\em Introduction to information retrieval}.
\newblock Cambridge University Press, 2008.

\bibitem{DBLP:conf/icml/MirzasoleimanBL20}
Baharan Mirzasoleiman, Jeff~A. Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML}}, volume 119, pages 6950--6960, 2020.

\bibitem{DBLP:conf/nips/MirzasoleimanCL20}
Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec.
\newblock Coresets for robust training of deep neural networks against noisy
  labels.
\newblock In {\em Annual Conference on Neural Information Processing Systems
  2020, NeurIPS}, 2020.

\bibitem{DBLP:conf/pkdd/MosciRSVV10}
Sofia Mosci, Lorenzo Rosasco, Matteo Santoro, Alessandro Verri, and Silvia
  Villa.
\newblock Solving structured sparsity regularization with proximal methods.
\newblock In {\em European Conference on Machine Learning and Knowledge
  Discovery in Databases {ECML} {PKDD}}, volume 6322, pages 418--433, 2010.

\bibitem{munteanu2018coresets}
Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff.
\newblock On coresets for logistic regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6561--6570, 2018.

\bibitem{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate {O}(1/$k^2$).
\newblock {\em Soviet Mathematics Doklady}, 27(2):372--376, 1983.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{DBLP:journals/corr/Phillips16}
Jeff~M. Phillips.
\newblock Coresets and sketches.
\newblock {\em Computing Research Repository}, 2016.

\bibitem{DBLP:conf/aistats/RajMM20}
Anant Raj, Cameron Musco, and Lester Mackey.
\newblock Importance sampling via local sensitivity.
\newblock In Silvia Chiappa and Roberto Calandra, editors, {\em The 23rd
  International Conference on Artificial Intelligence and Statistics, {AISTATS}
  2020}, volume 108 of {\em Proceedings of Machine Learning Research}, pages
  3099--3109. {PMLR}, 2020.

\bibitem{DBLP:conf/uai/ReddiPS15}
Sashank~J. Reddi, Barnab{\'{a}}s P{\'{o}}czos, and Alexander~J. Smola.
\newblock Communication efficient coresets for empirical loss minimization.
\newblock In Marina Meila and Tom Heskes, editors, {\em Proceedings of the
  Thirty-First Conference on Uncertainty in Artificial Intelligence, {UAI}
  2015}, pages 752--761. {AUAI} Press, 2015.

\bibitem{DBLP:conf/aistats/SamadianPMIC20}
Alireza Samadian, Kirk Pruhs, Benjamin Moseley, Sungjin Im, and Ryan~R. Curtin.
\newblock Unconditional coresets for regularized loss minimization.
\newblock In {\em The 23rd International Conference on Artificial Intelligence
  and Statistics, {AISTATS}}, volume 108, pages 482--492, 2020.

\bibitem{DBLP:conf/iclr/SenerS18}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: {A} core-set
  approach.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR}}. OpenReview.net, 2018.

\bibitem{Sing1503:Comment}
Kamaljot Singh, Ranjeet~Kaur Sandhu, and Dinesh Kumar.
\newblock Comment volume prediction using neural networks and decision trees.
\newblock In {\em IEEE UKSim-AMSS 17th International Conference on Computer
  Modelling and Simulation, UKSim2015 (UKSim2015)}, Cambridge, United Kingdom,
  mar 2015.

\bibitem{10.2307/2346178}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, 58(1):267--288, 1996.

\bibitem{ridge}
Andrey Tikhonov.
\newblock Nonlinear ill-posed problems.
\newblock {\em Applied Mathematical Sciences}, 1998.

\bibitem{DBLP:conf/nips/TukanMF20}
Murad Tukan, Alaa Maalouf, and Dan Feldman.
\newblock Coresets for near-convex functions.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  NeurIPS}, 2020.

\bibitem{DBLP:conf/nips/Vapnik91}
Vladimir Vapnik.
\newblock Principles of risk minimization for learning theory.
\newblock In {\em Advances in Neural Information Processing Systems 4,
  {[NIPS}}, pages 831--838, 1991.

\bibitem{acsent}
Philip Wolfe.
\newblock Convergence conditions for ascent methods.
\newblock {\em {SIAM} Rev.}, 11(2):226–235, 1969.

\bibitem{DBLP:journals/combinatorica/Wolsey82}
Laurence~A. Wolsey.
\newblock An analysis of the greedy algorithm for the submodular set covering
  problem.
\newblock {\em Comb.}, 2(4):385--393, 1982.

\end{thebibliography}
