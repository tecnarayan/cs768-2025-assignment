\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm\'{a}n, and
  Talwar]{Bassily2020StabilityLosses}
Raef Bassily, Vitaly Feldman, Crist\'{o}bal Guzm\'{a}n, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 4381--4391, 2020.

\bibitem[Bousquet and Elisseeff(2002)]{Bousquet2002StabilityGeneralization}
Olivier Bousquet and André Elisseeff.
\newblock {Stability and Generalization}.
\newblock \emph{Journal of Machine Learning Research}, 2:\penalty0 499--526,
  2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{Bousquet2020SharperAlgorithms}
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Proceedings of Thirty Third Conference on Learning Theory},
  volume 125 of \emph{Proceedings of Machine Learning Research}, pages
  610--626. PMLR, 2020.

\bibitem[Brosse et~al.(2018)Brosse, Durmus, and
  Moulines]{Brosse2018TheDynamics}
Nicolas Brosse, Alain Durmus, and Eric Moulines.
\newblock The promises and pitfalls of stochastic gradient langevin dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Chau et~al.(2021)Chau, Moulines, R{\'a}sonyi, Sabanis, and
  Zhang]{Chau2019OnCase}
Ngoc~Huy Chau, {\'E}ric Moulines, Miklos R{\'a}sonyi, Sotirios Sabanis, and
  Ying Zhang.
\newblock On stochastic gradient langevin dynamics with dependent data streams:
  The fully nonconvex case.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 3\penalty0
  (3):\penalty0 959--986, 2021.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{Chen2018StabilityAlgorithms}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[Dalalyan and Karagulyan(2019)]{Dalalyan2019User-friendlyGradient}
Arnak~S Dalalyan and Avetik Karagulyan.
\newblock {User-friendly guarantees for the Langevin Monte Carlo with
  inaccurate gradient}.
\newblock \emph{Stochastic Processes and their Applications}, 129\penalty0
  (12):\penalty0 5278--5311, 2019.

\bibitem[Eberle(2013)]{Eberle2013ReflectionDiffusions}
Andreas Eberle.
\newblock {Reflection couplings and contraction rates for diffusions}.
\newblock \emph{Probability Theory and Related Fields}, 166\penalty0
  (3-4):\penalty0 851--886, 2013.

\bibitem[Eberle et~al.(2018)Eberle, Guillin, and
  Zimmer]{Eberle2018QuantitativeProcesses}
Andreas Eberle, Arnaud Guillin, and Raphael Zimmer.
\newblock {Quantitative Harris-type theorems for diffusions and McKean–Vlasov
  processes}.
\newblock \emph{Transactions of the American Mathematical Society},
  371\penalty0 (10):\penalty0 7135--7173, 2018.

\bibitem[Elisseeff et~al.(2005)Elisseeff, Evgeniou, and
  Pontil]{Elisseeff2005StabilityAlgorithms}
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil.
\newblock {Stability of Randomized Learning Algorithms}.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (3):\penalty0
  55--79, 2005.

\bibitem[Feldman and Vondrak(2019)]{Feldman2019HighRate}
Vitaly Feldman and Jan Vondrak.
\newblock {High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate}.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Learning
  Theory}, volume~99 of \emph{Proceedings of Machine Learning Research}, pages
  1270--1279. PMLR, 2019.

\bibitem[Haghifam et~al.(2020)Haghifam, Negrea, Khisti, Roy, and
  Dziugaite]{Haghifam2020SharpenedAlgorithms}
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel~M Roy, and
  Gintare~Karolina Dziugaite.
\newblock {Sharpened Generalization Bounds based on Conditional Mutual
  Information and an Application to Noisy, Iterative Algorithms}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 9925--9935, 2020.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{Hardt2015TrainDescent}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock {Train faster, generalize better: Stability of stochastic gradient
  descent}.
\newblock \emph{33rd International Conference on Machine Learning, ICML 2016},
  3:\penalty0 1868--1877, 2015.

\bibitem[Hasenclever et~al.(2017)Hasenclever, Webb, Lienart, Vollmer,
  Lakshminarayanan, Blundell, and Teh]{Hasenclever2017DistributedServer}
Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer, Balaji
  Lakshminarayanan, Charles Blundell, and Yee~Whye Teh.
\newblock {Distributed Bayesian Learning with Stochastic Natural Gradient
  Expectation Propagation and the Posterior Server}.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (1):\penalty0 3744--3780,
  2017.

\bibitem[Lei and Ying(2020)]{Lei2020Fine-GrainedDescent}
Yunwen Lei and Yiming Ying.
\newblock {Fine-Grained Analysis of Stability and Generalization for Stochastic
  Gradient Descent}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of \emph{Proceedings of Machine Learning Research},
  pages 5809--5819. PMLR, 2020.

\bibitem[Li et~al.(2020)Li, Luo, and Qiao]{Li2020OnLearning}
Jian Li, Xuanyuan Luo, and Mingda Qiao.
\newblock {On Generalization Error Bounds of Noisy Gradient Methods for
  Non-Convex Learning}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Mou et~al.(2018)Mou, Wang, Zhai, and
  Zheng]{Mou2018GeneralizationViewpoints}
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng.
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory},
  volume~75 of \emph{Proceedings of Machine Learning Research}, pages 605--638.
  PMLR, 2018.

\bibitem[Negrea et~al.(2019)Negrea, Haghifam, Dziugaite, Khisti, and
  Roy]{Negrea2019Information-TheoreticEstimates}
Jeffrey Negrea, Mahdi Haghifam, Gintare~Karolina Dziugaite, Ashish Khisti, and
  Daniel~M Roy.
\newblock {Information-Theoretic Generalization Bounds for SGLD via
  Data-Dependent Estimates}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Neu et~al.(2021)Neu, Dziugaite, Haghifam, and
  Roy]{Neu2021Information-TheoreticDescent}
Gergely Neu, Gintare~Karolina Dziugaite, Mahdi Haghifam, and Daniel~M. Roy.
\newblock Information-theoretic generalization bounds for stochastic gradient
  descent.
\newblock In \emph{Proceedings of Thirty Fourth Conference on Learning Theory},
  volume 134 of \emph{Proceedings of Machine Learning Research}, pages
  3526--3545. PMLR, 15--19 Aug 2021.

\bibitem[{\O}ksendal(2003)]{oksendal2003stochastic}
Bernt {\O}ksendal.
\newblock \emph{Stochastic differential equations}.
\newblock Springer, 2003.

\bibitem[Pavliotis(2014)]{Pavliotis2014StochasticApplications}
Grigorios~A. Pavliotis.
\newblock \emph{{Stochastic Processes and Applications}}, volume~60 of
  \emph{Texts in Applied Mathematics}.
\newblock Springer New York, 2014.

\bibitem[Pensia et~al.(2018)Pensia, Jog, and
  Loh]{Pensia2018GeneralizationAlgorithms}
Ankit Pensia, Varun Jog, and Po~Ling Loh.
\newblock {Generalization Error Bounds for Noisy, Iterative Algorithms}.
\newblock In \emph{IEEE International Symposium on Information Theory -
  Proceedings}, volume 2018-June, pages 546--550. Institute of Electrical and
  Electronics Engineers Inc., 2018.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{Raginsky2017Non-ConvexAnalysis}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock {Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A
  Nonasymptotic Analysis}.
\newblock In \emph{Proceedings of Machine Learning Research}, volume~65, pages
  1--30. PMLR, 2017.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{Shalev-Shwartz2010LearnabilityConvergence}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2635--2670, 2010.

\bibitem[Teh et~al.(2016)Teh, Thiery, and Vollmer]{Teh2016ConsistencyDynamics}
Yee~Whye Teh, Alexandre~H Thiery, and Sebastian~J Vollmer.
\newblock {Consistency and Fluctuations For Stochastic Gradient Langevin
  Dynamics}.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (7):\penalty0 1--33, 2016.

\bibitem[Villani(2009)]{Villani2009OptimalNew}
Cédric Villani.
\newblock \emph{{Optimal Transport: Old and New}}, volume 338 of
  \emph{Grundlehren der mathematischen Wissenschaften}.
\newblock Springer Berlin Heidelberg, 2009.

\bibitem[Wang(2020)]{Wang2020ExponentialCurvature}
Feng~Yu Wang.
\newblock {Exponential Contraction in Wasserstein Distances for Diffusion
  Semigroups with Negative Curvature}.
\newblock \emph{Potential Analysis}, 53\penalty0 (3):\penalty0 1123--1144,
  2020.

\bibitem[Wang et~al.(2016)Wang, Lei, and Fienberg]{Wang2016LearningPrinciple}
Yu-Xiang Wang, Jing Lei, and Stephen~E Fienberg.
\newblock Learning with differential privacy: Stability, learnability and the
  sufficiency and necessity of erm principle.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 6353--6392, 2016.

\bibitem[Welling and Teh(2011)]{Welling2011BayesianDynamics}
M~Welling and YW~Teh.
\newblock {Bayesian learning via stochastic gradient langevin dynamics}.
\newblock \emph{Proceedings of the 28th International Conference on Machine
  Learning, ICML 2011}, 2011.

\bibitem[Xu and Raginsky(2017)]{Xu2017Information-theoreticAlgorithms}
Aolin Xu and Maxim Raginsky.
\newblock {Information-theoretic analysis of generalization capability of
  learning algorithms}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc., 2017.

\bibitem[Xu et~al.(2018)Xu, Chen, Zou, and Gu]{Xu2018GlobalOptimization}
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu.
\newblock {Global Convergence of Langevin Dynamics Based Algorithms for
  Nonconvex Optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31. Curran Associates, Inc., 2018.

\bibitem[Zhang et~al.(2019)Zhang, Akyildiz, Damoulas, and
  Sabanis]{Zhang2019NonasymptoticOptimization}
Ying Zhang, {\"O}mer~Deniz Akyildiz, Theodoros Damoulas, and Sotirios Sabanis.
\newblock Nonasymptotic estimates for stochastic gradient langevin dynamics
  under local conditions in nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1910.02008}, 2019.

\end{thebibliography}
