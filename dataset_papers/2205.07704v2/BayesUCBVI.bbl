\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal and Jia(2017)]{agrawal2020posterior}
Shipra Agrawal and Randy Jia.
\newblock
  \href{https://proceedings.neurips.cc/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf}{Optimistic
  posterior sampling for reinforcement learning: worst-case regret bounds}.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'{e}}mi Munos, and Hilbert~J. Kappen.
\newblock \href{https://hal.archives-ouvertes.fr/hal-00831875}{{Minimax PAC
  bounds on the sample complexity of reinforcement learning with a generative
  model}}.
\newblock \emph{Machine Learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'{e}}mi Munos.
\newblock \href{https://arxiv.org/pdf/1703.05449.pdf}{{Minimax regret bounds
  for reinforcement learning}}.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Azizzadenesheli et~al.(2018)Azizzadenesheli, Brunskill, and
  Anandkumar]{azizzadenesheli2018efficient}
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar.
\newblock \href{http://dx.doi.org/10.1109/ITA.2018.8503252}{Efficient
  exploration through bayesian deep q-networks}.
\newblock In \emph{2018 Information Theory and Applications Workshop, {ITA}
  2018, San Diego, CA, USA, February 11-16, 2018}, pages 1--9. {IEEE}, 2018.

\bibitem[Badia et~al.(2020)Badia, Sprechmann, Vitvitskyi, Guo, Piot,
  Kapturowski, Tieleman, Arjovsky, Pritzel, Bolt, and Blundell]{badia2020never}
Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel
  Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Mart{\'{i}}n Arjovsky,
  Alexander Pritzel, Andew Bolt, and Charles Blundell.
\newblock \href{https://arxiv.org/abs/2002.06038}{{Never Give Up: Learning
  directed exploration Strategies}}.
\newblock In \emph{International Conference on Learning Representations},
  2020.

\bibitem[Bai et~al.(2021)Bai, Wang, Han, Hao, Garg, Liu, and
  Wang]{bai21principled}
Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng Liu, and
  Zhaoran Wang.
\newblock \href{https://proceedings.mlr.press/v139/bai21d.html}{Principled
  exploration via optimistic bootstrapping and backward induction}.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 577--587. PMLR,  2021.

\bibitem[Bas-Serrano et~al.(2021)Bas-Serrano, Curi, Krause, and
  Neu]{bas2020logistic}
Joan Bas-Serrano, Sebastian Curi, Andreas Krause, and Gergely Neu.
\newblock
  \href{https://proceedings.mlr.press/v130/bas-serrano21a.html}{Logistic
  q-learning}.
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, \emph{Proceedings of
  The 24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, pages
  3610--3618. PMLR,  2021.

\bibitem[Baudry et~al.(2021)Baudry, Saux, and Maillard]{baudry2021optimality}
Dorian Baudry, Patrick Saux, and Odalric-Ambrym Maillard.
\newblock From optimality to robustness: Dirichlet sampling strategies in
  stochastic bandits.
\newblock In \emph{Neurips 2021}, 2021.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock
  \href{https://proceedings.neurips.cc/paper/2016/file/afda332245e2af431fb7b672a68b659d-Paper.pdf}{Unifying
  count-based exploration and intrinsic motivation}.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Borwein and Chan(2007)]{borwein2007uniform}
Jonathan~(Jon) Borwein and O-Yeat Chan.
\newblock \href{http://dx.doi.org/10.7153/mia-12-10}{Uniform bounds for the
  complementary incomplete gamma function}.
\newblock \emph{Mathematical Inequalities and Applications}, 12,  2007.

\bibitem[Brafman and Tennenholtz(2002)]{Brafman02RMAX}
Ronen~I. Brafman and Moshe Tennenholtz.
\newblock {R-MAX} - {A} general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 213--231,
  2002.

\bibitem[Burda et~al.(2019)Burda, Edwards, Storkey, and
  Klimov]{burda2019exploration}
Yuri Burda, Harrison Edwards, Amos~J. Storkey, and Oleg Klimov.
\newblock \href{https://openreview.net/forum?id=H1lJJnR5Ym}{Exploration by
  random network distillation}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock \href{https://arxiv.org/pdf/1703.07710.pdf}{{Unifying {PAC} and
  regret: Uniform {PAC} bounds for episodic reinforcement learning}}.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[de~la Pe{\~{n}}a et~al.(2004)de~la Pe{\~{n}}a, Klass, and
  Lai]{de2004self}
Victor~H. de~la Pe{\~{n}}a, Michael~J. Klass, and Tze~Leung Lai.
\newblock \href{https://arxiv.org/pdf/math/0410102.pdf}{{Self-normalized
  processes: {E}xponential inequalities, moment bounds and iterated logarithm
  laws}}.
\newblock \emph{Annals of probability}, 32:\penalty0 1902--1933, 2004.

\bibitem[Dirksen(2015)]{dirksen2015sections}
Hauke Carl-Erwin Dirksen.
\newblock
  \href{https://macau.uni-kiel.de/receive/diss_mods_00018308}{\emph{Sections of
  simplices and cylinders: Volume formulas and estimates}}.
\newblock PhD thesis, 2015.

\bibitem[Domingues et~al.(2021{\natexlab{a}})Domingues, Flet-Berliac, Leurent,
  M{\'e}nard, Shang, and Valko]{rlberry}
Omar~Darwiche Domingues, Yannis Flet-Berliac, Edouard Leurent, Pierre
  M{\'e}nard, Xuedong Shang, and Michal Valko.
\newblock \href{http://dx.doi.org/10.5281/zenodo.5544540}{{rlberry - A
  Reinforcement Learning Library for Research and Education}},
  2021{\natexlab{a}}.

\bibitem[Domingues et~al.(2021{\natexlab{b}})Domingues, M{\'e}nard, Kaufmann,
  and Valko]{domingues2021episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock \href{https://arxiv.org/pdf/2010.03531.pdf}{Episodic reinforcement
  learning in finite mdps: Minimax lower bounds revisited}.
\newblock In \emph{Algorithmic Learning Theory}, 2021{\natexlab{b}}.

\bibitem[Domingues et~al.(2021{\natexlab{c}})Domingues, Menard, Pirotta,
  Kaufmann, and Valko]{domingues2020regret}
Omar~Darwiche Domingues, Pierre Menard, Matteo Pirotta, Emilie Kaufmann, and
  Michal Valko.
\newblock
  \href{https://proceedings.mlr.press/v139/domingues21a.html}{Kernel-based
  reinforcement learning: A finite-time analysis}.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 2783--2792. PMLR,
  2021{\natexlab{c}}.

\bibitem[Efron(1979)]{efron1979bootstrap}
B.~Efron.
\newblock \href{http://dx.doi.org/10.1214/aos/1176344552}{Bootstrap methods:
  Another look at the jackknife}.
\newblock \emph{The Annals of Statistics}, 7\penalty0 (1):\penalty0 1 -- 26,
  1979.

\bibitem[Evans and Garzepy(2018)]{evans2018measure}
Lawrence~C Evans and Ronald~F Garzepy.
\newblock \emph{Measure theory and fine properties of functions}.
\newblock Routledge, 2018.

\bibitem[Fedoryuk(1977)]{fedoryuk1977metod}
M.~V. Fedoryuk.
\newblock \emph{Metod perevala}.
\newblock Izdat. ``Nauka'', Moscow, 1977.

\bibitem[Fiechter(1994)]{fiechter1994efficient}
Claude-Nicolas Fiechter.
\newblock
  \href{http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7F5F8FCD1AA7ED07356410DDD5B384FE?doi=10.1.1.49.8652\&rep=rep1\&type=pdf}{{Efficient
  reinforcement learning}}.
\newblock In \emph{Conference on Learning Theory}, 1994.

\bibitem[Fortunato et~al.(2018)Fortunato, Azar, Piot, Menick, Hessel, Osband,
  Graves, Mnih, Munos, Hassabis, Pietquin, Blundell, and
  Legg]{fortunato2018noisy}
Meire Fortunato, Mohammad~Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo
  Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, R{\'{e}}mi Munos, Demis
  Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg.
\newblock \href{https://openreview.net/forum?id=rywHCPkAW}{Noisy networks for
  exploration}.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem[Fox et~al.(2018)Fox, Choshen, and Loewenstein]{choshen2018dora}
Lior Fox, Leshem Choshen, and Yonatan Loewenstein.
\newblock \href{https://openreview.net/forum?id=ry1arUgCW}{{DORA} the explorer:
  Directed outreaching reinforcement action-selection}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and
  Ortner]{fruit2018efficient}
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1578--1586. PMLR, 2018.

\bibitem[Garivier and Capp\'{e}(2011)]{garivier2011the}
Aur\'{e}lien Garivier and Olivier Capp\'{e}.
\newblock \href{https://proceedings.mlr.press/v19/garivier11a.html}{The kl-ucb
  algorithm for bounded stochastic bandits and beyond}.
\newblock In Sham~M. Kakade and Ulrike von Luxburg, editors, \emph{Proceedings
  of the 24th Annual Conference on Learning Theory}, volume~19 of
  \emph{Proceedings of Machine Learning Research}, pages 359--376, Budapest,
  Hungary,  2011. PMLR.

\bibitem[Garivier et~al.(2018)Garivier, Hadiji, Menard, and
  Stoltz]{garivier2018kl}
Aur{\'e}lien Garivier, H{\'e}di Hadiji, Pierre Menard, and Gilles Stoltz.
\newblock Kl-ucb-switch: optimal regret bounds for stochastic bandits from both
  a distribution-dependent and a distribution-free viewpoints.
\newblock \emph{arXiv preprint arXiv:1805.05071}, 2018.

\bibitem[Groeneveld and Meeden(1977)]{groeneveld1977mode}
Richard~A. Groeneveld and Glen Meeden.
\newblock \href{http://dx.doi.org/10.1080/00031305.1977.10479215}{The mode,
  median, and mean inequality}.
\newblock \emph{The American Statistician}, 31\penalty0 (3):\penalty0 120--121,
  1977.

\bibitem[Haber et~al.(2018)Haber, Mrowca, Wang, Fei-Fei, and
  Yamins]{haber2018learning}
Nick Haber, Damian Mrowca, Stephanie Wang, Li~F Fei-Fei, and Daniel~L Yamins.
\newblock
  \href{https://proceedings.neurips.cc/paper/2018/file/71e63ef5b7249cfc60852f0e0f5bf4c8-Paper.pdf}{Learning
  to play with intrinsically-motivated, self-aware agents}.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Hasselt(2010)]{hasselt2010double}
Hado Hasselt.
\newblock Double q-learning.
\newblock \emph{Advances in neural information processing systems},
  23:\penalty0 2613--2621, 2010.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
\newblock Neural networks for machine learning lecture 6a overview of
  mini-batch gradient descent.
\newblock \emph{Cited on}, 14\penalty0 (8):\penalty0 2, 2012.

\bibitem[Honda and Takemura(2010)]{honda2010asymptotically}
Junya Honda and Akimichi Takemura.
\newblock
  \href{http://dblp.uni-trier.de/db/conf/colt/colt2010.html#HondaT10}{An
  asymptotically optimal bandit algorithm for bounded support models.}
\newblock In Adam~Tauman Kalai and Mehryar Mohri, editors, \emph{COLT}, pages
  67--79. Omnipress, 2010.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock
  \href{http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf}{{Near-optimal
  regret bounds for reinforcement learning}}.
\newblock \emph{Journal of Machine Learning Research}, 99:\penalty0 1563--1600,
  2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018is}
Chi Jin, Zeyuan Allen-Zhu, S{\'{e}}bastien Bubeck, and Michael~I. Jordan.
\newblock \href{https://arxiv.org/pdf/1807.03765.pdf}{{Is Q-learning provably
  efficient?}}
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Jonsson et~al.(2020)Jonsson, Kaufmann, M\'{e}nard, Domingues, Leurent,
  and Valko]{jonsson2020planning}
Anders Jonsson, Emilie Kaufmann, Pierre M\'{e}nard, Omar~Darwiche Domingues,
  Edouard Leurent, and Michal Valko.
\newblock Planning in markov decision processes with gap-dependent sample
  complexity.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Cappe, and Garivier]{kaufmann12}
Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier.
\newblock \href{https://proceedings.mlr.press/v22/kaufmann12.html}{On bayesian
  upper confidence bounds for bandit problems}.
\newblock In Neil~D. Lawrence and Mark Girolami, editors, \emph{Proceedings of
  the Fifteenth International Conference on Artificial Intelligence and
  Statistics}, volume~22 of \emph{Proceedings of Machine Learning Research},
  pages 592--600, La Palma, Canary Islands,  2012. PMLR.

\bibitem[Kaufmann et~al.(2021)Kaufmann, M{\'e}nard, Darwiche~Domingues,
  Jonsson, Leurent, and Valko]{kaufmann2020adaptive}
Emilie Kaufmann, Pierre M{\'e}nard, Omar Darwiche~Domingues, Anders Jonsson,
  Edouard Leurent, and Michal Valko.
\newblock \href{https://proceedings.mlr.press/v132/kaufmann21a.html}{Adaptive
  reward-free exploration}.
\newblock In Vitaly Feldman, Katrina Ligett, and Sivan Sabato, editors,
  \emph{Proceedings of the 32nd International Conference on Algorithmic
  Learning Theory}, volume 132 of \emph{Proceedings of Machine Learning
  Research}, pages 865--891. PMLR,  2021.

\bibitem[Kveton et~al.(2019)Kveton, Szepesvari, Vaswani, Wen, Lattimore, and
  Ghavamzadeh]{kveton2019garbage}
Branislav Kveton, Csaba Szepesvari, Sharan Vaswani, Zheng Wen, Tor Lattimore,
  and Mohammad Ghavamzadeh.
\newblock \href{https://proceedings.mlr.press/v97/kveton19a.html}{Garbage in,
  reward out: Bootstrapping exploration in multi-armed bandits}.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  3601--3610. PMLR,  2019.

\bibitem[Lasserre(2020)]{lasserre2020simple}
Jean-Bernard Lasserre.
\newblock \href{http://dx.doi.org/10.1007/s10543-020-00828-x}{Simple formula
  for integration of polynomials on a simplex}.
\newblock \emph{BIT Numerical Mathematics}, 61,  2020.

\bibitem[M{\'{e}}nard et~al.(2021)M{\'{e}}nard, Domingues, Shang, and
  Valko]{menard2021ucb}
Pierre M{\'{e}}nard, Omar~Darwiche Domingues, Xuedong Shang, and Michal Valko.
\newblock \href{http://proceedings.mlr.press/v139/menard21b.html}{{UCB}
  momentum q-learning: Correcting the bias without forgetting}.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning, {ICML} 2021, 18-24 July
  2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pages 7609--7618. {PMLR}, 2021.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In \emph{NIPS Deep Learning Workshop}. 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015humanlevel}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock \href{http://dx.doi.org/10.1038/nature14236}{Human-level control
  through deep reinforcement learning}.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533,  2015.

\bibitem[Nikolov et~al.(2019)Nikolov, Kirschner, Berkenkamp, and
  Krause]{nikolov2019information}
Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause.
\newblock
  \href{https://openreview.net/forum?id=Byx83s09Km}{Information-directed
  exploration for deep reinforcement learning}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Olver(1997)]{olver1997asymptotics}
Frank W.~J. Olver.
\newblock \emph{Asymptotics and special functions}.
\newblock AKP Classics. A K Peters, Ltd., Wellesley, MA, 1997.
\newblock Reprint of the 1974 original [Academic Press, New York; MR0435697 (55
  \#8655)].

\bibitem[Osband and Roy(2017)]{osband2017gaussian}
Ian Osband and Benjamin~Van Roy.
\newblock \href{http://arxiv.org/abs/1702.04126}{Gaussian-dirichlet posterior
  dominance in sequential learning}.
\newblock \emph{CoRR}, abs/1702.04126, 2017.

\bibitem[Osband and Van~Roy(2015)]{osband2015bootstrap}
Ian Osband and Benjamin Van~Roy.
\newblock \href{http://arxiv.org/abs/1507.00300}{Bootstrapped thompson sampling
  and deep exploration}.
\newblock \emph{CoRR}, abs/1507.00300, 2015.

\bibitem[Osband and Van~Roy(2017)]{osband2017why}
Ian Osband and Benjamin Van~Roy.
\newblock \href{https://proceedings.mlr.press/v70/osband17a.html}{Why is
  posterior sampling better than optimism for reinforcement learning?}
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 2701--2710. PMLR,
  2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{obsband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock
  \href{https://proceedings.neurips.cc/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Paper.pdf}{(more)
  efficient reinforcement learning via posterior sampling}.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.

\bibitem[Osband et~al.(2016{\natexlab{a}})Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock
  \href{https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf}{Deep
  exploration via bootstrapped dqn}.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016{\natexlab{a}}.

\bibitem[Osband et~al.(2016{\natexlab{b}})Osband, Roy, and
  Wen]{osband16generalization}
Ian Osband, Benjamin~Van Roy, and Zheng Wen.
\newblock \href{https://proceedings.mlr.press/v48/osband16.html}{Generalization
  and exploration via randomized value functions}.
\newblock In Maria~Florina Balcan and Kilian~Q. Weinberger, editors,
  \emph{Proceedings of The 33rd International Conference on Machine Learning},
  volume~48 of \emph{Proceedings of Machine Learning Research}, pages
  2377--2386, New York, New York, USA,  2016{\natexlab{b}}. PMLR.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband18randomized}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock
  \href{https://proceedings.neurips.cc/paper/2018/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf}{Randomized
  prior functions for deep reinforcement learning}.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Osband et~al.(2019)Osband, Roy, Russo, and Wen]{osband2019deep}
Ian Osband, Benjamin~Van Roy, Daniel~J. Russo, and Zheng Wen.
\newblock \href{http://jmlr.org/papers/v20/18-339.html}{Deep exploration via
  randomized value functions}.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (124):\penalty0 1--62, 2019.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, van~den Oord, and
  Munos]{ostrovski2017count}
Georg Ostrovski, Marc~G Bellemare, A{\"a}ron van~den Oord, and R{\'e}mi Munos.
\newblock Count-based exploration with neural density models.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2721--2730. JMLR. org, 2017.

\bibitem[Pacchiano et~al.(2021)Pacchiano, Ball, Parker-Holder, Choromanski, and
  Roberts]{pacchiano2021towards}
Aldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and
  Stephen Roberts.
\newblock \href{https://proceedings.mlr.press/v161/pacchiano21a.html}{Towards
  tractable optimism in model-based reinforcement learning}.
\newblock In Cassio de~Campos and Marloes~H. Maathuis, editors,
  \emph{Proceedings of the Thirty-Seventh Conference on Uncertainty in
  Artificial Intelligence}, volume 161 of \emph{Proceedings of Machine Learning
  Research}, pages 1413--1423. PMLR,  2021.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A. Efros, and Trevor Darrell.
\newblock
  \href{https://proceedings.mlr.press/v70/pathak17a.html}{Curiosity-driven
  exploration by self-supervised prediction}.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 2778--2787. PMLR,
  2017.

\bibitem[Puterman(1994)]{puterman1994markov}
Martin~L. Puterman.
\newblock
  \href{https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887}{\emph{{Markov
  decision processes: Discrete stochastic dynamic programming}}}.
\newblock John Wiley {\&} Sons, New York, NY, 1994.

\bibitem[Qian et~al.(2020)Qian, Fruit, Pirotta, and
  Lazaric]{qian2020concentration}
Jian Qian, Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric.
\newblock \href{https://arxiv.org/abs/2001.11595}{Concentration inequalities
  for multinoulli random variables}.
\newblock \emph{CoRR}, abs/2001.11595, 2020.

\bibitem[Riou and Honda(2020)]{riou20a}
Charles Riou and Junya Honda.
\newblock \href{https://proceedings.mlr.press/v117/riou20a.html}{Bandit
  algorithms based on thompson sampling for bounded reward distributions}.
\newblock In Aryeh Kontorovich and Gergely Neu, editors, \emph{Proceedings of
  the 31st International Conference on Algorithmic Learning Theory}, volume 117
  of \emph{Proceedings of Machine Learning Research}, pages 777--826. PMLR,
  2020.

\bibitem[Rubin(1981)]{rubin1981bayesian}
Donald~B Rubin.
\newblock The bayesian bootstrap.
\newblock \emph{The annals of statistics}, pages 130--134, 1981.

\bibitem[Russo(2019)]{russo2019worst}
Daniel Russo.
\newblock
  \href{https://proceedings.neurips.cc/paper/2019/file/451ae86722d26a608c2e174b2b2773f1-Paper.pdf}{Worst-case
  regret bounds for exploration via randomized value functions}.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Russo and Van~Roy(2014)]{russo2014learning}
Daniel Russo and Benjamin Van~Roy.
\newblock
  \href{https://proceedings.neurips.cc/paper/2014/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf}{Learning
  to optimize via information-directed sampling}.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.

\bibitem[Schmidhuber(1991)]{schmidhuber1991possibility}
J{\"u}rgen Schmidhuber.
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In \emph{Proc. of the international conference on simulation of
  adaptive behavior: From animals to animats}, pages 222--227, 1991.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Szita and L\H{o}rincz(2008)]{szita2008}
Istv\'{a}n Szita and Andr\'{a}s L\H{o}rincz.
\newblock \href{http://dx.doi.org/10.1145/1390156.1390288}{The many faces of
  optimism: A unifying approach}.
\newblock In \emph{Proceedings of the 25th International Conference on Machine
  Learning}, ICML '08, page 1048â€“1055, New York, NY, USA, 2008. Association
  for Computing Machinery.

\bibitem[Talebi and Maillard(2018)]{talebi2018variance}
Mohammad~Sadegh Talebi and Odalric-Ambrym Maillard.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock In \emph{Algorithmic Learning Theory}, pages 770--805, 2018.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Xi~Chen, Duan,
  Schulman, DeTurck, and Abbeel]{tang2017exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi~Chen, Yan
  Duan, John Schulman, Filip DeTurck, and Pieter Abbeel.
\newblock
  \href{https://proceedings.neurips.cc/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf}{\#exploration:
  A study of count-based exploration for deep reinforcement learning}.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Thompson(1933)]{thompson1933on}
William~R Thompson.
\newblock \href{http://dx.doi.org/10.1093/biomet/25.3-4.285}{On the likelihood
  that one unknown probability exceeds another in view of the evidence of two
  samples}.
\newblock \emph{Biometrika}, 25\penalty0 (3-4):\penalty0 285--294,  1933.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~30, 2016.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
Chris~J. Watkins and Peter Dayan.
\newblock
  \href{https://link.springer.com/content/pdf/10.1007/BF00992698.pdf}{{Q-learning}}.
\newblock \emph{Machine Learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Xiong et~al.(2021)Xiong, Shen, Cui, and Du]{xiong2021nearoptimal}
Zhihan Xiong, Ruoqi Shen, Qiwen Cui, and Simon~S. Du.
\newblock Near-optimal randomized exploration for tabular mdp, 2021.

\bibitem[Yang et~al.(2021)Yang, Tang, Bai, Liu, Hao, Meng, and
  Liu]{yang2021exploration}
Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng,
  and Peng Liu.
\newblock \href{https://arxiv.org/abs/2109.06668}{Exploration in deep
  reinforcement learning: {A} comprehensive survey}.
\newblock \emph{CoRR}, abs/2109.06668, 2021.

\bibitem[Zanette and Brunskill(2019{\natexlab{a}})]{Zanette19Euler}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, (ICML)}, 2019{\natexlab{a}}.

\bibitem[Zanette and Brunskill(2019{\natexlab{b}})]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock \href{https://arxiv.org/pdf/1901.00210.pdf}{{Tighter
  problem-dependent regret bounds in reinforcement learning without domain
  knowledge using value function bounds}}.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{zhang2020advantage}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.

\end{thebibliography}
