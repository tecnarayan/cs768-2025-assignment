\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and
  Legenstein]{bellec2018deep}
Bellec, G., Kappel, D., Maass, W., and Legenstein, R.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJ_wN01C-}.

\bibitem[Dai et~al.(2018)Dai, Li, and Yu]{dai2018fast}
Dai, R., Li, L., and Yu, W.
\newblock Fast training and model compression of gated rnns via singular value
  decomposition.
\newblock In \emph{2018 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--7. IEEE, 2018.

\bibitem[de~Jorge et~al.(2020)de~Jorge, Sanyal, Behl, Torr, Rogez, and
  Dokania]{de2020progressive}
de~Jorge, P., Sanyal, A., Behl, H.~S., Torr, P.~H., Rogez, G., and Dokania,
  P.~K.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock \emph{arXiv preprint arXiv:2006.09081}, 2020.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2019sparse}
Dettmers, T. and Zettlemoyer, L.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{arXiv preprint arXiv:1907.04840}, 2019.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.~A.
\newblock Essentially no barriers in neural network energy landscape.
\newblock \emph{arXiv preprint arXiv:1803.00885}, 2018.

\bibitem[Elman(1990)]{elman1990finding}
Elman, J.~L.
\newblock Finding structure in time.
\newblock \emph{Cognitive science}, 14\penalty0 (2):\penalty0 179--211, 1990.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2019rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Fort \& Jastrzebski(2019)Fort and Jastrzebski]{fort2019large}
Fort, S. and Jastrzebski, S.
\newblock Large scale structure of neural network loss landscapes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6709--6717, 2019.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018the}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016theoretically}
Gal, Y. and Ghahramani, Z.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1019--1027, 2016.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8789--8798, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Goodfellow, I.~J., Vinyals, O., and Saxe, A.~M.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{arXiv preprint arXiv:1412.6544}, 2014.

\bibitem[Grave et~al.(2017)Grave, Joulin, and Usunier]{grave2016improving}
Grave, E., Joulin, A., and Usunier, N.
\newblock Improving neural language models with a continuous cache.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=B184E5qee}.

\bibitem[Gray et~al.(2017)Gray, Radford, and Kingma]{gray2017gpu}
Gray, S., Radford, A., and Kingma, D.~P.
\newblock Gpu kernels for block-sparse weights.
\newblock \emph{arXiv preprint arXiv:1711.09224}, 3, 2017.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hirschman et~al.(1999)Hirschman, Light, Breck, and
  Burger]{hirschman1999deep}
Hirschman, L., Light, M., Breck, E., and Burger, J.~D.
\newblock Deep read: A reading comprehension system.
\newblock In \emph{Proceedings of the 37th annual meeting of the Association
  for Computational Linguistics}, pp.\  325--332, 1999.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Inan et~al.(2017)Inan, Khosravi, and Socher]{inan2016tying}
Inan, H., Khosravi, K., and Socher, R.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg2014speeding}
Jaderberg, M., Vedaldi, A., and Zisserman, A.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock \emph{arXiv preprint arXiv:1405.3866}, 2014.

\bibitem[Janowsky(1989)]{janowsky1989pruning}
Janowsky, S.~A.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A}, 39\penalty0 (12):\penalty0 6600, 1989.

\bibitem[Jayakumar et~al.(2020)Jayakumar, Pascanu, Rae, Osindero, and
  Elsen]{jayakumar2020top}
Jayakumar, S., Pascanu, R., Rae, J., Osindero, S., and Elsen, E.
\newblock Top-kast: Top-k always sparse training.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kalchbrenner \& Blunsom(2013)Kalchbrenner and
  Blunsom]{kalchbrenner2013recurrent}
Kalchbrenner, N. and Blunsom, P.
\newblock Recurrent continuous translation models.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1700--1709, 2013.

\bibitem[Kalchbrenner et~al.(2018)Kalchbrenner, Elsen, Simonyan, Noury,
  Casagrande, Lockhart, Stimberg, Oord, Dieleman, and
  Kavukcuoglu]{kalchbrenner2018efficient}
Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart,
  E., Stimberg, F., Oord, A. v.~d., Dieleman, S., and Kavukcuoglu, K.
\newblock Efficient neural audio synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kepner \& Robinett(2019)Kepner and Robinett]{kepner2019radix}
Kepner, J. and Robinett, R.
\newblock Radix-net: Structured sparse matrices for deep neural networks.
\newblock In \emph{2019 IEEE International Parallel and Distributed Processing
  Symposium Workshops (IPDPSW)}, pp.\  268--274. IEEE, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock \emph{arXiv preprint arXiv:1506.02557}, 2015.

\bibitem[Krause et~al.(2018)Krause, Kahembwe, Murray, and
  Renals]{krause2018dynamic}
Krause, B., Kahembwe, E., Murray, I., and Renals, S.
\newblock Dynamic evaluation of neural sequence models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2766--2775, 2018.

\bibitem[Kusupati et~al.(2020)Kusupati, Ramanujan, Somani, Wortsman, Jain,
  Kakade, and Farhadi]{kusupati2020soft}
Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S.,
  and Farhadi, A.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5544--5555. PMLR, 2020.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.
\newblock {SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION}
  {SENSITIVITY}.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1VZqjAcYX}.

\bibitem[Lee et~al.(2020)Lee, Ajanthan, Gould, and Torr]{Lee2020A}
Lee, N., Ajanthan, T., Gould, S., and Torr, P. H.~S.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJeTo2VFwH}.

\bibitem[Li et~al.(2016)Li, Yosinski, Clune, Lipson, and
  Hopcroft]{li2016convergent}
Li, Y., Yosinski, J., Clune, J., Lipson, H., and Hopcroft, J.~E.
\newblock Convergent learning: Do different neural networks learn the same
  representations?
\newblock In \emph{Iclr}, 2016.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, XU, SHI, Cheung, and
  So]{LIU2020Dynamic}
Liu, J., XU, Z., SHI, R., Cheung, R. C.~C., and So, H.~K.
\newblock Dynamic sparse training: Find efficient sparse network from scratch
  with trainable masked layers.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=SJlbGJrtDB}.

\bibitem[Liu et~al.(2019)Liu, Mocanu, and Pechenizkiy]{liu2019intrinsically}
Liu, S., Mocanu, D.~C., and Pechenizkiy, M.
\newblock Intrinsically sparse long short-term memory networks.
\newblock \emph{arXiv preprint arXiv:1901.09208}, 2019.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Mocanu, Matavalam, Pei, and
  Pechenizkiy]{liu2020sparse}
Liu, S., Mocanu, D.~C., Matavalam, A. R.~R., Pei, Y., and Pechenizkiy, M.
\newblock Sparse evolutionary deep learning with over one million artificial
  neurons on commodity hardware.
\newblock \emph{Neural Computing and Applications}, pp.\  1--16,
  2020{\natexlab{b}}.

\bibitem[Liu et~al.(2020{\natexlab{c}})Liu, van~der Lee, Yaman, Atashgahi,
  Ferrar, Sokar, Pechenizkiy, and Mocanu]{liu2020topological}
Liu, S., van~der Lee, T., Yaman, A., Atashgahi, Z., Ferrar, D., Sokar, G.,
  Pechenizkiy, M., and Mocanu, D.
\newblock Topological insights into sparse neural networks.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 2020{\natexlab{c}}.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and Kingma]{louizos2017learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ma et~al.(2019)Ma, Miao, Niu, and Zhang]{ma2019transformed}
Ma, R., Miao, J., Niu, L., and Zhang, P.
\newblock Transformed l1 regularization for learning sparse deep neural
  networks.
\newblock \emph{Neural Networks}, 119:\penalty0 286--298, 2019.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and
  Marcinkiewicz]{marcus1993building}
Marcus, M., Santorini, B., and Marcinkiewicz, M.~A.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock 1993.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
Melis, G., Dyer, C., and Blunsom, P.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Melis et~al.(2018)Melis, Dyer, and Blunsom]{melis2018on}
Melis, G., Dyer, C., and Blunsom, P.
\newblock On the state of the art of evaluation in neural language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=ByJHuTgA-}.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2017pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018regularizing}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SyyGPP0TZ}.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, {\v{C}}ernock{\`y},
  and Khudanpur]{mikolov2010recurrent}
Mikolov, T., Karafi{\'a}t, M., Burget, L., {\v{C}}ernock{\`y}, J., and
  Khudanpur, S.
\newblock Recurrent neural network based language model.
\newblock In \emph{Eleventh annual conference of the international speech
  communication association}, 2010.

\bibitem[Mocanu et~al.(2016)Mocanu, Mocanu, Nguyen, Gibescu, and
  Liotta]{Mocanu2016xbm}
Mocanu, D.~C., Mocanu, E., Nguyen, P.~H., Gibescu, M., and Liotta, A.
\newblock A topological insight into restricted boltzmann machines.
\newblock \emph{Machine Learning}, 104\penalty0 (2):\penalty0 243--270, Sep
  2016.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/s10994-016-5570-z}.
\newblock URL \url{https://doi.org/10.1007/s10994-016-5570-z}.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature Communications}, 9\penalty0 (1):\penalty0 2383, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2498--2507. JMLR. org, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Moradi et~al.(2019)Moradi, Berangi, and Minaei]{moradi2019sparsemaps}
Moradi, R., Berangi, R., and Minaei, B.
\newblock Sparsemaps: convolutional networks with sparse feature maps for tiny
  image classification.
\newblock \emph{Expert Systems with Applications}, 119:\penalty0 142--154,
  2019.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Mostafa, H. and Wang, X.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989using}
Mozer, M.~C. and Smolensky, P.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16, 1989.

\bibitem[NVIDIA(2020)]{NVIDIA2020}
NVIDIA.
\newblock Nvidia a100 tensor core gpu architecture.
\newblock \emph{Retrieved from:
  https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  2020.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}
Polyak, B.~T. and Juditsky, A.~B.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Prabhu et~al.(2018)Prabhu, Varma, and Namboodiri]{prabhu2018deep}
Prabhu, A., Varma, G., and Namboodiri, A.
\newblock Deep expander networks: Efficient deep networks from graph theory.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  20--35, 2018.

\bibitem[Raihan \& Aamodt(2020)Raihan and Aamodt]{raihan2020sparse}
Raihan, M.~A. and Aamodt, T.~M.
\newblock Sparse weight activation training.
\newblock \emph{arXiv preprint arXiv:2001.01969}, 2020.

\bibitem[Sanfeliu \& Fu(1983)Sanfeliu and Fu]{sanfeliu1983distance}
Sanfeliu, A. and Fu, K.-S.
\newblock A distance measure between attributed relational graphs for pattern
  recognition.
\newblock \emph{IEEE transactions on systems, man, and cybernetics}, \penalty0
  (3):\penalty0 353--362, 1983.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{ICLR}, 2014.

\bibitem[Shen et~al.(2019)Shen, Tan, Sordoni, and Courville]{shen2018ordered}
Shen, Y., Tan, S., Sordoni, A., and Courville, A.
\newblock Ordered neurons: Integrating tree structures into recurrent neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1l6qiR5F7}.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015training}
Srivastava, R.~K., Greff, K., and Schmidhuber, J.
\newblock Training very deep networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2377--2385, 2015.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{arXiv preprint arXiv:2006.05467}, 2020.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{Wang2020Picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkgsACVKPH}.

\bibitem[Wang \& Jiang(2017)Wang and Jiang]{wang2017machine}
Wang, S. and Jiang, J.
\newblock Machine comprehension using match-lstm and answer pointer.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Wen et~al.(2018)Wen, He, Rajbhandari, Zhang, Wang, Liu, Hu, Chen, and
  Li]{wen2018learning}
Wen, W., He, Y., Rajbhandari, S., Zhang, M., Wang, W., Liu, F., Hu, B., Chen,
  Y., and Li, H.
\newblock Learning intrinsic sparse structures within long short-term memory.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rk6cfpRjZ}.

\bibitem[Xiao et~al.(2019)Xiao, Wang, and Rajasekaran]{xiao2019autoprune}
Xiao, X., Wang, Z., and Rajasekaran, S.
\newblock Autoprune: Automatic network pruning by regularizing auxiliary
  parameters.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Yang \& Ma(2019)Yang and Ma]{yang2019feed}
Yang, J. and Ma, J.
\newblock Feed-forward neural network training using sparse representation.
\newblock \emph{Expert Systems with Applications}, 116:\penalty0 255--264,
  2019.

\bibitem[Yang et~al.(2018)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2018breaking}
Yang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W.~W.
\newblock Breaking the softmax bottleneck: A high-rank {RNN} language model.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HkwZSG-CZ}.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Zaremba, W., Sutskever, I., and Vinyals, O.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019deconstructing}
Zhou, H., Lan, J., Liu, R., and Yosinski, J.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3597--3607, 2019.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}, 2017.

\bibitem[Zilly et~al.(2017)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2017recurrent}
Zilly, J.~G., Srivastava, R.~K., Koutn{\'\i}k, J., and Schmidhuber, J.
\newblock Recurrent highway networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  4189--4198. JMLR. org, 2017.

\end{thebibliography}
