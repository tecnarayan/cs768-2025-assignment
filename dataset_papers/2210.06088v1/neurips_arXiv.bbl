\begin{thebibliography}{10}

\bibitem{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In {\em International conference on machine learning}. PMLR, 2017.

\bibitem{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock {\em Advances in neural information processing systems}, 2018.

\bibitem{soltanolkotabi2018theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 65(2):742--769, 2018.

\bibitem{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671, 2018.

\bibitem{goldt2019generalisation}
Sebastian Goldt, Madhu~S Advani, Andrew~M Saxe, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Generalisation dynamics of online learning in over-parameterised
  neural networks.
\newblock {\em arXiv preprint arXiv:1901.09085}, 2019.

\bibitem{tian2020student}
Yuandong Tian.
\newblock Student specialization in deep rectified networks with finite width
  and input dimension.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2020.

\bibitem{safran2021effects}
Itay~M Safran, Gilad Yehudai, and Ohad Shamir.
\newblock The effects of mild over-parameterization on the optimization
  landscape of shallow relu neural networks.
\newblock In {\em Conference on Learning Theory}. PMLR, 2021.

\bibitem{blum1989training}
Avrim Blum and Ronald~L Rivest.
\newblock Training a 3-node neural network is np-complete.
\newblock In {\em Advances in neural information processing systems}, pages
  494--501, 1989.

\bibitem{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev{-}Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018}.

\bibitem{shamir2018distribution}
Ohad Shamir.
\newblock Distribution-specific hardness of learning neural networks.
\newblock {\em The Journal of Machine Learning Research}, 19(1):1135--1163,
  2018.

\bibitem{du2017gradient}
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos.
\newblock Gradient descent learns one-hidden-layer cnn: Donâ€™t be afraid of
  spurious local minima.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2018.

\bibitem{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em International conference on machine learning}. PMLR, 2017.

\bibitem{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem{tian2017analytical}
Yuandong Tian.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock In {\em International conference on machine learning}. PMLR, 2017.

\bibitem{ge2017learning}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018}.

\bibitem{aubin2019committee}
Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas
  Macris, and Lenka Zdeborov{\'a}.
\newblock The committee machine: Computational to statistical gaps in learning
  a two-layers neural network.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2019(12):124023, 2019.

\bibitem{akiyama2021learnability}
Shunta Akiyama and Taiji Suzuki.
\newblock On learnability via gradient method for two-layer relu neural
  networks in teacher-student setting.
\newblock In {\em International Conference on Machine Learning}, pages
  152--162. PMLR, 2021.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem{safran2017spurious}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In {\em International conference on machine learning}. PMLR, 2018.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock {\em Advances in neural information processing systems}, 2016.

\bibitem{li2020learning}
Yuanzhi Li, Tengyu Ma, and Hongyang~R Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In {\em Conference on learning theory}. PMLR, 2020.

\bibitem{goldt2019dynamics}
Sebastian Goldt, Madhu Advani, Andrew~M Saxe, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{oostwal2021hidden}
Elisa Oostwal, Michiel Straat, and Michael Biehl.
\newblock Hidden unit specialization in layered neural networks: Relu vs.
  sigmoidal activation.
\newblock {\em Physica A: Statistical Mechanics and its Applications},
  564:125517, 2021.

\bibitem{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{arjevanifield2019spurious}
Yossi Arjevani and Michael Field.
\newblock On the principle of least symmetry breaking in shallow relu models.
\newblock {\em arXiv preprint arXiv:1912.11939}, 2019.

\bibitem{arjevanifield2021tensor}
Yossi Arjevani, Joan Bruna, Michael Field, Joe Kileel, Matthew Trager, and
  Francis Williams.
\newblock Symmetry breaking in symmetric tensor decomposition.
\newblock {\em arXiv preprint arXiv:2103.06234}, 2021.

\bibitem{arjevanifield2022analytictensor}
Yossi Arjevani and Michael Field.
\newblock Analytic study of spurious minima in symmetric tensor decomposition
  problems.
\newblock {\em In preparation}.

\bibitem{ArjevaniField2020}
Yossi Arjevani and Michael Field.
\newblock Symmetry \& critical points for a model shallow neural network.
\newblock {\em Physica D: Nonlinear Phenomena}, 427:133014, 2021.

\bibitem{arjevanifield2020hessian}
Yossi Arjevani and Michael Field.
\newblock Analytic characterization of the hessian in shallow relu models: {A}
  tale of symmetry.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{arjevanifield2021analytic}
Yossi Arjevani and Michael Field.
\newblock Analytic study of families of spurious minima in two-layer relu
  neural networks: A tale of symmetry ii.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{arjevanifield2022equivariant}
Yossi Arjevani and Michael Field.
\newblock Equivariant bifurcation, quadratic equivariants, and symmetry
  breaking for the standard representation of s k.
\newblock {\em Nonlinearity}, 35(6):2809, 2022.

\bibitem{livni2014computational}
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{bierstone1998subanalytic}
Edward Bierstone and Pierre~D Milman.
\newblock Subanalytic geometry.
\newblock {\em Model theory, algebra, and geometry (ed. by Haskell, D. et al.),
  MSRI}, 39:151--172, 1998.

\bibitem{john1968singular}
John Milnor.
\newblock Singular points of complex hypersurfaces.
\newblock {\em Ann. Math. St}, 61:591--648, 1968.

\bibitem{James1978}
GD~James.
\newblock The representation theory of the symmetric groups.
\newblock Springer, 1978.

\bibitem{fulton1991representation}
William Fulton and Joe Harris.
\newblock Representation theory, volume 129 of.
\newblock {\em Graduate Texts in Mathematics}, 1991.

\bibitem{johnson1985matrix}
Charles~R Johnson and Roger~A Horn.
\newblock {\em Matrix analysis}.
\newblock Cambridge university press Cambridge, 1985.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock {\em Mathematical Programming}, 108(1):177--205, 2006.

\bibitem{arjevani2022lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock {\em Mathematical Programming}, pages 1--50, 2022.

\bibitem{arjevanifield2022bifurcation}
Yossi Arjevani and Michael Field.
\newblock Bifurcation, spurious minima and over-paramerization.
\newblock {\em In preparation}.

\bibitem{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In {\em International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018.

\end{thebibliography}
