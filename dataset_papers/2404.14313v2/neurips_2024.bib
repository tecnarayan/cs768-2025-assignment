@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@inproceedings{le2019RevisitingTE,
  title={Revisiting the Evaluation of Theory of Mind through Question Answering},
  author={Matt Le and Y-Lan Boureau and Maximilian Nickel},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}
@inproceedings{shu2021agent,
  title={Agent: A benchmark for core psychological reasoning},
  author={Shu, Tianmin and Bhandwaldar, Abhishek and Gan, Chuang and Smith, Kevin and Liu, Shari and Gutfreund, Dan and Spelke, Elizabeth and Tenenbaum, Joshua and Ullman, Tomer},
  booktitle={International Conference on Machine Learning},
  pages={9614--9625},
  year={2021},
  organization={PMLR}
}

@inproceedings{wu2023computational,
	author = {Sarah A. Wu and Shruti Sridhar and Tobias Gerstenberg},
	booktitle = {{Proceedings of the 45th Annual Conference of the Cognitive Science Society}},
	date-added = {2023-05-11 13:41:13 -0700},
	date-modified = {2023-05-11 14:37:05 -0700},
	editor = {Micah B. Goldwater and Florencia Anggoro and Brett Hayes and Desmond C Ong},
	title = {A computational model of responsibility judgments from counterfactual simulations and intention inferences},
	url = {https://psyarxiv.com/uwdbr/},
	year = {2023},
	bdsk-url-1 = {https://psyarxiv.com/br2vz}}

@article{sosa2021dynamics,
	author = {Sosa, Felix A and Ullman, Tomer and Tenenbaum, Joshua B and Gershman, Samuel J and Gerstenberg, Tobias},
	date-added = {2022-06-27 13:53:13 +0100},
	date-modified = {2022-06-27 13:54:07 +0100},
	journal = {Cognition},
	pages = {104890},
	publisher = {Elsevier},
	title = {Moral dynamics: Grounding moral judgment in intuitive physics and intuitive psychology},
	volume = {217},
	year = {2021}}

@inproceedings{kosoy2022learning,
  title={Learning Causal Overhypotheses through Exploration in Children and Computational Models},
  author={Kosoy, Eliza and Liu, Adrian and Collins, Jasmine L and Chan, David and Hamrick, Jessica B and Ke, Nan Rosemary and Huang, Sandy and Kaufmann, Bryanna and Canny, John and Gopnik, Alison},
  booktitle={Conference on Causal Learning and Reasoning},
  pages={390--406},
  year={2022},
  organization={PMLR}
}

@article{stojnic2023commonsense,
  title={Commonsense psychology in human infants and machines},
  author={Stojni{\'c}, Gala and Gandhi, Kanishk and Yasuda, Shannon and Lake, Brenden M and Dillon, Moira R},
  journal={Cognition},
  volume={235},
  pages={105406},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{raileanu2018modeling,
  title={Modeling others using oneself in multi-agent reinforcement learning},
  author={Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  booktitle={International conference on machine learning},
  pages={4257--4266},
  year={2018},
  organization={PMLR}
}
@incollection{spelke2016core,
    author = {Spelke, Elizabeth S.},
    isbn = {9780190467630},
    title = "{279Core Knowledge and Conceptual Change: A Perspective on Social Cognition}",
    booktitle = "{Core Knowledge and Conceptual Change}",
    publisher = {Oxford University Press},
    year = {2016},
    month = {09},
    abstract = "{Two hallmarks of human cognitive development are (a) that concepts appear to get more powerful over human development and human history, and (b) that these changes appear to involve learning. But how can we learn about things that we cannot already represent? The difficulty of answering this question has led many to propose, in one way or another, that the appearance of concept enrichment through learning is illusory. However, some researchers, like Susan Carey, have insisted nevertheless that conceptual enrichment in development is real. This chapter argues that this idea is right: Over human development and human history, people learn concepts that they could not previously represent.}",
    doi = {10.1093/acprof:oso/9780190467630.003.0016},
    url = {https://doi.org/10.1093/acprof:oso/9780190467630.003.0016},
    eprint = {https://academic.oup.com/book/0/chapter/147719644/chapter-ag-pdf/44995424/book\_5121\_section\_147719644.ag.pdf},
}


@article{gandhi2021baby,
  title={Baby Intuitions Benchmark (BIB): Discerning the goals, preferences, and actions of others},
  author={Gandhi, Kanishk and Stojnic, Gala and Lake, Brenden M and Dillon, Moira R},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9963--9976},
  year={2021}
}

@article{gerstenberg2021counterfactual,
  title={A counterfactual simulation model of causal judgments for physical events.},
  author={Gerstenberg, Tobias and Goodman, Noah D and Lagnado, David A and Tenenbaum, Joshua B},
  journal={Psychological review},
  volume={128},
  number={5},
  pages={936},
  year={2021},
  publisher={American Psychological Association}
}

@article{langenhoff2021predicting,
  title={Predicting responsibility judgments from dispositional inferences and causal attributions},
  author={Langenhoff, Antonia F and Wiegmann, Alex and Halpern, Joseph Y and Tenenbaum, Joshua B and Gerstenberg, Tobias},
  journal={Cognitive Psychology},
  volume={129},
  pages={101412},
  year={2021},
  publisher={Elsevier}
}

@article{nguyen2023language,
  title={Language Models are Pragmatic Speakers},
  author={Nguyen, Khanh},
  journal={arXiv preprint arXiv:2305.17760},
  year={2023}
}
@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{tian2022understanding,
  title={Understanding deep contrastive learning via coordinate-wise optimization},
  author={Tian, Yuandong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19511--19522},
  year={2022}
}

@article{eysenbach2022contrastive,
  title={Contrastive learning as goal-conditioned reinforcement learning},
  author={Eysenbach, Benjamin and Zhang, Tianjun and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35603--35620},
  year={2022}
}

@article{liu2021return,
  title={Return-based contrastive representation learning for reinforcement learning},
  author={Liu, Guoqing and Zhang, Chuheng and Zhao, Li and Qin, Tao and Zhu, Jinhua and Li, Jian and Yu, Nenghai and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2102.10960},
  year={2021}
}




@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}


@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zelikman2024quiet,
  title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}




@inproceedings{poole2019variational,
  title={On variational bounds of mutual information},
  author={Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
  booktitle={International Conference on Machine Learning},
  pages={5171--5180},
  year={2019},
  organization={PMLR}
}

@article{anthony2017thinking,
  title={Thinking fast and slow with deep learning and tree search},
  author={Anthony, Thomas and Tian, Zheng and Barber, David},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{sinkhorn1967concerning,
  title={Concerning nonnegative matrices and doubly stochastic matrices},
  author={Sinkhorn, Richard and Knopp, Paul},
  journal={Pacific Journal of Mathematics},
  volume={21},
  number={2},
  pages={343--348},
  year={1967},
  publisher={Mathematical Sciences Publishers}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{park2024disentangling,
  title={Disentangling Length from Quality in Direct Preference Optimization},
  author={Park, Ryan and Rafailov, Rafael and Ermon, Stefano and Finn, Chelsea},
  journal={arXiv preprint arXiv:2403.19159},
  year={2024}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}


@misc{dubois2023alpacafarm,
      title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
      author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
      year={2023},
      eprint={2305.14387},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2023alpacaeval,
  title={Alpacaeval: An automatic evaluator of instruction-following models},
  author={Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{liu2024large,
  title={How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?},
  author={Liu, Ryan and Sumers, Theodore R and Dasgupta, Ishita and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2402.07282},
  year={2024}
}

@article{sumers2023reconciling,
  title={Reconciling truthfulness and relevance as epistemic and decision-theoretic utility.},
  author={Sumers, Theodore R and Ho, Mark K and Griffiths, Thomas L and Hawkins, Robert D},
  journal={Psychological Review},
  year={2023},
  publisher={American Psychological Association}
}


@article{niemoca,
  title={MoCa: Cognitive Scaffolding for Language Models in Causal and Moral Judgment Tasks},
  author={Nie, Allen and Zhang, Yuhui and Amdekar, Atharva and Piech, Christopher J and Hashimoto, Tatsunori and Gerstenberg, Tobias}
}

@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{frank2012predicting,
  title={Predicting pragmatic reasoning in language games},
  author={Frank, Michael C and Goodman, Noah D},
  journal={Science},
  volume={336},
  number={6084},
  pages={998--998},
  year={2012},
  publisher={American Association for the Advancement of Science}
}


@article{shapira2023clever,
    title={Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models},
    author={Shapira, Natalie and Levy, Mosh and Alavi, Seyed Hossein and Zhou, Xuhui and Choi, Yejin and Goldberg, Yoav and Sap, Maarten and Shwartz, Vered},
    journal={arXiv preprint arXiv:2305.14763},
    year={2023},
}

@article{goodman2016pragmatic,
  title={Pragmatic language interpretation as probabilistic inference},
  author={Goodman, Noah D and Frank, Michael C},
  journal={Trends in cognitive sciences},
  volume={20},
  number={11},
  pages={818--829},
  year={2016},
  publisher={Elsevier}
}

@article{sun2024principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{franken2023naive,
  title={Na{\"\i}ve information aggregation in human social learning},
  author={Fr{\"a}nken, Jan-Philipp and Valentin, Simon and Lucas, Christopher G and Bramley, Neil},
  year={2023},
  journal={PsyArXiv}
}

@article{hawthorne2019reasoning,
  title={Reasoning about social sources to learn from actions and outcomes.},
  author={Hawthorne-Madell, Daniel and Goodman, Noah D},
  journal={Decision},
  volume={6},
  number={1},
  pages={17},
  year={2019},
  publisher={Educational Publishing Foundation}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{dodell2011fmri,
  title={fMRI item analysis in a theory of mind task},
  author={Dodell-Feder, David and Koster-Hale, Jorie and Bedny, Marina and Saxe, Rebecca},
  journal={neuroimage},
  volume={55},
  number={2},
  pages={705--712},
  year={2011},
  publisher={Elsevier}
}

@article{baron1985does,
  title={Does the autistic child have a “theory of mind”?},
  author={Baron-Cohen, Simon and Leslie, Alan M and Frith, Uta},
  journal={Cognition},
  volume={21},
  number={1},
  pages={37--46},
  year={1985},
  publisher={Elsevier}
}

@book{wellman1992child,
  title={The child's theory of mind.},
  author={Wellman, Henry M},
  year={1992},
  publisher={The MIT Press}
}

@article{bowman2023eight,
  title={Eight things to know about large language models},
  author={Bowman, Samuel R},
  journal={arXiv preprint arXiv:2304.00612},
  year={2023}
}

@article{weidinger2023using,
  title={Using the Veil of Ignorance to align AI systems with principles of justice},
  author={Weidinger, Laura and McKee, Kevin R and Everett, Richard and Huang, Saffron and Zhu, Tina O and Chadwick, Martin J and Summerfield, Christopher and Gabriel, Iason},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={18},
  pages={e2213709120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{bowman2022measuring,
  title={Measuring progress on scalable oversight for large language models},
  author={Bowman, Samuel R and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukosuite, Kamile and Askell, Amanda and Jones, Andy and Chen, Anna and others},
  journal={arXiv preprint arXiv:2211.03540},
  year={2022}
}

@misc{frank2023experimentology,
  title={Experimentology: An open science approach to experimental psychology methods},
  author={Frank, Michael C and Braginsky, Mika and Cachia, Julie and Coles, Nicholas and Hardwicke, Tom and Hawkins, Robert and Mathur, Maya B and Williams, Rondeline},
  year={2023},
  publisher={MIT Press}
}

@article{openai_gpt4_2023,
    title = {{GPT-4 Technical Report}},
    author = {{OpenAI}},
    year = {2023},
    journal={arXiv preprint arXiv:2303.08774},
}

@article{efrat2020turking,
  title={The turking test: Can language models understand instructions?},
  author={Efrat, Avia and Levy, Omer},
  journal={arXiv preprint arXiv:2010.11982},
  year={2020}
}

@article{schick2021generating,
  title={Generating datasets with pretrained language models},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2104.07540},
  year={2021}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}

@article{handa2024bayesian,
  title={Bayesian preference elicitation with language models},
  author={Handa, Kunal and Gal, Yarin and Pavlick, Ellie and Goodman, Noah and Andreas, Jacob and Tamkin, Alex and Li, Belinda Z},
  journal={arXiv preprint arXiv:2403.05534},
  year={2024}
}

@inproceedings{ghazal2013bigbench,
  title={Bigbench: Towards an industry standard benchmark for big data analytics},
  author={Ghazal, Ahmad and Rabl, Tilmann and Hu, Minqing and Raab, Francois and Poess, Meikel and Crolotte, Alain and Jacobsen, Hans-Arno},
  booktitle={Proceedings of the 2013 ACM SIGMOD international conference on Management of data},
  pages={1197--1208},
  year={2013}
}

@article{li2023eliciting,
  title={Eliciting human preferences with language models},
  author={Li, Belinda Z and Tamkin, Alex and Goodman, Noah and Andreas, Jacob},
  journal={arXiv preprint arXiv:2310.11589},
  year={2023}
}

@article{kiela2021dynabench,
  title={Dynabench: Rethinking benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  journal={arXiv preprint arXiv:2104.14337},
  year={2021}
}

@article{perner1987three,
  title={Three-year-olds' difficulty with false belief: The case for a conceptual deficit},
  author={Perner, Josef and Leekam, Susan R and Wimmer, Heinz},
  journal={British journal of developmental psychology},
  volume={5},
  number={2},
  pages={125--137},
  year={1987},
  publisher={Wiley Online Library}
}
@article{wimmer1983beliefs,
  title={Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception},
  author={Wimmer, Heinz and Perner, Josef},
  journal={Cognition},
  volume={13},
  number={1},
  pages={103--128},
  year={1983},
  publisher={Elsevier}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}



@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{yin2024relative,
  title={Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts},
  author={Yin, Yueqin and Wang, Zhendong and Gu, Yi and Huang, Hai and Chen, Weizhu and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2402.10958},
  year={2024}
}


@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{kundu2023specific,
  title={Specific versus general principles for constitutional ai},
  author={Kundu, Sandipan and Bai, Yuntao and Kadavath, Saurav and Askell, Amanda and Callahan, Andrew and Chen, Anna and Goldie, Anna and Balwit, Avital and Mirhoseini, Azalia and McLean, Brayden and others},
  journal={arXiv preprint arXiv:2310.13798},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}


@article{trott2022large,
  title={Do Large Language Models know what humans know?},
  author={Trott, Sean and Jones, Cameron and Chang, Tyler and Michaelov, James and Bergen, Benjamin},
  journal={arXiv preprint arXiv:2209.01515},
  year={2022}
}

@misc{Gerganov2023,
  author = {Gerganov, Georgi},
  title = {llama.cpp},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ggerganov/llama.cpp}}
}


@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}


@article{sap2022neural,
  title={Neural theory-of-mind? on the limits of social intelligence in large lms},
  author={Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.13312},
  year={2022}
}

@inproceedings{baker2008theory,
  title={Theory-based social goal inference},
  author={Baker, Chris L and Goodman, Noah D and Tenenbaum, Joshua B},
  booktitle={Proceedings of the thirtieth annual conference of the cognitive science society},
  pages={1447--1452},
  year={2008},
  organization={Cognitive Science Society Austin, TX}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{ma2023tomchallenges,
  title={ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind},
  author={Ma, Xiaomeng and Gao, Lingyu and Xu, Qihui},
  journal={arXiv preprint arXiv:2305.15068},
  year={2023}
}

@article{sileo2023mindgames,
  title={Mindgames: Targeting theory of mind in large language models with dynamic epistemic modal logic},
  author={Sileo, Damien and Lernould, Antoine},
  journal={arXiv preprint arXiv:2305.03353},
  year={2023}
}


@article{holterman2023does,
  title={Does ChatGPT have Theory of Mind?},
  author={Holterman, Bart and van Deemter, Kees},
  journal={arXiv preprint arXiv:2305.14020},
  year={2023}
}


@article{palan2018prolific,
  title={Prolific. ac—A subject pool for online experiments},
  author={Palan, Stefan and Schitter, Christian},
  journal={Journal of Behavioral and Experimental Finance},
  volume={17},
  pages={22--27},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{goodman2009cause,
  title={Cause and intent: Social reasoning in causal learning},
  author={Goodman, Noah D and Baker, Chris L and Tenenbaum, Joshua B},
  booktitle={Proceedings of the 31st annual conference of the cognitive science society},
  pages={2759--2764},
  year={2009},
  organization={Cognitive Science Society Austin, TX}
}

@article{gergely2003teleological,
  title={Teleological reasoning in infancy: The na{\i}ve theory of rational action},
  author={Gergely, Gy{\"o}rgy and Csibra, Gergely},
  journal={Trends in cognitive sciences},
  volume={7},
  number={7},
  pages={287--292},
  year={2003},
  publisher={Elsevier}
}

@article{onishi200515,
  title={Do 15-month-old infants understand false beliefs?},
  author={Onishi, Kristine H and Baillargeon, Ren{\'e}e},
  journal={science},
  volume={308},
  number={5719},
  pages={255--258},
  year={2005},
  publisher={American Association for the Advancement of Science}
}

@article{leslie2004core,
  title={Core mechanisms in ‘theory of mind’},
  author={Leslie, Alan M and Friedman, Ori and German, Tim P},
  journal={Trends in cognitive sciences},
  volume={8},
  number={12},
  pages={528--533},
  year={2004},
  publisher={Elsevier}
}

@article{frith2005theory,
  title={Theory of mind},
  author={Frith, Chris and Frith, Uta},
  journal={Current biology},
  volume={15},
  number={17},
  pages={R644--R645},
  year={2005},
  publisher={Elsevier}
}

@article{premack1978does,
  title={Does the chimpanzee have a theory of mind?},
  author={Premack, David and Woodruff, Guy},
  journal={Behavioral and brain sciences},
  volume={1},
  number={4},
  pages={515--526},
  year={1978},
  publisher={Cambridge University Press}
}

@article{baker2017rational,
  title={Rational quantitative attribution of beliefs, desires and percepts in human mentalizing},
  author={Baker, Chris L and Jara-Ettinger, Julian and Saxe, Rebecca and Tenenbaum, Joshua B},
  journal={Nature Human Behaviour},
  volume={1},
  number={4},
  pages={0064},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{rabinowitz2018machine,
  title={Machine theory of mind},
  author={Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, SM Ali and Botvinick, Matthew},
  booktitle={International conference on machine learning},
  pages={4218--4227},
  year={2018},
  organization={PMLR}
}
@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  pages={e253},
  year={2017},
  publisher={Cambridge University Press}
}
@inproceedings{andreas-2022-language,
    title = "Language Models as Agent Models",
    author = "Andreas, Jacob",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.423",
    pages = "5769--5779",
    abstract = "Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them{---}a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents{'} communicative intentions influence their language. I survey findings from the recent literature showing that{---}even in today{'}s non-robust and error-prone models{---}LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.",
}

@incollection{grice1975logic,
  title={Logic and conversation},
  author={Grice, Herbert P},
  booktitle={Speech acts},
  pages={41--58},
  year={1975},
  publisher={Brill}
}

@book{williams1989style,
  title={Style: Ten lessons in clarity and grace},
  author={Williams, Joseph M and Nadel, Ira Bruce},
  year={1989},
  publisher={Scott, Foresman Glenview, IL}
}
@article{ericsson2017protocol,
  title={Protocol analysis},
  author={Ericsson, K Anders},
  journal={A companion to cognitive science},
  pages={425--432},
  year={2017},
  publisher={Wiley Online Library}
}

@article{durmus2023towards,
  title={Towards measuring the representation of subjective global opinions in language models},
  author={Durmus, Esin and Nyugen, Karina and Liao, Thomas I and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and others},
  journal={arXiv preprint arXiv:2306.16388},
  year={2023}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{deshpande2023toxicity,
  title={Toxicity in chatgpt: Analyzing persona-assigned language models},
  author={Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2304.05335},
  year={2023}
}

@misc{meta_llama_3_2024,
  author = {{Meta}},
  title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
  year = {2024},
  howpublished = {\url{https://www.meta.com/blog/meta-llama-3-introduction}},
  month = apr,
  day = {18},
  note = {Accessed: 2024-05-12}
}


@article{franken2023social,
  title={Social Contract AI: Aligning AI Assistants with Implicit Group Norms},
  author={Fr{\"a}nken, Jan-Philipp and Kwok, Sam and Ye, Peixuan and Gandhi, Kanishk and Arumugam, Dilip and Moore, Jared and Tamkin, Alex and Gerstenberg, Tobias and Goodman, Noah D},
  journal={arXiv preprint arXiv:2310.17769},
  year={2023}
}

@misc{occhipinti2023prodigy,
  title={PRODIGy: a PROfile-based DIalogue Generation dataset},
  author={Daniela Occhipinti and Serra Sinem Tekiroglu and Marco Guerini},
  year={2023},
  eprint={2311.05195},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{lichtenstein2006construction,
  title={The construction of preference: An overview},
  author={Lichtenstein, Sarah and Slovic, Paul},
  journal={The construction of preference},
  volume={1},
  pages={1--40},
  year={2006},
  publisher={Cambridge University Press Cambridge}
}

@article{fischhoff1988knowing,
  title={Knowing what you want: Measuring labile values},
  author={Fischhoff, Baruch and Slovic, Paul and Lichtenstein, Sarah},
  journal={Decision Making: Descriptive, Normative and Prescriptive Interactions, Cambridge University Press, Cambridge},
  pages={398--421},
  year={1988}
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


@article{wang2021calibrate,
  title={Calibrate your listeners! Robust communication-based training for pragmatic speakers},
  author={Wang, Rose E and White, Julia and Mu, Jesse and Goodman, Noah D},
  journal={arXiv preprint arXiv:2110.05422},
  year={2021}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}
@article{dasgupta2022language,
  title={Language models show human-like content effects on reasoning},
  author={Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  journal={arXiv preprint arXiv:2207.07051},
  year={2022}
}
@inproceedings{ganguli2022predictability,
  title={Predictability and surprise in large generative models},
  author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and others},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1747--1764},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{perez2022discovering,
  title={Discovering Language Model Behaviors with Model-Written Evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@article{andukuri2024star,
  title={STaR-GATE: Teaching Language Models to Ask Clarifying Questions},
  author={Andukuri, Chinmaya and Fr{\"a}nken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.19154},
  year={2024}
}


@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}


@article{kenton2021alignment,
  title={Alignment of language agents},
  author={Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2103.14659},
  year={2021}
}

@inproceedings{
    Lin2024ReAlign,
    title={The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning},
    author={Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
    booktitle={International Conference on Learning Representations},
    year={2024},
    url={https://arxiv.org/abs/2312.01552}
}

@article{moghaddam2023boosting,
  title={Boosting Theory-of-Mind Performance in Large Language Models via Prompting},
  author={Moghaddam, Shima Rahimi and Honey, Christopher J},
  journal={arXiv preprint arXiv:2304.11490},
  year={2023}
}

@article{sun2023salmon,
  title={Salmon: Self-alignment with principle-following reward models},
  author={Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2310.05910},
  year={2023}
}

@article{yang2023rlcd,
  title={Rlcd: Reinforcement learning from contrast distillation for language model alignment},
  author={Yang, Kevin and Klein, Dan and Celikyilmaz, Asli and Peng, Nanyun and Tian, Yuandong},
  journal={arXiv preprint arXiv:2307.12950},
  year={2023}
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle = {NeurIPS},
  year = 2020,
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{hosseini2024v,
  title={V-STaR: Training Verifiers for Self-Taught Reasoners},
  author={Hosseini, Arian and Yuan, Xingdi and Malkin, Nikolay and Courville, Aaron and Sordoni, Alessandro and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2402.06457},
  year={2024}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}



@article{ethayarajh2024kto,
  title={KTO: Model Alignment as Prospect Theoretic Optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{azar2023general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2310.12036},
  year={2023}
}

@article{tang2024generalized,
  title={Generalized Preference Optimization: A Unified Approach to Offline Alignment},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\'A}vila and Piot, Bilal},
  journal={arXiv preprint arXiv:2402.05749},
  year={2024}
}


@article{kosinski2023theory,
  title={Theory of mind may have spontaneously emerged in large language models},
  author={Kosinski, Michal},
  journal={arXiv preprint arXiv:2302.02083},
  year={2023}
}

@article{ullman2023large,
  title={Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks},
  author={Ullman, Tomer},
  journal={arXiv preprint arXiv:2302.08399},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{tamkin2022task,
  title={Task Ambiguity in Humans and Language Models},
  author={Tamkin, Alex and Handa, Kunal and Shrestha, Avash and Goodman, Noah},
  journal={arXiv preprint arXiv:2212.10711},
  year={2022}
}

@misc{chen2024selfplay,
      title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, 
      author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
      year={2024},
      eprint={2401.01335},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ziegler2020finetuning,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{stiennon2022learning,
      title={Learning to summarize from human feedback}, 
      author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
      year={2022},
      eprint={2009.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gregor2016variational,
      title={Variational Intrinsic Control}, 
      author={Karol Gregor and Danilo Jimenez Rezende and Daan Wierstra},
      year={2016},
      eprint={1611.07507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{eysenbach2018diversity,
      title={Diversity is All You Need: Learning Skills without a Reward Function}, 
      author={Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
      year={2018},
      journal={International Conference on Learning Representations}
}

@misc{sharma2020dynamicsaware,
      title={Dynamics-Aware Unsupervised Discovery of Skills}, 
      author={Archit Sharma and Shixiang Gu and Sergey Levine and Vikash Kumar and Karol Hausman},
      year={2020},
      journal={International Conference on Learning Representations}
}