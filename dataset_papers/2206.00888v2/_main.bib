
%% Created for Prasanta Ghosh on -01-20
@article{li2019jasper,
  title={Jasper: An end-to-end convolutional neural acoustic model},
  author={Li, Jason and Lavrukhin, Vitaly and Ginsburg, Boris and Leary, Ryan and Kuchaiev, Oleksii and Cohen, Jonathan M and Nguyen, Huyen and Gadde, Ravi Teja},
  journal={arXiv preprint arXiv:1904.03288},
  year={2019}
}

@inproceedings{kriman2020quartznet,
  title={{QuartzNet}: Deep automatic speech recognition with 1d time-channel separable convolutions},
  author={Kriman, Samuel and Beliaev, Stanislav and Ginsburg, Boris and Huang, Jocelyn and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Zhang, Yang},
  booktitle={ICASSP},
  pages={6124--6128},
  year={2020},
  organization={IEEE}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented {Transformer} for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and  Pang, Ruoming},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@article{han2020contextnet,
  title={{ContextNet}: Improving convolutional neural networks for automatic speech recognition with global context},
  author={Han, Wei and Zhang, Zhengdong and Zhang, Yu and Yu, Jiahui and Chiu, Chung-Cheng and Qin, James and Gulati, Anmol and Pang, Ruoming and Wu, Yonghui},
  journal={arXiv preprint arXiv:2005.03191},
  year={2020}
}

@article{majumdar2021citrinet,
  title={Citrinet: Closing the gap between non-autoregressive and autoregressive end-to-end models for automatic speech recognition},
  author={Majumdar, Somshubra and Balam, Jagadeesh and Hrinchuk, Oleksii and Lavrukhin, Vitaly and Noroozi, Vahid and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2104.01721},
  year={2021}
}

@inproceedings{guo2021recent,
  title={Recent developments on {ESPNet} toolkit boosted by {Conformer}},
  author={Guo, Pengcheng and others},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5874--5878},
  year={2021},
  organization={IEEE}
}

@article{zhang2020pushing,
  title={Pushing the limits of semi-supervised learning for automatic speech recognition},
  author={Zhang, Yu and Qin, James and Park, Daniel S and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V and Wu, Yonghui},
  journal={arXiv preprint arXiv:2010.10504},
  year={2020}
}

@article{ng2021pushing,
  title={Pushing the limits of non-autoregressive speech recognition},
  author={Ng, Edwin G and Chiu, Chung-Cheng and Zhang, Yu and Chan, William},
  journal={arXiv preprint arXiv:2104.03416},
  year={2021}
}

@article{lu2019understanding,
  title={Understanding and improving {Transformer} from a multi-particle dynamic system point of view},
  author={Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1906.02762},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{kim2021bert,
  title={{I-BERT}: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}

@article{yu2021nn,
  title={{NN-LUT}: Neural Approximation of Non-Linear Operations for Efficient {Transformer} Inference},
  author={Yu, Joonsang and Park, Junki and Park, Seongmin and Kim, Minsoo and Lee, Sihwa and Lee, Dong Hyun and Choi, Jungwook},
  journal={arXiv preprint arXiv:2112.02191},
  year={2021}
}

@ONLINE{nvdla_primer,
  author={{NVDLA Primer}},
  title = {http://nvdla.org/primer.html},
  year = 2021
}

@inproceedings{ronneberger2015u,
  title={U-{Net}: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{dollar2021fast,
  title={Fast and accurate model scaling},
  author={Doll{\'a}r, Piotr and Singh, Mannat and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={924--932},
  year={2021}
}

@inproceedings{battenberg2017exploring,
  title={Exploring neural transducers for end-to-end speech recognition},
  author={Battenberg, Eric and Chen, Jitong and Child, Rewon and Coates, Adam and Li, Yashesh Gaur Yi and Liu, Hairong and Satheesh, Sanjeev and Sriram, Anuroop and Zhu, Zhenyao},
  booktitle={2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={206--213},
  year={2017},
  organization={IEEE}
}

@inproceedings{dwsconv,
  title={Learning visual representations at scale},
  author={Vanhoucke, Vincent},
  journal={ICLR},
  year={2014}
}

@article{sifre2014rigid,
  title={Rigid-motion scattering for texture classification},
  author={Sifre, Laurent and Mallat, St{\'e}phane},
  journal={arXiv preprint arXiv:1403.1687},
  year={2014}
}

@article{howard2017mobilenets,
  title={{MobileNets}: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@inproceedings{hu2018squeeze,
  title={Squeeze-and-{Excitation} networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}

@inproceedings{zhang2020transformer,
  title={Transformer transducer: A streamable speech recognition model with {Transformer} encoders and {RNN-T} loss},
  author={Zhang, Qian and Lu, Han and Sak, Hasim and Tripathi, Anshuman and McDermott, Erik and Koo, Stephen and Kumar, Shankar},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7829--7833},
  year={2020},
  organization={IEEE}
}

@inproceedings{povey2018time,
  title={A time-restricted self-attention layer for {ASR}},
  author={Povey, Daniel and Hadian, Hossein and Ghahremani, Pegah and Li, Ke and Khudanpur, Sanjeev},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5874--5878},
  year={2018},
  organization={IEEE}
}

@inproceedings{liu2021improving,
  title={Improving {RNN Transducer} based {ASR} with auxiliary tasks},
  author={Liu, Chunxi and Zhang, Frank and Le, Duc and Kim, Suyoun and Saraf, Yatharth and Zweig, Geoffrey},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={172--179},
  year={2021},
  organization={IEEE}
}

@article{zhang2020faster,
  title={Faster, simpler and more accurate hybrid asr systems using wordpieces},
  author={Zhang, Frank and Wang, Yongqiang and Zhang, Xiaohui and Liu, Chunxi and Saraf, Yatharth and Zweig, Geoffrey},
  journal={arXiv preprint arXiv:2005.09150},
  year={2020}
}

@article{likhomanenko2020rethinking,
  title={Rethinking evaluation in {ASR}: {Are} our models robust enough?},
  author={Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Tomasello, Paden and Kahn, Jacob and Avidov, Gilad and Collobert, Ronan and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2010.11745},
  year={2020}
}

@article{burchi2021efficient,
  title={{Efficient Conformer}: Progressive downsampling and grouped attention for automatic speech recognition},
  author={Burchi, Maxime and Vielzeuf, Valentin},
  journal={arXiv preprint arXiv:2109.01163},
  year={2021}
}

@inproceedings{graves2006connectionist,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006}
}

@article{graves2012sequence,
  title={Sequence transduction with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1211.3711},
  year={2012}
}

@inproceedings{karita2019comparative,
  title={A comparative study on {Transformer vs RNN} in speech applications},
  author={Karita, Shigeki and others},
  booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={449--456},
  year={2019},
  organization={IEEE}
}

@inproceedings{wang2020transformer,
  title={Transformer-based acoustic modeling for hybrid speech recognition},
  author={Wang, Yongqiang and others},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6874--6878},
  year={2020},
  organization={IEEE}
}

@inproceedings{han2019state,
  title={State-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions},
  author={Han, Kyu J and Prieto, Ramon and Ma, Tao},
  booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={54--61},
  year={2019},
  organization={IEEE}
}

@article{luscher2019rwth,
  title={{RWTH ASR Systems for LibriSpeech}: Hybrid vs Attention--w/o Data Augmentation},
  author={L{\"u}scher, Christoph and Beck, Eugen and Irie, Kazuki and Kitza, Markus and Michel, Wilfried and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  journal={arXiv preprint arXiv:1905.03072},
  year={2019}
}

@article{pan2020asapp,
  title={{ASAPP-ASR: Multistream CNN and self-attentive SRU for SOTA speech recognition}},
  author={Pan, Jing and Shapiro, Joshua and Wohlwend, Jeremy and Han, Kyu J and Lei, Tao and Ma, Tao},
  journal={arXiv preprint arXiv:2005.10469},
  year={2020}
}

@article{synnaeve2019end,
  title={{End-to-end ASR: From} supervised to semi-supervised learning with modern architectures},
  author={Synnaeve, Gabriel and Xu, Qiantong and Kahn, Jacob and Likhomanenko, Tatiana and Grave, Edouard and Pratap, Vineel and Sriram, Anuroop and Liptchinsky, Vitaliy and Collobert, Ronan},
  journal={arXiv preprint arXiv:1911.08460},
  year={2019}
}


@article{baevski2020wav2vec,
  title={Wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{hsu2021hubert,
  title={{HuBERT}: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@inproceedings{wang2021unispeech,
  title={Unispeech: Unified speech representation learning with labeled and unlabeled data},
  author={Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  booktitle={International Conference on Machine Learning},
  pages={10937--10947},
  year={2021},
  organization={PMLR}
}

@article{chen2021wavlm,
  title={{WavLM}: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={arXiv preprint arXiv:2110.13900},
  year={2021}
}

@article{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  journal={arXiv preprint arXiv:2202.03555},
  year={2022}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the {Transformer} architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@article{nguyen2019transformers,
  title={Transformers without tears: Improving the normalization of self-attention},
  author={Nguyen, Toan Q and Salazar, Julian},
  journal={arXiv preprint arXiv:1910.05895},
  year={2019}
}
@article{wang2022deepnet,
  title={{DeepNet: Scaling Transformers} to 1,000 Layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}

@inproceedings{brock2021high,
  title={High-performance large-scale image recognition without normalization},
  author={Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen},
  booktitle={International Conference on Machine Learning},
  pages={1059--1071},
  year={2021},
  organization={PMLR}
}

@inproceedings{panayotov2015librispeech,
  title={{Librispeech: an ASR corpus based on public domain audio books}},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

@article{park2019specaugment,
  title={Specaugment: {A} simple data augmentation method for automatic speech recognition},
  author={Park, Daniel S and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.08779},
  year={2019}
}
@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@inproceedings{ko2015audio,
  title={Audio augmentation for speech recognition},
  author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={Sixteenth annual conference of the international speech communication association},
  year={2015}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}
@article{liu2022convnet,
  title={A ConvNet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  journal={arXiv preprint arXiv:2201.03545},
  year={2022}
}
@inproceedings{zhang2021usefulness,
  title={On the usefulness of self-attention for automatic speech recognition with {Transformers}},
  author={Zhang, Shucong and Loweimi, Erfan and Bell, Peter and Renals, Steve},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={89--96},
  year={2021},
  organization={IEEE}
}

@article{zhang2020stochastic,
  title={Stochastic attention head removal: A simple and effective method for improving {Transformer} based {ASR} models},
  author={Zhang, Shucong and Loweimi, Erfan and Bell, Peter and Renals, Steve},
  journal={arXiv preprint arXiv:2011.04004},
  year={2020}
}

@inproceedings{chang2020end,
  title={{End-to-end ASR} with Adaptive Span Self-Attention.},
  author={Chang, Xuankai and Subramanian, Aswin Shanmugam and Guo, Pengcheng and Watanabe, Shinji and Fujita, Yuya and Omachi, Motoi},
  booktitle={INTERSPEECH},
  pages={3595--3599},
  year={2020}
}

@inproceedings{shim2021understanding,
  title={Understanding the role of self attention for efficient speech recognition},
  author={Shim, Kyuhong and Choi, Jungwook and Sung, Wonyong},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{perslev2019u,
  title={{U-Time}: A fully convolutional network for time series segmentation applied to sleep staging},
  author={Perslev, Mathias and Jensen, Michael and Darkner, Sune and Jennum, Poul J{\o}rgen and Igel, Christian},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional {Transformers} for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: {Transformers} for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{fan2021multiscale,
  title={{Multiscale Vision Transformers}},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6824--6835},
  year={2021}
}

@article{li2021improved,
  title={Improved {Multiscale Vision Transformers} for classification and detection},
  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2112.01526},
  year={2021}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image {Transformers} \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}


@article{zhang2017towards,
  title={Towards end-to-end speech recognition with deep convolutional neural networks},
  author={Zhang, Ying and Pezeshki, Mohammad and Brakel, Phil{\'e}mon and Zhang, Saizheng and Bengio, Cesar Laurent Yoshua and Courville, Aaron},
  journal={arXiv preprint arXiv:1701.02720},
  year={2017}
}
@ONLINE{nemo,
  author={{NVDIA Nemo}},
  title = {https://github.com/NVIDIA/NeMo},
}


@ONLINE{squezeformer_github,
  author={Kim, Sehoon},
  title = {https://github.com/kssteven418/Squeezeformer},
}


@article{garofolo1993timit,
  title={Timit acoustic phonetic continuous speech corpus},
  author={Garofolo, John S},
  journal={Linguistic Data Consortium, 1993},
  year={1993}
}
@inproceedings{zhang2021benchmarking,
  title={Benchmarking LF-MMI, CTC And RNN-T Criteria For Streaming ASR},
  author={Zhang, Xiaohui and Zhang, Frank and Liu, Chunxi and Schubert, Kjell and Chan, Julian and Prakash, Pradyot and Liu, Jun and Yeh, Ching-Feng and Peng, Fuchun and Saraf, Yatharth and others},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={46--51},
  year={2021},
  organization={IEEE}
}

@article{geng2018hardware1,
  title={Hardware-aware exponential approximation for deep neural network},
  author={Geng, Xue and Lin, Jie and Zhao, Bin and Wang, Zhe and Aly, Mohamed M Sabry and Chandrasekhar, Vijay},
  year={2018}
}

@inproceedings{geng2018hardware2,
  title={Hardware-aware softmax approximation for deep neural networks},
  author={Geng, Xue and Lin, Jie and Zhao, Bin and Kong, Anmin and Aly, Mohamed M Sabry and Chandrasekhar, Vijay},
  booktitle={Asian Conference on Computer Vision},
  pages={107--122},
  year={2018},
  organization={Springer}
}