\begin{thebibliography}{10}

\bibitem{schlkopf2002learning}
Bernhard Sch{\"o}lkopf and Alexander~J. Smola.
\newblock {\em {Learning with Kernels: Support Vector Machines, Regularization,
  Optimization, and Beyond (Adaptive Computation and Machine Learning)}}.
\newblock MIT Press, 2002.

\bibitem{conf/icml/SmolaS00}
Alex~J. Smola and Bernhard Sch{\"o}lkopf.
\newblock {Sparse Greedy Matrix Approximation for Machine Learning}.
\newblock In {\em {ICML}}, pages 911--918. Morgan Kaufmann, 2000.

\bibitem{conf/nips/WilliamsS00}
Christopher Williams and Matthias Seeger.
\newblock {Using the Nystr{\"o}m Method to Speed Up Kernel Machines}.
\newblock In {\em {NIPS}}, pages 682--688. MIT Press, 2000.

\bibitem{conf/nips/RahimiR07}
Ali Rahimi and Benjamin Recht.
\newblock {Random Features for Large-Scale Kernel Machines}.
\newblock In {\em {NIPS}}, pages 1177--1184. Curran Associates, Inc., 2007.

\bibitem{conf/icml/YangSAM14}
Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael~W. Mahoney.
\newblock {Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels}.
\newblock In {\em {ICML}}, volume~32 of {\em {JMLR Proceedings}}, pages
  485--493. JMLR.org, 2014.

\bibitem{conf/icml/LeSS13}
Quoc~V. Le, Tam{\'a}s Sarl{\'o}s, and Alexander~J. Smola.
\newblock {Fastfood - Computing Hilbert Space Expansions in loglinear time}.
\newblock In {\em {ICML}}, volume~28 of {\em {JMLR Proceedings}}, pages
  244--252. JMLR.org, 2013.

\bibitem{conf/icml/SiHD14}
Si~Si, Cho-Jui Hsieh, and Inderjit~S. Dhillon.
\newblock {Memory Efficient Kernel Approximation}.
\newblock In {\em {ICML}}, volume~32 of {\em {JMLR Proceedings}}, pages
  701--709. JMLR.org, 2014.

\bibitem{conf/colt/ZhangDW13}
Yuchen Zhang, John~C. Duchi, and Martin~J. Wainwright.
\newblock {Divide and Conquer Kernel Ridge Regression}.
\newblock In {\em {COLT}}, volume~30 of {\em {JMLR Proceedings}}, pages
  592--617. JMLR.org, 2013.

\bibitem{conf/nips/KumarMT09}
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
\newblock {Ensemble Nystrom Method}.
\newblock In {\em {NIPS}}, pages 1060--1068. Curran Associates, Inc., 2009.

\bibitem{conf/icml/LiKL10}
Mu~Li, James~T. Kwok, and Bao-Liang Lu.
\newblock {Making Large-Scale Nystr{\"o}m Approximation Possible}.
\newblock In {\em {ICML}}, pages 631--638. Omnipress, 2010.

\bibitem{Zhang:2008:INL:1390156.1390311}
Kai Zhang, Ivor~W. Tsang, and James~T. Kwok.
\newblock {Improved Nystr{\"o}m Low-rank Approximation and Error Analysis}.
\newblock {ICML}, pages 1232--1239. ACM, 2008.

\bibitem{conf/nips/DaiXHLRBS14}
Bo~Dai, Bo~Xie 0002, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan,
  and Le~Song.
\newblock {Scalable Kernel Methods via Doubly Stochastic Gradients}.
\newblock In {\em {NIPS}}, pages 3041--3049, 2014.

\bibitem{Drineas:2005:NMA:1046920.1194916}
Petros Drineas and Michael~W. Mahoney.
\newblock {On the Nystr{\"o}m Method for Approximating a Gram Matrix for
  Improved Kernel-Based Learning}.
\newblock {\em JMLR}, 6:2153--2175, December 2005.

\bibitem{gittens2013revisiting}
Alex Gittens and Michael~W. Mahoney.
\newblock {Revisiting the Nystrom method for improved large-scale machine
  learning.}
\newblock 28:567--575, 2013.

\bibitem{Wang:2013:ICM:2567709.2567748}
Shusen Wang and Zhihua Zhang.
\newblock {Improving CUR Matrix Decomposition and the Nystr{\"o}m Approximation
  via Adaptive Sampling}.
\newblock {\em JMLR}, 14(1):2729--2769, 2013.

\bibitem{journals/jmlr/DrineasMMW12}
Petros Drineas, Malik Magdon-Ismail, Michael~W. Mahoney, and David~P. Woodruff.
\newblock {Fast approximation of matrix coherence and statistical leverage}.
\newblock {\em JMLR}, 13:3475--3506, 2012.

\bibitem{conf/innovations/CohenLMMPS15}
Michael~B. Cohen, Yin~Tat Lee, Cameron Musco, Christopher Musco, Richard Peng,
  and Aaron Sidford.
\newblock {Uniform Sampling for Matrix Approximation}.
\newblock In {\em {ITCS}}, pages 181--190. ACM, 2015.

\bibitem{conf/aistats/WangZ14}
Shusen Wang and Zhihua Zhang.
\newblock {Efficient Algorithms and Error Analysis for the Modified Nystrom
  Method}.
\newblock In {\em {AISTATS}}, volume~33 of {\em {JMLR Proceedings}}, pages
  996--1004. JMLR.org, 2014.

\bibitem{Kumar:2012:SMN:2503308.2343678}
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
\newblock {Sampling Methods for the Nystr{\"o}m Method}.
\newblock {\em JMLR}, 13(1):981--1006, 2012.

\bibitem{journals/jmlr/CortesMT10}
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar.
\newblock {On the Impact of Kernel Approximation on Learning Accuracy}.
\newblock In {\em {AISTATS}}, volume~9 of {\em {JMLR Proceedings}}, pages
  113--120. JMLR.org, 2010.

\bibitem{6547995}
Rong Jin, Tianbao Yang, M.~Mahdavi, Yu-Feng Li, and Zhi-Hua Zhou.
\newblock {Improved Bounds for the Nystr{\"o}m Method With Application to
  Kernel Classification}.
\newblock {\em Information Theory, IEEE Transactions on}, 59(10):6939--6949,
  Oct 2013.

\bibitem{conf/nips/YangLMJZ12}
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou.
\newblock {Nystr{\"o}m Method vs Random Fourier Features: A Theoretical and
  Empirical Comparison}.
\newblock In {\em {NIPS}}, pages 485--493, 2012.

\bibitem{conf/colt/Bach13}
Francis Bach.
\newblock {Sharp analysis of low-rank kernel matrix approximations}.
\newblock In {\em {COLT}}, volume~30 of {\em {JMLR Proceedings}}, pages
  185--209. JMLR.org, 2013.

\bibitem{alaoui2014fast}
Ahmed Alaoui and Michael~W Mahoney.
\newblock {Fast Randomized Kernel Methods With Statistical Guarantees}.
\newblock {\em arXiv}, 2014.

\bibitem{steinwart2008support}
I.~Steinwart and A.~Christmann.
\newblock {\em {Support Vector Machines}}.
\newblock {Information Science and Statistics}. Springer New York, 2008.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto {De Vito}.
\newblock {Optimal rates for the regularized least-squares algorithm}.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{journals/neco/GerfoROVV08}
L.~Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto~De Vito, and Alessandro
  Verri.
\newblock {Spectral Algorithms for Supervised Learning}.
\newblock {\em Neural Computation}, 20(7):1873--1897, 2008.

\bibitem{SteinwartHS09}
I.~Steinwart, D.~R. Hush, and C.~Scovel.
\newblock {Optimal Rates for Regularized Least Squares Regression}.
\newblock In {\em {COLT}}, 2009.

\bibitem{mendelson2010regularization}
S.~Mendelson and J.~Neeman.
\newblock {Regularization in kernel learning}.
\newblock {\em The Annals of Statistics}, 38(1):526--565, 2010.

\bibitem{bauer}
F.~Bauer, S.~Pereverzev, and L.~Rosasco.
\newblock {On regularization algorithms in learning theory}.
\newblock {\em Journal of complexity}, 23(1):52--72, 2007.

\bibitem{CapYao06}
A.~Caponnetto and Yuan Yao.
\newblock {Adaptive rates for regularization operators in learning theory}.
\newblock {\em Analysis and Applications}, 08, 2010.

\bibitem{yiming}
Y.~Ying and M.~Pontil.
\newblock Online gradient descent learning algorithms.
\newblock {\em Foundations of Computational Mathematics}, 8(5):561--596, 2008.

\bibitem{rudi2013sample}
Alessandro Rudi, Guillermo~D Canas, and Lorenzo Rosasco.
\newblock {On the Sample Complexity of Subspace Learning}.
\newblock In {\em {NIPS}}, pages 2067--2075, 2013.

\bibitem{Golub1996}
Gene~H. Golub and Charles F.~Van Loan.
\newblock {\em {Matrix Computations}}.
\newblock 3rd edition, 1996.

\end{thebibliography}
